[29.05.2025 13:25] Read previous papers.
[29.05.2025 13:25] Generating top page (month).
[29.05.2025 13:25] Writing top page (month).
[29.05.2025 14:11] Read previous papers.
[29.05.2025 14:11] Get feed.
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22617
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21600
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22651
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22312
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22453
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20411
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21136
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22334
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22457
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21925
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19253
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18600
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22232
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19075
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21887
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22129
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22648
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19187
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17663
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22019
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22202
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20779
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22613
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22525
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22523
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22338
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22203
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21876
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18700
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17870
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15813
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21960
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21191
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20715
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17507
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12667
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22645
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21582
[29.05.2025 14:11] Extract page data from URL. URL: https://huggingface.co/papers/2505.20444
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20298
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19051
[29.05.2025 14:11] Extract page data from URL. URL: https://huggingface.co/papers/2505.18227
[29.05.2025 14:11] Extract page data from URL. URL: https://huggingface.co/papers/2505.22664
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21060
[29.05.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18149
[29.05.2025 14:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[29.05.2025 14:11] No deleted papers detected.
[29.05.2025 14:11] Downloading and parsing papers (pdf, html). Total: 45.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22617.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22617.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22617.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.21600.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.21600.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.21600.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22651.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22651.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22651.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22312.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22312.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22312.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22453.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22453.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22453.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.20411.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.20411.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.20411.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.21136.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.21136.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.21136.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22334.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22334.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22334.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22457.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22457.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22457.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.21925.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.21925.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.21925.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.19253.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.19253.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.19253.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.18600.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.18600.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.18600.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22232.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22232.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22232.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.19075.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.19075.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.19075.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.21887.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.21887.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.21887.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22129.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22129.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22129.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22648.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22648.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22648.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.19187.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.19187.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.19187.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.17663.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.17663.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.17663.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22019.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22019.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22019.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22202.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22202.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22202.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.20779.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.20779.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.20779.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22613.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22613.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22613.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22525.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22525.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22525.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22523.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22523.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22523.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22338.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22338.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22338.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22203.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22203.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22203.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.21876.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.21876.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.21876.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.18700.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.18700.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.18700.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.17870.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.17870.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.17870.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.15813.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.15813.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.15813.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.21960.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.21960.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.21960.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.21191.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.21191.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.21191.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.20715.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.20715.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.20715.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.17507.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.17507.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.17507.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.12667.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.12667.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.12667.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22645.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.22645.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.22645.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.21582.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.21582.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.21582.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.20444.
[29.05.2025 14:11] Downloading paper 2505.20444 from http://arxiv.org/pdf/2505.20444v1...
[29.05.2025 14:11] Extracting affiliations from text.
[29.05.2025 14:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 1 4 4 4 0 2 . 5 0 5 2 : r Preprint. HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models Haoran Li1, Yingjie Qin2, Baoyuan Ou2, Lai Xu2, Ruiwen Xu2 1Carnegie Mellon University 2Xiaohongshu Inc. "
[29.05.2025 14:11] Response: ```python
["Carnegie Mellon University", "Xiaohongshu Inc."]
```
[29.05.2025 14:11] Deleting PDF ./assets/pdf/2505.20444.pdf.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.20298.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.20298.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.20298.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.19051.
[29.05.2025 14:11] Extra JSON file exists (./assets/json/2505.19051.json), skip PDF parsing.
[29.05.2025 14:11] Paper image links file exists (./assets/img_data/2505.19051.json), skip HTML parsing.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.18227.
[29.05.2025 14:11] Downloading paper 2505.18227 from http://arxiv.org/pdf/2505.18227v1...
[29.05.2025 14:11] Extracting affiliations from text.
[29.05.2025 14:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 2 ] . [ 1 7 2 2 8 1 . 5 0 5 2 : r Token Reduction Should Go Beyond Efficiency in Generative Models From Vision, Language to Multimodality Zhenglun Kong1, Yize Li2, Fanhu Zeng3, Lei Xin4,6, Shvat Messica1, Xue Lin2, Pu Zhao2, Manolis Kellis5, Hao Tang6, Marinka Zitnik1 1Harvard University, 2Northeastern University, 3Chinese Academy of Sciences, 4Wuhan University, 5Massachusetts Institute of Technology, 6Peking University, {zhenglun_kong,marinka}@hms.harvard.edu, li.yize@northeastern.edu "
[29.05.2025 14:11] Response: ```python
[
    "Harvard University",
    "Northeastern University",
    "Chinese Academy of Sciences",
    "Wuhan University",
    "Massachusetts Institute of Technology",
    "Peking University"
]
```
[29.05.2025 14:11] Deleting PDF ./assets/pdf/2505.18227.pdf.
[29.05.2025 14:11] Success.
[29.05.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2505.22664.
[29.05.2025 14:11] Downloading paper 2505.22664 from http://arxiv.org/pdf/2505.22664v1...
[29.05.2025 14:12] Extracting affiliations from text.
[29.05.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Zero-Shot Vision Encoder Grafting via LLM Surrogates Kaiyu Yue Vasu Singla Menglin Jia Meta https://github.com/facebookresearch/zero 5 2 0 2 8 2 ] . [ 1 4 6 6 2 2 . 5 0 5 2 : r Figure 1. Zero-shot vision encoder grafting via small language surrogate (srgt) model to trigger the target LLM to perform visual understanding task without any additional training. Figure 2. Reducing full decoder training cost with our surrogatetrained encoder for Llama-70B in VLMs. Hollow (cid:155) indicates the average score of the surrogate-trained encoder on the left. "
[29.05.2025 14:12] Response: ```python
["Meta"]
```
[29.05.2025 14:12] Deleting PDF ./assets/pdf/2505.22664.pdf.
[29.05.2025 14:12] Success.
[29.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.21060.
[29.05.2025 14:12] Extra JSON file exists (./assets/json/2505.21060.json), skip PDF parsing.
[29.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.21060.json), skip HTML parsing.
[29.05.2025 14:12] Success.
[29.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.18149.
[29.05.2025 14:12] Extra JSON file exists (./assets/json/2505.18149.json), skip PDF parsing.
[29.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.18149.json), skip HTML parsing.
[29.05.2025 14:12] Success.
[29.05.2025 14:12] Enriching papers with extra data.
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 0. This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished ...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 1. Roads to Rome (R2R) selectively utilizes large language models for critical reasoning tasks to enhance efficiency and performance in lightweight models.  					AI-generated summary 				 Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhea...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 2. Sherlock, a self-correction and self-improvement framework for reasoning vision-language models, enhances accuracy across benchmarks using limited annotated data.  					AI-generated summary 				 Reasoning Vision-Language Models (VLMs) have shown promising performance on complex multimodal tasks. How...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 3. The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on th...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 4. Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal data--an ultimately unsustainable resource. While rec...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 5. A novel pipeline extracts real-world, interactive software engineering tasks from GitHub to create SWE-rebench, improving the evaluation of reinforcement learning models in SWE.  					AI-generated summary 				 LLM-based agents have shown promising capabilities in a growing range of software engineer...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 6. The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by utilizing quantization to accelerate matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster ins...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 7. Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While "aha moment" patterns--where models exhibit self-correction through reflection--are often attribut...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 8. Next-event prediction (NEP) is proposed as a learning task to enable MLLMs to reason temporally over video inputs, using future video segments as a self-supervised signal.  					AI-generated summary 				 Next-token prediction serves as the foundational learning task enabling reasoning in LLMs. But w...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 9. We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning. Instead of taking a physics-centric approach to rendering, we formula...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 10. DeepResearchGym provides an open-source evaluation framework for deep research systems using a reproducible search API and LLM-as-a-judge assessments.  					AI-generated summary 				 Deep research systems represent an emerging class of agentic information retrieval methods that generate comprehensiv...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 11. Chain-of-Zoom (CoZ) enhances single-image super-resolution models by using an autoregressive chain of intermediate scale-states and multi-scale-aware prompts to achieve extreme magnifications with high quality.  					AI-generated summary 				 Modern single-image super-resolution (SISR) models delive...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 12. JQL systematically curates high-quality multilingual training data using pretrained multilingual embeddings, outperforming heuristic methods and improving downstream model training across diverse languages.  					AI-generated summary 				 High-quality multilingual training data is essential for effe...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 13. UniR, a lightweight reasoning module, enhances Large Language Models with specialized reasoning abilities through modular composition, improving performance and generalization at lower computational costs.  					AI-generated summary 				 Large Language Models (LLMs) have demonstrated remarkable gene...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 14. SVRPBench introduces a new benchmark for vehicle routing under uncertainty, simulating realistic urban conditions and highlighting the limitations of state-of-the-art RL solvers.  					AI-generated summary 				 Robust routing under uncertainty is central to real-world logistics, yet most benchmarks ...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 15. Analysis of fine-tuning diffusion models for panoramic image generation reveals distinct roles of attention module matrices and introduces UniPano, a memory-efficient and speed-enhanced baseline framework.  					AI-generated summary 				 Recent prosperity of text-to-image diffusion models, e.g. Stab...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 16. Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-t...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 17. A framework called PIR refines the importance of reasoning steps in large language models by pruning low-importance functional elements, leading to more concise reasoning chains with improved accuracy and reduced computational demands.  					AI-generated summary 				 Large language models (LLMs) hav...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 18. The DynToM benchmark evaluates LLMs' ability to track and understand the temporal progression of mental states, revealing significant gaps compared to human performance.  					AI-generated summary 				 As Large Language Models (LLMs) increasingly participate in human-AI interactions, evaluating thei...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 19. VRAG-RL, a reinforcement learning framework, enhances reasoning and visual information handling in RAG methods by integrating visual perception tokens and employing specialized action spaces and rewards.  					AI-generated summary 				 Effectively retrieving, reasoning and understanding visually ric...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 20. Pretrained language models can adapt to operate in sentence space, reason over structured semantic units, and achieve competitive performance with Chain-of-Thought while reducing inference-time FLOPs.  					AI-generated summary 				 Autoregressive language models (LMs) generate one token at a time, ...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 21. A large-scale knowledge base of recombination examples is built from scientific paper abstracts using an LLM-based extraction model to analyze and inspire new creative directions in AI.  					AI-generated summary 				 A hallmark of human innovation is the process of recombination -- creating origina...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 22. A novel iterative framework, RICO, improves image caption accuracy by using visual reconstruction and a text-to-image model to refine discrepancies, while RICO-Flash enhances efficiency using DPO.  					AI-generated summary 				 Image recaptioning is widely used to generate training datasets with en...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 23. Thinking with Generated Images allows large multimodal models to generate and critique intermediate visual steps, enhancing visual reasoning capabilities and achieving significant improvements in complex scenarios.  					AI-generated summary 				 We present Thinking with Generated Images, a novel pa...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 24. Generating high-quality, multi-layer transparent images from text prompts can unlock a new level of creative control, allowing users to edit each layer as effortlessly as editing text outputs from LLMs. However, the development of multi-layer generative models lags behind that of conventional text-t...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 25. Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model param...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 26. The study examines the effectiveness and reliability of rule-based and model-based verifiers in reinforcement learning with verifiable reward, highlighting limitations and vulnerabilities in their use for mathematical reasoning tasks.  					AI-generated summary 				 Trustworthy verifiers are essenti...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 27. EPiC is a framework for efficient 3D camera control in video diffusion models that constructs high-quality anchor videos through first-frame visibility masking, integrates them using a lightweight ControlNet module, and achieves state-of-the-art performance on I2V tasks with minimal resources.  				...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 28. Recent advances in Visual Language Models (VLMs) have demonstrated exceptional performance in visual reasoning tasks. However, geo-localization presents unique challenges, requiring the extraction of multigranular visual cues from images and their integration with external world knowledge for system...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 29. A generative AI model is fine-tuned with labeled falsehoods to reduce misinformation generation, analogous to biological immunization.  					AI-generated summary 				 Generative AI models often learn and reproduce false information present in their training corpora. This position paper argues that, ...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 30. BraInCoRL employs a transformer-based in-context learning approach to model higher visual cortex neural responses with few-shot examples, demonstrating superior performance and generalizability across new subjects, stimuli, and datasets.  					AI-generated summary 				 Understanding functional repre...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 31. Time-independent Unified Encoder TiUE reduces inference time and improves diversity and quality in Text-to-Image diffusion models by sharing encoder features across decoder time steps.  					AI-generated summary 				 Text-to-Image (T2I) diffusion models have made remarkable advancements in generativ...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 32. The finetuning of Large Language Models (LLMs) has significantly advanced their instruction-following capabilities, yet the underlying computational mechanisms driving these improvements remain poorly understood. This study systematically examines how fine-tuning reconfigures LLM computations by iso...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 33. MUSEG, an RL-based method with timestamp-aware multi-segment grounding, significantly enhances the temporal understanding of large language models by improving alignment with video segments and demonstrating superior performance in temporal reasoning tasks.  					AI-generated summary 				 Video temp...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 34. HuggingKG, a large-scale knowledge graph, enhances open source ML resource management by enabling advanced queries and analyses via HuggingBench.  					AI-generated summary 				 The rapid growth of open source machine learning (ML) resources, such as models and datasets, has accelerated IR research....
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 35. Safe-Sora embeds invisible watermarks into AI-generated videos using a hierarchical adaptive matching mechanism and a 3D wavelet transform-enhanced Mamba architecture, achieving top performance in video quality, watermark fidelity, and robustness.  					AI-generated summary 				 The explosive growth...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 36. Research examines LLM performance biases between Simplified and Traditional Chinese in regional term and name choice tasks, attributing differences to training data and tokenization.  					AI-generated summary 				 While the capabilities of Large Language Models (LLMs) have been studied in both Simp...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 37. An agent-based tutoring system for electrical engineering enhances learning through natural circuit interaction, context-retrieving generation, and guided questioning, demonstrating superior performance compared to baseline methods.  					AI-generated summary 				 Intelligent tutoring systems combin...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 38. Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 39. Manga, or Japanese comics, is a richly multimodal narrative form that blends images and text in complex ways. Teaching large multimodal models (LMMs) to understand such narratives at a human-like level could help manga creators reflect on and refine their stories. To this end, we introduce two bench...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 40. Effective data selection is critical for efficient training of modern Large Language Models (LLMs). This paper introduces Influence Distillation, a novel, mathematically-justified framework for data selection that employs second-order information to optimally weight training samples. By distilling e...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 41. In Transformer architectures, tokens\textemdash discrete units derived from raw data\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input's essential information. Due to the...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 42. The approach of training vision encoders with small surrogate models before transferring them to large language models reduces training costs and enhances performance.  					AI-generated summary 				 Vision language models (VLMs) typically pair a modestly sized vision encoder with a large language m...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 43. A novel feed-forward model achieves fast 3D stylization using sparse view images, maintaining multi-view consistency and high-quality style transfer while retaining reconstruction accuracy.  					AI-generated summary 				 Stylizing 3D scenes instantly while maintaining multi-view consistency and fai...
[29.05.2025 14:12] ********************************************************************************
[29.05.2025 14:12] Abstract 44. First Finish Search improves accuracy in large language models by stopping inference at the first completed sample, significantly outperforming other decoding strategies in reasoning tasks.  					AI-generated summary 				 Test-time scaling (TTS), which involves dynamic allocation of compute during i...
[29.05.2025 14:12] Read previous papers.
[29.05.2025 14:12] Generating reviews via LLM API.
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#reasoning"], "emoji": "üî¨", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–µ–π –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–Ω–∏–∂–µ–Ω–∏—è —ç–Ω—Ç—Ä–æ–ø–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#small_models", "#architecture", "#optimization", "#reasoning", "#benchmark", "#math", "#training"], "emoji": "üõ£Ô∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –∏ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Roads to Rome (R2R), –∫–æ—Ç–æ—Ä—ã–π —Å–µ–ª
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#training", "#multimodal", "#reasoning"], "emoji": "üïµÔ∏è", "ru": {"title": "–°–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è VLM: –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö, –≤—ã—à–µ —Ç–æ—á–Ω–æ—Å—Ç—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Sherlock - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –∏ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∑—Ä–∏—Ç–µ–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rl", "#benchmark", "#open_source", "#training"], "emoji": "üß†", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Skywork-OR1, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#rlhf", "#multimodal", "#training", "#synthetic"], "emoji": "ü§ñ", "ru": {"title": "–°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ò–ò –±–µ–∑ —É—á–∏—Ç–µ–ª—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ MM-UPT –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#data", "#open_source", "#games", "#rl", "#dataset"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ü–û", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#architecture", "#inference"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "SageAttention2++ - —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è SageAttention2, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–∞—Ç—Ä–∏—á–Ω—ã—Ö —É–º–Ω–æ–∂–µ–Ω–∏–π –≤ –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è. –û—Å–Ω–æ–≤–Ω–æ–µ –Ω–æ–≤–æ–≤–≤–µ–¥
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#multimodal", "#benchmark", "#open_source", "#training"], "emoji": "üß†", "ru": {"title": "–î–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ—Ä—ã–≤–∞ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#benchmark", "#training", "#video", "#multimodal", "#dataset", "#reasoning"], "emoji": "üé¨", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –ò–ò –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –º—ã—à–ª–µ–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) - –ø—Ä–µ–¥—Å–∫–∞
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#architecture", "#3d"], "emoji": "üé®", "ru": {"title": "–ù–µ–π—Ä–æ–Ω–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ –±–µ–∑ —Ñ–∏–∑–∏–∫–∏: –æ—Ç —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–æ–≤ –∫ –ø–∏–∫—Å–µ–ª—è–º", "desc": "RenderFormer - —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–ø—Ä—è–º—É—é —Å–æ–∑–¥–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ —Ç—Ä–µ—É–≥–æ–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—Ü–µ–Ω—ã —Å –ø–æ–ª–Ω—ã–º–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –æ—Å–≤
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#alignment", "#rag"], "emoji": "üî¨", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "desc": "DeepResearchGym - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π API –∏ –æ—Ü–µ
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#alignment", "#cv", "#optimization", "#diffusion", "#rlhf", "#rag"], "emoji": "üîç", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–≤–µ—Ä—Ö—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –æ—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫ –¥–µ—Ç–∞–ª—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Chain-of-Zoom (CoZ) - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å–≤–µ—Ä—Ö—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#multilingual", "#data", "#transfer_learning", "#open_source", "#low_resource", "#dataset"], "emoji": "üåç", "ru": {"title": "JQL: –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "JQL - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#open_source", "#training", "#architecture", "#reasoning", "#rlhf"], "emoji": "üß†", "ru": {"title": "UniR: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–æ–¥—É–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "UniR - —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–æ–¥—É–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#dataset", "#rl", "#benchmark"], "emoji": "üöö", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –≤ —É—Å–ª–æ–≤–∏—è—Ö –≥–æ—Ä–æ–¥—Å–∫–æ–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏", "desc": "SVRPBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤ –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω–µ–æ–ø—Ä–µ–¥–µ–ª
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#training", "#diffusion", "#cv"], "emoji": "ÔøΩpanorama", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –ø–∞–Ω–æ—Ä–∞–º–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#agi", "#reasoning", "#agents", "#benchmark", "#training"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ù–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Å–µ—Ç–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#optimization", "#inference", "#reasoning", "#benchmark", "#training"], "emoji": "‚úÇÔ∏è", "ru": {"title": "PIR: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ò–ò —á–µ—Ä–µ–∑ —É–º–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ", "desc": "PIR - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç–µ–º —É–¥–∞–ª–µ–Ω–∏—è 
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#alignment"], "emoji": "üß†", "ru": {"title": "–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ—Ç—Å—Ç–∞—é—Ç –æ—Ç –ª—é–¥–µ–π –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–∏–Ω–∞–º–∏–∫–∏ –ø—Å–∏—Ö–∏—á–µ—Å–∫–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ DynToM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø—Å–∏—Ö–∏
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#rl", "#rag", "#multimodal", "#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "VRAG-RL: –£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ RAG —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "VRAG-RL - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–∑—É–∞–ª
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#architecture", "#inference", "#interpretability", "#data", "#reasoning"], "emoji": "üß†", "ru": {"title": "–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç—Å—è –º—ã—Å–ª–∏—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π,
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#science", "#multimodal", "#data", "#dataset", "#transfer_learning"], "emoji": "üß¨", "ru": {"title": "CHIMERA: –≤–¥–æ—Ö–Ω–æ–≤–µ–Ω–∏–µ –¥–ª—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ CHIMERA - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä–µ–∫–æ–º–±–∏–Ω–∞—Ü–∏–∏, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∏–∑ –Ω–∞—É—á–Ω
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#training", "#open_source", "#hallucinations", "#rlhf", "#multimodal"], "emoji": "üñºÔ∏è", "ru": {"title": "–¢–æ—á–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω—É—é —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é", "desc": "RICO - —ç—Ç–æ –Ω–æ–≤–∞—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#cv", "#reasoning", "#multimodal"], "emoji": "üß†", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–æ–µ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ò–ò: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –º—ã—à–ª–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É '–ú—ã—à–ª–µ–Ω–∏–µ —Å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏', –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫—Ä—É–ø–Ω
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#data", "#cv", "#diffusion", "#dataset", "#open_source", "#synthetic"], "emoji": "üé®", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º—ã—Ö –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –ø—Ä–æ–∑—Ä–∞—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#interpretability", "#rlhf"], "emoji": "üîç", "ru": {"title": "Text2Grad: –¢–æ—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Text2Grad. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#math", "#rl", "#security", "#reasoning"], "emoji": "üîç", "ru": {"title": "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –≤ RLVR: –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –∏ –º–æ–¥–µ–ª–µ–π –≤ –æ–±—É
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#optimization", "#video", "#training", "#transfer_learning", "#diffusion", "#3d", "#multimodal"], "emoji": "üé•", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π 3D-–∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞–º–µ—Ä—ã –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π", "desc": "EPiC - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ 3D-–∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞–º–µ—Ä—ã –≤ –º–æ–¥–µ–ª—è—Ö –≤–∏–¥–µ–æ–¥–∏—Ñ—Ñ—É–∑–∏–∏. –û–Ω —Å–æ–∑–¥–∞
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#cv", "#interpretability", "#reasoning"], "emoji": "üåç", "ru": {"title": "–£–º–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è: VLM —Å —É—Å–∏–ª–µ–Ω–Ω—ã–º –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ Geo Reason Enhancement (GRE) Suite –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–æ–ª–æ–∫–∞
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#training", "#rlhf", "#alignment", "#ethics", "#hallucinations"], "emoji": "üíâ", "ru": {"title": "–í–∞–∫—Ü–∏–Ω–∞—Ü–∏—è –ò–ò –ø—Ä–æ—Ç–∏–≤ –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ò–ò –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∞–Ω–∞–ª–æ–≥–∏–∏ —Å –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#architecture", "#interpretability", "#transfer_learning", "#training", "#multimodal", "#cv", "#dataset"], "emoji": "üß†", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –∑—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∫–æ—Ä—ã: –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å BraInCoRL", "desc": "BraInCoRL - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#diffusion", "#cv", "#inference"], "emoji": "üñºÔ∏è", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Time-independent Unified Encoder (TiUE) - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#interpretability", "#dataset", "#architecture"], "emoji": "üß†", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞—è —Ç–∞–π–Ω—ã –¥–æ–æ–±—É—á–µ–Ω–∏—è: –∫–∞–∫ LLM —É—á–∞—Ç—Å—è –≤—ã–ø–æ–ª–Ω—è—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç, –∫–∞–∫ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –∏–∑–º–µ–Ω—è–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#training", "#rl", "#multimodal", "#alignment", "#video", "#reasoning"], "emoji": "‚è±Ô∏è", "ru": {"title": "MUSEG: –ü—Ä–æ—Ä—ã–≤ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "MUSEG - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –ø–æ–Ω
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#benchmark", "#graphs"], "emoji": "üß†", "ru": {"title": "HuggingKG: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ ML –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "HuggingKG - —ç—Ç–æ –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π, —Å–æ–∑–¥–∞–Ω–Ω—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ Hugging Face –¥–ª—è —É–ø—Ä–∞–≤–ª–µ
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#video", "#3d", "#architecture", "#security"], "emoji": "üé•", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –∞–≤—Ç–æ—Ä—Å–∫–∏—Ö –ø—Ä–∞–≤ –Ω–∞ AI-–≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –Ω–µ–≤–∏–¥–∏–º—ã—Ö –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤", "desc": "Safe-Sora - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –Ω–µ–≤–∏–¥–∏–º—ã—Ö –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –≤ –≤–∏–¥–µ–æ, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º. –û–Ω–∞ –∏—Å–ø
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#ethics", "#low_resource", "#multilingual", "#dataset", "#benchmark", "#open_source"], "emoji": "üá®üá≥", "ru": {"title": "–°–∫—Ä—ã—Ç—ã–µ –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è LLM –≤ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ: —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π vs —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#graphs", "#interpretability", "#healthcare", "#science", "#rag", "#games", "#agents", "#multimodal", "#reasoning"], "emoji": "‚ö°", "ru": {"title": "–ò–ò-—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä –ø–æ —ç–ª–µ–∫—Ç—Ä–æ—Ç–µ—Ö–Ω–∏–∫–µ: –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ", "desc": "AITEE - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ —Ä–µ–ø–µ—Ç–∏—Ç–æ
[29.05.2025 14:12] Querying the API.
[29.05.2025 14:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs), extending vanilla RoPE to capture the intricate spatial-temporal dependencies in videos remains an unsolved challenge. Existing methods typically allocate different frequencies within RoPE to encode 3D positional information. However, these allocation strategies mainly rely on heuristics, lacking in-depth theoretical analysis. In this paper, we first study how different allocation strategies impact the long-context capabilities of VLMs. Our analysis reveals that current multimodal RoPEs fail to reliably capture semantic similarities over extended contexts. To address this issue, we propose HoPE, a Hybrid of Position Embedding designed to improve the long-context capabilities of VLMs. HoPE introduces a hybrid frequency allocation strategy for reliable semantic modeling over arbitrarily long context, and a dynamic temporal scaling mechanism to facilitate robust learning and flexible inference across diverse context lengths. Extensive experiments across four video benchmarks on long video understanding and retrieval tasks demonstrate that HoPE consistently outperforms existing methods, confirming its effectiveness. Code is available at https://github.com/hrlics/HoPE.
[29.05.2025 14:12] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π HoPE (Hybrid of Position Embedding). –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –≤–∏–¥–µ–æ –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è. HoPE –≤–∫–ª—é—á–∞–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –º–µ—Ö–∞–Ω–∏–∑–º –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥–∞ —Å —Ä–∞–∑–ª–∏—á–Ω–æ–π –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —á–µ—Ç—ã—Ä–µ—Ö –≤–∏–¥–µ–æ-–±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ HoPE –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ.",
  "emoji": "üé¨",
  "title": "HoPE: –£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –≤–∏–¥–µ–æ"
}
[29.05.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs), extending vanilla RoPE to capture the intricate spatial-temporal dependencies in videos remains an unsolved challenge. Existing methods typically allocate different frequencies within RoPE to encode 3D positional information. However, these allocation strategies mainly rely on heuristics, lacking in-depth theoretical analysis. In this paper, we first study how different allocation strategies impact the long-context capabilities of VLMs. Our analysis reveals that current multimodal RoPEs fail to reliably capture semantic similarities over extended contexts. To address this issue, we propose HoPE, a Hybrid of Position Embedding designed to improve the long-context capabilities of VLMs. HoPE introduces a hybrid frequency allocation strategy for reliable semantic modeling over arbitrarily long context, and a dynamic temporal scaling mechanism to facilitate robust learning and flexible inference across diverse context lengths. Extensive experiments across four video benchmarks on long video understanding and retrieval tasks demonstrate that HoPE consistently outperforms existing methods, confirming its effectiveness. Code is available at https://github.com/hrlics/HoPE."

[29.05.2025 14:12] Response: ```python
['MULTIMODAL', 'VIDEO', 'ARCHITECTURE', 'BENCHMARK']
```
[29.05.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs), extending vanilla RoPE to capture the intricate spatial-temporal dependencies in videos remains an unsolved challenge. Existing methods typically allocate different frequencies within RoPE to encode 3D positional information. However, these allocation strategies mainly rely on heuristics, lacking in-depth theoretical analysis. In this paper, we first study how different allocation strategies impact the long-context capabilities of VLMs. Our analysis reveals that current multimodal RoPEs fail to reliably capture semantic similarities over extended contexts. To address this issue, we propose HoPE, a Hybrid of Position Embedding designed to improve the long-context capabilities of VLMs. HoPE introduces a hybrid frequency allocation strategy for reliable semantic modeling over arbitrarily long context, and a dynamic temporal scaling mechanism to facilitate robust learning and flexible inference across diverse context lengths. Extensive experiments across four video benchmarks on long video understanding and retrieval tasks demonstrate that HoPE consistently outperforms existing methods, confirming its effectiveness. Code is available at https://github.com/hrlics/HoPE."

[29.05.2025 14:12] Response: ```python
["LONG_CONTEXT"]
```
[29.05.2025 14:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the limitations of Vision-Language Models (VLMs) in handling long-context scenarios, especially with long videos. It critiques existing Rotary Position Embedding (RoPE) methods for their inadequate ability to capture complex spatial-temporal dependencies. The authors introduce HoPE, a new Hybrid of Position Embedding that employs a novel frequency allocation strategy and a dynamic temporal scaling mechanism. Through extensive experiments, HoPE shows significant improvements in long video understanding and retrieval tasks compared to current methods.","title":"Enhancing Long-Context Understanding in VLMs with HoPE"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the limitations of Vision-Language Models (VLMs) in handling long-context scenarios, especially with long videos. It critiques existing Rotary Position Embedding (RoPE) methods for their inadequate ability to capture complex spatial-temporal dependencies. The authors introduce HoPE, a new Hybrid of Position Embedding that employs a novel frequency allocation strategy and a dynamic temporal scaling mechanism. Through extensive experiments, HoPE shows significant improvements in long video understanding and retrieval tasks compared to current methods.', title='Enhancing Long-Context Understanding in VLMs with HoPE'))
[29.05.2025 14:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®ÈïøËßÜÈ¢ëÁ≠âÈïø‰∏ä‰∏ãÊñáÂú∫ÊôØ‰∏≠ÁöÑË°®Áé∞ÂæÄÂæÄ‰∏ãÈôç„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰ΩçÁΩÆÂµåÂÖ•ÊñπÊ≥ïHoPEÔºåÊó®Âú®ÊîπÂñÑVLMsÂú®Èïø‰∏ä‰∏ãÊñá‰∏≠ÁöÑËÉΩÂäõ„ÄÇHoPEÈááÁî®Ê∑∑ÂêàÈ¢ëÁéáÂàÜÈÖçÁ≠ñÁï•Ôºå‰ª•ÂèØÈù†Âú∞Âª∫Ê®°ËØ≠‰πâÔºåÂπ∂ÂºïÂÖ•Âä®ÊÄÅÊó∂Èó¥Áº©ÊîæÊú∫Âà∂Ôºå‰ª•ÊîØÊåÅ‰∏çÂêå‰∏ä‰∏ãÊñáÈïøÂ∫¶ÁöÑÁÅµÊ¥ªÊé®ÁêÜ„ÄÇÈÄöËøáÂú®Âõõ‰∏™ËßÜÈ¢ëÂü∫ÂáÜ‰∏äÁöÑÂπøÊ≥õÂÆûÈ™åÔºåHoPEÁöÑË°®Áé∞‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ","title":"ÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÈïø‰∏ä‰∏ãÊñáËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®ÈïøËßÜÈ¢ëÁ≠âÈïø‰∏ä‰∏ãÊñáÂú∫ÊôØ‰∏≠ÁöÑË°®Áé∞ÂæÄÂæÄ‰∏ãÈôç„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰ΩçÁΩÆÂµåÂÖ•ÊñπÊ≥ïHoPEÔºåÊó®Âú®ÊîπÂñÑVLMsÂú®Èïø‰∏ä‰∏ãÊñá‰∏≠ÁöÑËÉΩÂäõ„ÄÇHoPEÈááÁî®Ê∑∑ÂêàÈ¢ëÁéáÂàÜÈÖçÁ≠ñÁï•Ôºå‰ª•ÂèØÈù†Âú∞Âª∫Ê®°ËØ≠‰πâÔºåÂπ∂ÂºïÂÖ•Âä®ÊÄÅÊó∂Èó¥Áº©ÊîæÊú∫Âà∂Ôºå‰ª•ÊîØÊåÅ‰∏çÂêå‰∏ä‰∏ãÊñáÈïøÂ∫¶ÁöÑÁÅµÊ¥ªÊé®ÁêÜ„ÄÇÈÄöËøáÂú®Âõõ‰∏™ËßÜÈ¢ëÂü∫ÂáÜ‰∏äÁöÑÂπøÊ≥õÂÆûÈ™åÔºåHoPEÁöÑË°®Áé∞‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ', title='ÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÈïø‰∏ä‰∏ãÊñáËÉΩÂäõ'))
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#multimodal", "#training", "#games"], "emoji": "üìö", "ru": {"title": "–ù–æ–≤—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –∏ –º–æ–¥–µ–ª—å –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∞–Ω–≥–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–≤–∞ –Ω–æ–≤—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∞–Ω–≥–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
[29.05.2025 14:12] Using data from previous issue: {"categories": ["#training", "#data", "#optimization"], "emoji": "üéØ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Influence Distillation. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏
[29.05.2025 14:12] Querying the API.
[29.05.2025 14:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

In Transformer architectures, tokens\textemdash discrete units derived from raw data\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input's essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate "overthinking" and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, and broader ML and scientific domains. We highlight its potential to drive new model architectures and learning strategies that improve robustness, increase interpretability, and better align with the objectives of generative modeling.
[29.05.2025 14:12] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ä–æ–ª—å —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –≤—ã—Ö–æ–¥—è –∑–∞ —Ä–∞–º–∫–∏ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é, —Å–Ω–∏–∑–∏—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –∏ –ø–æ–≤—ã—Å–∏—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –∫–∞–∫ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–∏–Ω—Ü–∏–ø –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏, –≤–ª–∏—è—é—â–∏–π –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ –∏ –µ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è. –°—Ç–∞—Ç—å—è –Ω–∞–º–µ—á–∞–µ—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –≤–∫–ª—é—á–∞—è –¥–∏–∑–∞–π–Ω –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.",
  "emoji": "üî¨",
  "title": "–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤: –æ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫ –Ω–æ–≤—ã–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è"
}
[29.05.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In Transformer architectures, tokens\textemdash discrete units derived from raw data\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input's essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate "overthinking" and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, and broader ML and scientific domains. We highlight its potential to drive new model architectures and learning strategies that improve robustness, increase interpretability, and better align with the objectives of generative modeling."

[29.05.2025 14:12] Response: ```python
['ARCHITECTURE', 'TRAINING', 'MULTIMODAL', 'RL']
```
[29.05.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"In Transformer architectures, tokens\textemdash discrete units derived from raw data\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input's essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate "overthinking" and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, and broader ML and scientific domains. We highlight its potential to drive new model architectures and learning strategies that improve robustness, increase interpretability, and better align with the objectives of generative modeling."

[29.05.2025 14:12] Response: ```python
['LONG_CONTEXT', 'HALLUCINATIONS', 'ALIGNMENT', 'INTERPRETABILITY', 'OPTIMIZATION']
```
[29.05.2025 14:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the role of token reduction in Transformer architectures, which are commonly used in machine learning for processing data. Traditionally, token reduction has been seen as a way to improve efficiency by reducing computational costs and memory usage. However, the authors argue that token reduction should be viewed as a fundamental principle that can enhance generative modeling across various domains, including vision and language. They propose that effective token reduction can lead to better model integration, reduce errors, and improve the overall stability and coherence of models.","title":"Token Reduction: A Key to Enhanced Generative Modeling"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the role of token reduction in Transformer architectures, which are commonly used in machine learning for processing data. Traditionally, token reduction has been seen as a way to improve efficiency by reducing computational costs and memory usage. However, the authors argue that token reduction should be viewed as a fundamental principle that can enhance generative modeling across various domains, including vision and language. They propose that effective token reduction can lead to better model integration, reduce errors, and improve the overall stability and coherence of models.', title='Token Reduction: A Key to Enhanced Generative Modeling'))
[29.05.2025 14:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Âú®TransformerÊû∂ÊûÑ‰∏≠Ôºå‰ª§ÁâåÊòØ‰ªéÂéüÂßãÊï∞ÊçÆ‰∏≠ÊèêÂèñÁöÑÁ¶ªÊï£ÂçïÂÖÉÔºåÈÄöËøáÂ∞ÜËæìÂÖ•ÂàÜÂâ≤ÊàêÂõ∫ÂÆöÈïøÂ∫¶ÁöÑÂùóÊù•ÂΩ¢Êàê„ÄÇÊØè‰∏™‰ª§ÁâåË¢´Êò†Â∞ÑÂà∞‰∏Ä‰∏™ÂµåÂÖ•Ôºå‰ΩøÂæóÂú®‰øùÊåÅËæìÂÖ•ÂÖ≥ÈîÆ‰ø°ÊÅØÁöÑÂêåÊó∂ËÉΩÂ§üËøõË°åÂπ∂Ë°åÊ≥®ÊÑèÂäõËÆ°ÁÆó„ÄÇÂ∞ΩÁÆ°‰ª§ÁâåÂáèÂ∞ë‰∏ªË¶ÅË¢´ËßÜ‰∏∫ÊèêÈ´òÊïàÁéáÁöÑÁ≠ñÁï•Ôºå‰ΩÜÊú¨ÊñáËÆ§‰∏∫Âú®Â§ßÂûãÁîüÊàêÊ®°ÂûãÊó∂‰ª£Ôºå‰ª§ÁâåÂáèÂ∞ëÂ∫îË∂ÖË∂ä‰º†ÁªüÁöÑÊïàÁéáÂØºÂêëËßíËâ≤ÔºåÊàê‰∏∫ÁîüÊàêÂª∫Ê®°ÁöÑÂü∫Êú¨ÂéüÂàô„ÄÇÊàë‰ª¨ÊèêÂá∫Ôºå‰ª§ÁâåÂáèÂ∞ëÂèØ‰ª•‰øÉËøõÂ§öÊ®°ÊÄÅÊï¥Âêà„ÄÅÂáèËΩª‚ÄúËøáÂ∫¶ÊÄùËÄÉ‚ÄùÂíåÂπªËßâ„ÄÅ‰øùÊåÅÈïøËæìÂÖ•ÁöÑ‰∏ÄËá¥ÊÄßÔºåÂπ∂Â¢ûÂº∫ËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÁ≠â„ÄÇ","title":"‰ª§ÁâåÂáèÂ∞ëÔºöÁîüÊàêÂª∫Ê®°ÁöÑÊñ∞ÂéüÂàô"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Âú®TransformerÊû∂ÊûÑ‰∏≠Ôºå‰ª§ÁâåÊòØ‰ªéÂéüÂßãÊï∞ÊçÆ‰∏≠ÊèêÂèñÁöÑÁ¶ªÊï£ÂçïÂÖÉÔºåÈÄöËøáÂ∞ÜËæìÂÖ•ÂàÜÂâ≤ÊàêÂõ∫ÂÆöÈïøÂ∫¶ÁöÑÂùóÊù•ÂΩ¢Êàê„ÄÇÊØè‰∏™‰ª§ÁâåË¢´Êò†Â∞ÑÂà∞‰∏Ä‰∏™ÂµåÂÖ•Ôºå‰ΩøÂæóÂú®‰øùÊåÅËæìÂÖ•ÂÖ≥ÈîÆ‰ø°ÊÅØÁöÑÂêåÊó∂ËÉΩÂ§üËøõË°åÂπ∂Ë°åÊ≥®ÊÑèÂäõËÆ°ÁÆó„ÄÇÂ∞ΩÁÆ°‰ª§ÁâåÂáèÂ∞ë‰∏ªË¶ÅË¢´ËßÜ‰∏∫ÊèêÈ´òÊïàÁéáÁöÑÁ≠ñÁï•Ôºå‰ΩÜÊú¨ÊñáËÆ§‰∏∫Âú®Â§ßÂûãÁîüÊàêÊ®°ÂûãÊó∂‰ª£Ôºå‰ª§ÁâåÂáèÂ∞ëÂ∫îË∂ÖË∂ä‰º†ÁªüÁöÑÊïàÁéáÂØºÂêëËßíËâ≤ÔºåÊàê‰∏∫ÁîüÊàêÂª∫Ê®°ÁöÑÂü∫Êú¨ÂéüÂàô„ÄÇÊàë‰ª¨ÊèêÂá∫Ôºå‰ª§ÁâåÂáèÂ∞ëÂèØ‰ª•‰øÉËøõÂ§öÊ®°ÊÄÅÊï¥Âêà„ÄÅÂáèËΩª‚ÄúËøáÂ∫¶ÊÄùËÄÉ‚ÄùÂíåÂπªËßâ„ÄÅ‰øùÊåÅÈïøËæìÂÖ•ÁöÑ‰∏ÄËá¥ÊÄßÔºåÂπ∂Â¢ûÂº∫ËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÁ≠â„ÄÇ', title='‰ª§ÁâåÂáèÂ∞ëÔºöÁîüÊàêÂª∫Ê®°ÁöÑÊñ∞ÂéüÂàô'))
[29.05.2025 14:12] Querying the API.
[29.05.2025 14:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The approach of training vision encoders with small surrogate models before transferring them to large language models reduces training costs and enhances performance.  					AI-generated summary 				 Vision language models (VLMs) typically pair a modestly sized vision encoder with a large language model (LLM), e.g., Llama-70B, making the decoder the primary computational burden during training. To reduce costs, a potential promising strategy is to first train the vision encoder using a small language model before transferring it to the large one. We construct small "surrogate models" that share the same embedding space and representation language as the large target LLM by directly inheriting its shallow layers. Vision encoders trained on the surrogate can then be directly transferred to the larger model, a process we call zero-shot grafting -- when plugged directly into the full-size target LLM, the grafted pair surpasses the encoder-surrogate pair and, on some benchmarks, even performs on par with full decoder training with the target LLM. Furthermore, our surrogate training approach reduces overall VLM training costs by ~45% when using Llama-70B as the decoder.
[29.05.2025 14:13] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–æ—á–µ—Ç–∞—é—â–∏—Ö –∑—Ä–µ–Ω–∏–µ –∏ —è–∑—ã–∫. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∞—Ç—å —ç–Ω–∫–æ–¥–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –Ω–µ–±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏-—Å—É—Ä—Ä–æ–≥–∞—Ç–∞, –∞ –∑–∞—Ç–µ–º –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –µ–≥–æ –≤ –±–æ–ª—å—à—É—é —Ü–µ–ª–µ–≤—É—é –º–æ–¥–µ–ª—å. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π 'zero-shot grafting', –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∑–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –Ω–µ —Ç–æ–ª—å–∫–æ —ç–∫–æ–Ω–æ–º–∏—Ç —Ä–µ—Å—É—Ä—Å—ã, –Ω–æ –∏ –º–æ–∂–µ—Ç –¥–∞–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é.",

  "emoji": "üîÄ",

  "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑—Ä–∏—Ç–µ–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å—É—Ä—Ä–æ–≥–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏"
}
[29.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The approach of training vision encoders with small surrogate models before transferring them to large language models reduces training costs and enhances performance.  					AI-generated summary 				 Vision language models (VLMs) typically pair a modestly sized vision encoder with a large language model (LLM), e.g., Llama-70B, making the decoder the primary computational burden during training. To reduce costs, a potential promising strategy is to first train the vision encoder using a small language model before transferring it to the large one. We construct small "surrogate models" that share the same embedding space and representation language as the large target LLM by directly inheriting its shallow layers. Vision encoders trained on the surrogate can then be directly transferred to the larger model, a process we call zero-shot grafting -- when plugged directly into the full-size target LLM, the grafted pair surpasses the encoder-surrogate pair and, on some benchmarks, even performs on par with full decoder training with the target LLM. Furthermore, our surrogate training approach reduces overall VLM training costs by ~45% when using Llama-70B as the decoder."

[29.05.2025 14:13] Response: ```python
['TRAINING', 'SMALL_MODELS', 'CV']
```
[29.05.2025 14:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The approach of training vision encoders with small surrogate models before transferring them to large language models reduces training costs and enhances performance.  					AI-generated summary 				 Vision language models (VLMs) typically pair a modestly sized vision encoder with a large language model (LLM), e.g., Llama-70B, making the decoder the primary computational burden during training. To reduce costs, a potential promising strategy is to first train the vision encoder using a small language model before transferring it to the large one. We construct small "surrogate models" that share the same embedding space and representation language as the large target LLM by directly inheriting its shallow layers. Vision encoders trained on the surrogate can then be directly transferred to the larger model, a process we call zero-shot grafting -- when plugged directly into the full-size target LLM, the grafted pair surpasses the encoder-surrogate pair and, on some benchmarks, even performs on par with full decoder training with the target LLM. Furthermore, our surrogate training approach reduces overall VLM training costs by ~45% when using Llama-70B as the decoder."

[29.05.2025 14:13] Response: ```python
['TRANSFER_LEARNING', 'OPTIMIZATION']
```
[29.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a method for training vision encoders using smaller surrogate models before integrating them with larger language models. By training the vision encoder on a compact model that shares the same embedding space as the larger model, the authors demonstrate a significant reduction in training costs and improved performance. The technique, termed \'zero-shot grafting\', allows the trained vision encoder to be directly transferred to the larger language model, achieving results comparable to full decoder training. Overall, this approach can lower the training expenses of vision language models by approximately 45%.","title":"Efficient Vision Encoder Training with Surrogate Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents a method for training vision encoders using smaller surrogate models before integrating them with larger language models. By training the vision encoder on a compact model that shares the same embedding space as the larger model, the authors demonstrate a significant reduction in training costs and improved performance. The technique, termed 'zero-shot grafting', allows the trained vision encoder to be directly transferred to the larger language model, achieving results comparable to full decoder training. Overall, this approach can lower the training expenses of vision language models by approximately 45%.", title='Efficient Vision Encoder Training with Surrogate Models'))
[29.05.2025 14:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöËøáÂÖàÁî®Â∞èÂûãÊõø‰ª£Ê®°ÂûãËÆ≠ÁªÉËßÜËßâÁºñÁ†ÅÂô®ÔºåÂÜçÂ∞ÜÂÖ∂ËΩ¨ÁßªÂà∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñπÊ≥ï„ÄÇËøôÁßçÊñπÊ≥ïÂèØ‰ª•Èôç‰ΩéËÆ≠ÁªÉÊàêÊú¨Âπ∂ÊèêÈ´òÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊûÑÂª∫ÁöÑÂ∞èÂûãÊõø‰ª£Ê®°Âûã‰∏éÂ§ßÂûãÁõÆÊ†áËØ≠Ë®ÄÊ®°ÂûãÂÖ±‰∫´Áõ∏ÂêåÁöÑÂµåÂÖ•Á©∫Èó¥ÂíåË°®Á§∫ËØ≠Ë®ÄÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÈõ∂-shotÂ´ÅÊé•„ÄÇÂÆûÈ™åË°®ÊòéÔºåÁªèËøáËøôÁßçËÆ≠ÁªÉÁöÑËßÜËßâÁºñÁ†ÅÂô®Âú®‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁªìÂêàÊó∂ÔºåÊÄßËÉΩË∂ÖËøá‰∫ÜÁõ¥Êé•ËÆ≠ÁªÉÁöÑÁºñÁ†ÅÂô®-Êõø‰ª£Ê®°ÂûãÁªÑÂêàÔºåÂπ∂‰∏îÂú®Êüê‰∫õÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰∏éÂÆåÊï¥Ëß£Á†ÅÂô®ËÆ≠ÁªÉÁõ∏ÂΩì„ÄÇ","title":"Â∞èÊ®°ÂûãÂä©ÂäõÂ§ßÊ®°ÂûãÔºåÈôç‰ΩéÊàêÊú¨ÊèêÂçáÊÄßËÉΩ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöËøáÂÖàÁî®Â∞èÂûãÊõø‰ª£Ê®°ÂûãËÆ≠ÁªÉËßÜËßâÁºñÁ†ÅÂô®ÔºåÂÜçÂ∞ÜÂÖ∂ËΩ¨ÁßªÂà∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñπÊ≥ï„ÄÇËøôÁßçÊñπÊ≥ïÂèØ‰ª•Èôç‰ΩéËÆ≠ÁªÉÊàêÊú¨Âπ∂ÊèêÈ´òÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊûÑÂª∫ÁöÑÂ∞èÂûãÊõø‰ª£Ê®°Âûã‰∏éÂ§ßÂûãÁõÆÊ†áËØ≠Ë®ÄÊ®°ÂûãÂÖ±‰∫´Áõ∏ÂêåÁöÑÂµåÂÖ•Á©∫Èó¥ÂíåË°®Á§∫ËØ≠Ë®ÄÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÈõ∂-shotÂ´ÅÊé•„ÄÇÂÆûÈ™åË°®ÊòéÔºåÁªèËøáËøôÁßçËÆ≠ÁªÉÁöÑËßÜËßâÁºñÁ†ÅÂô®Âú®‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁªìÂêàÊó∂ÔºåÊÄßËÉΩË∂ÖËøá‰∫ÜÁõ¥Êé•ËÆ≠ÁªÉÁöÑÁºñÁ†ÅÂô®-Êõø‰ª£Ê®°ÂûãÁªÑÂêàÔºåÂπ∂‰∏îÂú®Êüê‰∫õÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰∏éÂÆåÊï¥Ëß£Á†ÅÂô®ËÆ≠ÁªÉÁõ∏ÂΩì„ÄÇ', title='Â∞èÊ®°ÂûãÂä©ÂäõÂ§ßÊ®°ÂûãÔºåÈôç‰ΩéÊàêÊú¨ÊèêÂçáÊÄßËÉΩ'))
[29.05.2025 14:13] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#3d"], "emoji": "üé®", "ru": {"title": "–ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è 3D-—Å—Ç–∏–ª–∏–∑–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ü–µ–Ω—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –ø—Ä—è–º–æ–π –ø–æ–¥–∞—á–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π 3D-—Å—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º–∏ –≤–∏–¥–∞–º–∏. –ú–æ–¥–µ–ª—å 
[29.05.2025 14:13] Using data from previous issue: {"categories": ["#training", "#inference", "#optimization", "#reasoning"], "emoji": "üèÅ", "ru": {"title": "–ü–µ—Ä–≤—ã–π —Ñ–∏–Ω–∏—à–∏—Ä—É–µ—Ç - –ø–µ—Ä–≤—ã–π –ø–æ–±–µ–∂–¥–∞–µ—Ç: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ú–µ—Ç–æ–¥ First Finish Search (FFS) —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –≤—ã–≤–æ–¥ –Ω–∞ –ø–µ—Ä–≤
[29.05.2025 14:13] Loading Chinese text from previous data.
[29.05.2025 14:13] Renaming data file.
[29.05.2025 14:13] Renaming previous data. hf_papers.json to ./d/2025-05-29.json
[29.05.2025 14:13] Saving new data file.
[29.05.2025 14:13] Generating page.
[29.05.2025 14:13] Renaming previous page.
[29.05.2025 14:13] Renaming previous data. index.html to ./d/2025-05-29.html
[29.05.2025 14:13] [Experimental] Generating Chinese page for reading.
[29.05.2025 14:13] Chinese vocab [{'word': 'Êó®Âú®', 'pinyin': 'zh«ê z√†i', 'trans': 'aim to'}, {'word': 'Ëß£ÂÜ≥', 'pinyin': 'jiƒõ ju√©', 'trans': 'solve'}, {'word': 'Âº∫ÂåñÂ≠¶‰π†', 'pinyin': 'qi√°ng hu√† xu√© x√≠', 'trans': 'reinforcement learning'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': '‰∏ªË¶Å', 'pinyin': 'zh«î y√†o', 'trans': 'major'}, {'word': 'ÈöúÁ¢ç', 'pinyin': 'zh√†ng √†i', 'trans': 'obstacle'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'ÁÜµ', 'pinyin': 'shƒÅng', 'trans': 'entropy'}, {'word': 'Â¥©Ê∫É', 'pinyin': 'bƒìng ku√¨', 'trans': 'collapse'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'ÂèëÁé∞', 'pinyin': 'fƒÅ xi√†n', 'trans': 'discover'}, {'word': 'Âπ≤È¢Ñ', 'pinyin': 'gƒÅn y√π', 'trans': 'intervention'}, {'word': 'ÊÉÖÂÜµ', 'pinyin': 'q√≠ng ku√†ng', 'trans': 'situation'}, {'word': 'Èò∂ÊÆµ', 'pinyin': 'jiƒì du√†n', 'trans': 'stage'}, {'word': 'ÊÄ•Ââß', 'pinyin': 'j√≠ j√π', 'trans': 'drastic'}, {'word': '‰∏ãÈôç', 'pinyin': 'xi√† ji√†ng', 'trans': 'decline'}, {'word': 'ÂØºËá¥', 'pinyin': 'd«éo zh√¨', 'trans': 'lead to'}, {'word': 'Êé¢Á¥¢', 'pinyin': 't√†n su«í', 'trans': 'exploration'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': 'ÂáèÂº±', 'pinyin': 'ji«én ru√≤', 'trans': 'weaken'}, {'word': 'ÂÅúÊªû', 'pinyin': 't√≠ng zh√¨', 'trans': 'stagnate'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'ËΩ¨Êç¢', 'pinyin': 'zhu«én hu√†n', 'trans': 'conversion'}, {'word': 'ÊñπÁ®ã', 'pinyin': 'fƒÅng ch√©ng', 'trans': 'equation'}, {'word': '‰∏ãÊ∏∏', 'pinyin': 'xi√† y√≥u', 'trans': 'downstream'}, {'word': 'ÁêÜËÆ∫', 'pinyin': 'l«ê l√πn', 'trans': 'theory'}, {'word': 'ÂÆûËØÅ', 'pinyin': 'sh√≠ zh√®ng', 'trans': 'empirical'}, {'word': 'ÂàÜÊûê', 'pinyin': 'fƒìn xƒ´', 'trans': 'analysis'}, {'word': 'Âä®ÊÄÅ', 'pinyin': 'd√≤ng t√†i', 'trans': 'dynamics'}, {'word': 'ÊúÄÁªà', 'pinyin': 'zu√¨ zh≈çng', 'trans': 'ultimately'}, {'word': 'ÊäÄÊúØ', 'pinyin': 'j√¨ sh√π', 'trans': 'technique'}, {'word': 'ÊéßÂà∂', 'pinyin': 'k√≤ng zh√¨', 'trans': 'control'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'Ë°®Êòé', 'pinyin': 'bi«éo m√≠ng', 'trans': 'indicate'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': '‰øÉËøõ', 'pinyin': 'c√π j√¨n', 'trans': 'promote'}, {'word': 'ÈÅøÂÖç', 'pinyin': 'b√¨ mi«én', 'trans': 'avoid'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}]
[29.05.2025 14:13] Renaming previous Chinese page.
[29.05.2025 14:13] Renaming previous data. zh.html to ./d/2025-05-28_zh_reading_task.html
[29.05.2025 14:13] Writing Chinese reading task.
[29.05.2025 14:13] Writing result.
[29.05.2025 14:13] Renaming log file.
[29.05.2025 14:13] Renaming previous data. log.txt to ./logs/2025-05-29_last_log.txt
