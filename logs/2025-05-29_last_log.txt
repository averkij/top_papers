[29.05.2025 17:11] Read previous papers.
[29.05.2025 17:11] Generating top page (month).
[29.05.2025 17:11] Writing top page (month).
[29.05.2025 18:15] Read previous papers.
[29.05.2025 18:15] Get feed.
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22617
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21600
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20411
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22651
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22312
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22453
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21136
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22457
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22334
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21925
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19253
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18600
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22232
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21887
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19075
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22129
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22648
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22202
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19187
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17663
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22019
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22525
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20779
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21876
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22613
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18227
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22523
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22338
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18366
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22203
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18700
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17870
[29.05.2025 18:15] Extract page data from URL. URL: https://huggingface.co/papers/2505.17330
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21960
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20298
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17507
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15813
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12667
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22645
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21191
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21060
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21582
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20715
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20589
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19051
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22664
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20444
[29.05.2025 18:15] Extract page data from URL. URL: https://huggingface.co/papers/2505.22642
[29.05.2025 18:15] Extract page data from URL. URL: https://huggingface.co/papers/2505.21862
[29.05.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18149
[29.05.2025 18:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[29.05.2025 18:15] No deleted papers detected.
[29.05.2025 18:15] Downloading and parsing papers (pdf, html). Total: 50.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22617.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22617.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22617.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.21600.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.21600.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.21600.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.20411.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.20411.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.20411.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22651.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22651.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22651.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22312.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22312.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22312.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22453.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22453.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22453.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.21136.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.21136.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.21136.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22457.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22457.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22457.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22334.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22334.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22334.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.21925.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.21925.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.21925.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.19253.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.19253.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.19253.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.18600.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.18600.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.18600.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22232.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22232.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22232.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.21887.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.21887.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.21887.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.19075.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.19075.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.19075.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22129.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22129.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22129.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22648.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22648.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22648.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22202.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22202.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22202.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.19187.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.19187.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.19187.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.17663.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.17663.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.17663.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22019.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22019.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22019.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22525.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22525.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22525.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.20779.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.20779.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.20779.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.21876.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.21876.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.21876.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22613.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22613.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22613.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.18227.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.18227.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.18227.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22523.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22523.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22523.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22338.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22338.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22338.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.18366.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.18366.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.18366.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.22203.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.22203.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.22203.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.18700.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.18700.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.18700.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.17870.
[29.05.2025 18:15] Extra JSON file exists (./assets/json/2505.17870.json), skip PDF parsing.
[29.05.2025 18:15] Paper image links file exists (./assets/img_data/2505.17870.json), skip HTML parsing.
[29.05.2025 18:15] Success.
[29.05.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2505.17330.
[29.05.2025 18:15] Downloading paper 2505.17330 from http://arxiv.org/pdf/2505.17330v1...
[29.05.2025 18:15] Extracting affiliations from text.
[29.05.2025 18:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Published in the Proceedings of the COLING 2025 FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding Amit Agarwal1, Srikant Panda2, Kulbhushan Pachuri2 1 OCI, Oracle USA, 2 OCI, Oracle India Correspondence: amit.h.agarwal@oracle.com "
[29.05.2025 18:16] Response: ```python
["OCI, Oracle USA", "OCI, Oracle India"]
```
[29.05.2025 18:16] Deleting PDF ./assets/pdf/2505.17330.pdf.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.21960.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.21960.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.21960.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.20298.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.20298.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.20298.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.17507.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.17507.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.17507.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.15813.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.15813.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.15813.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.12667.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.12667.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.12667.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.22645.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.22645.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.22645.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.21191.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.21191.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.21191.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.21060.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.21060.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.21060.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.21582.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.21582.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.21582.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.20715.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.20715.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.20715.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.20589.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.20589.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.20589.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.19051.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.19051.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.19051.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.22664.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.22664.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.22664.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.20444.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.20444.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.20444.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.22642.
[29.05.2025 18:16] Downloading paper 2505.22642 from http://arxiv.org/pdf/2505.22642v1...
[29.05.2025 18:16] Extracting affiliations from text.
[29.05.2025 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 2 ] . [ 1 2 4 6 2 2 . 5 0 5 2 : r FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control Younggyo Seo1 Carmelo Sferrazza1 Haoran Geng1 Michal Nauman1,2 Zhao-Heng Yin1 Pieter Abbeel1 1University of California, Berkeley 2University of Warsaw https://younggyo.me/fast_td "
[29.05.2025 18:16] Response: ```python
["University of California, Berkeley", "University of Warsaw"]
```
[29.05.2025 18:16] Deleting PDF ./assets/pdf/2505.22642.pdf.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.21862.
[29.05.2025 18:16] Downloading paper 2505.21862 from http://arxiv.org/pdf/2505.21862v1...
[29.05.2025 18:16] Extracting affiliations from text.
[29.05.2025 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 2 ] . [ 1 2 6 8 1 2 . 5 0 5 2 : r Towards Scalable Language-Image Pre-training for 3D Medical Imaging Chenhui Zhao Yiwei Lyu Asadur Chowdury Edward Harake Akhil Kondepudi Akshay Rao Xinhai Hou Honglak Lee Todd Hollon "
[29.05.2025 18:16] Response: []
[29.05.2025 18:16] Extracting affiliations from text.
[29.05.2025 18:16] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 2 ] . [ 1 2 6 8 1 2 . 5 0 5 2 : r Towards Scalable Language-Image Pre-training for 3D Medical Imaging Chenhui Zhao Yiwei Lyu Asadur Chowdury Edward Harake Akhil Kondepudi Akshay Rao Xinhai Hou Honglak Lee Todd HollonLanguage-image pre-training has demonstrated strong performance in 2D medical imaging, but its success in 3D modalities such as CT and MRI remains limited due to the high computational demands of volumetric data, which pose significant barrier to training on large-scale, uncurated clinical studies. In this study, we introduce Hierarchical attention for Language-Image Pre-training (HLIP), scalable pre-training framework for 3D medical imaging. HLIP adopts lightweight hierarchical attention mechanism inspired by the natural hierarchy of radiology data: slice, scan, and study. This mechanism exhibits strong generalizability, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when pre-trained on CT-RATE. Moreover, the computational efficiency of HLIP enables direct training on uncurated datasets. Trained on 220K patients with 3.13 million scans for brain MRI and 240K patients with 1.44 million scans for head CT, HLIP achieves state-of-theart performance, e.g., +32.4% balanced ACC on the proposed publicly available brain MRI benchmark Pub-Brain-5; +1.4% and +6.9% macro AUC on head CT benchmarks RSNA and CQ500, respectively. These results demonstrate that, with HLIP, directly pre-training on uncurated clinical datasets is scalable and effective direction for language-image pre-training in 3D medical imaging. The code is available at https://github.com/Zch0414/hlipLanguage-supervised pre-training is well-suited for radiology, where each study comprises medical images paired with corresponding radiologists report. This natural alignment between visual and textual information has motivated the adaptation of language-image pre-training methods, such as CLIP [1, 2], to learn clinically meaningful radiology representations. CLIP-based models are especially notable for their clinical utility: they demonstrate strong zero-shot transfer performance on diagnostic tasks [3, 4], and their encoders consistently improve performance on multimodal learning benchmarks [5, 6]. In the domain of 2D medical imaging, such as chest X-rays, language-supervised pre-training is key driver for integrating computer vision into clinical workflows [7, 8, 9]. However, language-supervised pre-training in 3D medical imaging has yet to reach the scale or performance demonstrated in 2D modalities. For instance, chest X-ray CLIP models have been trained on over 500K 2D images [8, 9, 10], achieving human-level performance on multiple diagnostic In contrast, progress in 3D medical imaging remains limited, with existing models tasks [9]. underperforming relative to their 2D counterparts [3, 5]. We attribute this performance gap between 2D and 3D language-image pre-training to three primary factors: the need for data curation and annotation, limitations in model architectures, and the complexity of 3D medical imaging studies. Computed tomography (CT) and magnetic resonance imaging (MRI) generate 3D volumetric images across various anatomical regions, including the brain, chest, and abdomen. Unlike 2D imaging, single clinical CT or MRI study may comprise multiple 3D volumes, often called scans or sequences, each capturing different imaging protocols or orientations. As illustrated in Figure 1, standard Figure 1: Illustration of (a) an uncurated study for patient. While previous work has relied on annotation and curation, HLIP enables language-image pre-training directly on such data. (b) HLIP achieves higher performance in zero-shot brain MRI disease classification while using less memory than the original ViT, which can only be trained with gradient checkpointing. MRI study typically includes several sequences (e.g., T1-weighted, T2-weighted, and FLAIR), each contributing distinct diagnostic information. Similarly, CT studies often include scans acquired with varying orientations or scanner settings within the same study. Naively encoding such uncurated studies with standard visual encoders, such as the Vision Transformer (ViT) [11], can produce on the order of 104 tokens per study, resulting in substantial computational overhead. The sheer size and structural complexity of uncurated studies therefore present significant barrier to scalable languageimage pre-training. common strategy is to curate datasets by having radiologists manually select representative scan or 2D slice from each study, as shown in Figure 1 (a) [3, 4, 5, 6]. Alternatively, specialized model architectures, which depend heavily on preprocessing, have been proposed to reduce complexity[3]. Yet, these bespoke designs tend to generalize and scale poorly in real-world settings and are incompatible with established ViT training protocols [2, 12, 13] In this work, we address the key barriers to scaling language-image pre-training for 3D medical imaging by directly leveraging uncurated studies and the original ViT. To effectively extract features from uncurated studies, we introduce Hierarchical attention for Language-Image Pre-training (HLIP), lightweight adaptation that leverages the natural hierarchy of radiology data: slice, scan, and study. Unlike architectural designs such as Swin [14], MViT [15, 16], and Hiera [17], HLIP utilizes the radiology data structure to define the attention scope. Furthermore, compared to efficiency-oriented modules such as window attention [18], HLIP does not trade accuracy for reduced compute. As illustrated in Figure 1 (b), our method achieves higher performance while using less memory than ViT. Moreover, HLIP is orthogonal to flash attention [19] and patch dropout [20], further alleviating the computational burden associated with uncurated studies that contain multiple 3D scans. When trained on the public CT-RATE [3] dataset, HLIP demonstrates strong generalizability, outperforming the state-of-the-art model [21] by 4.3% macro AUC on the external Rad-ChestCT [22] dataset. Furthermore, HLIP enables scalable and effective vision-language pre-training on health system-scale datasets. Specifically, HLIP trained on our health system outperforms 2D foundation models [23, 24] on the proposed publicly available brain MRI benchmark, Pub-Brain-5, by 32.4% balanced ACC; and surpasses the head CT foundation model [25] by 1.4% and 6.9% macro AUC on the RSNA [26] and CQ500 [27] benchmarks, respectively. Our key contr"
[29.05.2025 18:16] Mistral response. {"id": "af7b532a5f514376abd6715d2e171826", "object": "chat.completion", "created": 1748542596, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1712, "total_tokens": 1720, "completion_tokens": 8}}
[29.05.2025 18:16] Response: ```python
[]
```
[29.05.2025 18:16] Deleting PDF ./assets/pdf/2505.21862.pdf.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2505.18149.
[29.05.2025 18:16] Extra JSON file exists (./assets/json/2505.18149.json), skip PDF parsing.
[29.05.2025 18:16] Paper image links file exists (./assets/img_data/2505.18149.json), skip HTML parsing.
[29.05.2025 18:16] Success.
[29.05.2025 18:16] Enriching papers with extra data.
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 0. This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished ...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 1. Roads to Rome (R2R) selectively utilizes large language models for critical reasoning tasks to enhance efficiency and performance in lightweight models.  					AI-generated summary 				 Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhea...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 2. A novel pipeline extracts real-world, interactive software engineering tasks from GitHub to create SWE-rebench, improving the evaluation of reinforcement learning models in SWE.  					AI-generated summary 				 LLM-based agents have shown promising capabilities in a growing range of software engineer...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 3. Sherlock, a self-correction and self-improvement framework for reasoning vision-language models, enhances accuracy across benchmarks using limited annotated data.  					AI-generated summary 				 Reasoning Vision-Language Models (VLMs) have shown promising performance on complex multimodal tasks. How...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 4. The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on th...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 5. Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal data--an ultimately unsustainable resource. While rec...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 6. The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by utilizing quantization to accelerate matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster ins...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 7. Next-event prediction (NEP) is proposed as a learning task to enable MLLMs to reason temporally over video inputs, using future video segments as a self-supervised signal.  					AI-generated summary 				 Next-token prediction serves as the foundational learning task enabling reasoning in LLMs. But w...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 8. Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While "aha moment" patterns--where models exhibit self-correction through reflection--are often attribut...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 9. We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning. Instead of taking a physics-centric approach to rendering, we formula...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 10. DeepResearchGym provides an open-source evaluation framework for deep research systems using a reproducible search API and LLM-as-a-judge assessments.  					AI-generated summary 				 Deep research systems represent an emerging class of agentic information retrieval methods that generate comprehensiv...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 11. Chain-of-Zoom (CoZ) enhances single-image super-resolution models by using an autoregressive chain of intermediate scale-states and multi-scale-aware prompts to achieve extreme magnifications with high quality.  					AI-generated summary 				 Modern single-image super-resolution (SISR) models delive...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 12. JQL systematically curates high-quality multilingual training data using pretrained multilingual embeddings, outperforming heuristic methods and improving downstream model training across diverse languages.  					AI-generated summary 				 High-quality multilingual training data is essential for effe...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 13. SVRPBench introduces a new benchmark for vehicle routing under uncertainty, simulating realistic urban conditions and highlighting the limitations of state-of-the-art RL solvers.  					AI-generated summary 				 Robust routing under uncertainty is central to real-world logistics, yet most benchmarks ...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 14. UniR, a lightweight reasoning module, enhances Large Language Models with specialized reasoning abilities through modular composition, improving performance and generalization at lower computational costs.  					AI-generated summary 				 Large Language Models (LLMs) have demonstrated remarkable gene...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 15. Analysis of fine-tuning diffusion models for panoramic image generation reveals distinct roles of attention module matrices and introduces UniPano, a memory-efficient and speed-enhanced baseline framework.  					AI-generated summary 				 Recent prosperity of text-to-image diffusion models, e.g. Stab...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 16. Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-t...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 17. Pretrained language models can adapt to operate in sentence space, reason over structured semantic units, and achieve competitive performance with Chain-of-Thought while reducing inference-time FLOPs.  					AI-generated summary 				 Autoregressive language models (LMs) generate one token at a time, ...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 18. A framework called PIR refines the importance of reasoning steps in large language models by pruning low-importance functional elements, leading to more concise reasoning chains with improved accuracy and reduced computational demands.  					AI-generated summary 				 Large language models (LLMs) hav...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 19. The DynToM benchmark evaluates LLMs' ability to track and understand the temporal progression of mental states, revealing significant gaps compared to human performance.  					AI-generated summary 				 As Large Language Models (LLMs) increasingly participate in human-AI interactions, evaluating thei...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 20. VRAG-RL, a reinforcement learning framework, enhances reasoning and visual information handling in RAG methods by integrating visual perception tokens and employing specialized action spaces and rewards.  					AI-generated summary 				 Effectively retrieving, reasoning and understanding visually ric...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 21. Thinking with Generated Images allows large multimodal models to generate and critique intermediate visual steps, enhancing visual reasoning capabilities and achieving significant improvements in complex scenarios.  					AI-generated summary 				 We present Thinking with Generated Images, a novel pa...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 22. A large-scale knowledge base of recombination examples is built from scientific paper abstracts using an LLM-based extraction model to analyze and inspire new creative directions in AI.  					AI-generated summary 				 A hallmark of human innovation is the process of recombination -- creating origina...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 23. EPiC is a framework for efficient 3D camera control in video diffusion models that constructs high-quality anchor videos through first-frame visibility masking, integrates them using a lightweight ControlNet module, and achieves state-of-the-art performance on I2V tasks with minimal resources.  				...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 24. A novel iterative framework, RICO, improves image caption accuracy by using visual reconstruction and a text-to-image model to refine discrepancies, while RICO-Flash enhances efficiency using DPO.  					AI-generated summary 				 Image recaptioning is widely used to generate training datasets with en...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 25. In Transformer architectures, tokens\textemdash discrete units derived from raw data\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input's essential information. Due to the...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 26. Generating high-quality, multi-layer transparent images from text prompts can unlock a new level of creative control, allowing users to edit each layer as effortlessly as editing text outputs from LLMs. However, the development of multi-layer generative models lags behind that of conventional text-t...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 27. Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model param...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 28. Enterprise search systems often struggle to retrieve accurate, domain-specific information due to semantic mismatches and overlapping terminologies. These issues can degrade the performance of downstream applications such as knowledge management, customer support, and retrieval-augmented generation ...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 29. The study examines the effectiveness and reliability of rule-based and model-based verifiers in reinforcement learning with verifiable reward, highlighting limitations and vulnerabilities in their use for mathematical reasoning tasks.  					AI-generated summary 				 Trustworthy verifiers are essenti...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 30. Recent advances in Visual Language Models (VLMs) have demonstrated exceptional performance in visual reasoning tasks. However, geo-localization presents unique challenges, requiring the extraction of multigranular visual cues from images and their integration with external world knowledge for system...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 31. A generative AI model is fine-tuned with labeled falsehoods to reduce misinformation generation, analogous to biological immunization.  					AI-generated summary 				 Generative AI models often learn and reproduce false information present in their training corpora. This position paper argues that, ...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 32. FS-DAG, a modular model architecture, efficiently adapts to diverse document types with few-shot learning for visually rich document understanding in resource-constrained environments.  					AI-generated summary 				 In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable and ef...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 33. Time-independent Unified Encoder TiUE reduces inference time and improves diversity and quality in Text-to-Image diffusion models by sharing encoder features across decoder time steps.  					AI-generated summary 				 Text-to-Image (T2I) diffusion models have made remarkable advancements in generativ...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 34. Manga, or Japanese comics, is a richly multimodal narrative form that blends images and text in complex ways. Teaching large multimodal models (LMMs) to understand such narratives at a human-like level could help manga creators reflect on and refine their stories. To this end, we introduce two bench...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 35. HuggingKG, a large-scale knowledge graph, enhances open source ML resource management by enabling advanced queries and analyses via HuggingBench.  					AI-generated summary 				 The rapid growth of open source machine learning (ML) resources, such as models and datasets, has accelerated IR research....
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 36. BraInCoRL employs a transformer-based in-context learning approach to model higher visual cortex neural responses with few-shot examples, demonstrating superior performance and generalizability across new subjects, stimuli, and datasets.  					AI-generated summary 				 Understanding functional repre...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 37. Safe-Sora embeds invisible watermarks into AI-generated videos using a hierarchical adaptive matching mechanism and a 3D wavelet transform-enhanced Mamba architecture, achieving top performance in video quality, watermark fidelity, and robustness.  					AI-generated summary 				 The explosive growth...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 38. Research examines LLM performance biases between Simplified and Traditional Chinese in regional term and name choice tasks, attributing differences to training data and tokenization.  					AI-generated summary 				 While the capabilities of Large Language Models (LLMs) have been studied in both Simp...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 39. The finetuning of Large Language Models (LLMs) has significantly advanced their instruction-following capabilities, yet the underlying computational mechanisms driving these improvements remain poorly understood. This study systematically examines how fine-tuning reconfigures LLM computations by iso...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 40. A novel feed-forward model achieves fast 3D stylization using sparse view images, maintaining multi-view consistency and high-quality style transfer while retaining reconstruction accuracy.  					AI-generated summary 				 Stylizing 3D scenes instantly while maintaining multi-view consistency and fai...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 41. An agent-based tutoring system for electrical engineering enhances learning through natural circuit interaction, context-retrieving generation, and guided questioning, demonstrating superior performance compared to baseline methods.  					AI-generated summary 				 Intelligent tutoring systems combin...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 42. MUSEG, an RL-based method with timestamp-aware multi-segment grounding, significantly enhances the temporal understanding of large language models by improving alignment with video segments and demonstrating superior performance in temporal reasoning tasks.  					AI-generated summary 				 Video temp...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 43. Prot2Token unifies protein prediction tasks using an autoregressive decoder with task tokens, improving efficiency and accuracy across different benchmarks compared to specialized models.  					AI-generated summary 				 The diverse nature of protein prediction tasks has traditionally necessitated sp...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 44. Effective data selection is critical for efficient training of modern Large Language Models (LLMs). This paper introduces Influence Distillation, a novel, mathematically-justified framework for data selection that employs second-order information to optimally weight training samples. By distilling e...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 45. The approach of training vision encoders with small surrogate models before transferring them to large language models reduces training costs and enhances performance.  					AI-generated summary 				 Vision language models (VLMs) typically pair a modestly sized vision encoder with a large language m...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 46. Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 47. FastTD3, an enhanced RL algorithm with parallel simulation and distributional critic, significantly accelerates training for humanoid robots.  					AI-generated summary 				 Reinforcement learning (RL) has driven significant progress in robotics, but its complexity and long training times remain maj...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 48. Hierarchical attention mechanism for language-image pre-training in 3D medical imaging achieves state-of-the-art performance on uncurated clinical datasets.  					AI-generated summary 				 Language-image pre-training has demonstrated strong performance in 2D medical imaging, but its success in 3D mo...
[29.05.2025 18:16] ********************************************************************************
[29.05.2025 18:16] Abstract 49. First Finish Search improves accuracy in large language models by stopping inference at the first completed sample, significantly outperforming other decoding strategies in reasoning tasks.  					AI-generated summary 				 Test-time scaling (TTS), which involves dynamic allocation of compute during i...
[29.05.2025 18:16] Read previous papers.
[29.05.2025 18:16] Generating reviews via LLM API.
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#reasoning"], "emoji": "🔬", "ru": {"title": "Управление энтропией для масштабирования обучения с подкреплением в больших языковых моделях", "desc": "Статья рассматривает проблему снижения энтропии политики при масштабировании обучения с подкрепле
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#small_models", "#architecture", "#optimization", "#reasoning", "#benchmark", "#math", "#training"], "emoji": "🛣️", "ru": {"title": "Эффективное сочетание больших и малых языковых моделей для улучшения рассуждений", "desc": "Статья представляет метод Roads to Rome (R2R), который сел
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#data", "#open_source", "#games", "#rl", "#dataset"], "emoji": "🤖", "ru": {"title": "Автоматизированное создание датасетов для обучения ИИ-ассистентов в разработке ПО", "desc": "Статья представляет новый подход к созданию датасета для обучения мод
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#training", "#multimodal", "#reasoning"], "emoji": "🕵️", "ru": {"title": "Самокоррекция VLM: меньше данных, выше точность", "desc": "Статья представляет Sherlock - фреймворк для самокоррекции и самосовершенствования моделей зрительно-языкового рассужде
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rl", "#benchmark", "#open_source", "#training"], "emoji": "🧠", "ru": {"title": "Усиление рассуждений языковых моделей с помощью обучения с подкреплением", "desc": "Статья представляет Skywork-OR1, эффективную реализацию обучения с подкреплением для ул
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#rlhf", "#multimodal", "#training", "#synthetic"], "emoji": "🤖", "ru": {"title": "Самосовершенствование ИИ без учителя", "desc": "Статья представляет новый метод MM-UPT для улучшения мультимодальных больших языковых моделей (MLLM) без использования размеченных д
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#architecture", "#inference"], "emoji": "🚀", "ru": {"title": "Ускорение механизма внимания без потери точности", "desc": "SageAttention2++ - это улучшенная версия SageAttention2, которая использует квантизацию для ускорения матричных умножений в механизме внимания. Основное нововвед
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#benchmark", "#training", "#video", "#multimodal", "#dataset", "#reasoning"], "emoji": "🎬", "ru": {"title": "Предсказание будущего: новый подход к обучению ИИ временному мышлению", "desc": "Статья предлагает новую задачу обучения для мультимодальных языковых моделей (MLLM) - предска
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#multimodal", "#benchmark", "#open_source", "#training"], "emoji": "🧠", "ru": {"title": "Двухэтапное обучение для прорыва в мультимодальном рассуждении", "desc": "Статья исследует улучшение мультимодального рассуждения в больших языковых моделях. Авторы предлага
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#architecture", "#3d"], "emoji": "🎨", "ru": {"title": "Нейронный рендеринг без физики: от треугольников к пикселям", "desc": "RenderFormer - это нейронная система рендеринга, которая напрямую создает изображение из треугольного представления сцены с полными эффектами глобального осв
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#alignment", "#rag"], "emoji": "🔬", "ru": {"title": "Открытая платформа для оценки систем глубокого исследования", "desc": "DeepResearchGym - это открытая система оценки для систем глубокого исследования, использующая воспроизводимый поисковый API и оце
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#alignment", "#cv", "#optimization", "#diffusion", "#rlhf", "#rag"], "emoji": "🔍", "ru": {"title": "Революция в сверхразрешении изображений: от пикселей к деталям", "desc": "Статья представляет Chain-of-Zoom (CoZ) - фреймворк для улучшения моделей сверхразрешения одиночных изображен
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#multilingual", "#data", "#transfer_learning", "#open_source", "#low_resource", "#dataset"], "emoji": "🌍", "ru": {"title": "JQL: качественные многоязычные данные для обучения языковых моделей", "desc": "JQL - это новый подход к созданию многоязычных обучающих данных высокого качеств
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#dataset", "#rl", "#benchmark"], "emoji": "🚚", "ru": {"title": "Реалистичный бенчмарк для маршрутизации в условиях городской неопределенности", "desc": "SVRPBench представляет новый бенчмарк для маршрутизации транспортных средств в условиях неопредел
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#open_source", "#training", "#architecture", "#reasoning", "#rlhf"], "emoji": "🧠", "ru": {"title": "UniR: Универсальный модуль рассуждений для усиления больших языковых моделей", "desc": "UniR - это универсальный модуль рассуждений для больших языковых моделей (LLM). Он позволяет ул
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#training", "#diffusion", "#cv"], "emoji": "�panorama", "ru": {"title": "Раскрытие механизмов адаптации диффузионных моделей к панорамной генерации", "desc": "Исследование процесса тонкой настройки диффузионных моделей для генерации панорамных изображени
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#agi", "#reasoning", "#agents", "#benchmark", "#training"], "emoji": "🕸️", "ru": {"title": "Новая парадигма обучения агентов для автономного поиска информации в сети", "desc": "Эта статья представляет новый подход к созданию агентов для автономного поиска информации. Авторы предлага
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#architecture", "#inference", "#interpretability", "#data", "#reasoning"], "emoji": "🧠", "ru": {"title": "Языковые модели учатся мыслить предложениями", "desc": "Исследование показывает, что предобученные языковые модели могут быть адаптированы для работы в пространстве предложений,
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#optimization", "#inference", "#reasoning", "#benchmark", "#training"], "emoji": "✂️", "ru": {"title": "PIR: Оптимизация рассуждений в ИИ через умное сокращение", "desc": "PIR - это фреймворк, который оптимизирует важность шагов рассуждения в больших языковых моделях путем удаления 
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#alignment"], "emoji": "🧠", "ru": {"title": "Большие языковые модели отстают от людей в понимании динамики психических состояний", "desc": "Представлен новый бенчмарк DynToM для оценки способности больших языковых моделей (LLM) отслеживать изменение психи
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#rl", "#rag", "#multimodal", "#reasoning", "#optimization"], "emoji": "🧠", "ru": {"title": "VRAG-RL: Улучшение визуальных рассуждений в RAG с помощью обучения с подкреплением", "desc": "VRAG-RL - это новая система обучения с подкреплением для улучшения рассуждений и обработки визуал
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#cv", "#reasoning", "#multimodal"], "emoji": "🧠", "ru": {"title": "Визуальное воображение ИИ: новый уровень мышления мультимодальных моделей", "desc": "Статья представляет новую парадигму 'Мышление с генерируемыми изображениями', которая позволяет крупн
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#science", "#multimodal", "#data", "#dataset", "#transfer_learning"], "emoji": "🧬", "ru": {"title": "CHIMERA: вдохновение для инноваций через анализ научной литературы", "desc": "Исследователи создали CHIMERA - крупномасштабную базу знаний примеров рекомбинации, извлеченных из научн
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#optimization", "#video", "#training", "#transfer_learning", "#diffusion", "#3d", "#multimodal"], "emoji": "🎥", "ru": {"title": "Эффективный 3D-контроль камеры без сложных аннотаций", "desc": "EPiC - это фреймворк для эффективного 3D-контроля камеры в моделях видеодиффузии. Он созда
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#training", "#open_source", "#hallucinations", "#rlhf", "#multimodal"], "emoji": "🖼️", "ru": {"title": "Точные подписи к изображениям через визуальную реконструкцию", "desc": "RICO - это новая итеративная система для улучшения точности подписей к изображ
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#rl", "#optimization", "#multimodal", "#interpretability", "#long_context", "#architecture", "#hallucinations", "#alignment", "#training"], "emoji": "🔬", "ru": {"title": "Уменьшение токенов: от эффективности к новым горизонтам генеративного моделирования", "desc": "Статья исследует 
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#data", "#cv", "#diffusion", "#dataset", "#open_source", "#synthetic"], "emoji": "🎨", "ru": {"title": "Прорыв в создании редактируемых многослойных изображений с помощью ИИ", "desc": "Данная статья представляет новый подход к генерации многослойных прозрачных изображений на основе т
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#optimization", "#training", "#interpretability", "#rlhf"], "emoji": "🔍", "ru": {"title": "Text2Grad: Точная настройка языковых моделей с помощью текстовых градиентов", "desc": "Статья представляет новый подход к обучению языковых моделей под названием Text2Grad. Этот метод преобраз
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#rag", "#optimization", "#dataset", "#benchmark"], "emoji": "🔍", "ru": {"title": "Повышение точности корпоративного поиска с помощью интеллектуального отбора негативных примеров", "desc": "Статья представляет новый подход к улучшению корпоративных поисковых систем с использованием м
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#optimization", "#training", "#math", "#rl", "#security", "#reasoning"], "emoji": "🔍", "ru": {"title": "Ограничения верификаторов в RLVR: необходимость более надежных систем", "desc": "Исследование анализирует эффективность и надежность верификаторов на основе правил и моделей в обу
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#cv", "#interpretability", "#reasoning"], "emoji": "🌍", "ru": {"title": "Умное определение местоположения: VLM с усиленным географическим рассуждением", "desc": "Статья представляет набор инструментов Geo Reason Enhancement (GRE) Suite для улучшения геолока
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#training", "#rlhf", "#alignment", "#ethics", "#hallucinations"], "emoji": "💉", "ru": {"title": "Вакцинация ИИ против дезинформации", "desc": "Статья предлагает новый метод обучения генеративных моделей ИИ для снижения генерации дезинформации. Метод основан на аналогии с биологическ
[29.05.2025 18:16] Querying the API.
[29.05.2025 18:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FS-DAG, a modular model architecture, efficiently adapts to diverse document types with few-shot learning for visually rich document understanding in resource-constrained environments.  					AI-generated summary 				 In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable and efficient model architecture for visually rich document understanding (VRDU) in few-shot settings. FS-DAG leverages domain-specific and language/vision specific backbones within a modular framework to adapt to diverse document types with minimal data. The model is robust to practical challenges such as handling OCR errors, misspellings, and domain shifts, which are critical in real-world deployments. FS-DAG is highly performant with less than 90M parameters, making it well-suited for complex real-world applications for Information Extraction (IE) tasks where computational resources are limited. We demonstrate FS-DAG's capability through extensive experiments for information extraction task, showing significant improvements in convergence speed and performance compared to state-of-the-art methods. Additionally, this work highlights the ongoing progress in developing smaller, more efficient models that do not compromise on performance. Code : https://github.com/oracle-samples/fs-dag
[29.05.2025 18:16] Response: {
  "desc": "FS-DAG - это модульная архитектура модели для понимания визуально насыщенных документов в условиях ограниченных ресурсов. Она использует few-shot обучение для эффективной адаптации к различным типам документов. FS-DAG объединяет специфичные для домена и языка/зрения backbone-сети в модульную структуру. Модель устойчива к практическим проблемам, таким как ошибки OCR и смещение домена, и демонстрирует улучшенную производительность по сравнению с современными методами.",
  "emoji": "📄",
  "title": "FS-DAG: Эффективное понимание документов с минимальными данными"
}
[29.05.2025 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FS-DAG, a modular model architecture, efficiently adapts to diverse document types with few-shot learning for visually rich document understanding in resource-constrained environments.  					AI-generated summary 				 In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable and efficient model architecture for visually rich document understanding (VRDU) in few-shot settings. FS-DAG leverages domain-specific and language/vision specific backbones within a modular framework to adapt to diverse document types with minimal data. The model is robust to practical challenges such as handling OCR errors, misspellings, and domain shifts, which are critical in real-world deployments. FS-DAG is highly performant with less than 90M parameters, making it well-suited for complex real-world applications for Information Extraction (IE) tasks where computational resources are limited. We demonstrate FS-DAG's capability through extensive experiments for information extraction task, showing significant improvements in convergence speed and performance compared to state-of-the-art methods. Additionally, this work highlights the ongoing progress in developing smaller, more efficient models that do not compromise on performance. Code : https://github.com/oracle-samples/fs-dag"

[29.05.2025 18:16] Response: ```python
['ARCHITECTURE', 'SMALL_MODELS', 'DATASET']
```
[29.05.2025 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FS-DAG, a modular model architecture, efficiently adapts to diverse document types with few-shot learning for visually rich document understanding in resource-constrained environments.  					AI-generated summary 				 In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable and efficient model architecture for visually rich document understanding (VRDU) in few-shot settings. FS-DAG leverages domain-specific and language/vision specific backbones within a modular framework to adapt to diverse document types with minimal data. The model is robust to practical challenges such as handling OCR errors, misspellings, and domain shifts, which are critical in real-world deployments. FS-DAG is highly performant with less than 90M parameters, making it well-suited for complex real-world applications for Information Extraction (IE) tasks where computational resources are limited. We demonstrate FS-DAG's capability through extensive experiments for information extraction task, showing significant improvements in convergence speed and performance compared to state-of-the-art methods. Additionally, this work highlights the ongoing progress in developing smaller, more efficient models that do not compromise on performance. Code : https://github.com/oracle-samples/fs-dag"

[29.05.2025 18:16] Response: ```python
['TRANSFER_LEARNING', 'OPTIMIZATION']
```
[29.05.2025 18:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The FS-DAG model is designed for visually rich document understanding using few-shot learning, which allows it to adapt to various document types with limited data. It employs a modular architecture that integrates domain-specific and language/vision backbones, enhancing its flexibility and efficiency. The model effectively addresses common challenges like OCR errors and domain shifts, making it suitable for real-world applications. With under 90 million parameters, FS-DAG achieves high performance in information extraction tasks while being resource-efficient.","title":"Efficient Few-Shot Learning for Document Understanding with FS-DAG"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The FS-DAG model is designed for visually rich document understanding using few-shot learning, which allows it to adapt to various document types with limited data. It employs a modular architecture that integrates domain-specific and language/vision backbones, enhancing its flexibility and efficiency. The model effectively addresses common challenges like OCR errors and domain shifts, making it suitable for real-world applications. With under 90 million parameters, FS-DAG achieves high performance in information extraction tasks while being resource-efficient.', title='Efficient Few-Shot Learning for Document Understanding with FS-DAG'))
[29.05.2025 18:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FS-DAG是一种模块化模型架构，旨在通过少量样本学习来高效适应多样化的文档类型，特别是在资源受限的环境中进行视觉丰富文档理解。该模型利用特定领域和语言/视觉的基础架构，在模块化框架内进行适应，能够处理OCR错误、拼写错误和领域转移等实际挑战。FS-DAG的参数少于9000万，适合在计算资源有限的情况下进行信息提取任务。通过大量实验，FS-DAG在收敛速度和性能上显著优于现有的最先进方法，展示了小型高效模型的持续进展。","title":"高效适应多样文档的少样本学习模型"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FS-DAG是一种模块化模型架构，旨在通过少量样本学习来高效适应多样化的文档类型，特别是在资源受限的环境中进行视觉丰富文档理解。该模型利用特定领域和语言/视觉的基础架构，在模块化框架内进行适应，能够处理OCR错误、拼写错误和领域转移等实际挑战。FS-DAG的参数少于9000万，适合在计算资源有限的情况下进行信息提取任务。通过大量实验，FS-DAG在收敛速度和性能上显著优于现有的最先进方法，展示了小型高效模型的持续进展。', title='高效适应多样文档的少样本学习模型'))
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#diffusion", "#cv", "#inference"], "emoji": "🖼️", "ru": {"title": "Ускорение генерации изображений без потери качества", "desc": "Статья представляет Time-independent Unified Encoder (TiUE) - новый подход к дистилляции диффузионных моделей для генер
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#multimodal", "#training", "#games"], "emoji": "📚", "ru": {"title": "Новые бенчмарки и модель для глубокого понимания манги искусственным интеллектом", "desc": "Статья представляет два новых бенчмарка для оценки понимания манги мультимодальными моделями
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#benchmark", "#graphs"], "emoji": "🧠", "ru": {"title": "HuggingKG: Структурированное представление ресурсов ML для продвинутого анализа", "desc": "HuggingKG - это первый крупномасштабный граф знаний, созданный на основе сообщества Hugging Face для управле
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#architecture", "#interpretability", "#transfer_learning", "#training", "#multimodal", "#cv", "#dataset"], "emoji": "🧠", "ru": {"title": "Революция в моделировании зрительной коры: обучение на малых данных с BraInCoRL", "desc": "BraInCoRL - это новый подход к моделированию нейронных
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#video", "#3d", "#architecture", "#security"], "emoji": "🎥", "ru": {"title": "Защита авторских прав на AI-видео с помощью невидимых водяных знаков", "desc": "Safe-Sora - это новая система для внедрения невидимых водяных знаков в видео, генерируемые искусственным интеллектом. Она исп
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#ethics", "#low_resource", "#multilingual", "#dataset", "#benchmark", "#open_source"], "emoji": "🇨🇳", "ru": {"title": "Скрытые предубеждения LLM в китайском языке: упрощенный vs традиционный", "desc": "Исследование анализирует различия в производительности больших языковых моделей (
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#training", "#optimization", "#interpretability", "#dataset", "#architecture"], "emoji": "🧠", "ru": {"title": "Раскрывая тайны дообучения: как LLM учатся выполнять инструкции", "desc": "Исследование анализирует, как дообучение изменяет вычислительные механизмы больших языковых модел
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#3d"], "emoji": "🎨", "ru": {"title": "Мгновенная 3D-стилизация с сохранением структуры сцены", "desc": "Статья представляет новую модель прямой подачи для быстрой 3D-стилизации с использованием изображений с разреженными видами. Модель 
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#graphs", "#interpretability", "#healthcare", "#science", "#rag", "#games", "#agents", "#multimodal", "#reasoning"], "emoji": "⚡", "ru": {"title": "ИИ-репетитор по электротехнике: персонализированное обучение через интерактивное взаимодействие", "desc": "AITEE - это система репетито
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#training", "#rl", "#multimodal", "#alignment", "#video", "#reasoning"], "emoji": "⏱️", "ru": {"title": "MUSEG: Прорыв во временном понимании видео для больших языковых моделей", "desc": "MUSEG - это новый метод, основанный на обучении с подкреплением, который улучшает временное пон
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#architecture", "#science", "#open_source", "#training", "#dataset", "#optimization"], "emoji": "🧬", "ru": {"title": "Единая модель для всех задач предсказания свойств белков", "desc": "Prot2Token - это унифицированный фреймворк для различных задач предсказания свойств белков, испол
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#training", "#data", "#optimization"], "emoji": "🎯", "ru": {"title": "Эффективный отбор данных для точной настройки языковых моделей", "desc": "Статья представляет новый метод отбора данных для обучения больших языковых моделей (LLM), называемый Influence Distillation. Этот подход и
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#transfer_learning", "#training", "#cv"], "emoji": "🔀", "ru": {"title": "Эффективное обучение зрительно-языковых моделей через суррогатные модели", "desc": "Статья представляет новый подход к обучению мультимодальных моделей, сочетающих зрение и язы
[29.05.2025 18:16] Using data from previous issue: {"categories": ["#video", "#benchmark", "#multimodal", "#long_context", "#architecture"], "emoji": "🎬", "ru": {"title": "HoPE: Улучшение работы мультимодальных моделей с длинными видео", "desc": "Статья представляет новый метод позиционного кодирования для мультимодальных моделей, названный HoPE (Hy
[29.05.2025 18:16] Querying the API.
[29.05.2025 18:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FastTD3, an enhanced RL algorithm with parallel simulation and distributional critic, significantly accelerates training for humanoid robots.  					AI-generated summary 				 Reinforcement learning (RL) has driven significant progress in robotics, but its complexity and long training times remain major bottlenecks. In this report, we introduce FastTD3, a simple, fast, and capable RL algorithm that significantly speeds up training for humanoid robots in popular suites such as HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably simple: we train an off-policy TD3 agent with several modifications -- parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours on a single A100 GPU, while remaining stable during training. We also provide a lightweight and easy-to-use implementation of FastTD3 to accelerate RL research in robotics.
[29.05.2025 18:16] Response: {
  "desc": "FastTD3 - это усовершенствованный алгоритм обучения с подкреплением для ускорения обучения человекоподобных роботов. Он использует параллельное моделирование, обновления с большими батчами и распределенный критик. FastTD3 решает задачи из HumanoidBench менее чем за 3 часа на одном GPU A100. Алгоритм показывает стабильные результаты во время обучения и предоставляется в виде легкой в использовании реализации.",
  "emoji": "🤖",
  "title": "Ускоренное обучение человекоподобных роботов с FastTD3"
}
[29.05.2025 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FastTD3, an enhanced RL algorithm with parallel simulation and distributional critic, significantly accelerates training for humanoid robots.  					AI-generated summary 				 Reinforcement learning (RL) has driven significant progress in robotics, but its complexity and long training times remain major bottlenecks. In this report, we introduce FastTD3, a simple, fast, and capable RL algorithm that significantly speeds up training for humanoid robots in popular suites such as HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably simple: we train an off-policy TD3 agent with several modifications -- parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours on a single A100 GPU, while remaining stable during training. We also provide a lightweight and easy-to-use implementation of FastTD3 to accelerate RL research in robotics."

[29.05.2025 18:16] Response: ```python
["RL", "ROBOTICS", "TRAINING"]
```
[29.05.2025 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FastTD3, an enhanced RL algorithm with parallel simulation and distributional critic, significantly accelerates training for humanoid robots.  					AI-generated summary 				 Reinforcement learning (RL) has driven significant progress in robotics, but its complexity and long training times remain major bottlenecks. In this report, we introduce FastTD3, a simple, fast, and capable RL algorithm that significantly speeds up training for humanoid robots in popular suites such as HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably simple: we train an off-policy TD3 agent with several modifications -- parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours on a single A100 GPU, while remaining stable during training. We also provide a lightweight and easy-to-use implementation of FastTD3 to accelerate RL research in robotics."

[29.05.2025 18:16] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[29.05.2025 18:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FastTD3 is a novel reinforcement learning (RL) algorithm designed to enhance the training efficiency of humanoid robots. It incorporates several key improvements, including parallel simulation and a distributional critic, which allow for faster learning and more stable performance. By utilizing off-policy training with large-batch updates and optimized hyperparameters, FastTD3 can solve complex tasks in less than 3 hours on a single A100 GPU. This approach not only accelerates training but also provides a user-friendly implementation to facilitate further research in robotic applications.","title":"Accelerating Humanoid Robot Training with FastTD3"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FastTD3 is a novel reinforcement learning (RL) algorithm designed to enhance the training efficiency of humanoid robots. It incorporates several key improvements, including parallel simulation and a distributional critic, which allow for faster learning and more stable performance. By utilizing off-policy training with large-batch updates and optimized hyperparameters, FastTD3 can solve complex tasks in less than 3 hours on a single A100 GPU. This approach not only accelerates training but also provides a user-friendly implementation to facilitate further research in robotic applications.', title='Accelerating Humanoid Robot Training with FastTD3'))
[29.05.2025 18:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FastTD3是一种增强的强化学习算法，旨在加速人形机器人的训练。它通过并行仿真、大批量更新和分布式评论者等多种改进，显著提高了训练效率。该算法在单个A100 GPU上能够在3小时内解决多个HumanoidBench任务，同时保持训练的稳定性。我们还提供了一个轻量级且易于使用的FastTD3实现，以促进机器人领域的强化学习研究。","title":"FastTD3：加速人形机器人训练的强化学习新算法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FastTD3是一种增强的强化学习算法，旨在加速人形机器人的训练。它通过并行仿真、大批量更新和分布式评论者等多种改进，显著提高了训练效率。该算法在单个A100 GPU上能够在3小时内解决多个HumanoidBench任务，同时保持训练的稳定性。我们还提供了一个轻量级且易于使用的FastTD3实现，以促进机器人领域的强化学习研究。', title='FastTD3：加速人形机器人训练的强化学习新算法'))
[29.05.2025 18:16] Querying the API.
[29.05.2025 18:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Hierarchical attention mechanism for language-image pre-training in 3D medical imaging achieves state-of-the-art performance on uncurated clinical datasets.  					AI-generated summary 				 Language-image pre-training has demonstrated strong performance in 2D medical imaging, but its success in 3D modalities such as CT and MRI remains limited due to the high computational demands of volumetric data, which pose a significant barrier to training on large-scale, uncurated clinical studies. In this study, we introduce Hierarchical attention for Language-Image Pre-training (HLIP), a scalable pre-training framework for 3D medical imaging. HLIP adopts a lightweight hierarchical attention mechanism inspired by the natural hierarchy of radiology data: slice, scan, and study. This mechanism exhibits strong generalizability, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when pre-trained on CT-RATE. Moreover, the computational efficiency of HLIP enables direct training on uncurated datasets. Trained on 220K patients with 3.13 million scans for brain MRI and 240K patients with 1.44 million scans for head CT, HLIP achieves state-of-the-art performance, e.g., +32.4% balanced ACC on the proposed publicly available brain MRI benchmark Pub-Brain-5; +1.4% and +6.9% macro AUC on head CT benchmarks RSNA and CQ500, respectively. These results demonstrate that, with HLIP, directly pre-training on uncurated clinical datasets is a scalable and effective direction for language-image pre-training in 3D medical imaging. The code is available at https://github.com/Zch0414/hlip
[29.05.2025 18:17] Response: {
  "desc": "Исследователи представили HLIP - новый фреймворк для предобучения моделей на 3D медицинских изображениях. HLIP использует иерархический механизм внимания, учитывающий естественную иерархию радиологических данных: срез, скан и исследование. Этот подход позволяет эффективно обрабатывать объемные данные КТ и МРТ, преодолевая вычислительные ограничения. Модель, предобученная на больших неотобранных клинических наборах данных, достигла лучших результатов на нескольких бенчмарках для 3D медицинских изображений.",

  "emoji": "🧠",

  "title": "Иерархическое внимание открывает новые горизонты в анализе 3D медицинских изображений"
}
[29.05.2025 18:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Hierarchical attention mechanism for language-image pre-training in 3D medical imaging achieves state-of-the-art performance on uncurated clinical datasets.  					AI-generated summary 				 Language-image pre-training has demonstrated strong performance in 2D medical imaging, but its success in 3D modalities such as CT and MRI remains limited due to the high computational demands of volumetric data, which pose a significant barrier to training on large-scale, uncurated clinical studies. In this study, we introduce Hierarchical attention for Language-Image Pre-training (HLIP), a scalable pre-training framework for 3D medical imaging. HLIP adopts a lightweight hierarchical attention mechanism inspired by the natural hierarchy of radiology data: slice, scan, and study. This mechanism exhibits strong generalizability, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when pre-trained on CT-RATE. Moreover, the computational efficiency of HLIP enables direct training on uncurated datasets. Trained on 220K patients with 3.13 million scans for brain MRI and 240K patients with 1.44 million scans for head CT, HLIP achieves state-of-the-art performance, e.g., +32.4% balanced ACC on the proposed publicly available brain MRI benchmark Pub-Brain-5; +1.4% and +6.9% macro AUC on head CT benchmarks RSNA and CQ500, respectively. These results demonstrate that, with HLIP, directly pre-training on uncurated clinical datasets is a scalable and effective direction for language-image pre-training in 3D medical imaging. The code is available at https://github.com/Zch0414/hlip"

[29.05.2025 18:17] Response: ```python
['DATASET', 'CV', 'HEALTHCARE', '3D']
```
[29.05.2025 18:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Hierarchical attention mechanism for language-image pre-training in 3D medical imaging achieves state-of-the-art performance on uncurated clinical datasets.  					AI-generated summary 				 Language-image pre-training has demonstrated strong performance in 2D medical imaging, but its success in 3D modalities such as CT and MRI remains limited due to the high computational demands of volumetric data, which pose a significant barrier to training on large-scale, uncurated clinical studies. In this study, we introduce Hierarchical attention for Language-Image Pre-training (HLIP), a scalable pre-training framework for 3D medical imaging. HLIP adopts a lightweight hierarchical attention mechanism inspired by the natural hierarchy of radiology data: slice, scan, and study. This mechanism exhibits strong generalizability, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when pre-trained on CT-RATE. Moreover, the computational efficiency of HLIP enables direct training on uncurated datasets. Trained on 220K patients with 3.13 million scans for brain MRI and 240K patients with 1.44 million scans for head CT, HLIP achieves state-of-the-art performance, e.g., +32.4% balanced ACC on the proposed publicly available brain MRI benchmark Pub-Brain-5; +1.4% and +6.9% macro AUC on head CT benchmarks RSNA and CQ500, respectively. These results demonstrate that, with HLIP, directly pre-training on uncurated clinical datasets is a scalable and effective direction for language-image pre-training in 3D medical imaging. The code is available at https://github.com/Zch0414/hlip"

[29.05.2025 18:17] Response: ```python
["OPTIMIZATION", "SCIENCE", "OPEN_SOURCE"]
```
[29.05.2025 18:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new method called Hierarchical attention for Language-Image Pre-training (HLIP) specifically designed for 3D medical imaging. HLIP uses a lightweight hierarchical attention mechanism that reflects the natural structure of radiology data, allowing it to efficiently process large volumes of data. The method shows significant improvements in performance on various benchmarks, achieving state-of-the-art results in brain MRI and head CT tasks. By enabling direct training on large, uncurated clinical datasets, HLIP addresses the challenges of computational demands in 3D imaging.","title":"Revolutionizing 3D Medical Imaging with Hierarchical Attention"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new method called Hierarchical attention for Language-Image Pre-training (HLIP) specifically designed for 3D medical imaging. HLIP uses a lightweight hierarchical attention mechanism that reflects the natural structure of radiology data, allowing it to efficiently process large volumes of data. The method shows significant improvements in performance on various benchmarks, achieving state-of-the-art results in brain MRI and head CT tasks. By enabling direct training on large, uncurated clinical datasets, HLIP addresses the challenges of computational demands in 3D imaging.', title='Revolutionizing 3D Medical Imaging with Hierarchical Attention'))
[29.05.2025 18:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本研究提出了一种层次注意力机制，用于3D医学影像的语言-图像预训练，称为HLIP。该方法通过轻量级的层次注意力机制，灵感来源于放射学数据的自然层次结构，能够有效处理体积数据的高计算需求。HLIP在未整理的临床数据集上直接训练，展现出强大的通用性和计算效率，取得了多项基准测试的最佳性能。通过在大规模患者数据上训练，HLIP为3D医学影像的语言-图像预训练提供了一种可扩展且有效的解决方案。","title":"层次注意力机制：3D医学影像的突破性进展"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本研究提出了一种层次注意力机制，用于3D医学影像的语言-图像预训练，称为HLIP。该方法通过轻量级的层次注意力机制，灵感来源于放射学数据的自然层次结构，能够有效处理体积数据的高计算需求。HLIP在未整理的临床数据集上直接训练，展现出强大的通用性和计算效率，取得了多项基准测试的最佳性能。通过在大规模患者数据上训练，HLIP为3D医学影像的语言-图像预训练提供了一种可扩展且有效的解决方案。', title='层次注意力机制：3D医学影像的突破性进展'))
[29.05.2025 18:17] Using data from previous issue: {"categories": ["#training", "#inference", "#optimization", "#reasoning"], "emoji": "🏁", "ru": {"title": "Первый финиширует - первый побеждает: революция в декодировании языковых моделей", "desc": "Метод First Finish Search (FFS) улучшает точность больших языковых моделей, останавливая вывод на перв
[29.05.2025 18:17] Loading Chinese text from previous data.
[29.05.2025 18:17] Renaming data file.
[29.05.2025 18:17] Renaming previous data. hf_papers.json to ./d/2025-05-29.json
[29.05.2025 18:17] Saving new data file.
[29.05.2025 18:17] Generating page.
[29.05.2025 18:17] Renaming previous page.
[29.05.2025 18:17] Renaming previous data. index.html to ./d/2025-05-29.html
[29.05.2025 18:17] [Experimental] Generating Chinese page for reading.
[29.05.2025 18:17] Chinese vocab [{'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'}, {'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'}, {'word': '强化学习', 'pinyin': 'qiáng huà xué xí', 'trans': 'reinforcement learning'}, {'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'}, {'word': '主要', 'pinyin': 'zhǔ yào', 'trans': 'major'}, {'word': '障碍', 'pinyin': 'zhàng ài', 'trans': 'obstacle'}, {'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'}, {'word': '熵', 'pinyin': 'shāng', 'trans': 'entropy'}, {'word': '崩溃', 'pinyin': 'bēng kuì', 'trans': 'collapse'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'}, {'word': '发现', 'pinyin': 'fā xiàn', 'trans': 'discover'}, {'word': '干预', 'pinyin': 'gān yù', 'trans': 'intervention'}, {'word': '情况', 'pinyin': 'qíng kuàng', 'trans': 'situation'}, {'word': '阶段', 'pinyin': 'jiē duàn', 'trans': 'stage'}, {'word': '急剧', 'pinyin': 'jí jù', 'trans': 'drastic'}, {'word': '下降', 'pinyin': 'xià jiàng', 'trans': 'decline'}, {'word': '导致', 'pinyin': 'dǎo zhì', 'trans': 'lead to'}, {'word': '探索', 'pinyin': 'tàn suǒ', 'trans': 'exploration'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '减弱', 'pinyin': 'jiǎn ruò', 'trans': 'weaken'}, {'word': '停滞', 'pinyin': 'tíng zhì', 'trans': 'stagnate'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '转换', 'pinyin': 'zhuǎn huàn', 'trans': 'conversion'}, {'word': '方程', 'pinyin': 'fāng chéng', 'trans': 'equation'}, {'word': '下游', 'pinyin': 'xià yóu', 'trans': 'downstream'}, {'word': '理论', 'pinyin': 'lǐ lùn', 'trans': 'theory'}, {'word': '实证', 'pinyin': 'shí zhèng', 'trans': 'empirical'}, {'word': '分析', 'pinyin': 'fēn xī', 'trans': 'analysis'}, {'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamics'}, {'word': '最终', 'pinyin': 'zuì zhōng', 'trans': 'ultimately'}, {'word': '技术', 'pinyin': 'jì shù', 'trans': 'technique'}, {'word': '控制', 'pinyin': 'kòng zhì', 'trans': 'control'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '表明', 'pinyin': 'biǎo míng', 'trans': 'indicate'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '促进', 'pinyin': 'cù jìn', 'trans': 'promote'}, {'word': '避免', 'pinyin': 'bì miǎn', 'trans': 'avoid'}, {'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'}]
[29.05.2025 18:17] Renaming previous Chinese page.
[29.05.2025 18:17] Renaming previous data. zh.html to ./d/2025-05-28_zh_reading_task.html
[29.05.2025 18:17] Writing Chinese reading task.
[29.05.2025 18:17] Writing result.
[29.05.2025 18:17] Renaming log file.
[29.05.2025 18:17] Renaming previous data. log.txt to ./logs/2025-05-29_last_log.txt
