[03.06.2025 18:16] Read previous papers.
[03.06.2025 18:16] Generating top page (month).
[03.06.2025 18:16] Writing top page (month).
[03.06.2025 19:09] Read previous papers.
[03.06.2025 19:09] Get feed.
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01939
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24760
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01844
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01049
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00539
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23590
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00996
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00411
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01853
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01943
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01713
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01667
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24298
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24846
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01863
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01413
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00577
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01952
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23907
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23059
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23001
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24523
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23977
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01881
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00338
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24842
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24625
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24183
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01084
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00512
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24452
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.23504
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21179
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01484
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00643
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24086
[03.06.2025 19:09] Extract page data from URL. URL: https://huggingface.co/papers/2505.22954
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01928
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01920
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01920
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21724
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19621
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01074
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00930
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00789
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00772
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00723
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00530
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00385
[03.06.2025 19:09] Extract page data from URL. URL: https://huggingface.co/papers/2506.01062
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00979
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00469
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00381
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.24216
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.22865
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.21668
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20285
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18128
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15772
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.01666
[03.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.00523
[03.06.2025 19:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[03.06.2025 19:09] No deleted papers detected.
[03.06.2025 19:09] Downloading and parsing papers (pdf, html). Total: 61.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01939.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01939.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01939.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.24760.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.24760.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.24760.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01844.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01844.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01844.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01049.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01049.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01049.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.00539.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.00539.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.00539.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.23590.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.23590.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.23590.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.00996.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.00996.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.00996.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.00411.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.00411.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.00411.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01853.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01853.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01853.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01943.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01943.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01943.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01713.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01713.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01713.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01667.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01667.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01667.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.24298.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.24298.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.24298.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.24846.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.24846.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.24846.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01863.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01863.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01863.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01413.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01413.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01413.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.00577.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.00577.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.00577.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01952.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01952.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01952.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.23907.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.23907.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.23907.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.23059.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.23059.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.23059.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.23001.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.23001.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.23001.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.24523.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.24523.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.24523.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.23977.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.23977.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.23977.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01881.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01881.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01881.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.00338.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.00338.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.00338.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.24842.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.24842.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.24842.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.24625.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.24625.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.24625.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.24183.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.24183.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.24183.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01084.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01084.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01084.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.00512.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.00512.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.00512.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.24452.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.24452.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.24452.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.23504.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.23504.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.23504.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.21179.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.21179.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.21179.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.01484.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.01484.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.01484.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.00643.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2506.00643.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.00643.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.24086.
[03.06.2025 19:09] Extra JSON file exists (./assets/json/2505.24086.json), skip PDF parsing.
[03.06.2025 19:09] Paper image links file exists (./assets/img_data/2505.24086.json), skip HTML parsing.
[03.06.2025 19:09] Success.
[03.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.22954.
[03.06.2025 19:09] Downloading paper 2505.22954 from http://arxiv.org/pdf/2505.22954v1...
[03.06.2025 19:10] Extracting affiliations from text.
[03.06.2025 19:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Darwin GÃ¶del Machine: Open-Ended Evolution of Self-Improving Agents Jenny Zhang*,1,2 1University of British Columbia Shengran Hu*,1,2,3 Cong Lu1,2,3 Robert Lange,3 2Vector Institute 3Sakana AI Jeff Clune,1,2,4 4Canada CIFAR AI Chair 5 2 0 2 9 2 ] . [ 1 4 5 9 2 2 . 5 0 5 2 : r {jennyzzt,srhu,conglu}@cs.ubc.ca, robert@sakana.ai, jeff.clune@ubc.ca "
[03.06.2025 19:10] Response: ```python
["University of British Columbia", "Vector Institute", "Sakana AI", "Canada CIFAR AI Chair"]
```
[03.06.2025 19:10] Deleting PDF ./assets/pdf/2505.22954.pdf.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.01928.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.01928.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.01928.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.01920.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.01920.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.01920.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.01920.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.01920.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.01920.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2505.21724.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2505.21724.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2505.21724.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2505.19621.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2505.19621.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2505.19621.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.01074.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.01074.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.01074.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.00930.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.00930.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.00930.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.00789.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.00789.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.00789.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.00772.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.00772.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.00772.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.00723.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.00723.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.00723.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.00530.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.00530.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.00530.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.00385.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.00385.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.00385.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.01062.
[03.06.2025 19:10] Downloading paper 2506.01062 from http://arxiv.org/pdf/2506.01062v1...
[03.06.2025 19:10] Extracting affiliations from text.
[03.06.2025 19:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SEALQA: Raising the Bar for Reasoning in Search-Augmented Language Models Thinh Pham Nguyen Nguyen Pratibha Zunjare Weiyuan Chen Yu-Min Tseng Tu Vu Virginia Tech Blacksburg, VA 24061 {thinhphp,tuvu}@vt.edu "
[03.06.2025 19:10] Response: ```python
["Virginia Tech, Blacksburg, VA 24061"]
```
[03.06.2025 19:10] Deleting PDF ./assets/pdf/2506.01062.pdf.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.00979.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.00979.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.00979.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.00469.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.00469.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.00469.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.00381.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.00381.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.00381.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2505.24216.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2505.24216.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2505.24216.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2505.22865.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2505.22865.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2505.22865.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2505.21668.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2505.21668.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2505.21668.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2505.20285.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2505.20285.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2505.20285.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2505.18128.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2505.18128.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2505.18128.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2505.15772.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2505.15772.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2505.15772.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.01666.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.01666.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.01666.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Downloading and parsing paper https://huggingface.co/papers/2506.00523.
[03.06.2025 19:10] Extra JSON file exists (./assets/json/2506.00523.json), skip PDF parsing.
[03.06.2025 19:10] Paper image links file exists (./assets/img_data/2506.00523.json), skip HTML parsing.
[03.06.2025 19:10] Success.
[03.06.2025 19:10] Enriching papers with extra data.
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 0. Token entropy patterns are crucial in RLVR, with high-entropy tokens significantly impacting reasoning performance and model optimization.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities ...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 1. We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various comm...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 2. SmolVLA is a compact, efficient vision-language-action model that achieves competitive performance at reduced computational costs and can be deployed on consumer-grade hardware.  					AI-generated summary 				 Vision-language models (VLMs) pretrained on large-scale multimodal datasets encode rich vi...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 3. SGG, an optimizer wrapper, enhances adaptive learning rates for large language models by grouping gradients and applying cluster-specific scaling, improving convergence and stability.  					AI-generated summary 				 Training large language models (LLMs) poses challenges due to their massive scale an...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 4. ARIA aggregates rewards in an intention space to mitigate reward sparsity and improve policy optimization in language-based reinforcement learning tasks.  					AI-generated summary 				 Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-for...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 5. The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, ...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 6. Temporal In-Context Fine-Tuning (TIC-FT) enhances pretrained video diffusion models for diverse conditional generation tasks with minimal data and without architectural changes.  					AI-generated summary 				 Recent advances in text-to-video diffusion models have enabled high-quality video synthesi...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 7. A unified vision language action framework, LoHoVLA, combines a large pretrained vision language model with hierarchical closed-loop control to improve performance on long-horizon embodied tasks.  					AI-generated summary 				 Real-world embodied agents face long-horizon tasks, characterized by hig...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 8. A native 3D large language model, ShapeLLM-Omni, is proposed to understand and generate 3D assets and text, trained using a 3D vector-quantized variational autoencoder and a new 3D-Alpaca dataset.  					AI-generated summary 				 Recently, the powerful text-to-image capabilities of ChatGPT-4o have le...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 9. Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture m...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 10. Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic an...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 11. EarthMind is a vision-language framework that uses spatial attention prompting and cross-modal fusion for efficient multi-granular and multi-sensor Earth Observation data understanding, outperforming larger models on specialized benchmarks.  					AI-generated summary 				 Large Multimodal Models (LM...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 12. AReaL, a fully asynchronous reinforcement learning system, decouples generation and training to achieve higher GPU utilization and up to 2.57x training speedup for large language models on reasoning tasks.  					AI-generated summary 				 Reinforcement learning (RL) has become a trending paradigm for...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 13. MiCRo, a two-stage framework, improves personalized preference learning for large language models by leveraging binary preference datasets and dynamically adapting mixture weights based on context, effectively capturing diverse human preferences.  					AI-generated summary 				 Reward modeling is a ...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 14. A study on scaling laws and compression techniques shows that a unified capacity metric can predict model performance across different compressed formats, including scalar-quantized, sparse-quantized, and vector-quantized representations.  					AI-generated summary 				 Scaling laws have shaped rece...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 15. Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabi...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 16. Post-training techniques such as Supervised Fine-Tuning and Reinforcement Learning with Verifiable Rewards improve the reasoning and economic rationality of Large Language Models in multi-agent scenarios through domain-aligned training.  					AI-generated summary 				 Directly training Large Languag...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 17. WebChoreArena, a new benchmark comprising 532 tasks, extends the scope of WebArena to more complex and tedious web browsing tasks, measuring advancements in LLM capabilities.  					AI-generated summary 				 Powered by a large language model (LLM), a web browsing agent operates web browsers in a huma...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 18. Cora framework enhances image editing through correspondence-aware noise correction and interpolated attention maps, excelling in structure and texture preservation and generation.  					AI-generated summary 				 Image editing is an important task in computer graphics, vision, and VFX, with recent d...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 19. State Machine Reasoning (SMR) improves information retrieval performance and reduces token usage in large language models by addressing overthinking through a discrete action framework.  					AI-generated summary 				 Chain-of-Thought (CoT) prompting enables complex reasoning in large language model...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 20. DyePack, a framework using backdoor attacks, identifies models that leveraged benchmark test sets during training by introducing benign backdoor samples, ensuring precise false positive rates while preventing false accusations.  					AI-generated summary 				 Open benchmarks are essential for evalua...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 21. Adversarial attacks using Direct Preference Optimization fine-tune language models to evade detection, leading to a significant drop in the performance of existing MGT detectors.  					AI-generated summary 				 Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the cr...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 22. VisualSphinx provides a large-scale synthetic dataset to improve multimodal reasoning in vision language models, enhancing performance on various logical reasoning tasks.  					AI-generated summary 				 Vision language models (VLMs) are expected to perform effective multimodal reasoning and make log...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 23. STORM frameworks facilitates collaborative intent formation in task-oriented dialogue systems by modeling asymmetric information dynamics between UserLLM and AgentLLM.  					AI-generated summary 				 Task-oriented dialogue systems often face difficulties when user utterances seem semantically comple...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 24. The OWSM project is enhanced with a large-scale, cleaned web dataset, leading to improved multilingual speech models comparable to leading industrial models.  					AI-generated summary 				 The Open Whisper-style Speech Models (OWSM) project has developed a series of fully open speech foundation mod...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 25. Model distillation has become essential for creating smaller, deployable language models that retain larger system capabilities. However, widespread deployment raises concerns about resilience to adversarial manipulation. This paper investigates vulnerability of distilled models to adversarial injec...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 26. A novel Video-3D Geometry Large Language Model (VG LLM) extracts 3D information directly from video sequences to enhance 3D scene understanding without additional 3D data, achieving competitive results in various tasks.  					AI-generated summary 				 Previous research has investigated the applicati...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 27. CodeV-R1, an RLVR framework for Verilog generation, achieves state-of-the-art performance in EDA using a rule-based testbench, round-trip data synthesis, and adaptive RLVR training.  					AI-generated summary 				 Large language models (LLMs) trained via reinforcement learning with verifiable reward...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 28. A framework called zip2zip dynamically adjusts token vocabulary in LLMs at inference time using LZW compression, reducing token sequence length and improving inference speed.  					AI-generated summary 				 Tokenization efficiency plays a critical role in the performance and cost of large language m...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 29. A progressive-views paradigm with Pro3D-Editor achieves consistent 3D editing by propagating semantics from key views to less edited ones.  					AI-generated summary 				 Text-guided 3D editing aims to precisely edit semantically relevant local 3D regions, which has significant potential for various...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 30. A unified budget-aware learning rate schedule is proposed to optimize training within limited iteration budgets, outperforming traditional schedules across various tasks and network architectures.  					AI-generated summary 				 The expanding computational costs and limited resources underscore the ...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 31. VAU-R1 uses Multimodal Large Language Models with Reinforcement Fine-Tuning to enhance video anomaly reasoning, complemented by VAU-Bench, a Chain-of-Thought benchmark for evaluating anomaly understanding.  					AI-generated summary 				 Video Anomaly Understanding (VAU) is essential for application...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 32. Normalized Attention Guidance (NAG) enhances diffusion models by providing effective negative guidance across regimes and modalities without retraining.  					AI-generated summary 				 Negative guidance -- explicitly suppressing unwanted attributes -- remains a fundamental challenge in diffusion mod...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 33. A novel pipeline using GPT-4o-mini generates a large-scale dataset for hate speech detoxification, improving baseline model performance in style accuracy, content preservation, and fluency.  					AI-generated summary 				 Detoxification, the task of rewriting harmful language into non-toxic text, ha...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 34. SATA-BENCH evaluates LLMs on multi-answer questions, revealing selections biases and proposing Choice Funnel to improve accuracy and reduce costs in multi-answer reasoning tasks.  					AI-generated summary 				 Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice ...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 35. ComposeAnything improves text-to-image generation by using LLMs for 2.5D semantic layouts, enhancing object placement and coherence in diffusion-based models.  					AI-generated summary 				 Generating images from text involving complex and novel object arrangements remains a significant challenge f...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 36. The Darwin G\"odel Machine improves its coding capabilities through iterative self-modification and open-ended exploration, surpassing other approaches in benchmarks.  					AI-generated summary 				 Today's AI systems have human-designed, fixed architectures and cannot autonomously and continuously ...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 37. Eso-LMs, a novel fusion of autoregressive and masked diffusion models, introduce KV caching to MDMs, achieving faster inference and superior performance on language modeling benchmarks.  					AI-generated summary 				 Diffusion-based language models offer a compelling alternative to autoregressive (...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 38. A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.  					AI-generated summary 				 This paper addresses critical gaps in Arabic language model evaluation by estab...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 39. A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.  					AI-generated summary 				 This paper addresses critical gaps in Arabic language model evaluation by estab...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 40. OmniResponse, a Multimodal Large Language Model, generates high-quality synchronized verbal and non-verbal listener responses using text as an intermediate modality.  					AI-generated summary 				 In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 41. The Preference, Opinion, and Belief survey assesses the subjective tendencies and biases of Large Language Models across various domains and highlights a trend of increased bias in newer model versions.  					AI-generated summary 				 As Large Language Models (LLMs) become deeply integrated into hum...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 42. LLMs representing multiple programming languages in their concept space tend to cluster closer to English and exhibit distinct neuron activations for specific languages, particularly in the upper layers, with highly aligned languages sharing similar representations.  					AI-generated summary 				 S...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 43. A framework called PCogAlign constructs a reward model for aligning vision-language models with personalized situated cognition, using a benchmark with varied Role-Sets.  					AI-generated summary 				 Vision-language models (VLMs) aligned with general human objectives, such as being harmless and ha...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 44. RARE evaluates the robustness of Retrieval-Augmented Generation (RAG) systems against real-world noise, context conflicts, and time-sensitive data with a knowledge-graph-driven benchmark.  					AI-generated summary 				 Retrieval-Augmented Generation (RAG) enhances recency and factuality in answers....
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 45. Leveraging low-rank approximation to identify critical weights for sparse fine-tuning of large language models enhances performance and efficiency compared to full fine-tuning.  					AI-generated summary 				 Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-qua...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 46. Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. In this paper, we argue that, as a community, we should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. ...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 47. Understanding urban socioeconomic conditions through visual data is a challenging yet essential task for sustainable urban development and policy planning. In this work, we introduce CityLens, a comprehensive benchmark designed to evaluate the capabilities of large language-vision models (LLVMs) in ...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 48. MagiCodec, a Transformer-based audio codec, enhances semantic tokenization while maintaining high reconstruction quality, improving compatibility with generative models.  					AI-generated summary 				 Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into ...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 49. SealQA evaluates search-augmented language models' performance on fact-seeking questions with conflicting or noisy search results, revealing limitations in reasoning and factual accuracy.  					AI-generated summary 				 We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented L...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 50. IVY-FAKE dataset and Ivy Explainable Detector (IVY-XDETECTOR) architecture address the limitations of current AIGC detection by providing a unified, explainable framework for images and videos.  					AI-generated summary 				 The rapid advancement of Artificial Intelligence Generated Content (AIGC) ...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 51. Bilingual translation data enhances language transfer and performance in massively multilingual language adaptation of the Llama3 family of models.  					AI-generated summary 				 This paper investigates a critical design decision in the practice of massively multilingual continual pre-training -- t...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 52. Neuro2Semantic reconstructs semantic content from neural signals using LSTM-based alignment and text generation, outperforming existing methods with limited data.  					AI-generated summary 				 Decoding continuous language from neural signals remains a significant challenge in the intersection of n...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 53. A new augmentation technique, Shuffle PatchMix, and a reweighting strategy improve performance in source-free domain adaptation, achieving state-of-the-art results on PACS, VisDA-C, and DomainNet-126 benchmarks.  					AI-generated summary 				 This work investigates Source-Free Domain Adaptation (SF...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 54. A flow matching based streaming binaural speech synthesis framework called BinauralFlow generates high-quality, indistinguishable binaural audio using a causal U-Net architecture and continuous inference pipeline.  					AI-generated summary 				 Binaural rendering aims to synthesize binaural audio t...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 55. R1-Code-Interpreter extends text-only LLMs with improved code generation abilities through supervised fine-tuning and reinforcement learning, enhancing performance on diverse reasoning and planning tasks.  					AI-generated summary 				 Despite advances in reasoning and planning of R1-like models, L...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 56. A novel pre-training framework, MaskSearch, enhances Large Language Models with universal retrieval and reasoning capabilities through a Retrieval Augmented Mask Prediction task, improving their performance in open-domain multi-hop question answering.  					AI-generated summary 				 Retrieval-Augmen...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 57. We introduce Frankentexts, a new type of long-form narratives produced by LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied verbatim from human writings. This task presents a challenging test of controllable generation, requiring models to satisfy a writing prompt, integr...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 58. Acquiring large-scale emotional speech data with strong consistency remains a challenge for speech synthesis. This paper presents MIKU-PAL, a fully automated multimodal pipeline for extracting high-consistency emotional speech from unlabeled video data. Leveraging face detection and tracking algorit...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 59. A multimodal denoising diffusion model is introduced for generating both the structure and continuous parameters of quantum circuits, offering an efficient alternative to traditional quantum operation compilation methods.  					AI-generated summary 				 Efficiently compiling quantum operations remai...
[03.06.2025 19:10] ********************************************************************************
[03.06.2025 19:10] Abstract 60. Implicit distribution alignment and intra-segment guidance enhance distribution matching distillation for large-scale text-to-image and flow-based models, improving convergence and performance.  					AI-generated summary 				 The Distribution Matching Distillation (DMD) has been successfully applied...
[03.06.2025 19:10] Read previous papers.
[03.06.2025 19:10] Generating reviews via LLM API.
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#rl", "#optimization", "#training"], "emoji": "ð§ ", "ru": {"title": "ÐÑÑÐ¾ÐºÐ¾ÑÐ½ÑÑÐ¾Ð¿Ð¸Ð¹Ð½ÑÐµ ÑÐ¾ÐºÐµÐ½Ñ - ÐºÐ»ÑÑ Ðº ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ð¹ ÐÐ", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑÐ²Ð°ÐµÑ, ÑÑÐ¾ Ð¿Ð°ÑÑÐµÑÐ½Ñ ÑÐ½ÑÑÐ¾Ð¿Ð¸Ð¸ ÑÐ¾ÐºÐµÐ½Ð¾Ð² Ð¸Ð³ÑÐ°ÑÑ ÐºÐ»ÑÑÐµÐ²ÑÑ ÑÐ¾Ð»Ñ Ð² Ð¾Ð±ÑÑÐµÐ½Ð¸Ð¸ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ñ Ð¿ÑÐ¾Ð²ÐµÑÑÐµÐ¼ÑÐ¼Ð¸ Ð²Ð¾Ð·Ð½
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#reasoning", "#games", "#dataset"], "emoji": "ð§ ", "ru": {"title": "ÐÐµÑÐºÐ¾Ð½ÐµÑÐ½Ð°Ñ ÑÑÐµÐ½Ð¸ÑÐ¾Ð²ÐºÐ° ÐÐ Ð² Ð¸ÑÐºÑÑÑÑÐ²Ðµ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ð¹", "desc": "Reasoning Gym (RG) - ÑÑÐ¾ Ð±Ð¸Ð±Ð»Ð¸Ð¾ÑÐµÐºÐ° ÑÑÐµÐ´ Ð´Ð»Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð² Ð·Ð°Ð´Ð°ÑÐ°Ñ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ Ñ Ð¿ÑÐ¾Ð²ÐµÑÑÐµÐ¼ÑÐ¼Ð¸ Ð½Ð°Ð³ÑÐ°Ð´Ð°Ð¼Ð¸. ÐÐ½Ð° Ð²ÐºÐ»ÑÑÐ°ÐµÑ Ð±Ð¾Ð»
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#multimodal", "#small_models", "#training", "#benchmark", "#dataset", "#robotics"], "emoji": "ð¤", "ru": {"title": "ÐÐ°Ð»ÐµÐ½ÑÐºÐ°Ñ Ð¼Ð¾Ð´ÐµÐ»Ñ - Ð±Ð¾Ð»ÑÑÐ¸Ðµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑÐ¸", "desc": "SmolVLA - ÑÑÐ¾ ÐºÐ¾Ð¼Ð¿Ð°ÐºÑÐ½Ð°Ñ Ð¸ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»Ñ Ð´Ð»Ñ Ð¾Ð±ÑÐ°Ð±Ð¾ÑÐºÐ¸ Ð·ÑÐµÐ½Ð¸Ñ, ÑÐ·ÑÐºÐ° Ð¸ Ð´ÐµÐ¹ÑÑÐ²Ð¸Ð¹ 
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#training"], "emoji": "ð", "ru": {"title": "SGG: ÐÑÑÐ¿Ð¿Ð¾Ð²Ð¾Ðµ Ð¼Ð°ÑÑÑÐ°Ð±Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð³ÑÐ°Ð´Ð¸ÐµÐ½ÑÐ¾Ð² Ð´Ð»Ñ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ð¸ Ð´Ð»Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ SGG (Scaling with Gradient Gro
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#games", "#reasoning", "#agents", "#rl", "#optimization", "#training"], "emoji": "ð§ ", "ru": {"title": "ARIA: ÐÐ³ÑÐµÐ³Ð°ÑÐ¸Ñ Ð½Ð°Ð³ÑÐ°Ð´ Ð² Ð¿ÑÐ¾ÑÑÑÐ°Ð½ÑÑÐ²Ðµ Ð½Ð°Ð¼ÐµÑÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ ÐÐ-Ð°Ð³ÐµÐ½ÑÐ¾Ð²", "desc": "ARIA - ÑÑÐ¾ Ð¼ÐµÑÐ¾Ð´, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð°Ð³ÑÐµÐ³Ð¸ÑÑÐµÑ Ð½Ð°Ð³ÑÐ°Ð´Ñ Ð² Ð¿ÑÐ¾ÑÑÑÐ°Ð½ÑÑÐ²Ðµ Ð½Ð°Ð¼ÐµÑÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ Ñ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#multimodal", "#training", "#transfer_learning", "#rl", "#cv", "#open_source", "#reasoning", "#games"], "emoji": "ð§©", "ru": {"title": "ÐÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¾ÑÐ²Ð°Ð¸Ð²Ð°ÑÑ Ð¿Ð°Ð·Ð»Ñ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿ÑÐ¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿ÑÐ°Ð²
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#diffusion", "#inference", "#video", "#optimization", "#training"], "emoji": "ð¬", "ru": {"title": "Ð­ÑÑÐµÐºÑÐ¸Ð²Ð½Ð°Ñ Ð°Ð´Ð°Ð¿ÑÐ°ÑÐ¸Ñ Ð²Ð¸Ð´ÐµÐ¾Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑÐ½ÑÐ¼Ð¸ Ð´Ð°Ð½Ð½ÑÐ¼Ð¸", "desc": "ÐÐµÑÐ¾Ð´ Temporal In-Context Fine-Tuning (TIC-FT) ÑÐ»ÑÑÑÐ°ÐµÑ Ð¿ÑÐµÐ´Ð¾Ð±ÑÑÐµÐ½Ð½ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð¸ÑÑÑÐ·Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð»Ñ ÑÐ°Ð·Ð½Ð¾Ð¾Ð±ÑÐ°Ð·Ð½ÑÑ Ð·Ð°Ð´Ð°Ñ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#cv", "#architecture", "#robotics", "#agents", "#dataset", "#agi", "#long_context"], "emoji": "ð¤", "ru": {"title": "ÐÐ´Ð¸Ð½Ð°Ñ Ð°ÑÑÐ¸ÑÐµÐºÑÑÑÐ° Ð´Ð»Ñ Ð´Ð¾Ð»Ð³Ð¾ÑÑÐ¾ÑÐ½ÑÑ Ð·Ð°Ð´Ð°Ñ Ð²Ð¾Ð¿Ð»Ð¾ÑÐµÐ½Ð½Ð¾Ð³Ð¾ ÐÐ", "desc": "LoHoVLA - ÑÑÐ¾ Ð½Ð¾Ð²Ð°Ñ ÑÐ½Ð¸ÑÐ¸ÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½Ð°Ñ Ð°ÑÑÐ¸ÑÐµÐºÑÑÑÐ° Ð´Ð»Ñ ÑÐµÑÐµÐ½Ð¸Ñ Ð´Ð¾Ð»Ð³Ð¾ÑÑÐ¾ÑÐ½ÑÑ Ð·Ð°Ð´Ð°Ñ Ð²Ð¾Ð¿Ð»Ð¾ÑÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¸ÑÐºÑÑÑ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#synthetic", "#3d", "#games", "#dataset", "#agi", "#multimodal"], "emoji": "ð§", "ru": {"title": "Ð ÐµÐ²Ð¾Ð»ÑÑÐ¸Ñ Ð² 3D: ÑÐ·ÑÐºÐ¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»Ñ, Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÑÑÐ°Ñ ÑÑÐµÑÐ¼ÐµÑÐ½Ð¾Ðµ Ð¿ÑÐ¾ÑÑÑÐ°Ð½ÑÑÐ²Ð¾", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»Ð¸ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð¸Ð»Ð¸ ShapeLLM-Omni - Ð½Ð°ÑÐ¸Ð²Ð½ÑÑ 3D Ð±Ð¾Ð»ÑÑÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»Ñ, ÑÐ¿Ð¾ÑÐ¾Ð±Ð½ÑÑ Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÑÑ Ð¸ Ð³ÐµÐ½ÐµÑÐ¸Ñ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#robotics", "#video", "#diffusion"], "emoji": "ð¤", "ru": {"title": "RoboMaster: Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº Ð¼Ð¾Ð´ÐµÐ»Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ ÑÐ»Ð¾Ð¶Ð½ÑÑ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑÐ²Ð¸Ð¹ Ð² ÑÐ¾Ð±Ð¾ÑÐ¾ÑÐµÑÐ½Ð¸ÐºÐµ", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ RoboMaster Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑÐ²Ð¸Ñ Ð½ÐµÑÐºÐ¾Ð»ÑÐºÐ¸Ñ Ð¾Ð±ÑÐµÐºÑÐ¾Ð² Ð² ÑÐ¾Ð±Ð¾ÑÐ¾ÑÐµÑÐ½Ð¸ÐºÐµ. ÐÐµÑÐ¾Ð´ ÑÐ°Ð·
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#rl", "#benchmark", "#dataset", "#optimization"], "emoji": "ð¤", "ru": {"title": "SRPO: Ð£ÑÐ¸Ð»ÐµÐ½Ð¸Ðµ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ ÐÐ ÑÐµÑÐµÐ· ÑÐ°Ð¼Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ SRPO Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#survey", "#reasoning", "#multimodal"], "emoji": "ð", "ru": {"title": "EarthMind: Ð­ÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ðµ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð¼Ð½Ð¾Ð³Ð¾ÑÐµÐ½ÑÐ¾ÑÐ½ÑÑ Ð´Ð°Ð½Ð½ÑÑ Ð½Ð°Ð±Ð»ÑÐ´ÐµÐ½Ð¸Ñ ÐÐµÐ¼Ð»Ð¸", "desc": "EarthMind - ÑÑÐ¾ Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð´Ð°Ð½Ð½ÑÑ Ð´Ð¸ÑÑÐ°Ð½ÑÐ¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð·Ð¾Ð½Ð´Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ ÐÐµÐ¼Ð»Ð¸, Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÑÑÐ°Ñ Ð¼ÐµÑÐ¾Ð´Ñ Ð¾Ð±ÑÐ°Ð±Ð¾ÑÐº
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#rl"], "emoji": "ð", "ru": {"title": "AReaL: ÐÑÐ¸Ð½ÑÑÐ¾Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑÑÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ ÑÑÐºÐ¾ÑÐµÐ½Ð¸Ñ ÐÐ", "desc": "AReaL - ÑÑÐ¾ Ð°ÑÐ¸Ð½ÑÑÐ¾Ð½Ð½Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. ÐÐ½Ð° ÑÐ°Ð·Ð´ÐµÐ»ÑÐµÑ Ð¿ÑÐ¾ÑÐµÑÑÑ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸ Ð¾Ð±ÑÑÐµÐ½Ð¸
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#dataset", "#training", "#rlhf", "#alignment"], "emoji": "ð­", "ru": {"title": "MiCRo: ÐÐµÑÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð°ÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð±ÐµÐ· Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸ÑÐµÐ»ÑÐ½ÑÑ Ð°Ð½Ð½Ð¾ÑÐ°ÑÐ¸Ð¹", "desc": "MiCRo - ÑÑÐ¾ Ð´Ð²ÑÑÑÑÐ°Ð¿Ð½Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ Ð¿ÐµÑÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð¸ÑÐ¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð¿ÑÐµÐ´Ð¿Ð¾ÑÑÐµÐ½Ð¸ÑÐ¼ Ð² Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÑÑ. ÐÐ½Ð° Ð¸ÑÐ¿
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training"], "emoji": "ð¬", "ru": {"title": "ÐÐ´Ð¸Ð½Ð°Ñ Ð¼ÐµÑÑÐ¸ÐºÐ° Ð´Ð»Ñ Ð¿ÑÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾ÑÑÐ¸ ÑÐ¶Ð°ÑÑÑ Ð½ÐµÐ¹ÑÐ¾ÑÐµÑÐµÐ¹", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÑÐ²ÑÑÐµÐ½Ð¾ Ð²Ð·Ð°Ð¸Ð¼Ð¾ÑÐ²ÑÐ·Ð¸ Ð¼ÐµÐ¶Ð´Ñ Ð·Ð°ÐºÐ¾Ð½Ð°Ð¼Ð¸ Ð¼Ð°ÑÑÑÐ°Ð±Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð¸ Ð¼ÐµÑÐ¾Ð´Ð°Ð¼Ð¸ ÑÐ¶Ð°ÑÐ¸Ñ Ð² Ð¼Ð°ÑÐ¸Ð½Ð½Ð¾Ð¼ Ð¾Ð±ÑÑÐµÐ½Ð¸Ð¸. ÐÐ²ÑÐ¾ÑÑ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÑÑ ÐµÐ´Ð¸Ð½ÑÑ Ð¼ÐµÑ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#training", "#rl", "#benchmark", "#reasoning", "#optimization"], "emoji": "ð§ ", "ru": {"title": "Ð Ð°ÑÑÑÐ¶Ð´Ð°Ð¹ ÑÐ¼Ð½ÐµÐµ, Ð° Ð½Ðµ Ð±Ð¾Ð»ÑÑÐµ: Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð­ÑÐ° ÑÑÐ°ÑÑÑ Ð¿Ð¾ÑÐ²ÑÑÐµÐ½Ð° ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐ¸ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð²ÑÐ¿Ð¾Ð»Ð½ÑÑÑ ÑÐ»Ð¾Ð¶Ð½ÑÐµ Ð¸Ð½ÑÑÑÑÐºÑÐ¸Ð¸ Ñ 
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#training", "#rl", "#benchmark", "#agents", "#open_source", "#reasoning", "#games", "#dataset"], "emoji": "ð¡", "ru": {"title": "ÐÐ¾Ð¾Ð±ÑÑÐµÐ½Ð¸Ðµ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐ»ÑÑÑÐ°ÐµÑ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÑÐµÑÐºÐ¸Ðµ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑÐ²Ð°ÐµÑ, ÑÑÐ¾ Ð¼ÐµÑÐ¾Ð´Ñ Ð´Ð¾Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ, ÑÐ°ÐºÐ¸Ðµ ÐºÐ°Ðº ÐºÐ¾Ð½ÑÑÐ¾Ð»Ð¸ÑÑÐµÐ¼Ð°Ñ ÑÐ¾Ð½ÐºÐ°Ñ Ð½
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#agi", "#benchmark", "#long_context"], "emoji": "ð¤", "ru": {"title": "WebChoreArena: Ð½Ð¾Ð²ÑÐ¹ ÑÑÐ±ÐµÐ¶ Ð² Ð¾ÑÐµÐ½ÐºÐµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑÐµÐ¹ ÐÐ-Ð°Ð³ÐµÐ½ÑÐ¾Ð² Ð´Ð»Ñ Ð²ÐµÐ±-Ð·Ð°Ð´Ð°Ñ", "desc": "WebChoreArena - ÑÑÐ¾ Ð½Ð¾Ð²ÑÐ¹ Ð½Ð°Ð±Ð¾Ñ ÑÐµÑÑÐ¾Ð², ÑÐ¾ÑÑÐ¾ÑÑÐ¸Ð¹ Ð¸Ð· 532 Ð·Ð°Ð´Ð°Ñ, ÐºÐ¾ÑÐ¾ÑÑÐ¹ ÑÐ°ÑÑÐ¸ÑÑÐµÑ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑÐ¸ WebArena 
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#cv", "#video", "#diffusion"], "emoji": "ð¼ï¸", "ru": {"title": "Ð£Ð¼Ð½Ð¾Ðµ ÑÐµÐ´Ð°ÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ñ ÑÐ¾ÑÑÐ°Ð½ÐµÐ½Ð¸ÐµÐ¼ ÑÑÑÑÐºÑÑÑÑ Ð¸ ÑÐµÐºÑÑÑÑ", "desc": "Cora - ÑÑÐ¾ Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° ÑÐµÐ´Ð°ÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹, Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÑÑÐ°Ñ ÐºÐ¾ÑÑÐµÐºÑÐ¸Ñ ÑÑÐ¼Ð° Ñ ÑÑÐµÑÐ¾Ð¼ ÑÐ¾Ð¾ÑÐ²ÐµÑÑÑÐ²Ð¸Ð¹ Ð¸ Ð¸Ð½ÑÐµÑÐ¿Ð¾Ð»Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐµ ÐºÐ°ÑÑÑ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ. ÐÐ½Ð°
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#reasoning", "#optimization", "#dataset"], "emoji": "ð¤", "ru": {"title": "SMR: Ð­ÑÑÐµÐºÑÐ¸Ð²Ð½ÑÐµ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ Ð´Ð»Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ State Machine Reasoning (SMR). S
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#security", "#leakage"], "emoji": "ðµï¸", "ru": {"title": "DyePack: ÐÐ¾Ð²ÑÑÐºÐ° Ð´Ð»Ñ Ð½ÐµÑÐµÑÑÐ½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¼Ð°ÑÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ", "desc": "DyePack - ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº, Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÑÑÐ¸Ð¹ Ð°ÑÐ°ÐºÐ¸ ÑÐ¸Ð¿Ð° backdoor Ð´Ð»Ñ Ð²ÑÑÐ²Ð»ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾ÑÐ¾ÑÑÐµ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°Ð»Ð¸ ÑÐµÑÑÐ¾Ð²ÑÐµ Ð½Ð°Ð±Ð¾ÑÑ Ð´Ð°Ð½Ð½ÑÑ Ð²Ð¾ Ð²ÑÐµ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#data", "#hallucinations", "#benchmark", "#security", "#rlhf", "#alignment"], "emoji": "ðµï¸", "ru": {"title": "ÐÐ±Ð¼Ð°Ð½ Ð´ÐµÑÐµÐºÑÐ¾ÑÐ¾Ð²: ÐºÐ°Ðº ÑÐ¾ÑÑÑÐ·Ð°ÑÐµÐ»ÑÐ½ÑÐµ Ð°ÑÐ°ÐºÐ¸ Ð¿Ð¾Ð´ÑÑÐ²Ð°ÑÑ Ð¾Ð±Ð½Ð°ÑÑÐ¶ÐµÐ½Ð¸Ðµ ÐÐ-ÑÐµÐºÑÑÐ°", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¾Ð¿Ð¸ÑÑÐ²Ð°ÐµÑ Ð¼ÐµÑÐ¾Ð´ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÑÐ¾ÑÑÑÐ·Ð°ÑÐµÐ»ÑÐ½ÑÑ Ð°ÑÐ°Ðº Ð½Ð° Ð´ÐµÑÐµÐºÑÐ¾ÑÑ Ð¼Ð°ÑÐ¸Ð½Ð½Ð¾-ÑÐ³ÐµÐ½ÐµÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½Ð¾
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#synthetic", "#cv", "#multimodal", "#reasoning", "#dataset"], "emoji": "ð§ ", "ru": {"title": "Ð¡Ð¸Ð½ÑÐµÑÐ¸ÑÐµÑÐºÐ¸Ðµ Ð´Ð°Ð½Ð½ÑÐµ Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ Ð»Ð¾Ð³Ð¸ÑÐµÑÐºÐ¾Ð³Ð¾ Ð¼ÑÑÐ»ÐµÐ½Ð¸Ñ ÐÐ", "desc": "VisualSphinx - ÑÑÐ¾ ÐºÑÑÐ¿Ð½Ð¾Ð¼Ð°ÑÑÑÐ°Ð±Ð½ÑÐ¹ ÑÐ¸Ð½ÑÐµÑÐ¸ÑÐµÑÐºÐ¸Ð¹ Ð½Ð°Ð±Ð¾Ñ Ð´Ð°Ð½Ð½ÑÑ Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½Ð¾Ð³Ð¾ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ Ð² Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½Ð¾-ÑÐ·
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#alignment", "#agents"], "emoji": "ðªï¸", "ru": {"title": "STORM: Ð°ÑÐ¸Ð¼Ð¼ÐµÑÑÐ¸ÑÐ½Ð¾Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð½Ð°Ð¼ÐµÑÐµÐ½Ð¸Ð¹ Ð² Ð´Ð¸Ð°Ð»Ð¾Ð³Ð¾Ð²ÑÑ ÑÐ¸ÑÑÐµÐ¼Ð°Ñ", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº STORM Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð°ÑÐ¸Ð¼Ð¼ÐµÑÑÐ¸ÑÐ½Ð¾Ð¹ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ Ð¸Ð½ÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¸ Ð² Ð´Ð¸Ð°Ð»Ð¾Ð³Ð¾Ð²ÑÑ ÑÐ¸ÑÑÐµÐ¼Ð°Ñ. STORM Ð¸Ñ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#training", "#low_resource", "#open_source", "#multilingual", "#data", "#audio", "#dataset"], "emoji": "ð", "ru": {"title": "ÐÑÐºÑÑÑÑÐµ ÑÐµÑÐµÐ²ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð¾ÑÑÐ¸Ð³Ð°ÑÑ ÑÑÐ¾Ð²Ð½Ñ Ð¿ÑÐ¾Ð¼ÑÑÐ»ÐµÐ½Ð½ÑÑ ÑÑÐ°Ð½Ð´Ð°ÑÑÐ¾Ð²", "desc": "ÐÑÐ¾ÐµÐºÑ OWSM ÑÐ»ÑÑÑÐµÐ½ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ Ð¼Ð°ÑÑÑÐ°Ð±Ð½Ð¾Ð³Ð¾ Ð¾ÑÐ¸ÑÐµÐ½Ð½Ð¾Ð³Ð¾ Ð²ÐµÐ±-Ð´Ð°ÑÐ°ÑÐµÑÐ°, ÑÑÐ¾ Ð¿ÑÐ¸Ð²ÐµÐ»Ð¾ Ðº Ñ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#security", "#ethics", "#training", "#inference", "#data", "#dataset"], "emoji": "ðµï¸", "ru": {"title": "Ð¡ÐºÑÑÑÐ°Ñ ÑÐ³ÑÐ¾Ð·Ð°: ÐºÐ°Ðº Ð¿ÑÐµÐ´Ð²Ð·ÑÑÐ¾ÑÑÑ ÑÑÐ¸Ð»Ð¸Ð²Ð°ÐµÑÑÑ Ð¿ÑÐ¸ Ð´Ð¸ÑÑÐ¸Ð»Ð»ÑÑÐ¸Ð¸ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¸ÑÑÐ»ÐµÐ´ÑÐµÑ ÑÑÐ·Ð²Ð¸Ð¼Ð¾ÑÑÑ Ð´Ð¸ÑÑÐ¸Ð»Ð»Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ðº Ð²Ð½ÐµÐ´ÑÐµÐ½Ð¸Ñ Ð¿ÑÐµÐ´Ð²Ð·ÑÑÐ¾Ð³Ð¾ ÐºÐ¾Ð½Ñ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#video", "#games", "#architecture", "#3d"], "emoji": "ð¥", "ru": {"title": "Ð ÐµÐ²Ð¾Ð»ÑÑÐ¸Ñ Ð² 3D-Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ð¸: Ð¸Ð·Ð²Ð»ÐµÑÐµÐ½Ð¸Ðµ Ð³ÐµÐ¾Ð¼ÐµÑÑÐ¸Ð¸ Ð½Ð°Ð¿ÑÑÐ¼ÑÑ Ð¸Ð· Ð²Ð¸Ð´ÐµÐ¾", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»Ð¸ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð¸Ð»Ð¸ Ð½Ð¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»Ñ Video-3D Geometry Large Language Model (VG LLM), ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð¸Ð·Ð²Ð»ÐµÐºÐ°
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#games", "#rl", "#dataset", "#optimization", "#open_source", "#training"], "emoji": "ð§", "ru": {"title": "CodeV-R1: ÐÑÐ¾ÑÑÐ² Ð² Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸Ð·Ð°ÑÐ¸Ð¸ Ð¿ÑÐ¾ÐµÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ ÑÐ»ÐµÐºÑÑÐ¾Ð½Ð¸ÐºÐ¸", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð° Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° CodeV-R1 Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ ÐºÐ¾Ð´Ð° Ð½Ð° ÑÐ·ÑÐºÐµ Verilog Ñ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ LLM Ð¸ Ð¼Ðµ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization"], "emoji": "â¡", "ru": {"title": "Ð£ÑÐºÐ¾ÑÐµÐ½Ð¸Ðµ LLM Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÑÐµÑÐºÐ¾Ð³Ð¾ ÑÐ¶Ð°ÑÐ¸Ñ ÑÐ¾ÐºÐµÐ½Ð¾Ð²", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð° Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° zip2zip, ÐºÐ¾ÑÐ¾ÑÐ°Ñ ÑÐ»ÑÑÑÐ°ÐµÑ ÑÐ°Ð±Ð¾ÑÑ LLM, Ð´Ð¸Ð½Ð°Ð¼Ð¸ÑÐµÑÐºÐ¸ Ð¸Ð·Ð¼ÐµÐ½ÑÑ ÑÐ»Ð¾Ð²Ð°ÑÑ ÑÐ¾ÐºÐµÐ½Ð¾Ð² Ð²Ð¾ Ð²ÑÐµÐ¼Ñ Ð²ÑÐ²Ð¾Ð´Ð°. 
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#games", "#3d"], "emoji": "ð¨", "ru": {"title": "ÐÑÐ¾Ð³ÑÐµÑÑÐ¸Ð²Ð½Ð¾Ðµ 3D-ÑÐµÐ´Ð°ÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ: Ð¾Ñ ÑÐµÐºÑÑÐ° Ðº ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½ÑÐ¼ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸ÑÐ¼", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº ÑÐµÐ´Ð°ÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ 3D-Ð¾Ð±ÑÐµÐºÑÐ¾Ð² Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ ÑÐµÐºÑÑÐ¾Ð²ÑÑ Ð¸Ð½ÑÑÑÑÐºÑÐ¸Ð¹. ÐÐ²ÑÐ¾ÑÑ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÑÑ Ð¿Ð°ÑÐ°Ð´Ð¸Ð³Ð¼Ñ Ð¿ÑÐ¾Ð³ÑÐµÑÑÐ¸Ð²Ð½ÑÑ Ð²Ð¸Ð´Ð¾Ð², ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð¾Ð±Ðµ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#training", "#optimization"], "emoji": "ð", "ru": {"title": "Ð£Ð¼Ð½ÑÐ¹ Ð³ÑÐ°ÑÐ¸Ðº Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ: Ð¼Ð°ÐºÑÐ¸Ð¼ÑÐ¼ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾ÑÑÐ¸ Ð¿ÑÐ¸ Ð¾Ð³ÑÐ°Ð½Ð¸ÑÐµÐ½Ð½ÑÑ ÑÐµÑÑÑÑÐ°Ñ", "desc": "ÐÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ ÑÐ½Ð¸ÑÐ¸ÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐ¹ Ð³ÑÐ°ÑÐ¸Ðº ÑÐºÐ¾ÑÐ¾ÑÑÐ¸ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ ÑÑÐµÑÐ¾Ð¼ Ð±ÑÐ´Ð¶ÐµÑÐ° Ð´Ð»Ñ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ð¸ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð² ÑÑÐ»Ð¾Ð²Ð¸ÑÑ Ð¾Ð³ÑÐ°Ð½Ð¸ÑÐµÐ½Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð»Ð¸ÑÐµÑÑÐ²Ð° Ð¸ÑÐµÑÐ°ÑÐ¸Ð¹. 
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#rl", "#interpretability", "#multimodal", "#reasoning", "#video", "#benchmark"], "emoji": "ð¥", "ru": {"title": "Ð£Ð¼Ð½Ð¾Ðµ Ð²Ð¸Ð´ÐµÐ¾Ð½Ð°Ð±Ð»ÑÐ´ÐµÐ½Ð¸Ðµ: ÐÐ ÑÑÐ¸ÑÑÑ Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÑÑ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¸", "desc": "VAU-R1 - ÑÑÐ¾ Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð´Ð»Ñ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð°Ð½Ð¾Ð¼Ð°Ð»Ð¸Ð¹ Ð² Ð²Ð¸Ð´ÐµÐ¾, Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÑÑÐ°Ñ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÐµ Ð±Ð¾Ð»ÑÑÐ¸Ðµ ÑÐ·ÑÐºÐ¾Ð²ÑÐµ Ð¼Ð¾Ð´Ðµ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#diffusion", "#inference", "#cv", "#optimization", "#video"], "emoji": "ð§ ", "ru": {"title": "NAG: ÑÐ½Ð¸Ð²ÐµÑÑÐ°Ð»ÑÐ½Ð¾Ðµ Ð½ÐµÐ³Ð°ÑÐ¸Ð²Ð½Ð¾Ðµ ÑÑÐºÐ¾Ð²Ð¾Ð´ÑÑÐ²Ð¾ Ð´Ð»Ñ Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ Normalized Attention Guidance (NAG) Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ ÑÐ°Ð±Ð¾ÑÑ Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#ethics", "#benchmark", "#dataset", "#data", "#open_source"], "emoji": "ð§¼", "ru": {"title": "ÐÐ Ð¾ÑÐ¸ÑÐ°ÐµÑ Ð¸Ð½ÑÐµÑÐ½ÐµÑ Ð¾Ñ ÑÐ·ÑÐºÐ° Ð½ÐµÐ½Ð°Ð²Ð¸ÑÑÐ¸", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº Ð´ÐµÑÐ¾ÐºÑÐ¸ÑÐ¸ÐºÐ°ÑÐ¸Ð¸ ÑÐ·ÑÐºÐ° Ð½ÐµÐ½Ð°Ð²Ð¸ÑÑÐ¸ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ GPT-4o-mini. ÐÐ²ÑÐ¾ÑÑ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ ÐºÑÑÐ¿Ð½Ð¾Ð¼Ð°ÑÑÑÐ°Ð±Ð½ÑÐ¹ Ð´Ð°ÑÐ°ÑÐµÑ PARA
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#training", "#benchmark", "#open_source", "#interpretability", "#data", "#reasoning"], "emoji": "ð§ ", "ru": {"title": "ÐÑÐµÐ¾Ð´Ð¾Ð»ÐµÐ½Ð¸Ðµ Ð¾Ð³ÑÐ°Ð½Ð¸ÑÐµÐ½Ð¸Ð¹ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Ð·Ð°Ð´Ð°ÑÐ°Ñ Ñ Ð¼Ð½Ð¾Ð¶ÐµÑÑÐ²ÐµÐ½Ð½ÑÐ¼ Ð²ÑÐ±Ð¾ÑÐ¾Ð¼", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ SATA-BENCH - Ð¿ÐµÑÐ²ÑÐ¹ ÑÐ¿ÐµÑÐ¸Ð°Ð»Ð¸Ð·Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐ¹ Ð±ÐµÐ½ÑÐ¼Ð°ÑÐº Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ 
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#diffusion", "#interpretability", "#cv", "#benchmark"], "emoji": "ð¼ï¸", "ru": {"title": "Ð£Ð»ÑÑÑÐµÐ½Ð¸Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð·Ð¸ÑÐ¸Ð¸ Ð² Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ 2.5D ÑÐµÐ¼Ð°Ð½ÑÐ¸ÑÐµÑÐºÐ¸Ñ Ð¼Ð°ÐºÐµÑÐ¾Ð²", "desc": "ComposeAnything - ÑÑÐ¾ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ Ñ
[03.06.2025 19:10] Querying the API.
[03.06.2025 19:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The Darwin G\"odel Machine improves its coding capabilities through iterative self-modification and open-ended exploration, surpassing other approaches in benchmarks.  					AI-generated summary 				 Today's AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The G\"odel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. We introduce the Darwin G\"odel Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation.
[03.06.2025 19:10] Response: {
  "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Darwin GÃ¶del Machine (DGM) - ÑÐ¸ÑÑÐµÐ¼Ñ Ð¸ÑÐºÑÑÑÑÐ²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¸Ð½ÑÐµÐ»Ð»ÐµÐºÑÐ°, ÑÐ¿Ð¾ÑÐ¾Ð±Ð½ÑÑ Ðº ÑÐ°Ð¼Ð¾ÑÐ¾Ð²ÐµÑÑÐµÐ½ÑÑÐ²Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿ÑÑÐµÐ¼ Ð¸ÑÐµÑÐ°ÑÐ¸Ð²Ð½Ð¾Ð¹ Ð¼Ð¾Ð´Ð¸ÑÐ¸ÐºÐ°ÑÐ¸Ð¸ ÑÐ¾Ð±ÑÑÐ²ÐµÐ½Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð´Ð°. DGM Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÐµÑ Ð¿ÑÐ¸Ð½ÑÐ¸Ð¿Ñ Ð´Ð°ÑÐ²Ð¸Ð½Ð¾Ð²ÑÐºÐ¾Ð¹ ÑÐ²Ð¾Ð»ÑÑÐ¸Ð¸ Ð¸ Ð¾ÑÐºÑÑÑÐ¾Ð³Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ, Ð¿Ð¾Ð´Ð´ÐµÑÐ¶Ð¸Ð²Ð°Ñ Ð°ÑÑÐ¸Ð² ÐºÐ¾Ð´Ð¸ÑÑÑÑÐ¸Ñ Ð°Ð³ÐµÐ½ÑÐ¾Ð² Ð¸ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ Ð¸Ñ Ð½Ð¾Ð²ÑÐµ Ð²ÐµÑÑÐ¸Ð¸ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ Ð±Ð°Ð·Ð¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸. Ð¡Ð¸ÑÑÐµÐ¼Ð° Ð·Ð½Ð°ÑÐ¸ÑÐµÐ»ÑÐ½Ð¾ ÑÐ»ÑÑÑÐ¸Ð»Ð° ÑÐ²Ð¾Ð¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐ¸ Ðº ÐºÐ¾Ð´Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ, Ð¿Ð¾Ð²ÑÑÐ¸Ð² Ð¿ÑÐ¾Ð¸Ð·Ð²Ð¾Ð´Ð¸ÑÐµÐ»ÑÐ½Ð¾ÑÑÑ Ð½Ð° Ð±ÐµÐ½ÑÐ¼Ð°ÑÐºÐ°Ñ SWE-bench Ð¸ Polyglot. DGM Ð¿ÑÐµÐ²Ð·Ð¾ÑÐ»Ð° Ð±Ð°Ð·Ð¾Ð²ÑÐµ Ð¿Ð¾Ð´ÑÐ¾Ð´Ñ Ð±ÐµÐ· ÑÐ°Ð¼Ð¾ÑÐ¾Ð²ÐµÑÑÐµÐ½ÑÑÐ²Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ Ð¾ÑÐºÑÑÑÐ¾Ð³Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ, Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÑ Ð²Ð°Ð¶Ð½ÑÐ¹ ÑÐ°Ð³ Ðº ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÑÐ°Ð¼Ð¾ÑÐ¾Ð²ÐµÑÑÐµÐ½ÑÑÐ²ÑÑÑÐµÐ³Ð¾ÑÑ ÐÐ.",
  "emoji": "ð§¬",
  "title": "Ð­Ð²Ð¾Ð»ÑÑÐ¸Ð¾Ð½Ð¸ÑÑÑÑÐ¸Ð¹ ÐÐ: ÑÐ°Ð¼Ð¾ÑÐ¾Ð²ÐµÑÑÐµÐ½ÑÑÐ²Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÐµÑÐµÐ· Ð¸ÑÐµÑÐ°ÑÐ¸Ð²Ð½ÑÑ Ð¼Ð¾Ð´Ð¸ÑÐ¸ÐºÐ°ÑÐ¸Ñ ÐºÐ¾Ð´Ð°"
}
[03.06.2025 19:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The Darwin G\"odel Machine improves its coding capabilities through iterative self-modification and open-ended exploration, surpassing other approaches in benchmarks.  					AI-generated summary 				 Today's AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The G\"odel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. We introduce the Darwin G\"odel Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation."

[03.06.2025 19:10] Response: ```python
['BENCHMARK', 'AGENTS', 'ARCHITECTURE', 'TRAINING']
```
[03.06.2025 19:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The Darwin G\"odel Machine improves its coding capabilities through iterative self-modification and open-ended exploration, surpassing other approaches in benchmarks.  					AI-generated summary 				 Today's AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The G\"odel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. We introduce the Darwin G\"odel Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation."

[03.06.2025 19:10] Response: ```python
["AGI", "OPTIMIZATION", "OPEN_SOURCE"]
```
[03.06.2025 19:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The Darwin G\\"odel Machine (DGM) is a self-improving AI system that enhances its coding abilities through iterative self-modification and open-ended exploration. Unlike traditional AI, which relies on fixed architectures, the DGM autonomously evolves by modifying its own code and validating these changes against coding benchmarks. It employs a Darwinian approach, maintaining an archive of coding agents and generating new versions to explore diverse solutions. This method has shown significant performance improvements in coding tasks, demonstrating the potential for continuous and safe AI advancement.","title":"Evolving AI: The Future of Self-Improvement"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The Darwin G"odel Machine (DGM) is a self-improving AI system that enhances its coding abilities through iterative self-modification and open-ended exploration. Unlike traditional AI, which relies on fixed architectures, the DGM autonomously evolves by modifying its own code and validating these changes against coding benchmarks. It employs a Darwinian approach, maintaining an archive of coding agents and generating new versions to explore diverse solutions. This method has shown significant performance improvements in coding tasks, demonstrating the potential for continuous and safe AI advancement.', title='Evolving AI: The Future of Self-Improvement'))
[03.06.2025 19:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"è¾¾å°æå¥å¾·å°æºå¨ï¼DGMï¼éè¿è¿­ä»£èªæä¿®æ¹åå¼æ¾å¼æ¢ç´¢æ¥æé«å¶ç¼ç è½åï¼è¶è¶äºå¶ä»æ¹æ³çåºåæµè¯ãä¸ä¼ ç»çåºå®æ¶æAIç³»ç»ä¸åï¼DGMè½å¤èªä¸»ä¸æç»­å°æ¹è¿èªèº«ãå®åé´äºè¾¾å°æè¿åççå¿µï¼ç»´æ¤ä¸ä¸ªçæç¼ç ä»£ççæ¡£æ¡åºï¼å¹¶éè¿éæ ·ååºç¡æ¨¡åçææ°çæ¬ï¼å½¢æå¤æ ·åçé«è´¨éä»£çæ ãå®éªç»æè¡¨æï¼DGMå¨ç¼ç è½åä¸æ¾èæåï¼è¡¨ç°åºæ´å¥½çä»£ç ç¼è¾å·¥å·ååè¡è¯å®¡æºå¶ï¼æ å¿çèªææ¹è¿AIçéè¦è¿å±ã","title":"èªææ¹è¿çAIï¼è¾¾å°æå¥å¾·å°æºå¨çåæ°ä¹è·¯"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='è¾¾å°æå¥å¾·å°æºå¨ï¼DGMï¼éè¿è¿­ä»£èªæä¿®æ¹åå¼æ¾å¼æ¢ç´¢æ¥æé«å¶ç¼ç è½åï¼è¶è¶äºå¶ä»æ¹æ³çåºåæµè¯ãä¸ä¼ ç»çåºå®æ¶æAIç³»ç»ä¸åï¼DGMè½å¤èªä¸»ä¸æç»­å°æ¹è¿èªèº«ãå®åé´äºè¾¾å°æè¿åççå¿µï¼ç»´æ¤ä¸ä¸ªçæç¼ç ä»£ççæ¡£æ¡åºï¼å¹¶éè¿éæ ·ååºç¡æ¨¡åçææ°çæ¬ï¼å½¢æå¤æ ·åçé«è´¨éä»£çæ ãå®éªç»æè¡¨æï¼DGMå¨ç¼ç è½åä¸æ¾èæåï¼è¡¨ç°åºæ´å¥½çä»£ç ç¼è¾å·¥å·ååè¡è¯å®¡æºå¶ï¼æ å¿çèªææ¹è¿AIçéè¦è¿å±ã', title='èªææ¹è¿çAIï¼è¾¾å°æå¥å¾·å°æºå¨çåæ°ä¹è·¯'))
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#inference", "#open_source", "#benchmark", "#architecture", "#optimization", "#diffusion"], "emoji": "ð", "ru": {"title": "Eso-LMs: Ð ÐµÐ²Ð¾Ð»ÑÑÐ¸Ñ Ð² ÑÐ·ÑÐºÐ¾Ð²Ð¾Ð¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ð¸ Ñ KV-ÐºÑÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸ÐµÐ¼", "desc": "Eso-LMs Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÑÑ ÑÐ¾Ð±Ð¾Ð¹ Ð½Ð¾Ð²Ð¾Ðµ ÑÐµÐ¼ÐµÐ¹ÑÑÐ²Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, Ð¾Ð±ÑÐµÐ´Ð¸Ð½ÑÑÑÐ¸Ñ Ð°Ð²ÑÐ¾ÑÐµÐ³ÑÐµÑÑÐ¸Ð¾Ð½Ð½ÑÐµ Ð¸ Ð¼Ð°
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#machine_translation", "#benchmark", "#dataset", "#low_resource", "#multilingual"], "emoji": "ð¦ðª", "ru": {"title": "ÐÑÐ»ÑÑÑÑÐ½Ð¾-Ð¾ÑÐ¸ÐµÐ½ÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½Ð°Ñ Ð¾ÑÐµÐ½ÐºÐ° Ð°ÑÐ°Ð±ÑÐºÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "ÐÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½ Ð½Ð¾Ð²ÑÐ¹ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð¾ÑÐµÐ½ÐºÐ¸ Ð¸ Ð½Ð°Ð±Ð¾Ñ Ð´Ð°Ð½Ð½ÑÑ ADMD Ð´Ð»Ñ ÑÐµÑÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð°ÑÐ°Ð±ÑÐºÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#machine_translation", "#benchmark", "#dataset", "#low_resource", "#multilingual"], "emoji": "ð¦ðª", "ru": {"title": "ÐÑÐ»ÑÑÑÑÐ½Ð¾-Ð¾ÑÐ¸ÐµÐ½ÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½Ð°Ñ Ð¾ÑÐµÐ½ÐºÐ° Ð°ÑÐ°Ð±ÑÐºÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "ÐÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½ Ð½Ð¾Ð²ÑÐ¹ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð¾ÑÐµÐ½ÐºÐ¸ Ð¸ Ð½Ð°Ð±Ð¾Ñ Ð´Ð°Ð½Ð½ÑÑ ADMD Ð´Ð»Ñ ÑÐµÑÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð°ÑÐ°Ð±ÑÐºÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#cv", "#dataset", "#optimization", "#audio", "#games"], "emoji": "ð¤", "ru": {"title": "ÐÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÐ¹ ÐÐ Ð´Ð»Ñ ÐµÑÑÐµÑÑÐ²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ OmniResponse - Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ Ð±Ð¾Ð»ÑÑÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»Ñ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ ÑÐ¸Ð½ÑÑÐ¾Ð½Ð¸Ð·Ð¸ÑÐ¾Ð²Ð°Ð½
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#benchmark", "#data", "#hallucinations", "#multimodal", "#ethics", "#alignment"], "emoji": "ð§ ", "ru": {"title": "ÐÑÑÐ²Ð»ÐµÐ½Ð¸Ðµ ÑÐºÑÑÑÑÑ Ð¿ÑÐµÐ´ÑÐ±ÐµÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð² ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÑÑ", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð±ÐµÐ½ÑÐ¼Ð°ÑÐº Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ Preference, Opinion, and Belief survey (POBs) Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ ÑÑ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#transfer_learning", "#plp", "#machine_translation", "#multilingual"], "emoji": "ð§ ", "ru": {"title": "Ð¯Ð·ÑÐºÐ¸ Ð¿ÑÐ¾Ð³ÑÐ°Ð¼Ð¼Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð² Ð¼Ð¾Ð·Ð³Ðµ Ð½ÐµÐ¹ÑÐ¾ÑÐµÑÐ¸: Ð±Ð»Ð¸Ð¶Ðµ Ðº Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¾Ð¼Ñ, ÑÐµÐ¼ ÐºÐ°Ð¶ÐµÑÑÑ", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑÐ²Ð°ÐµÑ, ÑÑÐ¾ Ð² ÐºÐ¾Ð½ÑÐµÐ¿ÑÑÐ°Ð»ÑÐ½Ð¾Ð¼ Ð¿ÑÐ¾ÑÑÑÐ°Ð½ÑÑÐ²Ðµ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð¿ÑÐµÐ´Ñ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#multimodal", "#open_source", "#alignment"], "emoji": "ð§ ", "ru": {"title": "ÐÐµÑÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð°ÑÐ¸Ñ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½Ð¾-ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑÐ°Ð»ÑÐ½Ð¾Ð³Ð¾ Ð¿Ð¾Ð·Ð½Ð°Ð½Ð¸Ñ", "desc": "PCogAlign - ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð²Ð¾Ð·Ð½Ð°Ð³ÑÐ°Ð¶Ð´ÐµÐ½Ð¸Ñ, Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÑÑÐµÐ¹ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²ÑÐ²Ð°ÑÑ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½Ð¾-Ñ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#graphs", "#rag", "#security"], "emoji": "ð§ ", "ru": {"title": "RARE: Ð¡ÑÑÐµÑÑ-ÑÐµÑÑ Ð´Ð»Ñ RAG-ÑÐ¸ÑÑÐµÐ¼ Ð² ÑÐµÐ°Ð»ÑÐ½Ð¾Ð¼ Ð¼Ð¸ÑÐµ", "desc": "RARE - ÑÑÐ¾ Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð¾ÑÐµÐ½ÐºÐ¸ ÑÑÑÐ¾Ð¹ÑÐ¸Ð²Ð¾ÑÑÐ¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ñ Ð´Ð¾Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸ÐµÐ¼ Ð¸Ð· Ð²Ð½ÐµÑÐ½Ð¸Ñ Ð¸ÑÑÐ¾ÑÐ½Ð¸ÐºÐ¾Ð² (RAG) Ðº ÑÐµÐ°Ð»ÑÐ½ÑÐ¼ ÑÑÐ¼Ð°Ð¼ Ð¸ ÐºÐ¾Ð½ÑÐ»Ð¸ÐºÑÐ°Ð¼
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#low_resource", "#training"], "emoji": "ð§ ", "ru": {"title": "Ð­ÑÑÐµÐºÑÐ¸Ð²Ð½Ð°Ñ ÑÐ¾ÑÐ½Ð°Ñ Ð½Ð°ÑÑÑÐ¾Ð¹ÐºÐ° Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ ÑÐ°Ð·ÑÐµÐ¶ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð²ÐµÑÐ¾Ð²", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ ÑÐ¾ÑÐ½Ð¾Ð¹ Ð½Ð°ÑÑÑÐ¾Ð¹ÐºÐ¸ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#data", "#leakage", "#benchmark", "#evaluation"], "emoji": "â ï¸", "ru": {"title": "ÐÑÑÐ¾ÑÐ¾Ð¶Ð½Ð¾ Ñ Ð²ÑÐ²Ð¾Ð´Ð°Ð¼Ð¸: ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑÐ¸ Ð¾ÑÐµÐ½ÐºÐ¸ LLM Ð² Ð¿ÑÐ¾Ð³Ð½Ð¾Ð·Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ð¸", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¾Ð±ÑÑÐ¶Ð´Ð°ÐµÑ Ð¿ÑÐ¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ñ Ð¿ÑÐ¾Ð³Ð½Ð¾Ð·Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ. ÐÐ²ÑÐ¾ÑÑ Ð¿ÑÐµÐ´ÑÐ¿ÑÐµÐ¶Ð´Ð°ÑÑ Ð¾ ÑÑÑÐ´Ð½Ð¾ÑÑÑÑ Ð² Ð¾ÑÐµÐ½ÐºÐµ 
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#benchmark", "#dataset", "#reasoning", "#survey"], "emoji": "ðï¸", "ru": {"title": "CityLens: ÐÐ·Ð³Ð»ÑÐ´ Ð½Ð° Ð³Ð¾ÑÐ¾Ð´ Ð³Ð»Ð°Ð·Ð°Ð¼Ð¸ Ð¸ÑÐºÑÑÑÑÐ²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¸Ð½ÑÐµÐ»Ð»ÐµÐºÑÐ°", "desc": "CityLens - ÑÑÐ¾ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½ÑÐ¹ Ð±ÐµÐ½ÑÐ¼Ð°ÑÐº Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐµÐ¹ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLVM
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#audio", "#optimization", "#diffusion", "#open_source", "#multimodal"], "emoji": "ðµ", "ru": {"title": "MagiCodec: Ð¡ÐµÐ¼Ð°Ð½ÑÐ¸ÑÐµÑÐºÐ°Ñ ÑÐ¾ÐºÐµÐ½Ð¸Ð·Ð°ÑÐ¸Ñ Ð°ÑÐ´Ð¸Ð¾ Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸", "desc": "MagiCodec - ÑÑÐ¾ Ð½Ð¾Ð²ÑÐ¹ Ð°ÑÐ´Ð¸Ð¾ ÐºÐ¾Ð´ÐµÐº Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼ÐµÑÐ°, ÑÐ°Ð·ÑÐ°Ð±Ð¾ÑÐ°Ð½Ð½ÑÐ¹ Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ ÑÐµÐ¼Ð°Ð½ÑÐ¸ÑÐµÑÐºÐ¾Ð¹ Ñ
[03.06.2025 19:10] Querying the API.
[03.06.2025 19:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SealQA evaluates search-augmented language models' performance on fact-seeking questions with conflicting or noisy search results, revealing limitations in reasoning and factual accuracy.  					AI-generated summary 				 We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the "lost-in-the-middle" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa.
[03.06.2025 19:10] Response: {
  "desc": "SealQA - ÑÑÐ¾ Ð½Ð¾Ð²ÑÐ¹ ÑÑÐ°Ð»Ð¾Ð½Ð½ÑÐ¹ ÑÐµÑÑ Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐ·ÑÐºÐ° Ñ Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²ÑÐ¼ Ð´Ð¾Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸ÐµÐ¼ Ð½Ð° ÑÐ°ÐºÑÐ¾Ð»Ð¾Ð³Ð¸ÑÐµÑÐºÐ¸Ñ Ð²Ð¾Ð¿ÑÐ¾ÑÐ°Ñ Ñ Ð¿ÑÐ¾ÑÐ¸Ð²Ð¾ÑÐµÑÐ¸Ð²ÑÐ¼Ð¸ Ð¸Ð»Ð¸ Ð·Ð°ÑÑÐ¼Ð»ÐµÐ½Ð½ÑÐ¼Ð¸ ÑÐµÐ·ÑÐ»ÑÑÐ°ÑÐ°Ð¼Ð¸ Ð¿Ð¾Ð¸ÑÐºÐ°. Ð¢ÐµÑÑ Ð²ÐºÐ»ÑÑÐ°ÐµÑ ÑÑÐ¸ Ð²Ð°ÑÐ¸Ð°Ð½ÑÐ°: Seal-0, Seal-Hard Ð¸ LongSeal, Ð¾ÑÐµÐ½Ð¸Ð²Ð°ÑÑÐ¸Ðµ ÑÐ°ÐºÑÐ¸ÑÐµÑÐºÑÑ ÑÐ¾ÑÐ½Ð¾ÑÑÑ Ð¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐ¸ Ðº ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ. ÐÑÐµÐ½ÐºÐ° Ð²ÑÑÐ²Ð¸Ð»Ð° ÐºÑÐ¸ÑÐ¸ÑÐµÑÐºÐ¸Ðµ Ð¾Ð³ÑÐ°Ð½Ð¸ÑÐµÐ½Ð¸Ñ ÑÐ¾Ð²ÑÐµÐ¼ÐµÐ½Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, Ð´Ð°Ð¶Ðµ Ð¿ÐµÑÐµÐ´Ð¾Ð²ÑÐµ LLM Ð¿Ð¾ÐºÐ°Ð·ÑÐ²Ð°ÑÑ Ð½Ð¸Ð·ÐºÑÑ Ð¿ÑÐ¾Ð¸Ð·Ð²Ð¾Ð´Ð¸ÑÐµÐ»ÑÐ½Ð¾ÑÑÑ Ð²Ð¾ Ð²ÑÐµÑ Ð²Ð°ÑÐ¸Ð°Ð½ÑÐ°Ñ SealQA. ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÐ°ÐºÐ¶Ðµ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð¾, ÑÑÐ¾ ÑÐ²ÐµÐ»Ð¸ÑÐµÐ½Ð¸Ðµ Ð²ÑÑÐ¸ÑÐ»Ð¸ÑÐµÐ»ÑÐ½Ð¾Ð¹ Ð¼Ð¾ÑÐ½Ð¾ÑÑÐ¸ Ð½Ðµ Ð¿ÑÐ¸Ð²Ð¾Ð´Ð¸Ñ Ðº Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾Ð¼Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ ÑÐµÐ·ÑÐ»ÑÑÐ°ÑÐ¾Ð².",
  "emoji": "ð",
  "title": "SealQA: ÐÑÑÐ²Ð»ÐµÐ½Ð¸Ðµ Ð¾Ð³ÑÐ°Ð½Ð¸ÑÐµÐ½Ð¸Ð¹ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² ÑÐ°Ð±Ð¾ÑÐµ Ñ Ð¿ÑÐ¾ÑÐ¸Ð²Ð¾ÑÐµÑÐ¸Ð²Ð¾Ð¹ Ð¸Ð½ÑÐ¾ÑÐ¼Ð°ÑÐ¸ÐµÐ¹"
}
[03.06.2025 19:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SealQA evaluates search-augmented language models' performance on fact-seeking questions with conflicting or noisy search results, revealing limitations in reasoning and factual accuracy.  					AI-generated summary 				 We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the "lost-in-the-middle" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa."

[03.06.2025 19:10] Response: ```python
['BENCHMARK', 'DATASET']
```
[03.06.2025 19:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SealQA evaluates search-augmented language models' performance on fact-seeking questions with conflicting or noisy search results, revealing limitations in reasoning and factual accuracy.  					AI-generated summary 				 We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the "lost-in-the-middle" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa."

[03.06.2025 19:10] Response: ```python
["REASONING", "LONG_CONTEXT"]
```
[03.06.2025 19:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SealQA is a benchmark designed to evaluate the performance of search-augmented language models on fact-seeking questions, especially when search results are conflicting or noisy. It consists of three versions: Seal-0, which focuses on challenging questions; Seal-Hard, which tests reasoning and factual accuracy; and LongSeal, which assesses long-context reasoning in complex scenarios. The evaluation shows that even advanced language models struggle significantly, with low accuracy rates on Seal-0 and vulnerability to noisy search results. Furthermore, increasing computational resources does not consistently improve performance, highlighting the need for better models in handling complex information retrieval tasks.","title":"SealQA: Unveiling the Limits of Search-Augmented Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SealQA is a benchmark designed to evaluate the performance of search-augmented language models on fact-seeking questions, especially when search results are conflicting or noisy. It consists of three versions: Seal-0, which focuses on challenging questions; Seal-Hard, which tests reasoning and factual accuracy; and LongSeal, which assesses long-context reasoning in complex scenarios. The evaluation shows that even advanced language models struggle significantly, with low accuracy rates on Seal-0 and vulnerability to noisy search results. Furthermore, increasing computational resources does not consistently improve performance, highlighting the need for better models in handling complex information retrieval tasks.', title='SealQA: Unveiling the Limits of Search-Augmented Language Models'))
[03.06.2025 19:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SealQAæ¯ä¸ä¸ªæ°çåºåï¼ç¨äºè¯ä¼°å¢å¼ºæç´¢çè¯­è¨æ¨¡åå¨é¢å¯¹å²çªæåªå£°æç´¢ç»ææ¶çè¡¨ç°ï¼ç¹å«æ¯å¨äºå®å¯»æ±é®é¢ä¸ãè¯¥åºååä¸ºä¸ç§ç±»åï¼Seal-0åSeal-Hardä¸»è¦è¯ä¼°æ¨¡åçäºå®åç¡®æ§åæ¨çè½åï¼èLongSealåæµè¯é¿ä¸ä¸æåå¤ææ¡£æ¨çè½åãè¯ä¼°ç»ææ¾ç¤ºï¼å½åçåæ²¿æ¨¡åå¨ææSealQAç±»åä¸­è¡¨ç°ä¸ä½³ï¼å°¤å¶æ¯å¨Seal-0ä¸­ï¼åç¡®çæä½ãå°½ç®¡ä¸äºåè¿çæ¨çæ¨¡åå­å¨ï¼ä½å®ä»¬å¨é¢å¯¹åªå£°æç´¢ç»ææ¶ä»ç¶éå¸¸èå¼±ï¼ä¸å¢å è®¡ç®èµæºå¹¶æªæ¾èæé«æ§è½ã","title":"SealQAï¼è¯ä¼°è¯­è¨æ¨¡åå¨å¤ææç´¢ä¸­çè¡¨ç°"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SealQAæ¯ä¸ä¸ªæ°çåºåï¼ç¨äºè¯ä¼°å¢å¼ºæç´¢çè¯­è¨æ¨¡åå¨é¢å¯¹å²çªæåªå£°æç´¢ç»ææ¶çè¡¨ç°ï¼ç¹å«æ¯å¨äºå®å¯»æ±é®é¢ä¸ãè¯¥åºååä¸ºä¸ç§ç±»åï¼Seal-0åSeal-Hardä¸»è¦è¯ä¼°æ¨¡åçäºå®åç¡®æ§åæ¨çè½åï¼èLongSealåæµè¯é¿ä¸ä¸æåå¤ææ¡£æ¨çè½åãè¯ä¼°ç»ææ¾ç¤ºï¼å½åçåæ²¿æ¨¡åå¨ææSealQAç±»åä¸­è¡¨ç°ä¸ä½³ï¼å°¤å¶æ¯å¨Seal-0ä¸­ï¼åç¡®çæä½ãå°½ç®¡ä¸äºåè¿çæ¨çæ¨¡åå­å¨ï¼ä½å®ä»¬å¨é¢å¯¹åªå£°æç´¢ç»ææ¶ä»ç¶éå¸¸èå¼±ï¼ä¸å¢å è®¡ç®èµæºå¹¶æªæ¾èæé«æ§è½ã', title='SealQAï¼è¯ä¼°è¯­è¨æ¨¡åå¨å¤ææç´¢ä¸­çè¡¨ç°'))
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#cv", "#dataset", "#diffusion", "#interpretability", "#multimodal", "#synthetic", "#benchmark", "#architecture"], "emoji": "ðµï¸", "ru": {"title": "ÐÐ´Ð¸Ð½Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð´Ð»Ñ Ð¿ÑÐ¾Ð·ÑÐ°ÑÐ½Ð¾Ð³Ð¾ Ð¾Ð±Ð½Ð°ÑÑÐ¶ÐµÐ½Ð¸Ñ Ð¸ÑÐºÑÑÑÑÐ²ÐµÐ½Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½ÑÐµÐ½ÑÐ°", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð½Ð°Ð±Ð¾Ñ Ð´Ð°Ð½Ð½ÑÑ IVY-FAKE Ð¸ Ð°ÑÑÐ¸ÑÐµÐºÑÑ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#multilingual", "#transfer_learning", "#open_source", "#machine_translation", "#training", "#low_resource"], "emoji": "ð", "ru": {"title": "ÐÐ²ÑÑÐ·ÑÑÐ½ÑÐµ Ð´Ð°Ð½Ð½ÑÐµ ÑÐ»ÑÑÑÐ°ÑÑ Ð¼Ð½Ð¾Ð³Ð¾ÑÐ·ÑÑÐ½ÑÑ Ð°Ð´Ð°Ð¿ÑÐ°ÑÐ¸Ñ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð­ÑÐ¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÑÐ²ÑÑÐµÐ½Ð¾ Ð²Ð»Ð¸Ñ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#data", "#dataset", "#transfer_learning", "#multimodal", "#healthcare", "#science"], "emoji": "ð§ ", "ru": {"title": "ÐÑ Ð¼Ð¾Ð·Ð³Ð¾Ð²ÑÑ Ð²Ð¾Ð»Ð½ Ðº ÑÐ»Ð¾Ð²Ð°Ð¼: Ð½Ð¾Ð²ÑÐ¹ ÑÑÐ¾Ð²ÐµÐ½Ñ Ð½ÐµÐ¹ÑÐ¾Ð´ÐµÐºÐ¾Ð´Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ", "desc": "Neuro2Semantic - ÑÑÐ¾ Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð´Ð»Ñ ÑÐµÐºÐ¾Ð½ÑÑÑÑÐºÑÐ¸Ð¸ ÑÐµÐ¼Ð°Ð½ÑÐ¸ÑÐµÑÐºÐ¾Ð³Ð¾ ÑÐ¾Ð´ÐµÑÐ¶Ð°Ð½Ð¸Ñ 
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#training", "#transfer_learning", "#benchmark", "#data", "#optimization"], "emoji": "ð", "ru": {"title": "Ð£Ð»ÑÑÑÐµÐ½Ð¸Ðµ Ð°Ð´Ð°Ð¿ÑÐ°ÑÐ¸Ð¸ Ðº Ð´Ð¾Ð¼ÐµÐ½Ñ Ð±ÐµÐ· Ð¸ÑÑÐ¾Ð´Ð½ÑÑ Ð´Ð°Ð½Ð½ÑÑ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ ÑÐ¼Ð½Ð¾Ð¹ Ð°ÑÐ³Ð¼ÐµÐ½ÑÐ°ÑÐ¸Ð¸ Ð¸ Ð¿ÐµÑÐµÐ²Ð·Ð²ÐµÑÐ¸Ð²Ð°Ð½Ð¸Ñ", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ Ð°ÑÐ³Ð¼ÐµÐ½ÑÐ°ÑÐ¸Ð¸ Ð´Ð°Ð½Ð½ÑÑ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ Shuffl
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#audio", "#diffusion"], "emoji": "ð§", "ru": {"title": "Ð ÐµÐ°Ð»Ð¸ÑÑÐ¸ÑÐ½ÑÐ¹ Ð±Ð¸Ð½Ð°ÑÑÐ°Ð»ÑÐ½ÑÐ¹ ÑÐ¸Ð½ÑÐµÐ· ÑÐµÑÐ¸ Ð² ÑÐµÐ°Ð»ÑÐ½Ð¾Ð¼ Ð²ÑÐµÐ¼ÐµÐ½Ð¸", "desc": "BinauralFlow - ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ Ð¿Ð¾ÑÐ¾ÐºÐ¾Ð²Ð¾Ð³Ð¾ Ð±Ð¸Ð½Ð°ÑÑÐ°Ð»ÑÐ½Ð¾Ð³Ð¾ ÑÐ¸Ð½ÑÐµÐ·Ð° ÑÐµÑÐ¸, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½ÑÐ¹ Ð½Ð° Ð¼ÐµÑÐ¾Ð´Ðµ ÑÐ¾Ð¿Ð¾ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¿Ð¾ÑÐ¾ÐºÐ¾Ð². ÐÐ½ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÐµÑ ÐºÐ°ÑÐ·Ð°Ð»ÑÐ½ÑÑ Ð°ÑÑÐ¸ÑÐµÐºÑÑÑÑ U-Net Ð´
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#data", "#dataset", "#training", "#rl", "#architecture"], "emoji": "ð§ ", "ru": {"title": "Ð£ÑÐ¸Ð»ÐµÐ½Ð¸Ðµ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð°Ð²ÑÐ¾Ð½Ð¾Ð¼Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸ÐµÐ¹ ÐºÐ¾Ð´Ð°", "desc": "R1-Code-Interpreter - ÑÑÐ¾ ÑÐ°ÑÑÐ¸ÑÐµÐ½Ð¸Ðµ ÑÐµÐºÑÑÐ¾Ð²Ð¾Ð¹ ÑÐ·ÑÐºÐ¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸, Ð¾Ð±ÑÑÐµÐ½Ð½Ð¾Ðµ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ Ð¼Ð½Ð¾Ð³Ð¾ÑÑÐ°Ð¿Ð½Ð¾Ð¹ ÐºÐ¾
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#training", "#agents", "#optimization", "#rag", "#reasoning", "#rl"], "emoji": "ð", "ru": {"title": "MaskSearch: ÑÐ½Ð¸Ð²ÐµÑÑÐ°Ð»ÑÐ½ÑÐ¹ Ð¿Ð¾Ð¸ÑÐº Ð¸ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð° Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð¿ÑÐµÐ´Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ MaskSearch, ÐºÐ¾ÑÐ¾ÑÐ°Ñ ÑÐ»ÑÑÑÐ°ÐµÑ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐ¸ Ð±Ð¾Ð»ÑÑÐ¸
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#hallucinations", "#multimodal", "#story_generation", "#training"], "emoji": "ð§ââï¸", "ru": {"title": "Frankentexts: Ð½Ð¾Ð²ÑÐ¹ Ð²ÑÐ·Ð¾Ð² Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸ Ð´ÐµÑÐµÐºÑÐ¸Ð¸ ÐÐ-ÑÐµÐºÑÑÐ¾Ð²", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»Ð¸ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð¸Ð»Ð¸ Ð½Ð¾Ð²ÑÐ¹ ÑÐ¸Ð¿ Ð´Ð»Ð¸Ð½Ð½ÑÑ ÑÐµÐºÑÑÐ¾Ð², Ð½Ð°Ð·ÑÐ²Ð°ÐµÐ¼ÑÑ Frankentexts, ÐºÐ¾ÑÐ¾ÑÑÐµ ÑÐ¾Ð·Ð´Ð°ÑÑÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÐ¼Ð¸ Ð¼Ð¾
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#dataset", "#audio", "#data"], "emoji": "ð­", "ru": {"title": "ÐÐ²ÑÐ¾Ð¼Ð°ÑÐ¸Ð·Ð¸ÑÐ¾Ð²Ð°Ð½Ð½Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð²ÑÑÐ¾ÐºÐ¾ÐºÐ°ÑÐµÑÑÐ²ÐµÐ½Ð½ÑÑ Ð½Ð°Ð±Ð¾ÑÐ¾Ð² Ð´Ð°Ð½Ð½ÑÑ ÑÐ¼Ð¾ÑÐ¸Ð¾Ð½Ð°Ð»ÑÐ½Ð¾Ð¹ ÑÐµÑÐ¸", "desc": "MIKU-PAL - ÑÑÐ¾ Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸Ð·Ð¸ÑÐ¾Ð²Ð°Ð½Ð½Ð°Ñ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑÐµÐ½Ð¸Ñ ÑÐ¼Ð¾ÑÐ¸Ð¾Ð½Ð°Ð»ÑÐ½Ð¾Ð¹ Ñ
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#science", "#dataset", "#diffusion", "#multimodal", "#optimization", "#benchmark"], "emoji": "ð¬", "ru": {"title": "ÐÐ²Ð°Ð½ÑÐ¾Ð²Ð°Ñ ÐºÐ¾Ð¼Ð¿Ð¸Ð»ÑÑÐ¸Ñ Ð½Ð° Ð½Ð¾Ð²Ð¾Ð¼ ÑÑÐ¾Ð²Ð½Ðµ: Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»Ñ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ ÑÑÐµÐ¼", "desc": "ÐÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð° Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»Ñ ÑÑÐ¼Ð¾Ð¿Ð¾Ð´Ð°Ð²Ð»ÑÑÑÐµÐ¹ Ð´Ð¸ÑÑÑÐ·Ð¸Ð¸ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ ÑÑÑÑÐº
[03.06.2025 19:10] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#cv", "#training"], "emoji": "ð¼ï¸", "ru": {"title": "Ð£Ð»ÑÑÑÐµÐ½Ð½Ð°Ñ Ð´Ð¸ÑÑÐ¸Ð»Ð»ÑÑÐ¸Ñ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ ÑÐµÐºÑÑÑ", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ ÑÑÐ¾Ð²ÐµÑÑÐµÐ½ÑÑÐ²Ð¾Ð²Ð°Ð½Ð½ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ Ð´Ð¸ÑÑÐ¸Ð»Ð»ÑÑÐ¸Ð¸ Ð´Ð»Ñ ÐºÑÑÐ¿Ð½Ð¾Ð¼Ð°ÑÑÑÐ°Ð±Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ ÑÐµÐºÑÑÑ. ÐÐ²ÑÐ¾ÑÑ Ð¿ÑÐµ
[03.06.2025 19:10] Loading Chinese text from previous data.
[03.06.2025 19:10] Renaming data file.
[03.06.2025 19:10] Renaming previous data. hf_papers.json to ./d/2025-06-03.json
[03.06.2025 19:10] Saving new data file.
[03.06.2025 19:10] Generating page.
[03.06.2025 19:10] Renaming previous page.
[03.06.2025 19:10] Renaming previous data. index.html to ./d/2025-06-03.html
[03.06.2025 19:10] [Experimental] Generating Chinese page for reading.
[03.06.2025 19:10] Chinese vocab [{'word': 'æ¢è®¨', 'pinyin': 'tÃ n tÇo', 'trans': 'discuss'}, {'word': 'å¢å¼º', 'pinyin': 'zÄng qiÃ¡ng', 'trans': 'enhance'}, {'word': 'æ¨ç', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'}, {'word': 'è½å', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'}, {'word': 'æ¹æ³', 'pinyin': 'fÄng fÇ', 'trans': 'method'}, {'word': 'å¯éªè¯', 'pinyin': 'kÄ yÃ n zhÃ¨ng', 'trans': 'verifiable'}, {'word': 'å¥å±', 'pinyin': 'jiÇng lÃ¬', 'trans': 'reward'}, {'word': 'å¼ºå', 'pinyin': 'qiÃ¡ng huÃ ', 'trans': 'reinforce'}, {'word': 'å­¦ä¹ ', 'pinyin': 'xuÃ© xÃ­', 'trans': 'learning'}, {'word': 'ç ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'}, {'word': 'åç°', 'pinyin': 'fÄ xiÃ n', 'trans': 'discover'}, {'word': 'çµå¼', 'pinyin': 'shÄng zhÃ­', 'trans': 'entropy value'}, {'word': 'æ¾è', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'}, {'word': 'å½±å', 'pinyin': 'yÇng xiÇng', 'trans': 'impact'}, {'word': 'ä¼å', 'pinyin': 'yÅu huÃ ', 'trans': 'optimization'}, {'word': 'æ¨¡å¼', 'pinyin': 'mÃ³ shÃ¬', 'trans': 'pattern'}, {'word': 'è§å¯', 'pinyin': 'guÄn chÃ¡', 'trans': 'observe'}, {'word': 'å°é', 'pinyin': 'shÇo liÃ ng', 'trans': 'small amount'}, {'word': 'å³å®', 'pinyin': 'juÃ© dÃ¬ng', 'trans': 'determine'}, {'word': 'è·¯å¾', 'pinyin': 'lÃ¹ jÃ¬ng', 'trans': 'path'}, {'word': 'è°æ´', 'pinyin': 'tiÃ¡o zhÄng', 'trans': 'adjust'}, {'word': 'ç­ç¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'}, {'word': 'æ¢¯åº¦', 'pinyin': 'tÄ« dÃ¹', 'trans': 'gradient'}, {'word': 'æ´æ°', 'pinyin': 'gÃ¨ng xÄ«n', 'trans': 'update'}, {'word': 'åå¾', 'pinyin': 'qÇ dÃ©', 'trans': 'achieve'}, {'word': 'æ§è½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'}]
[03.06.2025 19:10] Renaming previous Chinese page.
[03.06.2025 19:10] Renaming previous data. zh.html to ./d/2025-06-02_zh_reading_task.html
[03.06.2025 19:10] Writing Chinese reading task.
[03.06.2025 19:10] Writing result.
[03.06.2025 19:10] Renaming log file.
[03.06.2025 19:10] Renaming previous data. log.txt to ./logs/2025-06-03_last_log.txt
