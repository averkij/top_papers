[31.12.2025 06:36] Read previous papers.
[31.12.2025 06:36] Generating top page (month).
[31.12.2025 06:36] Writing top page (month).
[31.12.2025 07:25] Read previous papers.
[31.12.2025 07:25] Get feed.
[31.12.2025 07:25] Get page data from previous paper. URL: https://huggingface.co/papers/2512.22525
[31.12.2025 07:25] Extract page data from URL. URL: https://huggingface.co/papers/2512.23675
[31.12.2025 07:25] Extract page data from URL. URL: https://huggingface.co/papers/2512.22469
[31.12.2025 07:25] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[31.12.2025 07:25] No deleted papers detected.
[31.12.2025 07:25] Downloading and parsing papers (pdf, html). Total: 3.
[31.12.2025 07:25] Downloading and parsing paper https://huggingface.co/papers/2512.22525.
[31.12.2025 07:25] Extra JSON file exists (./assets/json/2512.22525.json), skip PDF parsing.
[31.12.2025 07:25] Paper image links file exists (./assets/img_data/2512.22525.json), skip HTML parsing.
[31.12.2025 07:25] Success.
[31.12.2025 07:25] Downloading and parsing paper https://huggingface.co/papers/2512.23675.
[31.12.2025 07:25] Downloading paper 2512.23675 from https://arxiv.org/pdf/2512.23675v1...
[31.12.2025 07:25] Extracting affiliations from text.
[31.12.2025 07:25] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"End-to-End Test-Time Training for Long Context Arnuv Tandon 1,3, Karan Dalal 1,4, Xinhao Li 5, Daniel Koceja 3, Marcel RÃ¸d 3, Sam Buchanan4, 5 2 0 2 9 2 ] . [ 1 5 7 6 3 2 . 2 1 5 2 : r Xiaolong Wang5, Jure Leskovec3, Sanmi Koyejo3, Tatsunori Hashimoto3, Carlos Guestrin3, Jed McCaleb1, Yejin Choi2, Yu Sun 2,3 1 Astera Institute 2 NVIDIA 3 Stanford University 4 UC Berkeley 5 UC San Diego Abstract We formulate long-context language modeling as problem in continual learning rather than architecture design. Under this formulation, we only use standard architecture Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the models initialization for learning at test time via meta-learning at training time. Overall, our method, form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 faster than full attention for 128K context. Our code is publicly available. Figure 1. Scaling with context length, in terms of test loss (left) and latency (right). Left: Our method (TTT-E2E) turns the worst line (green) into the best (blue) at 128K context length. Loss (), the y-value, is computed as (loss of the reported method) (loss of Transformer with full attention), so loss of full attention itself (orange) is the flat line at = 0. While other methods produce worse loss in longer context, TTT-E2E maintains the same advantage over full "
[31.12.2025 07:25] Response: ```python
[
    "Astera Institute",
    "NVIDIA",
    "Stanford University",
    "UC Berkeley",
    "UC San Diego"
]
```
[31.12.2025 07:25] Deleting PDF ./assets/pdf/2512.23675.pdf.
[31.12.2025 07:25] Success.
[31.12.2025 07:25] Downloading and parsing paper https://huggingface.co/papers/2512.22469.
[31.12.2025 07:25] Downloading paper 2512.22469 from https://arxiv.org/pdf/2512.22469v1...
[31.12.2025 07:25] Extracting affiliations from text.
[31.12.2025 07:25] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"GraphLocator: Graph-guided Causal Reasoning for Issue Localization WEI LIU, Key Lab of High Confidence Software Technologies (PKU), MoE, China, China and School of Computer Science, Peking University, China CHAO PENG, Bytedance, China PENGFEI GAO, Bytedance, China AOFAN LIU, School of Electronic and Computer Engineering, Peking University, China WEI ZHANG, Key Lab of High Confidence Software Technologies (PKU), MoE, China, China and School of Computer Science, Peking University, China HAIYAN ZHAO, Key Lab of High Confidence Software Technologies (PKU), MoE, China, China and School of Computer Science, Peking University, China ZHI JIN, Key Lab of High Confidence Software Technologies (PKU), MoE, China, China and School of Computer Science, Peking University, China The issue localization task aims to identify the locations in software repository that requires modification given natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches: (1) symptomto-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an LLM-based approach that mitigates symptomto-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact of GraphLocator is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over nei"
[31.12.2025 07:25] Response: ```python
[
    "Key Lab of High Confidence Software Technologies (PKU), MoE, China",
    "School of Computer Science, Peking University, China",
    "Bytedance, China",
    "School of Electronic and Computer Engineering, Peking University, China"
]
```
[31.12.2025 07:25] Deleting PDF ./assets/pdf/2512.22469.pdf.
[31.12.2025 07:25] Success.
[31.12.2025 07:25] Enriching papers with extra data.
[31.12.2025 07:25] ********************************************************************************
[31.12.2025 07:25] Abstract 0. Recently unified generation and editing models have achieved remarkable success with their impressive performance. These models rely mainly on text prompts for instruction-based editing and generation, but language often fails to capture users intended edit locations and fine-grained visual details....
[31.12.2025 07:25] ********************************************************************************
[31.12.2025 07:25] Abstract 1. We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on ...
[31.12.2025 07:25] ********************************************************************************
[31.12.2025 07:25] Abstract 2. The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code impl...
[31.12.2025 07:25] Read previous papers.
[31.12.2025 07:25] Generating reviews via LLM API.
[31.12.2025 07:25] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#dataset", "#benchmark"], "emoji": "ğŸ¨", "ru": {"title": "Ğ¡ĞºĞµÑ‚Ñ‡-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DreamOmni3, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°
[31.12.2025 07:25] Querying the API.
[31.12.2025 07:25] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.
[31.12.2025 07:25] Response: ```json
{
  "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ continual learning Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Transformer Ñ sliding-window attention Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ€ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ² ÑĞ²Ğ¾Ğ¸ Ğ²ĞµÑĞ° Ñ‡ĞµÑ€ĞµĞ· next-token prediction. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ meta-learning Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ TTT-E2E Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ full attention Transformer, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ğ¼ĞµÑ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ½ÑƒÑ latency Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ² 2.7 Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 128K.",
  "emoji": "ğŸ§ ",
  "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ²"
}
```
[31.12.2025 07:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available."

[31.12.2025 07:25] Response: ```python
["TRAINING", "ARCHITECTURE"]
```
[31.12.2025 07:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available."

[31.12.2025 07:25] Response: ```python
['LONG_CONTEXT', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[31.12.2025 07:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new approach to long-context language modeling by treating it as a continual learning problem instead of focusing solely on architectural changes. The authors utilize a standard Transformer model with sliding-window attention, allowing the model to learn continuously during test time through next-token prediction. They enhance the model\'s ability to learn at test time by employing meta-learning during training, resulting in a method called Test-Time Training (TTT) that operates end-to-end. The proposed TTT-E2E method demonstrates improved scaling properties and faster inference times compared to existing models, making it a significant advancement in the field.","title":"Revolutionizing Long-Context Language Modeling with Test-Time Training"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents a new approach to long-context language modeling by treating it as a continual learning problem instead of focusing solely on architectural changes. The authors utilize a standard Transformer model with sliding-window attention, allowing the model to learn continuously during test time through next-token prediction. They enhance the model's ability to learn at test time by employing meta-learning during training, resulting in a method called Test-Time Training (TTT) that operates end-to-end. The proposed TTT-E2E method demonstrates improved scaling properties and faster inference times compared to existing models, making it a significant advancement in the field.", title='Revolutionizing Long-Context Language Modeling with Test-Time Training'))
[31.12.2025 07:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡å°†é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡è§†ä¸ºæŒç»­å­¦ä¹ çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯æ¶æ„è®¾è®¡ã€‚æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„Transformeræ¶æ„ï¼Œå¹¶é€šè¿‡æ»‘åŠ¨çª—å£æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå¤„ç†ã€‚åœ¨æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å¯¹ç»™å®šä¸Šä¸‹æ–‡çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ¥æŒç»­å­¦ä¹ ï¼Œå¹¶å°†è¯»å–çš„ä¸Šä¸‹æ–‡å‹ç¼©åˆ°å…¶æƒé‡ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨è®­ç»ƒæ—¶çš„å…ƒå­¦ä¹ æ¥æ”¹å–„æ¨¡å‹åœ¨æµ‹è¯•æ—¶çš„åˆå§‹åŒ–ã€‚","title":"æŒç»­å­¦ä¹ ä¸‹çš„é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡å°†é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡è§†ä¸ºæŒç»­å­¦ä¹ çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯æ¶æ„è®¾è®¡ã€‚æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„Transformeræ¶æ„ï¼Œå¹¶é€šè¿‡æ»‘åŠ¨çª—å£æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå¤„ç†ã€‚åœ¨æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å¯¹ç»™å®šä¸Šä¸‹æ–‡çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ¥æŒç»­å­¦ä¹ ï¼Œå¹¶å°†è¯»å–çš„ä¸Šä¸‹æ–‡å‹ç¼©åˆ°å…¶æƒé‡ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨è®­ç»ƒæ—¶çš„å…ƒå­¦ä¹ æ¥æ”¹å–„æ¨¡å‹åœ¨æµ‹è¯•æ—¶çš„åˆå§‹åŒ–ã€‚', title='æŒç»­å­¦ä¹ ä¸‹çš„é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡'))
[31.12.2025 07:25] Querying the API.
[31.12.2025 07:25] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task.
[31.12.2025 07:25] Response: ```json
{
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GraphLocator â€” Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ (CIG) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¼ĞµÑÑ‚Ğ° ÑĞ¸Ğ¼Ğ¿Ñ‚Ğ¾Ğ¼Ğ¾Ğ² Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ³Ñ€Ğ°Ñ„ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… Ğ²ĞµÑ€ÑˆĞ¸Ğ½Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ +19.49% Ğ² recall Ğ¸ +11.89% Ğ² precision Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.",
  "emoji": "ğŸ”",
  "title": "Ğ“Ñ€Ğ°Ñ„ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² ĞºĞ¾Ğ´Ğµ"
}
```
[31.12.2025 07:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task."

[31.12.2025 07:25] Response: ```python
["PLP", "BENCHMARK"]
```
[31.12.2025 07:25] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task."

[31.12.2025 07:25] Response: ```python
['GRAPHS', 'REASONING', 'SCIENCE']
```

**Justification:**

1. **GRAPHS**: The paper explicitly proposes GraphLocator, which uses graph-based representations (causal issue graph with vertices and edges) and operates on repository graphs. Graph neural network concepts are central to the approach.

2. **REASONING**: The paper addresses reasoning over code dependencies and causal relationships. The approach involves "iteratively reasoning over neighboring vertices" and discovering causal dependencies, which relates to logical reasoning capabilities.

3. **SCIENCE**: The paper applies machine learning/NLP techniques to automated software engineering, which is a scientific application domain. It addresses the automation of software maintenance tasks, which falls under research automatization in the scientific/technical domain.
[31.12.2025 07:25] Error. Failed to parse JSON from LLM. ["GRAPHS", "REASONING", "SCIENCE"]


**Justification:**

1. **GRAPHS**: The paper explicitly proposes GraphLocator, which uses graph-based representations (causal issue graph with vertices and edges) and operates on repository graphs. Graph neural network concepts are central to the approach.

2. **REASONING**: The paper addresses reasoning over code dependencies and causal relationships. The approach involves "iteratively reasoning over neighboring vertices" and discovering causal dependencies, which relates to logical reasoning capabilities.

3. **SCIENCE**: The paper applies machine learning/NLP techniques to automated software engineering, which is a scientific application domain. It addresses the automation of software maintenance tasks, which falls under research automatization in the scientific/technical domain.
[31.12.2025 07:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents GraphLocator, a novel approach for the issue localization task in software engineering, which aims to identify code locations needing changes based on natural language issue descriptions. It addresses two main challenges: symptom-to-cause mismatches, where the issue description does not clearly indicate the root cause, and one-to-many mismatches, where a single issue relates to multiple code entities. GraphLocator utilizes a causal issue graph (CIG) to represent sub-issues and their dependencies, enhancing the understanding of the relationship between symptoms and code. Experimental results show that GraphLocator significantly improves localization accuracy, outperforming existing methods in both recall and precision metrics.","title":"Bridging the Gap: GraphLocator for Accurate Issue Localization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents GraphLocator, a novel approach for the issue localization task in software engineering, which aims to identify code locations needing changes based on natural language issue descriptions. It addresses two main challenges: symptom-to-cause mismatches, where the issue description does not clearly indicate the root cause, and one-to-many mismatches, where a single issue relates to multiple code entities. GraphLocator utilizes a causal issue graph (CIG) to represent sub-issues and their dependencies, enhancing the understanding of the relationship between symptoms and code. Experimental results show that GraphLocator significantly improves localization accuracy, outperforming existing methods in both recall and precision metrics.', title='Bridging the Gap: GraphLocator for Accurate Issue Localization'))
[31.12.2025 07:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºGraphLocatorçš„æ–¹æ³•ï¼Œç”¨äºè§£å†³è½¯ä»¶ä»“åº“ä¸­é—®é¢˜å®šä½çš„ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡å‘ç°å› æœç»“æ„æ¥å‡è½»ç—‡çŠ¶ä¸åŸå› ä¹‹é—´çš„å·®è·ï¼Œå¹¶é€šè¿‡åŠ¨æ€é—®é¢˜è§£ç¼ æ¥è§£å†³ä¸€å¯¹å¤šçš„åŒ¹é…é—®é¢˜ã€‚GraphLocatorçš„æ ¸å¿ƒæ˜¯å› æœé—®é¢˜å›¾ï¼ˆCIGï¼‰ï¼Œå…¶ä¸­é¡¶ç‚¹è¡¨ç¤ºå‘ç°çš„å­é—®é¢˜åŠå…¶ç›¸å…³ä»£ç å®ä½“ï¼Œè¾¹è¡¨ç¤ºå®ƒä»¬ä¹‹é—´çš„å› æœä¾èµ–å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphLocatoråœ¨åŠŸèƒ½çº§å¬å›ç‡å’Œç²¾åº¦ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚","title":"GraphLocatorï¼šæ™ºèƒ½è½¯ä»¶é—®é¢˜å®šä½çš„æ–°æ–¹æ³•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºGraphLocatorçš„æ–¹æ³•ï¼Œç”¨äºè§£å†³è½¯ä»¶ä»“åº“ä¸­é—®é¢˜å®šä½çš„ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡å‘ç°å› æœç»“æ„æ¥å‡è½»ç—‡çŠ¶ä¸åŸå› ä¹‹é—´çš„å·®è·ï¼Œå¹¶é€šè¿‡åŠ¨æ€é—®é¢˜è§£ç¼ æ¥è§£å†³ä¸€å¯¹å¤šçš„åŒ¹é…é—®é¢˜ã€‚GraphLocatorçš„æ ¸å¿ƒæ˜¯å› æœé—®é¢˜å›¾ï¼ˆCIGï¼‰ï¼Œå…¶ä¸­é¡¶ç‚¹è¡¨ç¤ºå‘ç°çš„å­é—®é¢˜åŠå…¶ç›¸å…³ä»£ç å®ä½“ï¼Œè¾¹è¡¨ç¤ºå®ƒä»¬ä¹‹é—´çš„å› æœä¾èµ–å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphLocatoråœ¨åŠŸèƒ½çº§å¬å›ç‡å’Œç²¾åº¦ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚', title='GraphLocatorï¼šæ™ºèƒ½è½¯ä»¶é—®é¢˜å®šä½çš„æ–°æ–¹æ³•'))
[31.12.2025 07:26] Renaming data file.
[31.12.2025 07:26] Renaming previous data. hf_papers.json to ./d/2025-12-31.json
[31.12.2025 07:26] Saving new data file.
[31.12.2025 07:26] Generating page.
[31.12.2025 07:26] Renaming previous page.
[31.12.2025 07:26] Renaming previous data. index.html to ./d/2025-12-31.html
[31.12.2025 07:26] Writing result.
[31.12.2025 07:26] Renaming log file.
[31.12.2025 07:26] Renaming previous data. log.txt to ./logs/2025-12-31_last_log.txt
