[06.02.2025 03:14] Read previous papers.
[06.02.2025 03:14] Generating top page (month).
[06.02.2025 03:14] Writing top page (month).
[06.02.2025 04:12] Read previous papers.
[06.02.2025 04:12] Get feed.
[06.02.2025 04:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01506
[06.02.2025 04:12] Extract page data from URL. URL: https://huggingface.co/papers/2502.03373
[06.02.2025 04:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.02339
[06.02.2025 04:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.02.2025 04:12] No deleted papers detected.
[06.02.2025 04:12] Downloading and parsing papers (pdf, html). Total: 3.
[06.02.2025 04:12] Downloading and parsing paper https://huggingface.co/papers/2502.01506.
[06.02.2025 04:12] Extra JSON file exists (./assets/json/2502.01506.json), skip PDF parsing.
[06.02.2025 04:12] Paper image links file exists (./assets/img_data/2502.01506.json), skip HTML parsing.
[06.02.2025 04:12] Success.
[06.02.2025 04:12] Downloading and parsing paper https://huggingface.co/papers/2502.03373.
[06.02.2025 04:12] Downloading paper 2502.03373 from http://arxiv.org/pdf/2502.03373v1...
[06.02.2025 04:12] Extracting affiliations from text.
[06.02.2025 04:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Demystifying Long Chain-of-Thought Reasoning in LLMs Edward Yeo * 1 Yuxuan Tong * 2 Morry Niu 1 Graham Neubig 3 Xiang Yue * 3 5 2 0 2 5 ] . [ 1 3 7 3 3 0 . 2 0 5 2 : r a "
[06.02.2025 04:12] Response: ```python
[]
```
[06.02.2025 04:12] Extracting affiliations from text.
[06.02.2025 04:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Demystifying Long Chain-of-Thought Reasoning in LLMs Edward Yeo * 1 Yuxuan Tong * 2 Morry Niu 1 Graham Neubig 3 Xiang Yue * 3 5 2 0 2 5 ] . [ 1 3 7 3 3 0 . 2 0 5 2 : r aScaling inference compute enhances reasoning in large language models (LLMs), with long chainsof-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot. 1. Introduction Large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023; Anthropic, 2023; OpenAI, 2023) have *Project Lead. 1IN.AI 2Tsinghua University. Work started when interning at CMU. 3Carnegie Mellon University. Correspondence to: Xiang Yue <xyue2@andrew.cmu.edu>. demonstrated remarkable reasoning abilities in domains like mathematics (Cobbe et al., 2021) and programming (Chen et al., 2021). key technique for enabling reasoning abilities in LLMs is chain-of-thought (CoT) prompting (Wei et al., 2022), which guides models to generate intermediate reasoning steps before arriving at final answer. Despite these advancements, LLMs still struggle with highly complex reasoning tasks, such as mathematical competitions (Hendrycks et al., 2021), PhD-level scientific QA (Rein et al., 2024), and software engineering (Jimenez et al., 2024), even with CoT. Recently, OpenAIs o1 models (OpenAI, 2024) have demonstrated significant breakthroughs in these tasks. key distinguishing feature of these models is their ability to scale up inference compute with long CoTs, which include strategies such as recognizing and correcting mistakes, breaking down difficult steps, and iterating on alternative approaches, leading to substantially longer and more structured reasoning processes. Several efforts have attempted to replicate the performance of o1 models by training LLMs to generate long CoTs (Qwen Team, 2024b; DeepSeek-AI, 2025; Kimi Team, 2025; Pan et al., 2025; Zeng et al., 2025). Most of these approaches rely on verifiable rewards, such as accuracy based on ground-truth answers, which helps to avoid reward hacking in reinforcement learning (RL) at scale. However, comprehensive understanding of how models learn and generate long CoTs remains limited. In this work, we systematically investigate the underlying mechanics of long CoT generation. Specifically, we explore: 1) Supervised fine-tuning (SFT) for long CoTs the most direct way to enable long CoT reasoning. We analyze its scaling behavior and impact on RL, finding that long CoT SFT allows models to reach higher performance and also facilitates easier RL improvements than short CoT. 2) Challenges in RL-driven CoT scaling we observe that RL does not always stably extend CoT length and complexity. To address this, we introduce cosine length-scaling reward with repetition penalty, which stabilizes CoT growth while encouraging emergent reasoning behaviors such as branching and backtracking. 3) Scaling up verifiable signals for long CoT RL Verifiable reward signals are essential for stabilizing long CoT Demystifying Long Chain-of-Thought Reasoning in LLMs RL. However, scaling them up remains challenging due to the limited availability of high-quality, verifiable data. To address this, we explore the use of data containing noisy, web-extracted solutions (Yue et al., 2024b). While these silver supervision signals introduce uncertainty, we find that, with an appropriate mixture in SFT and filtration in RL, they show promise, especially in out-of-distribution (OOD) reasoning scenarios such as STEM problem-solving. 4) Origins of Long CoT Abilities and RL Challenges Core skills like branching and error validation are inherently present in base models, but effective RL-driven incentivization demands careful designs. We examine RL incentives on long CoT generation, trace reasoning patterns in pre-training data, and discuss nuances in measuring their emergence. 2. Problem Formulation In this section, we define the notation, followed by an overview of SFT and RL methods for eliciting long CoTs.Our goal is to demystify long chain-of-thought reasoning in LLMs. Through systematic analysis and ablations, we extract key insights and offer practical strategies to enhance and stabilize its performance. 2.1. Notation Let be query, and let be the output sequence. We consider LLM parameterized by θ, which defines conditional distribution over output tokens: πθ(yt x, y1:t1). We denote by CoT(y) the tokens in the generated output that constitute the chain-of-thought, which is often reasoning trace or explanatory sequence. The final answer can be separate set of tokens or simply the last part of y. In this work, we use the term long chain-of-thought (long CoT) to describe an extended sequence of reasoning tokens that not only exhibits larger-than-usual token length but also demonstrates more sophisticated behaviors such as: 1) Branching and Backtracking: The model systematically explores multiple paths (branching) and reverts to earlier points if particular path proves wrong (backtracking). 2) Error Validation and Correction: The model detects inconsistencies or mistakes in its intermediate steps and takes corrective actions to restore coherence "
[06.02.2025 04:12] Mistral response. {"id": "c80099fdcc83430290bee40b57e031c9", "object": "chat.completion", "created": 1738815165, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\"IN.AI\", \"Tsinghua University\", \"Carnegie Mellon University\"]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1671, "total_tokens": 1692, "completion_tokens": 21}}
[06.02.2025 04:12] Response: ["IN.AI", "Tsinghua University", "Carnegie Mellon University"]
[06.02.2025 04:12] Deleting PDF ./assets/pdf/2502.03373.pdf.
[06.02.2025 04:12] Success.
[06.02.2025 04:12] Downloading and parsing paper https://huggingface.co/papers/2502.02339.
[06.02.2025 04:12] Extra JSON file exists (./assets/json/2502.02339.json), skip PDF parsing.
[06.02.2025 04:12] Paper image links file exists (./assets/img_data/2502.02339.json), skip HTML parsing.
[06.02.2025 04:12] Success.
[06.02.2025 04:12] Enriching papers with extra data.
[06.02.2025 04:12] ********************************************************************************
[06.02.2025 04:12] Abstract 0. The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Re...
[06.02.2025 04:12] ********************************************************************************
[06.02.2025 04:12] Abstract 1. Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which l...
[06.02.2025 04:12] ********************************************************************************
[06.02.2025 04:12] Abstract 2. Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillati...
[06.02.2025 04:12] Read previous papers.
[06.02.2025 04:12] Generating reviews via LLM API.
[06.02.2025 04:12] Using data from previous issue: {"categories": ["#multimodal", "#agents"], "emoji": "📊", "ru": {"title": "LLM-агенты раскрывают тайны социально-экономической динамики", "desc": "Статья представляет новый фреймворк TwinMarket для моделирования социально-экономических систем с использованием больших языковых моделей (LLM). Авторы пр
[06.02.2025 04:12] Querying the API.
[06.02.2025 04:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot.
[06.02.2025 04:12] Response: {
  "desc": "Статья исследует механизмы длинных цепочек рассуждений (CoT) в больших языковых моделях (LLM). Авторы выявляют ключевые факторы, влияющие на способность моделей генерировать длинные CoT траектории через эксперименты с обучением с подкреплением (RL) и тонкой настройкой. Исследование показывает важность масштабирования вычислительных ресурсов, формирования наград и использования веб-данных для улучшения рассуждений. Результаты предоставляют практические рекомендации по оптимизации стратегий обучения для усиления длинных CoT рассуждений в LLM.",
  "emoji": "🧠",
  "title": "Раскрывая секреты длинных цепочек рассуждений в ИИ"
}
[06.02.2025 04:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot."

[06.02.2025 04:12] Response: ```python
['RL', 'TRAINING']
```
[06.02.2025 04:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot."

[06.02.2025 04:12] Response: ```python
["REASONING", "LONG_CONTEXT", "OPTIMIZATION"]
```
[06.02.2025 04:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how to improve reasoning in large language models (LLMs) by enhancing their inference capabilities through long chains-of-thought (CoTs). It highlights the importance of reinforcement learning (RL) in developing these reasoning skills, while also addressing the unclear conditions for the emergence of long CoTs. The study presents four key findings, including the role of supervised fine-tuning (SFT) in simplifying training, the necessity of reward shaping for stabilizing CoT growth, and the significance of scaling reward signals for effective RL. Overall, the research provides valuable insights for optimizing training strategies to boost long CoT reasoning in LLMs.","title":"Unlocking Reasoning Power in Large Language Models"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper explores how to improve reasoning in large language models (LLMs) by enhancing their inference capabilities through long chains-of-thought (CoTs). It highlights the importance of reinforcement learning (RL) in developing these reasoning skills, while also addressing the unclear conditions for the emergence of long CoTs. The study presents four key findings, including the role of supervised fine-tuning (SFT) in simplifying training, the necessity of reward shaping for stabilizing CoT growth, and the significance of scaling reward signals for effective RL. Overall, the research provides valuable insights for optimizing training strategies to boost long CoT reasoning in LLMs.', title='Unlocking Reasoning Power in Large Language Models'))
[06.02.2025 04:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本研究探讨了大型语言模型（LLMs）中长推理链（CoTs）的生成机制，揭示了影响模型生成长推理链的关键因素。我们发现，虽然监督微调（SFT）不是绝对必要的，但它可以简化训练过程并提高效率。随着训练计算能力的增加，推理能力有可能出现，但其发展并不总是保证，因此奖励设计对于稳定推理链的长度增长至关重要。最后，我们的研究为优化训练策略以增强LLMs中的长推理链提供了实用指导。","title":"优化训练策略，提升长推理链能力"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='本研究探讨了大型语言模型（LLMs）中长推理链（CoTs）的生成机制，揭示了影响模型生成长推理链的关键因素。我们发现，虽然监督微调（SFT）不是绝对必要的，但它可以简化训练过程并提高效率。随着训练计算能力的增加，推理能力有可能出现，但其发展并不总是保证，因此奖励设计对于稳定推理链的长度增长至关重要。最后，我们的研究为优化训练策略以增强LLMs中的长推理链提供了实用指导。', title='优化训练策略，提升长推理链能力'))
[06.02.2025 04:12] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#benchmark", "#multimodal", "#architecture", "#training"], "emoji": "🧠", "ru": {"title": "AStar: Эффективное структурированное мышление для мультимодальных ИИ", "desc": "Статья представляет новый подход к улучшению визуального рассуждения мультимодальн
[06.02.2025 04:12] Loading Chinese text from previous data.
[06.02.2025 04:12] Renaming data file.
[06.02.2025 04:12] Renaming previous data. hf_papers.json to ./d/2025-02-06.json
[06.02.2025 04:12] Saving new data file.
[06.02.2025 04:12] Generating page.
[06.02.2025 04:12] Renaming previous page.
[06.02.2025 04:12] Renaming previous data. index.html to ./d/2025-02-06.html
[06.02.2025 04:12] [Experimental] Generating Chinese page for reading.
[06.02.2025 04:12] Chinese vocab [{'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'}, {'word': '桥', 'pinyin': 'qiáo', 'trans': 'bridge'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '翻译', 'pinyin': 'fān yì', 'trans': 'translation'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'}, {'word': '蒸馏', 'pinyin': 'zhēng liú', 'trans': 'distillation'}, {'word': '加速', 'pinyin': 'jiā sù', 'trans': 'accelerate'}, {'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'process'}, {'word': '有条件', 'pinyin': 'yǒu tiáo jiàn', 'trans': 'conditional'}, {'word': '无条件', 'pinyin': 'wú tiáo jiàn', 'trans': 'unconditional'}, {'word': '受损', 'pinyin': 'shòu sǔn', 'trans': 'damaged'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generation'}, {'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'}]
[06.02.2025 04:12] Renaming previous Chinese page.
[06.02.2025 04:12] Renaming previous data. zh.html to ./d/2025-02-05_zh_reading_task.html
[06.02.2025 04:12] Writing Chinese reading task.
[06.02.2025 04:12] Writing result.
[06.02.2025 04:12] Renaming log file.
[06.02.2025 04:12] Renaming previous data. log.txt to ./logs/2025-02-06_last_log.txt
