[03.04.2025 08:17] Read previous papers.
[03.04.2025 08:17] Generating top page (month).
[03.04.2025 08:17] Writing top page (month).
[03.04.2025 09:11] Read previous papers.
[03.04.2025 09:11] Get feed.
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00999
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00883
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01014
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20783
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01956
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01724
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00824
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01934
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01848
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01308
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01204
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2405.20216
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23135
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00406
[03.04.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.18817
[03.04.2025 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[03.04.2025 09:11] No deleted papers detected.
[03.04.2025 09:11] Downloading and parsing papers (pdf, html). Total: 15.
[03.04.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2504.00999.
[03.04.2025 09:11] Extra JSON file exists (./assets/json/2504.00999.json), skip PDF parsing.
[03.04.2025 09:11] Paper image links file exists (./assets/img_data/2504.00999.json), skip HTML parsing.
[03.04.2025 09:11] Success.
[03.04.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2504.00883.
[03.04.2025 09:11] Extra JSON file exists (./assets/json/2504.00883.json), skip PDF parsing.
[03.04.2025 09:11] Paper image links file exists (./assets/img_data/2504.00883.json), skip HTML parsing.
[03.04.2025 09:11] Success.
[03.04.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2504.01014.
[03.04.2025 09:11] Extra JSON file exists (./assets/json/2504.01014.json), skip PDF parsing.
[03.04.2025 09:11] Paper image links file exists (./assets/img_data/2504.01014.json), skip HTML parsing.
[03.04.2025 09:11] Success.
[03.04.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.20783.
[03.04.2025 09:11] Downloading paper 2503.20783 from http://arxiv.org/pdf/2503.20783v1...
[03.04.2025 09:12] Failed to download and parse paper https://huggingface.co/papers/2503.20783: 'LTChar' object is not iterable
[03.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.01956.
[03.04.2025 09:12] Extra JSON file exists (./assets/json/2504.01956.json), skip PDF parsing.
[03.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.01956.json), skip HTML parsing.
[03.04.2025 09:12] Success.
[03.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.01724.
[03.04.2025 09:12] Extra JSON file exists (./assets/json/2504.01724.json), skip PDF parsing.
[03.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.01724.json), skip HTML parsing.
[03.04.2025 09:12] Success.
[03.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.00824.
[03.04.2025 09:12] Extra JSON file exists (./assets/json/2504.00824.json), skip PDF parsing.
[03.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.00824.json), skip HTML parsing.
[03.04.2025 09:12] Success.
[03.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.01934.
[03.04.2025 09:12] Extra JSON file exists (./assets/json/2504.01934.json), skip PDF parsing.
[03.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.01934.json), skip HTML parsing.
[03.04.2025 09:12] Success.
[03.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.01848.
[03.04.2025 09:12] Extra JSON file exists (./assets/json/2504.01848.json), skip PDF parsing.
[03.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.01848.json), skip HTML parsing.
[03.04.2025 09:12] Success.
[03.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.01308.
[03.04.2025 09:12] Extra JSON file exists (./assets/json/2504.01308.json), skip PDF parsing.
[03.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.01308.json), skip HTML parsing.
[03.04.2025 09:12] Success.
[03.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.01204.
[03.04.2025 09:12] Extra JSON file exists (./assets/json/2504.01204.json), skip PDF parsing.
[03.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.01204.json), skip HTML parsing.
[03.04.2025 09:12] Success.
[03.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2405.20216.
[03.04.2025 09:12] Extra JSON file exists (./assets/json/2405.20216.json), skip PDF parsing.
[03.04.2025 09:12] Paper image links file exists (./assets/img_data/2405.20216.json), skip HTML parsing.
[03.04.2025 09:12] Success.
[03.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.23135.
[03.04.2025 09:12] Extra JSON file exists (./assets/json/2503.23135.json), skip PDF parsing.
[03.04.2025 09:12] Paper image links file exists (./assets/img_data/2503.23135.json), skip HTML parsing.
[03.04.2025 09:12] Success.
[03.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2504.00406.
[03.04.2025 09:12] Extra JSON file exists (./assets/json/2504.00406.json), skip PDF parsing.
[03.04.2025 09:12] Paper image links file exists (./assets/img_data/2504.00406.json), skip HTML parsing.
[03.04.2025 09:12] Success.
[03.04.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2503.18817.
[03.04.2025 09:12] Extra JSON file exists (./assets/json/2503.18817.json), skip PDF parsing.
[03.04.2025 09:12] Paper image links file exists (./assets/img_data/2503.18817.json), skip HTML parsing.
[03.04.2025 09:12] Success.
[03.04.2025 09:12] Enriching papers with extra data.
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 0. Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. T...
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 1. Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of ML...
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 2. Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favo...
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 3. DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range...
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 4. Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degrada...
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 5. While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffu...
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 6. Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academi...
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 7. We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, gen...
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 8. We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. F...
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 9. Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate...
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 10. We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Fr...
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 11. The generation of high-quality human images through text-to-image (T2I) methods is a significant yet challenging task. Distinct from general image generation, human image synthesis must satisfy stringent criteria related to human pose, anatomy, and alignment with textual prompts, making it particula...
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 12. Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have...
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 13. Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tas...
[03.04.2025 09:12] ********************************************************************************
[03.04.2025 09:12] Abstract 14. Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies ha...
[03.04.2025 09:12] Read previous papers.
[03.04.2025 09:12] Generating reviews via LLM API.
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#cv"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "MergeVQ: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MergeVQ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (MIM) Ñ Ğ²ĞµĞºÑ‚
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#training", "#dataset", "#optimization", "#video", "#reasoning", "#multimodal"], "emoji": "ğŸ§ ", "ru": {"title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#diffusion", "#games", "#multimodal", "#video"], "emoji": "ğŸ®", "ru": {"title": "AnimeGamer: Ğ¿Ğ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€ Ğ°Ğ½Ğ¸Ğ¼Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸", "desc": "AnimeGamer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#rl", "#reasoning"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#video", "#diffusion", "#3d"], "emoji": "ğŸ¬", "ru": {"title": "VideoScene: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoScene - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization", "#3d", "#diffusion"], "emoji": "ğŸ­", "ru": {"title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¼Ğ¸Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹", "desc": "DreamActor-M1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ° Ğ¸Ñ
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#science", "#dataset", "#rag", "#multimodal", "#alignment"], "emoji": "ğŸ“", "ru": {"title": "ScholarCopilot: Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¸ÑÑŒĞ¼Ğ°", "desc": "ScholarCopilot - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#diffusion", "#cv", "#training", "#games", "#multimodal", "#benchmark"], "emoji": "ğŸ§ ", "ru": {"title": "ILLUME+: Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ", "desc": "ILLUME+ - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#open_source", "#agents", "#benchmark", "#survey"], "emoji": "ğŸ§ ", "ru": {"title": "PaperBench: Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ", "desc": "PaperBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#security", "#cv", "#diffusion", "#training", "#dataset", "#optimization", "#multimodal"], "emoji": "ğŸ›¡ï¸", "ru": {"title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑˆÑƒĞ¼Ğ¾Ğ²Ğ¾Ğ¹ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#diffusion", "#games", "#3d"], "emoji": "ğŸ¦¾", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹: Ğ¾Ñ‚ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ", "desc": "Articulated Kinematics Distillation (AKD) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑĞºĞµĞ»ĞµÑ‚Ğ½ÑƒÑ Ğ°Ğ½Ğ¸Ğ¼Ğ°
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#rlhf", "#dataset", "#optimization", "#training", "#diffusion", "#cv"], "emoji": "ğŸ§‘â€ğŸ¨", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ DPO", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Direct Preference Opti
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#cv"], "emoji": "ğŸ‘ï¸", "ru": {"title": "LSNet: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ 'Ğ¡Ğ¼Ğ¾Ñ‚Ñ€Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞ¹ÑÑ ÑƒĞ·ĞºĞ¾'", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ 
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#interpretability", "#training", "#math", "#agents", "#reasoning", "#inference"], "emoji": "ğŸ”", "ru": {"title": "VerifiAgent: ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VerifiAgent - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… 
[03.04.2025 09:12] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#multimodal", "#training", "#transfer_learning"], "emoji": "ğŸ”", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² (OoDD) Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.
[03.04.2025 09:12] Trying to get texts in Chinese.
[03.04.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.
[03.04.2025 09:12] Mistral response. {"id": "8e0550b2ef9e45dab2fd99c447a618d2", "object": "chat.completion", "created": 1743671520, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u53eb\u505a MergeVQ\u3002\u5b83\u7ed3\u5408\u4e86\u5411\u91cf\u91cf\u5316\u548c token \u5408\u5e76\u6280\u672f\uff0c\u65e8\u5728\u89e3\u51b3\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u8868\u793a\u5b66\u4e60\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002MergeVQ \u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u901a\u8fc7 token \u5408\u5e76\u6a21\u5757\u63d0\u53d6\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u5728\u89e3\u7801\u9636\u6bb5\u6062\u590d\u7ec6\u8282\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMergeVQ \u5728\u56fe\u50cf\u751f\u6210\u548c\u8868\u793a\u5b66\u4e60\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u6548\u7387\u9ad8\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728\u7f51\u4e0a\u516c\u5f00\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 307, "total_tokens": 462, "completion_tokens": 155}}
[03.04.2025 09:12] Response: è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå«åš MergeVQã€‚å®ƒç»“åˆäº†å‘é‡é‡åŒ–å’Œ token åˆå¹¶æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³å›¾åƒç”Ÿæˆè´¨é‡å’Œè¡¨ç¤ºå­¦ä¹ æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚MergeVQ åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡ token åˆå¹¶æ¨¡å—æå–è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åœ¨è§£ç é˜¶æ®µæ¢å¤ç»†èŠ‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMergeVQ åœ¨å›¾åƒç”Ÿæˆå’Œè¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œä¸”æ•ˆç‡é«˜ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨ç½‘ä¸Šå…¬å¼€ã€‚
[03.04.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå«åš MergeVQã€‚å®ƒç»“åˆäº†å‘é‡é‡åŒ–å’Œ token åˆå¹¶æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³å›¾åƒç”Ÿæˆè´¨é‡å’Œè¡¨ç¤ºå­¦ä¹ æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚MergeVQ åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡ token åˆå¹¶æ¨¡å—æå–è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åœ¨è§£ç é˜¶æ®µæ¢å¤ç»†èŠ‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMergeVQ åœ¨å›¾åƒç”Ÿæˆå’Œè¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œä¸”æ•ˆç‡é«˜ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨ç½‘ä¸Šå…¬å¼€ã€‚
[03.04.2025 09:12] Mistral response. {"id": "ffe8c2002ba34019aa8be68ffbc8fefc", "object": "chat.completion", "created": 1743671523, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u53eb\u505a MergeVQ\u3002\u5b83\u7ed3\u5408\u4e86\u5411\u91cf\u91cf\u5316\u548c token \u5408\u5e76\u6280\u672f\uff0c\u65e8\u5728\u89e3\u51b3\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u8868\u793a\u5b66\u4e60\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002MergeVQ \u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u901a\u8fc7 token \u5408\u5e76\u6a21\u5757\u63d0\u53d6\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u5728\u89e3\u7801\u9636\u6bb5\u6062\u590d\u7ec6\u8282\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMergeVQ \u5728\u56fe\u50cf\u751f\u6210\u548c\u8868\u793a\u5b66\u4e60\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u6548\u7387\u9ad8\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728\u7f51\u4e0a\u516c\u5f00\u3002\n\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng ji\u00e8sh\u00e0o le y\u012bzh\u01d2ng x\u012bn de t\u00faxi\u00e0ng sh\u0113ngch\u00e9ng m\u00f3x\u00edng, ji\u00e0ozu\u00f2 MergeVQ. T\u0101 ji\u0113h\u00e9 le xi\u00e0ngli\u00e0ng li\u00e0ngg\u00e9hu\u00e0 h\u00e9 token h\u00e9b\u00ecng j\u00ecsh\u00f9, zh\u01d0y\u00fa ji\u011bju\u00e9 t\u00faxi\u00e0ng sh\u0113ngch\u00e9ng zh\u00ecli\u00e0ng h\u00e9 bi\u01ceosh\u00ec xu\u00e9x\u00ed xi\u00e0oy\u00ec zh\u012bji\u0101n de p\u00edngh\u00e9ng w\u00e8nt\u00ed. MergeVQ z\u00e0i y\u00f9x\u00f9nli\u00e0n ji\u0113du\u00e0n t\u014dnggu\u00f2 token h\u00e9b\u00ecng m\u00f3ku\u00e0i t\u00edq\u01d4 y\u01d4y\u00ec x\u00ecnx\u012b, b\u00ecng z\u00e0i ji\u011bm\u01ce ji\u0113du\u00e0n hu\u012bf\u00f9 x\u00ecji\u00e8. Sh\u00edy\u00e0n ji\u00e9gu\u01d2 xi\u01censh\u00ec, MergeVQ z\u00e0i t\u00faxi\u00e0ng sh\u0113ngch\u00e9ng h\u00e9 bi\u01ceosh\u00ec xu\u00e9x\u00ed r\u00e8nw\u00f9 zh\u014dng d\u014du bi\u01ceoxi\u00e0n ch\u016bs\u00e8, qi\u011b xi\u00e0oy\u00ec g\u0101o. D\u00e0im\u01ce h\u00e9 m\u00f3x\u00edng ji\u0101ng z\u00e0i w\u01cengsh\u00e0ng g\u014dngk\u0101i."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 169, "total_tokens": 653, "completion_tokens": 484}}
[03.04.2025 09:12] Response: è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå«åš MergeVQã€‚å®ƒç»“åˆäº†å‘é‡é‡åŒ–å’Œ token åˆå¹¶æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³å›¾åƒç”Ÿæˆè´¨é‡å’Œè¡¨ç¤ºå­¦ä¹ æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚MergeVQ åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡ token åˆå¹¶æ¨¡å—æå–è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åœ¨è§£ç é˜¶æ®µæ¢å¤ç»†èŠ‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMergeVQ åœ¨å›¾åƒç”Ÿæˆå’Œè¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œä¸”æ•ˆç‡é«˜ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨ç½‘ä¸Šå…¬å¼€ã€‚

ZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ«zhÇ’ng xÄ«n de tÃºxiÃ ng shÄ“ngchÃ©ng mÃ³xÃ­ng, jiÃ ozuÃ² MergeVQ. TÄ jiÄ“hÃ© le xiÃ ngliÃ ng liÃ nggÃ©huÃ  hÃ© token hÃ©bÃ¬ng jÃ¬shÃ¹, zhÇyÃº jiÄ›juÃ© tÃºxiÃ ng shÄ“ngchÃ©ng zhÃ¬liÃ ng hÃ© biÇoshÃ¬ xuÃ©xÃ­ xiÃ oyÃ¬ zhÄ«jiÄn de pÃ­nghÃ©ng wÃ¨ntÃ­. MergeVQ zÃ i yÃ¹xÃ¹nliÃ n jiÄ“duÃ n tÅngguÃ² token hÃ©bÃ¬ng mÃ³kuÃ i tÃ­qÇ” yÇ”yÃ¬ xÃ¬nxÄ«, bÃ¬ng zÃ i jiÄ›mÇ jiÄ“duÃ n huÄ«fÃ¹ xÃ¬jiÃ¨. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, MergeVQ zÃ i tÃºxiÃ ng shÄ“ngchÃ©ng hÃ© biÇoshÃ¬ xuÃ©xÃ­ rÃ¨nwÃ¹ zhÅng dÅu biÇoxiÃ n chÅ«sÃ¨, qiÄ› xiÃ oyÃ¬ gÄo. DÃ imÇ hÃ© mÃ³xÃ­ng jiÄng zÃ i wÇngshÃ ng gÅngkÄi.
[03.04.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå«åš MergeVQã€‚å®ƒç»“åˆäº†å‘é‡é‡åŒ–å’Œ token åˆå¹¶æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³å›¾åƒç”Ÿæˆè´¨é‡å’Œè¡¨ç¤ºå­¦ä¹ æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚MergeVQ åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡ token åˆå¹¶æ¨¡å—æå–è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åœ¨è§£ç é˜¶æ®µæ¢å¤ç»†èŠ‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMergeVQ åœ¨å›¾åƒç”Ÿæˆå’Œè¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œä¸”æ•ˆç‡é«˜ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨ç½‘ä¸Šå…¬å¼€ã€‚
[03.04.2025 09:12] Mistral response. {"id": "092243ec061b462f873708e3bfd0ea34", "object": "chat.completion", "created": 1743671533, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u5411\u91cf\u91cf\u5316\", \"pinyin\": \"xi\u00e0ngli\u00e0ng li\u00e0ngzh\u00ec\", \"trans\": \"vector quantization\"},\n    {\"word\": \"token\", \"pinyin\": \"t\u014duk\u00e8n\", \"trans\": \"token\"},\n    {\"word\": \"\u5408\u5e76\", \"pinyin\": \"h\u00e9b\u00ecng\", \"trans\": \"merge\"},\n    {\"word\": \"\u65e8\u5728\", \"pinyin\": \"zh\u01d0z\u00e0i\", \"trans\": \"aim to\"},\n    {\"word\": \"\u5e73\u8861\", \"pinyin\": \"p\u00edngh\u00e9ng\", \"trans\": \"balance\"},\n    {\"word\": \"\u9884\u8bad\u7ec3\", \"pinyin\": \"y\u00f9 x\u00f9nli\u00e0n\", \"trans\": \"pre-training\"},\n    {\"word\": \"\u8bed\u4e49\", \"pinyin\": \"y\u01d4y\u00ec\", \"trans\": \"semantic\"},\n    {\"word\": \"\u89e3\u7801\", \"pinyin\": \"ji\u011bm\u01ce\", \"trans\": \"decode\"},\n    {\"word\": \"\u6062\u590d\", \"pinyin\": \"hu\u012bf\u00f9\", \"trans\": \"recover\"},\n    {\"word\": \"\u7ec6\u8282\", \"pinyin\": \"x\u00ecji\u00e9\", \"trans\": \"detail\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceoxi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u51fa\u8272\", \"pinyin\": \"ch\u016bs\u00e8\", \"trans\": \"outstanding\"},\n    {\"word\": \"\u6548\u7387\", \"pinyin\": \"xi\u00e0ol\u01dc\", \"trans\": \"efficiency\"},\n    {\"word\": \"\u516c\u5f00\", \"pinyin\": \"g\u014dngk\u0101i\", \"trans\": \"public\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 201, "total_tokens": 603, "completion_tokens": 402}}
[03.04.2025 09:12] Response: [
    {"word": "å‘é‡é‡åŒ–", "pinyin": "xiÃ ngliÃ ng liÃ ngzhÃ¬", "trans": "vector quantization"},
    {"word": "token", "pinyin": "tÅukÃ¨n", "trans": "token"},
    {"word": "åˆå¹¶", "pinyin": "hÃ©bÃ¬ng", "trans": "merge"},
    {"word": "æ—¨åœ¨", "pinyin": "zhÇzÃ i", "trans": "aim to"},
    {"word": "å¹³è¡¡", "pinyin": "pÃ­nghÃ©ng", "trans": "balance"},
    {"word": "é¢„è®­ç»ƒ", "pinyin": "yÃ¹ xÃ¹nliÃ n", "trans": "pre-training"},
    {"word": "è¯­ä¹‰", "pinyin": "yÇ”yÃ¬", "trans": "semantic"},
    {"word": "è§£ç ", "pinyin": "jiÄ›mÇ", "trans": "decode"},
    {"word": "æ¢å¤", "pinyin": "huÄ«fÃ¹", "trans": "recover"},
    {"word": "ç»†èŠ‚", "pinyin": "xÃ¬jiÃ©", "trans": "detail"},
    {"word": "è¡¨ç°", "pinyin": "biÇoxiÃ n", "trans": "performance"},
    {"word": "å‡ºè‰²", "pinyin": "chÅ«sÃ¨", "trans": "outstanding"},
    {"word": "æ•ˆç‡", "pinyin": "xiÃ olÇœ", "trans": "efficiency"},
    {"word": "å…¬å¼€", "pinyin": "gÅngkÄi", "trans": "public"}
]
[03.04.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå«åš MergeVQã€‚å®ƒç»“åˆäº†å‘é‡é‡åŒ–å’Œ token åˆå¹¶æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³å›¾åƒç”Ÿæˆè´¨é‡å’Œè¡¨ç¤ºå­¦ä¹ æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚MergeVQ åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡ token åˆå¹¶æ¨¡å—æå–è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åœ¨è§£ç é˜¶æ®µæ¢å¤ç»†èŠ‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMergeVQ åœ¨å›¾åƒç”Ÿæˆå’Œè¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œä¸”æ•ˆç‡é«˜ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨ç½‘ä¸Šå…¬å¼€ã€‚
[03.04.2025 09:12] Mistral response. {"id": "d573f439697f4922804276b5406c94e3", "object": "chat.completion", "created": 1743671547, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article introduces a new image generation model called MergeVQ. It combines vector quantization and token merging techniques to address the balance between image generation quality and representation learning efficiency. MergeVQ extracts semantic information through a token merging module during the pre-training phase and recovers details during the decoding phase. Experimental results show that MergeVQ performs excellently in both image generation and representation learning tasks, with high efficiency. The code and model will be made publicly available online."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 167, "total_tokens": 275, "completion_tokens": 108}}
[03.04.2025 09:12] Response: This article introduces a new image generation model called MergeVQ. It combines vector quantization and token merging techniques to address the balance between image generation quality and representation learning efficiency. MergeVQ extracts semantic information through a token merging module during the pre-training phase and recovers details during the decoding phase. Experimental results show that MergeVQ performs excellently in both image generation and representation learning tasks, with high efficiency. The code and model will be made publicly available online.
[03.04.2025 09:12] Renaming data file.
[03.04.2025 09:12] Renaming previous data. hf_papers.json to ./d/2025-04-03.json
[03.04.2025 09:12] Saving new data file.
[03.04.2025 09:12] Generating page.
[03.04.2025 09:12] Renaming previous page.
[03.04.2025 09:12] Renaming previous data. index.html to ./d/2025-04-03.html
[03.04.2025 09:12] [Experimental] Generating Chinese page for reading.
[03.04.2025 09:12] Chinese vocab [{'word': 'å‘é‡é‡åŒ–', 'pinyin': 'xiÃ ngliÃ ng liÃ ngzhÃ¬', 'trans': 'vector quantization'}, {'word': 'token', 'pinyin': 'tÅukÃ¨n', 'trans': 'token'}, {'word': 'åˆå¹¶', 'pinyin': 'hÃ©bÃ¬ng', 'trans': 'merge'}, {'word': 'æ—¨åœ¨', 'pinyin': 'zhÇzÃ i', 'trans': 'aim to'}, {'word': 'å¹³è¡¡', 'pinyin': 'pÃ­nghÃ©ng', 'trans': 'balance'}, {'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹nliÃ n', 'trans': 'pre-training'}, {'word': 'è¯­ä¹‰', 'pinyin': 'yÇ”yÃ¬', 'trans': 'semantic'}, {'word': 'è§£ç ', 'pinyin': 'jiÄ›mÇ', 'trans': 'decode'}, {'word': 'æ¢å¤', 'pinyin': 'huÄ«fÃ¹', 'trans': 'recover'}, {'word': 'ç»†èŠ‚', 'pinyin': 'xÃ¬jiÃ©', 'trans': 'detail'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇoxiÃ n', 'trans': 'performance'}, {'word': 'å‡ºè‰²', 'pinyin': 'chÅ«sÃ¨', 'trans': 'outstanding'}, {'word': 'æ•ˆç‡', 'pinyin': 'xiÃ olÇœ', 'trans': 'efficiency'}, {'word': 'å…¬å¼€', 'pinyin': 'gÅngkÄi', 'trans': 'public'}]
[03.04.2025 09:12] Renaming previous Chinese page.
[03.04.2025 09:12] Renaming previous data. zh.html to ./d/2025-04-02_zh_reading_task.html
[03.04.2025 09:12] Writing Chinese reading task.
[03.04.2025 09:12] Writing result.
[03.04.2025 09:12] Renaming log file.
[03.04.2025 09:12] Renaming previous data. log.txt to ./logs/2025-04-03_last_log.txt
