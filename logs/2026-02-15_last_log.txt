[14.02.2026 18:44] Read previous papers.
[14.02.2026 18:44] Generating top page (month).
[14.02.2026 18:44] Writing top page (month).
[15.02.2026 02:49] Read previous papers.
[15.02.2026 02:49] Get feed.
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09877
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12036
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12205
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12125
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10934
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12099
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09070
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12056
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11731
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12280
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11748
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11075
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09021
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12153
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10106
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05827
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12092
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05548
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11733
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11298
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08277
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12262
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11964
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07885
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12176
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11683
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08194
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11761
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11337
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11543
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10575
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12203
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12164
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12116
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11541
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11509
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11636
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11598
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10585
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09891
[15.02.2026 02:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11792
[15.02.2026 02:49] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.02.2026 02:49] No deleted papers detected.
[15.02.2026 02:49] Downloading and parsing papers (pdf, html). Total: 41.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.09877.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.09877.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.09877.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.12036.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.12036.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.12036.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.12205.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.12205.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.12205.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.12125.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.12125.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.12125.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.10934.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.10934.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.10934.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.12099.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.12099.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.12099.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.09070.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.09070.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.09070.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.12056.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.12056.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.12056.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11731.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11731.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11731.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.12280.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.12280.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.12280.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11748.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11748.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11748.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11075.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11075.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11075.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.09021.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.09021.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.09021.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.12153.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.12153.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.12153.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.10106.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.10106.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.10106.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.05827.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.05827.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.05827.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.12092.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.12092.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.12092.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.05548.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.05548.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.05548.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11733.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11733.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11733.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11298.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11298.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11298.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.08277.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.08277.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.08277.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.12262.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.12262.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.12262.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11964.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11964.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11964.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.07885.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.07885.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.07885.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.12176.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.12176.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.12176.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11683.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11683.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11683.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.08194.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.08194.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.08194.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11761.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11761.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11761.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11337.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11337.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11337.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11543.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11543.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11543.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.10575.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.10575.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.10575.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.12203.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.12203.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.12203.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.12164.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.12164.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.12164.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.12116.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.12116.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.12116.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11541.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11541.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11541.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11509.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11509.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11509.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11636.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11636.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11636.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11598.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11598.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11598.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.10585.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.10585.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.10585.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.09891.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.09891.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.09891.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Downloading and parsing paper https://huggingface.co/papers/2602.11792.
[15.02.2026 02:49] Extra JSON file exists (./assets/json/2602.11792.json), skip PDF parsing.
[15.02.2026 02:49] Paper image links file exists (./assets/img_data/2602.11792.json), skip HTML parsing.
[15.02.2026 02:49] Success.
[15.02.2026 02:49] Enriching papers with extra data.
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 0. Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution.  					AI-generated summary 				 The emergence of multi-agent systems built from large language models (LLMs)...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 1. Composition-RL improves reasoning capabilities by automatically composing multiple problems into new verifiable questions for reinforcement learning training.  					AI-generated summary 				 Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR),...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 2. A lightweight 5B unified multimodal model achieves competitive performance through hierarchical feature extraction, learnable think tokens, and progressive training strategies including alignment pre-training, joint supervised fine-tuning, and reinforcement learning with MR-GRPO.  					AI-generated ...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 3. On-policy distillation is extended through a generalized framework that introduces flexible reference models and reward scaling factors, demonstrating improved performance through reward extrapolation and reward correction techniques.  					AI-generated summary 				 On-policy distillation (OPD), whi...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 4. A fully end-to-end Transformer-based audio tokenizer architecture achieves high-fidelity reconstruction across diverse audio domains and enables superior text-to-speech and automatic speech recognition performance.  					AI-generated summary 				 Discrete audio tokenizers are fundamental to empoweri...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 5. A vision-language-action model enhanced with world model-based reinforcement learning demonstrates improved performance and long-horizon execution capabilities for robotic manipulation tasks.  					AI-generated summary 				 Vision-language-action (VLA) models that directly predict multi-step action ...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 6. NarraScore presents a hierarchical framework that uses frozen Vision-Language Models as affective sensors to generate coherent soundtracks for long-form videos by combining global semantic anchors with token-level adaptive modulation.  					AI-generated summary 				 Synthesizing coherent soundtracks...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 7. LawThinker is an autonomous legal research agent that uses an Explore-Verify-Memorize strategy with a DeepVerifier module to ensure accurate and procedurally compliant legal reasoning through dynamic verification of intermediate steps.  					AI-generated summary 				 Legal reasoning requires not onl...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 8. Visual reasoning is enhanced by reconstructing logical structures from compressed visual tokens through a DSL-based approach that generates deterministic visual proofs for verification.  					AI-generated summary 				 Existing multimodal large language models have achieved high-fidelity visual perce...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 9. Progressive Semantic Illusions use a generative framework with dual-branch Score Distillation Sampling to create vector sketches that transform semantically through sequential stroke additions, achieving superior recognizability and illusion strength.  					AI-generated summary 				 Visual illusions...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 10. Models require in-context exploration capabilities to scale effectively at test time, but autoregressive generation faces exponential decay in sampling long sequences, which is addressed by a length-incentivized exploration method that improves performance on both in-domain and out-of-domain tasks. ...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 11. RISE is a robotic reinforcement learning framework that uses a compositional world model to predict multi-view futures and evaluate imagined outcomes, enabling policy improvement through virtual interactions rather than physical trials.  					AI-generated summary 				 Despite the sustained scaling o...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 12. A resource-efficient robotic manipulation framework addresses distributional shifts through model arithmetic, stage-aware advantage estimation, and train-deploy alignment to achieve long-horizon task reliability.  					AI-generated summary 				 High-reliability long-horizon robotic manipulation has ...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 13. Diffusion large language models enable parallel token generation and efficient reasoning enhancement through a voting technique that identifies and refines uncertain predictions across multiple samples.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) represent a new paradigm ...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 14. EgoHumanoid enables humanoid loco-manipulation through co-training vision-language-action policies using egocentric human demonstrations and limited robot data, addressing embodiment gaps via view and action alignment techniques.  					AI-generated summary 				 Human demonstrations offer rich enviro...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 15. Vision-language navigation systems traditionally require detailed instructions but can be improved by incorporating video generation models with sparse future planning for faster, more efficient real-world deployment.  					AI-generated summary 				 Why must vision-language navigation be bound to de...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 16. DeepSight is an open-source project that integrates safety evaluation and diagnosis for large language and multimodal models, enabling white-box insights through unified protocols and specialized toolkits.  					AI-generated summary 				 As the development of Large Models (LMs) progresses rapidly, t...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 17. Asymmetric Group Relative Advantage Estimation addresses exploration and difficulty adaptation challenges in reinforcement learning with large language models by dynamically modulating exploration incentives and sample difficulty focus.  					AI-generated summary 				 Reinforcement Learning with Ver...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 18. General-purpose Vision-Language Models can be effectively adapted for e-commerce applications through targeted techniques that enhance product understanding while maintaining broad multimodal capabilities.  					AI-generated summary 				 E-commerce product understanding demands by nature, strong mul...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 19. Voxtral Realtime is a streaming speech recognition model trained end-to-end for sub-second latency with performance matching offline systems.  					AI-generated summary 				 We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription qual...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 20. Video diffusion model PISCO enables precise instance insertion with sparse keyframe control through variable-information guidance and distribution-preserving temporal masking.  					AI-generated summary 				 The landscape of AI video generation is undergoing a pivotal shift: moving beyond general ge...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 21. A trajectory self-distillation framework with direct discriminative optimization improves few-step decoding efficiency in diffusion large language models while maintaining generation quality.  					AI-generated summary 				 Diffusion large language models (DLLMs) have the potential to enable fast te...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 22. Gaia2 presents a benchmark for evaluating large language model agents in asynchronous, dynamic environments with temporal constraints and multi-agent collaboration, featuring a write-action verifier for reinforcement learning and revealing trade-offs between reasoning, efficiency, and robustness.  	...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 23. MemFly addresses the challenge of long-term memory in language models by using information bottleneck principles to create an adaptive memory structure with hybrid retrieval mechanisms for improved task performance.  					AI-generated summary 				 Long-term memory enables large language model agents...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 24. Single-minus tree-level n-gluon scattering amplitudes are reconsidered. Often presumed to vanish, they are shown here to be nonvanishing for certain "half-collinear" configurations existing in Klein space or for complexified momenta. We derive a piecewise-constant closed-form expression for the deca...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 25. ThinkRouter is a confidence-aware routing mechanism that improves reasoning efficiency by switching between discrete token and latent spaces based on model confidence, achieving better accuracy and faster generation.  					AI-generated summary 				 Recent work explores latent reasoning to improve re...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 26. Foundation models generate executable environment code to scaffold learning progress in open-ended worlds, enabling agents to acquire long-horizon skills through curriculum control.  					AI-generated summary 				 Open-ended learning frames intelligence as emerging from continual interaction with an...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 27. MiniCPM-SALA combines sparse and linear attention mechanisms in a hybrid architecture to enable efficient processing of ultra-long contexts while maintaining model performance and reducing training costs.  					AI-generated summary 				 The evolution of large language models (LLMs) towards applicati...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 28. MolmoSpaces presents an open ecosystem with diverse indoor environments and annotated objects for large-scale robot policy benchmarking across multiple tasks and simulators.  					AI-generated summary 				 Deploying robots at scale demands robustness to the long tail of everyday situations. The coun...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 29. A memory-efficient decentralized framework for training mixture-of-experts language models using sparse expert synchronization and expert-merging warm-up strategies.  					AI-generated summary 				 Pretraining large language models (LLMs) typically requires centralized clusters with thousands of hig...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 30. MetaphorStar, an end-to-end visual reinforcement learning framework, significantly enhances metaphor comprehension in images through a specialized dataset, RL method, and benchmark, achieving state-of-the-art performance on multiple visual reasoning tasks.  					AI-generated summary 				 Metaphorica...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 31. A new benchmark dataset called ExStrucTiny is introduced for structured information extraction from document images, addressing limitations of existing datasets and evaluating vision-language models on diverse document types and flexible schemas.  					AI-generated summary 				 Enterprise documents,...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 32. Sci-CoE is a two-stage scientific co-evolving framework that enables large language models to self-evolve as both solver and verifier through sparse-to-unsupervised learning transitions, improving scientific reasoning capabilities and evaluation system robustness.  					AI-generated summary 				 Lar...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 33. Personalized generative reward models address challenges in adapting language model responses to individual user preferences by using structured evaluation chains and dual-granularity scaling mechanisms for improved generalization and accuracy.  					AI-generated summary 				 Personalized alignment ...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 34. Budget-constrained tool-augmented agents use a hierarchical world model and intent-aware planning to optimize multi-step task completion under monetary constraints.  					AI-generated summary 				 We study budget-constrained tool-augmented agents, where a large language model must solve multi-step t...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 35. MuRGAt is a benchmark for evaluating fact-level multimodal attribution in complex reasoning tasks, requiring models to provide precise citations for their answers across video, audio, and other modalities.  					AI-generated summary 				 Multimodal large language models (MLLMs) are increasingly used...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 36. ScalSelect is a scalable training-free method for selecting representative multimodal data that achieves near-full-dataset performance with significantly reduced computational requirements.  					AI-generated summary 				 Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for adva...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 37. A unified Vision-Language-Action model with a hierarchical architecture combining semantic reasoning and continuous trajectory generation achieves state-of-the-art performance across multiple embodied navigation tasks.  					AI-generated summary 				 Embodied navigation has long been fragmented by t...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 38. Neural Additive Experts combines multiple specialized networks with a dynamic gating mechanism to balance predictive accuracy and feature interpretability in machine learning models.  					AI-generated summary 				 The trade-off between interpretability and accuracy remains a core challenge in machi...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 39. Stemphonic is a diffusion- and flow-based framework that generates variable sets of synchronized musical stems in single inference passes, improving both quality and efficiency over existing methods.  					AI-generated summary 				 Music stem generation, the task of producing musically-synchronized ...
[15.02.2026 02:49] ********************************************************************************
[15.02.2026 02:49] Abstract 40. Reinforcement learning with verifiable rewards induces behavioral signatures that can be detected using a black-box method based on prompt generation diversity, outperforming existing contamination detection approaches.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (...
[15.02.2026 02:49] Read previous papers.
[15.02.2026 02:49] Generating reviews via LLM API.
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#alignment", "#training", "#agents", "#security"], "emoji": "‚öñÔ∏è", "ru": {"title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–µ–π –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞, —á—Ç–æ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –Ω–µ –º–æ–≥—É—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –¥–æ—Å—Ç–∏—á—å 
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#rl", "#training", "#reasoning", "#optimization"], "emoji": "üß©", "ru": {"title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏—è –∑–∞–¥–∞—á –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Composition-RL ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –ª–æ–≥–∏—á–µ—Å–∫
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#rlhf", "#training", "#alignment", "#open_source", "#small_models", "#dataset", "#optimization"], "emoji": "üé®", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–æ–π –º–æ—â—å—é: 5B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±–≥–æ–Ω—è—é—Ç 80B –≤ –∫–∞—á–µ—Å—Ç–≤–µ", "desc": "DeepGen 1.0 ‚Äî —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è 5-–º–∏–ª–ª–∏–∞—Ä–¥–Ω–∞
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#alignment", "#training", "#rl", "#optimization", "#reasoning"], "emoji": "üéì", "ru": {"title": "–£–º–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è: –∫–∞–∫ —É—á–µ–Ω–∏–∫ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —É—á–∏—Ç–µ–ª—è —á–µ—Ä–µ–∑ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è", "desc": "–†–∞–±–æ—Ç–∞ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ ¬´–Ω–∞ –ø–æ–ª–∏—Ç–∏–∫–µ¬ª (on-policy distillation), –ø—Ä–µ–¥–ª–∞–≥–∞—è –æ–±–æ–±—â—ë–Ω
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#optimization", "#audio", "#training", "#architecture"], "emoji": "üéµ", "ru": {"title": "–°–∫–≤–æ–∑–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é –æ–¥–Ω–æ—Ä–æ–¥–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ CAT (Causal Audio Tokenizer with Transformer) ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–Ω–∞
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#robotics", "#rl", "#cv", "#benchmark", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ú–∏—Ä–æ–≤–∞—è –º–æ–¥–µ–ª—å –∫–∞–∫ –æ—Å–Ω–æ–≤–∞ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è GigaBrain-0.5M*, –º–æ–¥–µ–ª—å Vision-Language-Action, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–µ–Ω–∞ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#multimodal", "#long_context", "#audio", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–≠–º–æ—Ü–∏–∏ –∫–∞–∫ –∫–æ–º–ø—Ä–µ—Å—Å –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è: —É–º–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º—É–∑—ã–∫–∏ –¥–ª—è –≤–∏–¥–µ–æ", "desc": "NarraScore ‚Äî —ç—Ç–æ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –º—É–∑—ã–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä—Ç–∏—Ç—É—Ä –∫ –¥–ª–∏–Ω–Ω—ã–º –≤–∏–¥–µ–æ
[15.02.2026 02:49] Using data from previous issue: {"categories": [], "emoji": "‚öñÔ∏è", "ru": {"title": "–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–≥–µ–Ω—Ç —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "LawThinker ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Explore-Verify-Memorize —Å –º–æ–¥—É–ª–µ–º DeepVerifier –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ –∏
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#cv", "#interpretability", "#architecture", "#reasoning"], "emoji": "üî¨", "ru": {"title": "–û—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤—É —á–µ—Ä–µ–∑ –∫–æ–¥", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#multimodal"], "emoji": "üé®", "ru": {"title": "–û—Ç —É—Ç–∫–∏ –∫ –æ–≤—Ü–µ: –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∏–ª–ª—é–∑–∏–∏ —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —à—Ç—Ä–∏—Ö–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∏–ª–ª—é–∑–∏–π ‚Äî –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç—Å–∫–∏–∑–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–µ–Ω—è—é—Ç —Å–≤–æ—ë –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ 
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "üîç", "ru": {"title": "–ü–æ–æ—â—Ä–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤–æ –≤—Ä–µ–º—è –∞–≤—Ç–æ–º–∞
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#training", "#robotics", "#multimodal", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–í–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –æ–ø—ã—Ç–∞: –æ–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞ –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "RISE ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#training", "#robotics", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç-–º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç œá‚ÇÄ ‚Äî —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º-–º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–º –Ω–∞ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–µ–π—Å—Ç–≤–∏–π. –û
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#reasoning", "#diffusion", "#open_source"], "emoji": "üó≥Ô∏è", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (dLLM), –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç—Ö–æ–¥—è—Ç –æ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –≥–µ–Ω–µ—Ä–∏
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#multimodal", "#robotics", "#training", "#synthetic", "#transfer_learning", "#data"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç —Ä—É–∫ —á–µ–ª–æ–≤–µ–∫–∞ –∫ –ª–æ–≤–∫–æ—Å—Ç–∏ –≥—É–º–∞–Ω–æ–∏–¥–∞ —á–µ—Ä–µ–∑ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö", "desc": "EgoHumanoid –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è vision-language-action –ø–æ–ª–∏
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#video", "#inference"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç –≤–∏–¥–µ–æ–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫ —Ä–µ–∞–ª—å–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏: —Ä–µ–¥–∫–∏–µ –±—É–¥—É—â–∏–µ –∫–∞–¥—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö, –≥–¥–µ —Ä–æ–±–æ—Ç –¥–æ–ª–∂–µ–Ω 
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#security", "#benchmark", "#alignment", "#open_source", "#dataset"], "emoji": "üîç", "ru": {"title": "–û—Ç —á—ë—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞ –∫ –±–µ–ª–æ–º—É: –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "DeepSight ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π –ø—Ä–æ–µ–∫—Ç –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#optimization", "#reasoning"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ê—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è exploration –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Asymmetric GRAE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑
[15.02.2026 02:49] Using data from previous issue: {"categories": [], "emoji": "üõçÔ∏è", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –æ–±—â–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö Vision-Language –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —à–∏—Ä–æ–∫–∏—Ö –º—É
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#audio", "#open_source", "#low_resource", "#training", "#architecture", "#multilingual"], "emoji": "üéôÔ∏è", "ru": {"title": "–ü–æ—Ç–æ–∫–æ–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –≤ –ø–æ–ª—Å–µ–∫—É–Ω–¥—ã", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Voxtral Realtime ‚Äî –ø–æ—Ç–æ–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏, –æ–±—É—á–µ–Ω–Ω–∞—è end-to-en
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–¢–æ—á–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å —Ä–µ–¥–∫–∏–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–ª—é—á–µ–≤—ã–º–∏ –∫–∞–¥—Ä–∞–º–∏", "desc": "PISCO ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—Å—Ç–∞–≤–ª—è—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –≤–∏–¥–µ–æ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–°–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–Ω–æ–≥–æ—Ç–æ–∫–µ–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ —Å–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#rl", "#benchmark", "#optimization", "#agents", "#reasoning"], "emoji": "‚è±Ô∏è", "ru": {"title": "–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –ø–æ–¥ –¥–∞–≤–ª–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏: –∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã –º–µ–∂–¥—É —Ä–∞–∑—É–º–æ–º, —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å—é", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Gaia2 ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ 
[15.02.2026 02:49] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å LLM —á–µ—Ä–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–∑–∫–æ–µ –º–µ—Å—Ç–æ", "desc": "MemFly —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –ø—Ä–∏–º–µ–Ω—è—è –ø—Ä–∏–Ω—Ü–∏–ø—ã –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —É–∑–∫–æ–≥–æ –º–µ—Å—Ç–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–∞–º—è—Ç–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç —ç–Ω—Ç—Ä–æ–ø–∏—é 
[15.02.2026 02:49] Using data from previous issue: {"categories": [], "emoji": "‚öõÔ∏è", "ru": {"title": "–ù–µ–Ω—É–ª–µ–≤—ã–µ –∞–º–ø–ª–∏—Ç—É–¥—ã –≥–ª—é–æ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å–µ—è–Ω–∏—è –≤ —ç–∫–∑–æ—Ç–∏—á–µ—Å–∫–∏—Ö –∫–∏–Ω–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–µ—Ä–µ—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∞–º–ø–ª–∏—Ç—É–¥—ã —Ä–∞—Å—Å–µ—è–Ω–∏—è n –≥–ª—é–æ–Ω–æ–≤ —Å –æ–¥–Ω–æ–π –º–∏–Ω—É—Å-—Å–ø–∏—Ä–∞–ª—å–Ω–æ—Å—Ç—å—é –Ω–∞ —É—Ä–æ–≤–Ω–µ –¥–µ—Ä–µ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–Ω–µ–µ —Å—á–∏—Ç–∞–ª–∏—Å—å —Ä–∞–≤–Ω—ã–º–∏ –Ω—É–ª—é. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "üõ£Ô∏è", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è: –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É —è–≤–Ω—ã–º –∏ —Å–∫—Ä—ã—Ç—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏", "desc": "ThinkRouter ‚Äî —ç—Ç–æ –º–µ—Ö–∞–Ω–∏–∑–º –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –º–µ–∂–¥—É –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#training", "#agents"], "emoji": "üß†", "ru": {"title": "–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–∏—Ä–æ–≤ —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –∫–æ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Dreaming in Code (DiCode) ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –≤ –∫–æ—Ç–æ—Ä–æ–º –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π 
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#small_models", "#training", "#architecture", "#long_context", "#optimization", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —É–ª—å—Ç—Ä–∞-–¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "MiniCPM-SALA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ—Ç–æ—Ä–∞—è –æ
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#open_source", "#benchmark", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–∏—Ç–∏–∫", "desc": "MolmoSpaces –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Ç–∫—Ä—ã—Ç—É—é —ç–∫–æ—Å–∏—Å—Ç–µ–º—É —Å –±–æ–ª–µ–µ —á–µ–º 230 —Ç—ã—Å—è—á–∞–º–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä
[15.02.2026 02:49] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Å —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –±–µ–∑ —Å—É–ø–µ—Ä–∫–ª–∞—Å—Ç–µ—Ä–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ SPES ‚Äî –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π mixture-of-experts, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –Ω–∞ GPU –ø—É—Ç—ë–º
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#training", "#benchmark", "#multimodal", "#rl", "#open_source", "#optimization", "#dataset", "#reasoning"], "emoji": "üé®", "ru": {"title": "–ù–∞—É—á–∏—Ç—å –ò–ò –ø–æ–Ω–∏–º–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ —Å–º—ã—Å–ª—ã: –º–µ—Ç–∞—Ñ–æ—Ä—ã –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "MetaphorStar ‚Äî —ç—Ç–æ –ø–µ—Ä–≤—ã–π end-to-end —Ñ—Ä–µ–π–º
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#benchmark", "#synthetic", "#multimodal", "#cv"], "emoji": "üìã", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–∏–±–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ExStrucTiny –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#reasoning", "#math", "#science", "#open_source", "#benchmark", "#training"], "emoji": "üß¨", "ru": {"title": "–ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —Å–∞–º–∞ —Å–µ–±—è –ø—Ä–æ–≤–µ—Ä—è—Ç—å: –∫–æ—ç–≤–æ–ª—é—Ü–∏—è —Ä–µ—à–∞—Ç–µ–ª—è –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "Sci-CoE –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å–∏—Å—Ç–µ–º—É –Ω–∞—É—á–Ω–æ–≥–æ —Å–æ-—ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–≥
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#alignment", "#transfer_learning"], "emoji": "üéØ", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ –æ—Ü–µ–Ω–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç P-GenRM ‚Äî –ø–µ—Ä–≤—É—é –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –±–æ–ª—å—à
[15.02.2026 02:49] Using data from previous issue: {"categories": [], "emoji": "üí∞", "ru": {"title": "–≠–∫–æ–Ω–æ–º–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –Ω–∞–º–µ—Ä–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ INTENT –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ—à–∞—é—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–Ω–µ—à–Ω–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏ —Å—Ç—Ä–æ–≥–æ–º –±—é
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#long_context", "#hallucinations", "#audio", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º–æ—Å—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ç–æ—á–Ω–æ–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", "desc": "MuRGAt ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#multimodal", "#training", "#data"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ScalSelect ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã—Ö –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ 
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#transfer_learning", "#3d", "#architecture", "#synthetic", "#benchmark", "#multimodal", "#agents", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å Vision-Language-Action (VLA)
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#interpretability", "#open_source"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å—é —á–µ—Ä–µ–∑ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ —Å–º–µ—Å–∏", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Neural Additive Experts (NAE) ‚Äî –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫—É—é –¥–∏–ª–µ–º–º—É –º–µ–∂–¥—É –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#inference", "#training", "#audio", "#open_source", "#diffusion"], "emoji": "üéµ", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º—É–∑—ã–∫–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ –æ–¥–Ω–æ–º –ø—Ä–æ—Ö–æ–¥–µ", "desc": "Stemphonic ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ flow-based –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω
[15.02.2026 02:49] Using data from previous issue: {"categories": ["#security", "#benchmark", "#reasoning", "#rl"], "emoji": "üîç", "ru": {"title": "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–º–∏–Ω–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∫–æ–Ω—Ç–∞–º–∏–Ω–∞—Ü–∏–∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –≤—ã–∑–≤–∞–Ω–Ω–æ–π –æ–±—É—á–µ–Ω–∏–µ–º —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω
[15.02.2026 02:49] Renaming data file.
[15.02.2026 02:49] Renaming previous data. hf_papers.json to ./d/2026-02-13.json
[15.02.2026 02:49] Saving new data file.
[15.02.2026 02:49] Generating page.
[15.02.2026 02:49] Renaming previous page.
[15.02.2026 02:49] Renaming previous data. index.html to ./d/2026-02-13.html
[15.02.2026 02:49] Writing result.
[15.02.2026 02:49] Renaming log file.
[15.02.2026 02:49] Renaming previous data. log.txt to ./logs/2026-02-15_last_log.txt
