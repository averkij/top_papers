[10.10.2025 17:11] Read previous papers.
[10.10.2025 17:11] Generating top page (month).
[10.10.2025 17:11] Writing top page (month).
[10.10.2025 18:16] Read previous papers.
[10.10.2025 18:16] Get feed.
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08558
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08540
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03279
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08377
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23768
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03259
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08555
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07499
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08240
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07242
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07172
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08551
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08483
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08191
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08308
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08211
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08143
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08565
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07546
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03222
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08485
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03663
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08529
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06915
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08002
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03117
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08425
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08431
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08276
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08559
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08549
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08203
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08008
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08556
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07958
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07790
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07429
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24817
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06209
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08547
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08271
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07314
[10.10.2025 18:16] Extract page data from URL. URL: https://huggingface.co/papers/2510.06679
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02994
[10.10.2025 18:16] Extract page data from URL. URL: https://huggingface.co/papers/2509.26633
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24797
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23500
[10.10.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07048
[10.10.2025 18:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.10.2025 18:16] No deleted papers detected.
[10.10.2025 18:16] Downloading and parsing papers (pdf, html). Total: 48.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08558.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08558.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08558.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08540.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08540.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08540.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.03279.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.03279.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.03279.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08377.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08377.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08377.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.23768.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2509.23768.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2509.23768.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.03259.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.03259.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.03259.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08555.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08555.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08555.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.07499.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.07499.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.07499.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08240.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08240.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08240.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.07242.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.07242.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.07242.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.07172.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.07172.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.07172.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08551.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08551.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08551.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08483.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08483.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08483.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08191.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08191.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08191.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08308.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08308.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08308.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08211.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08211.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08211.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08143.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08143.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08143.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08565.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08565.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08565.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.07546.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.07546.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.07546.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.03222.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.03222.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.03222.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08485.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08485.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08485.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.03663.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.03663.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.03663.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08529.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08529.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08529.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.06915.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.06915.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.06915.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08002.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08002.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08002.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.03117.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.03117.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.03117.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08425.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08425.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08425.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08431.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08431.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08431.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08276.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08276.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08276.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08559.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08559.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08559.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08549.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08549.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08549.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08203.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08203.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08203.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08008.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08008.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08008.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08556.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08556.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08556.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.07958.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.07958.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.07958.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.07790.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.07790.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.07790.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.07429.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.07429.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.07429.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2509.24817.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2509.24817.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2509.24817.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.06209.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.06209.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.06209.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08547.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08547.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08547.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.08271.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.08271.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.08271.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.07314.
[10.10.2025 18:16] Extra JSON file exists (./assets/json/2510.07314.json), skip PDF parsing.
[10.10.2025 18:16] Paper image links file exists (./assets/img_data/2510.07314.json), skip HTML parsing.
[10.10.2025 18:16] Success.
[10.10.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2510.06679.
[10.10.2025 18:16] Downloading paper 2510.06679 from http://arxiv.org/pdf/2510.06679v1...
[10.10.2025 18:17] Extracting affiliations from text.
[10.10.2025 18:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 ] . [ 1 9 7 6 6 0 . 0 1 5 2 : r Work in progress DREAMOMNI2: MULTIMODAL INSTRUCTION-BASED EDITING AND GENERATION Bin Xia1,4, Bohao Peng1, Yuechen Zhang1, Junjia Huang4, Jiyang Liu4, Jingyao Li1, Haoru Tan3, Sitong Wu1, Chengyao Wang1, Yitong Wang4, Xinglong Wu4, Bei Yu1, and Jiaya Jia2 1CUHK 2HKUST 3HKU 4ByteDance Inc https://github.com/dvlab-research/DreamOmni2 Figure 1: The gallery of overview: Enabling multimodal instruction-based editing and generation, extending beyond concrete objects to abstract attributions. "
[10.10.2025 18:17] Response: ```python
["CUHK", "HKUST", "HKU", "ByteDance Inc"]
```
[10.10.2025 18:17] Deleting PDF ./assets/pdf/2510.06679.pdf.
[10.10.2025 18:17] Success.
[10.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.02994.
[10.10.2025 18:17] Extra JSON file exists (./assets/json/2510.02994.json), skip PDF parsing.
[10.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.02994.json), skip HTML parsing.
[10.10.2025 18:17] Success.
[10.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2509.26633.
[10.10.2025 18:17] Downloading paper 2509.26633 from http://arxiv.org/pdf/2509.26633v2...
[10.10.2025 18:17] Extracting affiliations from text.
[10.10.2025 18:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction Lujie Yang*1,2, Xiaoyu Huang*1,3, Zhen Wu*1, Angjoo Kanazawa1,3, Pieter Abbeel1,3, Carmelo Sferrazza1, C. Karen Liu1,4, Rocky Duan1, Guanya Shi1,5 1Amazon FAR (Frontier AI & Robotics), 2MIT, 3UC Berkeley, 4Stanford University, 5CMU 5 2 0 2 8 ] . [ 2 3 3 6 6 2 . 9 0 5 2 : r Fig. 1: OMNIRETARGET enables reinforcement learning policies to learn complex, long-horizon loco-manipulation skills in challenging environments that transfer zero-shot from simulation to Unitree G1 humanoid. Thanks to the high-quality interaction-preserving motion retargeting, these policies are trained and deployed in minimal and unified way: it involves only 5 rewards, 4 robot domain randomization terms, and purely proprioceptive observation space, shared by all tasks. Demonstrated behaviors include (a) 30-second parkour course involving chair moving, stepping & vault, and jump & roll, (b) object transportation, (c) crawling on slope, and (d) fast platform climbing and sitting. Abstract dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OMNIRETARGET, an interactionpreserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OMNIRETAR"
[10.10.2025 18:17] Response: ```python
[
    "Amazon FAR (Frontier AI & Robotics)",
    "MIT",
    "UC Berkeley",
    "Stanford University",
    "CMU"
]
```
[10.10.2025 18:17] Deleting PDF ./assets/pdf/2509.26633.pdf.
[10.10.2025 18:17] Success.
[10.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2509.24797.
[10.10.2025 18:17] Extra JSON file exists (./assets/json/2509.24797.json), skip PDF parsing.
[10.10.2025 18:17] Paper image links file exists (./assets/img_data/2509.24797.json), skip HTML parsing.
[10.10.2025 18:17] Success.
[10.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2509.23500.
[10.10.2025 18:17] Extra JSON file exists (./assets/json/2509.23500.json), skip PDF parsing.
[10.10.2025 18:17] Paper image links file exists (./assets/img_data/2509.23500.json), skip HTML parsing.
[10.10.2025 18:17] Success.
[10.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.07048.
[10.10.2025 18:17] Extra JSON file exists (./assets/json/2510.07048.json), skip PDF parsing.
[10.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.07048.json), skip HTML parsing.
[10.10.2025 18:17] Success.
[10.10.2025 18:17] Enriching papers with extra data.
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 0. Early experience, using agent-generated interaction data without reward signals, improves policy effectiveness and generalization, serving as a bridge between imitation learning and reinforcement learning.  					AI-generated summary 				 A long-term goal of language agents is to learn and improve th...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 1. Existing Multimodal Large Language Models show performance deficits in long-chain reflective reasoning, which is addressed by developing MM-HELIX-100K and Adaptive Hybrid Policy Optimization, leading to improved accuracy and generalization.  					AI-generated summary 				 While current Multimodal La...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 2. MemMamba, a novel architecture integrating state summarization and cross-attention, improves long-range memory and efficiency in sequence modeling compared to Mamba and Transformers.  					AI-generated summary 				 With the explosive growth of data, long-sequence modeling has become increasingly imp...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 3. UniVideo, a dual-stream framework combining a Multimodal Large Language Model and a Multimodal DiT, extends unified modeling to video generation and editing, achieving state-of-the-art performance and supporting task composition and generalization.  					AI-generated summary 				 Unified multimodal ...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 4. ChemMAS, a multi-agent system, improves reaction condition recommendation by providing interpretable rationales, outperforming existing methods in accuracy and explainability.  					AI-generated summary 				 The chemical reaction recommendation is to select proper reaction condition parameters for c...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 5. A training pipeline called MASA enhances meta-awareness in reasoning models, leading to improved accuracy and efficiency across various benchmarks.  					AI-generated summary 				 Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by it...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 6. VideoCanvas addresses temporal ambiguity in latent video diffusion models to enable flexible spatio-temporal video completion using a hybrid conditioning strategy.  					AI-generated summary 				 We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arb...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 7. Thought templates enhance long-context language models by structuring evidence combination and guiding multi-hop inference, leading to consistent performance improvements across various benchmarks.  					AI-generated summary 				 Recent Long-Context Language Models (LCLMs) can process hundreds of th...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 8. WaltzRL, a multi-agent reinforcement learning framework, improves LLM safety and helpfulness by collaboratively training a conversation agent and a feedback agent, reducing unsafe responses and overrefusals.  					AI-generated summary 				 Harnessing the power of LLMs requires a delicate dance betwe...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 9. HERO, a reinforcement learning framework, combines verifier signals with reward-model scores to enhance reasoning in large language models, outperforming both RM-only and verifier-only methods.  					AI-generated summary 				 Post-training for reasoning of large language models (LLMs) increasingly r...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 10. NewtonBench is a benchmark for scientific law discovery that addresses scalability, scientific relevance, and memorization resistance by using metaphysical shifts and interactive model discovery.  					AI-generated summary 				 Large language models are emerging as powerful tools for scientific law ...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 11. ARTDECO combines feed-forward models and SLAM pipelines for efficient and accurate 3D reconstruction from monocular images.  					AI-generated summary 				 On-the-fly 3D reconstruction from monocular image sequences is a long-standing challenge in computer vision, critical for applications such as r...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 12. DeepPrune, a novel framework using dynamic pruning and a specialized judge model, significantly reduces computational inefficiency in parallel scaling of large language models by pruning redundant reasoning traces.  					AI-generated summary 				 Parallel scaling has emerged as a powerful paradigm t...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 13. Training-Free GRPO enhances LLM agent performance in specialized domains by learning experiential knowledge as a token prior without parameter updates, improving out-of-domain tasks with minimal data.  					AI-generated summary 				 Recent advances in Large Language Model (LLM) agents have demonstra...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 14. Analysis of reflective behaviors in reasoning models shows that reflections primarily confirm initial answers, and training with more reflections improves first-answer correctness; a question-aware early-stopping method reduces unnecessary reflections and tokens with minimal accuracy loss.  					AI-...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 15. LLMs finetuned on misaligned data exhibit dishonest behavior, which can be exacerbated in downstream tasks and human-AI interactions.  					AI-generated summary 				 Previous research has shown that LLMs finetuned on malicious or incorrect completions within narrow domains (e.g., insecure code or in...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 16. UniMMVSR is a unified generative video super-resolution framework that incorporates hybrid-modal conditions, including text, images, and videos, within a latent video diffusion model, achieving superior detail and conformity to multi-modal conditions.  					AI-generated summary 				 Cascaded video s...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 17. Native end-to-end training of Multimodal Large Language Models (MLLMs) achieves competitive performance with a balanced design and scaling relationship between visual encoders and LLMs.  					AI-generated summary 				 Compositional training has been the de-facto paradigm in existing Multimodal Large...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 18. PickStyle uses diffusion models with style adapters and synthetic video clips to perform video style transfer from text prompts, preserving context and style.  					AI-generated summary 				 We address the task of video style transfer with diffusion models, where the goal is to preserve the context ...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 19. Low-probability Regularization (Lp-Reg) enhances exploration in Reinforcement Learning with Verifiable Rewards (RLVR) by preserving valuable low-probability tokens, leading to improved performance in complex reasoning tasks.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewa...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 20. InstructX integrates Multimodal Large Language Models and diffusion models for instruction-driven image and video editing, achieving state-of-the-art performance across diverse tasks.  					AI-generated summary 				 With recent advances in Multimodal Large Language Models (MLLMs) showing strong visu...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 21. UniDoc-Bench is a large-scale benchmark for multimodal retrieval-augmented generation, evaluating systems across text, images, and their fusion in real-world document-centric scenarios.  					AI-generated summary 				 Multimodal retrieval-augmented generation (MM-RAG) is a key approach for applying ...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 22. Co-Evolving Multi-Agent Systems (CoMAS) enable LLM-based agents to improve autonomously through inter-agent interactions and intrinsic rewards, achieving state-of-the-art performance.  					AI-generated summary 				 Self-evolution is a central research topic in enabling large language model (LLM)-ba...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 23. A benchmark and training strategy for reward models to improve long-context consistency and performance in large language models.  					AI-generated summary 				 Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasin...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 24. MUSE, a novel agent framework with a hierarchical Memory Module, enables continuous learning and self-evolution, achieving state-of-the-art performance on long-horizon productivity tasks using a lightweight model.  					AI-generated summary 				 Large Language Models have demonstrated remarkable cap...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 25. A novel dual-tower diffusion transformer with a Dual CrossAttention mechanism addresses challenges in Text-to-Sounding-Video generation by disentangling captions and enabling symmetric information exchange.  					AI-generated summary 				 This study focuses on a challenging yet promising task, Text-...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 26. DGPO, a new online RL algorithm, enhances diffusion models by learning from group-level preferences, enabling the use of efficient deterministic ODE samplers and achieving faster training and superior performance.  					AI-generated summary 				 While reinforcement learning methods such as Group Rel...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 27. Score-regularized continuous-time consistency model (rCM) improves large-scale diffusion distillation by addressing fine-detail generation and diversity issues, achieving high fidelity and accelerated sampling.  					AI-generated summary 				 This work represents the first effort to scale up continu...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 28. DeepMiner, a framework using high-difficulty training tasks and dynamic context management, enhances multi-turn reasoning agents through reinforcement learning, achieving significant performance improvements across benchmarks.  					AI-generated summary 				 While recent advances in reasoning models...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 29. SciVideoBench is a benchmark designed to evaluate advanced video reasoning in scientific contexts, challenging models with sophisticated domain-specific knowledge and logical reasoning.  					AI-generated summary 				 Large Multimodal Models (LMMs) have achieved remarkable progress across various ca...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 30. ERA, a new paradigm using specially designed activations, enhances performance across LLMs, reinforcement learning, and image classification with minimal computational overhead.  					AI-generated summary 				 We propose ERA, a new paradigm that constrains the sampling entropy above given thresholds...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 31. Function tokens in large language models activate predictive features during inference and guide memory consolidation during pre-training by predicting subsequent content tokens.  					AI-generated summary 				 The remarkable success of large language models (LLMs) stems from their ability to consol...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 32. Recycling pretrained checkpoints through orthogonal growth methods improves large language model performance with reduced computational cost.  					AI-generated summary 				 The rapidly increasing computational cost of pretraining Large Language Models necessitates more efficient approaches. Numerou...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 33. A novel framework enables a single simulation-trained policy to generalize to diverse real-world object rotations by learning joint-wise dynamics and autonomously collecting data.  					AI-generated summary 				 Achieving generalized in-hand object rotation remains a significant challenge in robotic...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 34. A$^2$Search is an annotation-free framework that handles ambiguity in open-domain QA by detecting ambiguous questions, gathering alternative answers, and optimizing with RL, achieving state-of-the-art performance across benchmarks.  					AI-generated summary 				 Recent advances in Large Language Mo...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 35. Group Contrastive Policy Optimization (GCPO) enhances reinforcement learning for large language models by incorporating external reference answers, improving training efficiency and generalization.  					AI-generated summary 				 Reinforcement learning has been widely applied to enhance the reasonin...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 36. BaRP, a Bandit-feedback Routing with Preferences approach, optimizes large language model selection in an online setting with partial feedback, outperforming offline routers and large models.  					AI-generated summary 				 Efficient use of large language models (LLMs) is critical for deployment at ...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 37. UP2You reconstructs high-fidelity 3D clothed portraits from unconstrained 2D photos using a data rectifier and pose-correlated feature aggregation, achieving superior geometric and texture accuracy.  					AI-generated summary 				 We present UP2You, the first tuning-free solution for reconstructing ...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 38. A novel approach combining driving models and generative world models evaluates and enhances the realism and generalization of synthetic video data for autonomous vehicle testing and planning.  					AI-generated summary 				 Recent advances in generative models have sparked exciting new possibilitie...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 39. A real-to-real 3D data generation framework enhances data efficiency for generalized robotic manipulation by augmenting pointcloud observations without simulation.  					AI-generated summary 				 Towards the aim of generalized robotic manipulation, spatial generalization is the most fundamental capa...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 40. A latent video diffusion model predicts multi-view consistent PBR materials from a single image, enabling relighting and novel view synthesis with high quality.  					AI-generated summary 				 We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically base...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 41. GyroSwin, a scalable 5D neural surrogate model, captures nonlinear gyrokinetic dynamics and improves heat flux prediction in plasma turbulence simulations.  					AI-generated summary 				 Nuclear fusion plays a pivotal role in the quest for reliable and sustainable energy production. A major roadblo...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 42. DreamOmni2 addresses limitations in instruction-based image editing and subject-driven generation by introducing multimodal instruction-based editing and generation tasks, utilizing feature mixing, index encoding, and joint training with a VLM.  					AI-generated summary 				 Recent advancements in ...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 43. A new framework, 3DEditFormer, uses a 3D-structure-preserving conditional transformer to enable precise and consistent 3D editing without manual masks, outperforming existing methods.  					AI-generated summary 				 3D editing - the task of locally modifying the geometry or appearance of a 3D asset ...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 44. OmniRetarget generates high-quality, interaction-preserving motion data for training RL policies, enabling complex skills like parkour and loco-manipulation on humanoid robots.  					AI-generated summary 				 A dominant paradigm for teaching humanoid robots complex skills is to retarget human motion...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 45. Coherent Information Fidelity Tuning (CIFT) improves out-of-distribution generalization in robot policies by optimizing data composition with a generative engine, enhancing robustness and performance.  					AI-generated summary 				 Generalist robot policies trained on large-scale, visually homogene...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 46. The study investigates how different optimizers impact model performance under post-training and quantization-aware training quantization, finding that Shampoo optimizer shows the lowest accuracy degradation and highest parameter efficiency.  					AI-generated summary 				 As new optimizers gain tra...
[10.10.2025 18:17] ********************************************************************************
[10.10.2025 18:17] Abstract 47. Search-R3 is a framework that adapts LLMs to generate effective search embeddings through chain-of-thought reasoning, supervised learning, and reinforcement learning.  					AI-generated summary 				 Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) ha...
[10.10.2025 18:17] Read previous papers.
[10.10.2025 18:17] Generating reviews via LLM API.
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#transfer_learning", "#rl", "#rlhf", "#reasoning", "#agents"], "emoji": "üåâ", "ru": {"title": "–†–∞–Ω–Ω–∏–π –æ–ø—ã—Ç: –º–æ—Å—Ç –º–µ–∂–¥—É –∏–º–∏—Ç–∞—Ü–∏–µ–π –∏ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é language-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ \"—Ä–∞–Ω–Ω–∏–π –æ–ø—ã—Ç\" - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, —Å–≥–µ–Ω–µ—Ä–∏—Ä
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#training", "#benchmark", "#rl", "#multimodal", "#dataset", "#optimization", "#reasoning"], "emoji": "üîÑ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–æ–º—É –º—ã—à–ª–µ–Ω–∏—é —á–µ—Ä–µ–∑ –≥–∏–±—Ä–∏–¥–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ LLM –ø–ª–æ—Ö–æ —Å
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#long_context", "#math", "#optimization", "#architecture"], "emoji": "üß†", "ru": {"title": "–î–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MemMamba - –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#agi", "#architecture", "#games", "#multimodal", "#video", "#transfer_learning", "#open_source"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –ø–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º", "desc": "UniVideo ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å –¥–≤—É—Ö–ø–æ—Ç–æ—á–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π, –æ–±—ä–µ–¥–∏–Ω
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#interpretability", "#healthcare", "#science", "#agents", "#multimodal", "#reasoning"], "emoji": "‚öóÔ∏è", "ru": {"title": "–ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏ –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ —É—Å–ª–æ–≤–∏–π —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∞–∫—Ü–∏–π", "desc": "ChemMAS - —ç—Ç–æ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —É—Å–ª–æ–≤–∏–π
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#benchmark", "#training", "#optimization", "#math", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ú–µ—Ç–∞–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ —Å–∞–º–æ–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –±–æ–ª—å—à–∏–µ reasoning-–º–æ–¥–µ–ª–∏ –ø–ª–æ—Ö–æ –ø–æ–Ω–∏–º–∞—é—Ç —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –º—ã—à–ª–µ–Ω–∏—è - —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#games", "#benchmark", "#diffusion", "#video"], "emoji": "üé®", "ru": {"title": "–í–∏–¥–µ–æ –∫–∞–∫ —Ö–æ–ª—Å—Ç: –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏ –≤—Ä–µ–º–µ–Ω–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VideoCanvas ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –≤–∏–¥–µ–æ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏ –≤—Ä–µ–º–µ–Ω–∏, –≥–¥–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç —Ä–∞–∑
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#open_source", "#long_context", "#benchmark", "#multimodal", "#reasoning", "#training", "#small_models"], "emoji": "üß©", "ru": {"title": "–®–∞–±–ª–æ–Ω—ã –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ ToTAL, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π 
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#rl", "#security", "#alignment", "#agents", "#rlhf"], "emoji": "üíÉ", "ru": {"title": "–¢–∞–Ω—Ü—É—è –º–µ–∂–¥—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é –∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å—é: –¥–≤–∞ AI-–∞–≥–µ–Ω—Ç–∞ —É—á–∞—Ç—Å—è –≤–º–µ—Å—Ç–µ", "desc": "WaltzRL - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ–º—É –ø–æ–≤–µ–¥–µ–Ω–∏—é —á–µ—Ä–µ–∑ multi-agent reinforcement lea
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#optimization", "#reasoning"], "emoji": "‚öñÔ∏è", "ru": {"title": "–õ—É—á—à–µ–µ –∏–∑ –¥–≤—É—Ö –º–∏—Ä–æ–≤: –≥–∏–±—Ä–∏–¥–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç HERO ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è reinforcement learning, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –±–∏–Ω–∞—Ä–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –æ—Ç –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#science", "#benchmark", "#agents"], "emoji": "üî¨", "ru": {"title": "–ù–∞—É—á–Ω–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ –∑–∞–∫–æ–Ω–æ–≤ –ø—Ä–∏—Ä–æ–¥—ã —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "desc": "NewtonBench ‚Äî —ç—Ç–æ benchmark –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –æ—Ç–∫—Ä—ã–≤–∞—Ç—å –Ω–∞—É—á–Ω—ã–µ –∑–∞–∫–æ–Ω—ã, –≤–∫–ª—é—á–∞—é—â–∏–π 324 –∑–∞–¥–∞—á–∏ –∏–∑ 12 –æ–±–ª–∞—Å—Ç–µ–π —Ñ–∏–∑–∏–∫–∏. –í–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#3d", "#cv", "#robotics"], "emoji": "üèõÔ∏è", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –∏ —Ç–æ—á–Ω–∞—è 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –ª—É—á—à–µ–µ –∏–∑ –¥–≤—É—Ö –º–∏—Ä–æ–≤", "desc": "ARTDECO - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å feed-forward –º–æ–¥–µ–ª–µ–π —Å –Ω–∞
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#benchmark", "#training", "#inference", "#reasoning", "#optimization"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö LLM", "desc": "DeepPrune ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Å–∫–µ–π–ª–∏–Ω–≥–∞ LLM, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#optimization", "#transfer_learning", "#agents"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –±–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ –æ–ø—ã—Ç–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Training-Free GRPO, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Å–ø–µ—Ü–∏–∞–ª–∏–∑
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#training", "#math", "#inference", "#data", "#reasoning", "#optimization"], "emoji": "ü™û", "ru": {"title": "–†–µ—Ñ–ª–µ–∫—Å–∏–∏ LLM –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, –∞ –Ω–µ –∏—Å–ø—Ä–∞–≤–ª—è—é—Ç: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Ä–∞–Ω–Ω—é—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –≤ reasoning-–º–æ–¥–µ–ª—è—Ö –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç –ø–µ
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#hallucinations", "#data", "#training", "#alignment", "#ethics", "#rlhf"], "emoji": "üé≠", "ru": {"title": "–ö–∞–∫ –º–∞–ª–∞—è –¥–æ–ª—è –ø–ª–æ—Ö–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–µ–ª–∞–µ—Ç AI –Ω–µ—á–µ—Å—Ç–Ω—ã–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–∞—á–∏–Ω–∞—é—Ç –ø—Ä–æ—è–≤–ª—è—Ç—å –Ω–µ—á–µ—Å—Ç–Ω–æ–µ 
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#video"], "emoji": "üé¨", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–ø—Å–∫–µ–π–ª–∏–Ω–≥ –≤–∏–¥–µ–æ –¥–æ 4K —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è", "desc": "UniMMVSR ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–ø—Å–∫–µ–π–ª–∏–Ω–≥–∞ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏: —Ç–µ–∫—Å—Ç–æ–º, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –≤–∏–¥–µ–æ –≤–Ω—É—Ç—Ä–∏ 
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#benchmark", "#training", "#architecture", "#multimodal", "#optimization", "#agi"], "emoji": "üîó", "ru": {"title": "–ù–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –Ω—É–ª—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ end-to-end –æ–±—É—á–µ–Ω–∏–µ Multimodal Large Language Models (MLLM) –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–º–ø–æ
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#synthetic", "#video", "#dataset"], "emoji": "üé®", "ru": {"title": "–ü–µ—Ä–µ–Ω–æ—Å —Å—Ç–∏–ª—è –Ω–∞ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä—ã –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω PickStyle ‚Äî –º–µ—Ç–æ–¥ –ø–µ—Ä–µ–Ω–æ—Å–∞ —Å—Ç–∏–ª—è –Ω–∞ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ì–ª–∞–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ ‚Äî –æ
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#math", "#reasoning"], "emoji": "‚ú®", "ru": {"title": "–ó–∞—â–∏—Ç–∞ —Ä–µ–¥–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ –ø—Ä–æ–±–ª–µ–º—É –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è LLM: —Ü–µ–Ω–Ω—ã–µ —Ä–µ–¥–∫–∏–µ —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –∞–≤—Ç–æ—Ä—ã –Ω–∞–∑—ã–≤–∞—é—Ç ¬´–∏—Å–∫—Ä–∞
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#video", "#diffusion", "#multimodal", "#cv"], "emoji": "üé¨", "ru": {"title": "–û–¥–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ñ–æ—Ç–æ, –∏ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º", "desc": "InstructX - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –û–Ω –∏–Ω—Ç–µ–≥—Ä–∏
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#benchmark", "#survey", "#rag", "#multimodal", "#reasoning", "#games"], "emoji": "üìö", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π RAG: —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–º–µ—Å—Ç–µ —Å–∏–ª—å–Ω–µ–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UniDoc-Bench ‚Äî –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º retrieval-a
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#rl", "#optimization", "#reasoning", "#rlhf", "#agents", "#agi"], "emoji": "ü§ù", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã —É—á–∞—Ç—Å—è –¥—Ä—É–≥ —É –¥—Ä—É–≥–∞: –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–∞—è —ç–≤–æ–ª—é—Ü–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ CoMAS ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#benchmark", "#training", "#alignment", "#long_context"], "emoji": "üìè", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Long-RewardBench ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ reward models –≤ —É—Å–ª–æ–≤–∏—è—Ö –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –≥–¥–µ –º–æ–¥–µ–ª
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#optimization", "#training", "#agi", "#long_context", "#agents"], "emoji": "üß†", "ru": {"title": "–ê–≥–µ–Ω—Ç —Å –ø–∞–º—è—Ç—å—é, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—Å—è –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–º –æ–ø—ã—Ç–µ", "desc": "MUSE ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AI-–∞–≥–µ–Ω—Ç–∞ —Å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–º –º–æ–¥—É–ª–µ–º –ø–∞–º—è—Ç–∏, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ –æ–±—É—á–∞—Ç—å—Å—è –∏ —Å–∞
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#diffusion", "#video", "#open_source", "#benchmark", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –≤–∏–¥–µ–æ –∏ –∑–≤—É–∫–∞ —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∑–∞–¥–∞—á–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å–æ –∑–≤—É–∫–æ–º –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è (Text-to-Sounding-Video). –ê–≤—Ç
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#games", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≥—Ä—É–ø–ø–æ–≤—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω DGPO ‚Äî –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#training", "#cv", "#benchmark", "#diffusion", "#video", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤ 50 —Ä–∞–∑ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥ rCM (score-regularized continuous-time con
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#long_context", "#rl", "#benchmark", "#optimization", "#data", "#reasoning", "#agents"], "emoji": "‚õèÔ∏è", "ru": {"title": "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏", "desc": "DeepMiner ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è multi-turn reasoning –∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#benchmark", "#science", "#multimodal", "#video", "#reasoning"], "emoji": "üî¨", "ru": {"title": "–ù–∞—É—á–Ω–æ–µ –≤–∏–¥–µ–æ-–º—ã—à–ª–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "SciVideoBench ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –∫ —Å–ª–æ–∂–Ω–æ–º—É –≤–∏–¥–µ
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#training", "#architecture", "#cv", "#rl", "#optimization"], "emoji": "üéØ", "ru": {"title": "ERA: –ö–æ–Ω—Ç—Ä–æ–ª—å —ç–Ω—Ç—Ä–æ–ø–∏–∏ —á–µ—Ä–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ ERA ‚Äî –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É, –∫–æ—Ç–æ—Ä–∞—è –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —ç–Ω—Ç—Ä–æ–ø–∏—é –≤—ã–±–æ—Ä–∫–∏ —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#graphs", "#reasoning", "#training", "#interpretability", "#inference"], "emoji": "üîë", "ru": {"title": "–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∫–∞–∫ –∫–ª—é—á –∫ –ø–∞–º—è—Ç–∏ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–ø–æ—Ç–µ–∑—É —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture"], "emoji": "‚ôªÔ∏è", "ru": {"title": "–ü–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∏ —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Ö –ø–∞
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#training", "#robotics", "#optimization", "#data", "#transfer_learning", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–û–¥–Ω–∞ –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–ª—è –≤—Ä–∞—â–µ–Ω–∏—è –ª—é–±—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä—É–∫–µ —Ä–æ–±–æ—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–µ—à–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –ø–µ—Ä–µ–Ω–æ—Å–∞ –Ω–∞–≤—ã–∫–æ–≤ –ª–æ–≤–∫–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –∏–∑ —Å–∏–º—É–ª—è—Ü–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –≤—Ä–∞—â
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#optimization", "#dataset", "#reasoning"], "emoji": "üîç", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ QA-—Å–∏—Å—Ç–µ–º —É—á–∏—Ç—ã–≤–∞—Ç—å –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å –≤–æ–ø—Ä–æ—Å–æ–≤ –±–µ–∑ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "A¬≤Search ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ —Ä—É—á–Ω–æ–π —Ä–∞–∑
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#reasoning", "#training", "#benchmark", "#rl", "#optimization"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –∫–æ–Ω—Ç—Ä–∞—Å—Ç–æ–º: –∫–æ–≥–¥–∞ –≤–Ω–µ—à–Ω–∏–µ –æ—Ç–≤–µ—Ç—ã –ø–æ–º–æ–≥–∞—é—Ç LLM —É—á–∏—Ç—å—Å—è –ª—É—á—à–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –º–µ—Ç–æ–¥ Group Contrastive Policy Optimization (GCPO) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#rlhf", "#training", "#optimization"], "emoji": "üé∞", "ru": {"title": "–£–º–Ω—ã–π –≤—ã–±–æ—Ä —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ö–æ–¥—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç BaRP ‚Äî —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º–µ –æ–Ω–ª–∞–π–Ω —Å —á–∞—Å—Ç–∏—á–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö —Ä–æ—É—Ç–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#3d", "#open_source"], "emoji": "üë§", "ru": {"title": "3D-–ø–æ—Ä—Ç—Ä–µ—Ç—ã –∏–∑ –æ–±—ã—á–Ω—ã—Ö —Ñ–æ—Ç–æ –±–µ–∑ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏", "desc": "UP2You - —ç—Ç–æ –ø–µ—Ä–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π –æ–¥–µ—Ç—ã—Ö –ª—é–¥–µ–π –∏–∑ –æ–±—ã—á–Ω—ã—Ö –Ω–µ–ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#synthetic", "#video", "#optimization", "#dataset", "#agents"], "emoji": "üöó", "ru": {"title": "–í–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ –¥–æ—Ä–æ–≥–∏: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ–ø–∏–ª–æ—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±—ä–µ–¥–∏–Ω–∏–ª–∏ –º–æ–¥–µ–ª–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ world models –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#3d", "#transfer_learning", "#robotics", "#data", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö 3D-–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –±–µ–∑ —Å–∏–º—É–ª—è—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç R2RGen - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–ø—Ä—è–º—É—é –∞—É–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –ø–∞—Ä—ã –Ω–∞–±–ª—é–¥–µ–Ω–∏–π-–¥–µ
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#games", "#3d", "#diffusion", "#video", "#cv"], "emoji": "üíé", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ 3D-–º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏–∑ –æ–¥–Ω–æ–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SViM3D, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã (PBR) –¥–ª—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å —Ä–∞—Å—à–∏—Ä—è–µ—Ç lat
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#architecture", "#training", "#science", "#optimization", "#dataset"], "emoji": "‚öõÔ∏è", "ru": {"title": "–ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è —Ç–µ—Ä–º–æ—è–¥–µ—Ä–Ω–æ–π –ø–ª–∞–∑–º—ã: 5D-–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—É—Ä–±—É–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GyroSwin ‚Äî –ø–µ—Ä–≤—É—é –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å—É—Ä—Ä–æ–≥–∞—Ç–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è 5D
[10.10.2025 18:17] Querying the API.
[10.10.2025 18:17] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DreamOmni2 addresses limitations in instruction-based image editing and subject-driven generation by introducing multimodal instruction-based editing and generation tasks, utilizing feature mixing, index encoding, and joint training with a VLM.  					AI-generated summary 				 Recent advancements in instruction-based image editing and subject-driven generation have garnered significant attention, yet both tasks still face limitations in meeting practical user needs. Instruction-based editing relies solely on language instructions, which often fail to capture specific editing details, making reference images necessary. Meanwhile, subject-driven generation is limited to combining concrete objects or people, overlooking broader, abstract concepts. To address these challenges, we propose two novel tasks: multimodal instruction-based editing and generation. These tasks support both text and image instructions and extend the scope to include both concrete and abstract concepts, greatly enhancing their practical applications. We introduce DreamOmni2, tackling two primary challenges: data creation and model framework design. Our data synthesis pipeline consists of three steps: (1) using a feature mixing method to create extraction data for both abstract and concrete concepts, (2) generating multimodal instruction-based editing training data using the editing and extraction models, and (3) further applying the extraction model to create training data for multimodal instruction-based editing. For the framework, to handle multi-image input, we propose an index encoding and position encoding shift scheme, which helps the model distinguish images and avoid pixel confusion. Additionally, we introduce joint training with the VLM and our generation/editing model to better process complex instructions. In addition, we have proposed comprehensive benchmarks for these two new tasks to drive their development. Experiments show that DreamOmni2 has achieved impressive results. Models and codes will be released.
[10.10.2025 18:17] Response: ```json
{
  "desc": "DreamOmni2 —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å —É—á—ë—Ç–æ–º —Å—É–±—ä–µ–∫—Ç–æ–≤, –≤–≤–æ–¥—è –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏–Ω–¥–µ–∫—Å–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å VLM –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö, —Ç–∞–∫ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –°–∏—Å—Ç–µ–º–∞ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –¥–æ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π pipeline –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ç–∏—Ö –∑–∞–¥–∞—á.",
  "emoji": "üé®",
  "title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏"
}
```
[10.10.2025 18:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DreamOmni2 addresses limitations in instruction-based image editing and subject-driven generation by introducing multimodal instruction-based editing and generation tasks, utilizing feature mixing, index encoding, and joint training with a VLM.  					AI-generated summary 				 Recent advancements in instruction-based image editing and subject-driven generation have garnered significant attention, yet both tasks still face limitations in meeting practical user needs. Instruction-based editing relies solely on language instructions, which often fail to capture specific editing details, making reference images necessary. Meanwhile, subject-driven generation is limited to combining concrete objects or people, overlooking broader, abstract concepts. To address these challenges, we propose two novel tasks: multimodal instruction-based editing and generation. These tasks support both text and image instructions and extend the scope to include both concrete and abstract concepts, greatly enhancing their practical applications. We introduce DreamOmni2, tackling two primary challenges: data creation and model framework design. Our data synthesis pipeline consists of three steps: (1) using a feature mixing method to create extraction data for both abstract and concrete concepts, (2) generating multimodal instruction-based editing training data using the editing and extraction models, and (3) further applying the extraction model to create training data for multimodal instruction-based editing. For the framework, to handle multi-image input, we propose an index encoding and position encoding shift scheme, which helps the model distinguish images and avoid pixel confusion. Additionally, we introduce joint training with the VLM and our generation/editing model to better process complex instructions. In addition, we have proposed comprehensive benchmarks for these two new tasks to drive their development. Experiments show that DreamOmni2 has achieved impressive results. Models and codes will be released."

[10.10.2025 18:17] Response: ```python
['MULTIMODAL', 'DATASET', 'BENCHMARK', 'CV']
```
[10.10.2025 18:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DreamOmni2 addresses limitations in instruction-based image editing and subject-driven generation by introducing multimodal instruction-based editing and generation tasks, utilizing feature mixing, index encoding, and joint training with a VLM.  					AI-generated summary 				 Recent advancements in instruction-based image editing and subject-driven generation have garnered significant attention, yet both tasks still face limitations in meeting practical user needs. Instruction-based editing relies solely on language instructions, which often fail to capture specific editing details, making reference images necessary. Meanwhile, subject-driven generation is limited to combining concrete objects or people, overlooking broader, abstract concepts. To address these challenges, we propose two novel tasks: multimodal instruction-based editing and generation. These tasks support both text and image instructions and extend the scope to include both concrete and abstract concepts, greatly enhancing their practical applications. We introduce DreamOmni2, tackling two primary challenges: data creation and model framework design. Our data synthesis pipeline consists of three steps: (1) using a feature mixing method to create extraction data for both abstract and concrete concepts, (2) generating multimodal instruction-based editing training data using the editing and extraction models, and (3) further applying the extraction model to create training data for multimodal instruction-based editing. For the framework, to handle multi-image input, we propose an index encoding and position encoding shift scheme, which helps the model distinguish images and avoid pixel confusion. Additionally, we introduce joint training with the VLM and our generation/editing model to better process complex instructions. In addition, we have proposed comprehensive benchmarks for these two new tasks to drive their development. Experiments show that DreamOmni2 has achieved impressive results. Models and codes will be released."

[10.10.2025 18:17] Response: ```python
["GAMES", "OPTIMIZATION", "OPEN_SOURCE"]
```
[10.10.2025 18:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DreamOmni2 improves image editing and generation by allowing both text and image instructions, addressing the limitations of traditional methods. It introduces multimodal tasks that can handle both concrete and abstract concepts, enhancing user experience. The model uses a unique data synthesis pipeline that includes feature mixing and joint training with a Vision-Language Model (VLM) to better interpret complex instructions. Comprehensive benchmarks have been established to evaluate these new tasks, demonstrating the effectiveness of DreamOmni2 in practical applications.","title":"Revolutionizing Image Editing with Multimodal Instructions"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DreamOmni2 improves image editing and generation by allowing both text and image instructions, addressing the limitations of traditional methods. It introduces multimodal tasks that can handle both concrete and abstract concepts, enhancing user experience. The model uses a unique data synthesis pipeline that includes feature mixing and joint training with a Vision-Language Model (VLM) to better interpret complex instructions. Comprehensive benchmarks have been established to evaluate these new tasks, demonstrating the effectiveness of DreamOmni2 in practical applications.', title='Revolutionizing Image Editing with Multimodal Instructions'))
[10.10.2025 18:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DreamOmni2 Ëß£ÂÜ≥‰∫ÜÂü∫‰∫éÊåá‰ª§ÁöÑÂõæÂÉèÁºñËæëÂíå‰∏ªÈ¢òÈ©±Âä®ÁîüÊàêÁöÑÂ±ÄÈôêÊÄßÔºåÊèêÂá∫‰∫ÜÂ§öÊ®°ÊÄÅÊåá‰ª§ÁºñËæëÂíåÁîüÊàê‰ªªÂä°„ÄÇËøô‰∫õ‰ªªÂä°ÊîØÊåÅÊñáÊú¨ÂíåÂõæÂÉèÊåá‰ª§ÔºåÊâ©Â±ï‰∫ÜÂÖ∑‰ΩìÂíåÊäΩË±°Ê¶ÇÂøµÁöÑËåÉÂõ¥ÔºåÂ¢ûÂº∫‰∫ÜÂÆûÈôÖÂ∫îÁî®„ÄÇÊàë‰ª¨ÈááÁî®ÁâπÂæÅÊ∑∑Âêà„ÄÅÁ¥¢ÂºïÁºñÁ†ÅÂíå‰∏éËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑËÅîÂêàËÆ≠ÁªÉÊù•Â§ÑÁêÜÂ§çÊùÇÊåá‰ª§„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDreamOmni2 Âú®Ëøô‰∏§‰∏™Êñ∞‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊàêÊûú„ÄÇ","title":"DreamOmni2ÔºöÂ§öÊ®°ÊÄÅÊåá‰ª§ÁºñËæë‰∏éÁîüÊàêÁöÑÁ™ÅÁ†¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DreamOmni2 Ëß£ÂÜ≥‰∫ÜÂü∫‰∫éÊåá‰ª§ÁöÑÂõæÂÉèÁºñËæëÂíå‰∏ªÈ¢òÈ©±Âä®ÁîüÊàêÁöÑÂ±ÄÈôêÊÄßÔºåÊèêÂá∫‰∫ÜÂ§öÊ®°ÊÄÅÊåá‰ª§ÁºñËæëÂíåÁîüÊàê‰ªªÂä°„ÄÇËøô‰∫õ‰ªªÂä°ÊîØÊåÅÊñáÊú¨ÂíåÂõæÂÉèÊåá‰ª§ÔºåÊâ©Â±ï‰∫ÜÂÖ∑‰ΩìÂíåÊäΩË±°Ê¶ÇÂøµÁöÑËåÉÂõ¥ÔºåÂ¢ûÂº∫‰∫ÜÂÆûÈôÖÂ∫îÁî®„ÄÇÊàë‰ª¨ÈááÁî®ÁâπÂæÅÊ∑∑Âêà„ÄÅÁ¥¢ÂºïÁºñÁ†ÅÂíå‰∏éËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁöÑËÅîÂêàËÆ≠ÁªÉÊù•Â§ÑÁêÜÂ§çÊùÇÊåá‰ª§„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDreamOmni2 Âú®Ëøô‰∏§‰∏™Êñ∞‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊàêÊûú„ÄÇ', title='DreamOmni2ÔºöÂ§öÊ®°ÊÄÅÊåá‰ª§ÁºñËæë‰∏éÁîüÊàêÁöÑÁ™ÅÁ†¥'))
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#dataset", "#architecture", "#open_source"], "emoji": "üé®", "ru": {"title": "–¢–æ—á–Ω–æ–µ 3D-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ –º–∞—Å–æ–∫ —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ 3DEditFormer ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É—Å–ª–æ–≤–Ω—ã–π —Ç—Ä–∞–Ω—Å
[10.10.2025 18:17] Querying the API.
[10.10.2025 18:17] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

OmniRetarget generates high-quality, interaction-preserving motion data for training RL policies, enabling complex skills like parkour and loco-manipulation on humanoid robots.  					AI-generated summary 				 A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OmniRetarget by retargeting motions from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines. Such high-quality data enables proprioceptive RL policies to successfully execute long-horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum.
[10.10.2025 18:17] Response: ```json
{
  "desc": "OmniRetarget ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –¥–≤–∏–∂–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤ —á–µ—Ä–µ–∑ reinforcement learning. –ö–ª—é—á–µ–≤–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ interaction mesh, –∫–æ—Ç–æ—Ä–∞—è —è–≤–Ω–æ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–æ–º, –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º –∏ –æ–±—ä–µ–∫—Ç–∞–º–∏. –°–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, –∏–∑–±–µ–≥–∞—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤—Ä–æ–¥–µ –ø—Ä–æ—Å–∫–∞–ª—å–∑—ã–≤–∞–Ω–∏—è —Å—Ç–æ–ø, –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤ –∏ —É—Å–ª–æ–≤–∏–π. –û–±—É—á–µ–Ω–Ω—ã–µ RL-–ø–æ–ª–∏—Ç–∏–∫–∏ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω—è—é—Ç —Å–ª–æ–∂–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ –ø–∞—Ä–∫—É—Ä–∞ –∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –¥–æ 30 —Å–µ–∫—É–Ω–¥ –Ω–∞ —Ä–æ–±–æ—Ç–µ Unitree G1 —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º reward-—Ñ—É–Ω–∫—Ü–∏–π.",
  "emoji": "ü§∏",
  "title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ —Å–ª–æ–∂–Ω—ã–º –¥–≤–∏–∂–µ–Ω–∏—è–º —á–µ—Ä–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º"
}
```
[10.10.2025 18:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"OmniRetarget generates high-quality, interaction-preserving motion data for training RL policies, enabling complex skills like parkour and loco-manipulation on humanoid robots.  					AI-generated summary 				 A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OmniRetarget by retargeting motions from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines. Such high-quality data enables proprioceptive RL policies to successfully execute long-horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum."

[10.10.2025 18:17] Response: ```python
["RL", "DATASET", "TRAINING", "ROBOTICS"]
```
[10.10.2025 18:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"OmniRetarget generates high-quality, interaction-preserving motion data for training RL policies, enabling complex skills like parkour and loco-manipulation on humanoid robots.  					AI-generated summary 				 A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation. To address this, we introduce OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations. We comprehensively evaluate OmniRetarget by retargeting motions from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines. Such high-quality data enables proprioceptive RL policies to successfully execute long-horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum."

[10.10.2025 18:17] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[10.10.2025 18:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OmniRetarget is a novel approach for generating high-quality motion data that helps train reinforcement learning (RL) policies for humanoid robots. It addresses the challenges of the embodiment gap between humans and robots by preserving important interactions with the environment and objects. By using an interaction mesh, it ensures that the spatial and contact relationships are maintained, leading to more realistic and feasible motion trajectories. The method has been evaluated against existing techniques, showing superior performance in kinematic constraint satisfaction and interaction preservation, enabling robots to learn complex skills like parkour effectively.","title":"Bridging the Gap: Realistic Motion Data for Humanoid Robots"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OmniRetarget is a novel approach for generating high-quality motion data that helps train reinforcement learning (RL) policies for humanoid robots. It addresses the challenges of the embodiment gap between humans and robots by preserving important interactions with the environment and objects. By using an interaction mesh, it ensures that the spatial and contact relationships are maintained, leading to more realistic and feasible motion trajectories. The method has been evaluated against existing techniques, showing superior performance in kinematic constraint satisfaction and interaction preservation, enabling robots to learn complex skills like parkour effectively.', title='Bridging the Gap: Realistic Motion Data for Humanoid Robots'))
[10.10.2025 18:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OmniRetarget ÊòØ‰∏ÄÁßçÁîüÊàêÈ´òË¥®Èáè„ÄÅ‰øùÁïô‰∫§‰∫íÁöÑËøêÂä®Êï∞ÊçÆÁöÑÊñπÊ≥ïÔºåÁî®‰∫éËÆ≠ÁªÉÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁ≠ñÁï•Ôºå‰Ωø‰∫∫ÂΩ¢Êú∫Âô®‰∫∫ËÉΩÂ§üÊéåÊè°Â§çÊùÇÊäÄËÉΩÔºåÂ¶ÇË∑ëÈÖ∑ÂíåËøêÂä®ÊìçÊéß„ÄÇÁé∞ÊúâÁöÑËøêÂä®ÈáçÂÆöÂêëÊñπÊ≥ïÂ∏∏Â∏∏Èù¢‰∏¥‰∫∫Á±ª‰∏éÊú∫Âô®‰∫∫‰πãÈó¥ÁöÑÊòæËëóÂ∑ÆË∑ùÔºåÂØºËá¥ÁîüÊàê‰∏çÂàáÂÆûÈôÖÁöÑËøêÂä®ÊïàÊûú„ÄÇOmniRetarget ÈÄöËøáÂª∫Ê®°Âíå‰øùÁïô‰ª£ÁêÜ„ÄÅÂú∞ÂΩ¢ÂíåÊìçÊéßÁâ©‰Ωì‰πãÈó¥ÁöÑÁ©∫Èó¥ÂíåÊé•Ëß¶ÂÖ≥Á≥ªÔºåËß£ÂÜ≥‰∫ÜËøô‰∏ÄÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ïÁîüÊàêÁöÑËøêÂä®ËΩ®ËøπÂú®ËøêÂä®Â≠¶Á∫¶ÊùüÂíåÊé•Ëß¶‰øùÁïôÊñπÈù¢‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÊîØÊåÅÈ´òÊïàÁöÑÊï∞ÊçÆÂ¢ûÂº∫Ôºå‰øÉËøõÊú∫Âô®‰∫∫Âú®‰∏çÂêåÁéØÂ¢É‰∏≠Â≠¶‰π†Â§çÊùÇÊäÄËÉΩ„ÄÇ","title":"OmniRetargetÔºöÊèêÂçá‰∫∫ÂΩ¢Êú∫Âô®‰∫∫ËøêÂä®ÊäÄËÉΩÁöÑÂÖ≥ÈîÆ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OmniRetarget ÊòØ‰∏ÄÁßçÁîüÊàêÈ´òË¥®Èáè„ÄÅ‰øùÁïô‰∫§‰∫íÁöÑËøêÂä®Êï∞ÊçÆÁöÑÊñπÊ≥ïÔºåÁî®‰∫éËÆ≠ÁªÉÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁ≠ñÁï•Ôºå‰Ωø‰∫∫ÂΩ¢Êú∫Âô®‰∫∫ËÉΩÂ§üÊéåÊè°Â§çÊùÇÊäÄËÉΩÔºåÂ¶ÇË∑ëÈÖ∑ÂíåËøêÂä®ÊìçÊéß„ÄÇÁé∞ÊúâÁöÑËøêÂä®ÈáçÂÆöÂêëÊñπÊ≥ïÂ∏∏Â∏∏Èù¢‰∏¥‰∫∫Á±ª‰∏éÊú∫Âô®‰∫∫‰πãÈó¥ÁöÑÊòæËëóÂ∑ÆË∑ùÔºåÂØºËá¥ÁîüÊàê‰∏çÂàáÂÆûÈôÖÁöÑËøêÂä®ÊïàÊûú„ÄÇOmniRetarget ÈÄöËøáÂª∫Ê®°Âíå‰øùÁïô‰ª£ÁêÜ„ÄÅÂú∞ÂΩ¢ÂíåÊìçÊéßÁâ©‰Ωì‰πãÈó¥ÁöÑÁ©∫Èó¥ÂíåÊé•Ëß¶ÂÖ≥Á≥ªÔºåËß£ÂÜ≥‰∫ÜËøô‰∏ÄÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ïÁîüÊàêÁöÑËøêÂä®ËΩ®ËøπÂú®ËøêÂä®Â≠¶Á∫¶ÊùüÂíåÊé•Ëß¶‰øùÁïôÊñπÈù¢‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÊîØÊåÅÈ´òÊïàÁöÑÊï∞ÊçÆÂ¢ûÂº∫Ôºå‰øÉËøõÊú∫Âô®‰∫∫Âú®‰∏çÂêåÁéØÂ¢É‰∏≠Â≠¶‰π†Â§çÊùÇÊäÄËÉΩ„ÄÇ', title='OmniRetargetÔºöÊèêÂçá‰∫∫ÂΩ¢Êú∫Âô®‰∫∫ËøêÂä®ÊäÄËÉΩÁöÑÂÖ≥ÈîÆ'))
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#synthetic", "#optimization", "#training", "#robotics", "#data", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–æ–±–∞—Å—Ç–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ CIFT (Coherent Information Fidelity Tuning) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–æ–±–æ—Ç
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference"], "emoji": "üß¥", "ru": {"title": "Shampoo –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à—É—é —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –ø—Ä–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç, –∫–∞–∫ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –≤–ª–∏—è—é—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ - –∫–∞–∫ –ø—Ä–∏ –ø–æ—Å—Ç—Ç—Ä–µ–Ω–∏–Ω
[10.10.2025 18:17] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#optimization", "#rlhf", "#rl"], "emoji": "üîç", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–∞—è –∫ –ª—É—á—à–µ–º—É –ø–æ–∏—Å–∫—É: LLM –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ chain-of-thought", "desc": "Search-R3 - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏
[10.10.2025 18:17] Renaming data file.
[10.10.2025 18:17] Renaming previous data. hf_papers.json to ./d/2025-10-10.json
[10.10.2025 18:17] Saving new data file.
[10.10.2025 18:17] Generating page.
[10.10.2025 18:17] Renaming previous page.
[10.10.2025 18:17] Renaming previous data. index.html to ./d/2025-10-10.html
[10.10.2025 18:17] Writing result.
[10.10.2025 18:17] Renaming log file.
[10.10.2025 18:17] Renaming previous data. log.txt to ./logs/2025-10-10_last_log.txt
