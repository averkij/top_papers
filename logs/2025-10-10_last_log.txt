[10.10.2025 13:22] Read previous papers.
[10.10.2025 13:22] Generating top page (month).
[10.10.2025 13:22] Writing top page (month).
[10.10.2025 14:11] Read previous papers.
[10.10.2025 14:11] Get feed.
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08558
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08540
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23768
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08377
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03259
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07499
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08555
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03279
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08240
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07242
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07172
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08483
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08191
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08551
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08211
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08143
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08565
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08308
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08485
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03222
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03663
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08529
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06915
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08002
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03117
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08431
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08425
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08276
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08549
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08203
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08008
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08559
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08556
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07958
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07790
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07429
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24817
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08547
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24797
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08271
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07048
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02994
[10.10.2025 14:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23500
[10.10.2025 14:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.10.2025 14:11] No deleted papers detected.
[10.10.2025 14:11] Downloading and parsing papers (pdf, html). Total: 43.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08558.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08558.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08558.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08540.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08540.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08540.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2509.23768.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2509.23768.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2509.23768.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08377.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08377.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08377.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.03259.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.03259.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.03259.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.07499.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.07499.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.07499.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08555.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08555.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08555.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.03279.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.03279.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.03279.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08240.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08240.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08240.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.07242.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.07242.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.07242.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.07172.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.07172.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.07172.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08483.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08483.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08483.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08191.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08191.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08191.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08551.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08551.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08551.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08211.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08211.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08211.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08143.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08143.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08143.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08565.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08565.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08565.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08308.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08308.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08308.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08485.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08485.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08485.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.03222.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.03222.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.03222.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.03663.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.03663.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.03663.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08529.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08529.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08529.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.06915.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.06915.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.06915.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08002.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08002.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08002.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.03117.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.03117.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.03117.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08431.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08431.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08431.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08425.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08425.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08425.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08276.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08276.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08276.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08549.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08549.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08549.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08203.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08203.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08203.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08008.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08008.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08008.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08559.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08559.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08559.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08556.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08556.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08556.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.07958.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.07958.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.07958.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.07790.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.07790.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.07790.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.07429.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.07429.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.07429.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2509.24817.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2509.24817.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2509.24817.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08547.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08547.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08547.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2509.24797.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2509.24797.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2509.24797.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.08271.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.08271.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.08271.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.07048.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.07048.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.07048.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2510.02994.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2510.02994.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2510.02994.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Downloading and parsing paper https://huggingface.co/papers/2509.23500.
[10.10.2025 14:11] Extra JSON file exists (./assets/json/2509.23500.json), skip PDF parsing.
[10.10.2025 14:11] Paper image links file exists (./assets/img_data/2509.23500.json), skip HTML parsing.
[10.10.2025 14:11] Success.
[10.10.2025 14:11] Enriching papers with extra data.
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 0. Early experience, using agent-generated interaction data without reward signals, improves policy effectiveness and generalization, serving as a bridge between imitation learning and reinforcement learning.  					AI-generated summary 				 A long-term goal of language agents is to learn and improve th...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 1. Existing Multimodal Large Language Models show performance deficits in long-chain reflective reasoning, which is addressed by developing MM-HELIX-100K and Adaptive Hybrid Policy Optimization, leading to improved accuracy and generalization.  					AI-generated summary 				 While current Multimodal La...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 2. ChemMAS, a multi-agent system, improves reaction condition recommendation by providing interpretable rationales, outperforming existing methods in accuracy and explainability.  					AI-generated summary 				 The chemical reaction recommendation is to select proper reaction condition parameters for c...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 3. UniVideo, a dual-stream framework combining a Multimodal Large Language Model and a Multimodal DiT, extends unified modeling to video generation and editing, achieving state-of-the-art performance and supporting task composition and generalization.  					AI-generated summary 				 Unified multimodal ...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 4. A training pipeline called MASA enhances meta-awareness in reasoning models, leading to improved accuracy and efficiency across various benchmarks.  					AI-generated summary 				 Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by it...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 5. Thought templates enhance long-context language models by structuring evidence combination and guiding multi-hop inference, leading to consistent performance improvements across various benchmarks.  					AI-generated summary 				 Recent Long-Context Language Models (LCLMs) can process hundreds of th...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 6. VideoCanvas addresses temporal ambiguity in latent video diffusion models to enable flexible spatio-temporal video completion using a hybrid conditioning strategy.  					AI-generated summary 				 We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arb...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 7. MemMamba, a novel architecture integrating state summarization and cross-attention, improves long-range memory and efficiency in sequence modeling compared to Mamba and Transformers.  					AI-generated summary 				 With the explosive growth of data, long-sequence modeling has become increasingly imp...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 8. WaltzRL, a multi-agent reinforcement learning framework, improves LLM safety and helpfulness by collaboratively training a conversation agent and a feedback agent, reducing unsafe responses and overrefusals.  					AI-generated summary 				 Harnessing the power of LLMs requires a delicate dance betwe...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 9. HERO, a reinforcement learning framework, combines verifier signals with reward-model scores to enhance reasoning in large language models, outperforming both RM-only and verifier-only methods.  					AI-generated summary 				 Post-training for reasoning of large language models (LLMs) increasingly r...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 10. NewtonBench is a benchmark for scientific law discovery that addresses scalability, scientific relevance, and memorization resistance by using metaphysical shifts and interactive model discovery.  					AI-generated summary 				 Large language models are emerging as powerful tools for scientific law ...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 11. DeepPrune, a novel framework using dynamic pruning and a specialized judge model, significantly reduces computational inefficiency in parallel scaling of large language models by pruning redundant reasoning traces.  					AI-generated summary 				 Parallel scaling has emerged as a powerful paradigm t...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 12. Training-Free GRPO enhances LLM agent performance in specialized domains by learning experiential knowledge as a token prior without parameter updates, improving out-of-domain tasks with minimal data.  					AI-generated summary 				 Recent advances in Large Language Model (LLM) agents have demonstra...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 13. ARTDECO combines feed-forward models and SLAM pipelines for efficient and accurate 3D reconstruction from monocular images.  					AI-generated summary 				 On-the-fly 3D reconstruction from monocular image sequences is a long-standing challenge in computer vision, critical for applications such as r...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 14. LLMs finetuned on misaligned data exhibit dishonest behavior, which can be exacerbated in downstream tasks and human-AI interactions.  					AI-generated summary 				 Previous research has shown that LLMs finetuned on malicious or incorrect completions within narrow domains (e.g., insecure code or in...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 15. UniMMVSR is a unified generative video super-resolution framework that incorporates hybrid-modal conditions, including text, images, and videos, within a latent video diffusion model, achieving superior detail and conformity to multi-modal conditions.  					AI-generated summary 				 Cascaded video s...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 16. Native end-to-end training of Multimodal Large Language Models (MLLMs) achieves competitive performance with a balanced design and scaling relationship between visual encoders and LLMs.  					AI-generated summary 				 Compositional training has been the de-facto paradigm in existing Multimodal Large...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 17. Analysis of reflective behaviors in reasoning models shows that reflections primarily confirm initial answers, and training with more reflections improves first-answer correctness; a question-aware early-stopping method reduces unnecessary reflections and tokens with minimal accuracy loss.  					AI-...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 18. InstructX integrates Multimodal Large Language Models and diffusion models for instruction-driven image and video editing, achieving state-of-the-art performance across diverse tasks.  					AI-generated summary 				 With recent advances in Multimodal Large Language Models (MLLMs) showing strong visu...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 19. Low-probability Regularization (Lp-Reg) enhances exploration in Reinforcement Learning with Verifiable Rewards (RLVR) by preserving valuable low-probability tokens, leading to improved performance in complex reasoning tasks.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewa...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 20. UniDoc-Bench is a large-scale benchmark for multimodal retrieval-augmented generation, evaluating systems across text, images, and their fusion in real-world document-centric scenarios.  					AI-generated summary 				 Multimodal retrieval-augmented generation (MM-RAG) is a key approach for applying ...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 21. Co-Evolving Multi-Agent Systems (CoMAS) enable LLM-based agents to improve autonomously through inter-agent interactions and intrinsic rewards, achieving state-of-the-art performance.  					AI-generated summary 				 Self-evolution is a central research topic in enabling large language model (LLM)-ba...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 22. A benchmark and training strategy for reward models to improve long-context consistency and performance in large language models.  					AI-generated summary 				 Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasin...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 23. MUSE, a novel agent framework with a hierarchical Memory Module, enables continuous learning and self-evolution, achieving state-of-the-art performance on long-horizon productivity tasks using a lightweight model.  					AI-generated summary 				 Large Language Models have demonstrated remarkable cap...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 24. A novel dual-tower diffusion transformer with a Dual CrossAttention mechanism addresses challenges in Text-to-Sounding-Video generation by disentangling captions and enabling symmetric information exchange.  					AI-generated summary 				 This study focuses on a challenging yet promising task, Text-...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 25. Score-regularized continuous-time consistency model (rCM) improves large-scale diffusion distillation by addressing fine-detail generation and diversity issues, achieving high fidelity and accelerated sampling.  					AI-generated summary 				 This work represents the first effort to scale up continu...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 26. DGPO, a new online RL algorithm, enhances diffusion models by learning from group-level preferences, enabling the use of efficient deterministic ODE samplers and achieving faster training and superior performance.  					AI-generated summary 				 While reinforcement learning methods such as Group Rel...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 27. DeepMiner, a framework using high-difficulty training tasks and dynamic context management, enhances multi-turn reasoning agents through reinforcement learning, achieving significant performance improvements across benchmarks.  					AI-generated summary 				 While recent advances in reasoning models...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 28. ERA, a new paradigm using specially designed activations, enhances performance across LLMs, reinforcement learning, and image classification with minimal computational overhead.  					AI-generated summary 				 We propose ERA, a new paradigm that constrains the sampling entropy above given thresholds...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 29. Function tokens in large language models activate predictive features during inference and guide memory consolidation during pre-training by predicting subsequent content tokens.  					AI-generated summary 				 The remarkable success of large language models (LLMs) stems from their ability to consol...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 30. Recycling pretrained checkpoints through orthogonal growth methods improves large language model performance with reduced computational cost.  					AI-generated summary 				 The rapidly increasing computational cost of pretraining Large Language Models necessitates more efficient approaches. Numerou...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 31. SciVideoBench is a benchmark designed to evaluate advanced video reasoning in scientific contexts, challenging models with sophisticated domain-specific knowledge and logical reasoning.  					AI-generated summary 				 Large Multimodal Models (LMMs) have achieved remarkable progress across various ca...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 32. A novel framework enables a single simulation-trained policy to generalize to diverse real-world object rotations by learning joint-wise dynamics and autonomously collecting data.  					AI-generated summary 				 Achieving generalized in-hand object rotation remains a significant challenge in robotic...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 33. A$^2$Search is an annotation-free framework that handles ambiguity in open-domain QA by detecting ambiguous questions, gathering alternative answers, and optimizing with RL, achieving state-of-the-art performance across benchmarks.  					AI-generated summary 				 Recent advances in Large Language Mo...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 34. Group Contrastive Policy Optimization (GCPO) enhances reinforcement learning for large language models by incorporating external reference answers, improving training efficiency and generalization.  					AI-generated summary 				 Reinforcement learning has been widely applied to enhance the reasonin...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 35. BaRP, a Bandit-feedback Routing with Preferences approach, optimizes large language model selection in an online setting with partial feedback, outperforming offline routers and large models.  					AI-generated summary 				 Efficient use of large language models (LLMs) is critical for deployment at ...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 36. UP2You reconstructs high-fidelity 3D clothed portraits from unconstrained 2D photos using a data rectifier and pose-correlated feature aggregation, achieving superior geometric and texture accuracy.  					AI-generated summary 				 We present UP2You, the first tuning-free solution for reconstructing ...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 37. A real-to-real 3D data generation framework enhances data efficiency for generalized robotic manipulation by augmenting pointcloud observations without simulation.  					AI-generated summary 				 Towards the aim of generalized robotic manipulation, spatial generalization is the most fundamental capa...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 38. Coherent Information Fidelity Tuning (CIFT) improves out-of-distribution generalization in robot policies by optimizing data composition with a generative engine, enhancing robustness and performance.  					AI-generated summary 				 Generalist robot policies trained on large-scale, visually homogene...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 39. A latent video diffusion model predicts multi-view consistent PBR materials from a single image, enabling relighting and novel view synthesis with high quality.  					AI-generated summary 				 We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically base...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 40. Search-R3 is a framework that adapts LLMs to generate effective search embeddings through chain-of-thought reasoning, supervised learning, and reinforcement learning.  					AI-generated summary 				 Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) ha...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 41. A new framework, 3DEditFormer, uses a 3D-structure-preserving conditional transformer to enable precise and consistent 3D editing without manual masks, outperforming existing methods.  					AI-generated summary 				 3D editing - the task of locally modifying the geometry or appearance of a 3D asset ...
[10.10.2025 14:11] ********************************************************************************
[10.10.2025 14:11] Abstract 42. The study investigates how different optimizers impact model performance under post-training and quantization-aware training quantization, finding that Shampoo optimizer shows the lowest accuracy degradation and highest parameter efficiency.  					AI-generated summary 				 As new optimizers gain tra...
[10.10.2025 14:11] Read previous papers.
[10.10.2025 14:11] Generating reviews via LLM API.
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#transfer_learning", "#rl", "#rlhf", "#reasoning", "#agents"], "emoji": "üåâ", "ru": {"title": "–†–∞–Ω–Ω–∏–π –æ–ø—ã—Ç: –º–æ—Å—Ç –º–µ–∂–¥—É –∏–º–∏—Ç–∞—Ü–∏–µ–π –∏ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é language-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ \"—Ä–∞–Ω–Ω–∏–π –æ–ø—ã—Ç\" - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, —Å–≥–µ–Ω–µ—Ä–∏—Ä
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#training", "#benchmark", "#rl", "#multimodal", "#dataset", "#optimization", "#reasoning"], "emoji": "üîÑ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–æ–º—É –º—ã—à–ª–µ–Ω–∏—é —á–µ—Ä–µ–∑ –≥–∏–±—Ä–∏–¥–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ LLM –ø–ª–æ—Ö–æ —Å
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#interpretability", "#healthcare", "#science", "#agents", "#multimodal", "#reasoning"], "emoji": "‚öóÔ∏è", "ru": {"title": "–ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏ –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ —É—Å–ª–æ–≤–∏–π —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∞–∫—Ü–∏–π", "desc": "ChemMAS - —ç—Ç–æ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —É—Å–ª–æ–≤–∏–π
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#agi", "#architecture", "#games", "#multimodal", "#video", "#transfer_learning", "#open_source"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –ø–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º", "desc": "UniVideo ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å –¥–≤—É—Ö–ø–æ—Ç–æ—á–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π, –æ–±—ä–µ–¥–∏–Ω
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#benchmark", "#training", "#optimization", "#math", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ú–µ—Ç–∞–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ —Å–∞–º–æ–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –±–æ–ª—å—à–∏–µ reasoning-–º–æ–¥–µ–ª–∏ –ø–ª–æ—Ö–æ –ø–æ–Ω–∏–º–∞—é—Ç —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –º—ã—à–ª–µ–Ω–∏—è - —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#open_source", "#long_context", "#benchmark", "#multimodal", "#reasoning", "#training", "#small_models"], "emoji": "üß©", "ru": {"title": "–®–∞–±–ª–æ–Ω—ã –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ ToTAL, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π 
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#games", "#benchmark", "#diffusion", "#video"], "emoji": "üé®", "ru": {"title": "–í–∏–¥–µ–æ –∫–∞–∫ —Ö–æ–ª—Å—Ç: –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏ –≤—Ä–µ–º–µ–Ω–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VideoCanvas ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –≤–∏–¥–µ–æ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏ –≤—Ä–µ–º–µ–Ω–∏, –≥–¥–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç —Ä–∞–∑
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#long_context", "#math", "#optimization", "#architecture"], "emoji": "üß†", "ru": {"title": "–î–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MemMamba - –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#rl", "#security", "#alignment", "#agents", "#rlhf"], "emoji": "üíÉ", "ru": {"title": "–¢–∞–Ω—Ü—É—è –º–µ–∂–¥—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é –∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å—é: –¥–≤–∞ AI-–∞–≥–µ–Ω—Ç–∞ —É—á–∞—Ç—Å—è –≤–º–µ—Å—Ç–µ", "desc": "WaltzRL - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ–º—É –ø–æ–≤–µ–¥–µ–Ω–∏—é —á–µ—Ä–µ–∑ multi-agent reinforcement lea
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#optimization", "#reasoning"], "emoji": "‚öñÔ∏è", "ru": {"title": "–õ—É—á—à–µ–µ –∏–∑ –¥–≤—É—Ö –º–∏—Ä–æ–≤: –≥–∏–±—Ä–∏–¥–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç HERO ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è reinforcement learning, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –±–∏–Ω–∞—Ä–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –æ—Ç –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#science", "#benchmark", "#agents"], "emoji": "üî¨", "ru": {"title": "–ù–∞—É—á–Ω–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ –∑–∞–∫–æ–Ω–æ–≤ –ø—Ä–∏—Ä–æ–¥—ã —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "desc": "NewtonBench ‚Äî —ç—Ç–æ benchmark –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –æ—Ç–∫—Ä—ã–≤–∞—Ç—å –Ω–∞—É—á–Ω—ã–µ –∑–∞–∫–æ–Ω—ã, –≤–∫–ª—é—á–∞—é—â–∏–π 324 –∑–∞–¥–∞—á–∏ –∏–∑ 12 –æ–±–ª–∞—Å—Ç–µ–π —Ñ–∏–∑–∏–∫–∏. –í–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#benchmark", "#training", "#inference", "#reasoning", "#optimization"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö LLM", "desc": "DeepPrune ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Å–∫–µ–π–ª–∏–Ω–≥–∞ LLM, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#optimization", "#transfer_learning", "#agents"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –±–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ –æ–ø—ã—Ç–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Training-Free GRPO, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Å–ø–µ—Ü–∏–∞–ª–∏–∑
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#3d", "#cv", "#robotics"], "emoji": "üèõÔ∏è", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –∏ —Ç–æ—á–Ω–∞—è 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –ª—É—á—à–µ–µ –∏–∑ –¥–≤—É—Ö –º–∏—Ä–æ–≤", "desc": "ARTDECO - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å feed-forward –º–æ–¥–µ–ª–µ–π —Å –Ω–∞
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#hallucinations", "#data", "#training", "#alignment", "#ethics", "#rlhf"], "emoji": "üé≠", "ru": {"title": "–ö–∞–∫ –º–∞–ª–∞—è –¥–æ–ª—è –ø–ª–æ—Ö–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–µ–ª–∞–µ—Ç AI –Ω–µ—á–µ—Å—Ç–Ω—ã–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–∞—á–∏–Ω–∞—é—Ç –ø—Ä–æ—è–≤–ª—è—Ç—å –Ω–µ—á–µ—Å—Ç–Ω–æ–µ 
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#video"], "emoji": "üé¨", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–ø—Å–∫–µ–π–ª–∏–Ω–≥ –≤–∏–¥–µ–æ –¥–æ 4K —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è", "desc": "UniMMVSR ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–ø—Å–∫–µ–π–ª–∏–Ω–≥–∞ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏: —Ç–µ–∫—Å—Ç–æ–º, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –≤–∏–¥–µ–æ –≤–Ω—É—Ç—Ä–∏ 
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#benchmark", "#training", "#architecture", "#multimodal", "#optimization", "#agi"], "emoji": "üîó", "ru": {"title": "–ù–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –Ω—É–ª—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ end-to-end –æ–±—É—á–µ–Ω–∏–µ Multimodal Large Language Models (MLLM) –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–º–ø–æ
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#training", "#math", "#inference", "#data", "#reasoning", "#optimization"], "emoji": "ü™û", "ru": {"title": "–†–µ—Ñ–ª–µ–∫—Å–∏–∏ LLM –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, –∞ –Ω–µ –∏—Å–ø—Ä–∞–≤–ª—è—é—Ç: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Ä–∞–Ω–Ω—é—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –≤ reasoning-–º–æ–¥–µ–ª—è—Ö –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç –ø–µ
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#video", "#diffusion", "#multimodal", "#cv"], "emoji": "üé¨", "ru": {"title": "–û–¥–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ñ–æ—Ç–æ, –∏ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º", "desc": "InstructX - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –û–Ω –∏–Ω—Ç–µ–≥—Ä–∏
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#math", "#reasoning"], "emoji": "‚ú®", "ru": {"title": "–ó–∞—â–∏—Ç–∞ —Ä–µ–¥–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ –ø—Ä–æ–±–ª–µ–º—É –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è LLM: —Ü–µ–Ω–Ω—ã–µ —Ä–µ–¥–∫–∏–µ —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –∞–≤—Ç–æ—Ä—ã –Ω–∞–∑—ã–≤–∞—é—Ç ¬´–∏—Å–∫—Ä–∞
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#benchmark", "#survey", "#rag", "#multimodal", "#reasoning", "#games"], "emoji": "üìö", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π RAG: —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–º–µ—Å—Ç–µ —Å–∏–ª—å–Ω–µ–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UniDoc-Bench ‚Äî –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º retrieval-a
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#rl", "#optimization", "#reasoning", "#rlhf", "#agents", "#agi"], "emoji": "ü§ù", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã —É—á–∞—Ç—Å—è –¥—Ä—É–≥ —É –¥—Ä—É–≥–∞: –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–∞—è —ç–≤–æ–ª—é—Ü–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ CoMAS ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#benchmark", "#training", "#alignment", "#long_context"], "emoji": "üìè", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Long-RewardBench ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ reward models –≤ —É—Å–ª–æ–≤–∏—è—Ö –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –≥–¥–µ –º–æ–¥–µ–ª
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#optimization", "#training", "#agi", "#long_context", "#agents"], "emoji": "üß†", "ru": {"title": "–ê–≥–µ–Ω—Ç —Å –ø–∞–º—è—Ç—å—é, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—Å—è –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–º –æ–ø—ã—Ç–µ", "desc": "MUSE ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AI-–∞–≥–µ–Ω—Ç–∞ —Å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–º –º–æ–¥—É–ª–µ–º –ø–∞–º—è—Ç–∏, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ –æ–±—É—á–∞—Ç—å—Å—è –∏ —Å–∞
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#diffusion", "#video", "#open_source", "#benchmark", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –≤–∏–¥–µ–æ –∏ –∑–≤—É–∫–∞ —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∑–∞–¥–∞—á–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å–æ –∑–≤—É–∫–æ–º –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è (Text-to-Sounding-Video). –ê–≤—Ç
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#training", "#cv", "#benchmark", "#diffusion", "#video", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤ 50 —Ä–∞–∑ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥ rCM (score-regularized continuous-time con
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#games", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≥—Ä—É–ø–ø–æ–≤—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω DGPO ‚Äî –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#long_context", "#rl", "#benchmark", "#optimization", "#data", "#reasoning", "#agents"], "emoji": "‚õèÔ∏è", "ru": {"title": "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏", "desc": "DeepMiner ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è multi-turn reasoning –∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#training", "#architecture", "#cv", "#rl", "#optimization"], "emoji": "üéØ", "ru": {"title": "ERA: –ö–æ–Ω—Ç—Ä–æ–ª—å —ç–Ω—Ç—Ä–æ–ø–∏–∏ —á–µ—Ä–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ ERA ‚Äî –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É, –∫–æ—Ç–æ—Ä–∞—è –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —ç–Ω—Ç—Ä–æ–ø–∏—é –≤—ã–±–æ—Ä–∫–∏ —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#graphs", "#reasoning", "#training", "#interpretability", "#inference"], "emoji": "üîë", "ru": {"title": "–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∫–∞–∫ –∫–ª—é—á –∫ –ø–∞–º—è—Ç–∏ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–ø–æ—Ç–µ–∑—É —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture"], "emoji": "‚ôªÔ∏è", "ru": {"title": "–ü–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∏ —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Ö –ø–∞
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#benchmark", "#science", "#multimodal", "#video", "#reasoning"], "emoji": "üî¨", "ru": {"title": "–ù–∞—É—á–Ω–æ–µ –≤–∏–¥–µ–æ-–º—ã—à–ª–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "SciVideoBench ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –∫ —Å–ª–æ–∂–Ω–æ–º—É –≤–∏–¥–µ
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#training", "#robotics", "#optimization", "#data", "#transfer_learning", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–û–¥–Ω–∞ –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–ª—è –≤—Ä–∞—â–µ–Ω–∏—è –ª—é–±—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä—É–∫–µ —Ä–æ–±–æ—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–µ—à–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –ø–µ—Ä–µ–Ω–æ—Å–∞ –Ω–∞–≤—ã–∫–æ–≤ –ª–æ–≤–∫–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –∏–∑ —Å–∏–º—É–ª—è—Ü–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –≤—Ä–∞—â
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#optimization", "#dataset", "#reasoning"], "emoji": "üîç", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ QA-—Å–∏—Å—Ç–µ–º —É—á–∏—Ç—ã–≤–∞—Ç—å –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å –≤–æ–ø—Ä–æ—Å–æ–≤ –±–µ–∑ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "A¬≤Search ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ —Ä—É—á–Ω–æ–π —Ä–∞–∑
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#reasoning", "#training", "#benchmark", "#rl", "#optimization"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –∫–æ–Ω—Ç—Ä–∞—Å—Ç–æ–º: –∫–æ–≥–¥–∞ –≤–Ω–µ—à–Ω–∏–µ –æ—Ç–≤–µ—Ç—ã –ø–æ–º–æ–≥–∞—é—Ç LLM —É—á–∏—Ç—å—Å—è –ª—É—á—à–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –º–µ—Ç–æ–¥ Group Contrastive Policy Optimization (GCPO) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#rlhf", "#training", "#optimization"], "emoji": "üé∞", "ru": {"title": "–£–º–Ω—ã–π –≤—ã–±–æ—Ä —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ö–æ–¥—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç BaRP ‚Äî —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º–µ –æ–Ω–ª–∞–π–Ω —Å —á–∞—Å—Ç–∏—á–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö —Ä–æ—É—Ç–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#3d", "#open_source"], "emoji": "üë§", "ru": {"title": "3D-–ø–æ—Ä—Ç—Ä–µ—Ç—ã –∏–∑ –æ–±—ã—á–Ω—ã—Ö —Ñ–æ—Ç–æ –±–µ–∑ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏", "desc": "UP2You - —ç—Ç–æ –ø–µ—Ä–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π –æ–¥–µ—Ç—ã—Ö –ª—é–¥–µ–π –∏–∑ –æ–±—ã—á–Ω—ã—Ö –Ω–µ–ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#3d", "#transfer_learning", "#robotics", "#data", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö 3D-–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –±–µ–∑ —Å–∏–º—É–ª—è—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç R2RGen - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–ø—Ä—è–º—É—é –∞—É–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –ø–∞—Ä—ã –Ω–∞–±–ª—é–¥–µ–Ω–∏–π-–¥–µ
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#synthetic", "#optimization", "#training", "#robotics", "#data", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–æ–±–∞—Å—Ç–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ CIFT (Coherent Information Fidelity Tuning) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–æ–±–æ—Ç
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#games", "#3d", "#diffusion", "#video", "#cv"], "emoji": "üíé", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ 3D-–º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏–∑ –æ–¥–Ω–æ–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SViM3D, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã (PBR) –¥–ª—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å —Ä–∞—Å—à–∏—Ä—è–µ—Ç lat
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#optimization", "#rlhf", "#rl"], "emoji": "üîç", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–∞—è –∫ –ª—É—á—à–µ–º—É –ø–æ–∏—Å–∫—É: LLM –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ chain-of-thought", "desc": "Search-R3 - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#dataset", "#architecture", "#open_source"], "emoji": "üé®", "ru": {"title": "–¢–æ—á–Ω–æ–µ 3D-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ –º–∞—Å–æ–∫ —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ 3DEditFormer ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É—Å–ª–æ–≤–Ω—ã–π —Ç—Ä–∞–Ω—Å
[10.10.2025 14:11] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference"], "emoji": "üß¥", "ru": {"title": "Shampoo –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à—É—é —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –ø—Ä–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç, –∫–∞–∫ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –≤–ª–∏—è—é—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ - –∫–∞–∫ –ø—Ä–∏ –ø–æ—Å—Ç—Ç—Ä–µ–Ω–∏–Ω
[10.10.2025 14:11] Renaming data file.
[10.10.2025 14:11] Renaming previous data. hf_papers.json to ./d/2025-10-10.json
[10.10.2025 14:11] Saving new data file.
[10.10.2025 14:11] Generating page.
[10.10.2025 14:11] Renaming previous page.
[10.10.2025 14:11] Renaming previous data. index.html to ./d/2025-10-10.html
[10.10.2025 14:11] Writing result.
[10.10.2025 14:11] Renaming log file.
[10.10.2025 14:11] Renaming previous data. log.txt to ./logs/2025-10-10_last_log.txt
