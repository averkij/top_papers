[10.10.2025 07:12] Read previous papers.
[10.10.2025 07:12] Generating top page (month).
[10.10.2025 07:12] Writing top page (month).
[10.10.2025 08:16] Read previous papers.
[10.10.2025 08:16] Get feed.
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08540
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08558
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07499
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08555
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08377
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03259
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23768
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07242
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08240
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08483
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03279
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08211
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08191
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07172
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08565
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08143
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08551
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08308
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03222
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08485
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03663
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08529
[10.10.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2510.08002
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06915
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08431
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08276
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08549
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08559
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08425
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08008
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08556
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07429
[10.10.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2510.08203
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07958
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08547
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24817
[10.10.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2509.24797
[10.10.2025 08:16] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02994
[10.10.2025 08:16] Extract page data from URL. URL: https://huggingface.co/papers/2509.23500
[10.10.2025 08:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.10.2025 08:16] No deleted papers detected.
[10.10.2025 08:16] Downloading and parsing papers (pdf, html). Total: 39.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08540.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08540.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08540.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08558.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08558.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08558.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.07499.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.07499.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.07499.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08555.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08555.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08555.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08377.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08377.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08377.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.03259.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.03259.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.03259.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2509.23768.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2509.23768.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2509.23768.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.07242.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.07242.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.07242.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08240.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08240.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08240.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08483.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08483.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08483.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.03279.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.03279.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.03279.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08211.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08211.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08211.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08191.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08191.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08191.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.07172.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.07172.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.07172.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08565.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08565.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08565.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08143.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08143.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08143.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08551.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08551.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08551.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08308.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08308.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08308.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.03222.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.03222.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.03222.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08485.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08485.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08485.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.03663.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.03663.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.03663.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08529.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08529.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08529.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08002.
[10.10.2025 08:16] Downloading paper 2510.08002 from http://arxiv.org/pdf/2510.08002v1...
[10.10.2025 08:16] Extracting affiliations from text.
[10.10.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 ] . [ 1 2 0 0 8 0 . 0 1 5 2 : r a Learning on the Job: An Experience-Driven, Self-Evolving Agent for Long-Horizon Tasks Cheng Yang1,2,, Xuemeng Yang2,, Licheng Wen2,4,, Daocheng Fu3,2, Jianbiao Mei5,2, Rong Wu5,2, Pinlong Cai2, Yufan Shen2, Nianchen Deng2, Botian Shi2,(cid:66) 1 Central South University, 2 Shanghai Artificial Intelligence Laboratory, 3 Fudan University, 4 Shanghai Innovation Institute, 5 Zhejiang University , Yu Qiao2, Haifeng Li1,(cid:66) "
[10.10.2025 08:16] Response: ```python
[
    "Central South University",
    "Shanghai Artificial Intelligence Laboratory",
    "Fudan University",
    "Shanghai Innovation Institute",
    "Zhejiang University"
]
```
[10.10.2025 08:16] Deleting PDF ./assets/pdf/2510.08002.pdf.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.06915.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.06915.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.06915.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08431.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08431.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08431.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08276.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08276.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08276.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08549.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08549.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08549.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08559.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08559.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08559.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08425.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08425.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08425.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08008.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08008.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08008.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08556.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08556.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08556.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.07429.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.07429.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.07429.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08203.
[10.10.2025 08:16] Downloading paper 2510.08203 from http://arxiv.org/pdf/2510.08203v1...
[10.10.2025 08:16] Extracting affiliations from text.
[10.10.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 ] . [ 1 3 0 2 8 0 . 0 1 5 2 : r a Shaohua Zhang, Yuan Lin, Hang Li "
[10.10.2025 08:16] Response: []
[10.10.2025 08:16] Extracting affiliations from text.
[10.10.2025 08:16] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 ] . [ 1 3 0 2 8 0 . 0 1 5 2 : r aShaohua Zhang, Yuan Lin, Hang LiThe remarkable success of large language models (LLMs) stems from their ability to consolidate vast amounts of knowledge into the memory during pre-training and to retrieve it from the memory during inference, enabling advanced capabilities such as knowledge memorization, instructionfollowing and reasoning. However, the mechanisms of memory retrieval and consolidation in LLMs remain poorly understood. In this paper, we propose the function token hypothesis to explain the workings of LLMs: During inference, function tokens activate the most predictive features from context and govern next token prediction (memory retrieval). During pre-training, predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features of LLMs and updates the model parameters (memory consolidation). Function tokens here roughly correspond to function words in linguistics, including punctuation marks, articles, prepositions, and conjunctions, in contrast to content tokens. We provide extensive experimental evidence supporting this hypothesis. Using bipartite graph analysis, we show that small number of function tokens activate the majority of features. Case studies further reveal how function tokens activate the most predictive features from context to direct next token prediction. We also find that during pre-training, the training loss is dominated by predicting the next content tokens following function tokens, which forces the function tokens to select the most predictive features from context. Date: October 10, 2025 Correspondence: {zhangshaohua.cola,linyuan.0,lihang.lh}@bytedance.comLarge Language Models (LLMs) [2, 4, 25, 30, 31, 46] have demonstrated remarkable capabilities. They possess strong knowledge memorization abilities, ranging from remembering simple factual knowledge (e.g., The capital of the United States is Washington, D.C.) to the verbatim reproduction of lengthy passages (e.g., Recite Martin Luther King Jrs Have Dream" speech word by word ). Beyond that, LLMs also exhibit strong general skills, such as instruction following [32, 54] (e.g., As financial analyst: explain quantitative tightening, then list three stock market impacts.) and reasoning [22, 55] (e.g., The streets are wet and the sidewalks are slick. What is the most likely explanation? ). In the human brain, long-term memory forms through synaptic consolidation, where the synapses between neurons are strengthened, ultimately creating neural circuits that store knowledge [19]. Inspired by this biological mechanism, artificial neural networks have been developed. These systems consist of neurons linked by weighted connections, and their weights (parameters) are obtained by training on data. The weights of 1 Figure 1 Function tokens can dynamically activate the most predictive features from the context to guide the next-token prediction. For example, the function token in reactivates features J.K. Rowling and Location from context (while suppressing feature French) and activates England to predict Britain. In contrast, the content token Harry activates feature Harry Potter. neuron determines how it responds to its inputs to produce an activation [15]. technique utilizing Sparse Autoencoders (SAEs) [9] has been developed recently to analyze Transformer-based LLMs [50]. It enables the decomposition of neuron activations into interpretable features, providing insights into how the circuits within the Transformers layers are composed of these interpretable features [7, 12, 17]. Despite significant progress in understanding LLM neuron activations, the memory mechanisms remain poorly understood. In particular, two fundamental questions are still not well addressed: (1) How is the memory retrieved during inference? and (2) How is the memory consolidated during pre-training? In this paper, we present our investigation into these questions. We find that analyzing from the perspective of function tokens and content tokens can help unravel the mystery of memory retrieval and memory consolidation. In linguistics, function words are words that have little semantic meanings but play crucial grammatical and connective roles within and between sentences, such as articles, prepositions, and conjunctions [5]. In contrast, content words are words that convey semantically explicit and rich meanings. The distribution of words in natural language follows Zipfs law [20]. In this distribution, function words occur with disproportionately high frequencies, occupying the head, while content words appear with much lower frequencies, forming the long tail. LLMs utilize tokens, which may represent words, sub-words, or punctuation marks. In our work, for ease of experimentation, we automatically classify tokens into function tokens and content tokens based on their frequencies in the pre-training corpus, using this as an approximation of the linguistic concepts. To investigate the role of function tokens during inference, we construct bipartite graph connecting tokens to features obtained via SAE decomposition. We show that, although few in number, function tokens activate large proportion of the LLMs features. Furthermore, our case studies show that the activation patterns for function tokens differ from those for content tokens. Function tokens dynamically reactivate predictive features from the context, whereas content tokens show little evidence of this effect. To understand why feature activations are centered on function tokens, we conduct pre-training experiments. We track next-token prediction loss across four categories based on whether the current token and the next token are function or content tokens. We find that LLMs first learn to predict function tokens before gradually learning to predict content tokens, process accompanied by an increase in the number of features and the learning of the parameters. Furthermore, pre-training is dominated by the prediction of content tokens that follow function tokens. These observations reveal why function tokens can access large portion of the LLMs features. Based on these findings, we propose the Function Token Hypothesis (see an example in Figure 1). In this paper, the LLMs are GPT-type models with Transformer decoder architecture, obtained through pre-training and post-training (including SFT and RL) [28, 37]. Both pre-training and inference are conducted autoregressively via"
[10.10.2025 08:16] Mistral response. {"id": "7da7127fc8ae4bf3a539d097a1bac4ab", "created": 1760084185, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1334, "total_tokens": 1343, "completion_tokens": 9}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"ByteDance\"]\n```"}}]}
[10.10.2025 08:16] Response: ```python
["ByteDance"]
```
[10.10.2025 08:16] Deleting PDF ./assets/pdf/2510.08203.pdf.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.07958.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.07958.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.07958.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.08547.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.08547.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.08547.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2509.24817.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2509.24817.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2509.24817.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2509.24797.
[10.10.2025 08:16] Downloading paper 2509.24797 from http://arxiv.org/pdf/2509.24797v1...
[10.10.2025 08:16] Extracting affiliations from text.
[10.10.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Preprint. Under review. FIDELITY-AWARE DATA COMPOSITION FOR ROBUST ROBOT GENERALIZATION Zizhao Tong1,5 Di Chen3 Sicheng Hu5 Hongwei Fan2,5 Liliang Chen3 Guanghui Ren3 Hao Tang4 Hao Dong2,5 Ling Shao1 1UCAS-Terminus AI Lab, University of Chinese Academy of Sciences School of Computer Science, Peking University of Multimedia Information Processing, School of Computer Science, Peking University 5PKU-Agibot Lab 4State Key Laboratory 3Agibot 2CFCS, 5 2 0 2 9 2 ] . [ 1 7 9 7 4 2 . 9 0 5 2 : r a "
[10.10.2025 08:16] Response: ```python
["UCAS-Terminus AI Lab, University of Chinese Academy of Sciences", "School of Computer Science, Peking University", "PKU-Agibot Lab", "State Key Laboratory", "Agibot", "CFCS"]
```
[10.10.2025 08:16] Deleting PDF ./assets/pdf/2509.24797.pdf.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2510.02994.
[10.10.2025 08:16] Extra JSON file exists (./assets/json/2510.02994.json), skip PDF parsing.
[10.10.2025 08:16] Paper image links file exists (./assets/img_data/2510.02994.json), skip HTML parsing.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2509.23500.
[10.10.2025 08:16] Downloading paper 2509.23500 from http://arxiv.org/pdf/2509.23500v2...
[10.10.2025 08:16] Extracting affiliations from text.
[10.10.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 ] . [ 2 0 0 5 3 2 . 9 0 5 2 : r BEYOND OUTLIERS: STUDY OF OPTIMIZERS UNDER QUANTIZATION Georgios Vlassis ETH Zurich gvlassis@ethz.ch Saleh Ashkboos ETH Zurich saleh.ashkboos@inf.ethz.ch Alexandra Volkova ISTA avolkova@ist.ac.at Torsten Hoefler ETH Zurich htor@ethz.ch Dan Alistarh ISTA & Red Hat AI dan.alistarh@ist.ac.at "
[10.10.2025 08:16] Response: ```python
["ETH Zurich", "ISTA", "Red Hat AI"]
```
[10.10.2025 08:16] Deleting PDF ./assets/pdf/2509.23500.pdf.
[10.10.2025 08:16] Success.
[10.10.2025 08:16] Enriching papers with extra data.
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 0. Existing Multimodal Large Language Models show performance deficits in long-chain reflective reasoning, which is addressed by developing MM-HELIX-100K and Adaptive Hybrid Policy Optimization, leading to improved accuracy and generalization.  					AI-generated summary 				 While current Multimodal La...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 1. Early experience, using agent-generated interaction data without reward signals, improves policy effectiveness and generalization, serving as a bridge between imitation learning and reinforcement learning.  					AI-generated summary 				 A long-term goal of language agents is to learn and improve th...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 2. Thought templates enhance long-context language models by structuring evidence combination and guiding multi-hop inference, leading to consistent performance improvements across various benchmarks.  					AI-generated summary 				 Recent Long-Context Language Models (LCLMs) can process hundreds of th...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 3. VideoCanvas addresses temporal ambiguity in latent video diffusion models to enable flexible spatio-temporal video completion using a hybrid conditioning strategy.  					AI-generated summary 				 We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arb...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 4. UniVideo, a dual-stream framework combining a Multimodal Large Language Model and a Multimodal DiT, extends unified modeling to video generation and editing, achieving state-of-the-art performance and supporting task composition and generalization.  					AI-generated summary 				 Unified multimodal ...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 5. A training pipeline called MASA enhances meta-awareness in reasoning models, leading to improved accuracy and efficiency across various benchmarks.  					AI-generated summary 				 Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by it...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 6. ChemMAS, a multi-agent system, improves reaction condition recommendation by providing interpretable rationales, outperforming existing methods in accuracy and explainability.  					AI-generated summary 				 The chemical reaction recommendation is to select proper reaction condition parameters for c...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 7. HERO, a reinforcement learning framework, combines verifier signals with reward-model scores to enhance reasoning in large language models, outperforming both RM-only and verifier-only methods.  					AI-generated summary 				 Post-training for reasoning of large language models (LLMs) increasingly r...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 8. WaltzRL, a multi-agent reinforcement learning framework, improves LLM safety and helpfulness by collaboratively training a conversation agent and a feedback agent, reducing unsafe responses and overrefusals.  					AI-generated summary 				 Harnessing the power of LLMs requires a delicate dance betwe...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 9. DeepPrune, a novel framework using dynamic pruning and a specialized judge model, significantly reduces computational inefficiency in parallel scaling of large language models by pruning redundant reasoning traces.  					AI-generated summary 				 Parallel scaling has emerged as a powerful paradigm t...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 10. MemMamba, a novel architecture integrating state summarization and cross-attention, improves long-range memory and efficiency in sequence modeling compared to Mamba and Transformers.  					AI-generated summary 				 With the explosive growth of data, long-sequence modeling has become increasingly imp...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 11. LLMs finetuned on misaligned data exhibit dishonest behavior, which can be exacerbated in downstream tasks and human-AI interactions.  					AI-generated summary 				 Previous research has shown that LLMs finetuned on malicious or incorrect completions within narrow domains (e.g., insecure code or in...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 12. Training-Free GRPO enhances LLM agent performance in specialized domains by learning experiential knowledge as a token prior without parameter updates, improving out-of-domain tasks with minimal data.  					AI-generated summary 				 Recent advances in Large Language Model (LLM) agents have demonstra...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 13. NewtonBench is a benchmark for scientific law discovery that addresses scalability, scientific relevance, and memorization resistance by using metaphysical shifts and interactive model discovery.  					AI-generated summary 				 Large language models are emerging as powerful tools for scientific law ...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 14. Native end-to-end training of Multimodal Large Language Models (MLLMs) achieves competitive performance with a balanced design and scaling relationship between visual encoders and LLMs.  					AI-generated summary 				 Compositional training has been the de-facto paradigm in existing Multimodal Large...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 15. UniMMVSR is a unified generative video super-resolution framework that incorporates hybrid-modal conditions, including text, images, and videos, within a latent video diffusion model, achieving superior detail and conformity to multi-modal conditions.  					AI-generated summary 				 Cascaded video s...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 16. ARTDECO combines feed-forward models and SLAM pipelines for efficient and accurate 3D reconstruction from monocular images.  					AI-generated summary 				 On-the-fly 3D reconstruction from monocular image sequences is a long-standing challenge in computer vision, critical for applications such as r...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 17. Analysis of reflective behaviors in reasoning models shows that reflections primarily confirm initial answers, and training with more reflections improves first-answer correctness; a question-aware early-stopping method reduces unnecessary reflections and tokens with minimal accuracy loss.  					AI-...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 18. Low-probability Regularization (Lp-Reg) enhances exploration in Reinforcement Learning with Verifiable Rewards (RLVR) by preserving valuable low-probability tokens, leading to improved performance in complex reasoning tasks.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewa...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 19. InstructX integrates Multimodal Large Language Models and diffusion models for instruction-driven image and video editing, achieving state-of-the-art performance across diverse tasks.  					AI-generated summary 				 With recent advances in Multimodal Large Language Models (MLLMs) showing strong visu...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 20. UniDoc-Bench is a large-scale benchmark for multimodal retrieval-augmented generation, evaluating systems across text, images, and their fusion in real-world document-centric scenarios.  					AI-generated summary 				 Multimodal retrieval-augmented generation (MM-RAG) is a key approach for applying ...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 21. Co-Evolving Multi-Agent Systems (CoMAS) enable LLM-based agents to improve autonomously through inter-agent interactions and intrinsic rewards, achieving state-of-the-art performance.  					AI-generated summary 				 Self-evolution is a central research topic in enabling large language model (LLM)-ba...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 22. MUSE, a novel agent framework with a hierarchical Memory Module, enables continuous learning and self-evolution, achieving state-of-the-art performance on long-horizon productivity tasks using a lightweight model.  					AI-generated summary 				 Large Language Models have demonstrated remarkable cap...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 23. A benchmark and training strategy for reward models to improve long-context consistency and performance in large language models.  					AI-generated summary 				 Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasin...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 24. Score-regularized continuous-time consistency model (rCM) improves large-scale diffusion distillation by addressing fine-detail generation and diversity issues, achieving high fidelity and accelerated sampling.  					AI-generated summary 				 This work represents the first effort to scale up continu...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 25. DeepMiner, a framework using high-difficulty training tasks and dynamic context management, enhances multi-turn reasoning agents through reinforcement learning, achieving significant performance improvements across benchmarks.  					AI-generated summary 				 While recent advances in reasoning models...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 26. ERA, a new paradigm using specially designed activations, enhances performance across LLMs, reinforcement learning, and image classification with minimal computational overhead.  					AI-generated summary 				 We propose ERA, a new paradigm that constrains the sampling entropy above given thresholds...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 27. SciVideoBench is a benchmark designed to evaluate advanced video reasoning in scientific contexts, challenging models with sophisticated domain-specific knowledge and logical reasoning.  					AI-generated summary 				 Large Multimodal Models (LMMs) have achieved remarkable progress across various ca...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 28. DGPO, a new online RL algorithm, enhances diffusion models by learning from group-level preferences, enabling the use of efficient deterministic ODE samplers and achieving faster training and superior performance.  					AI-generated summary 				 While reinforcement learning methods such as Group Rel...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 29. Recycling pretrained checkpoints through orthogonal growth methods improves large language model performance with reduced computational cost.  					AI-generated summary 				 The rapidly increasing computational cost of pretraining Large Language Models necessitates more efficient approaches. Numerou...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 30. A novel framework enables a single simulation-trained policy to generalize to diverse real-world object rotations by learning joint-wise dynamics and autonomously collecting data.  					AI-generated summary 				 Achieving generalized in-hand object rotation remains a significant challenge in robotic...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 31. BaRP, a Bandit-feedback Routing with Preferences approach, optimizes large language model selection in an online setting with partial feedback, outperforming offline routers and large models.  					AI-generated summary 				 Efficient use of large language models (LLMs) is critical for deployment at ...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 32. Function tokens in large language models activate predictive features during inference and guide memory consolidation during pre-training by predicting subsequent content tokens.  					AI-generated summary 				 The remarkable success of large language models (LLMs) stems from their ability to consol...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 33. A$^2$Search is an annotation-free framework that handles ambiguity in open-domain QA by detecting ambiguous questions, gathering alternative answers, and optimizing with RL, achieving state-of-the-art performance across benchmarks.  					AI-generated summary 				 Recent advances in Large Language Mo...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 34. A real-to-real 3D data generation framework enhances data efficiency for generalized robotic manipulation by augmenting pointcloud observations without simulation.  					AI-generated summary 				 Towards the aim of generalized robotic manipulation, spatial generalization is the most fundamental capa...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 35. UP2You reconstructs high-fidelity 3D clothed portraits from unconstrained 2D photos using a data rectifier and pose-correlated feature aggregation, achieving superior geometric and texture accuracy.  					AI-generated summary 				 We present UP2You, the first tuning-free solution for reconstructing ...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 36. Coherent Information Fidelity Tuning (CIFT) improves out-of-distribution generalization in robot policies by optimizing data composition with a generative engine, enhancing robustness and performance.  					AI-generated summary 				 Generalist robot policies trained on large-scale, visually homogene...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 37. A new framework, 3DEditFormer, uses a 3D-structure-preserving conditional transformer to enable precise and consistent 3D editing without manual masks, outperforming existing methods.  					AI-generated summary 				 3D editing - the task of locally modifying the geometry or appearance of a 3D asset ...
[10.10.2025 08:16] ********************************************************************************
[10.10.2025 08:16] Abstract 38. The study investigates how different optimizers impact model performance under post-training and quantization-aware training quantization, finding that Shampoo optimizer shows the lowest accuracy degradation and highest parameter efficiency.  					AI-generated summary 				 As new optimizers gain tra...
[10.10.2025 08:16] Read previous papers.
[10.10.2025 08:16] Generating reviews via LLM API.
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#training", "#benchmark", "#rl", "#multimodal", "#dataset", "#optimization", "#reasoning"], "emoji": "🔄", "ru": {"title": "Обучение мультимодальных LLM рефлексивному мышлению через гибридную оптимизацию", "desc": "Исследователи обнаружили, что современные мультимодальные LLM плохо с
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#transfer_learning", "#rl", "#rlhf", "#reasoning", "#agents"], "emoji": "🌉", "ru": {"title": "Ранний опыт: мост между имитацией и подкреплением", "desc": "Статья предлагает новый подход к обучению language-агентов через \"ранний опыт\" - использование данных взаимодействия, сгенерир
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#open_source", "#long_context", "#benchmark", "#multimodal", "#reasoning", "#training", "#small_models"], "emoji": "🧩", "ru": {"title": "Шаблоны мышления для улучшения многошаговых рассуждений", "desc": "Исследователи предложили метод ToTAL, который улучшает работу языковых моделей 
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#games", "#benchmark", "#diffusion", "#video"], "emoji": "🎨", "ru": {"title": "Видео как холст: произвольное заполнение в пространстве и времени", "desc": "Статья представляет VideoCanvas — метод для произвольного заполнения видео в пространстве и времени, где пользователь может раз
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#agi", "#architecture", "#games", "#multimodal", "#video", "#transfer_learning", "#open_source"], "emoji": "🎬", "ru": {"title": "Единая модель для генерации и редактирования видео по мультимодальным инструкциям", "desc": "UniVideo — это фреймворк с двухпоточной архитектурой, объедин
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#benchmark", "#training", "#optimization", "#math", "#reasoning"], "emoji": "🧠", "ru": {"title": "Метаосознанность через самовыравнивание ускоряет обучение", "desc": "Исследователи обнаружили, что большие reasoning-модели плохо понимают свой собственный процесс мышления - существует
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#interpretability", "#healthcare", "#science", "#agents", "#multimodal", "#reasoning"], "emoji": "⚗️", "ru": {"title": "Мультиагентная система с объяснениями для подбора условий химических реакций", "desc": "ChemMAS - это мультиагентная система на основе LLM для рекомендации условий
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#optimization", "#reasoning"], "emoji": "⚖️", "ru": {"title": "Лучшее из двух миров: гибридные награды для обучения рассуждению", "desc": "Статья представляет HERO — фреймворк для reinforcement learning, который комбинирует бинарные сигналы от верификато
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#rl", "#security", "#alignment", "#agents", "#rlhf"], "emoji": "💃", "ru": {"title": "Танцуя между безопасностью и полезностью: два AI-агента учатся вместе", "desc": "WaltzRL - это новый фреймворк для обучения языковых моделей безопасному поведению через multi-agent reinforcement lea
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#benchmark", "#training", "#inference", "#reasoning", "#optimization"], "emoji": "✂️", "ru": {"title": "Умная обрезка избыточных рассуждений в параллельных LLM", "desc": "DeepPrune — это новый фреймворк для эффективного параллельного скейлинга LLM, который решает проблему избыточнос
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#long_context", "#math", "#optimization", "#architecture"], "emoji": "🧠", "ru": {"title": "Долговременная память для линейных моделей последовательностей", "desc": "Статья представляет MemMamba - новую архитектуру для моделирования длинных последовательностей, которая решает проблем
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#hallucinations", "#data", "#training", "#alignment", "#ethics", "#rlhf"], "emoji": "🎭", "ru": {"title": "Как малая доля плохих данных делает AI нечестным", "desc": "Исследование показывает, что языковые модели (LLM), дообученные на некорректных данных, начинают проявлять нечестное 
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#optimization", "#transfer_learning", "#agents"], "emoji": "🎯", "ru": {"title": "Обучение агентов без обновления параметров через опытное знание", "desc": "Статья представляет метод Training-Free GRPO, который улучшает работу LLM-агентов в специализ
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#science", "#benchmark", "#agents"], "emoji": "🔬", "ru": {"title": "Научное открытие законов природы через интерактивное исследование", "desc": "NewtonBench — это benchmark для оценки способности LLM открывать научные законы, включающий 324 задачи из 12 областей физики. Вместо прост
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#benchmark", "#training", "#architecture", "#multimodal", "#optimization", "#agi"], "emoji": "🔗", "ru": {"title": "Нативное обучение мультимодальных моделей с нуля", "desc": "Исследователи изучили end-to-end обучение Multimodal Large Language Models (MLLM) вместо традиционного компо
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#video"], "emoji": "🎬", "ru": {"title": "Мультимодальный апскейлинг видео до 4K разрешения", "desc": "UniMMVSR — это универсальная генеративная система для апскейлинга видео, которая работает с гибридными условиями: текстом, изображениями и видео внутри 
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#3d", "#cv", "#robotics"], "emoji": "🏛️", "ru": {"title": "Быстрая и точная 3D-реконструкция: лучшее из двух миров", "desc": "ARTDECO - это новый фреймворк для 3D-реконструкции из монокулярных изображений в реальном времени, который объединяет эффективность feed-forward моделей с на
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#training", "#math", "#inference", "#data", "#reasoning", "#optimization"], "emoji": "🪞", "ru": {"title": "Рефлексии LLM подтверждают, а не исправляют: оптимизация через раннюю остановку", "desc": "Исследование показывает, что рефлексии в reasoning-моделях в основном подтверждают пе
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#math", "#reasoning"], "emoji": "✨", "ru": {"title": "Защита редких токенов для стабильного обучения с подкреплением", "desc": "Исследование выявило проблему в обучении с подкреплением для LLM: ценные редкие токены, которые авторы называют «искра
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#video", "#diffusion", "#multimodal", "#cv"], "emoji": "🎬", "ru": {"title": "Одна модель для редактирования и фото, и видео по текстовым инструкциям", "desc": "InstructX - это унифицированный фреймворк для редактирования изображений и видео на основе текстовых инструкций. Он интегри
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#benchmark", "#survey", "#rag", "#multimodal", "#reasoning", "#games"], "emoji": "📚", "ru": {"title": "Мультимодальный RAG: текст и изображения вместе сильнее", "desc": "Статья представляет UniDoc-Bench — первый крупномасштабный бенчмарк для оценки мультимодальных систем retrieval-a
[10.10.2025 08:16] Using data from previous issue: {"categories": ["#rl", "#optimization", "#reasoning", "#rlhf", "#agents", "#agi"], "emoji": "🤝", "ru": {"title": "Агенты учатся друг у друга: коллективная эволюция без учителя", "desc": "Исследователи представили CoMAS — фреймворк для автономного самосовершенствования LLM-агентов через взаимодействи
[10.10.2025 08:16] Querying the API.
[10.10.2025 08:16] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MUSE, a novel agent framework with a hierarchical Memory Module, enables continuous learning and self-evolution, achieving state-of-the-art performance on long-horizon productivity tasks using a lightweight model.  					AI-generated summary 				 Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from a critical limitation: they are test-time static and cannot learn from experience, lacking the ability to accumulate knowledge and continuously improve on the job. To address this challenge, we propose MUSE, a novel agent framework that introduces an experience-driven, self-evolving system centered around a hierarchical Memory Module. MUSE organizes diverse levels of experience and leverages them to plan and execute long-horizon tasks across multiple applications. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module. This mechanism enables the agent to evolve beyond its static pretrained parameters, fostering continuous learning and self-evolution. We evaluate MUSE on the long-horizon productivity benchmark TAC. It achieves new SOTA performance by a significant margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments demonstrate that as the agent autonomously accumulates experience, it exhibits increasingly superior task completion capabilities, as well as robust continuous learning and self-evolution capabilities. Moreover, the accumulated experience from MUSE exhibits strong generalization properties, enabling zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI agents capable of real-world productivity task automation.
[10.10.2025 08:16] Response: ```json
{
  "title": "Агент с памятью, который учится на собственном опыте",
  "emoji": "🧠",
  "desc": "MUSE — это новая архитектура AI-агента с иерархическим модулем памяти, который позволяет системе непрерывно обучаться и самосовершенствоваться. После выполнения каждой подзадачи агент автономно анализирует свои действия, преобразует их в структурированный опыт и сохраняет в памяти. Это позволяет агенту выходить за рамки статических предобученных параметров и постоянно улучшать свои способности решения задач. На бенчмарке длинных продуктивных задач TAC система достигла state-of-the-art результатов, используя всего лишь легковесную модель Gemini-2.5 Flash, а накопленный опыт демонстрирует strong generalization на новые задачи."
}
```
[10.10.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MUSE, a novel agent framework with a hierarchical Memory Module, enables continuous learning and self-evolution, achieving state-of-the-art performance on long-horizon productivity tasks using a lightweight model.  					AI-generated summary 				 Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from a critical limitation: they are test-time static and cannot learn from experience, lacking the ability to accumulate knowledge and continuously improve on the job. To address this challenge, we propose MUSE, a novel agent framework that introduces an experience-driven, self-evolving system centered around a hierarchical Memory Module. MUSE organizes diverse levels of experience and leverages them to plan and execute long-horizon tasks across multiple applications. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module. This mechanism enables the agent to evolve beyond its static pretrained parameters, fostering continuous learning and self-evolution. We evaluate MUSE on the long-horizon productivity benchmark TAC. It achieves new SOTA performance by a significant margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments demonstrate that as the agent autonomously accumulates experience, it exhibits increasingly superior task completion capabilities, as well as robust continuous learning and self-evolution capabilities. Moreover, the accumulated experience from MUSE exhibits strong generalization properties, enabling zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI agents capable of real-world productivity task automation."

[10.10.2025 08:16] Response: ```python
['AGENTS', 'TRAINING']
```
[10.10.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MUSE, a novel agent framework with a hierarchical Memory Module, enables continuous learning and self-evolution, achieving state-of-the-art performance on long-horizon productivity tasks using a lightweight model.  					AI-generated summary 				 Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from a critical limitation: they are test-time static and cannot learn from experience, lacking the ability to accumulate knowledge and continuously improve on the job. To address this challenge, we propose MUSE, a novel agent framework that introduces an experience-driven, self-evolving system centered around a hierarchical Memory Module. MUSE organizes diverse levels of experience and leverages them to plan and execute long-horizon tasks across multiple applications. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module. This mechanism enables the agent to evolve beyond its static pretrained parameters, fostering continuous learning and self-evolution. We evaluate MUSE on the long-horizon productivity benchmark TAC. It achieves new SOTA performance by a significant margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments demonstrate that as the agent autonomously accumulates experience, it exhibits increasingly superior task completion capabilities, as well as robust continuous learning and self-evolution capabilities. Moreover, the accumulated experience from MUSE exhibits strong generalization properties, enabling zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI agents capable of real-world productivity task automation."

[10.10.2025 08:16] Response: ```python
["AGI", "LONG_CONTEXT", "OPTIMIZATION"]
```
[10.10.2025 08:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MUSE is a new framework designed for AI agents that allows them to learn continuously and improve over time. It features a hierarchical Memory Module that helps the agent organize and utilize its experiences effectively. By reflecting on its actions after completing tasks, MUSE transforms raw experiences into structured knowledge, which enhances its performance on long-term tasks. This innovative approach enables MUSE to achieve state-of-the-art results while using a lightweight model, demonstrating strong generalization and self-evolution capabilities.","title":"MUSE: Evolving AI Agents for Continuous Learning and Productivity"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MUSE is a new framework designed for AI agents that allows them to learn continuously and improve over time. It features a hierarchical Memory Module that helps the agent organize and utilize its experiences effectively. By reflecting on its actions after completing tasks, MUSE transforms raw experiences into structured knowledge, which enhances its performance on long-term tasks. This innovative approach enables MUSE to achieve state-of-the-art results while using a lightweight model, demonstrating strong generalization and self-evolution capabilities.', title='MUSE: Evolving AI Agents for Continuous Learning and Productivity'))
[10.10.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MUSE是一个新颖的智能体框架，采用分层记忆模块，能够实现持续学习和自我进化。它解决了现有大型语言模型在长时间任务中无法从经验中学习的限制。通过组织和利用多层次的经验，MUSE能够有效地规划和执行复杂任务，并在每次子任务执行后反思其过程，将原始轨迹转化为结构化经验。实验表明，MUSE在长时间生产力基准测试中表现出色，展示了强大的任务完成能力和持续学习能力。","title":"MUSE：智能体自我进化的新范式"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MUSE是一个新颖的智能体框架，采用分层记忆模块，能够实现持续学习和自我进化。它解决了现有大型语言模型在长时间任务中无法从经验中学习的限制。通过组织和利用多层次的经验，MUSE能够有效地规划和执行复杂任务，并在每次子任务执行后反思其过程，将原始轨迹转化为结构化经验。实验表明，MUSE在长时间生产力基准测试中表现出色，展示了强大的任务完成能力和持续学习能力。', title='MUSE：智能体自我进化的新范式'))
[10.10.2025 08:17] Using data from previous issue: {"categories": ["#benchmark", "#training", "#alignment", "#long_context"], "emoji": "📏", "ru": {"title": "Обучение моделей вознаграждения для работы с длинным контекстом", "desc": "Исследователи представили Long-RewardBench — бенчмарк для оценки reward models в условиях длинного контекста, где модел
[10.10.2025 08:17] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#training", "#cv", "#benchmark", "#diffusion", "#video", "#optimization"], "emoji": "⚡", "ru": {"title": "Ускорение диффузии в 50 раз без потери качества и разнообразия", "desc": "Исследователи разработали метод rCM (score-regularized continuous-time con
[10.10.2025 08:17] Using data from previous issue: {"categories": ["#long_context", "#rl", "#benchmark", "#optimization", "#data", "#reasoning", "#agents"], "emoji": "⛏️", "ru": {"title": "Глубокое обучение агентов для многоходовых рассуждений через сложные задачи", "desc": "DeepMiner — это фреймворк для улучшения multi-turn reasoning агентов с помо
[10.10.2025 08:17] Using data from previous issue: {"categories": ["#training", "#architecture", "#cv", "#rl", "#optimization"], "emoji": "🎯", "ru": {"title": "ERA: Контроль энтропии через активации для улучшения нейросетей", "desc": "Исследователи предложили ERA — новую парадигму, которая ограничивает энтропию выборки с помощью специально разработа
[10.10.2025 08:17] Using data from previous issue: {"categories": ["#benchmark", "#science", "#multimodal", "#video", "#reasoning"], "emoji": "🔬", "ru": {"title": "Научное видео-мышление: новый вызов для мультимодальных моделей", "desc": "SciVideoBench — это новый бенчмарк для оценки способностей больших мультимодальных моделей (LMM) к сложному виде
[10.10.2025 08:17] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#games", "#optimization"], "emoji": "⚡", "ru": {"title": "Быстрое обучение диффузионных моделей через групповые предпочтения", "desc": "В статье представлен DGPO — новый алгоритм онлайн-обучения с подкреплением для диффузионных моделей, который обучается
[10.10.2025 08:17] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture"], "emoji": "♻️", "ru": {"title": "Переработка чекпоинтов: эффективное повторное использование обученных LLM", "desc": "Статья предлагает метод переработки уже обученных чекпоинтов больших языковых моделей путём увеличения количества их па
[10.10.2025 08:17] Using data from previous issue: {"categories": ["#training", "#robotics", "#optimization", "#data", "#transfer_learning", "#agents"], "emoji": "🤖", "ru": {"title": "Одна политика для вращения любых объектов в руке робота", "desc": "Исследователи решили проблему переноса навыков ловкой манипуляции из симуляции в реальность для вращ
[10.10.2025 08:17] Using data from previous issue: {"categories": ["#rlhf", "#training", "#optimization"], "emoji": "🎰", "ru": {"title": "Умный выбор языковой модели на ходу", "desc": "Статья представляет BaRP — систему для выбора оптимальной языковой модели в режиме онлайн с частичной обратной связью. В отличие от классических роутеров, которые обу
[10.10.2025 08:17] Querying the API.
[10.10.2025 08:17] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Function tokens in large language models activate predictive features during inference and guide memory consolidation during pre-training by predicting subsequent content tokens.  					AI-generated summary 				 The remarkable success of large language models (LLMs) stems from their ability to consolidate vast amounts of knowledge into the memory during pre-training and to retrieve it from the memory during inference, enabling advanced capabilities such as knowledge memorization, instruction-following and reasoning. However, the mechanisms of memory retrieval and consolidation in LLMs remain poorly understood. In this paper, we propose the function token hypothesis to explain the workings of LLMs: During inference, function tokens activate the most predictive features from context and govern next token prediction (memory retrieval). During pre-training, predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features of LLMs and updates the model parameters (memory consolidation). Function tokens here roughly correspond to function words in linguistics, including punctuation marks, articles, prepositions, and conjunctions, in contrast to content tokens. We provide extensive experimental evidence supporting this hypothesis. Using bipartite graph analysis, we show that a small number of function tokens activate the majority of features. Case studies further reveal how function tokens activate the most predictive features from context to direct next token prediction. We also find that during pre-training, the training loss is dominated by predicting the next content tokens following function tokens, which forces the function tokens to select the most predictive features from context.
[10.10.2025 08:17] Response: ```json
{
  "title": "Функциональные токены как ключ к памяти LLM",
  "desc": "Исследователи предлагают гипотезу функциональных токенов для объяснения работы больших языковых моделей. Функциональные токены (знаки препинания, артикли, предлоги, союзы) активируют наиболее предсказательные признаки из контекста во время инференса и управляют предсказанием следующего токена. Во время предобучения предсказание контентных токенов, следующих за функциональными, увеличивает количество выученных признаков и обновляет параметры модели. Экспериментально показано, что небольшое количество функциональных токенов активирует большинство признаков, а loss при обучении доминируется предсказанием именно этих переходов.",
  "emoji": "🔑",
  "desc_meta": "Paper explains LLM memory through function tokens (punctuation, articles, prepositions) that activate predictive features during inference and guide learning during training by predicting subsequent content tokens."
}
```
[10.10.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Function tokens in large language models activate predictive features during inference and guide memory consolidation during pre-training by predicting subsequent content tokens.  					AI-generated summary 				 The remarkable success of large language models (LLMs) stems from their ability to consolidate vast amounts of knowledge into the memory during pre-training and to retrieve it from the memory during inference, enabling advanced capabilities such as knowledge memorization, instruction-following and reasoning. However, the mechanisms of memory retrieval and consolidation in LLMs remain poorly understood. In this paper, we propose the function token hypothesis to explain the workings of LLMs: During inference, function tokens activate the most predictive features from context and govern next token prediction (memory retrieval). During pre-training, predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features of LLMs and updates the model parameters (memory consolidation). Function tokens here roughly correspond to function words in linguistics, including punctuation marks, articles, prepositions, and conjunctions, in contrast to content tokens. We provide extensive experimental evidence supporting this hypothesis. Using bipartite graph analysis, we show that a small number of function tokens activate the majority of features. Case studies further reveal how function tokens activate the most predictive features from context to direct next token prediction. We also find that during pre-training, the training loss is dominated by predicting the next content tokens following function tokens, which forces the function tokens to select the most predictive features from context."

[10.10.2025 08:17] Response: ```python
["INFERENCE", "TRAINING"]
```
[10.10.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Function tokens in large language models activate predictive features during inference and guide memory consolidation during pre-training by predicting subsequent content tokens.  					AI-generated summary 				 The remarkable success of large language models (LLMs) stems from their ability to consolidate vast amounts of knowledge into the memory during pre-training and to retrieve it from the memory during inference, enabling advanced capabilities such as knowledge memorization, instruction-following and reasoning. However, the mechanisms of memory retrieval and consolidation in LLMs remain poorly understood. In this paper, we propose the function token hypothesis to explain the workings of LLMs: During inference, function tokens activate the most predictive features from context and govern next token prediction (memory retrieval). During pre-training, predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features of LLMs and updates the model parameters (memory consolidation). Function tokens here roughly correspond to function words in linguistics, including punctuation marks, articles, prepositions, and conjunctions, in contrast to content tokens. We provide extensive experimental evidence supporting this hypothesis. Using bipartite graph analysis, we show that a small number of function tokens activate the majority of features. Case studies further reveal how function tokens activate the most predictive features from context to direct next token prediction. We also find that during pre-training, the training loss is dominated by predicting the next content tokens following function tokens, which forces the function tokens to select the most predictive features from context."

[10.10.2025 08:17] Response: ```python
["REASONING", "INTERPRETABILITY", "GRAPHS"]
```
[10.10.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces the function token hypothesis to explain how large language models (LLMs) operate during inference and pre-training. It suggests that function tokens, which include elements like punctuation and conjunctions, play a crucial role in activating predictive features from the context to guide the prediction of subsequent content tokens. During pre-training, the model learns to predict these content tokens following function tokens, enhancing its memory consolidation and feature learning. The authors provide experimental evidence showing that a small set of function tokens can activate a large number of features, significantly influencing the model\'s performance.","title":"Unlocking Memory: The Power of Function Tokens in LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces the function token hypothesis to explain how large language models (LLMs) operate during inference and pre-training. It suggests that function tokens, which include elements like punctuation and conjunctions, play a crucial role in activating predictive features from the context to guide the prediction of subsequent content tokens. During pre-training, the model learns to predict these content tokens following function tokens, enhancing its memory consolidation and feature learning. The authors provide experimental evidence showing that a small set of function tokens can activate a large number of features, significantly influencing the model's performance.", title='Unlocking Memory: The Power of Function Tokens in LLMs'))
[10.10.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了功能标记假说，以解释大型语言模型（LLMs）的工作机制。在推理过程中，功能标记激活上下文中最具预测性的特征，并指导下一个标记的预测（记忆检索）。在预训练阶段，预测跟随功能标记的下一个内容标记可以增加LLMs学习的特征数量，并更新模型参数（记忆巩固）。我们的实验结果表明，少量功能标记能够激活大多数特征，从而支持这一假说。","title":"功能标记：大型语言模型的记忆与预测关键"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了功能标记假说，以解释大型语言模型（LLMs）的工作机制。在推理过程中，功能标记激活上下文中最具预测性的特征，并指导下一个标记的预测（记忆检索）。在预训练阶段，预测跟随功能标记的下一个内容标记可以增加LLMs学习的特征数量，并更新模型参数（记忆巩固）。我们的实验结果表明，少量功能标记能够激活大多数特征，从而支持这一假说。', title='功能标记：大型语言模型的记忆与预测关键'))
[10.10.2025 08:17] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#optimization", "#dataset", "#reasoning"], "emoji": "🔍", "ru": {"title": "Обучение QA-систем учитывать неоднозначность вопросов без ручной разметки", "desc": "A²Search — это фреймворк для обучения моделей вопросно-ответных систем, который работает без ручной раз
[10.10.2025 08:17] Using data from previous issue: {"categories": ["#3d", "#transfer_learning", "#robotics", "#data", "#optimization"], "emoji": "🤖", "ru": {"title": "Генерация реальных 3D-данных для обучения роботов без симуляции", "desc": "Статья представляет R2RGen - фреймворк для генерации данных, который напрямую аугментирует пары наблюдений-де
[10.10.2025 08:17] Using data from previous issue: {"categories": ["#3d", "#open_source"], "emoji": "👤", "ru": {"title": "3D-портреты из обычных фото без настройки", "desc": "UP2You - это первое решение без необходимости тонкой настройки для создания высококачественных 3D-моделей одетых людей из обычных неподготовленных фотографий. В отличие от пред
[10.10.2025 08:17] Querying the API.
[10.10.2025 08:17] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Coherent Information Fidelity Tuning (CIFT) improves out-of-distribution generalization in robot policies by optimizing data composition with a generative engine, enhancing robustness and performance.  					AI-generated summary 				 Generalist robot policies trained on large-scale, visually homogeneous datasets can be susceptible to shortcut learning, which impairs their out-of-distribution (OOD) generalization. While generative data augmentation is a common approach to introduce diversity, it presents a subtle challenge: data composition. Naively mixing real and synthetic data can corrupt the learning signal, as this process often prioritizes visual diversity at the expense of information fidelity. This paper suggests that robust generalization depends on principled, fidelity-aware data composition. We introduce Coherent Information Fidelity Tuning (CIFT), a framework that treats data composition as an optimization problem. CIFT uses a practical proxy for Information Fidelity based on the feature-space geometry of a dataset. This enables the identification of a phase transition, termed the Decoherence Point, where training stability degrades. The framework includes a generative engine, Multi-View Video Augmentation (MVAug), to synthesize a causally disentangled data spectrum for this tuning process. Applying CIFT to policy architectures such as pi_0 and Diffusion Policy improves OOD success rates by over 54\%. These results indicate that fidelity-aware composition, beyond data synthesis alone, is an important component for developing robust, general-purpose robots.
[10.10.2025 08:17] Response: ```json
{
  "desc": "Статья представляет метод CIFT (Coherent Information Fidelity Tuning) для улучшения обобщающей способности робототехнических политик на новых данных. Проблема заключается в том, что простое смешивание реальных и синтетических данных может нарушить обучающий сигнал, так как приоритет отдаётся визуальному разнообразию в ущерб точности информации. CIFT рассматривает композицию данных как задачу оптимизации, используя геометрию пространства признаков для определения критической точки деградации стабильности обучения. Применение CIFT к таким архитектурам как pi_0 и Diffusion Policy повышает успешность на out-of-distribution данных более чем на 54%.",
  "emoji": "🤖",
  "title": "Оптимальное смешивание данных для робастных роботов"
}
```
[10.10.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Coherent Information Fidelity Tuning (CIFT) improves out-of-distribution generalization in robot policies by optimizing data composition with a generative engine, enhancing robustness and performance.  					AI-generated summary 				 Generalist robot policies trained on large-scale, visually homogeneous datasets can be susceptible to shortcut learning, which impairs their out-of-distribution (OOD) generalization. While generative data augmentation is a common approach to introduce diversity, it presents a subtle challenge: data composition. Naively mixing real and synthetic data can corrupt the learning signal, as this process often prioritizes visual diversity at the expense of information fidelity. This paper suggests that robust generalization depends on principled, fidelity-aware data composition. We introduce Coherent Information Fidelity Tuning (CIFT), a framework that treats data composition as an optimization problem. CIFT uses a practical proxy for Information Fidelity based on the feature-space geometry of a dataset. This enables the identification of a phase transition, termed the Decoherence Point, where training stability degrades. The framework includes a generative engine, Multi-View Video Augmentation (MVAug), to synthesize a causally disentangled data spectrum for this tuning process. Applying CIFT to policy architectures such as pi_0 and Diffusion Policy improves OOD success rates by over 54\%. These results indicate that fidelity-aware composition, beyond data synthesis alone, is an important component for developing robust, general-purpose robots."

[10.10.2025 08:17] Response: ```python
["DATA", "AGENTS", "TRAINING", "ROBOTICS"]
```
[10.10.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Coherent Information Fidelity Tuning (CIFT) improves out-of-distribution generalization in robot policies by optimizing data composition with a generative engine, enhancing robustness and performance.  					AI-generated summary 				 Generalist robot policies trained on large-scale, visually homogeneous datasets can be susceptible to shortcut learning, which impairs their out-of-distribution (OOD) generalization. While generative data augmentation is a common approach to introduce diversity, it presents a subtle challenge: data composition. Naively mixing real and synthetic data can corrupt the learning signal, as this process often prioritizes visual diversity at the expense of information fidelity. This paper suggests that robust generalization depends on principled, fidelity-aware data composition. We introduce Coherent Information Fidelity Tuning (CIFT), a framework that treats data composition as an optimization problem. CIFT uses a practical proxy for Information Fidelity based on the feature-space geometry of a dataset. This enables the identification of a phase transition, termed the Decoherence Point, where training stability degrades. The framework includes a generative engine, Multi-View Video Augmentation (MVAug), to synthesize a causally disentangled data spectrum for this tuning process. Applying CIFT to policy architectures such as pi_0 and Diffusion Policy improves OOD success rates by over 54\%. These results indicate that fidelity-aware composition, beyond data synthesis alone, is an important component for developing robust, general-purpose robots."

[10.10.2025 08:17] Response: ```python
["OPTIMIZATION", "SYNTHETIC"]
```
[10.10.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents Coherent Information Fidelity Tuning (CIFT), a method designed to enhance the out-of-distribution (OOD) generalization of robot policies. It addresses the issue of shortcut learning that arises from training on visually homogeneous datasets by optimizing how real and synthetic data are combined. CIFT treats data composition as an optimization problem, focusing on maintaining information fidelity rather than just visual diversity. The framework includes a generative engine that creates a diverse data spectrum, leading to significant improvements in the robustness and performance of robot policies.","title":"Enhancing Robot Learning with Fidelity-Aware Data Composition"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents Coherent Information Fidelity Tuning (CIFT), a method designed to enhance the out-of-distribution (OOD) generalization of robot policies. It addresses the issue of shortcut learning that arises from training on visually homogeneous datasets by optimizing how real and synthetic data are combined. CIFT treats data composition as an optimization problem, focusing on maintaining information fidelity rather than just visual diversity. The framework includes a generative engine that creates a diverse data spectrum, leading to significant improvements in the robustness and performance of robot policies.', title='Enhancing Robot Learning with Fidelity-Aware Data Composition'))
[10.10.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种名为一致性信息保真调优（CIFT）的方法，旨在提高机器人策略在分布外（OOD）环境中的泛化能力。CIFT通过优化数据组合，利用生成引擎来增强数据的多样性，同时保持信息的保真度。研究表明，简单地混合真实和合成数据可能会损害学习信号，因此需要一种更有原则的数据组合方法。通过引入多视角视频增强（MVAug），CIFT能够合成因果解耦的数据谱，从而显著提高机器人策略的性能。","title":"提升机器人策略的泛化能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种名为一致性信息保真调优（CIFT）的方法，旨在提高机器人策略在分布外（OOD）环境中的泛化能力。CIFT通过优化数据组合，利用生成引擎来增强数据的多样性，同时保持信息的保真度。研究表明，简单地混合真实和合成数据可能会损害学习信号，因此需要一种更有原则的数据组合方法。通过引入多视角视频增强（MVAug），CIFT能够合成因果解耦的数据谱，从而显著提高机器人策略的性能。', title='提升机器人策略的泛化能力'))
[10.10.2025 08:17] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#dataset", "#architecture", "#open_source"], "emoji": "🎨", "ru": {"title": "Точное 3D-редактирование без масок через трансформер", "desc": "Исследователи представили 3DEditFormer — новый фреймворк для редактирования 3D-объектов, который использует условный транс
[10.10.2025 08:17] Querying the API.
[10.10.2025 08:17] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The study investigates how different optimizers impact model performance under post-training and quantization-aware training quantization, finding that Shampoo optimizer shows the lowest accuracy degradation and highest parameter efficiency.  					AI-generated summary 				 As new optimizers gain traction and model quantization becomes standard for efficient deployment, a key question arises: how does the choice of optimizer affect model performance in the presence of quantization? Despite progress in both areas, systematic evidence on optimizer-quantization interactions remains limited. To fill this gap, we study the impact of optimizer choice on model robustness under quantization, considering both post-training quantization (PTQ), and quantization-aware training (QAT). We first train full-precision models, ranging from 50M to 1.5B parameters, with six optimizers, to explore the hyperparameter landscape, and establish well-tuned baselines. We then apply PTQ to evaluate how model performance degrades when trained with different optimizers. We find that outlier-related metrics, such as the max-to-mean ratio (MMR) and Kurtosis, fail to predict the PTQ performance across different optimizers. We show analytically that this is due to the MMR capturing only isolated layer errors, while ignoring how quantization errors accumulate and propagate through the network. To study the QAT degradation, we train quantized models from scratch and compare them to our original-precision baselines. We find that optimizers performing well in the original pretraining setup may not remain optimal under QAT, and that models trained with Shampoo show the lowest accuracy degradation. Finally, we derive scaling laws for quantization-aware training under different optimizers, showing that Shampoo achieves the highest parameter efficiency of all tested optimizers.
[10.10.2025 08:17] Response: ```json
{
  "title": "Shampoo оптимизатор показывает лучшую устойчивость при квантизации моделей",
  "desc": "Исследование изучает, как различные оптимизаторы влияют на производительность моделей при квантизации - как при посттренинговой (PTQ), так и при квантизации в процессе обучения (QAT). Авторы обнаружили, что традиционные метрики выбросов не предсказывают качество работы PTQ, так как они не учитывают накопление ошибок квантизации по всей сети. Оптимизатор Shampoo продемонстрировал наименьшее падение точности при квантизации и самую высокую эффективность по параметрам среди всех протестированных оптимизаторов. Результаты показывают, что оптимизаторы, хорошо работающие при полноточном обучении, могут терять преимущество при использовании QAT.",
  "emoji": "🧴"
}
```
[10.10.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The study investigates how different optimizers impact model performance under post-training and quantization-aware training quantization, finding that Shampoo optimizer shows the lowest accuracy degradation and highest parameter efficiency.  					AI-generated summary 				 As new optimizers gain traction and model quantization becomes standard for efficient deployment, a key question arises: how does the choice of optimizer affect model performance in the presence of quantization? Despite progress in both areas, systematic evidence on optimizer-quantization interactions remains limited. To fill this gap, we study the impact of optimizer choice on model robustness under quantization, considering both post-training quantization (PTQ), and quantization-aware training (QAT). We first train full-precision models, ranging from 50M to 1.5B parameters, with six optimizers, to explore the hyperparameter landscape, and establish well-tuned baselines. We then apply PTQ to evaluate how model performance degrades when trained with different optimizers. We find that outlier-related metrics, such as the max-to-mean ratio (MMR) and Kurtosis, fail to predict the PTQ performance across different optimizers. We show analytically that this is due to the MMR capturing only isolated layer errors, while ignoring how quantization errors accumulate and propagate through the network. To study the QAT degradation, we train quantized models from scratch and compare them to our original-precision baselines. We find that optimizers performing well in the original pretraining setup may not remain optimal under QAT, and that models trained with Shampoo show the lowest accuracy degradation. Finally, we derive scaling laws for quantization-aware training under different optimizers, showing that Shampoo achieves the highest parameter efficiency of all tested optimizers."

[10.10.2025 08:17] Response: ```python
["INFERENCE", "TRAINING"]
```
[10.10.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The study investigates how different optimizers impact model performance under post-training and quantization-aware training quantization, finding that Shampoo optimizer shows the lowest accuracy degradation and highest parameter efficiency.  					AI-generated summary 				 As new optimizers gain traction and model quantization becomes standard for efficient deployment, a key question arises: how does the choice of optimizer affect model performance in the presence of quantization? Despite progress in both areas, systematic evidence on optimizer-quantization interactions remains limited. To fill this gap, we study the impact of optimizer choice on model robustness under quantization, considering both post-training quantization (PTQ), and quantization-aware training (QAT). We first train full-precision models, ranging from 50M to 1.5B parameters, with six optimizers, to explore the hyperparameter landscape, and establish well-tuned baselines. We then apply PTQ to evaluate how model performance degrades when trained with different optimizers. We find that outlier-related metrics, such as the max-to-mean ratio (MMR) and Kurtosis, fail to predict the PTQ performance across different optimizers. We show analytically that this is due to the MMR capturing only isolated layer errors, while ignoring how quantization errors accumulate and propagate through the network. To study the QAT degradation, we train quantized models from scratch and compare them to our original-precision baselines. We find that optimizers performing well in the original pretraining setup may not remain optimal under QAT, and that models trained with Shampoo show the lowest accuracy degradation. Finally, we derive scaling laws for quantization-aware training under different optimizers, showing that Shampoo achieves the highest parameter efficiency of all tested optimizers."

[10.10.2025 08:17] Response: ```python
["OPTIMIZATION"]
```
[10.10.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This study explores how different optimizers affect the performance of machine learning models when they undergo quantization, a process that reduces model size for efficient deployment. The researchers found that the Shampoo optimizer resulted in the least accuracy loss and the best parameter efficiency compared to other optimizers. They conducted experiments with various model sizes and discovered that traditional metrics like max-to-mean ratio do not effectively predict performance degradation during quantization. The findings highlight the importance of selecting the right optimizer for both post-training quantization and quantization-aware training to maintain model robustness.","title":"Optimizing Performance: Shampoo Shines in Quantization Challenges"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This study explores how different optimizers affect the performance of machine learning models when they undergo quantization, a process that reduces model size for efficient deployment. The researchers found that the Shampoo optimizer resulted in the least accuracy loss and the best parameter efficiency compared to other optimizers. They conducted experiments with various model sizes and discovered that traditional metrics like max-to-mean ratio do not effectively predict performance degradation during quantization. The findings highlight the importance of selecting the right optimizer for both post-training quantization and quantization-aware training to maintain model robustness.', title='Optimizing Performance: Shampoo Shines in Quantization Challenges'))
[10.10.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本研究探讨了不同优化器在后训练量化和量化感知训练中的模型性能影响。研究发现，Shampoo优化器在准确性下降和参数效率方面表现最佳。我们分析了优化器选择如何影响模型在量化下的鲁棒性，并发现传统的性能预测指标无法有效预测量化后的表现。最终，我们提出了不同优化器下量化感知训练的缩放规律，证明Shampoo优化器在所有测试的优化器中具有最高的参数效率。","title":"优化器选择影响量化模型性能的研究"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本研究探讨了不同优化器在后训练量化和量化感知训练中的模型性能影响。研究发现，Shampoo优化器在准确性下降和参数效率方面表现最佳。我们分析了优化器选择如何影响模型在量化下的鲁棒性，并发现传统的性能预测指标无法有效预测量化后的表现。最终，我们提出了不同优化器下量化感知训练的缩放规律，证明Shampoo优化器在所有测试的优化器中具有最高的参数效率。', title='优化器选择影响量化模型性能的研究'))
[10.10.2025 08:17] Renaming data file.
[10.10.2025 08:17] Renaming previous data. hf_papers.json to ./d/2025-10-10.json
[10.10.2025 08:17] Saving new data file.
[10.10.2025 08:17] Generating page.
[10.10.2025 08:17] Renaming previous page.
[10.10.2025 08:17] Renaming previous data. index.html to ./d/2025-10-10.html
[10.10.2025 08:17] Writing result.
[10.10.2025 08:17] Renaming log file.
[10.10.2025 08:17] Renaming previous data. log.txt to ./logs/2025-10-10_last_log.txt
