[10.10.2025 14:11] Read previous papers.
[10.10.2025 14:11] Generating top page (month).
[10.10.2025 14:11] Writing top page (month).
[10.10.2025 15:11] Read previous papers.
[10.10.2025 15:11] Get feed.
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08558
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08540
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03279
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23768
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08377
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03259
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07499
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08555
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08240
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07242
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07172
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08551
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08483
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08191
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08211
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08143
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08565
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08308
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03222
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08485
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03663
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08529
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06915
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08002
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.03117
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08431
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08425
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08276
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08549
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08203
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08008
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08559
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08556
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07958
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07790
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07429
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24817
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08547
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08271
[10.10.2025 15:11] Extract page data from URL. URL: https://huggingface.co/papers/2510.07314
[10.10.2025 15:11] Extract page data from URL. URL: https://huggingface.co/papers/2510.06209
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02994
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24797
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23500
[10.10.2025 15:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07048
[10.10.2025 15:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.10.2025 15:11] No deleted papers detected.
[10.10.2025 15:11] Downloading and parsing papers (pdf, html). Total: 45.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08558.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08558.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08558.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08540.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08540.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08540.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.03279.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.03279.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.03279.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.23768.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2509.23768.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2509.23768.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08377.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08377.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08377.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.03259.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.03259.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.03259.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.07499.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.07499.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.07499.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08555.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08555.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08555.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08240.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08240.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08240.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.07242.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.07242.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.07242.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.07172.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.07172.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.07172.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08551.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08551.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08551.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08483.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08483.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08483.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08191.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08191.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08191.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08211.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08211.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08211.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08143.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08143.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08143.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08565.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08565.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08565.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08308.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08308.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08308.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.03222.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.03222.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.03222.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08485.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08485.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08485.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.03663.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.03663.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.03663.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08529.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08529.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08529.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.06915.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.06915.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.06915.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08002.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08002.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08002.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.03117.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.03117.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.03117.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08431.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08431.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08431.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08425.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08425.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08425.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08276.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08276.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08276.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08549.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08549.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08549.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08203.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08203.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08203.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08008.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08008.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08008.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08559.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08559.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08559.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08556.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08556.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08556.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.07958.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.07958.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.07958.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.07790.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.07790.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.07790.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.07429.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.07429.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.07429.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.24817.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2509.24817.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2509.24817.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08547.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08547.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08547.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.08271.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.08271.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.08271.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.07314.
[10.10.2025 15:11] Downloading paper 2510.07314 from http://arxiv.org/pdf/2510.07314v1...
[10.10.2025 15:11] Extracting affiliations from text.
[10.10.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 ] - l . s [ 1 4 1 3 7 0 . 0 1 5 2 : r GyroSwin: 5D Surrogates for Gyrokinetic Plasma Turbulence Simulations Fabian Paischer1, Gianluca Galletti1 William Hornsby2 Paul Setinek1 Lorenzo Zanisi2 Naomi Carey2 Stanislas Pamela2 Johannes Brandstetter1, 1 ELLIS Unit, Institute for Machine Learning, JKU Linz 2 United Kingdom Atomic Energy Authority, Culham campus 3 EMMI AI, Linz {paischer,galletti,brandstetter}@ml.jku.at github.com/ml-jku/neural-gyrokinetics "
[10.10.2025 15:11] Response: ```python
["ELLIS Unit, Institute for Machine Learning, JKU Linz", "United Kingdom Atomic Energy Authority, Culham campus", "EMMI AI, Linz"]
```
[10.10.2025 15:11] Deleting PDF ./assets/pdf/2510.07314.pdf.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.06209.
[10.10.2025 15:11] Downloading paper 2510.06209 from http://arxiv.org/pdf/2510.06209v1...
[10.10.2025 15:11] Extracting affiliations from text.
[10.10.2025 15:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models Jiahao Wang1, Zhenpei Yang2, Yijing Bai2, Yingwei Li2, Yuliang Zou2, Bo Sun2, Abhijit Kundu3, Jose Lezama3, Luna Yue Huang2, Zehao Zhu2, Jyh-Jing Hwang2, Dragomir Anguelov2, Mingxing Tan2, Chiyu Max Jiang2 5 2 0 2 7 ] . [ 1 9 0 2 6 0 . 0 1 5 2 : r a gRealg Fig. 1: By connecting driving video generation model with an end-to-end (E2E) planner, we can (1) Evaluate Synthetic Data Quality via Planner by controlling for the same traffic layout and scene conditions as the real videos to assess planner response discrepancies, (2) Assess End-to-end Planner Domain Gap via controlled experiments on operational conditions, and (3) Improve E2E Planner Performance on out-of-distribution domains via synthetic data from the video model. Planner Predictions () overlaid. Generated data in italics. Abstract Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evalu"
[10.10.2025 15:11] Response: ```python
[]
```
[10.10.2025 15:11] Extracting affiliations from text.
[10.10.2025 15:11] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models Jiahao Wang1, Zhenpei Yang2, Yijing Bai2, Yingwei Li2, Yuliang Zou2, Bo Sun2, Abhijit Kundu3, Jose Lezama3, Luna Yue Huang2, Zehao Zhu2, Jyh-Jing Hwang2, Dragomir Anguelov2, Mingxing Tan2, Chiyu Max Jiang2 5 2 0 2 7 ] . [ 1 9 0 2 6 0 . 0 1 5 2 : r agRealgFig. 1: By connecting driving video generation model with an end-to-end (E2E) planner, we can (1) Evaluate Synthetic Data Quality via Planner by controlling for the same traffic layout and scene conditions as the real videos to assess planner response discrepancies, (2) Assess End-to-end Planner Domain Gap via controlled experiments on operational conditions, and (3) Improve E2E Planner Performance on out-of-distribution domains via synthetic data from the video model. Planner Predictions () overlaid. Generated data in italics. Abstract Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing facilitating the expansion of Operational Design Domains, autonomous vehicle services into new operational contexts. *This work was supported by Waymo. 1Jiahao Wang is with Johns Hopkins University. Work done at Waymo. E-Mail: jiahaowg@gmail.com 2Authors are with Waymo. 3Authors are with Google DeepMind. I. INTRODUCTION Autonomous vehicles (AV) promise to revolutionize transportation, but ensuring their safety and reliability remains critical challenge. Typical AV development relies heavily on expensive and time-consuming real-world testing. Recently, two promising technologies have emerged with the potential to transform AV development: end-to-end (E2E) driving models [1], [2] and video generation models [3], [4], [5]. E2E models offer simplified approach to AV control by directly mapping sensor input to planning output, enabling the simplification of the AV stack and model scaling. On the other hand, video generation models can generate realistic sensor data for testing and training. Despite their potential, key questions remain for these technologies. While recent work on generating synthetic driving videos have shown increasingly impressive visual quality, it remains unclear if that correlates with the planners response. As shown in the adversarial literature [6], even the slightest perturbation in the image that is barely visible to the human eye can result in dramatically different output response of downstream deep learning model (e.g., predicting panda to be baboon). How planning models perceive the realism gap between real and synthetic data remains an open question. To our knowledge, we are among the first works to study the realism of such video generation model to facilitate the development and evaluation of an end-to-end planner. Meanwhile, E2E planner models present different set of challenges. While E2E models greatly simplify the model formulation by directly mapping sensor inputs to controls,Fig. 2: Generated videos conditioned on various conditions. (1) The top row displays the input conditions, including road maps and bounding boxes, projected to the camera. (2) The second row shows the corresponding real-world video. The subsequent rows demonstrate the models ability to generate videos under different conditions: (3) identical conditions to the original video, (4) changing the weather from no-rain to rain, (5) changing the time of day to 00:00 (at midnight), (6) with both rain and nighttime conditions. it poses key challenge on how to evaluate such models, especially their performance on out-of-distribution domains. To address these questions, our key observation is that for certain driving scene, the expected driving behavior should largely be result of the underlying traffic scene layout (e.g., road map layout and agent features such as locations, types, and sizes) and mostly independent of other visual features, such as lighting conditions, weather conditions and the appearance of each agent (e.g., red vs blue car). This is the core underlying assumption in all behavioral simulation tasks [7]. video generation model, conditioned on both the scene layout and visual features such as weather and time-of-day, can generate the same underlying traffic scenario under different visual conditions. In this light, we present Drive&Gen, framework for co-evaluating E2E driving models and video generation models (see Fig. 1). First, by controlling for the same scene layout and visual conditions as the real videos, we can study the responses from the same end-to-end planner model based on each real scene and its synthetic counterpart to evaluate the sim-to-real domain gap of the video generation model. We introduce novel statistical measures utilizing E2E driver behavior within the generated environments to quantify the realism of these virtual worlds. Second, due to the ability of the controllable video generation models to generate traffic scenarios of the same layout and different operational design domains (ODD) such as varied weather and time-of-day, we are able to do controlled experiment"
[10.10.2025 15:11] Mistral response. {"id": "7c61a43fae494c23928ceda963dc71c8", "created": 1760109118, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1368, "total_tokens": 1393, "completion_tokens": 25}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Johns Hopkins University\",\n    \"Waymo\",\n    \"Google DeepMind\"\n]\n```"}}]}
[10.10.2025 15:11] Response: ```python
[
    "Johns Hopkins University",
    "Waymo",
    "Google DeepMind"
]
```
[10.10.2025 15:11] Deleting PDF ./assets/pdf/2510.06209.pdf.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.02994.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.02994.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.02994.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.24797.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2509.24797.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2509.24797.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2509.23500.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2509.23500.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2509.23500.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Downloading and parsing paper https://huggingface.co/papers/2510.07048.
[10.10.2025 15:11] Extra JSON file exists (./assets/json/2510.07048.json), skip PDF parsing.
[10.10.2025 15:11] Paper image links file exists (./assets/img_data/2510.07048.json), skip HTML parsing.
[10.10.2025 15:11] Success.
[10.10.2025 15:11] Enriching papers with extra data.
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 0. Early experience, using agent-generated interaction data without reward signals, improves policy effectiveness and generalization, serving as a bridge between imitation learning and reinforcement learning.  					AI-generated summary 				 A long-term goal of language agents is to learn and improve th...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 1. Existing Multimodal Large Language Models show performance deficits in long-chain reflective reasoning, which is addressed by developing MM-HELIX-100K and Adaptive Hybrid Policy Optimization, leading to improved accuracy and generalization.  					AI-generated summary 				 While current Multimodal La...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 2. MemMamba, a novel architecture integrating state summarization and cross-attention, improves long-range memory and efficiency in sequence modeling compared to Mamba and Transformers.  					AI-generated summary 				 With the explosive growth of data, long-sequence modeling has become increasingly imp...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 3. ChemMAS, a multi-agent system, improves reaction condition recommendation by providing interpretable rationales, outperforming existing methods in accuracy and explainability.  					AI-generated summary 				 The chemical reaction recommendation is to select proper reaction condition parameters for c...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 4. UniVideo, a dual-stream framework combining a Multimodal Large Language Model and a Multimodal DiT, extends unified modeling to video generation and editing, achieving state-of-the-art performance and supporting task composition and generalization.  					AI-generated summary 				 Unified multimodal ...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 5. A training pipeline called MASA enhances meta-awareness in reasoning models, leading to improved accuracy and efficiency across various benchmarks.  					AI-generated summary 				 Recent studies on reasoning models explore the meta-awareness of language models, the ability to know how to think by it...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 6. Thought templates enhance long-context language models by structuring evidence combination and guiding multi-hop inference, leading to consistent performance improvements across various benchmarks.  					AI-generated summary 				 Recent Long-Context Language Models (LCLMs) can process hundreds of th...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 7. VideoCanvas addresses temporal ambiguity in latent video diffusion models to enable flexible spatio-temporal video completion using a hybrid conditioning strategy.  					AI-generated summary 				 We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arb...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 8. WaltzRL, a multi-agent reinforcement learning framework, improves LLM safety and helpfulness by collaboratively training a conversation agent and a feedback agent, reducing unsafe responses and overrefusals.  					AI-generated summary 				 Harnessing the power of LLMs requires a delicate dance betwe...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 9. HERO, a reinforcement learning framework, combines verifier signals with reward-model scores to enhance reasoning in large language models, outperforming both RM-only and verifier-only methods.  					AI-generated summary 				 Post-training for reasoning of large language models (LLMs) increasingly r...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 10. NewtonBench is a benchmark for scientific law discovery that addresses scalability, scientific relevance, and memorization resistance by using metaphysical shifts and interactive model discovery.  					AI-generated summary 				 Large language models are emerging as powerful tools for scientific law ...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 11. ARTDECO combines feed-forward models and SLAM pipelines for efficient and accurate 3D reconstruction from monocular images.  					AI-generated summary 				 On-the-fly 3D reconstruction from monocular image sequences is a long-standing challenge in computer vision, critical for applications such as r...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 12. DeepPrune, a novel framework using dynamic pruning and a specialized judge model, significantly reduces computational inefficiency in parallel scaling of large language models by pruning redundant reasoning traces.  					AI-generated summary 				 Parallel scaling has emerged as a powerful paradigm t...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 13. Training-Free GRPO enhances LLM agent performance in specialized domains by learning experiential knowledge as a token prior without parameter updates, improving out-of-domain tasks with minimal data.  					AI-generated summary 				 Recent advances in Large Language Model (LLM) agents have demonstra...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 14. LLMs finetuned on misaligned data exhibit dishonest behavior, which can be exacerbated in downstream tasks and human-AI interactions.  					AI-generated summary 				 Previous research has shown that LLMs finetuned on malicious or incorrect completions within narrow domains (e.g., insecure code or in...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 15. UniMMVSR is a unified generative video super-resolution framework that incorporates hybrid-modal conditions, including text, images, and videos, within a latent video diffusion model, achieving superior detail and conformity to multi-modal conditions.  					AI-generated summary 				 Cascaded video s...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 16. Native end-to-end training of Multimodal Large Language Models (MLLMs) achieves competitive performance with a balanced design and scaling relationship between visual encoders and LLMs.  					AI-generated summary 				 Compositional training has been the de-facto paradigm in existing Multimodal Large...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 17. Analysis of reflective behaviors in reasoning models shows that reflections primarily confirm initial answers, and training with more reflections improves first-answer correctness; a question-aware early-stopping method reduces unnecessary reflections and tokens with minimal accuracy loss.  					AI-...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 18. Low-probability Regularization (Lp-Reg) enhances exploration in Reinforcement Learning with Verifiable Rewards (RLVR) by preserving valuable low-probability tokens, leading to improved performance in complex reasoning tasks.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewa...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 19. InstructX integrates Multimodal Large Language Models and diffusion models for instruction-driven image and video editing, achieving state-of-the-art performance across diverse tasks.  					AI-generated summary 				 With recent advances in Multimodal Large Language Models (MLLMs) showing strong visu...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 20. UniDoc-Bench is a large-scale benchmark for multimodal retrieval-augmented generation, evaluating systems across text, images, and their fusion in real-world document-centric scenarios.  					AI-generated summary 				 Multimodal retrieval-augmented generation (MM-RAG) is a key approach for applying ...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 21. Co-Evolving Multi-Agent Systems (CoMAS) enable LLM-based agents to improve autonomously through inter-agent interactions and intrinsic rewards, achieving state-of-the-art performance.  					AI-generated summary 				 Self-evolution is a central research topic in enabling large language model (LLM)-ba...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 22. A benchmark and training strategy for reward models to improve long-context consistency and performance in large language models.  					AI-generated summary 				 Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasin...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 23. MUSE, a novel agent framework with a hierarchical Memory Module, enables continuous learning and self-evolution, achieving state-of-the-art performance on long-horizon productivity tasks using a lightweight model.  					AI-generated summary 				 Large Language Models have demonstrated remarkable cap...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 24. A novel dual-tower diffusion transformer with a Dual CrossAttention mechanism addresses challenges in Text-to-Sounding-Video generation by disentangling captions and enabling symmetric information exchange.  					AI-generated summary 				 This study focuses on a challenging yet promising task, Text-...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 25. Score-regularized continuous-time consistency model (rCM) improves large-scale diffusion distillation by addressing fine-detail generation and diversity issues, achieving high fidelity and accelerated sampling.  					AI-generated summary 				 This work represents the first effort to scale up continu...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 26. DGPO, a new online RL algorithm, enhances diffusion models by learning from group-level preferences, enabling the use of efficient deterministic ODE samplers and achieving faster training and superior performance.  					AI-generated summary 				 While reinforcement learning methods such as Group Rel...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 27. DeepMiner, a framework using high-difficulty training tasks and dynamic context management, enhances multi-turn reasoning agents through reinforcement learning, achieving significant performance improvements across benchmarks.  					AI-generated summary 				 While recent advances in reasoning models...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 28. ERA, a new paradigm using specially designed activations, enhances performance across LLMs, reinforcement learning, and image classification with minimal computational overhead.  					AI-generated summary 				 We propose ERA, a new paradigm that constrains the sampling entropy above given thresholds...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 29. Function tokens in large language models activate predictive features during inference and guide memory consolidation during pre-training by predicting subsequent content tokens.  					AI-generated summary 				 The remarkable success of large language models (LLMs) stems from their ability to consol...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 30. Recycling pretrained checkpoints through orthogonal growth methods improves large language model performance with reduced computational cost.  					AI-generated summary 				 The rapidly increasing computational cost of pretraining Large Language Models necessitates more efficient approaches. Numerou...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 31. SciVideoBench is a benchmark designed to evaluate advanced video reasoning in scientific contexts, challenging models with sophisticated domain-specific knowledge and logical reasoning.  					AI-generated summary 				 Large Multimodal Models (LMMs) have achieved remarkable progress across various ca...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 32. A novel framework enables a single simulation-trained policy to generalize to diverse real-world object rotations by learning joint-wise dynamics and autonomously collecting data.  					AI-generated summary 				 Achieving generalized in-hand object rotation remains a significant challenge in robotic...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 33. A$^2$Search is an annotation-free framework that handles ambiguity in open-domain QA by detecting ambiguous questions, gathering alternative answers, and optimizing with RL, achieving state-of-the-art performance across benchmarks.  					AI-generated summary 				 Recent advances in Large Language Mo...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 34. Group Contrastive Policy Optimization (GCPO) enhances reinforcement learning for large language models by incorporating external reference answers, improving training efficiency and generalization.  					AI-generated summary 				 Reinforcement learning has been widely applied to enhance the reasonin...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 35. BaRP, a Bandit-feedback Routing with Preferences approach, optimizes large language model selection in an online setting with partial feedback, outperforming offline routers and large models.  					AI-generated summary 				 Efficient use of large language models (LLMs) is critical for deployment at ...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 36. UP2You reconstructs high-fidelity 3D clothed portraits from unconstrained 2D photos using a data rectifier and pose-correlated feature aggregation, achieving superior geometric and texture accuracy.  					AI-generated summary 				 We present UP2You, the first tuning-free solution for reconstructing ...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 37. A real-to-real 3D data generation framework enhances data efficiency for generalized robotic manipulation by augmenting pointcloud observations without simulation.  					AI-generated summary 				 Towards the aim of generalized robotic manipulation, spatial generalization is the most fundamental capa...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 38. A latent video diffusion model predicts multi-view consistent PBR materials from a single image, enabling relighting and novel view synthesis with high quality.  					AI-generated summary 				 We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically base...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 39. GyroSwin, a scalable 5D neural surrogate model, captures nonlinear gyrokinetic dynamics and improves heat flux prediction in plasma turbulence simulations.  					AI-generated summary 				 Nuclear fusion plays a pivotal role in the quest for reliable and sustainable energy production. A major roadblo...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 40. A novel approach combining driving models and generative world models evaluates and enhances the realism and generalization of synthetic video data for autonomous vehicle testing and planning.  					AI-generated summary 				 Recent advances in generative models have sparked exciting new possibilitie...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 41. A new framework, 3DEditFormer, uses a 3D-structure-preserving conditional transformer to enable precise and consistent 3D editing without manual masks, outperforming existing methods.  					AI-generated summary 				 3D editing - the task of locally modifying the geometry or appearance of a 3D asset ...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 42. Coherent Information Fidelity Tuning (CIFT) improves out-of-distribution generalization in robot policies by optimizing data composition with a generative engine, enhancing robustness and performance.  					AI-generated summary 				 Generalist robot policies trained on large-scale, visually homogene...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 43. The study investigates how different optimizers impact model performance under post-training and quantization-aware training quantization, finding that Shampoo optimizer shows the lowest accuracy degradation and highest parameter efficiency.  					AI-generated summary 				 As new optimizers gain tra...
[10.10.2025 15:11] ********************************************************************************
[10.10.2025 15:11] Abstract 44. Search-R3 is a framework that adapts LLMs to generate effective search embeddings through chain-of-thought reasoning, supervised learning, and reinforcement learning.  					AI-generated summary 				 Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) ha...
[10.10.2025 15:11] Read previous papers.
[10.10.2025 15:11] Generating reviews via LLM API.
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#transfer_learning", "#rl", "#rlhf", "#reasoning", "#agents"], "emoji": "ðŸŒ‰", "ru": {"title": "Ð Ð°Ð½Ð½Ð¸Ð¹ Ð¾Ð¿Ñ‹Ñ‚: Ð¼Ð¾ÑÑ‚ Ð¼ÐµÐ¶Ð´Ñƒ Ð¸Ð¼Ð¸Ñ‚Ð°Ñ†Ð¸ÐµÐ¹ Ð¸ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÑŽ language-Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· \"Ñ€Ð°Ð½Ð½Ð¸Ð¹ Ð¾Ð¿Ñ‹Ñ‚\" - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ, ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#training", "#benchmark", "#rl", "#multimodal", "#dataset", "#optimization", "#reasoning"], "emoji": "ðŸ”„", "ru": {"title": "ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… LLM Ñ€ÐµÑ„Ð»ÐµÐºÑÐ¸Ð²Ð½Ð¾Ð¼Ñƒ Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸ÑŽ Ñ‡ÐµÑ€ÐµÐ· Ð³Ð¸Ð±Ñ€Ð¸Ð´Ð½ÑƒÑŽ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑŽ", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶Ð¸Ð»Ð¸, Ñ‡Ñ‚Ð¾ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ðµ LLM Ð¿Ð»Ð¾Ñ…Ð¾ Ñ
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#long_context", "#math", "#optimization", "#architecture"], "emoji": "ðŸ§ ", "ru": {"title": "Ð”Ð¾Ð»Ð³Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒ Ð´Ð»Ñ Ð»Ð¸Ð½ÐµÐ¹Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÐµÐ¹", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ MemMamba - Ð½Ð¾Ð²ÑƒÑŽ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñƒ Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÐµÐ¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#interpretability", "#healthcare", "#science", "#agents", "#multimodal", "#reasoning"], "emoji": "âš—ï¸", "ru": {"title": "ÐœÑƒÐ»ÑŒÑ‚Ð¸Ð°Ð³ÐµÐ½Ñ‚Ð½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ñ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð´Ð»Ñ Ð¿Ð¾Ð´Ð±Ð¾Ñ€Ð° ÑƒÑÐ»Ð¾Ð²Ð¸Ð¹ Ñ…Ð¸Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ€ÐµÐ°ÐºÑ†Ð¸Ð¹", "desc": "ChemMAS - ÑÑ‚Ð¾ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð°Ð³ÐµÐ½Ñ‚Ð½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ LLM Ð´Ð»Ñ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸ ÑƒÑÐ»Ð¾Ð²Ð¸Ð¹
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#agi", "#architecture", "#games", "#multimodal", "#video", "#transfer_learning", "#open_source"], "emoji": "ðŸŽ¬", "ru": {"title": "Ð•Ð´Ð¸Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾ Ð¿Ð¾ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ð¼ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÑÐ¼", "desc": "UniVideo â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ñ Ð´Ð²ÑƒÑ…Ð¿Ð¾Ñ‚Ð¾Ñ‡Ð½Ð¾Ð¹ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð¾Ð¹, Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#training", "#optimization", "#math", "#reasoning"], "emoji": "ðŸ§ ", "ru": {"title": "ÐœÐµÑ‚Ð°Ð¾ÑÐ¾Ð·Ð½Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒ Ñ‡ÐµÑ€ÐµÐ· ÑÐ°Ð¼Ð¾Ð²Ñ‹Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ðµ ÑƒÑÐºÐ¾Ñ€ÑÐµÑ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶Ð¸Ð»Ð¸, Ñ‡Ñ‚Ð¾ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ reasoning-Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¿Ð»Ð¾Ñ…Ð¾ Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÑŽÑ‚ ÑÐ²Ð¾Ð¹ ÑÐ¾Ð±ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ñ - ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#open_source", "#long_context", "#benchmark", "#multimodal", "#reasoning", "#training", "#small_models"], "emoji": "ðŸ§©", "ru": {"title": "Ð¨Ð°Ð±Ð»Ð¾Ð½Ñ‹ Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ñ Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¼Ð½Ð¾Ð³Ð¾ÑˆÐ°Ð³Ð¾Ð²Ñ‹Ñ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸Ð»Ð¸ Ð¼ÐµÑ‚Ð¾Ð´ ToTAL, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ 
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#games", "#benchmark", "#diffusion", "#video"], "emoji": "ðŸŽ¨", "ru": {"title": "Ð’Ð¸Ð´ÐµÐ¾ ÐºÐ°Ðº Ñ…Ð¾Ð»ÑÑ‚: Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð»ÑŒÐ½Ð¾Ðµ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð² Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ðµ Ð¸ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ VideoCanvas â€” Ð¼ÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾ Ð² Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ðµ Ð¸ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸, Ð³Ð´Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ Ð¼Ð¾Ð¶ÐµÑ‚ Ñ€Ð°Ð·
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#security", "#alignment", "#agents", "#rlhf"], "emoji": "ðŸ’ƒ", "ru": {"title": "Ð¢Ð°Ð½Ñ†ÑƒÑ Ð¼ÐµÐ¶Ð´Ñƒ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒÑŽ Ð¸ Ð¿Ð¾Ð»ÐµÐ·Ð½Ð¾ÑÑ‚ÑŒÑŽ: Ð´Ð²Ð° AI-Ð°Ð³ÐµÐ½Ñ‚Ð° ÑƒÑ‡Ð°Ñ‚ÑÑ Ð²Ð¼ÐµÑÑ‚Ðµ", "desc": "WaltzRL - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾Ð¼Ñƒ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸ÑŽ Ñ‡ÐµÑ€ÐµÐ· multi-agent reinforcement lea
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#optimization", "#reasoning"], "emoji": "âš–ï¸", "ru": {"title": "Ð›ÑƒÑ‡ÑˆÐµÐµ Ð¸Ð· Ð´Ð²ÑƒÑ… Ð¼Ð¸Ñ€Ð¾Ð²: Ð³Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ðµ Ð½Ð°Ð³Ñ€Ð°Ð´Ñ‹ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸ÑŽ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ HERO â€” Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ reinforcement learning, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€ÑƒÐµÑ‚ Ð±Ð¸Ð½Ð°Ñ€Ð½Ñ‹Ðµ ÑÐ¸Ð³Ð½Ð°Ð»Ñ‹ Ð¾Ñ‚ Ð²ÐµÑ€Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#science", "#benchmark", "#agents"], "emoji": "ðŸ”¬", "ru": {"title": "ÐÐ°ÑƒÑ‡Ð½Ð¾Ðµ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Ð·Ð°ÐºÐ¾Ð½Ð¾Ð² Ð¿Ñ€Ð¸Ñ€Ð¾Ð´Ñ‹ Ñ‡ÐµÑ€ÐµÐ· Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ", "desc": "NewtonBench â€” ÑÑ‚Ð¾ benchmark Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ LLM Ð¾Ñ‚ÐºÑ€Ñ‹Ð²Ð°Ñ‚ÑŒ Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ðµ Ð·Ð°ÐºÐ¾Ð½Ñ‹, Ð²ÐºÐ»ÑŽÑ‡Ð°ÑŽÑ‰Ð¸Ð¹ 324 Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¸Ð· 12 Ð¾Ð±Ð»Ð°ÑÑ‚ÐµÐ¹ Ñ„Ð¸Ð·Ð¸ÐºÐ¸. Ð’Ð¼ÐµÑÑ‚Ð¾ Ð¿Ñ€Ð¾ÑÑ‚
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#3d", "#cv", "#robotics"], "emoji": "ðŸ›ï¸", "ru": {"title": "Ð‘Ñ‹ÑÑ‚Ñ€Ð°Ñ Ð¸ Ñ‚Ð¾Ñ‡Ð½Ð°Ñ 3D-Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ: Ð»ÑƒÑ‡ÑˆÐµÐµ Ð¸Ð· Ð´Ð²ÑƒÑ… Ð¼Ð¸Ñ€Ð¾Ð²", "desc": "ARTDECO - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ 3D-Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ Ð¸Ð· Ð¼Ð¾Ð½Ð¾ÐºÑƒÐ»ÑÑ€Ð½Ñ‹Ñ… Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÐµÑ‚ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ feed-forward Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ð½Ð°
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#training", "#inference", "#reasoning", "#optimization"], "emoji": "âœ‚ï¸", "ru": {"title": "Ð£Ð¼Ð½Ð°Ñ Ð¾Ð±Ñ€ÐµÐ·ÐºÐ° Ð¸Ð·Ð±Ñ‹Ñ‚Ð¾Ñ‡Ð½Ñ‹Ñ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð² Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ñ‹Ñ… LLM", "desc": "DeepPrune â€” ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ ÑÐºÐµÐ¹Ð»Ð¸Ð½Ð³Ð° LLM, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ Ð¸Ð·Ð±Ñ‹Ñ‚Ð¾Ñ‡Ð½Ð¾Ñ
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#optimization", "#transfer_learning", "#agents"], "emoji": "ðŸŽ¯", "ru": {"title": "ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð±ÐµÐ· Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð¿Ñ‹Ñ‚Ð½Ð¾Ðµ Ð·Ð½Ð°Ð½Ð¸Ðµ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ Training-Free GRPO, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ LLM-Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð² ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#hallucinations", "#data", "#training", "#alignment", "#ethics", "#rlhf"], "emoji": "ðŸŽ­", "ru": {"title": "ÐšÐ°Ðº Ð¼Ð°Ð»Ð°Ñ Ð´Ð¾Ð»Ñ Ð¿Ð»Ð¾Ñ…Ð¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´ÐµÐ»Ð°ÐµÑ‚ AI Ð½ÐµÑ‡ÐµÑÑ‚Ð½Ñ‹Ð¼", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (LLM), Ð´Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ðµ Ð½Ð° Ð½ÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ð½Ð°Ñ‡Ð¸Ð½Ð°ÑŽÑ‚ Ð¿Ñ€Ð¾ÑÐ²Ð»ÑÑ‚ÑŒ Ð½ÐµÑ‡ÐµÑÑ‚Ð½Ð¾Ðµ 
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#video"], "emoji": "ðŸŽ¬", "ru": {"title": "ÐœÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð°Ð¿ÑÐºÐµÐ¹Ð»Ð¸Ð½Ð³ Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð¾ 4K Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ñ", "desc": "UniMMVSR â€” ÑÑ‚Ð¾ ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ Ð°Ð¿ÑÐºÐµÐ¹Ð»Ð¸Ð½Ð³Ð° Ð²Ð¸Ð´ÐµÐ¾, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ñ Ð³Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¼Ð¸ ÑƒÑÐ»Ð¾Ð²Ð¸ÑÐ¼Ð¸: Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼, Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ 
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#training", "#architecture", "#multimodal", "#optimization", "#agi"], "emoji": "ðŸ”—", "ru": {"title": "ÐÐ°Ñ‚Ð¸Ð²Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ð½ÑƒÐ»Ñ", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¸Ð·ÑƒÑ‡Ð¸Ð»Ð¸ end-to-end Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Multimodal Large Language Models (MLLM) Ð²Ð¼ÐµÑÑ‚Ð¾ Ñ‚Ñ€Ð°Ð´Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð¼Ð¿Ð¾
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#training", "#math", "#inference", "#data", "#reasoning", "#optimization"], "emoji": "ðŸªž", "ru": {"title": "Ð ÐµÑ„Ð»ÐµÐºÑÐ¸Ð¸ LLM Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´Ð°ÑŽÑ‚, Ð° Ð½Ðµ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÑÑŽÑ‚: Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‡ÐµÑ€ÐµÐ· Ñ€Ð°Ð½Ð½ÑŽÑŽ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÑƒ", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ Ñ€ÐµÑ„Ð»ÐµÐºÑÐ¸Ð¸ Ð² reasoning-Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Ð² Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¼ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´Ð°ÑŽÑ‚ Ð¿Ðµ
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#math", "#reasoning"], "emoji": "âœ¨", "ru": {"title": "Ð—Ð°Ñ‰Ð¸Ñ‚Ð° Ñ€ÐµÐ´ÐºÐ¸Ñ… Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð´Ð»Ñ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð²Ñ‹ÑÐ²Ð¸Ð»Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ Ð² Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ LLM: Ñ†ÐµÐ½Ð½Ñ‹Ðµ Ñ€ÐµÐ´ÐºÐ¸Ðµ Ñ‚Ð¾ÐºÐµÐ½Ñ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð°Ð²Ñ‚Ð¾Ñ€Ñ‹ Ð½Ð°Ð·Ñ‹Ð²Ð°ÑŽÑ‚ Â«Ð¸ÑÐºÑ€Ð°
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#video", "#diffusion", "#multimodal", "#cv"], "emoji": "ðŸŽ¬", "ru": {"title": "ÐžÐ´Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ Ñ„Ð¾Ñ‚Ð¾, Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¼ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÑÐ¼", "desc": "InstructX - ÑÑ‚Ð¾ ÑƒÐ½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¹. ÐžÐ½ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#survey", "#rag", "#multimodal", "#reasoning", "#games"], "emoji": "ðŸ“š", "ru": {"title": "ÐœÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ð¹ RAG: Ñ‚ÐµÐºÑÑ‚ Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð²Ð¼ÐµÑÑ‚Ðµ ÑÐ¸Ð»ÑŒÐ½ÐµÐµ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ UniDoc-Bench â€” Ð¿ÐµÑ€Ð²Ñ‹Ð¹ ÐºÑ€ÑƒÐ¿Ð½Ð¾Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð½Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼ retrieval-a
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#optimization", "#reasoning", "#rlhf", "#agents", "#agi"], "emoji": "ðŸ¤", "ru": {"title": "ÐÐ³ÐµÐ½Ñ‚Ñ‹ ÑƒÑ‡Ð°Ñ‚ÑÑ Ð´Ñ€ÑƒÐ³ Ñƒ Ð´Ñ€ÑƒÐ³Ð°: ÐºÐ¾Ð»Ð»ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ ÑÐ²Ð¾Ð»ÑŽÑ†Ð¸Ñ Ð±ÐµÐ· ÑƒÑ‡Ð¸Ñ‚ÐµÐ»Ñ", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ CoMAS â€” Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ð¾Ð³Ð¾ ÑÐ°Ð¼Ð¾ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½ÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð¸Ñ LLM-Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#training", "#alignment", "#long_context"], "emoji": "ðŸ“", "ru": {"title": "ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ð¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ Long-RewardBench â€” Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ reward models Ð² ÑƒÑÐ»Ð¾Ð²Ð¸ÑÑ… Ð´Ð»Ð¸Ð½Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°, Ð³Ð´Ðµ Ð¼Ð¾Ð´ÐµÐ»
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#optimization", "#training", "#agi", "#long_context", "#agents"], "emoji": "ðŸ§ ", "ru": {"title": "ÐÐ³ÐµÐ½Ñ‚ Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑƒÑ‡Ð¸Ñ‚ÑÑ Ð½Ð° ÑÐ¾Ð±ÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð¼ Ð¾Ð¿Ñ‹Ñ‚Ðµ", "desc": "MUSE â€” ÑÑ‚Ð¾ Ð½Ð¾Ð²Ð°Ñ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° AI-Ð°Ð³ÐµÐ½Ñ‚Ð° Ñ Ð¸ÐµÑ€Ð°Ñ€Ñ…Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ð¼Ð¾Ð´ÑƒÐ»ÐµÐ¼ Ð¿Ð°Ð¼ÑÑ‚Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ Ð½ÐµÐ¿Ñ€ÐµÑ€Ñ‹Ð²Ð½Ð¾ Ð¾Ð±ÑƒÑ‡Ð°Ñ‚ÑŒÑÑ Ð¸ ÑÐ°
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#diffusion", "#video", "#open_source", "#benchmark", "#multimodal"], "emoji": "ðŸŽ¬", "ru": {"title": "Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾Ðµ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²Ð¸Ð´ÐµÐ¾ Ð¸ Ð·Ð²ÑƒÐºÐ° Ñ‡ÐµÑ€ÐµÐ· Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ð¹", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ð¾ÑÐ²ÑÑ‰ÐµÐ½Ð° Ð·Ð°Ð´Ð°Ñ‡Ðµ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ ÑÐ¾ Ð·Ð²ÑƒÐºÐ¾Ð¼ Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ð¾Ð³Ð¾ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ (Text-to-Sounding-Video). ÐÐ²Ñ‚
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#training", "#cv", "#benchmark", "#diffusion", "#video", "#optimization"], "emoji": "âš¡", "ru": {"title": "Ð£ÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¸ Ð² 50 Ñ€Ð°Ð· Ð±ÐµÐ· Ð¿Ð¾Ñ‚ÐµÑ€Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð¸ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¸Ñ", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð»Ð¸ Ð¼ÐµÑ‚Ð¾Ð´ rCM (score-regularized continuous-time con
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#games", "#optimization"], "emoji": "âš¡", "ru": {"title": "Ð‘Ñ‹ÑÑ‚Ñ€Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ‡ÐµÑ€ÐµÐ· Ð³Ñ€ÑƒÐ¿Ð¿Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸Ñ", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½ DGPO â€” Ð½Ð¾Ð²Ñ‹Ð¹ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ Ð¾Ð½Ð»Ð°Ð¹Ð½-Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¾Ð±ÑƒÑ‡Ð°ÐµÑ‚ÑÑ
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#long_context", "#rl", "#benchmark", "#optimization", "#data", "#reasoning", "#agents"], "emoji": "â›ï¸", "ru": {"title": "Ð“Ð»ÑƒÐ±Ð¾ÐºÐ¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð¼Ð½Ð¾Ð³Ð¾Ñ…Ð¾Ð´Ð¾Ð²Ñ‹Ñ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ñ‡ÐµÑ€ÐµÐ· ÑÐ»Ð¾Ð¶Ð½Ñ‹Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸", "desc": "DeepMiner â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ multi-turn reasoning Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ñ Ð¿Ð¾Ð¼Ð¾
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#training", "#architecture", "#cv", "#rl", "#optimization"], "emoji": "ðŸŽ¯", "ru": {"title": "ERA: ÐšÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ ÑÐ½Ñ‚Ñ€Ð¾Ð¿Ð¸Ð¸ Ñ‡ÐµÑ€ÐµÐ· Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÐµÐ¹", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸Ð»Ð¸ ERA â€” Ð½Ð¾Ð²ÑƒÑŽ Ð¿Ð°Ñ€Ð°Ð´Ð¸Ð³Ð¼Ñƒ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÑ‚ ÑÐ½Ñ‚Ñ€Ð¾Ð¿Ð¸ÑŽ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ð¾ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#graphs", "#reasoning", "#training", "#interpretability", "#inference"], "emoji": "ðŸ”‘", "ru": {"title": "Ð¤ÑƒÐ½ÐºÑ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ ÐºÐ°Ðº ÐºÐ»ÑŽÑ‡ Ðº Ð¿Ð°Ð¼ÑÑ‚Ð¸ LLM", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð³Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ñƒ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð´Ð»Ñ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. Ð¤ÑƒÐ½ÐºÑ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ‚Ð¾ÐºÐµ
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture"], "emoji": "â™»ï¸", "ru": {"title": "ÐŸÐµÑ€ÐµÑ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚Ð¾Ð²: ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ð¾Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… LLM", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ Ð¿ÐµÑ€ÐµÑ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÑƒÐ¶Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚Ð¾Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿ÑƒÑ‚Ñ‘Ð¼ ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¸Ñ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° Ð¸Ñ… Ð¿Ð°
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#benchmark", "#science", "#multimodal", "#video", "#reasoning"], "emoji": "ðŸ”¬", "ru": {"title": "ÐÐ°ÑƒÑ‡Ð½Ð¾Ðµ Ð²Ð¸Ð´ÐµÐ¾-Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ðµ: Ð½Ð¾Ð²Ñ‹Ð¹ Ð²Ñ‹Ð·Ð¾Ð² Ð´Ð»Ñ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "SciVideoBench â€” ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LMM) Ðº ÑÐ»Ð¾Ð¶Ð½Ð¾Ð¼Ñƒ Ð²Ð¸Ð´Ðµ
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#training", "#robotics", "#optimization", "#data", "#transfer_learning", "#agents"], "emoji": "ðŸ¤–", "ru": {"title": "ÐžÐ´Ð½Ð° Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ° Ð´Ð»Ñ Ð²Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ñ Ð»ÑŽÐ±Ñ‹Ñ… Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð² Ñ€ÑƒÐºÐµ Ñ€Ð¾Ð±Ð¾Ñ‚Ð°", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ñ€ÐµÑˆÐ¸Ð»Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÐ° Ð½Ð°Ð²Ñ‹ÐºÐ¾Ð² Ð»Ð¾Ð²ÐºÐ¾Ð¹ Ð¼Ð°Ð½Ð¸Ð¿ÑƒÐ»ÑÑ†Ð¸Ð¸ Ð¸Ð· ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ð¸ Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð´Ð»Ñ Ð²Ñ€Ð°Ñ‰
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#optimization", "#dataset", "#reasoning"], "emoji": "ðŸ”", "ru": {"title": "ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ QA-ÑÐ¸ÑÑ‚ÐµÐ¼ ÑƒÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ð½ÐµÐ¾Ð´Ð½Ð¾Ð·Ð½Ð°Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð±ÐµÐ· Ñ€ÑƒÑ‡Ð½Ð¾Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÐ¸", "desc": "AÂ²Search â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ½Ð¾-Ð¾Ñ‚Ð²ÐµÑ‚Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð±ÐµÐ· Ñ€ÑƒÑ‡Ð½Ð¾Ð¹ Ñ€Ð°Ð·
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#reasoning", "#training", "#benchmark", "#rl", "#optimization"], "emoji": "ðŸŽ¯", "ru": {"title": "ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ ÐºÐ¾Ð½Ñ‚Ñ€Ð°ÑÑ‚Ð¾Ð¼: ÐºÐ¾Ð³Ð´Ð° Ð²Ð½ÐµÑˆÐ½Ð¸Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÑŽÑ‚ LLM ÑƒÑ‡Ð¸Ñ‚ÑŒÑÑ Ð»ÑƒÑ‡ÑˆÐµ", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ Ð¼ÐµÑ‚Ð¾Ð´ Group Contrastive Policy Optimization (GCPO) Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆ
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#rlhf", "#training", "#optimization"], "emoji": "ðŸŽ°", "ru": {"title": "Ð£Ð¼Ð½Ñ‹Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€ ÑÐ·Ñ‹ÐºÐ¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° Ñ…Ð¾Ð´Ñƒ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ BaRP â€” ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð´Ð»Ñ Ð²Ñ‹Ð±Ð¾Ñ€Ð° Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÑÐ·Ñ‹ÐºÐ¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð² Ñ€ÐµÐ¶Ð¸Ð¼Ðµ Ð¾Ð½Ð»Ð°Ð¹Ð½ Ñ Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ²ÑÐ·ÑŒÑŽ. Ð’ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð¸Ðµ Ð¾Ñ‚ ÐºÐ»Ð°ÑÑÐ¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ€Ð¾ÑƒÑ‚ÐµÑ€Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¾Ð±Ñƒ
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#3d", "#open_source"], "emoji": "ðŸ‘¤", "ru": {"title": "3D-Ð¿Ð¾Ñ€Ñ‚Ñ€ÐµÑ‚Ñ‹ Ð¸Ð· Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ñ… Ñ„Ð¾Ñ‚Ð¾ Ð±ÐµÐ· Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸", "desc": "UP2You - ÑÑ‚Ð¾ Ð¿ÐµÑ€Ð²Ð¾Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð±ÐµÐ· Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ñ‚Ð¾Ð½ÐºÐ¾Ð¹ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð²Ñ‹ÑÐ¾ÐºÐ¾ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… 3D-Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¾Ð´ÐµÑ‚Ñ‹Ñ… Ð»ÑŽÐ´ÐµÐ¹ Ð¸Ð· Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ñ… Ð½ÐµÐ¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½Ð½Ñ‹Ñ… Ñ„Ð¾Ñ‚Ð¾Ð³Ñ€Ð°Ñ„Ð¸Ð¹. Ð’ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð¸Ðµ Ð¾Ñ‚ Ð¿Ñ€ÐµÐ´
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#3d", "#transfer_learning", "#robotics", "#data", "#optimization"], "emoji": "ðŸ¤–", "ru": {"title": "Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ñ… 3D-Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¾Ð² Ð±ÐµÐ· ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ð¸", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ R2RGen - Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ Ð°ÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ð°Ñ€Ñ‹ Ð½Ð°Ð±Ð»ÑŽÐ´ÐµÐ½Ð¸Ð¹-Ð´Ðµ
[10.10.2025 15:11] Using data from previous issue: {"categories": ["#games", "#3d", "#diffusion", "#video", "#cv"], "emoji": "ðŸ’Ž", "ru": {"title": "Ð ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ðµ 3D-Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ñ‹ Ð¸Ð· Ð¾Ð´Ð½Ð¾Ð¹ Ñ„Ð¾Ñ‚Ð¾Ð³Ñ€Ð°Ñ„Ð¸Ð¸", "desc": "ÐŸÑ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° ÑÐ¸ÑÑ‚ÐµÐ¼Ð° SViM3D, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ñ‹ (PBR) Ð´Ð»Ñ 3D-Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð¸Ð· Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ñ€Ð°ÑÑˆÐ¸Ñ€ÑÐµÑ‚ lat
[10.10.2025 15:11] Querying the API.
[10.10.2025 15:11] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

GyroSwin, a scalable 5D neural surrogate model, captures nonlinear gyrokinetic dynamics and improves heat flux prediction in plasma turbulence simulations.  					AI-generated summary 				 Nuclear fusion plays a pivotal role in the quest for reliable and sustainable energy production. A major roadblock to viable fusion power is understanding plasma turbulence, which significantly impairs plasma confinement, and is vital for next-generation reactor design. Plasma turbulence is governed by the nonlinear gyrokinetic equation, which evolves a 5D distribution function over time. Due to its high computational cost, reduced-order models are often employed in practice to approximate turbulent transport of energy. However, they omit nonlinear effects unique to the full 5D dynamics. To tackle this, we introduce GyroSwin, the first scalable 5D neural surrogate that can model 5D nonlinear gyrokinetic simulations, thereby capturing the physical phenomena neglected by reduced models, while providing accurate estimates of turbulent heat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D, (ii) introduces cross-attention and integration modules for latent 3Dleftrightarrow5D interactions between electrostatic potential fields and the distribution function, and (iii) performs channelwise mode separation inspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely used reduced numerics on heat flux prediction, captures the turbulent energy cascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three orders of magnitude while remaining physically verifiable. GyroSwin shows promising scaling laws, tested up to one billion parameters, paving the way for scalable neural surrogates for gyrokinetic simulations of plasma turbulence.
[10.10.2025 15:12] Response: ```json
{
  "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ GyroSwin â€” Ð¿ÐµÑ€Ð²ÑƒÑŽ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼ÑƒÑŽ Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½ÑƒÑŽ ÑÑƒÑ€Ñ€Ð¾Ð³Ð°Ñ‚Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ 5D-Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ Ð¿Ð»Ð°Ð·Ð¼ÐµÐ½Ð½Ð¾Ð¹ Ñ‚ÑƒÑ€Ð±ÑƒÐ»ÐµÐ½Ñ‚Ð½Ð¾ÑÑ‚Ð¸ Ð² Ñ‚ÐµÑ€Ð¼Ð¾ÑÐ´ÐµÑ€Ð½Ð¾Ð¼ ÑÐ¸Ð½Ñ‚ÐµÐ·Ðµ. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ñ€Ð°ÑÑˆÐ¸Ñ€ÑÐµÑ‚ Ð¸ÐµÑ€Ð°Ñ€Ñ…Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Vision Transformers Ð½Ð° Ð¿ÑÑ‚ÑŒ Ð¸Ð·Ð¼ÐµÑ€ÐµÐ½Ð¸Ð¹ Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼Ñ‹ cross-attention Ð´Ð»Ñ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ð¼ÐµÐ¶Ð´Ñƒ ÑÐ»ÐµÐºÑ‚Ñ€Ð¾ÑÑ‚Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»Ð¾Ð¼ Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÐµÐ¹ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ. GyroSwin Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ð¸Ñ‚ Ñ‚Ñ€Ð°Ð´Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ñ€ÐµÐ´ÑƒÑ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð² Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¸ Ñ‚ÐµÐ¿Ð»Ð¾Ð²Ð¾Ð³Ð¾ Ð¿Ð¾Ñ‚Ð¾ÐºÐ° Ð¸ ÑƒÐ»Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÑ‚ ÑÑ„Ñ„ÐµÐºÑ‚Ñ‹ Ñ‚ÑƒÑ€Ð±ÑƒÐ»ÐµÐ½Ñ‚Ð½Ð¾Ð³Ð¾ ÑÐ½ÐµÑ€Ð³ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÐºÐ°ÑÐºÐ°Ð´Ð°. ÐœÐ¾Ð´ÐµÐ»ÑŒ ÑÐ½Ð¸Ð¶Ð°ÐµÑ‚ Ð²Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð·Ð°Ñ‚Ñ€Ð°Ñ‚Ñ‹ Ð½Ð° Ñ‚Ñ€Ð¸ Ð¿Ð¾Ñ€ÑÐ´ÐºÐ° Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ Ð¿Ð¾Ð»Ð½Ñ‹Ð¼ Ð½ÐµÐ»Ð¸Ð½ÐµÐ¹Ð½Ñ‹Ð¼ Ñ€ÐµÑˆÐµÐ½Ð¸ÐµÐ¼, Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÑ Ñ…Ð¾Ñ€Ð¾ÑˆÐµÐµ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð¾ Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð° Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð².",
  "emoji": "âš›ï¸",
  "title": "ÐÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÑŒ Ð´Ð»Ñ Ñ‚ÐµÑ€Ð¼Ð¾ÑÐ´ÐµÑ€Ð½Ð¾Ð¹ Ð¿Ð»Ð°Ð·Ð¼Ñ‹: 5D-Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ‚ÑƒÑ€Ð±ÑƒÐ»ÐµÐ½Ñ‚Ð½Ð¾ÑÑ‚Ð¸"
}
```
[10.10.2025 15:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"GyroSwin, a scalable 5D neural surrogate model, captures nonlinear gyrokinetic dynamics and improves heat flux prediction in plasma turbulence simulations.  					AI-generated summary 				 Nuclear fusion plays a pivotal role in the quest for reliable and sustainable energy production. A major roadblock to viable fusion power is understanding plasma turbulence, which significantly impairs plasma confinement, and is vital for next-generation reactor design. Plasma turbulence is governed by the nonlinear gyrokinetic equation, which evolves a 5D distribution function over time. Due to its high computational cost, reduced-order models are often employed in practice to approximate turbulent transport of energy. However, they omit nonlinear effects unique to the full 5D dynamics. To tackle this, we introduce GyroSwin, the first scalable 5D neural surrogate that can model 5D nonlinear gyrokinetic simulations, thereby capturing the physical phenomena neglected by reduced models, while providing accurate estimates of turbulent heat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D, (ii) introduces cross-attention and integration modules for latent 3Dleftrightarrow5D interactions between electrostatic potential fields and the distribution function, and (iii) performs channelwise mode separation inspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely used reduced numerics on heat flux prediction, captures the turbulent energy cascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three orders of magnitude while remaining physically verifiable. GyroSwin shows promising scaling laws, tested up to one billion parameters, paving the way for scalable neural surrogates for gyrokinetic simulations of plasma turbulence."

[10.10.2025 15:12] Response: ```python
['DATASET', 'ARCHITECTURE', 'TRAINING']
```
[10.10.2025 15:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"GyroSwin, a scalable 5D neural surrogate model, captures nonlinear gyrokinetic dynamics and improves heat flux prediction in plasma turbulence simulations.  					AI-generated summary 				 Nuclear fusion plays a pivotal role in the quest for reliable and sustainable energy production. A major roadblock to viable fusion power is understanding plasma turbulence, which significantly impairs plasma confinement, and is vital for next-generation reactor design. Plasma turbulence is governed by the nonlinear gyrokinetic equation, which evolves a 5D distribution function over time. Due to its high computational cost, reduced-order models are often employed in practice to approximate turbulent transport of energy. However, they omit nonlinear effects unique to the full 5D dynamics. To tackle this, we introduce GyroSwin, the first scalable 5D neural surrogate that can model 5D nonlinear gyrokinetic simulations, thereby capturing the physical phenomena neglected by reduced models, while providing accurate estimates of turbulent heat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D, (ii) introduces cross-attention and integration modules for latent 3Dleftrightarrow5D interactions between electrostatic potential fields and the distribution function, and (iii) performs channelwise mode separation inspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely used reduced numerics on heat flux prediction, captures the turbulent energy cascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three orders of magnitude while remaining physically verifiable. GyroSwin shows promising scaling laws, tested up to one billion parameters, paving the way for scalable neural surrogates for gyrokinetic simulations of plasma turbulence."

[10.10.2025 15:12] Response: ```python
["OPTIMIZATION", "SCIENCE"]
```
[10.10.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"GyroSwin is a novel 5D neural surrogate model designed to accurately capture the complex nonlinear dynamics of plasma turbulence, which is crucial for nuclear fusion research. By extending hierarchical Vision Transformers to handle 5D data, it incorporates advanced techniques like cross-attention and channelwise mode separation to effectively model interactions between electrostatic fields and particle distribution functions. This model significantly improves heat flux predictions compared to traditional reduced-order models, which often overlook important nonlinear effects. GyroSwin not only enhances computational efficiency by reducing the cost of simulations but also maintains physical accuracy, making it a promising tool for future plasma turbulence studies.","title":"GyroSwin: Revolutionizing Plasma Turbulence Predictions with 5D Neural Surrogates"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='GyroSwin is a novel 5D neural surrogate model designed to accurately capture the complex nonlinear dynamics of plasma turbulence, which is crucial for nuclear fusion research. By extending hierarchical Vision Transformers to handle 5D data, it incorporates advanced techniques like cross-attention and channelwise mode separation to effectively model interactions between electrostatic fields and particle distribution functions. This model significantly improves heat flux predictions compared to traditional reduced-order models, which often overlook important nonlinear effects. GyroSwin not only enhances computational efficiency by reducing the cost of simulations but also maintains physical accuracy, making it a promising tool for future plasma turbulence studies.', title='GyroSwin: Revolutionizing Plasma Turbulence Predictions with 5D Neural Surrogates'))
[10.10.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"GyroSwinæ˜¯ä¸€ç§å¯æ‰©å±•çš„äº”ç»´ç¥žç»ä»£ç†æ¨¡åž‹ï¼Œèƒ½å¤Ÿæ•æ‰éžçº¿æ€§å›žæ—‹åŠ¨åŠ›å­¦ï¼Œå¹¶æ”¹å–„ç­‰ç¦»å­ä½“æ¹æµæ¨¡æ‹Ÿä¸­çš„çƒ­é€šé‡é¢„æµ‹ã€‚è¯¥æ¨¡åž‹é€šè¿‡æ‰©å±•å±‚æ¬¡åŒ–è§†è§‰å˜æ¢å™¨ï¼Œç»“åˆäº¤å‰æ³¨æ„åŠ›å’Œé›†æˆæ¨¡å—ï¼Œå¤„ç†ç”µé™åŠ¿åœºä¸Žåˆ†å¸ƒå‡½æ•°ä¹‹é—´çš„ä¸‰ç»´åˆ°äº”ç»´çš„ç›¸äº’ä½œç”¨ã€‚GyroSwinåœ¨çƒ­é€šé‡é¢„æµ‹ä¸Šè¶…è¶Šäº†å¸¸ç”¨çš„é™é˜¶æ•°å€¼æ–¹æ³•ï¼Œå¹¶æœ‰æ•ˆæ•æ‰æ¹æµèƒ½é‡çº§è”çŽ°è±¡ã€‚å®ƒçš„è®¡ç®—æˆæœ¬æ¯”å®Œå…¨è§£æžçš„éžçº¿æ€§å›žæ—‹åŠ¨åŠ›å­¦é™ä½Žäº†ä¸‰ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶ä¿æŒäº†ç‰©ç†å¯éªŒè¯æ€§ã€‚","title":"GyroSwinï¼šæå‡ç­‰ç¦»å­ä½“æ¹æµæ¨¡æ‹Ÿçš„äº”ç»´ç¥žç»ä»£ç†æ¨¡åž‹"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='GyroSwinæ˜¯ä¸€ç§å¯æ‰©å±•çš„äº”ç»´ç¥žç»ä»£ç†æ¨¡åž‹ï¼Œèƒ½å¤Ÿæ•æ‰éžçº¿æ€§å›žæ—‹åŠ¨åŠ›å­¦ï¼Œå¹¶æ”¹å–„ç­‰ç¦»å­ä½“æ¹æµæ¨¡æ‹Ÿä¸­çš„çƒ­é€šé‡é¢„æµ‹ã€‚è¯¥æ¨¡åž‹é€šè¿‡æ‰©å±•å±‚æ¬¡åŒ–è§†è§‰å˜æ¢å™¨ï¼Œç»“åˆäº¤å‰æ³¨æ„åŠ›å’Œé›†æˆæ¨¡å—ï¼Œå¤„ç†ç”µé™åŠ¿åœºä¸Žåˆ†å¸ƒå‡½æ•°ä¹‹é—´çš„ä¸‰ç»´åˆ°äº”ç»´çš„ç›¸äº’ä½œç”¨ã€‚GyroSwinåœ¨çƒ­é€šé‡é¢„æµ‹ä¸Šè¶…è¶Šäº†å¸¸ç”¨çš„é™é˜¶æ•°å€¼æ–¹æ³•ï¼Œå¹¶æœ‰æ•ˆæ•æ‰æ¹æµèƒ½é‡çº§è”çŽ°è±¡ã€‚å®ƒçš„è®¡ç®—æˆæœ¬æ¯”å®Œå…¨è§£æžçš„éžçº¿æ€§å›žæ—‹åŠ¨åŠ›å­¦é™ä½Žäº†ä¸‰ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶ä¿æŒäº†ç‰©ç†å¯éªŒè¯æ€§ã€‚', title='GyroSwinï¼šæå‡ç­‰ç¦»å­ä½“æ¹æµæ¨¡æ‹Ÿçš„äº”ç»´ç¥žç»ä»£ç†æ¨¡åž‹'))
[10.10.2025 15:12] Querying the API.
[10.10.2025 15:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel approach combining driving models and generative world models evaluates and enhances the realism and generalization of synthetic video data for autonomous vehicle testing and planning.  					AI-generated summary 				 Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts.
[10.10.2025 15:12] Response: ```json
{
  "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½Ð¸Ð»Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð°Ð²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ð¾Ð³Ð¾ Ð²Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ world models Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ ÑÐ¸Ð½Ñ‚ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð²Ð¸Ð´ÐµÐ¾Ð´Ð°Ð½Ð½Ñ‹Ñ…. ÐžÐ½Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸Ð»Ð¸ Ð½Ð¾Ð²Ñ‹Ðµ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ñ€ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð²Ð¸Ð´ÐµÐ¾ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ end-to-end Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð²Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ. Ð¡ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð¸Ñ€ÑƒÐµÐ¼Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð°Ð²Ñ‚Ð¾Ñ€Ñ‹ Ð¸Ð·ÑƒÑ‡Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð¾Ð±Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸ÐºÐ¾Ð² Ð½Ð° out-of-distribution ÑÑ†ÐµÐ½Ð°Ñ€Ð¸Ð¸. Ð¡Ð¸Ð½Ñ‚ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾ÐºÐ°Ð·Ð°Ð»Ð¸ÑÑŒ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð¸ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ‡Ð½Ð¾Ð¹ Ð°Ð»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð¾Ð¹ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ð¼ Ð´Ð»Ñ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ñ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð°Ð²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ñ‹Ñ… Ñ‚Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼.",
  "emoji": "ðŸš—",
  "title": "Ð’Ð¸Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð´Ð¾Ñ€Ð¾Ð³Ð¸: Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð°Ð²Ñ‚Ð¾Ð¿Ð¸Ð»Ð¾Ñ‚Ð¾Ð²"
}
```
[10.10.2025 15:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel approach combining driving models and generative world models evaluates and enhances the realism and generalization of synthetic video data for autonomous vehicle testing and planning.  					AI-generated summary 				 Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts."

[10.10.2025 15:12] Response: ```python
['VIDEO', 'AGENTS', 'DATASET']
```
[10.10.2025 15:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel approach combining driving models and generative world models evaluates and enhances the realism and generalization of synthetic video data for autonomous vehicle testing and planning.  					AI-generated summary 				 Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts."

[10.10.2025 15:12] Response: ```python
['SYNTHETIC', 'OPTIMIZATION']
```
[10.10.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new method that combines driving models with generative world models to improve the quality and applicability of synthetic video data for testing autonomous vehicles. It addresses the challenge of ensuring that generated videos are realistic and meet specific conditions necessary for evaluating end-to-end (E2E) driving planners. The authors introduce statistical measures to assess the realism of these videos and conduct experiments to understand how distribution gaps can impact E2E planner performance. Ultimately, the study demonstrates that synthetic data can serve as a valuable and cost-effective resource for enhancing the generalization capabilities of E2E models in diverse driving scenarios.","title":"Enhancing Autonomous Vehicle Testing with Synthetic Video Data"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new method that combines driving models with generative world models to improve the quality and applicability of synthetic video data for testing autonomous vehicles. It addresses the challenge of ensuring that generated videos are realistic and meet specific conditions necessary for evaluating end-to-end (E2E) driving planners. The authors introduce statistical measures to assess the realism of these videos and conduct experiments to understand how distribution gaps can impact E2E planner performance. Ultimately, the study demonstrates that synthetic data can serve as a valuable and cost-effective resource for enhancing the generalization capabilities of E2E models in diverse driving scenarios.', title='Enhancing Autonomous Vehicle Testing with Synthetic Video Data'))
[10.10.2025 15:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†é©¾é©¶æ¨¡åž‹ä¸Žç”Ÿæˆä¸–ç•Œæ¨¡åž‹ç›¸ç»“åˆï¼Œä»¥è¯„ä¼°å’Œå¢žå¼ºåˆæˆè§†é¢‘æ•°æ®çš„çœŸå®žæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºŽè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„æµ‹è¯•å’Œè§„åˆ’ã€‚ç ”ç©¶æŽ¢è®¨äº†è§†é¢‘ç”Ÿæˆæ¨¡åž‹ä½œä¸ºå¯æŽ§è™šæ‹Ÿæµ‹è¯•çŽ¯å¢ƒçš„åº”ç”¨ï¼ŒåŒæ—¶æå‡ºäº†ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰é©¾é©¶æ¨¡åž‹ä½œä¸ºä¼ ç»Ÿæ¨¡å—åŒ–è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„ç®€åŒ–æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡åˆ©ç”¨E2Eé©¾é©¶æ¨¡åž‹ï¼Œæœ¬æ–‡æå‡ºäº†æ–°çš„ç»Ÿè®¡æµ‹é‡æ–¹æ³•æ¥è¯„ä¼°ç”Ÿæˆè§†é¢‘çš„çœŸå®žæ€§ï¼Œå¹¶è¿›è¡Œé’ˆå¯¹æ€§å®žéªŒä»¥ç ”ç©¶å½±å“E2Eè§„åˆ’è€…æ€§èƒ½çš„åˆ†å¸ƒå·®è·ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¡¨æ˜Žï¼Œç”Ÿæˆæ¨¡åž‹äº§ç”Ÿçš„åˆæˆæ•°æ®æ˜¯æ”¶é›†çœŸå®žä¸–ç•Œæ•°æ®çš„æˆæœ¬æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡E2Eæ¨¡åž‹åœ¨æ–°æ“ä½œçŽ¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚","title":"åˆæˆæ•°æ®åŠ©åŠ›è‡ªåŠ¨é©¾é©¶æ¨¡åž‹çš„æ³›åŒ–ä¸ŽçœŸå®žæ„Ÿæå‡"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†é©¾é©¶æ¨¡åž‹ä¸Žç”Ÿæˆä¸–ç•Œæ¨¡åž‹ç›¸ç»“åˆï¼Œä»¥è¯„ä¼°å’Œå¢žå¼ºåˆæˆè§†é¢‘æ•°æ®çš„çœŸå®žæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºŽè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„æµ‹è¯•å’Œè§„åˆ’ã€‚ç ”ç©¶æŽ¢è®¨äº†è§†é¢‘ç”Ÿæˆæ¨¡åž‹ä½œä¸ºå¯æŽ§è™šæ‹Ÿæµ‹è¯•çŽ¯å¢ƒçš„åº”ç”¨ï¼ŒåŒæ—¶æå‡ºäº†ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰é©¾é©¶æ¨¡åž‹ä½œä¸ºä¼ ç»Ÿæ¨¡å—åŒ–è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„ç®€åŒ–æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡åˆ©ç”¨E2Eé©¾é©¶æ¨¡åž‹ï¼Œæœ¬æ–‡æå‡ºäº†æ–°çš„ç»Ÿè®¡æµ‹é‡æ–¹æ³•æ¥è¯„ä¼°ç”Ÿæˆè§†é¢‘çš„çœŸå®žæ€§ï¼Œå¹¶è¿›è¡Œé’ˆå¯¹æ€§å®žéªŒä»¥ç ”ç©¶å½±å“E2Eè§„åˆ’è€…æ€§èƒ½çš„åˆ†å¸ƒå·®è·ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¡¨æ˜Žï¼Œç”Ÿæˆæ¨¡åž‹äº§ç”Ÿçš„åˆæˆæ•°æ®æ˜¯æ”¶é›†çœŸå®žä¸–ç•Œæ•°æ®çš„æˆæœ¬æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡E2Eæ¨¡åž‹åœ¨æ–°æ“ä½œçŽ¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚', title='åˆæˆæ•°æ®åŠ©åŠ›è‡ªåŠ¨é©¾é©¶æ¨¡åž‹çš„æ³›åŒ–ä¸ŽçœŸå®žæ„Ÿæå‡'))
[10.10.2025 15:12] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#dataset", "#architecture", "#open_source"], "emoji": "ðŸŽ¨", "ru": {"title": "Ð¢Ð¾Ñ‡Ð½Ð¾Ðµ 3D-Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð±ÐµÐ· Ð¼Ð°ÑÐ¾Ðº Ñ‡ÐµÑ€ÐµÐ· Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ð»Ð¸ 3DEditFormer â€” Ð½Ð¾Ð²Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ 3D-Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÑƒÑÐ»Ð¾Ð²Ð½Ñ‹Ð¹ Ñ‚Ñ€Ð°Ð½Ñ
[10.10.2025 15:12] Using data from previous issue: {"categories": ["#synthetic", "#optimization", "#training", "#robotics", "#data", "#agents"], "emoji": "ðŸ¤–", "ru": {"title": "ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ ÑÐ¼ÐµÑˆÐ¸Ð²Ð°Ð½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ñ€Ð¾Ð±Ð°ÑÑ‚Ð½Ñ‹Ñ… Ñ€Ð¾Ð±Ð¾Ñ‚Ð¾Ð²", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ CIFT (Coherent Information Fidelity Tuning) Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¾Ð±Ð¾Ð±Ñ‰Ð°ÑŽÑ‰ÐµÐ¹ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ñ€Ð¾Ð±Ð¾Ñ‚
[10.10.2025 15:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference"], "emoji": "ðŸ§´", "ru": {"title": "Shampoo Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð»ÑƒÑ‡ÑˆÑƒÑŽ ÑƒÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ð¾ÑÑ‚ÑŒ Ð¿Ñ€Ð¸ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð·ÑƒÑ‡Ð°ÐµÑ‚, ÐºÐ°Ðº Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ñ‹ Ð²Ð»Ð¸ÑÑŽÑ‚ Ð½Ð° Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ñ€Ð¸ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ - ÐºÐ°Ðº Ð¿Ñ€Ð¸ Ð¿Ð¾ÑÑ‚Ñ‚Ñ€ÐµÐ½Ð¸Ð½
[10.10.2025 15:12] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#optimization", "#rlhf", "#rl"], "emoji": "ðŸ”", "ru": {"title": "Ð Ð°ÑÑÑƒÐ¶Ð´Ð°Ñ Ðº Ð»ÑƒÑ‡ÑˆÐµÐ¼Ñƒ Ð¿Ð¾Ð¸ÑÐºÑƒ: LLM Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÑŽÑ‚ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ñ‡ÐµÑ€ÐµÐ· chain-of-thought", "desc": "Search-R3 - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (LLM) Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸
[10.10.2025 15:12] Renaming data file.
[10.10.2025 15:12] Renaming previous data. hf_papers.json to ./d/2025-10-10.json
[10.10.2025 15:12] Saving new data file.
[10.10.2025 15:12] Generating page.
[10.10.2025 15:12] Renaming previous page.
[10.10.2025 15:12] Renaming previous data. index.html to ./d/2025-10-10.html
[10.10.2025 15:12] Writing result.
[10.10.2025 15:12] Renaming log file.
[10.10.2025 15:12] Renaming previous data. log.txt to ./logs/2025-10-10_last_log.txt
