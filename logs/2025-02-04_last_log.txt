[04.02.2025 06:16] Read previous papers.
[04.02.2025 06:16] Generating top page (month).
[04.02.2025 06:16] Writing top page (month).
[04.02.2025 07:10] Read previous papers.
[04.02.2025 07:10] Get feed.
[04.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01456
[04.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01534
[04.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01068
[04.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.00094
[04.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01061
[04.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01081
[04.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01441
[04.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01100
[04.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01636
[04.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01637
[04.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.01591
[04.02.2025 07:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.02.2025 07:10] No deleted papers detected.
[04.02.2025 07:10] Downloading and parsing papers (pdf, html). Total: 11.
[04.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.01456.
[04.02.2025 07:10] Extra JSON file exists (./assets/json/2502.01456.json), skip PDF parsing.
[04.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.01456.json), skip HTML parsing.
[04.02.2025 07:10] Success.
[04.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.01534.
[04.02.2025 07:10] Extra JSON file exists (./assets/json/2502.01534.json), skip PDF parsing.
[04.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.01534.json), skip HTML parsing.
[04.02.2025 07:10] Success.
[04.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.01068.
[04.02.2025 07:10] Extra JSON file exists (./assets/json/2502.01068.json), skip PDF parsing.
[04.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.01068.json), skip HTML parsing.
[04.02.2025 07:10] Success.
[04.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.00094.
[04.02.2025 07:10] Extra JSON file exists (./assets/json/2502.00094.json), skip PDF parsing.
[04.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.00094.json), skip HTML parsing.
[04.02.2025 07:10] Success.
[04.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.01061.
[04.02.2025 07:10] Extra JSON file exists (./assets/json/2502.01061.json), skip PDF parsing.
[04.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.01061.json), skip HTML parsing.
[04.02.2025 07:10] Success.
[04.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.01081.
[04.02.2025 07:10] Extra JSON file exists (./assets/json/2502.01081.json), skip PDF parsing.
[04.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.01081.json), skip HTML parsing.
[04.02.2025 07:10] Success.
[04.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.01441.
[04.02.2025 07:10] Extra JSON file exists (./assets/json/2502.01441.json), skip PDF parsing.
[04.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.01441.json), skip HTML parsing.
[04.02.2025 07:10] Success.
[04.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.01100.
[04.02.2025 07:10] Extra JSON file exists (./assets/json/2502.01100.json), skip PDF parsing.
[04.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.01100.json), skip HTML parsing.
[04.02.2025 07:10] Success.
[04.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.01636.
[04.02.2025 07:10] Extra JSON file exists (./assets/json/2502.01636.json), skip PDF parsing.
[04.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.01636.json), skip HTML parsing.
[04.02.2025 07:10] Success.
[04.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.01637.
[04.02.2025 07:10] Extra JSON file exists (./assets/json/2502.01637.json), skip PDF parsing.
[04.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.01637.json), skip HTML parsing.
[04.02.2025 07:10] Success.
[04.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.01591.
[04.02.2025 07:10] Extra JSON file exists (./assets/json/2502.01591.json), skip PDF parsing.
[04.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.01591.json), skip HTML parsing.
[04.02.2025 07:10] Success.
[04.02.2025 07:10] Enriching papers with extra data.
[04.02.2025 07:10] ********************************************************************************
[04.02.2025 07:10] Abstract 0. Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement lea...
[04.02.2025 07:10] ********************************************************************************
[04.02.2025 07:10] Abstract 1. Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potentia...
[04.02.2025 07:10] ********************************************************************************
[04.02.2025 07:10] Abstract 2. While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing ...
[04.02.2025 07:10] ********************************************************************************
[04.02.2025 07:10] Abstract 3. Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narro...
[04.02.2025 07:10] ********************************************************************************
[04.02.2025 07:10] Abstract 4. End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propo...
[04.02.2025 07:10] ********************************************************************************
[04.02.2025 07:10] Abstract 5. The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI)....
[04.02.2025 07:10] ********************************************************************************
[04.02.2025 07:10] Abstract 6. Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success...
[04.02.2025 07:10] ********************************************************************************
[04.02.2025 07:10] Abstract 7. We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint...
[04.02.2025 07:10] ********************************************************************************
[04.02.2025 07:10] Abstract 8. Prior work in parameter-modifying knowledge editing has shown that large-scale sequential editing leads to significant model degradation. In this paper, we study the reasons behind this and scale sequential knowledge editing to 10,000 sequential edits, while maintaining the downstream performance of...
[04.02.2025 07:10] ********************************************************************************
[04.02.2025 07:10] Abstract 9. We propose SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequ...
[04.02.2025 07:10] ********************************************************************************
[04.02.2025 07:10] Abstract 10. We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term r...
[04.02.2025 07:10] Read previous papers.
[04.02.2025 07:10] Generating reviews via LLM API.
[04.02.2025 07:10] Using data from previous issue: {"categories": ["#training", "#optimization", "#rl", "#reasoning"], "emoji": "üß†", "ru": {"title": "PRIME: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò-–º–æ–¥–µ–ª–µ–π —Å –Ω–µ—è–≤–Ω—ã–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π PRIME. –û–Ω –∏—Å–ø–æ–ª—å
[04.02.2025 07:10] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#training", "#dataset", "#leakage"], "emoji": "üïµÔ∏è", "ru": {"title": "–û—Å—Ç–æ—Ä–æ–∂–Ω–æ: LLM-—Å—É–¥—å–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–µ–¥–≤–∑—è—Ç—ã!", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—É '—É—Ç–µ—á–∫–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π' –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å—É–¥–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ 
[04.02.2025 07:10] Using data from previous issue: {"categories": ["#long_context", "#training", "#inference", "#optimization"], "emoji": "üöÄ", "ru": {"title": "FastKV: –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FastKV - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è –∫—ç—à–∞ –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ (KV) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω
[04.02.2025 07:10] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#low_resource", "#multimodal", "#multilingual"], "emoji": "üåç", "ru": {"title": "AIN: –ü—Ä–æ—Ä—ã–≤ –≤ –∞—Ä–∞–±–æ—è–∑—ã—á–Ω–æ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å AIN - –¥–≤—É—è–∑—ã—á–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞—Ä–∞–±—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ 
[04.02.2025 07:10] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#video", "#training", "#diffusion"], "emoji": "üé≠", "ru": {"title": "OmniHuman: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ª—é–¥—å–º–∏", "desc": "OmniHuman - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ Diffusion Transformer –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ª—é
[04.02.2025 07:10] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#agi", "#open_source", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: –æ—Ç —Å–∏–º–≤–æ–ª–æ–≤ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç —ç–≤–æ–ª—é—Ü–∏—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–µ—Ä–∏–π G
[04.02.2025 07:10] Using data from previous issue: {"categories": ["#dataset", "#training", "#optimization", "#architecture", "#open_source", "#diffusion", "#video"], "emoji": "üß†", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã
[04.02.2025 07:10] Using data from previous issue: {"categories": ["#reasoning", "#training", "#inference", "#benchmark"], "emoji": "üß©", "ru": {"title": "–ü—Ä–æ–∫–ª—è—Ç–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤ –ª–æ–≥–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏ –∏—Ö –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –≤ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –Ω–µ–º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–≥
[04.02.2025 07:10] Using data from previous issue: {"categories": ["#training", "#interpretability", "#architecture", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã 
[04.02.2025 07:10] Using data from previous issue: {"categories": ["#architecture", "#inference", "#long_context", "#training", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç", "desc": "SCONE - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–ª–æ—ë–≤ –≤—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ
[04.02.2025 07:10] Using data from previous issue: {"categories": ["#rl", "#architecture", "#benchmark", "#games", "#training", "#reasoning", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ model-based RL: –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —á–µ–ª–æ–≤–µ–∫–∞ –≤ Craftax-classic", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏, –¥–æ—Å
[04.02.2025 07:10] Loading Chinese text from previous data.
[04.02.2025 07:10] Renaming data file.
[04.02.2025 07:10] Renaming previous data. hf_papers.json to ./d/2025-02-04.json
[04.02.2025 07:10] Saving new data file.
[04.02.2025 07:10] Generating page.
[04.02.2025 07:10] Renaming previous page.
[04.02.2025 07:10] Renaming previous data. index.html to ./d/2025-02-04.html
[04.02.2025 07:10] [Experimental] Generating Chinese page for reading.
[04.02.2025 07:10] Chinese vocab [{'word': 'Âª∫Ê®°', 'pinyin': 'ji√†n m√≥', 'trans': 'modeling'}, {'word': 'Áß∞‰∏∫', 'pinyin': 'chƒìng w√©i', 'trans': 'called'}, {'word': 'Áº©Êîæ', 'pinyin': 'su≈ç f√†ng', 'trans': 'scaling'}, {'word': 'Â±ïÁ§∫', 'pinyin': 'zh«én sh√¨', 'trans': 'demonstrate'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'capability'}, {'word': 'ÂÖ¨ÂºÄ', 'pinyin': 'g≈çng kƒÅi', 'trans': 'public'}, {'word': 'Â§çÂà∂', 'pinyin': 'f√π zh√¨', 'trans': 'replicate'}, {'word': 'Âä™Âäõ', 'pinyin': 'n«î l√¨', 'trans': 'efforts'}, {'word': 'Êï¥ÁêÜ', 'pinyin': 'zhƒõng l«ê', 'trans': 'organize'}, {'word': 'È¢ÑÁÆó', 'pinyin': 'y√π su√†n', 'trans': 'budget'}, {'word': 'Âº∫Âà∂', 'pinyin': 'qi√°ng zh√¨', 'trans': 'enforcement'}, {'word': 'ÈÄîÂæÑ', 'pinyin': 't√∫ j√¨ng', 'trans': 'means'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'Ë∂ÖË∂ä', 'pinyin': 'chƒÅo yu√®', 'trans': 'surpass'}, {'word': 'ÂºÄÊ∫ê', 'pinyin': 'kƒÅi yu√°n', 'trans': 'open-source'}]
[04.02.2025 07:10] Renaming previous Chinese page.
[04.02.2025 07:10] Renaming previous data. zh.html to ./d/2025-02-03_zh_reading_task.html
[04.02.2025 07:10] Writing Chinese reading task.
[04.02.2025 07:10] Writing result.
[04.02.2025 07:10] Renaming log file.
[04.02.2025 07:10] Renaming previous data. log.txt to ./logs/2025-02-04_last_log.txt
