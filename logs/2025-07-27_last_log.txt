[26.07.2025 18:35] Read previous papers.
[26.07.2025 18:35] Generating top page (month).
[26.07.2025 18:35] Writing top page (month).
[27.07.2025 02:26] Read previous papers.
[27.07.2025 02:26] Get feed.
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.13546
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18071
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14958
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15758
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18634
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15844
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18537
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18546
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.16535
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18464
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18013
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.14988
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18103
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.16038
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15595
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18405
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18192
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.16802
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.18565
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.17402
[27.07.2025 02:26] Get page data from previous paper. URL: https://huggingface.co/papers/2507.15807
[27.07.2025 02:26] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.07.2025 02:26] No deleted papers detected.
[27.07.2025 02:26] Downloading and parsing papers (pdf, html). Total: 21.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.13546.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.13546.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.13546.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.18071.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.18071.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.18071.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.14958.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.14958.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.14958.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.15758.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.15758.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.15758.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.18634.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.18634.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.18634.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.15844.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.15844.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.15844.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.18537.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.18537.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.18537.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.18546.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.18546.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.18546.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.16535.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.16535.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.16535.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.18464.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.18464.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.18464.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.18013.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.18013.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.18013.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.14988.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.14988.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.14988.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.18103.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.18103.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.18103.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.16038.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.16038.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.16038.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.15595.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.15595.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.15595.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.18405.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.18405.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.18405.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.18192.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.18192.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.18192.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.16802.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.16802.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.16802.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.18565.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.18565.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.18565.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.17402.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.17402.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.17402.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Downloading and parsing paper https://huggingface.co/papers/2507.15807.
[27.07.2025 02:26] Extra JSON file exists (./assets/json/2507.15807.json), skip PDF parsing.
[27.07.2025 02:26] Paper image links file exists (./assets/img_data/2507.15807.json), skip HTML parsing.
[27.07.2025 02:26] Success.
[27.07.2025 02:26] Enriching papers with extra data.
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 0. NABLA, a dynamic block-level attention mechanism, improves video diffusion transformers by enhancing computational efficiency without sacrificing generative quality.  					AI-generated summary 				 Recent progress in transformer-based architectures has demonstrated remarkable success in video genera...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 1. This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelih...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 2. Momentum Uncertainty-guided Reasoning (MUR) dynamically optimizes reasoning budgets in Large Language Models during inference, reducing computation and enhancing accuracy.  					AI-generated summary 				 Large Language Models (LLMs) have achieved impressive performance on reasoning-intensive tasks, ...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 3. Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning len...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 4. Captain Cinema generates high-quality short movies from textual descriptions using top-down keyframe planning and bottom-up video synthesis with interleaved training of Multimodal Diffusion Transformers.  					AI-generated summary 				 We present Captain Cinema, a generation framework for short movi...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 5. Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet exhibit significant computational inefficiency by applying uniform reasoning strategies regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcemen...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 6. TTS-VAR, a test-time scaling framework for visual auto-regressive models, improves generation quality by dynamically adjusting batch sizes and using clustering and resampling techniques.  					AI-generated summary 				 Scaling visual generation models is essential for real-world content creation, ye...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 7. GLiNER2 is a unified framework that supports multiple NLP tasks using a single efficient transformer model, improving deployment accessibility over large language models.  					AI-generated summary 				 Information extraction (IE) is fundamental to numerous NLP applications, yet existing solutions o...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 8. Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architect...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 9. DriftMoE, an online Mixture-of-Experts architecture with a compact neural router, achieves competitive results in adapting to concept drift in data streams through a symbiotic learning loop.  					AI-generated summary 				 Learning from non-stationary data streams subject to concept drift requires m...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 10. The TeleChat2, TeleChat2.5, and T1 models enhance language capabilities through advanced training strategies, including Supervised Fine-Tuning, Direct Preference Optimization, and reinforcement learning, achieving superior performance in reasoning and speed compared to previous models.  					AI-gene...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 11. DMOSpeech 2 optimizes duration prediction and introduces teacher-guided sampling to enhance speech synthesis performance and diversity.  					AI-generated summary 				 Diffusion-based text-to-speech (TTS) systems have made remarkable progress in zero-shot speech synthesis, yet optimizing all compone...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 12. New 2024 GloVe models improve upon 2014 versions by incorporating updated datasets and demonstrating enhanced performance on culturally and temporally relevant Named Entity Recognition tasks.  					AI-generated summary 				 This report documents, describes, and evaluates new 2024 English GloVe (Glob...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 13. A visual world model called SpelkeNet outperforms existing methods in identifying Spelke objects in images, improving performance in tasks like physical object manipulation.  					AI-generated summary 				 Segments in computer vision are often defined by semantic considerations and are highly depend...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 14. SegDT, a diffusion transformer-based segmentation model, achieves state-of-the-art results in skin lesion segmentation with fast inference speeds, making it suitable for real-world medical applications.  					AI-generated summary 				 Medical image segmentation is crucial for many healthcare tasks, ...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 15. Iwin Transformer, a hierarchical vision transformer without position embeddings, combines interleaved window attention and depthwise separable convolution for efficient global information exchange, achieving competitive performance in image classification, semantic segmentation, and video action rec...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 16. TeEFusion enhances text-to-image synthesis by efficiently incorporating classifier-free guidance into text embeddings, reducing inference costs without sacrificing image quality.  					AI-generated summary 				 Recent advances in text-to-image synthesis largely benefit from sophisticated sampling st...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 17. The Agentar-Fin-R1 series of financial large language models enhances reasoning, reliability, and domain specialization through a trustworthiness assurance framework and achieves state-of-the-art performance on financial and general reasoning tasks.  					AI-generated summary 				 Large Language Mod...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 18. A custom CNN architecture simultaneously classifies age and gender from facial images, improving performance by learning shared representations and achieving high accuracy and low mean absolute error.  					AI-generated summary 				 This paper presents a novel deep learning-based approach for simult...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 19. HLFormer uses a hyperbolic modeling framework with Lorentz and Euclidean attention blocks to improve video-text retrieval by addressing hierarchical and partial relevance issues.  					AI-generated summary 				 Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of matching un...
[27.07.2025 02:26] ********************************************************************************
[27.07.2025 02:26] Abstract 20. Multimodal Large Language Models (MLLMs), built on powerful language backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new tasks from a few multimodal demonstrations consisting of images, questions, and answers. Despite showing noticeable improvement on standard vision-langua...
[27.07.2025 02:26] Read previous papers.
[27.07.2025 02:26] Generating reviews via LLM API.
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#open_source", "#training", "#video", "#optimization", "#diffusion", "#architecture"], "emoji": "üé¨", "ru": {"title": "NABLA: –£–º–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "NABLA - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∞–¥–∞–ø—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ 
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#games"], "emoji": "üöÄ", "ru": {"title": "GSPO: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Group Seq
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#benchmark", "#architecture", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM: –º–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –≤—ã—à–µ —Ç–æ—á–Ω–æ—Å—Ç—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Momentum Uncertainty-guided Reasoning (MUR) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#optimization", "#training", "#reasoning", "#math", "#rl", "#benchmark"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: –º–µ–Ω—å—à–µ —Å–ª–æ–≤, –±–æ–ª—å—à–µ —Å–º—ã—Å–ª–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º LAPO (Len
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#diffusion", "#video", "#long_context", "#story_generation", "#multimodal", "#dataset"], "emoji": "üé¨", "ru": {"title": "–û—Ç —Ç–µ–∫—Å—Ç–∞ –∫ –∫–∏–Ω–æ: –ò–ò-—Ä–µ–∂–∏—Å—Å–µ—Ä Captain Cinema", "desc": "Captain Cinema - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ–∏–ª—å–º–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#optimization", "#training", "#reasoning", "#rl", "#benchmark"], "emoji": "üß†", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Hierarchical Budget Policy Optimization (HBPO) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–ª—É–±–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#cv", "#training", "#optimization"], "emoji": "üñºÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "TTS-VAR - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –û–Ω–∞ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π,
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#open_source", "#data", "#architecture", "#training"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞", "desc": "GLiNER2 - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ —Å –ø–æ–º–æ—â—å—é –æ–¥–Ω
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#diffusion", "#architecture", "#synthetic", "#3d", "#dataset"], "emoji": "üåé", "ru": {"title": "EarthCrafter: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —à–∏—Ä–æ–∫–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–º 3D-–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –ó–µ–º–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Aerial-Earth3D - –∫—Ä—É–ø–Ω–µ–π—à–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö 3D –∞—ç—Ä–æ—Å—ä–µ–º–∫–∏, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π 50 —Ç—ã—Å—è—á —Å—Ü–µ–Ω –ø–æ –≤—Å–µ–π 
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#open_source", "#architecture", "#data", "#optimization", "#benchmark"], "emoji": "üåä", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å DriftMoE: —É–∫—Ä–æ—â–µ–Ω–∏–µ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "desc": "DriftMoE - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ–º
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#reasoning", "#rlhf", "#open_source", "#architecture", "#training", "#optimization", "#rl"], "emoji": "üöÄ", "ru": {"title": "TeleChat: –Ω–æ–≤–æ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–ù–æ–≤–∞—è —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π TeleChat (TeleChat2, TeleChat2.5 –∏ T1) –¥–µ–º–æ–Ω—Å
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#diffusion", "#training", "#rl", "#audio", "#optimization"], "emoji": "üéôÔ∏è", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ –Ω–∞ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω—è—Ö", "desc": "DMOSpeech 2 - —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—â–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–≤—É–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –°–∏—Å
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#data", "#open_source", "#dataset", "#transfer_learning", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ GloVe: —à–∞–≥ –≤–ø–µ—Ä–µ–¥ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞", "desc": "–ù–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ GloVe 2024 –≥–æ–¥–∞ —É–ª—É—á—à–∞—é—Ç –≤–µ—Ä—Å–∏–∏ 2014 –≥–æ–¥–∞, –≤–∫–ª—é—á–∞—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∏ –¥–µ–º–æ–Ω
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#multimodal", "#games", "#cv", "#dataset", "#3d", "#optimization", "#benchmark"], "emoji": "üß†", "ru": {"title": "SpelkeNet: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º", "desc": "SpelkeNet - —ç—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ –∏–¥–µ–Ω—Ç–∏—Ñ–∏
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#inference", "#cv", "#diffusion", "#healthcare", "#open_source", "#benchmark"], "emoji": "üî¨", "ru": {"title": "SegDT: –ü—Ä–æ—Ä—ã–≤ –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ—Ä–∞–∂–µ–Ω–∏–π –∫–æ–∂–∏ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SegDT - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#open_source", "#video", "#optimization", "#architecture", "#cv"], "emoji": "üî¨", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –±–µ–∑ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Iwin Transformer - –Ω–æ–≤–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ 
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#optimization", "#training", "#open_source", "#inference", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "TeEFusion: –±—ã—Å—Ç—Ä—ã–π —Å–∏–Ω—Ç–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "TeEFusion - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É. –û–Ω –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —É—Å–ª–æ–≤–Ω—ã–µ –∏ –±–µ–∑—É—Å–ª–æ–≤–Ω—ã–µ —Ç–µ–∫
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#training", "#optimization", "#science", "#agents", "#reasoning"], "emoji": "üíπ", "ru": {"title": "–ù–∞–¥–µ–∂–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∑–∞–¥–∞—á", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —Å–µ—Ä–∏—é —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π Agentar-Fin-R1, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#cv", "#dataset", "#architecture", "#ethics", "#optimization", "#training"], "emoji": "üë§", "ru": {"title": "–ï–¥–∏–Ω–∞—è CNN –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ –ø–æ–ª–∞ –ø–æ –ª–∏—Ü—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ –ø–æ–ª–∞ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –ª–∏—Ü —Å –∏—Å–ø–æ
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#optimization", "#video", "#games"], "emoji": "üîç", "ru": {"title": "–ì–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "HLFormer - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ —á–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ (PRVR), –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å
[27.07.2025 02:26] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#training", "#multimodal", "#games"], "emoji": "üëÅÔ∏è", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (MICL) –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã
[27.07.2025 02:26] Renaming data file.
[27.07.2025 02:26] Renaming previous data. hf_papers.json to ./d/2025-07-25.json
[27.07.2025 02:26] Saving new data file.
[27.07.2025 02:26] Generating page.
[27.07.2025 02:26] Renaming previous page.
[27.07.2025 02:26] Renaming previous data. index.html to ./d/2025-07-25.html
[27.07.2025 02:26] Writing result.
[27.07.2025 02:26] Renaming log file.
[27.07.2025 02:26] Renaming previous data. log.txt to ./logs/2025-07-27_last_log.txt
