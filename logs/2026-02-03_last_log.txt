[03.02.2026 19:42] Read previous papers.
[03.02.2026 19:42] Generating top page (month).
[03.02.2026 19:42] Writing top page (month).
[03.02.2026 20:29] Read previous papers.
[03.02.2026 20:29] Get feed.
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00919
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02276
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22060
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02185
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02084
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02437
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02053
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01566
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02361
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01590
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02493
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02383
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02488
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01624
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01756
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02214
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01395
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01801
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01058
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02092
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01541
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01335
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01851
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01538
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02486
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01576
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02472
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02343
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02227
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01479
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02156
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01322
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20613
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02453
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01675
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01660
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01511
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01382
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00986
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00759
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02110
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02039
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22588
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01984
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01842
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01296
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01077
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22674
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01997
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01983
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23000
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00130
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02354
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02287
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01970
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00521
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22801
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00192
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22599
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22296
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21968
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21759
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14691
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02338
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01897
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01815
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01618
[03.02.2026 20:29] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01418
[03.02.2026 20:29] Extract page data from URL. URL: https://huggingface.co/papers/2602.00168
[03.02.2026 20:29] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[03.02.2026 20:29] No deleted papers detected.
[03.02.2026 20:29] Downloading and parsing papers (pdf, html). Total: 69.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.00919.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.00919.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.00919.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02276.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02276.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02276.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2601.22060.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2601.22060.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2601.22060.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02185.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02185.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02185.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02084.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02084.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02084.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02437.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02437.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02437.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02053.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02053.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02053.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01566.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01566.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01566.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02361.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02361.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02361.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01590.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01590.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01590.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02493.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02493.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02493.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02383.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02383.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02383.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02488.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02488.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02488.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01624.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01624.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01624.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01756.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01756.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01756.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02214.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02214.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02214.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01395.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01395.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01395.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01801.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01801.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01801.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01058.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01058.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01058.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02092.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02092.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02092.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01541.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01541.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01541.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01335.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01335.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01335.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01851.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01851.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01851.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01538.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01538.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01538.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02486.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02486.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02486.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01576.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01576.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01576.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02472.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02472.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02472.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02343.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02343.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02343.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02227.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02227.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02227.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01479.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01479.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01479.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02156.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02156.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02156.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01322.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01322.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01322.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2601.20613.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2601.20613.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2601.20613.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02453.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02453.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02453.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01675.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01675.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01675.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01660.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01660.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01660.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01511.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01511.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01511.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01382.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01382.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01382.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.00986.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.00986.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.00986.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.00759.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.00759.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.00759.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02110.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02110.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02110.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02039.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02039.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02039.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2601.22588.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2601.22588.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2601.22588.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01984.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01984.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01984.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01842.
[03.02.2026 20:29] Downloading paper 2602.01842 from https://arxiv.org/pdf/2602.01842v1...
[03.02.2026 20:29] Failed to download and parse paper https://huggingface.co/papers/2602.01842: 'LTChar' object is not iterable
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01296.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01296.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01296.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01077.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01077.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01077.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2601.22674.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2601.22674.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2601.22674.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01997.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01997.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01997.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01983.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01983.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01983.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2601.23000.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2601.23000.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2601.23000.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.00130.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.00130.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.00130.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02354.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02354.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02354.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02287.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02287.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02287.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01970.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01970.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01970.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.00521.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.00521.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.00521.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2601.22801.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2601.22801.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2601.22801.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.00192.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.00192.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.00192.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2601.22599.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2601.22599.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2601.22599.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2601.22296.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2601.22296.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2601.22296.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2601.21968.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2601.21968.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2601.21968.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2601.21759.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2601.21759.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2601.21759.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2601.14691.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2601.14691.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2601.14691.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.02338.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.02338.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.02338.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01897.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01897.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01897.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01815.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01815.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01815.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01618.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01618.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01618.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.01418.
[03.02.2026 20:29] Extra JSON file exists (./assets/json/2602.01418.json), skip PDF parsing.
[03.02.2026 20:29] Paper image links file exists (./assets/img_data/2602.01418.json), skip HTML parsing.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Downloading and parsing paper https://huggingface.co/papers/2602.00168.
[03.02.2026 20:29] Downloading paper 2602.00168 from https://arxiv.org/pdf/2602.00168v1...
[03.02.2026 20:29] Extracting affiliations from text.
[03.02.2026 20:29] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"YOLOE-26: INTEGRATING YOLO26 WITH YOLOE FOR REAL-TIME OPEN-VOCABULARY INSTANCE SEGMENTATION Cornell University, Biological & Environmental Engineering, Ithaca, NY 14850, USA rs2672@cornell.edu Ranjan Sapkota Manoj Karkee February 3, "
[03.02.2026 20:29] Response: ```python
["Cornell University, Biological & Environmental Engineering, Ithaca, NY 14850, USA"]
```
[03.02.2026 20:29] Deleting PDF ./assets/pdf/2602.00168.pdf.
[03.02.2026 20:29] Success.
[03.02.2026 20:29] Enriching papers with extra data.
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 0. Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.  					AI-generated summary 				 We introduce Green-VLA, a staged Vision-Language-Action (...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 1. Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.  					AI-generated summary 				 We introduce Kimi K2.5, an open-source multimodal agentic model designed to adva...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 2. Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning.  					AI-generated summary 				 Multimodal large langu...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 3. Vision-DeepResearch benchmark addresses limitations in evaluating visual-textual search capabilities of multimodal models by introducing realistic evaluation conditions and improving visual retrieval through multi-round cropped-search workflow.  					AI-generated summary 				 Multimodal Large Langua...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 4. RPG-Encoder framework transforms repository comprehension and generation into a unified cycle by encoding code into high-fidelity Repository Planning Graph representations that improve understanding and reconstruction accuracy.  					AI-generated summary 				 Current repository agents encounter a re...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 5. UniReason integrates text-to-image generation and image editing through a dual reasoning paradigm that enhances planning with world knowledge and uses editing for visual refinement, achieving superior performance on reasoning-intensive benchmarks.  					AI-generated summary 				 Unified multimodal m...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 6. WildGraphBench evaluates GraphRAG performance in realistic scenarios using Wikipedia's structured content to assess multi-fact aggregation and summarization capabilities across diverse document types.  					AI-generated summary 				 Graph-based Retrieval-Augmented Generation (GraphRAG) organizes ext...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 7. A file-system-based dual-agent framework enables large language model agents to perform extended research tasks beyond context window limitations by using persistent storage as external memory.  					AI-generated summary 				 Deep research is emerging as a representative long-horizon task for large ...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 8. A scalable framework for constructing real-world software engineering environments from GitHub pull requests using an efficient building agent with self-verification and hacking detection capabilities.  					AI-generated summary 				 We propose SWE-Universe, a scalable and efficient framework for au...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 9. Deep Research Agents demonstrate capabilities in autonomous information retrieval but show significant gaps when evaluated against expert-level Wikipedia articles using a new live benchmark and comprehensive evaluation framework.  					AI-generated summary 				 Deep Research Agents (DRAs) have demon...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 10. PixelGen is a pixel-space diffusion framework that uses perceptual supervision through LPIPS and DINO-based losses to generate high-quality images without requiring VAEs or latent representations.  					AI-generated summary 				 Pixel diffusion generates images directly in pixel space in an end-to-e...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 11. SLIME is a novel reference-free alignment objective for large language models that decouples preference learning from generation quality through a three-pronged approach combining likelihood maximization, probability stabilization, and dual-margin constraints.  					AI-generated summary 				 Direct ...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 12. RLAnything enhances reinforcement learning for LLMs and agents through dynamic model optimization and closed-loop feedback mechanisms that improve policy and reward model training.  					AI-generated summary 				 We propose RLAnything, a reinforcement learning framework that dynamically forges envir...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 13. PISCES is an annotation-free text-to-video generation method that uses dual optimal transport-aligned rewards to improve visual quality and semantic alignment without human preference annotations.  					AI-generated summary 				 Text-to-video (T2V) generation aims to synthesize videos with high visu...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 14. Mind-Brush presents a unified agentic framework for text-to-image generation that dynamically retrieves multimodal evidence and employs reasoning tools to improve understanding of implicit user intentions and complex knowledge reasoning.  					AI-generated summary 				 While text-to-image generation...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 15. A novel Causal Forcing method addresses the architectural gap in distilling bidirectional video diffusion models into autoregressive models by using AR teachers for ODE initialization, significantly improving video generation performance.  					AI-generated summary 				 To achieve real-time interact...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 16. Selective knowledge distillation in autoregressive language models using student-entropy-guided position selection improves accuracy and efficiency while reducing memory and storage requirements.  					AI-generated summary 				 Growing efforts to improve knowledge distillation (KD) in large language...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 17. Autoregressive video diffusion models face efficiency challenges due to growing KV caches and redundant attention computations, which are addressed through TempCache, AnnCA, and AnnSA techniques that reduce computational demands while maintaining visual quality and stable performance.  					AI-gener...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 18. Post-training of reasoning large language models can be improved by correcting distribution mismatches between supervised fine-tuning and reinforcement learning stages through importance sampling reweighting of the SFT loss.  					AI-generated summary 				 Post-training of reasoning LLMs is a holist...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 19. FSVideo is a fast transformer-based image-to-video diffusion framework that uses a compressed video autoencoder, diffusion transformer architecture with enhanced layer memory, and multi-resolution generation strategy to achieve high performance with significantly reduced computation time.  					AI-g...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 20. MLLMs equipped with Cognitive Supersensing and Latent Visual Imagery Prediction demonstrate enhanced cognitive reasoning capabilities through integrated visual and textual reasoning pathways.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved remarkable success in...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 21. Visual metaphor transfer enables creative AI systems to decompose abstract conceptual relationships from reference images and reapply them to new subjects through a multi-agent framework grounded in cognitive theory.  					AI-generated summary 				 A visual metaphor constitutes a high-order form of ...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 22. Visual Instruction Benchmark for Image Editing introduces a three-level interaction hierarchy for evaluating visual instruction following capabilities in generative models.  					AI-generated summary 				 Recent generative models have achieved remarkable progress in image editing. However, existing ...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 23. A dual-stream framework called InteractAvatar is presented for generating talking avatars that can interact with objects in their environment, addressing challenges in grounded human-object interaction through decoupled perception and planning modules.  					AI-generated summary 				 Generating talk...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 24. Re-TRAC is an agentic framework that enhances LLM-based research agents by enabling cross-trajectory exploration and iterative reflection through structured state representations, leading to more efficient and effective problem-solving compared to traditional ReAct approaches.  					AI-generated sum...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 25. Visual world models for mobile GUI agents are improved through renderable code generation using vision-language models, achieving better performance with reduced model size compared to existing approaches.  					AI-generated summary 				 Mobile Graphical User Interface (GUI) World Models (WMs) offer...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 26. SPARKLING is a framework for mid-stage width expansion in deep learning models that maintains signal preservation and breaks symmetry to stabilize training and reduce computational costs.  					AI-generated summary 				 Progressive Learning (PL) reduces pre-training computational overhead by gradual...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 27. Large language model control methods are unified under a dynamic weight update framework, revealing a preference-utility trade-off and enabling improved steering through SPLIT approach.  					AI-generated summary 				 Methods for controlling large language models (LLMs), including local weight fine-...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 28. LatentMorph integrates implicit latent reasoning into text-to-image generation through four lightweight components that enable adaptive self-refinement and improve both efficiency and cognitive alignment.  					AI-generated summary 				 Text-to-image (T2I) generation has achieved remarkable progress...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 29. A Japanese financial language understanding benchmark named Ebisu is introduced, featuring two expert-annotated tasks that evaluate implicit commitment recognition and hierarchical financial terminology extraction, revealing persistent challenges for current language models despite their advanced ca...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 30. Loop-ViT introduces a recursive vision transformer architecture that decouples reasoning depth from model capacity through weight-tied recurrence and dynamic exit mechanisms, achieving superior visual reasoning performance with fewer parameters.  					AI-generated summary 				 Recent advances in vis...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 31. PolySAE extends sparse autoencoders with polynomial decoding to capture feature interactions and compositional structure while maintaining linear encoders for interpretability.  					AI-generated summary 				 Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural netwo...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 32. AgentIF-OneDay evaluates AI agents' ability to handle diverse daily tasks through natural language instructions, requiring problem-solving, attachment understanding, and file-based outputs across three user-centric categories.  					AI-generated summary 				 The capacity of AI agents to effectively ...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 33. Thinking with Comics emerges as an effective visual reasoning approach that bridges images and videos by leveraging comic structures for improved multimodal reasoning efficiency and performance.  					AI-generated summary 				 Chain-of-Thought reasoning has driven large language models to extend fro...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 34. TRIP-Bench presents a comprehensive long-horizon benchmark for travel planning that evaluates LLM agents on complex multi-turn interactions, while GTPO offers an online reinforcement learning approach to enhance constraint satisfaction and robustness in extended dialogues.  					AI-generated summary...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 35. A novel framework called CoDiQ enables controllable difficulty generation for competition-level questions through test-time scaling, resulting in a corpus that significantly improves large reasoning model performance.  					AI-generated summary 				 Large Reasoning Models (LRMs) benefit substantiall...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 36. Rubric-ARM framework jointly optimizes rubric generation and judging through reinforcement learning to improve response quality assessment in creative and open-ended tasks.  					AI-generated summary 				 Standard reward models typically predict scalar scores that fail to capture the multifaceted na...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 37. Flow matching models for text-to-image generation are enhanced through a reinforcement learning framework that addresses sample inefficiency and prompt overfitting by incorporating language models for prompt refinement, achieving superior performance with reduced computational requirements.  					AI...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 38. Research identifies a sparse reward subsystem in LLM hidden states containing value neurons that represent internal state expectations and dopamine-like neurons encoding reward prediction errors.  					AI-generated summary 				 In this paper, we identify a sparse reward subsystem within the hidden s...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 39. Adaptive Ability Decomposing (AÂ²D) enhances reinforcement learning with verifiable rewards by decomposing complex questions into simpler sub-questions, improving LLM reasoning through guided exploration without requiring a teacher model.  					AI-generated summary 				 Reinforcement learning with ve...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 40. Post-training quantization effects in world models reveal unique failure modes and trade-offs between accuracy, bit-width, and planning performance, particularly in encoder-predictor module asymmetries and low-bit rollout stability.  					AI-generated summary 				 World models learn an internal repr...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 41. Agentic large language models require investigatory intelligence for autonomous data analysis, demonstrated through the Deep Data Research benchmark that evaluates their ability to extract insights from databases without explicit queries.  					AI-generated summary 				 The agency expected of Agenti...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 42. Small language models can effectively evaluate outputs by leveraging internal representations rather than generating responses, enabling a more efficient and interpretable evaluation approach through a probing-based framework.  					AI-generated summary 				 Large language models (LLMs) are widely u...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 43. Scaling hidden states of delimiter tokens in vision-language models reduces cross-image information leakage and improves multi-image reasoning performance.  					AI-generated summary 				 Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance dec...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 44. A new test-time scaling framework called Prism is introduced for discrete diffusion language models that improves reasoning performance through hierarchical trajectory search, local branching with partial remasking, and self-verified feedback mechanisms.  					AI-generated summary 				 Inference-tim...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 45. LiP-Map presents a line-plane joint optimization framework that explicitly models learnable line and planar primitives for accurate 3D line mapping in man-made environments.  					AI-generated summary 				 3D line mapping from multi-view RGB images provides a compact and structured visual representa...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 46. PISA is a novel sparse attention method that improves diffusion transformer efficiency by approximating non-critical attention blocks instead of discarding them, achieving faster processing with maintained quality.  					AI-generated summary 				 Diffusion Transformers are fundamental for video and ...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 47. VisionTrim is a training-free framework that accelerates multimodal large language models by selecting dominant visual tokens and merging them with text-guided complementation, improving efficiency without performance loss.  					AI-generated summary 				 Multimodal large language models (MLLMs) suf...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 48. Layer pruning compresses large language models while maintaining classification performance but causes significant degradation in generative reasoning tasks, with limited recovery possible through supervised finetuning on self-generated responses.  					AI-generated summary 				 Recent works have sh...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 49. A training-free framework enables language model agents to automatically create and optimize tools during inference, improving their reasoning capabilities through self-evolution and memory consolidation.  					AI-generated summary 				 Existing Tool-Integrated Reasoning (TIR) models have effectivel...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 50. A novel optimizer called Mano is proposed that combines manifold optimization with momentum projection onto tangent spaces, achieving superior performance over AdamW and Muon while reducing memory and computational requirements.  					AI-generated summary 				 While large language models (LLMs) have...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 51. Effective dimension, an unsupervised geometric metric, strongly predicts neural network performance across different architectures and domains, showing bidirectional causality between representation geometry and accuracy.  					AI-generated summary 				 We investigate the relationship between repres...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 52. Implicit neural representations operate continuously over UV coordinate space, demonstrating good image quality while balancing memory usage and rendering time, with applications in real-time rendering and downstream tasks.  					AI-generated summary 				 Implicit neural representation (INR) has pro...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 53. Controlled cross-lingual evaluation reveals instability in LLM assessment methods when targeting morphologically rich languages, indicating unreliable zero-shot judge transfer for discourse-level tasks.  					AI-generated summary 				 Cross-lingual evaluation of large language models (LLMs) typicall...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 54. Generalizable Predictive Prompt Selection (GPS) uses Bayesian inference with a lightweight generative model to efficiently select informative prompts for reinforcement learning-enhanced language models, improving training efficiency and performance.  					AI-generated summary 				 Reinforcement lear...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 55. A two-phase diagnostic framework based on Item Response Theory and Graded Response Model is introduced to assess the reliability of LLM-as-a-Judge by examining intrinsic consistency and human alignment.  					AI-generated summary 				 While LLM-as-a-Judge is widely used in automated evaluation, exis...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 56. Clipping-Free Policy Optimization replaces heuristic clipping with convex quadratic penalty to stabilize reinforcement learning training for large language models without performance loss.  					AI-generated summary 				 Reinforcement learning has become central to post-training large language model...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 57. VAE-based inpainting creates spectral shifts that fool detection systems, which can be mitigated through Inpainting Exchange to improve content-aware detection performance.  					AI-generated summary 				 Modern deep learning-based inpainting enables realistic local image manipulation, raising criti...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 58. Automated pipeline for sound separation using high-purity single-event segments from in-the-wild datasets achieves competitive performance with significantly reduced data requirements.  					AI-generated summary 				 Query-based universal sound separation is fundamental to intelligent auditory syste...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 59. Parallel Echo State Network (ParalESN) addresses reservoir computing limitations by enabling parallel temporal processing through diagonal linear recurrence, maintaining theoretical guarantees while achieving significant computational efficiency gains.  					AI-generated summary 				 Reservoir Compu...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 60. On-policy Verbal Distillation (OVD) enables efficient knowledge transfer from teacher to student models by replacing token-level probability matching with trajectory matching using discrete verbal scores, reducing memory consumption and enabling free exploration without token alignment constraints. ...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 61. An reinforcement learning-based sampling framework adaptively reweights training datasets to improve embedding model performance while reducing GPU costs.  					AI-generated summary 				 General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 62. Large language models used as judges for agent performance evaluation are vulnerable to manipulation of reasoning traces, with content-based fabrications being more effective than style-based alterations.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as judges to...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 63. ReSID presents a novel recommendation-native framework that improves sequential recommendation by learning predictive item representations and optimizing quantization for information preservation and sequential predictability.  					AI-generated summary 				 Semantic ID (SID)-based recommendation is...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 64. Internal flow signatures analyze depthwise dynamics in large language models to enable self-checking and targeted refinement without modifying the base model.  					AI-generated summary 				 Large language models can generate fluent answers that are unfaithful to the provided context, while many saf...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 65. Multi-agent systems for molecular discovery that use individualized scientist profiles based on publication and molecular history outperform traditional role-based approaches.  					AI-generated summary 				 Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery....
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 66. Researchers developed a novel agentic data-generation framework to create culturally grounded safety datasets for Southeast Asia, resulting in multilingual safeguard models that outperform existing approaches in detecting regionally sensitive content while maintaining general safety performance.  		...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 67. Parabolic Position Encoding (PaPE) is a novel position encoding method for vision modalities that improves upon existing approaches by incorporating translation invariance, rotation invariance, distance decay, directionality, and context awareness principles.  					AI-generated summary 				 We propo...
[03.02.2026 20:29] ********************************************************************************
[03.02.2026 20:29] Abstract 68. YOLOE-26 integrates YOLO26 architecture with open-vocabulary learning for real-time instance segmentation, utilizing convolutional backbones, end-to-end regression, and object embedding heads with text and visual prompting capabilities.  					AI-generated summary 				 This paper presents YOLOE-26, a...
[03.02.2026 20:29] Read previous papers.
[03.02.2026 20:29] Generating reviews via LLM API.
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#data", "#rl", "#robotics", "#training"], "emoji": "ð¤", "ru": {"title": "Ð£Ð½Ð¸Ð²ÐµÑÑÐ°Ð»ÑÐ½Ð°Ñ Ð¿Ð¾Ð»Ð¸ÑÐ¸ÐºÐ° Ð´ÐµÐ¹ÑÑÐ²Ð¸Ð¹ Ð´Ð»Ñ ÑÐ¾Ð±Ð¾ÑÐ¾Ð² ÑÐ°Ð·Ð½ÑÑ ÐºÐ¾Ð½ÑÑÑÑÐºÑÐ¸Ð¹ ÑÐµÑÐµÐ· Ð¼Ð½Ð¾Ð³Ð¾ÑÑÐ°Ð¿Ð½Ð¾Ðµ Ð¾Ð±ÑÑÐµÐ½Ð¸Ðµ Ñ ÑÐ·ÑÐºÐ¾Ð¼ Ð¸ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "Green-VLA Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ ÑÐ¾Ð±Ð¾Ð¹ Ð¿ÑÑÐ¸ÑÑÐ°Ð¿Ð½ÑÑ Ð°ÑÑÐ¸ÑÐµÐºÑÑÑÑ Vision-Language
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#plp", "#cv", "#agents", "#reasoning", "#open_source", "#training"], "emoji": "ð", "ru": {"title": "ÐÐ½Ð¾Ð³Ð¾Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÐ¹ Ð¸Ð½ÑÐµÐ»Ð»ÐµÐºÑ ÑÐµÑÐµÐ· ÑÐ¾ÐµÐ²ÑÑ ÐºÐ¾Ð¾ÑÐ´Ð¸Ð½Ð°ÑÐ¸Ñ Ð°Ð³ÐµÐ½ÑÐ¾Ð²", "desc": "ÐÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÐ¼ Kimi K2.5 â Ð¾ÑÐºÑÑÑÑÑ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ Ð¼Ð¾Ð´ÐµÐ»Ñ-Ð°Ð³ÐµÐ½ÑÐ°, ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð¸ÑÑÐµÑ Ð¾Ð±ÑÐ°Ð±Ð¾ÑÐºÑ ÑÐµÐº
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#rag", "#reasoning", "#optimization", "#rl", "#open_source"], "emoji": "ð", "ru": {"title": "ÐÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½Ð¾Ðµ Ð³Ð»ÑÐ±Ð¾ÐºÐ¾Ðµ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÐµÑÐµÐ· Ð¼Ð½Ð¾Ð³Ð¾ÑÐ¾Ð´Ð¾Ð²Ð¾Ð¹ Ð¿Ð¾Ð¸ÑÐº Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸ÐµÐ¼", "desc": "Vision-DeepResearch Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÑ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ Ð¿Ð°ÑÐ°Ð´Ð¸Ð³Ð¼Ñ Ð³Ð»ÑÐ±Ð¾ÐºÐ¾Ð³Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#dataset", "#open_source"], "emoji": "ð", "ru": {"title": "Ð ÐµÐ°Ð»Ð¸ÑÑÐ¸ÑÐ½Ð°Ñ Ð¾ÑÐµÐ½ÐºÐ° Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ° Ð² Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ ÑÐ¸ÑÑÐµÐ¼Ð°Ñ", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Vision-DeepResearch benchmark Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐµÐ¹ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Ð²
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#plp"], "emoji": "ð", "ru": {"title": "ÐÐ´Ð¸Ð½ÑÐ¹ ÑÐ¸ÐºÐ» Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð¸ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ ÐºÐ¾Ð´Ð° ÑÐµÑÐµÐ· Ð²ÑÑÐ¾ÐºÐ¾ÑÐ¾ÑÐ½ÑÐµ Ð³ÑÐ°ÑÐ¾Ð²ÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ RPG-Encoder â ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð¿ÑÐµÐ¾Ð±ÑÐ°Ð·ÑÐµÑ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð¸ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ñ ÐºÐ¾Ð´Ð° Ð² ÐµÐ´Ð¸Ð½ÑÐ¹ ÑÐ¸ÐºÐ» ÑÐµÑÐµÐ· ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²ÑÑÐ¾ÐºÐ¾
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#reasoning", "#synthetic"], "emoji": "ð¨", "ru": {"title": "ÐÐ´Ð¸Ð½Ð¾Ðµ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ ÑÐ¾Ð²ÐµÑÑÐµÐ½Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸: Ð¿Ð»Ð°Ð½Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ ÑÑÐ¾Ð½ÑÐµÐ½Ð¸Ðµ", "desc": "UniReason â ÑÑÐ¾ ÐµÐ´Ð¸Ð½Ð°Ñæ¡æ¶, ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð¾Ð±ÑÐµÐ´Ð¸Ð½ÑÐµÑ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ñ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸ Ð¸Ñ ÑÐµÐ´Ð°ÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ ÑÐµÑÐµÐ· Ð´Ð²Ð¾Ð¹Ð½ÑÑ 
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#dataset", "#rag", "#benchmark", "#open_source", "#graphs", "#long_context"], "emoji": "ð¸ï¸", "ru": {"title": "ÐÐµÐ½ÑÐ¼Ð°ÑÐº Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ Ð¸Ð·Ð²Ð»ÐµÑÐµÐ½Ð¸Ñ Ð¸ Ð°Ð³ÑÐµÐ³Ð°ÑÐ¸Ð¸ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð² ÑÐµÐ°Ð»ÑÐ½ÑÑ ÑÑÐµÐ½Ð°ÑÐ¸ÑÑ", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½ WildGraphBench â Ð±ÐµÐ½ÑÐ¼Ð°ÑÐº Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ GraphRAG Ð² ÑÐµÐ°Ð»Ð¸ÑÑÐ¸ÑÐ½ÑÑ ÑÑÐµÐ½Ð°ÑÐ¸ÑÑ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark", "#reasoning", "#open_source", "#long_context"], "emoji": "ð", "ru": {"title": "ÐÑÐµÐ¾Ð´Ð¾Ð»ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð½ÑÐµÐºÑÑÐ½ÑÑ Ð¾Ð³ÑÐ°Ð½Ð¸ÑÐµÐ½Ð¸Ð¹: Ð²Ð½ÐµÑÐ½ÑÑ Ð¿Ð°Ð¼ÑÑÑ Ð´Ð»Ñ Ð°Ð³ÐµÐ½ÑÐ¾Ð² LLM", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ FS-Researcher â Ð´Ð²ÑÑÐ°Ð³ÐµÐ½ÑÐ½Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð°, Ð³Ð´Ðµ Ð¾Ð´Ð¸Ð½ Ð°Ð³ÐµÐ½Ñ ÑÐ¾Ð±Ð¸ÑÐ°ÐµÑ Ð¸Ð½ÑÐ¾Ñ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multilingual", "#plp", "#dataset", "#agents", "#benchmark", "#rl", "#training"], "emoji": "ðï¸", "ru": {"title": "ÐÐ¸Ð»Ð»Ð¸Ð¾Ð½Ð½ÑÐ¹ Ð¼Ð°ÑÑÑÐ°Ð± Ð²ÐµÑÐ¸ÑÐ¸ÑÐ¸ÑÑÐµÐ¼ÑÑ Ð·Ð°Ð´Ð°Ñ ÑÐ°Ð·ÑÐ°Ð±Ð¾ÑÐºÐ¸ ÑÐµÑÐµÐ· ÑÐ¼Ð½ÑÑ Ð°Ð³ÐµÐ½ÑÐ¾Ð²", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑÑÑ SWE-Universe â Ð¼Ð°ÑÑÑÐ°Ð±Ð¸ÑÑÐµÐ¼Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð´Ð»Ñ Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸ÑÐµÑÐºÐ¾Ð³Ð¾ ÑÐ¾Ð·Ð´Ð°
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark", "#open_source", "#survey"], "emoji": "ð", "ru": {"title": "ÐÐ·Ð¼ÐµÑÑÑ Ð´Ð¸ÑÑÐ°Ð½ÑÐ¸Ñ Ð¼ÐµÐ¶Ð´Ñ AI-Ð°Ð³ÐµÐ½ÑÐ°Ð¼Ð¸ Ð¸ ÑÐºÑÐ¿ÐµÑÑÐ½ÑÐ¼Ð¸ Ð·Ð½Ð°Ð½Ð¸ÑÐ¼Ð¸", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð° Ð½Ð¾Ð²Ð°Ñ benchmark Wiki Live Challenge, ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÐµÑ ÑÑÐ°ÑÑÐ¸ Wikipedia Good Articles ÐºÐ°Ðº ÑÑÐ°Ð»Ð¾
[03.02.2026 20:29] Using data from previous issue: {"categories": [], "emoji": "ð¨", "ru": {"title": "ÐÑÑÐ¼Ð°Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ñ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ð² Ð¿Ð¸ÐºÑÐµÐ»ÑÐ½Ð¾Ð¼ Ð¿ÑÐ¾ÑÑÑÐ°Ð½ÑÑÐ²Ðµ ÑÐµÑÐµÐ· Ð¿ÐµÑÑÐµÐ¿ÑÐ¸Ð²Ð½ÑÐ¹ Ð½Ð°Ð´Ð·Ð¾Ñ", "desc": "PixelGen â ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð³ÐµÐ½ÐµÑÐ¸ÑÑÐµÑ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ñ Ð¿ÑÑÐ¼Ð¾ Ð² Ð¿Ð¸ÐºÑÐµÐ»ÑÐ½Ð¾Ð¼ Ð¿ÑÐ¾ÑÑÑÐ°Ð½ÑÑÐ²Ðµ, Ð¸Ð·Ð±ÐµÐ³Ð°Ñ Ð°ÑÑÐµÑÐ°ÐºÑÐ¾Ð², ÐºÐ¾ÑÐ¾ÑÑÐµ Ð²Ð½Ð¾ÑÑÑ VAE Ð² ÑÑÐ°Ð´Ð¸Ñ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#training"], "emoji": "âï¸", "ru": {"title": "Ð¡ÑÐ°Ð±Ð¸Ð»ÑÐ½Ð¾Ðµ Ð²ÑÑÐ°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð±ÐµÐ· ÐºÐ¾Ð¼Ð¿ÑÐ¾Ð¼Ð¸ÑÑÐ° Ð¼ÐµÐ¶Ð´Ñ Ð¿ÑÐµÐ´Ð¿Ð¾ÑÑÐµÐ½Ð¸ÑÐ¼Ð¸ Ð¸ ÐºÐ°ÑÐµÑÑÐ²Ð¾Ð¼", "desc": "SLIME â ÑÑÐ¾ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ Ð²ÑÑÐ°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ñ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾ÑÐ¾ÑÑÐ¹ ÑÐµÑÐ°ÐµÑ Ð¿ÑÐ¾Ð±Ð»ÐµÐ¼Ñ Ð½ÐµÑÐ¾Ð¾ÑÐ²ÐµÑÑÑÐ²Ð¸Ñ ÑÐµÐ»ÐµÐ²ÑÑ ÑÑÐ½ÐºÑÐ¸Ð¹ Ð² Ð¼ÐµÑÐ¾Ð´Ð°Ñ Ð¿ÑÑ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#rl", "#agents", "#training"], "emoji": "ð", "ru": {"title": "ÐÐ¸Ð½Ð°Ð¼Ð¸ÑÐµÑÐºÐ°Ñ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ñ ÑÐµÑÐµÐ· Ð·Ð°Ð¼ÐºÐ½ÑÑÑÑ Ð¾Ð±ÑÐ°ÑÐ½ÑÑ ÑÐ²ÑÐ·Ñ Ð² Ð¾Ð±ÑÑÐµÐ½Ð¸Ð¸ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "RLAnything â ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÑÐµÑÐºÐ¸ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð¸ÑÑÐµÑ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¾ÐºÑÑÐ¶ÐµÐ½Ð¸Ñ, Ð¿Ð¾Ð»Ð¸ÑÐ¸ÐºÐ¸ Ð¸ Ð²Ð¾Ð·Ð½Ð°Ð³ÑÐ°Ð¶Ð´ÐµÐ½
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#optimization", "#rl", "#video", "#training"], "emoji": "ð¬", "ru": {"title": "ÐÐ¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½ÑÐ¹ ÑÑÐ°Ð½ÑÐ¿Ð¾ÑÑ Ð´Ð»Ñ Ð±ÐµÐ·Ð°Ð½Ð½Ð¾ÑÐ°ÑÐ¸Ð¾Ð½Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð¸Ð· ÑÐµÐºÑÑÐ°", "desc": "PISCES â ÑÑÐ¾ Ð¼ÐµÑÐ¾Ð´ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð¸Ð· ÑÐµÐºÑÑÐ°, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð½Ðµ ÑÑÐµÐ±ÑÐµÑ Ð°Ð½Ð½Ð¾ÑÐ°ÑÐ¸Ð¹ ÑÐµÐ»Ð¾Ð²ÐµÐºÐ° Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ ÐºÐ°ÑÐµÑÑÐ²Ð°. ÐÐ²ÑÐ¾ÑÑ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÑÑ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#benchmark", "#rag", "#reasoning"], "emoji": "ð¨", "ru": {"title": "ÐÐ³ÐµÐ½ÑÐ½ÑÐ¹ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ ÐºÐ¾Ð½ÑÐµÐºÑÑÐ½Ð¾-Ð¾ÑÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ ÑÐµÑÐµÐ· Ð´Ð¸Ð½Ð°Ð¼Ð¸ÑÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð·Ð½Ð°Ð½Ð¸Ð¹", "desc": "Mind-Brush Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð°Ð³ÐµÐ½ÑÐ½ÑÐ¹ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸Ð· ÑÐµÐºÑÑ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#architecture", "#video", "#training"], "emoji": "ð¬", "ru": {"title": "ÐÑÐ¸ÑÐ¸Ð½Ð½Ð¾Ðµ Ð¿ÑÐ¸Ð½ÑÐ¶Ð´ÐµÐ½Ð¸Ðµ: Ð´Ð¸ÑÑÐ¸Ð»Ð»ÑÑÐ¸Ñ Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐµÑÐµÐ· Ð°Ð²ÑÐ¾ÑÐµÐ³ÑÐµÑÑÐ¸Ð²Ð½ÑÑ ÑÑÐ¸ÑÐµÐ»ÐµÐ¹", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ Causal Forcing Ð´Ð»Ñ Ð´Ð¸ÑÑÐ¸Ð»Ð»ÑÑÐ¸Ð¸ Ð´Ð²ÑÐ½Ð°Ð¿ÑÐ°Ð²Ð»ÐµÐ½Ð½ÑÑ Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#inference", "#optimization", "#training", "#small_models"], "emoji": "ð§ ", "ru": {"title": "Ð£Ð¼Ð½Ð°Ñ Ð´Ð¸ÑÑÐ¸Ð»Ð»ÑÑÐ¸Ñ: Ð²ÑÐ±Ð¸ÑÐ°ÐµÐ¼ Ð¿ÑÐ°Ð²Ð¸Ð»ÑÐ½ÑÐµ Ð¿Ð¾Ð·Ð¸ÑÐ¸Ð¸ Ð´Ð»Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑÐ¾Ð´ Ð²ÑÐ±Ð¾ÑÐ¾ÑÐ½Ð¾Ð¹ Ð´Ð¸ÑÑÐ¸Ð»Ð»ÑÑÐ¸Ð¸ Ð·Ð½Ð°Ð½Ð¸Ð¹ (knowledge distillation) Ð´Ð»Ñ Ð°Ð²ÑÐ¾Ð³ÑÐµÑÑÐ¸Ð²Ð½ÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´Ðµ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#video", "#inference", "#diffusion", "#optimization", "#long_context"], "emoji": "â¡", "ru": {"title": "ÐÐ¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ñ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð² Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÑÑ ÑÐµÑÐµÐ· ÑÐ¶Ð°ÑÐ¸Ðµ ÐºÐµÑÐ° Ð¸ ÑÐ°Ð·ÑÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð²ÑÑÐ¸ÑÐ»ÐµÐ½Ð¸Ð¹", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ñ ÑÑÐ¸ ÑÐµÑÐ½Ð¸ÐºÐ¸ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ð¸ Ð´Ð»Ñ Ð°Ð²ÑÐ¾ÑÐµÐ³ÑÐµÑÑÐ¸Ð²Ð½ÑÑ Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð¸ÑÑ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#math", "#reasoning", "#optimization", "#rl", "#training"], "emoji": "âï¸", "ru": {"title": "ÐÐ°Ð»Ð°Ð½ÑÐ¸ÑÐ¾Ð²ÐºÐ° ÑÐ°ÑÐ¿ÑÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ ÑÐ¸Ð½ÐµÑÐ³Ð¸Ð¸ SFT Ð¸ RL Ð² Ð¾Ð±ÑÑÐµÐ½Ð¸Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑÐ¾Ð´ PEAR Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ ÑÐ¾Ð²Ð¼ÐµÑÑÐ½Ð¾Ð³Ð¾ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ Ð½Ð° ÑÑÐ°Ð¿Ð°Ñ supervised fine-t
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#architecture", "#video", "#diffusion", "#optimization", "#open_source", "#training"], "emoji": "ð¬", "ru": {"title": "ÐÐ¾ÑÑÐ´Ð¾Ðº Ð²ÐµÐ»Ð¸ÑÐ¸Ð½Ñ Ð±ÑÑÑÑÐµÐµ: Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½Ð½ÑÐµ ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼ÐµÑÑ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð¸Ð· Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹", "desc": "FSVideo Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ ÑÐ¾Ð±Ð¾Ð¹ Ð±ÑÑÑÑÑÐ¹ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ Ð¿ÑÐµÐ¾Ð±ÑÐ°Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#reasoning", "#rl", "#open_source", "#training"], "emoji": "ð§ ", "ru": {"title": "ÐÐ½ÑÑÑÐµÐ½Ð½ÑÑ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½Ð°Ñ Ð¿Ð°Ð¼ÑÑÑ ÐºÐ°Ðº Ð¾ÑÐ½Ð¾Ð²Ð° ÐºÐ¾Ð³Ð½Ð¸ÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ Ð¼ÑÑÐ»ÐµÐ½Ð¸Ñ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Cognitive Supersensing Ð´Ð»
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#open_source"], "emoji": "ð¨", "ru": {"title": "Ð¢Ð²Ð¾ÑÑÐµÑÐºÐ¸Ð¹ Ð¿ÐµÑÐµÐ½Ð¾Ñ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÑ Ð¼ÐµÑÐ°ÑÐ¾Ñ ÑÐµÑÐµÐ· Ð¼Ð½Ð¾Ð³Ð¾Ð°Ð³ÐµÐ½ÑÐ½ÑÑ ÑÐ¸ÑÑÐµÐ¼Ñ Ñ ÐºÐ¾Ð³Ð½Ð¸ÑÐ¸Ð²Ð½ÑÐ¼ Ð¾Ð±Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð½Ð¾Ð²Ð°Ñ Ð·Ð°Ð´Ð°ÑÐ° Visual Metaphor Transfer, ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð·Ð°ÐºÐ»ÑÑÐ°ÐµÑÑÑ Ð² Ð¸Ð·Ð²Ð»ÐµÑÐµÐ½Ð¸Ð¸ Ð°Ð±ÑÑÑÐ°ÐºÑÐ½Ð¾Ð¹ ÑÐ²Ð¾ÑÑÐµÑÐºÐ¾Ð¹ ÑÑÑÐ½Ð¾ÑÑÐ¸ Ð¸Ð· ÑÑÐ°
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#benchmark"], "emoji": "âï¸", "ru": {"title": "ÐÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½Ð¾Ðµ ÑÐµÐ´Ð°ÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ: Ð¾Ñ ÑÐµÐºÑÑÐ° Ðº Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÐ¼ Ð¸Ð½ÑÑÑÑÐºÑÐ¸ÑÐ¼", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ VIBE â Visual Instruction Benchmark for Image Editing, Ð½Ð¾Ð²ÑÐ¹ Ð±ÐµÐ½ÑÐ¼Ð°ÑÐº Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐ¸ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð²Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐ»Ðµ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#video", "#games", "#benchmark"], "emoji": "ð¬", "ru": {"title": "ÐÐ¾Ð²Ð¾ÑÑÑÐ¸Ðµ Ð°Ð²Ð°ÑÐ°ÑÑ, ÐºÐ¾ÑÐ¾ÑÑÐµ ÑÐµÐ°Ð»ÑÐ½Ð¾ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑÐ²ÑÑÑ Ñ Ð¿ÑÐµÐ´Ð¼ÐµÑÐ°Ð¼Ð¸", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð´Ð²ÑÑÐ¿Ð¾ÑÐ¾ÑÐ½Ð°Ñ Ð°ÑÑÐ¸ÑÐµÐºÑÑÑÐ° InteractAvatar Ð´Ð»Ñ ÑÐ¸Ð½ÑÐµÐ·Ð° Ð²Ð¸Ð´ÐµÐ¾ Ð³Ð¾Ð²Ð¾ÑÑÑÐ¸Ñ Ð°Ð²Ð°ÑÐ°ÑÐ¾Ð², ÐºÐ¾ÑÐ¾ÑÑÐµ Ð¼Ð¾Ð³
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#long_context", "#optimization", "#reasoning"], "emoji": "ð", "ru": {"title": "Ð Ð°Ð·Ð¼ÑÑÐ»ÑÑÑÐ¸Ð¹ Ð°Ð³ÐµÐ½Ñ: Ð³Ð»Ð¾Ð±Ð°Ð»ÑÐ½Ð¾Ðµ Ð¿Ð»Ð°Ð½Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ ÑÐµÑÐµÐ· ÐºÑÐ¾ÑÑ-ÑÑÐ°ÐµÐºÑÐ¾ÑÐ½ÑÑ ÑÐµÑÐ»ÐµÐºÑÐ¸Ñ", "desc": "Re-TRAC â ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ Ð°Ð³ÐµÐ½ÑÐ¾Ð² Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð±Ð¾Ð»ÑÑÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð¿ÑÐµÐ¾Ð´Ð¾Ð»ÐµÐ²Ð°ÐµÑ Ð¾Ð³ÑÐ°Ð½Ð¸ÑÐµÐ½Ð¸Ñ Ð»Ð¸Ð½ÐµÐ¹Ð½Ð¾Ð³Ð¾ 
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#cv", "#agents", "#benchmark", "#small_models"], "emoji": "ð±", "ru": {"title": "ÐÑ Ð¿Ð¸ÐºÑÐµÐ»ÐµÐ¹ Ðº ÐºÐ¾Ð´Ñ: Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÐµ Ð¼Ð¸ÑÐ¾Ð²ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ÑÐµÑÐµÐ· Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ñ ÑÐµÐ½Ð´ÐµÑÐ¸ÑÑÐµÐ¼Ð¾Ð³Ð¾ Ð²ÐµÐ±-ÐºÐ¾Ð´Ð°", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¼Ð¸ÑÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð¼Ð¾Ð±Ð¸Ð»ÑÐ½ÑÑ GUI
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "â¡", "ru": {"title": "Ð¡ÑÐ°Ð±Ð¸Ð»ÑÐ½Ð¾Ðµ ÑÐ°ÑÑÐ¸ÑÐµÐ½Ð¸Ðµ ÑÐ¸ÑÐ¸Ð½Ñ ÑÐµÑÐ¸: Ð±Ð°Ð»Ð°Ð½Ñ ÑÐ¾ÑÑÐ°Ð½ÐµÐ½Ð¸Ñ ÑÐ¸Ð³Ð½Ð°Ð»Ð° Ð¸ Ð½Ð°ÑÑÑÐµÐ½Ð¸Ñ ÑÐ¸Ð¼Ð¼ÐµÑÑÐ¸Ð¸", "desc": "SPARKLING â ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ ÑÐ°ÑÑÐ¸ÑÐµÐ½Ð¸Ñ ÑÐ¸ÑÐ¸Ð½Ñ Ð½ÐµÐ¹ÑÐ¾ÑÐµÑÐµÐ¹ Ð½Ð° Ð¿ÑÐ¾Ð¼ÐµÐ¶ÑÑÐ¾ÑÐ½ÑÑ ÑÑÐ°Ð¿Ð°Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ, ÐºÐ¾ÑÐ¾ÑÑÐ¹ ÑÐ¾ÑÑÐ°Ð½ÑÐµÑ 
[03.02.2026 20:29] Using data from previous issue: {"categories": [], "emoji": "âï¸", "ru": {"title": "ÐÐ°Ð»Ð°Ð½Ñ Ð¼ÐµÐ¶Ð´Ñ ÐºÐ¾Ð½ÑÑÐ¾Ð»ÐµÐ¼ Ð¸ ÐºÐ°ÑÐµÑÑÐ²Ð¾Ð¼: ÐµÐ´Ð¸Ð½Ð°Ñ ÑÐµÐ¾ÑÐ¸Ñ ÑÐ¿ÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ LLM", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¾Ð±ÑÐµÐ´Ð¸Ð½ÑÐµÑ ÑÐ°Ð·Ð»Ð¸ÑÐ½ÑÐµ Ð¼ÐµÑÐ¾Ð´Ñ ÐºÐ¾Ð½ÑÑÐ¾Ð»Ñ Ð½Ð°Ð´ Ð±Ð¾Ð»ÑÑÐ¸Ð¼Ð¸ ÑÐ·ÑÐºÐ¾Ð²ÑÐ¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸ (LLM) - Ð¾Ñ Ð´Ð¾Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð²ÐµÑÐ¾Ð² Ð´Ð¾ LoRA Ð¸ Ð°ÐºÑÐ¸Ð²Ð°ÑÐ¸Ð¾Ð½Ð½ÑÑ Ð²Ð¼ÐµÑÐ°ÑÐµÐ»ÑÑÑÐ² - Ð² ÐµÐ´Ð¸Ð½ÑÑ ÐºÐ¾Ð½ÑÐµÐ¿ÑÑÐ°Ð»ÑÐ½ÑÑ 
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#reasoning", "#optimization", "#rl", "#training"], "emoji": "ð§ ", "ru": {"title": "Ð¡ÐºÑÑÑÐ¾Ðµ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð±Ð¾Ð»ÐµÐµ ÑÐ¼Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹", "desc": "LatentMorph Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸Ð· ÑÐµÐºÑÑÐ°, Ð¸Ð½ÑÐµÐ³ÑÐ¸ÑÑÑ Ð½ÐµÑÐ²Ð½Ð¾Ðµ Ð»Ð°ÑÐµÐ½ÑÐ½Ð¾Ðµ ÑÐ°ÑÑÑÐ¶Ð´Ðµ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#science", "#benchmark", "#low_resource", "#open_source"], "emoji": "ð", "ru": {"title": "Ð¯Ð·ÑÐºÐ¾Ð²Ð¾Ð¹ Ð±Ð°ÑÑÐµÑ: Ð¿Ð¾ÑÐµÐ¼Ñ LLM Ð±Ð¾ÑÑÑÑÑ Ñ ÑÐ¿Ð¾Ð½ÑÐºÐ¸Ð¼ ÑÐ¸Ð½Ð°Ð½ÑÐ¾Ð²ÑÐ¼ ÑÐµÐºÑÑÐ¾Ð¼", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½ ÑÑÐ°Ð»Ð¾Ð½Ð½ÑÐ¹ Ð½Ð°Ð±Ð¾Ñ Ebisu Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ ÑÐ¿Ð¾Ð½ÑÐºÐ¾Ð³Ð¾ ÑÐ¸Ð½Ð°Ð½ÑÐ¾Ð²Ð¾Ð³Ð¾ Ñ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#architecture", "#cv", "#benchmark", "#reasoning", "#optimization", "#open_source", "#small_models"], "emoji": "ð", "ru": {"title": "Ð­ÑÑÐµÐºÑÐ¸Ð²Ð½ÑÐµ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ ÑÐµÑÐµÐ· Ð°Ð´Ð°Ð¿ÑÐ¸Ð²Ð½ÑÑ ÑÐµÐºÑÑÑÐ¸Ñ Ð²Ð¼ÐµÑÑÐ¾ ÑÐ°ÑÑÐ¸ÑÐµÐ½Ð¸Ñ ÑÐµÑÐ¸", "desc": "Loop-ViT Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ ÑÐµÐºÑÑÑÐ¸Ð²Ð½ÑÑ Ð°ÑÑÐ¸ÑÐµÐºÑÑÑÑ ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼ÐµÑÐ° Ð´Ð»Ñ Ð·ÑÐµ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#interpretability", "#architecture", "#training"], "emoji": "ð§©", "ru": {"title": "ÐÐ¾Ð»Ð¸Ð½Ð¾Ð¼Ð¸Ð°Ð»ÑÐ½Ð¾Ðµ Ð´ÐµÐºÐ¾Ð´Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ Ð¸Ð½ÑÐµÑÐ¿ÑÐµÑÐ°ÑÐ¸Ð¸ ÐºÐ¾Ð¼Ð¿Ð¾Ð·Ð¸ÑÐ¸Ð¾Ð½Ð½ÑÑ ÑÑÑÑÐºÑÑÑ Ð² Ð½ÐµÐ¹ÑÐ¾ÑÐµÑÑÑ", "desc": "PolySAE ÑÐ»ÑÑÑÐ°ÐµÑ ÑÐ°Ð·ÑÐµÐ¶ÐµÐ½Ð½ÑÐµ Ð°Ð²ÑÐ¾ÐºÐ¾Ð´Ð¸ÑÐ¾Ð²ÑÐ¸ÐºÐ¸ (SAE) Ð¿ÑÑÑÐ¼ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»Ð¸Ð½Ð¾Ð¼Ð¸Ð°Ð»ÑÐ½Ð¾Ð³Ð¾ Ð´ÐµÐºÐ¾Ð´ÐµÑÐ°, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð¿Ð¾Ð·Ð²
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset"], "emoji": "ð¤", "ru": {"title": "ÐÑÐµÐ½ÐºÐ° ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐ¸ AI Ð°Ð³ÐµÐ½ÑÐ¾Ð² ÑÐµÑÐ°ÑÑ ÑÐµÐ°Ð»ÑÐ½ÑÐµ Ð¿Ð¾Ð²ÑÐµÐ´Ð½ÐµÐ²Ð½ÑÐµ Ð·Ð°Ð´Ð°ÑÐ¸", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½ Ð±ÐµÐ½ÑÐ¼Ð°ÑÐº AgentIF-OneDay Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐ¸ AI Ð°Ð³ÐµÐ½ÑÐ¾Ð² Ð²ÑÐ¿Ð¾Ð»Ð½ÑÑÑ ÑÐ°Ð·Ð½Ð¾Ð¾Ð±ÑÐ°Ð·Ð½ÑÐµ Ð¿Ð¾Ð²ÑÐµÐ´Ð½ÐµÐ²Ð½ÑÐµ Ð·Ð°Ð´Ð°ÑÐ¸, Ð¿Ð¾Ð»ÑÑÐ°ÐµÐ¼ÑÐµ ÑÐµÑ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#video", "#cv", "#benchmark", "#reasoning", "#long_context"], "emoji": "ð¨", "ru": {"title": "ÐÐ¾Ð¼Ð¸ÐºÑÑ ÐºÐ°Ðº Ð¾Ð¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½ÑÐ¹ Ð¼Ð¾ÑÑ Ð¼ÐµÐ¶Ð´Ñ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð»Ñ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½Ð¾Ð³Ð¾ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½Ð¾Ð¼Ñ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark", "#reasoning", "#optimization", "#rl", "#long_context"], "emoji": "âï¸", "ru": {"title": "ÐÐ»Ð¸Ð½Ð½ÑÐµ Ð³Ð¾ÑÐ¸Ð·Ð¾Ð½ÑÑ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°: Ð¾Ð±ÑÑÐµÐ½Ð¸Ðµ Ð°Ð³ÐµÐ½ÑÐ¾Ð² Ð´Ð»Ñ Ð½Ð°Ð´ÑÐ¶Ð½Ð¾Ð³Ð¾ Ð¿Ð»Ð°Ð½Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð¿ÑÑÐµÑÐµÑÑÐ²Ð¸Ð¹", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½ TRIP-Bench â ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½ÑÐ¹ Ð±ÐµÐ½ÑÐ¼Ð°ÑÐº Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#synthetic", "#open_source", "#reasoning"], "emoji": "ð¯", "ru": {"title": "ÐÐµÐ½ÐµÑÐ°ÑÐ¸Ñ ÐºÐ¾Ð½ÐºÑÑÑÐ½ÑÑ Ð·Ð°Ð´Ð°Ñ Ñ ÑÐ¾ÑÐ½ÑÐ¼ ÐºÐ¾Ð½ÑÑÐ¾Ð»ÐµÐ¼ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑÐ¸ Ð´Ð»Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ð¹", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð½Ð¾Ð²ÑÐ¹ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº CoDiQ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð·Ð°Ð´Ð°Ñ Ñ ÐºÐ¾Ð½ÑÑÐ¾Ð»Ð¸ÑÑÐµÐ¼Ð¾Ð¹ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑÑÑ Ð½Ð° ÑÑÐ¾Ð²Ð½Ðµ ÑÐ¾ÑÐµÐ²Ð½Ð¾Ð²Ð°
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#rl", "#rlhf", "#benchmark"], "emoji": "ð", "ru": {"title": "ÐÐ¸Ð½Ð°Ð¼Ð¸ÑÐµÑÐºÐ¸Ðµ ÐºÑÐ¸ÑÐµÑÐ¸Ð¸ Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ ÑÐ²Ð¾ÑÑÐµÑÐºÐ¸Ñ Ð¾ÑÐ²ÐµÑÐ¾Ð² ÑÐµÑÐµÐ· Ð¾Ð±ÑÑÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "Rubric-ARM â ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº, ÐºÐ¾ÑÐ¾ÑÑÐ¹ ÑÐ¾Ð²Ð¼ÐµÑÑÐ½Ð¾ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð¸ÑÑÐµÑ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ñ ÐºÑÐ¸ÑÐµÑÐ¸ÐµÐ² Ð¾ÑÐµÐ½ÐºÐ¸ Ð¸ ÑÑÐ´ÑÑ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#rl", "#rlhf", "#training"], "emoji": "ð¨", "ru": {"title": "ÐÑÑÑÑ ÑÐ·ÑÐºÐ¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»Ñ ÑÐ»ÑÑÑÐ°ÐµÑ Ð¿Ð¾Ð´ÑÐºÐ°Ð·ÐºÐ¸ Ð² Ð¾Ð±ÑÑÐµÐ½Ð¸Ð¸ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð° PromptRL â ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº, ÐºÐ¾ÑÐ¾ÑÑÐ¹ ÑÐ»ÑÑÑÐ°ÐµÑ Ð¼Ð¾Ð´ÐµÐ»Ð¸ flow matching Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ ÑÐµÐºÑ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#transfer_learning", "#interpretability", "#training"], "emoji": "ð§ ", "ru": {"title": "ÐÐ½ÑÑÑÐµÐ½Ð½ÑÑ ÑÐ¸ÑÑÐµÐ¼Ð° Ð²Ð¾Ð·Ð½Ð°Ð³ÑÐ°Ð¶Ð´ÐµÐ½Ð¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐ°ÑÐºÑÑÑÐ°", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»Ð¸ Ð¾Ð±Ð½Ð°ÑÑÐ¶Ð¸Ð»Ð¸ ÑÐ°Ð·ÑÐµÐ¶ÐµÐ½Ð½ÑÑ Ð¿Ð¾Ð´ÑÐ¸ÑÑÐµÐ¼Ñ Ð²Ð¾Ð·Ð½Ð°Ð³ÑÐ°Ð¶Ð´ÐµÐ½Ð¸Ñ Ð² ÑÐºÑÑÑÑÑ ÑÐ¾ÑÑÐ¾ÑÐ½Ð¸ÑÑ Ð±Ð¾Ð»ÑÑÑÑ ÑÐ·
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#optimization", "#rl", "#training", "#reasoning"], "emoji": "ð§©", "ru": {"title": "Ð Ð°Ð·Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑÐ¸: Ð¾Ñ Ð°Ð±ÑÑÑÐ°ÐºÑÐ½ÑÑ Ð²Ð¾Ð¿ÑÐ¾ÑÐ¾Ð² Ðº Ð¿Ð¾ÑÐ°Ð³Ð¾Ð²ÑÐ¼ ÑÐµÑÐµÐ½Ð¸ÑÐ¼", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑÐ¾Ð´ Adaptive Ability Decomposing (AÂ²D) Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿ÑÐ¾Ð²ÐµÑÑÐµÐ¼ÑÑ Ð²Ð¾
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#inference", "#rl", "#agents"], "emoji": "âï¸", "ru": {"title": "ÐÐ¾Ð¼Ð¿ÑÐ¾Ð¼Ð¸ÑÑÑ ÐºÐ²Ð°Ð½ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð² Ð¼Ð¾Ð´ÐµÐ»ÑÑ Ð¼Ð¸ÑÐ°: Ð¾Ñ ÑÐ¾ÑÐ½Ð¾ÑÑÐ¸ Ðº ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾ÑÑÐ¸", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¸ÑÑÐ»ÐµÐ´ÑÐµÑÑÑ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ Ð¿Ð¾ÑÑ-ÑÑÐµÐ½Ð¸ÑÐ¾Ð²Ð¾ÑÐ½Ð¾Ð³Ð¾ ÐºÐ²Ð°Ð½ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð½Ð° world models, ÐºÐ¾ÑÐ¾ÑÑÐµ Ð¾Ð±ÑÑÐ°ÑÑÑÑ ÐºÐ¾Ð¼Ð¿Ð°ÐºÑÐ½Ð¾ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÑÑ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÑ Ð¾ÐºÑÑÐ¶ÐµÐ½Ð¸Ñ Ð´Ð»
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset"], "emoji": "ð", "ru": {"title": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»ÑÑÐºÐ¸Ð¹ Ð¸Ð½ÑÐµÐ»Ð»ÐµÐºÑ: Ð¾Ñ Ð²ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´ Ðº Ð°Ð²ÑÐ¾Ð½Ð¾Ð¼Ð½Ð¾Ð¼Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ñ Ð´Ð°Ð½Ð½ÑÑ", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ ÐºÐ¾Ð½ÑÐµÐ¿ÑÐ¸Ñ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»ÑÑÐºÐ¾Ð³Ð¾ Ð¸Ð½ÑÐµÐ»Ð»ÐµÐºÑÐ° Ð´Ð»Ñ Ð°Ð³ÐµÐ½ÑÐ¸Ð²Ð½ÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾ÑÐ¾ÑÑÐµ Ð´Ð¾Ð»Ð¶Ð½Ñ ÑÐ°Ð¼Ð¾ÑÑÐ¾ÑÑÐµÐ»ÑÐ½Ð¾ ÑÑ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#benchmark", "#training", "#small_models"], "emoji": "ð", "ru": {"title": "ÐÑÐµÐ½ÐºÐ° Ð±ÐµÐ· Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸: ÑÐºÑÑÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¼Ð°Ð»ÐµÐ½ÑÐºÐ¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð²Ð¼ÐµÑÑÐ¾ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÑÐ´ÐµÐ¹", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑÐ²Ð°ÐµÑ, ÑÑÐ¾ Ð¼Ð°Ð»ÐµÐ½ÑÐºÐ¸Ðµ ÑÐ·ÑÐºÐ¾Ð²ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¼Ð¾Ð³ÑÑ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾ Ð¾ÑÐµÐ½Ð¸Ð²Ð°ÑÑ ÐºÐ°ÑÐµÑÑÐ²Ð¾ Ð¾ÑÐ²ÐµÑÐ¾Ð², Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÑ Ð²Ð½ÑÑ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#leakage", "#benchmark", "#reasoning"], "emoji": "ð¼ï¸", "ru": {"title": "ÐÐ°ÑÑÑÐ°Ð±Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ ÑÐ°Ð·Ð´ÐµÐ»Ð¸ÑÐµÐ»ÐµÐ¹ Ð´Ð»Ñ ÑÑÑÑÐ°Ð½ÐµÐ½Ð¸Ñ ÑÑÐµÑÐºÐ¸ Ð¸Ð½ÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¸ Ð¼ÐµÐ¶Ð´Ñ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð²ÑÑÐ²Ð»ÑÐµÑ Ð¿ÑÐ¾Ð±Ð»ÐµÐ¼Ñ ÑÑÐµÑÐºÐ¸ Ð¸Ð½ÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¸ Ð¼ÐµÐ¶Ð´Ñ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð² Ð±Ð¾Ð»ÑÑÐ¸Ñ Ð²Ð¸Ð´ÐµÐ¾-Ñ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#inference", "#benchmark", "#reasoning", "#diffusion", "#optimization", "#open_source", "#training"], "emoji": "ð³", "ru": {"title": "Ð£Ð¼Ð½Ð¾Ðµ Ð²ÐµÑÐ²Ð»ÐµÐ½Ð¸Ðµ: ÑÑÑÐµÐºÑÐ¸Ð²Ð½ÑÐ¹ Ð¸Ð½ÑÐµÑÑ Ð´Ð¸ÑÐºÑÐµÑÐ½ÑÑ Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐµÑÐµÐ· Ð¸ÐµÑÐ°ÑÑÐ¸ÑÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ð¸ÑÐº", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Prism Ð´Ð»Ñ Ð¼Ð°ÑÑÑÐ°
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#3d", "#cv"], "emoji": "ð", "ru": {"title": "Ð¡Ð¾Ð²Ð¼ÐµÑÑÐ½Ð°Ñ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ñ Ð»Ð¸Ð½Ð¸Ð¹ Ð¸ Ð¿Ð»Ð¾ÑÐºÐ¾ÑÑÐµÐ¹ Ð´Ð»Ñ ÑÑÑÑÐºÑÑÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ 3D-Ð¾ÑÐ¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ñ", "desc": "LiP-Map Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ ÑÐ¾Ð±Ð¾Ð¹ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº ÑÐ¾Ð²Ð¼ÐµÑÑÐ½Ð¾Ð¹ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ð¸ Ð»Ð¸Ð½Ð¸Ð¹ Ð¸ Ð¿Ð»Ð¾ÑÐºÐ¾ÑÑÐµÐ¹ Ð´Ð»Ñ Ð²Ð¾ÑÑÑÐ°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ ÑÑÑÑÐ¼ÐµÑÐ½ÑÑ Ð»Ð¸Ð½Ð¸Ð¹ Ð¸Ð· Ð¼Ð½Ð¾Ð³Ð¾Ð²Ð¸Ð´Ð¾Ð²ÑÑ RGB-Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹.
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#video", "#architecture", "#inference", "#diffusion", "#optimization"], "emoji": "â¡", "ru": {"title": "Ð¢Ð¾ÑÐ½Ð¾Ðµ Ð²ÑÑÐ¸ÑÐ»ÐµÐ½Ð¸Ðµ Ð²Ð°Ð¶Ð½Ð¾Ð³Ð¾, Ð¿ÑÐ¸Ð±Ð»Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ð¾ÑÑÐ°Ð»ÑÐ½Ð¾Ð³Ð¾: ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ðµ ÑÐ°Ð·ÑÐµÐ¶ÐµÐ½Ð½Ð¾Ðµ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑÐ¾Ð´ PISA, ÐºÐ¾ÑÐ¾ÑÑÐ¹ ÑÐ»ÑÑÑÐ°ÐµÑ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾ÑÑÑ Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multimodal", "#inference", "#video"], "emoji": "â¡", "ru": {"title": "Ð£ÑÐºÐ¾ÑÐµÐ½Ð¸Ðµ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐµÑÐµÐ· ÑÐ¼Ð½Ð¾Ðµ Ð¿ÑÐ¾ÑÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÑ ÑÐ¾ÐºÐµÐ½Ð¾Ð²", "desc": "VisionTrim Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð±ÐµÐ· Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð´Ð»Ñ ÑÑÐºÐ¾ÑÐµÐ½Ð¸Ñ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿ÑÑÑÐ¼ Ð²ÑÐ±Ð¾ÑÐ° Ð´Ð¾Ð¼Ð¸Ð½Ð°Ð½ÑÐ½
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#inference", "#benchmark", "#reasoning", "#optimization", "#training"], "emoji": "âï¸", "ru": {"title": "ÐÑÐµÐ´ÐµÐ»Ñ ÑÐ¶Ð°ÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹: Ð¿Ð¾ÑÐµÐ¼Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð²Ð½ÑÐ¹ Ð¸Ð½ÑÐµÐ»Ð»ÐµÐºÑ ÑÑÑÐ°Ð´Ð°ÐµÑ Ð±Ð¾Ð»ÑÑÐµ, ÑÐµÐ¼ ÐºÐ»Ð°ÑÑÐ¸ÑÐ¸ÐºÐ°ÑÐ¸Ñ", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¸ÑÑÐ»ÐµÐ´ÑÐµÑÑÑ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ ÑÐ´Ð°Ð»ÐµÐ½Ð¸Ñ ÑÐ»Ð¾ÑÐ² Ð½Ð° Ð±Ð¾Ð»ÑÑÐ¸Ðµ ÑÐ·ÑÐºÐ¾Ð²ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
[03.02.2026 20:29] Using data from previous issue: {"categories": [], "emoji": "ð ï¸", "ru": {"title": "ÐÐ³ÐµÐ½Ñ, ÑÐ¾Ð·Ð´Ð°ÑÑÐ¸Ð¹ ÑÐ²Ð¾Ð¸ ÑÐ¾Ð±ÑÑÐ²ÐµÐ½Ð½ÑÐµ Ð¸Ð½ÑÑÑÑÐ¼ÐµÐ½ÑÑ Ð² Ð¿ÑÐ¾ÑÐµÑÑÐµ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð°æ¡æ¶UCT, ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð¿ÑÐµÐ²ÑÐ°ÑÐ°ÐµÑ ÑÐ·ÑÐºÐ¾Ð²ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸Ð· Ð¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°ÑÐµÐ»ÐµÐ¹ Ð¸Ð½ÑÑÑÑÐ¼ÐµÐ½ÑÐ¾Ð² Ð² Ð¸Ñ ÑÐ¾Ð·Ð´Ð°ÑÐµÐ»ÐµÐ¹. Ð¡Ð¸ÑÑÐµÐ¼Ð° Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸ÑÐµÑÐºÐ¸ Ð³ÐµÐ½ÐµÑÐ¸ÑÑÐµÑ Ð¸ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð¸ÑÑÐµÑ ÑÐ¿ÐµÑÐ¸Ð°Ð»Ð¸Ð·Ð¸ÑÐ¾Ð²
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#optimization"], "emoji": "ð", "ru": {"title": "ÐÐ°Ð½Ð¸ÑÐ¾Ð»ÑÐ´Ð½Ð°Ñ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ñ Ñ Ð¿ÑÐ¾ÐµÐºÑÐ¸ÐµÐ¹ Ð¼Ð¾Ð¼ÐµÐ½ÑÐ° Ð´Ð»Ñ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "ÐÐ²ÑÐ¾ÑÑ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÑÑ Ð½Ð¾Ð²ÑÐ¹ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¾Ñ Mano, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð¿ÑÐ¸Ð¼ÐµÐ½ÑÐµÑ Ð¼ÐµÑÐ¾Ð´Ñ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ð¸ Ð½Ð° Ð¼Ð½Ð¾Ð³Ð¾Ð¾Ð±ÑÐ°Ð·Ð¸ÑÑ Ñ Ð¿ÑÐ¾ÐµÐºÑÐ¸ÐµÐ¹ Ð¼Ð¾Ð¼ÐµÐ½ÑÐ° Ð½Ð° ÐºÐ°ÑÐ°ÑÐµÐ»ÑÐ½
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#architecture", "#cv", "#benchmark", "#interpretability", "#training"], "emoji": "ð", "ru": {"title": "Ð­ÑÑÐµÐºÑÐ¸Ð²Ð½Ð°Ñ ÑÐ°Ð·Ð¼ÐµÑÐ½Ð¾ÑÑÑ ÐºÐ°Ðº ÑÐ½Ð¸Ð²ÐµÑÑÐ°Ð»ÑÐ½ÑÐ¹ Ð¿ÑÐµÐ´Ð¸ÐºÑÐ¾Ñ ÐºÐ°ÑÐµÑÑÐ²Ð° Ð½ÐµÐ¹ÑÐ¾Ð½Ð½ÑÑ ÑÐµÑÐµÐ¹", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¸ÑÑÐ»ÐµÐ´ÑÐµÑÑÑ ÑÐ²ÑÐ·Ñ Ð¼ÐµÐ¶Ð´Ñ Ð³ÐµÐ¾Ð¼ÐµÑÑÐ¸ÐµÐ¹ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ð¹ Ð¸ Ð¿ÑÐ¾Ð¸Ð·Ð²Ð¾Ð´Ð¸ÑÐµÐ»ÑÐ½Ð¾ÑÑÑÑ Ð½ÐµÐ¹ÑÐ¾Ð½Ð½ÑÑ ÑÐµ
[03.02.2026 20:29] Using data from previous issue: {"categories": [], "emoji": "ð¨", "ru": {"title": "ÐÐµÐ¿ÑÐµÑÑÐ²Ð½ÑÐµ Ð½ÐµÐ¹ÑÐ¾Ð½Ð½ÑÐµ ÑÐµÐºÑÑÑÑÑ Ð´Ð»Ñ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ ÑÐµÐ½Ð´ÐµÑÐ¸Ð½Ð³Ð°", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°ÑÑ Ð½ÐµÑÐ²Ð½ÑÐµ Ð½ÐµÐ¹ÑÐ¾Ð½Ð½ÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ (INR) Ð´Ð»Ñ ÑÐµÐºÑÑÑÑ, ÐºÐ¾ÑÐ¾ÑÑÐµ ÑÐ°Ð±Ð¾ÑÐ°ÑÑ Ð² Ð½ÐµÐ¿ÑÐµÑÑÐ²Ð½Ð¾Ð¼ Ð¿ÑÐ¾ÑÑÑÐ°Ð½ÑÑÐ²Ðµ UV-ÐºÐ¾Ð¾ÑÐ´Ð¸Ð½Ð°Ñ Ð²Ð¼ÐµÑÑÐ¾ Ð´Ð¸ÑÐºÑÐµÑÐ½Ð¾Ð³Ð¾ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ. ÐÐ²Ñ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multilingual", "#machine_translation", "#dataset", "#benchmark", "#low_resource", "#data", "#open_source"], "emoji": "â ï¸", "ru": {"title": "ÐÐ¾Ð³Ð´Ð° ÑÑÐ´ÑÐ¸ Ð¾ÑÐ¸Ð±Ð°ÑÑÑÑ: Ð½ÐµÑÑÐ°Ð±Ð¸Ð»ÑÐ½Ð¾ÑÑÑ ÐºÑÐ¾ÑÑ-Ð»Ð¸Ð½Ð³Ð²Ð°Ð»ÑÐ½Ð¾Ð¹ Ð¾ÑÐµÐ½ÐºÐ¸ LLM Ð½Ð° Ð¼Ð¾ÑÑÐ¾Ð»Ð¾Ð³Ð¸ÑÐµÑÐºÐ¸ ÑÐ»Ð¾Ð¶Ð½ÑÑ ÑÐ·ÑÐºÐ°Ñ", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¸ÑÑÐ»ÐµÐ´ÑÐµÑÑÑ Ð½Ð°Ð´ÑÐ¶Ð½Ð¾ÑÑÑ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#rl", "#training", "#small_models"], "emoji": "ð¯", "ru": {"title": "Ð£Ð¼Ð½ÑÐ¹ Ð²ÑÐ±Ð¾Ñ Ð¿Ð¾Ð´ÑÐºÐ°Ð·Ð¾Ðº ÑÐµÑÐµÐ· Ð±Ð°Ð¹ÐµÑÐ¾Ð²ÑÐºÐ¾Ðµ Ð¿ÑÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑÐ¸", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð¼ÐµÑÐ¾Ð´ Generalizable Predictive Prompt Selection (GPS), ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÐµÑ Ð±Ð°Ð¹ÐµÑÐ¾Ð²ÑÐºÐ¸Ð¹ Ð²Ñ
[03.02.2026 20:29] Using data from previous issue: {"categories": [], "emoji": "âï¸", "ru": {"title": "ÐÐ¸Ð°Ð³Ð½Ð¾ÑÑÐ¸ÐºÐ° Ð½Ð°Ð´ÑÐ¶Ð½Ð¾ÑÑÐ¸ LLM-ÑÑÐ´ÐµÐ¹ ÑÐµÑÐµÐ· ÑÐµÐ¾ÑÐ¸Ñ Ð¸Ð·Ð¼ÐµÑÐµÐ½Ð¸Ð¹ Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð· ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑÐ¸", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð´Ð²ÑÑÑÑÐ°Ð¿Ð½Ð°Ñ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑÐ¸ÑÐµÑÐºÐ°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÐµÐ¾ÑÐ¸Ð¸ Ð¾ÑÐ²ÐµÑÐ¾Ð² Ð½Ð° Ð·Ð°Ð´Ð°Ð½Ð¸Ñ Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð³ÑÐ°Ð´ÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÑ Ð¾ÑÐ²ÐµÑÐ¾Ð² Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ Ð½Ð°Ð´ÑÐ¶Ð½Ð¾ÑÑÐ¸ LLM-as-a-
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#reasoning", "#alignment", "#optimization", "#rl", "#rlhf", "#training"], "emoji": "ð¯", "ru": {"title": "ÐÐµÐ· Ð¾ÑÑÐµÑÐµÐ½Ð¸Ð¹ â Ðº ÑÑÐ°Ð±Ð¸Ð»ÑÐ½Ð¾Ð¹ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ð¸ Ð¿Ð¾Ð»Ð¸ÑÐ¸ÐºÐ¸ Ð² LLM", "desc": "ÐÐ²ÑÐ¾ÑÑ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÑÑ Clipping-Free Policy Optimization (CFPO) â Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ Ð´Ð»Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ 
[03.02.2026 20:29] Using data from previous issue: {"categories": [], "emoji": "ð¨", "ru": {"title": "Ð¡Ð¿ÐµÐºÑÑÐ°Ð»ÑÐ½ÑÐµ Ð»Ð¾Ð²ÑÑÐºÐ¸ VAE-Ð¸Ð½Ð¿ÐµÐ¹Ð½ÑÐ¸Ð½Ð³Ð°: Ð¾Ñ Ð¾Ð±Ð½Ð°ÑÑÐ¶ÐµÐ½Ð¸Ñ Ð°ÑÑÐµÑÐ°ÐºÑÐ¾Ð² Ðº ÐºÐ¾Ð½ÑÐµÐ½Ñ-Ð¾ÑÐ¸ÐµÐ½ÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð´ÐµÑÐµÐºÑÐ¸Ð¸", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¸ÑÑÐ»ÐµÐ´ÑÐµÑÑÑ Ð¿ÑÐ¾Ð±Ð»ÐµÐ¼Ð° Ð´ÐµÑÐµÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹, Ð¾ÑÑÐµÐ´Ð°ÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÑ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ VAE-based Ð¸Ð½Ð¿ÐµÐ¹Ð½ÑÐ¸Ð½Ð³Ð°. ÐÐ²ÑÐ¾ÑÑ Ð¾Ð±Ð½Ð°ÑÑÐ¶Ð¸Ð»Ð¸, ÑÑÐ¾ ÑÐ¾Ð²ÑÐµÐ¼ÐµÐ½
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#data", "#dataset", "#synthetic", "#open_source", "#audio"], "emoji": "ðµ", "ru": {"title": "Ð§Ð¸ÑÑÑÐµ Ð´Ð°Ð½Ð½ÑÐµ Ð²Ð¼ÐµÑÑÐ¾ Ð±Ð¾Ð»ÑÑÐ¸Ñ Ð¾Ð±ÑÑÐ¼Ð¾Ð²: ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ðµ ÑÐ°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð·Ð²ÑÐºÐ° ÑÐµÑÐµÐ· Ð²ÑÑÐ¾ÐºÐ°ÑÐµÑÑÐ²ÐµÐ½Ð½ÑÐ¹ ÑÐ¸Ð½ÑÐµÐ·", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸Ð·Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐ¹ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ Ð´Ð»Ñ ÑÐ°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ Ð·Ð²ÑÐºÐ¾Ð²ÑÑ ÑÐ¸Ð³Ð½Ð°Ð»Ð¾Ð², ÐºÐ¾
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#training"], "emoji": "â¡", "ru": {"title": "ÐÐ°ÑÐ°Ð»Ð»ÐµÐ»ÑÐ½Ð°Ñ Ð¾Ð±ÑÐ°Ð±Ð¾ÑÐºÐ° Ð²ÑÐµÐ¼ÐµÐ½Ð¸: ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ Ð·Ð°ÑÑÐ°Ñ Ð±ÐµÐ· Ð¿Ð¾ÑÐµÑÐ¸ ÐºÐ°ÑÐµÑÑÐ²Ð°", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð° Parallel Echo State Network (ParalESN) â Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº reservoir computing, ÑÐµÑÐ°ÑÑÐ¸Ð¹ Ð¿ÑÐ¾Ð±Ð»ÐµÐ¼Ñ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#transfer_learning", "#optimization", "#rl", "#training"], "emoji": "ð§ ", "ru": {"title": "ÐÐµÑÐ±Ð°Ð»ÑÐ½Ð°Ñ Ð´Ð¸ÑÑÐ¸Ð»Ð»ÑÑÐ¸Ñ: Ð¿ÐµÑÐµÐ´Ð°ÑÐ° Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð±ÐµÐ· Ð¾Ð³ÑÐ°Ð½Ð¸ÑÐµÐ½Ð¸Ð¹ ÑÐ¾ÐºÐµÐ½-ÑÑÐ¾Ð²Ð½ÐµÐ²Ð¾Ð³Ð¾ Ð²ÑÑÐ°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ñ", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð¼ÐµÑÐ¾Ð´ On-policy Verbal Distillation (OVD), ÐºÐ¾ÑÐ¾Ñ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multilingual", "#data", "#rl", "#optimization", "#training", "#small_models"], "emoji": "ð¯", "ru": {"title": "Ð£Ð¼Ð½Ð¾Ðµ Ð¿ÐµÑÐµweighting: ÐºÐ¾Ð³Ð´Ð° Ð¼Ð°ÑÐ¸Ð½Ð° Ð²ÑÐ±Ð¸ÑÐ°ÐµÑ, ÐºÐ°ÐºÐ¸Ðµ Ð´Ð°Ð½Ð½ÑÐµ ÑÑÐ¸ÑÑ", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Inf-DDS, ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÐµÑ Ð¾Ð±ÑÑÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ Ð°Ð´Ð°Ð¿ÑÐ¸Ð²
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "â ï¸", "ru": {"title": "Ð£ÑÐ·Ð²Ð¸Ð¼Ð¾ÑÑÑ LLM-ÑÑÐ´ÐµÐ¹ Ð¿ÐµÑÐµÐ´ Ð¼Ð°Ð½Ð¸Ð¿ÑÐ»ÑÑÐ¸ÑÐ¼Ð¸ Ð² ÑÐµÐ¿Ð¾ÑÐºÐ°Ñ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð°Ð³ÐµÐ½ÑÐ¾Ð²", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð½Ð¾, ÑÑÐ¾ Ð±Ð¾Ð»ÑÑÐ¸Ðµ ÑÐ·ÑÐºÐ¾Ð²ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸, Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÐµÐ¼ÑÐµ Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ Ð¿ÑÐ¾Ð¸Ð·Ð²Ð¾Ð´Ð¸ÑÐµÐ»ÑÐ½Ð¾ÑÑÐ¸ Ð°Ð³ÐµÐ½ÑÐ¾Ð², ÑÑÐ·Ð²Ð¸Ð¼Ñ Ð´Ð»Ñ Ð¼Ð°Ð½Ð¸Ð¿ÑÐ»ÑÑÐ¸Ð¹ Ñ ÑÐµÐ¿Ð¾ÑÐºÐ°Ð¼Ð¸ ÑÐ°ÑÑÑÐ¶Ð´
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#architecture", "#training"], "emoji": "ð¯", "ru": {"title": "ÐÑÐµÐ´ÑÐºÐ°Ð·Ð°ÑÐµÐ»ÑÐ½ÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¸ Ð°Ð´Ð°Ð¿ÑÐ¸Ð²Ð½Ð°Ñ ÐºÐ²Ð°Ð½ÑÐ¸Ð·Ð°ÑÐ¸Ñ Ð´Ð»Ñ ÑÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°ÑÐ¸Ð¹", "desc": "ReSID Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð¸Ð½Ð½Ð¾Ð²Ð°ÑÐ¸Ð¾Ð½Ð½ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»ÑÐ½ÑÐ¼ ÑÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°ÑÐ¸ÑÐ¼, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð¾Ð±ÑÑÐ°ÐµÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¿ÑÐµÐ´Ð¼ÐµÑÐ¾Ð², Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐµ Ð´Ð»Ñ Ð¿Ñ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#architecture", "#hallucinations", "#interpretability", "#open_source", "#training"], "emoji": "ð", "ru": {"title": "ÐÐ½ÑÑÑÐµÐ½Ð½ÐµÐµ Ð²Ð¸Ð´ÐµÐ½Ð¸Ðµ LLM: ÑÐ°Ð¼Ð¾Ð¿ÑÐ¾Ð²ÐµÑÐºÐ° ÑÐµÑÐµÐ· Ð°Ð½Ð°Ð»Ð¸Ð· Ð³Ð»ÑÐ±Ð¸Ð½Ð½Ð¾Ð¹ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ Ð°ÐºÑÐ¸Ð²Ð°ÑÐ¸Ð¹", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑÐ¾Ð´ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð²Ð½ÑÑÑÐµÐ½Ð½ÐµÐ¹ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»Ðµ
[03.02.2026 20:29] Using data from previous issue: {"categories": [], "emoji": "ð§¬", "ru": {"title": "ÐÐ°ÑÑÐ½Ð°Ñ ÐÐÐ Ð°Ð³ÐµÐ½ÑÐ¾Ð²: Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑÐ°Ð»Ð¸Ð·Ð°ÑÐ¸Ñ Ð´Ð»Ñ Ð¾ÑÐºÑÑÑÐ¸Ð¹ Ð² Ð¼Ð¾Ð»ÐµÐºÑÐ»ÑÑÐ½Ð¾Ð¹ ÑÐ¸Ð¼Ð¸Ð¸", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° ÑÐ¸ÑÑÐµÐ¼Ð° INDIBATOR Ð´Ð»Ñ Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸Ð·Ð°ÑÐ¸Ð¸ Ð¼Ð¾Ð»ÐµÐºÑÐ»ÑÑÐ½ÑÑ Ð¾ÑÐºÑÑÑÐ¸Ð¹, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð½Ð° Ð¼Ð½Ð¾Ð³Ð¾Ð°Ð³ÐµÐ½ÑÐ½Ð¾Ð¼ Ð¿Ð¾Ð´ÑÐ¾Ð´Ðµ. ÐÐ³ÐµÐ½ÑÑ Ð² ÑÐ¸ÑÑÐµÐ¼Ðµ ÑÐ°ÑÐ°ÐºÑÐµÑÐ¸Ð·ÑÑÑÑÑ Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑÐ°Ð»ÑÐ½ÑÐ¼Ð¸ Ð¿Ñ
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#agents", "#benchmark", "#low_resource", "#alignment", "#synthetic", "#open_source"], "emoji": "ð", "ru": {"title": "ÐÑÐ»ÑÑÑÑÐ½Ð¾-Ð°Ð´Ð°Ð¿ÑÐ¸Ð²Ð½ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑÐ¸ Ð´Ð»Ñ Ð®Ð³Ð¾-ÐÐ¾ÑÑÐ¾ÑÐ½Ð¾Ð¹ ÐÐ·Ð¸Ð¸", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»Ð¸ ÑÐ°Ð·ÑÐ°Ð±Ð¾ÑÐ°Ð»Ð¸ Ð°Ð³ÐµÐ½ÑÐ½ÑÐ¹ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸ÑÐµÑÐºÐ¾
[03.02.2026 20:29] Using data from previous issue: {"categories": ["#optimization", "#cv", "#multimodal", "#architecture"], "emoji": "ð", "ru": {"title": "ÐÐ°ÑÐ°Ð±Ð¾Ð»Ð¸ÑÐµÑÐºÐ¾Ðµ ÐºÐ¾Ð´Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾Ð·Ð¸ÑÐ¸Ð¹ Ð´Ð»Ñ ÑÐ½Ð¸Ð²ÐµÑÑÐ°Ð»ÑÐ½ÑÑ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÑ ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼ÐµÑÐ¾Ð²", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ ÐºÐ¾Ð´Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ð¾Ð·Ð¸ÑÐ¸Ð¹ (PaPE) Ð´Ð»Ñ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÑ Ð¼Ð¾Ð´Ð°Ð»ÑÐ½Ð¾ÑÑÐµÐ¹, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½ÑÐ¹ Ð½Ð° Ð¿Ð°Ñ
[03.02.2026 20:29] Querying the API.
[03.02.2026 20:29] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

YOLOE-26 integrates YOLO26 architecture with open-vocabulary learning for real-time instance segmentation, utilizing convolutional backbones, end-to-end regression, and object embedding heads with text and visual prompting capabilities.  					AI-generated summary 				 This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.
[03.02.2026 20:29] Response: ```json
{
  "desc": "YOLOE-26 Ð¾Ð±ÑÐµÐ´Ð¸Ð½ÑÐµÑ Ð°ÑÑÐ¸ÑÐµÐºÑÑÑÑ YOLO26 Ñ Ð¿Ð°ÑÐ°Ð´Ð¸Ð³Ð¼Ð¾Ð¹ Ð¾ÑÐºÑÑÑÐ¾Ð¹ Ð»ÐµÐºÑÐ¸ÐºÐ¸ Ð´Ð»Ñ ÑÐµÐ³Ð¼ÐµÐ½ÑÐ°ÑÐ¸Ð¸ ÑÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑÐ¾Ð² Ð² ÑÐµÐ°Ð»ÑÐ½Ð¾Ð¼ Ð²ÑÐµÐ¼ÐµÐ½Ð¸. ÐÐ¾Ð´ÐµÐ»Ñ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÐµÑ ÑÐ²ÐµÑÑÐ¾ÑÐ½ÑÐ¹ Ð¾ÑÑÐ¾Ð² Ñ Ð¼Ð½Ð¾Ð³Ð¾Ð¼Ð°ÑÑÑÐ°Ð±Ð½Ð¾Ð¹ Ð°Ð³ÑÐµÐ³Ð°ÑÐ¸ÐµÐ¹ Ð¿ÑÐ¸Ð·Ð½Ð°ÐºÐ¾Ð² Ð¸ Ð·Ð°Ð¼ÐµÐ½ÑÐµÑ ÑÐ¸ÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐµ Ð»Ð¾Ð³Ð¸ÑÑ ÐºÐ»Ð°ÑÑÐ¾Ð² Ð½Ð° Ð³Ð¾Ð»Ð¾Ð²Ñ Ð²ÑÑÑÐ°Ð¸Ð²Ð°Ð½Ð¸Ñ Ð¾Ð±ÑÐµÐºÑÐ¾Ð², ÐºÐ¾ÑÐ¾ÑÐ°Ñ ÑÐ¾ÑÐ¼ÑÐ»Ð¸ÑÑÐµÑ ÐºÐ»Ð°ÑÑÐ¸ÑÐ¸ÐºÐ°ÑÐ¸Ñ ÐºÐ°Ðº ÑÐ¾Ð¿Ð¾ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð´Ð¾Ð±Ð¸Ñ Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð°Ð¼Ð¸ Ð¿Ð¾Ð´ÑÐºÐ°Ð·Ð¾Ðº ÑÐµÐºÑÑÐ° Ð¸Ð»Ð¸ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÑ Ð¿ÑÐ¸Ð¼ÐµÑÐ¾Ð². ÐÑÑÐ¸ÑÐµÐºÑÑÑÐ° Ð²ÐºÐ»ÑÑÐ°ÐµÑ Ð¿ÐµÑÐµÐ¿Ð°ÑÐ°Ð¼ÐµÑÑÐ¸Ð·ÑÐµÐ¼Ð¾Ðµ Ð²ÑÑÐ°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð¾Ð±Ð»Ð°ÑÑÑ-ÑÐµÐºÑÑ Ð¸ ÐºÐ¾Ð´Ð¸ÑÐ¾Ð²ÑÐ¸Ðº ÑÐµÐ¼Ð°Ð½ÑÐ¸ÑÐµÑÐºÐ¸ Ð°ÐºÑÐ¸Ð²Ð¸ÑÐ¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½Ð¾Ð¹ Ð¿Ð¾Ð´ÑÐºÐ°Ð·ÐºÐ¸ Ð´Ð»Ñ ÑÐ°Ð·Ð»Ð¸ÑÐ½ÑÑ ÑÐµÐ¶Ð¸Ð¼Ð¾Ð² Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑÐ²Ð¸Ñ. Ð¡Ð¸ÑÑÐµÐ¼Ð° Ð´ÐµÐ¼Ð¾Ð½ÑÑÑÐ¸ÑÑÐµÑ ÑÑÑÐµÐºÑÐ¸Ð²Ð½ÑÐµ ÐºÐ¾Ð¼Ð¿ÑÐ¾Ð¼Ð¸ÑÑÑ Ð¼ÐµÐ¶Ð´Ñ ÑÐ¾ÑÐ½Ð¾ÑÑÑÑ Ð¸ Ð±ÑÑÑÑÐ¾Ð´ÐµÐ¹ÑÑÐ²Ð¸ÐµÐ¼, Ð¿Ð¾Ð´Ð´ÐµÑÐ¶Ð¸Ð²Ð°Ñ ÐºÐ°Ðº ÑÐµÐºÑÑÐ¾Ð²ÑÐµ, Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÐµ, ÑÐ°Ðº Ð¸ Ð¿Ð¾Ð»Ð½Ð¾ÑÑÑÑ Ð°Ð²ÑÐ¾Ð½Ð¾Ð¼Ð½ÑÐµ ÑÐµÐ¶Ð¸Ð¼Ñ ÑÐµÐ³Ð¼ÐµÐ½ÑÐ°ÑÐ¸Ð¸.",
  "emoji": "ð¯",
  "title": "YOLO26 Ð²ÑÑÑÐµÑÐ°ÐµÑ Ð¾ÑÐºÑÑÑÑÑ Ð»ÐµÐºÑÐ¸ÐºÑ: ÑÐ½Ð¸Ð²ÐµÑÑÐ°Ð»ÑÐ½Ð°Ñ ÑÐµÐ³Ð¼ÐµÐ½ÑÐ°ÑÐ¸Ñ Ð² ÑÐµÐ°Ð»ÑÐ½Ð¾Ð¼ Ð²ÑÐµÐ¼ÐµÐ½Ð¸"
}
```
[03.02.2026 20:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"YOLOE-26 integrates YOLO26 architecture with open-vocabulary learning for real-time instance segmentation, utilizing convolutional backbones, end-to-end regression, and object embedding heads with text and visual prompting capabilities.  					AI-generated summary 				 This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments."

[03.02.2026 20:29] Response: ```python
["CV", "ARCHITECTURE", "MULTIMODAL"]
```
[03.02.2026 20:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"YOLOE-26 integrates YOLO26 architecture with open-vocabulary learning for real-time instance segmentation, utilizing convolutional backbones, end-to-end regression, and object embedding heads with text and visual prompting capabilities.  					AI-generated summary 				 This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments."

[03.02.2026 20:29] Response: ```python
["OPEN_SOURCE"]
```
[03.02.2026 20:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"YOLOE-26 is a new framework that combines the YOLO26 architecture with open-vocabulary learning for real-time instance segmentation. It maintains the efficiency of the YOLO family while allowing recognition of objects beyond a fixed set of classes. The model uses a convolutional backbone and replaces traditional class logits with an object embedding head, enabling classification through similarity matching with text and visual prompts. This approach includes advanced techniques for prompt-based reasoning, making it adaptable for various segmentation tasks in real-world scenarios.","title":"YOLOE-26: Real-Time Open-Vocabulary Instance Segmentation Unleashed!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='YOLOE-26 is a new framework that combines the YOLO26 architecture with open-vocabulary learning for real-time instance segmentation. It maintains the efficiency of the YOLO family while allowing recognition of objects beyond a fixed set of classes. The model uses a convolutional backbone and replaces traditional class logits with an object embedding head, enabling classification through similarity matching with text and visual prompts. This approach includes advanced techniques for prompt-based reasoning, making it adaptable for various segmentation tasks in real-world scenarios.', title='YOLOE-26: Real-Time Open-Vocabulary Instance Segmentation Unleashed!'))
[03.02.2026 20:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"YOLOE-26æ¯ä¸ç§éæäºYOLO26æ¶æåå¼æ¾è¯æ±å­¦ä¹ çå®æ¶å®ä¾åå²æ¡æ¶ãå®å©ç¨å·ç§¯éª¨å¹²ç½åç«¯å°ç«¯åå½ï¼è½å¤å¤çææ¬åè§è§æç¤ºçå¯¹è±¡åµå¥ãè¯¥æ¹æ³éè¿å°åºå®ç±»å«é»è¾æ¿æ¢ä¸ºå¯¹è±¡åµå¥å¤´ï¼å®ç°äºåç±»ä¸æç¤ºåµå¥çç¸ä¼¼æ§å¹éãYOLOE-26å¨å¨æç¯å¢ä¸­æä¾äºä¸ç§å®ç¨ä¸å¯æ©å±çè§£å³æ¹æ¡ï¼æ¯æå¤ç§æç¤ºæ¹å¼ï¼ç¡®ä¿é«æçå®ä¾åå²ã","title":"YOLOE-26ï¼å®æ¶å¼æ¾è¯æ±å®ä¾åå²çåæ°è§£å³æ¹æ¡"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='YOLOE-26æ¯ä¸ç§éæäºYOLO26æ¶æåå¼æ¾è¯æ±å­¦ä¹ çå®æ¶å®ä¾åå²æ¡æ¶ãå®å©ç¨å·ç§¯éª¨å¹²ç½åç«¯å°ç«¯åå½ï¼è½å¤å¤çææ¬åè§è§æç¤ºçå¯¹è±¡åµå¥ãè¯¥æ¹æ³éè¿å°åºå®ç±»å«é»è¾æ¿æ¢ä¸ºå¯¹è±¡åµå¥å¤´ï¼å®ç°äºåç±»ä¸æç¤ºåµå¥çç¸ä¼¼æ§å¹éãYOLOE-26å¨å¨æç¯å¢ä¸­æä¾äºä¸ç§å®ç¨ä¸å¯æ©å±çè§£å³æ¹æ¡ï¼æ¯æå¤ç§æç¤ºæ¹å¼ï¼ç¡®ä¿é«æçå®ä¾åå²ã', title='YOLOE-26ï¼å®æ¶å¼æ¾è¯æ±å®ä¾åå²çåæ°è§£å³æ¹æ¡'))
[03.02.2026 20:29] Renaming data file.
[03.02.2026 20:29] Renaming previous data. hf_papers.json to ./d/2026-02-03.json
[03.02.2026 20:29] Saving new data file.
[03.02.2026 20:29] Generating page.
[03.02.2026 20:29] Renaming previous page.
[03.02.2026 20:29] Renaming previous data. index.html to ./d/2026-02-03.html
[03.02.2026 20:29] Writing result.
[03.02.2026 20:29] Renaming log file.
[03.02.2026 20:29] Renaming previous data. log.txt to ./logs/2026-02-03_last_log.txt
