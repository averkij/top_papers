[03.02.2026 21:29] Read previous papers.
[03.02.2026 21:29] Generating top page (month).
[03.02.2026 21:29] Writing top page (month).
[03.02.2026 22:24] Read previous papers.
[03.02.2026 22:24] Get feed.
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00919
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02276
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22060
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02185
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02084
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02437
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02053
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01566
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02361
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01590
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02493
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02383
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02488
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01624
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01058
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01756
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02214
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01395
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01801
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02092
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01479
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01541
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01335
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01851
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01538
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01576
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02486
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02472
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02343
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02227
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02156
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01322
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21123
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20613
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02453
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01675
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01660
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01511
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01382
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00986
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00759
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02110
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02039
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00269
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22588
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01984
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01842
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01296
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01077
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22674
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02477
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01997
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01983
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23000
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00130
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02354
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02287
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01970
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00521
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22801
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00192
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22599
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22296
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21968
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21759
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14691
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02338
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01897
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01815
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01618
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01418
[03.02.2026 22:24] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00168
[03.02.2026 22:24] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[03.02.2026 22:24] No deleted papers detected.
[03.02.2026 22:24] Downloading and parsing papers (pdf, html). Total: 72.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.00919.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.00919.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.00919.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02276.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02276.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02276.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2601.22060.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2601.22060.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2601.22060.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02185.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02185.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02185.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02084.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02084.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02084.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02437.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02437.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02437.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02053.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02053.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02053.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01566.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01566.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01566.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02361.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02361.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02361.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01590.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01590.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01590.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02493.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02493.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02493.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02383.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02383.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02383.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02488.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02488.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02488.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01624.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01624.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01624.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01058.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01058.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01058.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01756.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01756.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01756.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02214.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02214.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02214.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01395.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01395.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01395.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01801.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01801.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01801.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02092.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02092.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02092.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01479.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01479.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01479.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01541.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01541.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01541.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01335.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01335.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01335.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01851.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01851.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01851.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01538.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01538.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01538.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01576.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01576.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01576.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02486.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02486.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02486.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02472.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02472.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02472.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02343.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02343.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02343.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02227.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02227.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02227.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02156.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02156.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02156.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01322.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01322.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01322.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2601.21123.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2601.21123.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2601.21123.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2601.20613.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2601.20613.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2601.20613.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02453.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02453.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02453.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01675.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01675.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01675.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01660.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01660.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01660.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01511.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01511.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01511.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01382.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01382.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01382.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.00986.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.00986.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.00986.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.00759.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.00759.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.00759.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02110.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02110.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02110.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02039.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02039.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02039.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.00269.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.00269.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.00269.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2601.22588.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2601.22588.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2601.22588.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01984.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01984.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01984.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01842.
[03.02.2026 22:24] Downloading paper 2602.01842 from https://arxiv.org/pdf/2602.01842v1...
[03.02.2026 22:24] Failed to download and parse paper https://huggingface.co/papers/2602.01842: 'LTChar' object is not iterable
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01296.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01296.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01296.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01077.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01077.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01077.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2601.22674.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2601.22674.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2601.22674.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02477.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02477.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02477.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01997.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01997.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01997.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01983.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01983.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01983.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2601.23000.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2601.23000.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2601.23000.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.00130.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.00130.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.00130.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02354.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02354.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02354.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02287.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02287.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02287.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01970.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01970.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01970.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.00521.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.00521.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.00521.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2601.22801.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2601.22801.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2601.22801.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.00192.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.00192.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.00192.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2601.22599.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2601.22599.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2601.22599.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2601.22296.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2601.22296.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2601.22296.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2601.21968.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2601.21968.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2601.21968.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2601.21759.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2601.21759.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2601.21759.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2601.14691.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2601.14691.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2601.14691.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.02338.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.02338.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.02338.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01897.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01897.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01897.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01815.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01815.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01815.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01618.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01618.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01618.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.01418.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.01418.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.01418.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Downloading and parsing paper https://huggingface.co/papers/2602.00168.
[03.02.2026 22:24] Extra JSON file exists (./assets/json/2602.00168.json), skip PDF parsing.
[03.02.2026 22:24] Paper image links file exists (./assets/img_data/2602.00168.json), skip HTML parsing.
[03.02.2026 22:24] Success.
[03.02.2026 22:24] Enriching papers with extra data.
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 0. Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.  					AI-generated summary 				 We introduce Green-VLA, a staged Vision-Language-Action (...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 1. Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.  					AI-generated summary 				 We introduce Kimi K2.5, an open-source multimodal agentic model designed to adva...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 2. Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning.  					AI-generated summary 				 Multimodal large langu...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 3. Vision-DeepResearch benchmark addresses limitations in evaluating visual-textual search capabilities of multimodal models by introducing realistic evaluation conditions and improving visual retrieval through multi-round cropped-search workflow.  					AI-generated summary 				 Multimodal Large Langua...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 4. RPG-Encoder framework transforms repository comprehension and generation into a unified cycle by encoding code into high-fidelity Repository Planning Graph representations that improve understanding and reconstruction accuracy.  					AI-generated summary 				 Current repository agents encounter a re...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 5. UniReason integrates text-to-image generation and image editing through a dual reasoning paradigm that enhances planning with world knowledge and uses editing for visual refinement, achieving superior performance on reasoning-intensive benchmarks.  					AI-generated summary 				 Unified multimodal m...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 6. WildGraphBench evaluates GraphRAG performance in realistic scenarios using Wikipedia's structured content to assess multi-fact aggregation and summarization capabilities across diverse document types.  					AI-generated summary 				 Graph-based Retrieval-Augmented Generation (GraphRAG) organizes ext...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 7. A file-system-based dual-agent framework enables large language model agents to perform extended research tasks beyond context window limitations by using persistent storage as external memory.  					AI-generated summary 				 Deep research is emerging as a representative long-horizon task for large ...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 8. A scalable framework for constructing real-world software engineering environments from GitHub pull requests using an efficient building agent with self-verification and hacking detection capabilities.  					AI-generated summary 				 We propose SWE-Universe, a scalable and efficient framework for au...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 9. Deep Research Agents demonstrate capabilities in autonomous information retrieval but show significant gaps when evaluated against expert-level Wikipedia articles using a new live benchmark and comprehensive evaluation framework.  					AI-generated summary 				 Deep Research Agents (DRAs) have demon...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 10. PixelGen is a pixel-space diffusion framework that uses perceptual supervision through LPIPS and DINO-based losses to generate high-quality images without requiring VAEs or latent representations.  					AI-generated summary 				 Pixel diffusion generates images directly in pixel space in an end-to-e...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 11. SLIME is a novel reference-free alignment objective for large language models that decouples preference learning from generation quality through a three-pronged approach combining likelihood maximization, probability stabilization, and dual-margin constraints.  					AI-generated summary 				 Direct ...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 12. RLAnything enhances reinforcement learning for LLMs and agents through dynamic model optimization and closed-loop feedback mechanisms that improve policy and reward model training.  					AI-generated summary 				 We propose RLAnything, a reinforcement learning framework that dynamically forges envir...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 13. PISCES is an annotation-free text-to-video generation method that uses dual optimal transport-aligned rewards to improve visual quality and semantic alignment without human preference annotations.  					AI-generated summary 				 Text-to-video (T2V) generation aims to synthesize videos with high visu...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 14. Post-training of reasoning large language models can be improved by correcting distribution mismatches between supervised fine-tuning and reinforcement learning stages through importance sampling reweighting of the SFT loss.  					AI-generated summary 				 Post-training of reasoning LLMs is a holist...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 15. Mind-Brush presents a unified agentic framework for text-to-image generation that dynamically retrieves multimodal evidence and employs reasoning tools to improve understanding of implicit user intentions and complex knowledge reasoning.  					AI-generated summary 				 While text-to-image generation...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 16. A novel Causal Forcing method addresses the architectural gap in distilling bidirectional video diffusion models into autoregressive models by using AR teachers for ODE initialization, significantly improving video generation performance.  					AI-generated summary 				 To achieve real-time interact...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 17. Selective knowledge distillation in autoregressive language models using student-entropy-guided position selection improves accuracy and efficiency while reducing memory and storage requirements.  					AI-generated summary 				 Growing efforts to improve knowledge distillation (KD) in large language...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 18. Autoregressive video diffusion models face efficiency challenges due to growing KV caches and redundant attention computations, which are addressed through TempCache, AnnCA, and AnnSA techniques that reduce computational demands while maintaining visual quality and stable performance.  					AI-gener...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 19. FSVideo is a fast transformer-based image-to-video diffusion framework that uses a compressed video autoencoder, diffusion transformer architecture with enhanced layer memory, and multi-resolution generation strategy to achieve high performance with significantly reduced computation time.  					AI-g...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 20. A Japanese financial language understanding benchmark named Ebisu is introduced, featuring two expert-annotated tasks that evaluate implicit commitment recognition and hierarchical financial terminology extraction, revealing persistent challenges for current language models despite their advanced ca...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 21. MLLMs equipped with Cognitive Supersensing and Latent Visual Imagery Prediction demonstrate enhanced cognitive reasoning capabilities through integrated visual and textual reasoning pathways.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved remarkable success in...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 22. Visual metaphor transfer enables creative AI systems to decompose abstract conceptual relationships from reference images and reapply them to new subjects through a multi-agent framework grounded in cognitive theory.  					AI-generated summary 				 A visual metaphor constitutes a high-order form of ...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 23. Visual Instruction Benchmark for Image Editing introduces a three-level interaction hierarchy for evaluating visual instruction following capabilities in generative models.  					AI-generated summary 				 Recent generative models have achieved remarkable progress in image editing. However, existing ...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 24. A dual-stream framework called InteractAvatar is presented for generating talking avatars that can interact with objects in their environment, addressing challenges in grounded human-object interaction through decoupled perception and planning modules.  					AI-generated summary 				 Generating talk...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 25. Visual world models for mobile GUI agents are improved through renderable code generation using vision-language models, achieving better performance with reduced model size compared to existing approaches.  					AI-generated summary 				 Mobile Graphical User Interface (GUI) World Models (WMs) offer...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 26. Re-TRAC is an agentic framework that enhances LLM-based research agents by enabling cross-trajectory exploration and iterative reflection through structured state representations, leading to more efficient and effective problem-solving compared to traditional ReAct approaches.  					AI-generated sum...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 27. SPARKLING is a framework for mid-stage width expansion in deep learning models that maintains signal preservation and breaks symmetry to stabilize training and reduce computational costs.  					AI-generated summary 				 Progressive Learning (PL) reduces pre-training computational overhead by gradual...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 28. Large language model control methods are unified under a dynamic weight update framework, revealing a preference-utility trade-off and enabling improved steering through SPLIT approach.  					AI-generated summary 				 Methods for controlling large language models (LLMs), including local weight fine-...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 29. LatentMorph integrates implicit latent reasoning into text-to-image generation through four lightweight components that enable adaptive self-refinement and improve both efficiency and cognitive alignment.  					AI-generated summary 				 Text-to-image (T2I) generation has achieved remarkable progress...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 30. Loop-ViT introduces a recursive vision transformer architecture that decouples reasoning depth from model capacity through weight-tied recurrence and dynamic exit mechanisms, achieving superior visual reasoning performance with fewer parameters.  					AI-generated summary 				 Recent advances in vis...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 31. PolySAE extends sparse autoencoders with polynomial decoding to capture feature interactions and compositional structure while maintaining linear encoders for interpretability.  					AI-generated summary 				 Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural netwo...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 32. CUA-Skill introduces a large-scale library of engineered computer-use skills that enhance agent performance and efficiency on Windows-based tasks.  					AI-generated summary 				 Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 33. AgentIF-OneDay evaluates AI agents' ability to handle diverse daily tasks through natural language instructions, requiring problem-solving, attachment understanding, and file-based outputs across three user-centric categories.  					AI-generated summary 				 The capacity of AI agents to effectively ...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 34. Thinking with Comics emerges as an effective visual reasoning approach that bridges images and videos by leveraging comic structures for improved multimodal reasoning efficiency and performance.  					AI-generated summary 				 Chain-of-Thought reasoning has driven large language models to extend fro...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 35. TRIP-Bench presents a comprehensive long-horizon benchmark for travel planning that evaluates LLM agents on complex multi-turn interactions, while GTPO offers an online reinforcement learning approach to enhance constraint satisfaction and robustness in extended dialogues.  					AI-generated summary...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 36. A novel framework called CoDiQ enables controllable difficulty generation for competition-level questions through test-time scaling, resulting in a corpus that significantly improves large reasoning model performance.  					AI-generated summary 				 Large Reasoning Models (LRMs) benefit substantiall...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 37. Rubric-ARM framework jointly optimizes rubric generation and judging through reinforcement learning to improve response quality assessment in creative and open-ended tasks.  					AI-generated summary 				 Standard reward models typically predict scalar scores that fail to capture the multifaceted na...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 38. Flow matching models for text-to-image generation are enhanced through a reinforcement learning framework that addresses sample inefficiency and prompt overfitting by incorporating language models for prompt refinement, achieving superior performance with reduced computational requirements.  					AI...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 39. Research identifies a sparse reward subsystem in LLM hidden states containing value neurons that represent internal state expectations and dopamine-like neurons encoding reward prediction errors.  					AI-generated summary 				 In this paper, we identify a sparse reward subsystem within the hidden s...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 40. Adaptive Ability Decomposing (AÂ²D) enhances reinforcement learning with verifiable rewards by decomposing complex questions into simpler sub-questions, improving LLM reasoning through guided exploration without requiring a teacher model.  					AI-generated summary 				 Reinforcement learning with ve...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 41. Post-training quantization effects in world models reveal unique failure modes and trade-offs between accuracy, bit-width, and planning performance, particularly in encoder-predictor module asymmetries and low-bit rollout stability.  					AI-generated summary 				 World models learn an internal repr...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 42. Agentic large language models require investigatory intelligence for autonomous data analysis, demonstrated through the Deep Data Research benchmark that evaluates their ability to extract insights from databases without explicit queries.  					AI-generated summary 				 The agency expected of Agenti...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 43. VoxServe is a unified serving system for Speech Language Models that enhances streaming performance through model-execution abstraction, streaming-aware scheduling, and asynchronous inference pipelines.  					AI-generated summary 				 Deploying modern Speech Language Models (SpeechLMs) in streaming ...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 44. Small language models can effectively evaluate outputs by leveraging internal representations rather than generating responses, enabling a more efficient and interpretable evaluation approach through a probing-based framework.  					AI-generated summary 				 Large language models (LLMs) are widely u...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 45. Scaling hidden states of delimiter tokens in vision-language models reduces cross-image information leakage and improves multi-image reasoning performance.  					AI-generated summary 				 Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance dec...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 46. A new test-time scaling framework called Prism is introduced for discrete diffusion language models that improves reasoning performance through hierarchical trajectory search, local branching with partial remasking, and self-verified feedback mechanisms.  					AI-generated summary 				 Inference-tim...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 47. LiP-Map presents a line-plane joint optimization framework that explicitly models learnable line and planar primitives for accurate 3D line mapping in man-made environments.  					AI-generated summary 				 3D line mapping from multi-view RGB images provides a compact and structured visual representa...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 48. PISA is a novel sparse attention method that improves diffusion transformer efficiency by approximating non-critical attention blocks instead of discarding them, achieving faster processing with maintained quality.  					AI-generated summary 				 Diffusion Transformers are fundamental for video and ...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 49. VisionTrim is a training-free framework that accelerates multimodal large language models by selecting dominant visual tokens and merging them with text-guided complementation, improving efficiency without performance loss.  					AI-generated summary 				 Multimodal large language models (MLLMs) suf...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 50. An end-to-end reinforcement learning framework enhances large language models' reasoning capabilities by implementing divide-and-conquer strategies that outperform traditional chain-of-thought reasoning on challenging benchmarks.  					AI-generated summary 				 Large language models (LLMs) have demo...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 51. Layer pruning compresses large language models while maintaining classification performance but causes significant degradation in generative reasoning tasks, with limited recovery possible through supervised finetuning on self-generated responses.  					AI-generated summary 				 Recent works have sh...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 52. A training-free framework enables language model agents to automatically create and optimize tools during inference, improving their reasoning capabilities through self-evolution and memory consolidation.  					AI-generated summary 				 Existing Tool-Integrated Reasoning (TIR) models have effectivel...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 53. A novel optimizer called Mano is proposed that combines manifold optimization with momentum projection onto tangent spaces, achieving superior performance over AdamW and Muon while reducing memory and computational requirements.  					AI-generated summary 				 While large language models (LLMs) have...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 54. Effective dimension, an unsupervised geometric metric, strongly predicts neural network performance across different architectures and domains, showing bidirectional causality between representation geometry and accuracy.  					AI-generated summary 				 We investigate the relationship between repres...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 55. Implicit neural representations operate continuously over UV coordinate space, demonstrating good image quality while balancing memory usage and rendering time, with applications in real-time rendering and downstream tasks.  					AI-generated summary 				 Implicit neural representation (INR) has pro...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 56. Controlled cross-lingual evaluation reveals instability in LLM assessment methods when targeting morphologically rich languages, indicating unreliable zero-shot judge transfer for discourse-level tasks.  					AI-generated summary 				 Cross-lingual evaluation of large language models (LLMs) typicall...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 57. Generalizable Predictive Prompt Selection (GPS) uses Bayesian inference with a lightweight generative model to efficiently select informative prompts for reinforcement learning-enhanced language models, improving training efficiency and performance.  					AI-generated summary 				 Reinforcement lear...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 58. A two-phase diagnostic framework based on Item Response Theory and Graded Response Model is introduced to assess the reliability of LLM-as-a-Judge by examining intrinsic consistency and human alignment.  					AI-generated summary 				 While LLM-as-a-Judge is widely used in automated evaluation, exis...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 59. Clipping-Free Policy Optimization replaces heuristic clipping with convex quadratic penalty to stabilize reinforcement learning training for large language models without performance loss.  					AI-generated summary 				 Reinforcement learning has become central to post-training large language model...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 60. VAE-based inpainting creates spectral shifts that fool detection systems, which can be mitigated through Inpainting Exchange to improve content-aware detection performance.  					AI-generated summary 				 Modern deep learning-based inpainting enables realistic local image manipulation, raising criti...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 61. Automated pipeline for sound separation using high-purity single-event segments from in-the-wild datasets achieves competitive performance with significantly reduced data requirements.  					AI-generated summary 				 Query-based universal sound separation is fundamental to intelligent auditory syste...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 62. Parallel Echo State Network (ParalESN) addresses reservoir computing limitations by enabling parallel temporal processing through diagonal linear recurrence, maintaining theoretical guarantees while achieving significant computational efficiency gains.  					AI-generated summary 				 Reservoir Compu...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 63. On-policy Verbal Distillation (OVD) enables efficient knowledge transfer from teacher to student models by replacing token-level probability matching with trajectory matching using discrete verbal scores, reducing memory consumption and enabling free exploration without token alignment constraints. ...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 64. An reinforcement learning-based sampling framework adaptively reweights training datasets to improve embedding model performance while reducing GPU costs.  					AI-generated summary 				 General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 65. Large language models used as judges for agent performance evaluation are vulnerable to manipulation of reasoning traces, with content-based fabrications being more effective than style-based alterations.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as judges to...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 66. ReSID presents a novel recommendation-native framework that improves sequential recommendation by learning predictive item representations and optimizing quantization for information preservation and sequential predictability.  					AI-generated summary 				 Semantic ID (SID)-based recommendation is...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 67. Internal flow signatures analyze depthwise dynamics in large language models to enable self-checking and targeted refinement without modifying the base model.  					AI-generated summary 				 Large language models can generate fluent answers that are unfaithful to the provided context, while many saf...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 68. Multi-agent systems for molecular discovery that use individualized scientist profiles based on publication and molecular history outperform traditional role-based approaches.  					AI-generated summary 				 Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery....
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 69. Researchers developed a novel agentic data-generation framework to create culturally grounded safety datasets for Southeast Asia, resulting in multilingual safeguard models that outperform existing approaches in detecting regionally sensitive content while maintaining general safety performance.  		...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 70. Parabolic Position Encoding (PaPE) is a novel position encoding method for vision modalities that improves upon existing approaches by incorporating translation invariance, rotation invariance, distance decay, directionality, and context awareness principles.  					AI-generated summary 				 We propo...
[03.02.2026 22:24] ********************************************************************************
[03.02.2026 22:24] Abstract 71. YOLOE-26 integrates YOLO26 architecture with open-vocabulary learning for real-time instance segmentation, utilizing convolutional backbones, end-to-end regression, and object embedding heads with text and visual prompting capabilities.  					AI-generated summary 				 This paper presents YOLOE-26, a...
[03.02.2026 22:24] Read previous papers.
[03.02.2026 22:24] Generating reviews via LLM API.
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#data", "#rl", "#robotics", "#training"], "emoji": "ð¤", "ru": {"title": "Ð£Ð½Ð¸Ð²ÐµÑÑÐ°Ð»ÑÐ½Ð°Ñ Ð¿Ð¾Ð»Ð¸ÑÐ¸ÐºÐ° Ð´ÐµÐ¹ÑÑÐ²Ð¸Ð¹ Ð´Ð»Ñ ÑÐ¾Ð±Ð¾ÑÐ¾Ð² ÑÐ°Ð·Ð½ÑÑ ÐºÐ¾Ð½ÑÑÑÑÐºÑÐ¸Ð¹ ÑÐµÑÐµÐ· Ð¼Ð½Ð¾Ð³Ð¾ÑÑÐ°Ð¿Ð½Ð¾Ðµ Ð¾Ð±ÑÑÐµÐ½Ð¸Ðµ Ñ ÑÐ·ÑÐºÐ¾Ð¼ Ð¸ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "Green-VLA Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ ÑÐ¾Ð±Ð¾Ð¹ Ð¿ÑÑÐ¸ÑÑÐ°Ð¿Ð½ÑÑ Ð°ÑÑÐ¸ÑÐµÐºÑÑÑÑ Vision-Language
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#plp", "#cv", "#agents", "#reasoning", "#open_source", "#training"], "emoji": "ð", "ru": {"title": "ÐÐ½Ð¾Ð³Ð¾Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÐ¹ Ð¸Ð½ÑÐµÐ»Ð»ÐµÐºÑ ÑÐµÑÐµÐ· ÑÐ¾ÐµÐ²ÑÑ ÐºÐ¾Ð¾ÑÐ´Ð¸Ð½Ð°ÑÐ¸Ñ Ð°Ð³ÐµÐ½ÑÐ¾Ð²", "desc": "ÐÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÐ¼ Kimi K2.5 â Ð¾ÑÐºÑÑÑÑÑ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ Ð¼Ð¾Ð´ÐµÐ»Ñ-Ð°Ð³ÐµÐ½ÑÐ°, ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð¸ÑÑÐµÑ Ð¾Ð±ÑÐ°Ð±Ð¾ÑÐºÑ ÑÐµÐº
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#rag", "#reasoning", "#optimization", "#rl", "#open_source"], "emoji": "ð", "ru": {"title": "ÐÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½Ð¾Ðµ Ð³Ð»ÑÐ±Ð¾ÐºÐ¾Ðµ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÐµÑÐµÐ· Ð¼Ð½Ð¾Ð³Ð¾ÑÐ¾Ð´Ð¾Ð²Ð¾Ð¹ Ð¿Ð¾Ð¸ÑÐº Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸ÐµÐ¼", "desc": "Vision-DeepResearch Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÑ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ Ð¿Ð°ÑÐ°Ð´Ð¸Ð³Ð¼Ñ Ð³Ð»ÑÐ±Ð¾ÐºÐ¾Ð³Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#dataset", "#open_source"], "emoji": "ð", "ru": {"title": "Ð ÐµÐ°Ð»Ð¸ÑÑÐ¸ÑÐ½Ð°Ñ Ð¾ÑÐµÐ½ÐºÐ° Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ° Ð² Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ ÑÐ¸ÑÑÐµÐ¼Ð°Ñ", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Vision-DeepResearch benchmark Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐµÐ¹ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Ð²
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#plp"], "emoji": "ð", "ru": {"title": "ÐÐ´Ð¸Ð½ÑÐ¹ ÑÐ¸ÐºÐ» Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð¸ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ ÐºÐ¾Ð´Ð° ÑÐµÑÐµÐ· Ð²ÑÑÐ¾ÐºÐ¾ÑÐ¾ÑÐ½ÑÐµ Ð³ÑÐ°ÑÐ¾Ð²ÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ RPG-Encoder â ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð¿ÑÐµÐ¾Ð±ÑÐ°Ð·ÑÐµÑ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð¸ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ñ ÐºÐ¾Ð´Ð° Ð² ÐµÐ´Ð¸Ð½ÑÐ¹ ÑÐ¸ÐºÐ» ÑÐµÑÐµÐ· ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²ÑÑÐ¾ÐºÐ¾
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#reasoning", "#synthetic"], "emoji": "ð¨", "ru": {"title": "ÐÐ´Ð¸Ð½Ð¾Ðµ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ ÑÐ¾Ð²ÐµÑÑÐµÐ½Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸: Ð¿Ð»Ð°Ð½Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ ÑÑÐ¾Ð½ÑÐµÐ½Ð¸Ðµ", "desc": "UniReason â ÑÑÐ¾ ÐµÐ´Ð¸Ð½Ð°Ñæ¡æ¶, ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð¾Ð±ÑÐµÐ´Ð¸Ð½ÑÐµÑ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ñ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸ Ð¸Ñ ÑÐµÐ´Ð°ÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ ÑÐµÑÐµÐ· Ð´Ð²Ð¾Ð¹Ð½ÑÑ 
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#dataset", "#rag", "#benchmark", "#open_source", "#graphs", "#long_context"], "emoji": "ð¸ï¸", "ru": {"title": "ÐÐµÐ½ÑÐ¼Ð°ÑÐº Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ Ð¸Ð·Ð²Ð»ÐµÑÐµÐ½Ð¸Ñ Ð¸ Ð°Ð³ÑÐµÐ³Ð°ÑÐ¸Ð¸ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð² ÑÐµÐ°Ð»ÑÐ½ÑÑ ÑÑÐµÐ½Ð°ÑÐ¸ÑÑ", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½ WildGraphBench â Ð±ÐµÐ½ÑÐ¼Ð°ÑÐº Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ GraphRAG Ð² ÑÐµÐ°Ð»Ð¸ÑÑÐ¸ÑÐ½ÑÑ ÑÑÐµÐ½Ð°ÑÐ¸ÑÑ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark", "#reasoning", "#open_source", "#long_context"], "emoji": "ð", "ru": {"title": "ÐÑÐµÐ¾Ð´Ð¾Ð»ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð½ÑÐµÐºÑÑÐ½ÑÑ Ð¾Ð³ÑÐ°Ð½Ð¸ÑÐµÐ½Ð¸Ð¹: Ð²Ð½ÐµÑÐ½ÑÑ Ð¿Ð°Ð¼ÑÑÑ Ð´Ð»Ñ Ð°Ð³ÐµÐ½ÑÐ¾Ð² LLM", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ FS-Researcher â Ð´Ð²ÑÑÐ°Ð³ÐµÐ½ÑÐ½Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð°, Ð³Ð´Ðµ Ð¾Ð´Ð¸Ð½ Ð°Ð³ÐµÐ½Ñ ÑÐ¾Ð±Ð¸ÑÐ°ÐµÑ Ð¸Ð½ÑÐ¾Ñ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multilingual", "#plp", "#dataset", "#agents", "#benchmark", "#rl", "#training"], "emoji": "ðï¸", "ru": {"title": "ÐÐ¸Ð»Ð»Ð¸Ð¾Ð½Ð½ÑÐ¹ Ð¼Ð°ÑÑÑÐ°Ð± Ð²ÐµÑÐ¸ÑÐ¸ÑÐ¸ÑÑÐµÐ¼ÑÑ Ð·Ð°Ð´Ð°Ñ ÑÐ°Ð·ÑÐ°Ð±Ð¾ÑÐºÐ¸ ÑÐµÑÐµÐ· ÑÐ¼Ð½ÑÑ Ð°Ð³ÐµÐ½ÑÐ¾Ð²", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑÑÑ SWE-Universe â Ð¼Ð°ÑÑÑÐ°Ð±Ð¸ÑÑÐµÐ¼Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð´Ð»Ñ Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸ÑÐµÑÐºÐ¾Ð³Ð¾ ÑÐ¾Ð·Ð´Ð°
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark", "#open_source", "#survey"], "emoji": "ð", "ru": {"title": "ÐÐ·Ð¼ÐµÑÑÑ Ð´Ð¸ÑÑÐ°Ð½ÑÐ¸Ñ Ð¼ÐµÐ¶Ð´Ñ AI-Ð°Ð³ÐµÐ½ÑÐ°Ð¼Ð¸ Ð¸ ÑÐºÑÐ¿ÐµÑÑÐ½ÑÐ¼Ð¸ Ð·Ð½Ð°Ð½Ð¸ÑÐ¼Ð¸", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð° Ð½Ð¾Ð²Ð°Ñ benchmark Wiki Live Challenge, ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÐµÑ ÑÑÐ°ÑÑÐ¸ Wikipedia Good Articles ÐºÐ°Ðº ÑÑÐ°Ð»Ð¾
[03.02.2026 22:24] Using data from previous issue: {"categories": [], "emoji": "ð¨", "ru": {"title": "ÐÑÑÐ¼Ð°Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ñ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ð² Ð¿Ð¸ÐºÑÐµÐ»ÑÐ½Ð¾Ð¼ Ð¿ÑÐ¾ÑÑÑÐ°Ð½ÑÑÐ²Ðµ ÑÐµÑÐµÐ· Ð¿ÐµÑÑÐµÐ¿ÑÐ¸Ð²Ð½ÑÐ¹ Ð½Ð°Ð´Ð·Ð¾Ñ", "desc": "PixelGen â ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð³ÐµÐ½ÐµÑÐ¸ÑÑÐµÑ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ñ Ð¿ÑÑÐ¼Ð¾ Ð² Ð¿Ð¸ÐºÑÐµÐ»ÑÐ½Ð¾Ð¼ Ð¿ÑÐ¾ÑÑÑÐ°Ð½ÑÑÐ²Ðµ, Ð¸Ð·Ð±ÐµÐ³Ð°Ñ Ð°ÑÑÐµÑÐ°ÐºÑÐ¾Ð², ÐºÐ¾ÑÐ¾ÑÑÐµ Ð²Ð½Ð¾ÑÑÑ VAE Ð² ÑÑÐ°Ð´Ð¸Ñ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#training"], "emoji": "âï¸", "ru": {"title": "Ð¡ÑÐ°Ð±Ð¸Ð»ÑÐ½Ð¾Ðµ Ð²ÑÑÐ°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð±ÐµÐ· ÐºÐ¾Ð¼Ð¿ÑÐ¾Ð¼Ð¸ÑÑÐ° Ð¼ÐµÐ¶Ð´Ñ Ð¿ÑÐµÐ´Ð¿Ð¾ÑÑÐµÐ½Ð¸ÑÐ¼Ð¸ Ð¸ ÐºÐ°ÑÐµÑÑÐ²Ð¾Ð¼", "desc": "SLIME â ÑÑÐ¾ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ Ð²ÑÑÐ°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ñ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾ÑÐ¾ÑÑÐ¹ ÑÐµÑÐ°ÐµÑ Ð¿ÑÐ¾Ð±Ð»ÐµÐ¼Ñ Ð½ÐµÑÐ¾Ð¾ÑÐ²ÐµÑÑÑÐ²Ð¸Ñ ÑÐµÐ»ÐµÐ²ÑÑ ÑÑÐ½ÐºÑÐ¸Ð¹ Ð² Ð¼ÐµÑÐ¾Ð´Ð°Ñ Ð¿ÑÑ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#rl", "#agents", "#training"], "emoji": "ð", "ru": {"title": "ÐÐ¸Ð½Ð°Ð¼Ð¸ÑÐµÑÐºÐ°Ñ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ñ ÑÐµÑÐµÐ· Ð·Ð°Ð¼ÐºÐ½ÑÑÑÑ Ð¾Ð±ÑÐ°ÑÐ½ÑÑ ÑÐ²ÑÐ·Ñ Ð² Ð¾Ð±ÑÑÐµÐ½Ð¸Ð¸ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "RLAnything â ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÑÐµÑÐºÐ¸ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð¸ÑÑÐµÑ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¾ÐºÑÑÐ¶ÐµÐ½Ð¸Ñ, Ð¿Ð¾Ð»Ð¸ÑÐ¸ÐºÐ¸ Ð¸ Ð²Ð¾Ð·Ð½Ð°Ð³ÑÐ°Ð¶Ð´ÐµÐ½
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#optimization", "#rl", "#video", "#training"], "emoji": "ð¬", "ru": {"title": "ÐÐ¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½ÑÐ¹ ÑÑÐ°Ð½ÑÐ¿Ð¾ÑÑ Ð´Ð»Ñ Ð±ÐµÐ·Ð°Ð½Ð½Ð¾ÑÐ°ÑÐ¸Ð¾Ð½Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð¸Ð· ÑÐµÐºÑÑÐ°", "desc": "PISCES â ÑÑÐ¾ Ð¼ÐµÑÐ¾Ð´ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð¸Ð· ÑÐµÐºÑÑÐ°, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð½Ðµ ÑÑÐµÐ±ÑÐµÑ Ð°Ð½Ð½Ð¾ÑÐ°ÑÐ¸Ð¹ ÑÐµÐ»Ð¾Ð²ÐµÐºÐ° Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ ÐºÐ°ÑÐµÑÑÐ²Ð°. ÐÐ²ÑÐ¾ÑÑ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÑÑ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#math", "#reasoning", "#optimization", "#rl", "#training"], "emoji": "âï¸", "ru": {"title": "ÐÐ°Ð»Ð°Ð½ÑÐ¸ÑÐ¾Ð²ÐºÐ° ÑÐ°ÑÐ¿ÑÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ ÑÐ¸Ð½ÐµÑÐ³Ð¸Ð¸ SFT Ð¸ RL Ð² Ð¾Ð±ÑÑÐµÐ½Ð¸Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑÐ¾Ð´ PEAR Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ ÑÐ¾Ð²Ð¼ÐµÑÑÐ½Ð¾Ð³Ð¾ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ Ð½Ð° ÑÑÐ°Ð¿Ð°Ñ supervised fine-t
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#benchmark", "#rag", "#reasoning"], "emoji": "ð¨", "ru": {"title": "ÐÐ³ÐµÐ½ÑÐ½ÑÐ¹ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ ÐºÐ¾Ð½ÑÐµÐºÑÑÐ½Ð¾-Ð¾ÑÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ ÑÐµÑÐµÐ· Ð´Ð¸Ð½Ð°Ð¼Ð¸ÑÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð·Ð½Ð°Ð½Ð¸Ð¹", "desc": "Mind-Brush Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð°Ð³ÐµÐ½ÑÐ½ÑÐ¹ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸Ð· ÑÐµÐºÑÑ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#architecture", "#video", "#training"], "emoji": "ð¬", "ru": {"title": "ÐÑÐ¸ÑÐ¸Ð½Ð½Ð¾Ðµ Ð¿ÑÐ¸Ð½ÑÐ¶Ð´ÐµÐ½Ð¸Ðµ: Ð´Ð¸ÑÑÐ¸Ð»Ð»ÑÑÐ¸Ñ Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐµÑÐµÐ· Ð°Ð²ÑÐ¾ÑÐµÐ³ÑÐµÑÑÐ¸Ð²Ð½ÑÑ ÑÑÐ¸ÑÐµÐ»ÐµÐ¹", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ Causal Forcing Ð´Ð»Ñ Ð´Ð¸ÑÑÐ¸Ð»Ð»ÑÑÐ¸Ð¸ Ð´Ð²ÑÐ½Ð°Ð¿ÑÐ°Ð²Ð»ÐµÐ½Ð½ÑÑ Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#inference", "#optimization", "#training", "#small_models"], "emoji": "ð§ ", "ru": {"title": "Ð£Ð¼Ð½Ð°Ñ Ð´Ð¸ÑÑÐ¸Ð»Ð»ÑÑÐ¸Ñ: Ð²ÑÐ±Ð¸ÑÐ°ÐµÐ¼ Ð¿ÑÐ°Ð²Ð¸Ð»ÑÐ½ÑÐµ Ð¿Ð¾Ð·Ð¸ÑÐ¸Ð¸ Ð´Ð»Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑÐ¾Ð´ Ð²ÑÐ±Ð¾ÑÐ¾ÑÐ½Ð¾Ð¹ Ð´Ð¸ÑÑÐ¸Ð»Ð»ÑÑÐ¸Ð¸ Ð·Ð½Ð°Ð½Ð¸Ð¹ (knowledge distillation) Ð´Ð»Ñ Ð°Ð²ÑÐ¾Ð³ÑÐµÑÑÐ¸Ð²Ð½ÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´Ðµ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#video", "#inference", "#diffusion", "#optimization", "#long_context"], "emoji": "â¡", "ru": {"title": "ÐÐ¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ñ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð² Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÑÑ ÑÐµÑÐµÐ· ÑÐ¶Ð°ÑÐ¸Ðµ ÐºÐµÑÐ° Ð¸ ÑÐ°Ð·ÑÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð²ÑÑÐ¸ÑÐ»ÐµÐ½Ð¸Ð¹", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ñ ÑÑÐ¸ ÑÐµÑÐ½Ð¸ÐºÐ¸ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ð¸ Ð´Ð»Ñ Ð°Ð²ÑÐ¾ÑÐµÐ³ÑÐµÑÑÐ¸Ð²Ð½ÑÑ Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð¸ÑÑ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#architecture", "#video", "#diffusion", "#optimization", "#open_source", "#training"], "emoji": "ð¬", "ru": {"title": "ÐÐ¾ÑÑÐ´Ð¾Ðº Ð²ÐµÐ»Ð¸ÑÐ¸Ð½Ñ Ð±ÑÑÑÑÐµÐµ: Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½Ð½ÑÐµ ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼ÐµÑÑ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð¸Ð· Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹", "desc": "FSVideo Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ ÑÐ¾Ð±Ð¾Ð¹ Ð±ÑÑÑÑÑÐ¹ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ Ð¿ÑÐµÐ¾Ð±ÑÐ°Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#science", "#benchmark", "#low_resource", "#open_source"], "emoji": "ð", "ru": {"title": "Ð¯Ð·ÑÐºÐ¾Ð²Ð¾Ð¹ Ð±Ð°ÑÑÐµÑ: Ð¿Ð¾ÑÐµÐ¼Ñ LLM Ð±Ð¾ÑÑÑÑÑ Ñ ÑÐ¿Ð¾Ð½ÑÐºÐ¸Ð¼ ÑÐ¸Ð½Ð°Ð½ÑÐ¾Ð²ÑÐ¼ ÑÐµÐºÑÑÐ¾Ð¼", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½ ÑÑÐ°Ð»Ð¾Ð½Ð½ÑÐ¹ Ð½Ð°Ð±Ð¾Ñ Ebisu Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ ÑÐ¿Ð¾Ð½ÑÐºÐ¾Ð³Ð¾ ÑÐ¸Ð½Ð°Ð½ÑÐ¾Ð²Ð¾Ð³Ð¾ Ñ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#reasoning", "#rl", "#open_source", "#training"], "emoji": "ð§ ", "ru": {"title": "ÐÐ½ÑÑÑÐµÐ½Ð½ÑÑ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½Ð°Ñ Ð¿Ð°Ð¼ÑÑÑ ÐºÐ°Ðº Ð¾ÑÐ½Ð¾Ð²Ð° ÐºÐ¾Ð³Ð½Ð¸ÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ Ð¼ÑÑÐ»ÐµÐ½Ð¸Ñ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Cognitive Supersensing Ð´Ð»
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#open_source"], "emoji": "ð¨", "ru": {"title": "Ð¢Ð²Ð¾ÑÑÐµÑÐºÐ¸Ð¹ Ð¿ÐµÑÐµÐ½Ð¾Ñ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÑ Ð¼ÐµÑÐ°ÑÐ¾Ñ ÑÐµÑÐµÐ· Ð¼Ð½Ð¾Ð³Ð¾Ð°Ð³ÐµÐ½ÑÐ½ÑÑ ÑÐ¸ÑÑÐµÐ¼Ñ Ñ ÐºÐ¾Ð³Ð½Ð¸ÑÐ¸Ð²Ð½ÑÐ¼ Ð¾Ð±Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð½Ð¾Ð²Ð°Ñ Ð·Ð°Ð´Ð°ÑÐ° Visual Metaphor Transfer, ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð·Ð°ÐºÐ»ÑÑÐ°ÐµÑÑÑ Ð² Ð¸Ð·Ð²Ð»ÐµÑÐµÐ½Ð¸Ð¸ Ð°Ð±ÑÑÑÐ°ÐºÑÐ½Ð¾Ð¹ ÑÐ²Ð¾ÑÑÐµÑÐºÐ¾Ð¹ ÑÑÑÐ½Ð¾ÑÑÐ¸ Ð¸Ð· ÑÑÐ°
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#benchmark"], "emoji": "âï¸", "ru": {"title": "ÐÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½Ð¾Ðµ ÑÐµÐ´Ð°ÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ: Ð¾Ñ ÑÐµÐºÑÑÐ° Ðº Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÐ¼ Ð¸Ð½ÑÑÑÑÐºÑÐ¸ÑÐ¼", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ VIBE â Visual Instruction Benchmark for Image Editing, Ð½Ð¾Ð²ÑÐ¹ Ð±ÐµÐ½ÑÐ¼Ð°ÑÐº Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐ¸ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð²Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐ»Ðµ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#video", "#games", "#benchmark"], "emoji": "ð¬", "ru": {"title": "ÐÐ¾Ð²Ð¾ÑÑÑÐ¸Ðµ Ð°Ð²Ð°ÑÐ°ÑÑ, ÐºÐ¾ÑÐ¾ÑÑÐµ ÑÐµÐ°Ð»ÑÐ½Ð¾ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑÐ²ÑÑÑ Ñ Ð¿ÑÐµÐ´Ð¼ÐµÑÐ°Ð¼Ð¸", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð´Ð²ÑÑÐ¿Ð¾ÑÐ¾ÑÐ½Ð°Ñ Ð°ÑÑÐ¸ÑÐµÐºÑÑÑÐ° InteractAvatar Ð´Ð»Ñ ÑÐ¸Ð½ÑÐµÐ·Ð° Ð²Ð¸Ð´ÐµÐ¾ Ð³Ð¾Ð²Ð¾ÑÑÑÐ¸Ñ Ð°Ð²Ð°ÑÐ°ÑÐ¾Ð², ÐºÐ¾ÑÐ¾ÑÑÐµ Ð¼Ð¾Ð³
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#cv", "#agents", "#benchmark", "#small_models"], "emoji": "ð±", "ru": {"title": "ÐÑ Ð¿Ð¸ÐºÑÐµÐ»ÐµÐ¹ Ðº ÐºÐ¾Ð´Ñ: Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÐµ Ð¼Ð¸ÑÐ¾Ð²ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ ÑÐµÑÐµÐ· Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ñ ÑÐµÐ½Ð´ÐµÑÐ¸ÑÑÐµÐ¼Ð¾Ð³Ð¾ Ð²ÐµÐ±-ÐºÐ¾Ð´Ð°", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¼Ð¸ÑÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð¼Ð¾Ð±Ð¸Ð»ÑÐ½ÑÑ GUI
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#long_context", "#optimization", "#reasoning"], "emoji": "ð", "ru": {"title": "Ð Ð°Ð·Ð¼ÑÑÐ»ÑÑÑÐ¸Ð¹ Ð°Ð³ÐµÐ½Ñ: Ð³Ð»Ð¾Ð±Ð°Ð»ÑÐ½Ð¾Ðµ Ð¿Ð»Ð°Ð½Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ ÑÐµÑÐµÐ· ÐºÑÐ¾ÑÑ-ÑÑÐ°ÐµÐºÑÐ¾ÑÐ½ÑÑ ÑÐµÑÐ»ÐµÐºÑÐ¸Ñ", "desc": "Re-TRAC â ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ Ð°Ð³ÐµÐ½ÑÐ¾Ð² Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð±Ð¾Ð»ÑÑÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð¿ÑÐµÐ¾Ð´Ð¾Ð»ÐµÐ²Ð°ÐµÑ Ð¾Ð³ÑÐ°Ð½Ð¸ÑÐµÐ½Ð¸Ñ Ð»Ð¸Ð½ÐµÐ¹Ð½Ð¾Ð³Ð¾ 
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "â¡", "ru": {"title": "Ð¡ÑÐ°Ð±Ð¸Ð»ÑÐ½Ð¾Ðµ ÑÐ°ÑÑÐ¸ÑÐµÐ½Ð¸Ðµ ÑÐ¸ÑÐ¸Ð½Ñ ÑÐµÑÐ¸: Ð±Ð°Ð»Ð°Ð½Ñ ÑÐ¾ÑÑÐ°Ð½ÐµÐ½Ð¸Ñ ÑÐ¸Ð³Ð½Ð°Ð»Ð° Ð¸ Ð½Ð°ÑÑÑÐµÐ½Ð¸Ñ ÑÐ¸Ð¼Ð¼ÐµÑÑÐ¸Ð¸", "desc": "SPARKLING â ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ ÑÐ°ÑÑÐ¸ÑÐµÐ½Ð¸Ñ ÑÐ¸ÑÐ¸Ð½Ñ Ð½ÐµÐ¹ÑÐ¾ÑÐµÑÐµÐ¹ Ð½Ð° Ð¿ÑÐ¾Ð¼ÐµÐ¶ÑÑÐ¾ÑÐ½ÑÑ ÑÑÐ°Ð¿Ð°Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ, ÐºÐ¾ÑÐ¾ÑÑÐ¹ ÑÐ¾ÑÑÐ°Ð½ÑÐµÑ 
[03.02.2026 22:24] Using data from previous issue: {"categories": [], "emoji": "âï¸", "ru": {"title": "ÐÐ°Ð»Ð°Ð½Ñ Ð¼ÐµÐ¶Ð´Ñ ÐºÐ¾Ð½ÑÑÐ¾Ð»ÐµÐ¼ Ð¸ ÐºÐ°ÑÐµÑÑÐ²Ð¾Ð¼: ÐµÐ´Ð¸Ð½Ð°Ñ ÑÐµÐ¾ÑÐ¸Ñ ÑÐ¿ÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ LLM", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¾Ð±ÑÐµÐ´Ð¸Ð½ÑÐµÑ ÑÐ°Ð·Ð»Ð¸ÑÐ½ÑÐµ Ð¼ÐµÑÐ¾Ð´Ñ ÐºÐ¾Ð½ÑÑÐ¾Ð»Ñ Ð½Ð°Ð´ Ð±Ð¾Ð»ÑÑÐ¸Ð¼Ð¸ ÑÐ·ÑÐºÐ¾Ð²ÑÐ¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸ (LLM) - Ð¾Ñ Ð´Ð¾Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð²ÐµÑÐ¾Ð² Ð´Ð¾ LoRA Ð¸ Ð°ÐºÑÐ¸Ð²Ð°ÑÐ¸Ð¾Ð½Ð½ÑÑ Ð²Ð¼ÐµÑÐ°ÑÐµÐ»ÑÑÑÐ² - Ð² ÐµÐ´Ð¸Ð½ÑÑ ÐºÐ¾Ð½ÑÐµÐ¿ÑÑÐ°Ð»ÑÐ½ÑÑ 
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#reasoning", "#optimization", "#rl", "#training"], "emoji": "ð§ ", "ru": {"title": "Ð¡ÐºÑÑÑÐ¾Ðµ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð±Ð¾Ð»ÐµÐµ ÑÐ¼Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹", "desc": "LatentMorph Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸Ð· ÑÐµÐºÑÑÐ°, Ð¸Ð½ÑÐµÐ³ÑÐ¸ÑÑÑ Ð½ÐµÑÐ²Ð½Ð¾Ðµ Ð»Ð°ÑÐµÐ½ÑÐ½Ð¾Ðµ ÑÐ°ÑÑÑÐ¶Ð´Ðµ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#architecture", "#cv", "#benchmark", "#reasoning", "#optimization", "#open_source", "#small_models"], "emoji": "ð", "ru": {"title": "Ð­ÑÑÐµÐºÑÐ¸Ð²Ð½ÑÐµ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ ÑÐµÑÐµÐ· Ð°Ð´Ð°Ð¿ÑÐ¸Ð²Ð½ÑÑ ÑÐµÐºÑÑÑÐ¸Ñ Ð²Ð¼ÐµÑÑÐ¾ ÑÐ°ÑÑÐ¸ÑÐµÐ½Ð¸Ñ ÑÐµÑÐ¸", "desc": "Loop-ViT Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ ÑÐµÐºÑÑÑÐ¸Ð²Ð½ÑÑ Ð°ÑÑÐ¸ÑÐµÐºÑÑÑÑ ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼ÐµÑÐ° Ð´Ð»Ñ Ð·ÑÐµ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#interpretability", "#architecture", "#training"], "emoji": "ð§©", "ru": {"title": "ÐÐ¾Ð»Ð¸Ð½Ð¾Ð¼Ð¸Ð°Ð»ÑÐ½Ð¾Ðµ Ð´ÐµÐºÐ¾Ð´Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ Ð¸Ð½ÑÐµÑÐ¿ÑÐµÑÐ°ÑÐ¸Ð¸ ÐºÐ¾Ð¼Ð¿Ð¾Ð·Ð¸ÑÐ¸Ð¾Ð½Ð½ÑÑ ÑÑÑÑÐºÑÑÑ Ð² Ð½ÐµÐ¹ÑÐ¾ÑÐµÑÑÑ", "desc": "PolySAE ÑÐ»ÑÑÑÐ°ÐµÑ ÑÐ°Ð·ÑÐµÐ¶ÐµÐ½Ð½ÑÐµ Ð°Ð²ÑÐ¾ÐºÐ¾Ð´Ð¸ÑÐ¾Ð²ÑÐ¸ÐºÐ¸ (SAE) Ð¿ÑÑÑÐ¼ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»Ð¸Ð½Ð¾Ð¼Ð¸Ð°Ð»ÑÐ½Ð¾Ð³Ð¾ Ð´ÐµÐºÐ¾Ð´ÐµÑÐ°, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð¿Ð¾Ð·Ð²
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#open_source"], "emoji": "ð¤", "ru": {"title": "Ð¡ÑÑÑÐºÑÑÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐµ Ð½Ð°Ð²ÑÐºÐ¸ Ð´Ð»Ñ Ð¼Ð°ÑÑÑÐ°Ð±Ð¸ÑÑÐµÐ¼ÑÑ ÐºÐ¾Ð¼Ð¿ÑÑÑÐµÑÐ½ÑÑ Ð°Ð³ÐµÐ½ÑÐ¾Ð²", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑÑÑ CUA-Skill â Ð±Ð¾Ð»ÑÑÐ°Ñ Ð±Ð¸Ð±Ð»Ð¸Ð¾ÑÐµÐºÐ° ÑÑÑÑÐºÑÑÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÑ Ð½Ð°Ð²ÑÐºÐ¾Ð² Ð´Ð»Ñ Ð°Ð³ÐµÐ½ÑÐ¾Ð², ÐºÐ¾ÑÐ¾ÑÑÐµ Ð°Ð²ÑÐ¾Ð½Ð¾Ð¼Ð½Ð¾ ÑÐ¿ÑÐ°Ð²Ð»ÑÑÑ ÐºÐ¾Ð¼Ð¿ÑÑÑÐµÑÐ¾Ð¼ ÑÐµÑÐµÐ· Ð³ÑÐ°ÑÐ¸ÑÐµÑÐºÐ¸Ð¹ Ð¸Ð½ÑÐµÑÑÐµÐ¹
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset"], "emoji": "ð¤", "ru": {"title": "ÐÑÐµÐ½ÐºÐ° ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐ¸ AI Ð°Ð³ÐµÐ½ÑÐ¾Ð² ÑÐµÑÐ°ÑÑ ÑÐµÐ°Ð»ÑÐ½ÑÐµ Ð¿Ð¾Ð²ÑÐµÐ´Ð½ÐµÐ²Ð½ÑÐµ Ð·Ð°Ð´Ð°ÑÐ¸", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½ Ð±ÐµÐ½ÑÐ¼Ð°ÑÐº AgentIF-OneDay Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐ¸ AI Ð°Ð³ÐµÐ½ÑÐ¾Ð² Ð²ÑÐ¿Ð¾Ð»Ð½ÑÑÑ ÑÐ°Ð·Ð½Ð¾Ð¾Ð±ÑÐ°Ð·Ð½ÑÐµ Ð¿Ð¾Ð²ÑÐµÐ´Ð½ÐµÐ²Ð½ÑÐµ Ð·Ð°Ð´Ð°ÑÐ¸, Ð¿Ð¾Ð»ÑÑÐ°ÐµÐ¼ÑÐµ ÑÐµÑ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#video", "#cv", "#benchmark", "#reasoning", "#long_context"], "emoji": "ð¨", "ru": {"title": "ÐÐ¾Ð¼Ð¸ÐºÑÑ ÐºÐ°Ðº Ð¾Ð¿ÑÐ¸Ð¼Ð°Ð»ÑÐ½ÑÐ¹ Ð¼Ð¾ÑÑ Ð¼ÐµÐ¶Ð´Ñ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð´Ð»Ñ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½Ð¾Ð³Ð¾ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½Ð¾Ð¼Ñ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark", "#reasoning", "#optimization", "#rl", "#long_context"], "emoji": "âï¸", "ru": {"title": "ÐÐ»Ð¸Ð½Ð½ÑÐµ Ð³Ð¾ÑÐ¸Ð·Ð¾Ð½ÑÑ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°: Ð¾Ð±ÑÑÐµÐ½Ð¸Ðµ Ð°Ð³ÐµÐ½ÑÐ¾Ð² Ð´Ð»Ñ Ð½Ð°Ð´ÑÐ¶Ð½Ð¾Ð³Ð¾ Ð¿Ð»Ð°Ð½Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð¿ÑÑÐµÑÐµÑÑÐ²Ð¸Ð¹", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½ TRIP-Bench â ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½ÑÐ¹ Ð±ÐµÐ½ÑÐ¼Ð°ÑÐº Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#synthetic", "#open_source", "#reasoning"], "emoji": "ð¯", "ru": {"title": "ÐÐµÐ½ÐµÑÐ°ÑÐ¸Ñ ÐºÐ¾Ð½ÐºÑÑÑÐ½ÑÑ Ð·Ð°Ð´Ð°Ñ Ñ ÑÐ¾ÑÐ½ÑÐ¼ ÐºÐ¾Ð½ÑÑÐ¾Ð»ÐµÐ¼ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑÐ¸ Ð´Ð»Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ð¹", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð½Ð¾Ð²ÑÐ¹ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº CoDiQ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð·Ð°Ð´Ð°Ñ Ñ ÐºÐ¾Ð½ÑÑÐ¾Ð»Ð¸ÑÑÐµÐ¼Ð¾Ð¹ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑÑÑ Ð½Ð° ÑÑÐ¾Ð²Ð½Ðµ ÑÐ¾ÑÐµÐ²Ð½Ð¾Ð²Ð°
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#rl", "#rlhf", "#benchmark"], "emoji": "ð", "ru": {"title": "ÐÐ¸Ð½Ð°Ð¼Ð¸ÑÐµÑÐºÐ¸Ðµ ÐºÑÐ¸ÑÐµÑÐ¸Ð¸ Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ ÑÐ²Ð¾ÑÑÐµÑÐºÐ¸Ñ Ð¾ÑÐ²ÐµÑÐ¾Ð² ÑÐµÑÐµÐ· Ð¾Ð±ÑÑÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "Rubric-ARM â ÑÑÐ¾ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº, ÐºÐ¾ÑÐ¾ÑÑÐ¹ ÑÐ¾Ð²Ð¼ÐµÑÑÐ½Ð¾ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð¸ÑÑÐµÑ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ñ ÐºÑÐ¸ÑÐµÑÐ¸ÐµÐ² Ð¾ÑÐµÐ½ÐºÐ¸ Ð¸ ÑÑÐ´ÑÑ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#rl", "#rlhf", "#training"], "emoji": "ð¨", "ru": {"title": "ÐÑÑÑÑ ÑÐ·ÑÐºÐ¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»Ñ ÑÐ»ÑÑÑÐ°ÐµÑ Ð¿Ð¾Ð´ÑÐºÐ°Ð·ÐºÐ¸ Ð² Ð¾Ð±ÑÑÐµÐ½Ð¸Ð¸ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð° PromptRL â ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº, ÐºÐ¾ÑÐ¾ÑÑÐ¹ ÑÐ»ÑÑÑÐ°ÐµÑ Ð¼Ð¾Ð´ÐµÐ»Ð¸ flow matching Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ ÑÐµÐºÑ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#transfer_learning", "#interpretability", "#training"], "emoji": "ð§ ", "ru": {"title": "ÐÐ½ÑÑÑÐµÐ½Ð½ÑÑ ÑÐ¸ÑÑÐµÐ¼Ð° Ð²Ð¾Ð·Ð½Ð°Ð³ÑÐ°Ð¶Ð´ÐµÐ½Ð¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐ°ÑÐºÑÑÑÐ°", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»Ð¸ Ð¾Ð±Ð½Ð°ÑÑÐ¶Ð¸Ð»Ð¸ ÑÐ°Ð·ÑÐµÐ¶ÐµÐ½Ð½ÑÑ Ð¿Ð¾Ð´ÑÐ¸ÑÑÐµÐ¼Ñ Ð²Ð¾Ð·Ð½Ð°Ð³ÑÐ°Ð¶Ð´ÐµÐ½Ð¸Ñ Ð² ÑÐºÑÑÑÑÑ ÑÐ¾ÑÑÐ¾ÑÐ½Ð¸ÑÑ Ð±Ð¾Ð»ÑÑÑÑ ÑÐ·
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#optimization", "#rl", "#training", "#reasoning"], "emoji": "ð§©", "ru": {"title": "Ð Ð°Ð·Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑÐ¸: Ð¾Ñ Ð°Ð±ÑÑÑÐ°ÐºÑÐ½ÑÑ Ð²Ð¾Ð¿ÑÐ¾ÑÐ¾Ð² Ðº Ð¿Ð¾ÑÐ°Ð³Ð¾Ð²ÑÐ¼ ÑÐµÑÐµÐ½Ð¸ÑÐ¼", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑÐ¾Ð´ Adaptive Ability Decomposing (AÂ²D) Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿ÑÐ¾Ð²ÐµÑÑÐµÐ¼ÑÑ Ð²Ð¾
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#inference", "#rl", "#agents"], "emoji": "âï¸", "ru": {"title": "ÐÐ¾Ð¼Ð¿ÑÐ¾Ð¼Ð¸ÑÑÑ ÐºÐ²Ð°Ð½ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð² Ð¼Ð¾Ð´ÐµÐ»ÑÑ Ð¼Ð¸ÑÐ°: Ð¾Ñ ÑÐ¾ÑÐ½Ð¾ÑÑÐ¸ Ðº ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾ÑÑÐ¸", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¸ÑÑÐ»ÐµÐ´ÑÐµÑÑÑ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ Ð¿Ð¾ÑÑ-ÑÑÐµÐ½Ð¸ÑÐ¾Ð²Ð¾ÑÐ½Ð¾Ð³Ð¾ ÐºÐ²Ð°Ð½ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð½Ð° world models, ÐºÐ¾ÑÐ¾ÑÑÐµ Ð¾Ð±ÑÑÐ°ÑÑÑÑ ÐºÐ¾Ð¼Ð¿Ð°ÐºÑÐ½Ð¾ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÑÑ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÑ Ð¾ÐºÑÑÐ¶ÐµÐ½Ð¸Ñ Ð´Ð»
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset"], "emoji": "ð", "ru": {"title": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»ÑÑÐºÐ¸Ð¹ Ð¸Ð½ÑÐµÐ»Ð»ÐµÐºÑ: Ð¾Ñ Ð²ÑÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´ Ðº Ð°Ð²ÑÐ¾Ð½Ð¾Ð¼Ð½Ð¾Ð¼Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ñ Ð´Ð°Ð½Ð½ÑÑ", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ ÐºÐ¾Ð½ÑÐµÐ¿ÑÐ¸Ñ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»ÑÑÐºÐ¾Ð³Ð¾ Ð¸Ð½ÑÐµÐ»Ð»ÐµÐºÑÐ° Ð´Ð»Ñ Ð°Ð³ÐµÐ½ÑÐ¸Ð²Ð½ÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾ÑÐ¾ÑÑÐµ Ð´Ð¾Ð»Ð¶Ð½Ñ ÑÐ°Ð¼Ð¾ÑÑÐ¾ÑÑÐµÐ»ÑÐ½Ð¾ ÑÑ
[03.02.2026 22:24] Using data from previous issue: {"categories": [], "emoji": "ðï¸", "ru": {"title": "ÐÑÑÐ¾ÐºÐ¾Ð¿ÑÐ¾Ð¸Ð·Ð²Ð¾Ð´Ð¸ÑÐµÐ»ÑÐ½Ð°Ñ Ð¿Ð¾ÑÐ¾ÐºÐ¾Ð²Ð°Ñ Ð¾Ð±ÑÐ°Ð±Ð¾ÑÐºÐ° ÑÐµÑÐµÐ²ÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "VoxServe â ÑÑÐ¾ ÑÐ½Ð¸ÑÐ¸ÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð´Ð»Ñ ÑÐ°Ð·Ð²ÑÑÑÑÐ²Ð°Ð½Ð¸Ñ ÑÐµÑÐµÐ²ÑÑ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð¸ÑÑÐµÑ Ð¿Ð¾ÑÐ¾ÐºÐ¾Ð²ÑÑ Ð¾Ð±ÑÐ°Ð±Ð¾ÑÐºÑ Ð°ÑÐ´Ð¸Ð¾Ð´Ð°Ð½Ð½ÑÑ. Ð¡Ð¸ÑÑÐµÐ¼Ð° Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÐµÑ Ð°Ð±ÑÑÑÐ°ÐºÑÐ¸Ñ Ð¼Ð¾Ð´
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#benchmark", "#training", "#small_models"], "emoji": "ð", "ru": {"title": "ÐÑÐµÐ½ÐºÐ° Ð±ÐµÐ· Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð¸: ÑÐºÑÑÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¼Ð°Ð»ÐµÐ½ÑÐºÐ¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð²Ð¼ÐµÑÑÐ¾ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÑÐ´ÐµÐ¹", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑÐ²Ð°ÐµÑ, ÑÑÐ¾ Ð¼Ð°Ð»ÐµÐ½ÑÐºÐ¸Ðµ ÑÐ·ÑÐºÐ¾Ð²ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¼Ð¾Ð³ÑÑ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾ Ð¾ÑÐµÐ½Ð¸Ð²Ð°ÑÑ ÐºÐ°ÑÐµÑÑÐ²Ð¾ Ð¾ÑÐ²ÐµÑÐ¾Ð², Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÑ Ð²Ð½ÑÑ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#leakage", "#benchmark", "#reasoning"], "emoji": "ð¼ï¸", "ru": {"title": "ÐÐ°ÑÑÑÐ°Ð±Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ ÑÐ°Ð·Ð´ÐµÐ»Ð¸ÑÐµÐ»ÐµÐ¹ Ð´Ð»Ñ ÑÑÑÑÐ°Ð½ÐµÐ½Ð¸Ñ ÑÑÐµÑÐºÐ¸ Ð¸Ð½ÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¸ Ð¼ÐµÐ¶Ð´Ñ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð²ÑÑÐ²Ð»ÑÐµÑ Ð¿ÑÐ¾Ð±Ð»ÐµÐ¼Ñ ÑÑÐµÑÐºÐ¸ Ð¸Ð½ÑÐ¾ÑÐ¼Ð°ÑÐ¸Ð¸ Ð¼ÐµÐ¶Ð´Ñ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸ÑÐ¼Ð¸ Ð² Ð±Ð¾Ð»ÑÑÐ¸Ñ Ð²Ð¸Ð´ÐµÐ¾-Ñ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#inference", "#benchmark", "#reasoning", "#diffusion", "#optimization", "#open_source", "#training"], "emoji": "ð³", "ru": {"title": "Ð£Ð¼Ð½Ð¾Ðµ Ð²ÐµÑÐ²Ð»ÐµÐ½Ð¸Ðµ: ÑÑÑÐµÐºÑÐ¸Ð²Ð½ÑÐ¹ Ð¸Ð½ÑÐµÑÑ Ð´Ð¸ÑÐºÑÐµÑÐ½ÑÑ Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾Ð½Ð½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐµÑÐµÐ· Ð¸ÐµÑÐ°ÑÑÐ¸ÑÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ð¸ÑÐº", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Prism Ð´Ð»Ñ Ð¼Ð°ÑÑÑÐ°
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#3d", "#cv"], "emoji": "ð", "ru": {"title": "Ð¡Ð¾Ð²Ð¼ÐµÑÑÐ½Ð°Ñ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ñ Ð»Ð¸Ð½Ð¸Ð¹ Ð¸ Ð¿Ð»Ð¾ÑÐºÐ¾ÑÑÐµÐ¹ Ð´Ð»Ñ ÑÑÑÑÐºÑÑÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ 3D-Ð¾ÑÐ¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ñ", "desc": "LiP-Map Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ ÑÐ¾Ð±Ð¾Ð¹ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº ÑÐ¾Ð²Ð¼ÐµÑÑÐ½Ð¾Ð¹ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ð¸ Ð»Ð¸Ð½Ð¸Ð¹ Ð¸ Ð¿Ð»Ð¾ÑÐºÐ¾ÑÑÐµÐ¹ Ð´Ð»Ñ Ð²Ð¾ÑÑÑÐ°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ ÑÑÑÑÐ¼ÐµÑÐ½ÑÑ Ð»Ð¸Ð½Ð¸Ð¹ Ð¸Ð· Ð¼Ð½Ð¾Ð³Ð¾Ð²Ð¸Ð´Ð¾Ð²ÑÑ RGB-Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹.
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#video", "#architecture", "#inference", "#diffusion", "#optimization"], "emoji": "â¡", "ru": {"title": "Ð¢Ð¾ÑÐ½Ð¾Ðµ Ð²ÑÑÐ¸ÑÐ»ÐµÐ½Ð¸Ðµ Ð²Ð°Ð¶Ð½Ð¾Ð³Ð¾, Ð¿ÑÐ¸Ð±Ð»Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ð¾ÑÑÐ°Ð»ÑÐ½Ð¾Ð³Ð¾: ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ðµ ÑÐ°Ð·ÑÐµÐ¶ÐµÐ½Ð½Ð¾Ðµ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑÐ¾Ð´ PISA, ÐºÐ¾ÑÐ¾ÑÑÐ¹ ÑÐ»ÑÑÑÐ°ÐµÑ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾ÑÑÑ Ð´Ð¸ÑÑÑÐ·Ð¸Ð¾
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#inference", "#video"], "emoji": "â¡", "ru": {"title": "Ð£ÑÐºÐ¾ÑÐµÐ½Ð¸Ðµ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ÑÐµÑÐµÐ· ÑÐ¼Ð½Ð¾Ðµ Ð¿ÑÐ¾ÑÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÑ ÑÐ¾ÐºÐµÐ½Ð¾Ð²", "desc": "VisionTrim Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð±ÐµÐ· Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð´Ð»Ñ ÑÑÐºÐ¾ÑÐµÐ½Ð¸Ñ Ð¼ÑÐ»ÑÑÐ¸Ð¼Ð¾Ð´Ð°Ð»ÑÐ½ÑÑ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿ÑÑÑÐ¼ Ð²ÑÐ±Ð¾ÑÐ° Ð´Ð¾Ð¼Ð¸Ð½Ð°Ð½ÑÐ½
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#reasoning", "#training", "#rl"], "emoji": "ð¯", "ru": {"title": "ÐÐ±ÑÑÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ ÑÐµÑÐµÐ· ÑÐ°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð·Ð°Ð´Ð°Ñ", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ ÑÐ»ÑÑÑÐµÐ½Ð¸Ñ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑÐ¸ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ðº ÑÐ°
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#inference", "#benchmark", "#reasoning", "#optimization", "#training"], "emoji": "âï¸", "ru": {"title": "ÐÑÐµÐ´ÐµÐ»Ñ ÑÐ¶Ð°ÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹: Ð¿Ð¾ÑÐµÐ¼Ñ Ð³ÐµÐ½ÐµÑÐ°ÑÐ¸Ð²Ð½ÑÐ¹ Ð¸Ð½ÑÐµÐ»Ð»ÐµÐºÑ ÑÑÑÐ°Ð´Ð°ÐµÑ Ð±Ð¾Ð»ÑÑÐµ, ÑÐµÐ¼ ÐºÐ»Ð°ÑÑÐ¸ÑÐ¸ÐºÐ°ÑÐ¸Ñ", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¸ÑÑÐ»ÐµÐ´ÑÐµÑÑÑ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ ÑÐ´Ð°Ð»ÐµÐ½Ð¸Ñ ÑÐ»Ð¾ÑÐ² Ð½Ð° Ð±Ð¾Ð»ÑÑÐ¸Ðµ ÑÐ·ÑÐºÐ¾Ð²ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
[03.02.2026 22:24] Using data from previous issue: {"categories": [], "emoji": "ð ï¸", "ru": {"title": "ÐÐ³ÐµÐ½Ñ, ÑÐ¾Ð·Ð´Ð°ÑÑÐ¸Ð¹ ÑÐ²Ð¾Ð¸ ÑÐ¾Ð±ÑÑÐ²ÐµÐ½Ð½ÑÐµ Ð¸Ð½ÑÑÑÑÐ¼ÐµÐ½ÑÑ Ð² Ð¿ÑÐ¾ÑÐµÑÑÐµ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ñ", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð°æ¡æ¶UCT, ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð¿ÑÐµÐ²ÑÐ°ÑÐ°ÐµÑ ÑÐ·ÑÐºÐ¾Ð²ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸Ð· Ð¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°ÑÐµÐ»ÐµÐ¹ Ð¸Ð½ÑÑÑÑÐ¼ÐµÐ½ÑÐ¾Ð² Ð² Ð¸Ñ ÑÐ¾Ð·Ð´Ð°ÑÐµÐ»ÐµÐ¹. Ð¡Ð¸ÑÑÐµÐ¼Ð° Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸ÑÐµÑÐºÐ¸ Ð³ÐµÐ½ÐµÑÐ¸ÑÑÐµÑ Ð¸ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð¸ÑÑÐµÑ ÑÐ¿ÐµÑÐ¸Ð°Ð»Ð¸Ð·Ð¸ÑÐ¾Ð²
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#optimization"], "emoji": "ð", "ru": {"title": "ÐÐ°Ð½Ð¸ÑÐ¾Ð»ÑÐ´Ð½Ð°Ñ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ñ Ñ Ð¿ÑÐ¾ÐµÐºÑÐ¸ÐµÐ¹ Ð¼Ð¾Ð¼ÐµÐ½ÑÐ° Ð´Ð»Ñ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "ÐÐ²ÑÐ¾ÑÑ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÑÑ Ð½Ð¾Ð²ÑÐ¹ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¾Ñ Mano, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð¿ÑÐ¸Ð¼ÐµÐ½ÑÐµÑ Ð¼ÐµÑÐ¾Ð´Ñ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ð¸ Ð½Ð° Ð¼Ð½Ð¾Ð³Ð¾Ð¾Ð±ÑÐ°Ð·Ð¸ÑÑ Ñ Ð¿ÑÐ¾ÐµÐºÑÐ¸ÐµÐ¹ Ð¼Ð¾Ð¼ÐµÐ½ÑÐ° Ð½Ð° ÐºÐ°ÑÐ°ÑÐµÐ»ÑÐ½
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#architecture", "#cv", "#benchmark", "#interpretability", "#training"], "emoji": "ð", "ru": {"title": "Ð­ÑÑÐµÐºÑÐ¸Ð²Ð½Ð°Ñ ÑÐ°Ð·Ð¼ÐµÑÐ½Ð¾ÑÑÑ ÐºÐ°Ðº ÑÐ½Ð¸Ð²ÐµÑÑÐ°Ð»ÑÐ½ÑÐ¹ Ð¿ÑÐµÐ´Ð¸ÐºÑÐ¾Ñ ÐºÐ°ÑÐµÑÑÐ²Ð° Ð½ÐµÐ¹ÑÐ¾Ð½Ð½ÑÑ ÑÐµÑÐµÐ¹", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¸ÑÑÐ»ÐµÐ´ÑÐµÑÑÑ ÑÐ²ÑÐ·Ñ Ð¼ÐµÐ¶Ð´Ñ Ð³ÐµÐ¾Ð¼ÐµÑÑÐ¸ÐµÐ¹ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ð¹ Ð¸ Ð¿ÑÐ¾Ð¸Ð·Ð²Ð¾Ð´Ð¸ÑÐµÐ»ÑÐ½Ð¾ÑÑÑÑ Ð½ÐµÐ¹ÑÐ¾Ð½Ð½ÑÑ ÑÐµ
[03.02.2026 22:24] Using data from previous issue: {"categories": [], "emoji": "ð¨", "ru": {"title": "ÐÐµÐ¿ÑÐµÑÑÐ²Ð½ÑÐµ Ð½ÐµÐ¹ÑÐ¾Ð½Ð½ÑÐµ ÑÐµÐºÑÑÑÑÑ Ð´Ð»Ñ ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ð³Ð¾ ÑÐµÐ½Ð´ÐµÑÐ¸Ð½Ð³Ð°", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·Ð¾Ð²Ð°ÑÑ Ð½ÐµÑÐ²Ð½ÑÐµ Ð½ÐµÐ¹ÑÐ¾Ð½Ð½ÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ (INR) Ð´Ð»Ñ ÑÐµÐºÑÑÑÑ, ÐºÐ¾ÑÐ¾ÑÑÐµ ÑÐ°Ð±Ð¾ÑÐ°ÑÑ Ð² Ð½ÐµÐ¿ÑÐµÑÑÐ²Ð½Ð¾Ð¼ Ð¿ÑÐ¾ÑÑÑÐ°Ð½ÑÑÐ²Ðµ UV-ÐºÐ¾Ð¾ÑÐ´Ð¸Ð½Ð°Ñ Ð²Ð¼ÐµÑÑÐ¾ Ð´Ð¸ÑÐºÑÐµÑÐ½Ð¾Ð³Ð¾ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ. ÐÐ²Ñ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multilingual", "#machine_translation", "#dataset", "#benchmark", "#low_resource", "#data", "#open_source"], "emoji": "â ï¸", "ru": {"title": "ÐÐ¾Ð³Ð´Ð° ÑÑÐ´ÑÐ¸ Ð¾ÑÐ¸Ð±Ð°ÑÑÑÑ: Ð½ÐµÑÑÐ°Ð±Ð¸Ð»ÑÐ½Ð¾ÑÑÑ ÐºÑÐ¾ÑÑ-Ð»Ð¸Ð½Ð³Ð²Ð°Ð»ÑÐ½Ð¾Ð¹ Ð¾ÑÐµÐ½ÐºÐ¸ LLM Ð½Ð° Ð¼Ð¾ÑÑÐ¾Ð»Ð¾Ð³Ð¸ÑÐµÑÐºÐ¸ ÑÐ»Ð¾Ð¶Ð½ÑÑ ÑÐ·ÑÐºÐ°Ñ", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¸ÑÑÐ»ÐµÐ´ÑÐµÑÑÑ Ð½Ð°Ð´ÑÐ¶Ð½Ð¾ÑÑÑ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#rl", "#training", "#small_models"], "emoji": "ð¯", "ru": {"title": "Ð£Ð¼Ð½ÑÐ¹ Ð²ÑÐ±Ð¾Ñ Ð¿Ð¾Ð´ÑÐºÐ°Ð·Ð¾Ðº ÑÐµÑÐµÐ· Ð±Ð°Ð¹ÐµÑÐ¾Ð²ÑÐºÐ¾Ðµ Ð¿ÑÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑÐ¸", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð¼ÐµÑÐ¾Ð´ Generalizable Predictive Prompt Selection (GPS), ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÐµÑ Ð±Ð°Ð¹ÐµÑÐ¾Ð²ÑÐºÐ¸Ð¹ Ð²Ñ
[03.02.2026 22:24] Using data from previous issue: {"categories": [], "emoji": "âï¸", "ru": {"title": "ÐÐ¸Ð°Ð³Ð½Ð¾ÑÑÐ¸ÐºÐ° Ð½Ð°Ð´ÑÐ¶Ð½Ð¾ÑÑÐ¸ LLM-ÑÑÐ´ÐµÐ¹ ÑÐµÑÐµÐ· ÑÐµÐ¾ÑÐ¸Ñ Ð¸Ð·Ð¼ÐµÑÐµÐ½Ð¸Ð¹ Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð· ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑÐ¸", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð´Ð²ÑÑÑÑÐ°Ð¿Ð½Ð°Ñ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑÐ¸ÑÐµÑÐºÐ°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÐµÐ¾ÑÐ¸Ð¸ Ð¾ÑÐ²ÐµÑÐ¾Ð² Ð½Ð° Ð·Ð°Ð´Ð°Ð½Ð¸Ñ Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð³ÑÐ°Ð´ÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÑ Ð¾ÑÐ²ÐµÑÐ¾Ð² Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ Ð½Ð°Ð´ÑÐ¶Ð½Ð¾ÑÑÐ¸ LLM-as-a-
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#reasoning", "#alignment", "#optimization", "#rl", "#rlhf", "#training"], "emoji": "ð¯", "ru": {"title": "ÐÐµÐ· Ð¾ÑÑÐµÑÐµÐ½Ð¸Ð¹ â Ðº ÑÑÐ°Ð±Ð¸Ð»ÑÐ½Ð¾Ð¹ Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð°ÑÐ¸Ð¸ Ð¿Ð¾Ð»Ð¸ÑÐ¸ÐºÐ¸ Ð² LLM", "desc": "ÐÐ²ÑÐ¾ÑÑ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÑÑ Clipping-Free Policy Optimization (CFPO) â Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ Ð´Ð»Ñ Ð¾Ð±ÑÑÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ 
[03.02.2026 22:24] Using data from previous issue: {"categories": [], "emoji": "ð¨", "ru": {"title": "Ð¡Ð¿ÐµÐºÑÑÐ°Ð»ÑÐ½ÑÐµ Ð»Ð¾Ð²ÑÑÐºÐ¸ VAE-Ð¸Ð½Ð¿ÐµÐ¹Ð½ÑÐ¸Ð½Ð³Ð°: Ð¾Ñ Ð¾Ð±Ð½Ð°ÑÑÐ¶ÐµÐ½Ð¸Ñ Ð°ÑÑÐµÑÐ°ÐºÑÐ¾Ð² Ðº ÐºÐ¾Ð½ÑÐµÐ½Ñ-Ð¾ÑÐ¸ÐµÐ½ÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð´ÐµÑÐµÐºÑÐ¸Ð¸", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¸ÑÑÐ»ÐµÐ´ÑÐµÑÑÑ Ð¿ÑÐ¾Ð±Ð»ÐµÐ¼Ð° Ð´ÐµÑÐµÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±ÑÐ°Ð¶ÐµÐ½Ð¸Ð¹, Ð¾ÑÑÐµÐ´Ð°ÐºÑÐ¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÑ Ñ Ð¿Ð¾Ð¼Ð¾ÑÑÑ VAE-based Ð¸Ð½Ð¿ÐµÐ¹Ð½ÑÐ¸Ð½Ð³Ð°. ÐÐ²ÑÐ¾ÑÑ Ð¾Ð±Ð½Ð°ÑÑÐ¶Ð¸Ð»Ð¸, ÑÑÐ¾ ÑÐ¾Ð²ÑÐµÐ¼ÐµÐ½
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#data", "#dataset", "#synthetic", "#open_source", "#audio"], "emoji": "ðµ", "ru": {"title": "Ð§Ð¸ÑÑÑÐµ Ð´Ð°Ð½Ð½ÑÐµ Ð²Ð¼ÐµÑÑÐ¾ Ð±Ð¾Ð»ÑÑÐ¸Ñ Ð¾Ð±ÑÑÐ¼Ð¾Ð²: ÑÑÑÐµÐºÑÐ¸Ð²Ð½Ð¾Ðµ ÑÐ°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð·Ð²ÑÐºÐ° ÑÐµÑÐµÐ· Ð²ÑÑÐ¾ÐºÐ°ÑÐµÑÑÐ²ÐµÐ½Ð½ÑÐ¹ ÑÐ¸Ð½ÑÐµÐ·", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸Ð·Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐ¹ ÐºÐ¾Ð½Ð²ÐµÐ¹ÐµÑ Ð´Ð»Ñ ÑÐ°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ Ð·Ð²ÑÐºÐ¾Ð²ÑÑ ÑÐ¸Ð³Ð½Ð°Ð»Ð¾Ð², ÐºÐ¾
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#training"], "emoji": "â¡", "ru": {"title": "ÐÐ°ÑÐ°Ð»Ð»ÐµÐ»ÑÐ½Ð°Ñ Ð¾Ð±ÑÐ°Ð±Ð¾ÑÐºÐ° Ð²ÑÐµÐ¼ÐµÐ½Ð¸: ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ Ð·Ð°ÑÑÐ°Ñ Ð±ÐµÐ· Ð¿Ð¾ÑÐµÑÐ¸ ÐºÐ°ÑÐµÑÑÐ²Ð°", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð° Parallel Echo State Network (ParalESN) â Ð½Ð¾Ð²ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº reservoir computing, ÑÐµÑÐ°ÑÑÐ¸Ð¹ Ð¿ÑÐ¾Ð±Ð»ÐµÐ¼Ñ Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#transfer_learning", "#optimization", "#rl", "#training"], "emoji": "ð§ ", "ru": {"title": "ÐÐµÑÐ±Ð°Ð»ÑÐ½Ð°Ñ Ð´Ð¸ÑÑÐ¸Ð»Ð»ÑÑÐ¸Ñ: Ð¿ÐµÑÐµÐ´Ð°ÑÐ° Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð±ÐµÐ· Ð¾Ð³ÑÐ°Ð½Ð¸ÑÐµÐ½Ð¸Ð¹ ÑÐ¾ÐºÐµÐ½-ÑÑÐ¾Ð²Ð½ÐµÐ²Ð¾Ð³Ð¾ Ð²ÑÑÐ°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ñ", "desc": "Ð¡ÑÐ°ÑÑÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð¼ÐµÑÐ¾Ð´ On-policy Verbal Distillation (OVD), ÐºÐ¾ÑÐ¾Ñ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multilingual", "#data", "#rl", "#optimization", "#training", "#small_models"], "emoji": "ð¯", "ru": {"title": "Ð£Ð¼Ð½Ð¾Ðµ Ð¿ÐµÑÐµweighting: ÐºÐ¾Ð³Ð´Ð° Ð¼Ð°ÑÐ¸Ð½Ð° Ð²ÑÐ±Ð¸ÑÐ°ÐµÑ, ÐºÐ°ÐºÐ¸Ðµ Ð´Ð°Ð½Ð½ÑÐµ ÑÑÐ¸ÑÑ", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑÐµÐ¼Ð° Inf-DDS, ÐºÐ¾ÑÐ¾ÑÐ°Ñ Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÐµÑ Ð¾Ð±ÑÑÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ Ð°Ð´Ð°Ð¿ÑÐ¸Ð²
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "â ï¸", "ru": {"title": "Ð£ÑÐ·Ð²Ð¸Ð¼Ð¾ÑÑÑ LLM-ÑÑÐ´ÐµÐ¹ Ð¿ÐµÑÐµÐ´ Ð¼Ð°Ð½Ð¸Ð¿ÑÐ»ÑÑÐ¸ÑÐ¼Ð¸ Ð² ÑÐµÐ¿Ð¾ÑÐºÐ°Ñ ÑÐ°ÑÑÑÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð°Ð³ÐµÐ½ÑÐ¾Ð²", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð½Ð¾, ÑÑÐ¾ Ð±Ð¾Ð»ÑÑÐ¸Ðµ ÑÐ·ÑÐºÐ¾Ð²ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸, Ð¸ÑÐ¿Ð¾Ð»ÑÐ·ÑÐµÐ¼ÑÐµ Ð´Ð»Ñ Ð¾ÑÐµÐ½ÐºÐ¸ Ð¿ÑÐ¾Ð¸Ð·Ð²Ð¾Ð´Ð¸ÑÐµÐ»ÑÐ½Ð¾ÑÑÐ¸ Ð°Ð³ÐµÐ½ÑÐ¾Ð², ÑÑÐ·Ð²Ð¸Ð¼Ñ Ð´Ð»Ñ Ð¼Ð°Ð½Ð¸Ð¿ÑÐ»ÑÑÐ¸Ð¹ Ñ ÑÐµÐ¿Ð¾ÑÐºÐ°Ð¼Ð¸ ÑÐ°ÑÑÑÐ¶Ð´
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#architecture", "#training"], "emoji": "ð¯", "ru": {"title": "ÐÑÐµÐ´ÑÐºÐ°Ð·Ð°ÑÐµÐ»ÑÐ½ÑÐµ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¸ Ð°Ð´Ð°Ð¿ÑÐ¸Ð²Ð½Ð°Ñ ÐºÐ²Ð°Ð½ÑÐ¸Ð·Ð°ÑÐ¸Ñ Ð´Ð»Ñ ÑÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°ÑÐ¸Ð¹", "desc": "ReSID Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÑÐµÑ Ð¸Ð½Ð½Ð¾Ð²Ð°ÑÐ¸Ð¾Ð½Ð½ÑÐ¹ Ð¿Ð¾Ð´ÑÐ¾Ð´ Ðº Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»ÑÐ½ÑÐ¼ ÑÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°ÑÐ¸ÑÐ¼, ÐºÐ¾ÑÐ¾ÑÑÐ¹ Ð¾Ð±ÑÑÐ°ÐµÑ Ð¿ÑÐµÐ´ÑÑÐ°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¿ÑÐµÐ´Ð¼ÐµÑÐ¾Ð², Ð¾Ð¿ÑÐ¸Ð¼Ð¸Ð·Ð¸ÑÐ¾Ð²Ð°Ð½Ð½ÑÐµ Ð´Ð»Ñ Ð¿Ñ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#architecture", "#hallucinations", "#interpretability", "#open_source", "#training"], "emoji": "ð", "ru": {"title": "ÐÐ½ÑÑÑÐµÐ½Ð½ÐµÐµ Ð²Ð¸Ð´ÐµÐ½Ð¸Ðµ LLM: ÑÐ°Ð¼Ð¾Ð¿ÑÐ¾Ð²ÐµÑÐºÐ° ÑÐµÑÐµÐ· Ð°Ð½Ð°Ð»Ð¸Ð· Ð³Ð»ÑÐ±Ð¸Ð½Ð½Ð¾Ð¹ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ Ð°ÐºÑÐ¸Ð²Ð°ÑÐ¸Ð¹", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑÐ¾Ð´ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð²Ð½ÑÑÑÐµÐ½Ð½ÐµÐ¹ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ Ð±Ð¾Ð»ÑÑÐ¸Ñ ÑÐ·ÑÐºÐ¾Ð²ÑÑ Ð¼Ð¾Ð´ÐµÐ»Ðµ
[03.02.2026 22:24] Using data from previous issue: {"categories": [], "emoji": "ð§¬", "ru": {"title": "ÐÐ°ÑÑÐ½Ð°Ñ ÐÐÐ Ð°Ð³ÐµÐ½ÑÐ¾Ð²: Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑÐ°Ð»Ð¸Ð·Ð°ÑÐ¸Ñ Ð´Ð»Ñ Ð¾ÑÐºÑÑÑÐ¸Ð¹ Ð² Ð¼Ð¾Ð»ÐµÐºÑÐ»ÑÑÐ½Ð¾Ð¹ ÑÐ¸Ð¼Ð¸Ð¸", "desc": "Ð ÑÐ°Ð±Ð¾ÑÐµ Ð¿ÑÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° ÑÐ¸ÑÑÐµÐ¼Ð° INDIBATOR Ð´Ð»Ñ Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸Ð·Ð°ÑÐ¸Ð¸ Ð¼Ð¾Ð»ÐµÐºÑÐ»ÑÑÐ½ÑÑ Ð¾ÑÐºÑÑÑÐ¸Ð¹, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð½Ð° Ð¼Ð½Ð¾Ð³Ð¾Ð°Ð³ÐµÐ½ÑÐ½Ð¾Ð¼ Ð¿Ð¾Ð´ÑÐ¾Ð´Ðµ. ÐÐ³ÐµÐ½ÑÑ Ð² ÑÐ¸ÑÑÐµÐ¼Ðµ ÑÐ°ÑÐ°ÐºÑÐµÑÐ¸Ð·ÑÑÑÑÑ Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑÐ°Ð»ÑÐ½ÑÐ¼Ð¸ Ð¿Ñ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#agents", "#benchmark", "#low_resource", "#alignment", "#synthetic", "#open_source"], "emoji": "ð", "ru": {"title": "ÐÑÐ»ÑÑÑÑÐ½Ð¾-Ð°Ð´Ð°Ð¿ÑÐ¸Ð²Ð½ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑÐ¸ Ð´Ð»Ñ Ð®Ð³Ð¾-ÐÐ¾ÑÑÐ¾ÑÐ½Ð¾Ð¹ ÐÐ·Ð¸Ð¸", "desc": "ÐÑÑÐ»ÐµÐ´Ð¾Ð²Ð°ÑÐµÐ»Ð¸ ÑÐ°Ð·ÑÐ°Ð±Ð¾ÑÐ°Ð»Ð¸ Ð°Ð³ÐµÐ½ÑÐ½ÑÐ¹ ÑÑÐµÐ¹Ð¼Ð²Ð¾ÑÐº Ð´Ð»Ñ Ð°Ð²ÑÐ¾Ð¼Ð°ÑÐ¸ÑÐµÑÐºÐ¾
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#optimization", "#cv", "#multimodal", "#architecture"], "emoji": "ð", "ru": {"title": "ÐÐ°ÑÐ°Ð±Ð¾Ð»Ð¸ÑÐµÑÐºÐ¾Ðµ ÐºÐ¾Ð´Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾Ð·Ð¸ÑÐ¸Ð¹ Ð´Ð»Ñ ÑÐ½Ð¸Ð²ÐµÑÑÐ°Ð»ÑÐ½ÑÑ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÑ ÑÑÐ°Ð½ÑÑÐ¾ÑÐ¼ÐµÑÐ¾Ð²", "desc": "Ð ÑÑÐ°ÑÑÐµ Ð¿ÑÐµÐ´Ð»Ð°Ð³Ð°ÐµÑÑÑ Ð½Ð¾Ð²ÑÐ¹ Ð¼ÐµÑÐ¾Ð´ ÐºÐ¾Ð´Ð¸ÑÐ¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ð¾Ð·Ð¸ÑÐ¸Ð¹ (PaPE) Ð´Ð»Ñ Ð²Ð¸Ð·ÑÐ°Ð»ÑÐ½ÑÑ Ð¼Ð¾Ð´Ð°Ð»ÑÐ½Ð¾ÑÑÐµÐ¹, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½ÑÐ¹ Ð½Ð° Ð¿Ð°Ñ
[03.02.2026 22:24] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#cv", "#open_source"], "emoji": "ð¯", "ru": {"title": "YOLO26 Ð²ÑÑÑÐµÑÐ°ÐµÑ Ð¾ÑÐºÑÑÑÑÑ Ð»ÐµÐºÑÐ¸ÐºÑ: ÑÐ½Ð¸Ð²ÐµÑÑÐ°Ð»ÑÐ½Ð°Ñ ÑÐµÐ³Ð¼ÐµÐ½ÑÐ°ÑÐ¸Ñ Ð² ÑÐµÐ°Ð»ÑÐ½Ð¾Ð¼ Ð²ÑÐµÐ¼ÐµÐ½Ð¸", "desc": "YOLOE-26 Ð¾Ð±ÑÐµÐ´Ð¸Ð½ÑÐµÑ Ð°ÑÑÐ¸ÑÐµÐºÑÑÑÑ YOLO26 Ñ Ð¿Ð°ÑÐ°Ð´Ð¸Ð³Ð¼Ð¾Ð¹ Ð¾ÑÐºÑÑÑÐ¾Ð¹ Ð»ÐµÐºÑÐ¸ÐºÐ¸ Ð´Ð»Ñ ÑÐµÐ³Ð¼ÐµÐ½ÑÐ°ÑÐ¸Ð¸ ÑÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑÐ¾Ð² Ð² ÑÐµÐ°Ð»ÑÐ½Ð¾
[03.02.2026 22:24] Renaming data file.
[03.02.2026 22:24] Renaming previous data. hf_papers.json to ./d/2026-02-03.json
[03.02.2026 22:24] Saving new data file.
[03.02.2026 22:24] Generating page.
[03.02.2026 22:24] Renaming previous page.
[03.02.2026 22:24] Renaming previous data. index.html to ./d/2026-02-03.html
[03.02.2026 22:24] Writing result.
[03.02.2026 22:24] Renaming log file.
[03.02.2026 22:24] Renaming previous data. log.txt to ./logs/2026-02-03_last_log.txt
