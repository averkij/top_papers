[03.02.2026 18:46] Read previous papers.
[03.02.2026 18:46] Generating top page (month).
[03.02.2026 18:46] Writing top page (month).
[03.02.2026 19:41] Read previous papers.
[03.02.2026 19:41] Get feed.
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00919
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02276
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22060
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02185
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02084
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02437
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02053
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01566
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02361
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01590
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02493
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02383
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02488
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01624
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01756
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02214
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01395
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01058
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01801
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02092
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01541
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01335
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01851
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01538
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02486
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01576
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02472
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02343
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02227
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01479
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02156
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01322
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20613
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02453
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01675
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01660
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01511
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01382
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00986
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00759
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02110
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02039
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22588
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01842
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01296
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01077
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22674
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01997
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01984
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01983
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23000
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00130
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02354
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02287
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01970
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00521
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22801
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00192
[03.02.2026 19:41] Extract page data from URL. URL: https://huggingface.co/papers/2601.22599
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22296
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21968
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21759
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14691
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02338
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01897
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01815
[03.02.2026 19:41] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01618
[03.02.2026 19:41] Extract page data from URL. URL: https://huggingface.co/papers/2602.01418
[03.02.2026 19:41] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[03.02.2026 19:41] No deleted papers detected.
[03.02.2026 19:41] Downloading and parsing papers (pdf, html). Total: 68.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.00919.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.00919.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.00919.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02276.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02276.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02276.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2601.22060.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2601.22060.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2601.22060.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02185.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02185.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02185.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02084.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02084.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02084.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02437.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02437.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02437.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02053.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02053.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02053.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01566.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01566.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01566.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02361.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02361.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02361.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01590.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01590.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01590.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02493.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02493.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02493.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02383.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02383.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02383.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02488.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02488.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02488.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01624.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01624.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01624.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01756.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01756.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01756.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02214.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02214.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02214.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01395.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01395.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01395.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01058.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01058.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01058.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01801.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01801.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01801.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02092.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02092.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02092.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01541.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01541.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01541.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01335.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01335.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01335.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01851.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01851.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01851.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01538.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01538.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01538.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02486.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02486.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02486.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01576.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01576.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01576.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02472.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02472.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02472.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02343.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02343.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02343.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02227.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02227.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02227.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01479.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01479.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01479.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02156.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02156.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02156.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01322.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01322.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01322.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2601.20613.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2601.20613.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2601.20613.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02453.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02453.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02453.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01675.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01675.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01675.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01660.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01660.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01660.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01511.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01511.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01511.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01382.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01382.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01382.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.00986.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.00986.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.00986.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.00759.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.00759.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.00759.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02110.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02110.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02110.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02039.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02039.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02039.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2601.22588.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2601.22588.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2601.22588.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01842.
[03.02.2026 19:41] Downloading paper 2602.01842 from https://arxiv.org/pdf/2602.01842v1...
[03.02.2026 19:41] Failed to download and parse paper https://huggingface.co/papers/2602.01842: 'LTChar' object is not iterable
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01296.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01296.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01296.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01077.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01077.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01077.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2601.22674.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2601.22674.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2601.22674.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01997.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01997.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01997.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01984.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01984.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01984.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01983.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01983.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01983.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2601.23000.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2601.23000.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2601.23000.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.00130.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.00130.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.00130.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02354.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02354.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02354.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02287.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02287.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02287.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01970.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01970.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01970.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.00521.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.00521.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.00521.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2601.22801.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2601.22801.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2601.22801.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.00192.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.00192.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.00192.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2601.22599.
[03.02.2026 19:41] Downloading paper 2601.22599 from https://arxiv.org/pdf/2601.22599v1...
[03.02.2026 19:41] Extracting affiliations from text.
[03.02.2026 19:41] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"A Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Kai Li * 1 2 Jintao Cheng * 1 Chang Zeng 2 Zijun Yan 1 Helin Wang 3 Zixiong Su 2 Bo Zheng 2 Xiaolin Hu 1 "
[03.02.2026 19:41] Response: ```python
[]
```
[03.02.2026 19:41] Extracting affiliations from text.
[03.02.2026 19:41] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"A Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Kai Li * 1 2 Jintao Cheng * 1 Chang Zeng 2 Zijun Yan 1 Helin Wang 3 Zixiong Su 2 Bo Zheng 2 Xiaolin Hu 11. Introduction 6 2 0 2 0 3 ] . [ 1 9 9 5 2 2 . 1 0 6 2 : r Query-based universal sound separation is fundamental to intelligent auditory systems, aiming to isolate specific sources from mixtures. Despite recent advances, existing methods continue to suffer from residual interference in complex acoustic scenes. This performance limitation stems largely from data bottleneck: in-the-wild datasets contain weak labels and severe co-occurrence of events. These flaws induce models to learn spurious correlations between background noise and target categories instead of robust acoustic features. To address this, we propose an automated pipeline that eliminates co-occurrence of events by mining high-purity single-event segments from in-the-wild datasets via semantically consistent synthesis protocol. Utilizing this pipeline, we constructed Hive, high-quality synthetic dataset comprising 2.4k hours of raw audio. Experimental results demonstrate that, compared with the state-of-the-art model SAM-Audio which was trained on huge dataset 500 times larger than Hive, certain open-source models trained on Hive achieve competitive separation accuracy and perceptual quality. Moreover, these models exhibited remarkable zero-shot generalization on out-ofdistribution evaluation benchmarks. These findings highlight that prioritizing purity of supervised signals enables significant data efficiency, offering new paradigm for training robust auditory foundation models with reduced computational costs. Code and dataset are available at https://shandaai.github.io/Hive. The work was done while Kai Li is an intern there *Equal contribution 1Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing, China. 2Shanda AI Research Tokyo. 3Johns Hopkins University. 4Tsinghua Laboratory of Brain and Intelligence (THBI), IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China. 5Chinese Institute for Brain Research (CIBR), Beijing, China. Correspondence to: Xiaolin Hu <xlhu@tsinghua.edu.cn>. Technical Report. 1 Real-world acoustic environments are inherently polyphonic and contain multiple overlapping sound events (Heittola et al., 2013). While the human auditory system effectively isolates target sources from mixtures, capability known as the Cocktail Party Effect (Cherry, 1953; Arons, 1992), previous computational methods have focused on restricted domains like speech (Li et al., 2023; 2025) or music (Uhlich et al., 2024). Recently, computational auditory scene analysis (CASA) has expanded toward query-based universal sound separation (USS) (Lee et al., 2025a; Wang et al., 2025). In contrast to domain-specific methods, query-based USS targets arbitrary sound categories, including environmental and mechanical events. This capability is pivotal for applications such as immersive audio rendering (Gupta et al., 2022), machine hearing (Lyon, 2010), and intelligent audio editing (Yan et al., 2025). Conventional blind source separation (Li et al., 2023) is constrained by fixed output cardinality and the permutation problem, limiting its applicability to open-domain scenarios (Li et al., 2025). To address these limitations, query-based universal sound separation has emerged, leveraging multimodal prompts (e.g., text, audio, visual) to explicitly target specific sources (Kavalerov et al., 2019). Early approaches adopted discriminative paradigms, ranging from end-to-end architectures (Liu et al., 2022) to weakly supervised visual bridging (Dong et al., 2023) and large-scale open-domain frameworks (Liu et al., 2024). Recently, the field has shifted toward generative paradigms, such as FlowSep utilizing flow matching (Yuan et al., 2025), while DGMO (Lee et al., 2025b), ZeroSep (Huang et al., 2025) and ZETA (Manor & Michaeli, 2024) explore unsupervised strategies via testtime optimization or zero-shot settings. More recently, research has advanced toward unified prompting, where frameworks like OmniSep (Cheng et al., 2025) and SAM-Audio (Shi et al., 2025) integrate diverse modalities, including text, vision, and time segments, aiming to enhance controllability and facilitate separation-on-demand. Despite architectural evolution across both discriminative and generative paradigms, query-based USS methods persistently exhibit residual interference, characterized by the audible leakage of background noise or concurrent events Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation Table 1. Overview of the original dataset and mixing pipelines adopted by USS methods. Train. Dur. and Val. & Test Dur. refer to the total duration of the original unmixed training set and the combined validation and test sets, respectively. Principled mix: whether the mixing process follows explicit semantic or acoustic constraints rather than random combinations. Pub. mixed data and Pub. mix tool: whether pre-mixed data and mixing code are publicly released. (= yes, = partially, = no) Train. Dur. (h) Val. & Test Dur. (h) Sample-rate Single-label Principled mix Pub. mixed data Pub. mix tool 32k 16k 32k 16k 16k 48k (cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35) Dataset (Liu et al., 2022) (Dong et al., 2023) (Liu et al., 2024) (Yuan et al., 2025) (Cheng et al., 2025) (Shi et al., 2025) 17.3 550 14,100 1,680 560 1,000, 4 50 109 9 10 47 Hive 2,442 292 44.1k into the separated output (Huang et al., 2024; Hai et al., 2024; Lee et al., 2025b). We attribute this deficiency to systematic biases in training data. Current training paradigms predominantly rely on large-scale in-the-wild datasets, such as AudioSet (Gemmeke et al., 2017) and VGGSound (Chen et al., 2020), due to their vast scale and category diversity. However, these datasets inevitably suffer from weak labels and high co-occurrence of events, resulting in severe label-signal misalignment (Fonseca et al., 2021). Consider the case where Rain clips frequently contain concurrent wind or traffic: lacking fine-grained supervision, models inevitably hallucinate these environmental artifacts as inherent acoustic characteristics of the target category. This systematic bias causes the model to treat background noise as part of the desired signal. Given these constraints, the scarci"
[03.02.2026 19:41] Mistral response. {"id": "6d5b335c97814cabbeb11089df37bc29", "created": 1770147717, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1677, "total_tokens": 1776, "completion_tokens": 99, "num_cached_tokens": 1676}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing, China\",\n    \"Shanda AI Research Tokyo\",\n    \"Johns Hopkins University\",\n    \"Tsinghua Laboratory of Brain and Intelligence (THBI), IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China\",\n    \"Chinese Institute for Brain Research (CIBR), Beijing, China\"\n]\n```"}}]}
[03.02.2026 19:41] Response: ```python
[
    "Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing, China",
    "Shanda AI Research Tokyo",
    "Johns Hopkins University",
    "Tsinghua Laboratory of Brain and Intelligence (THBI), IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China",
    "Chinese Institute for Brain Research (CIBR), Beijing, China"
]
```
[03.02.2026 19:41] Deleting PDF ./assets/pdf/2601.22599.pdf.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2601.22296.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2601.22296.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2601.22296.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2601.21968.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2601.21968.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2601.21968.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2601.21759.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2601.21759.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2601.21759.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2601.14691.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2601.14691.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2601.14691.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.02338.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.02338.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.02338.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01897.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01897.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01897.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01815.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01815.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01815.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01618.
[03.02.2026 19:41] Extra JSON file exists (./assets/json/2602.01618.json), skip PDF parsing.
[03.02.2026 19:41] Paper image links file exists (./assets/img_data/2602.01618.json), skip HTML parsing.
[03.02.2026 19:41] Success.
[03.02.2026 19:41] Downloading and parsing paper https://huggingface.co/papers/2602.01418.
[03.02.2026 19:41] Downloading paper 2602.01418 from https://arxiv.org/pdf/2602.01418v1...
[03.02.2026 19:42] Extracting affiliations from text.
[03.02.2026 19:42] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Where to Attend: Principled Vision-Centric Position Encoding with Parabolas Christoffer Koo Øhrstrøm 1 Rafael I. Cabral Muchacho 2 Yifei Dong 2 Filippos Moumtzidellis 1 Ronja uldenring 1 Florian T. Pokorny 2 Lazaros Nalpantidis 1 6 2 0 2 1 ] . [ 1 8 1 4 1 0 . 2 0 6 2 : r Figure 1. Parabolic Position Encoding (PaPE). (a) PaPE encodes positions using an attention bias based on sum of learnable parabolas with the relative position between tokens as the dependent variable. (b) Our experiments show that PaPE is generaleither PaPE or PaPE-RI outperforms all baselines on 7 out of 8 datasets that span 4 vision modalities. (c) PaPE has remarkable classification extrapolation, showing high robustness beyond the training resolution of 2242. "
[03.02.2026 19:42] Response: ```python
[
    "Royal Institute of Technology (KTH)",
    "Lund University"
]
```
[03.02.2026 19:42] Deleting PDF ./assets/pdf/2602.01418.pdf.
[03.02.2026 19:42] Success.
[03.02.2026 19:42] Enriching papers with extra data.
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 0. Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.  					AI-generated summary 				 We introduce Green-VLA, a staged Vision-Language-Action (...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 1. Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.  					AI-generated summary 				 We introduce Kimi K2.5, an open-source multimodal agentic model designed to adva...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 2. Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning.  					AI-generated summary 				 Multimodal large langu...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 3. Vision-DeepResearch benchmark addresses limitations in evaluating visual-textual search capabilities of multimodal models by introducing realistic evaluation conditions and improving visual retrieval through multi-round cropped-search workflow.  					AI-generated summary 				 Multimodal Large Langua...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 4. RPG-Encoder framework transforms repository comprehension and generation into a unified cycle by encoding code into high-fidelity Repository Planning Graph representations that improve understanding and reconstruction accuracy.  					AI-generated summary 				 Current repository agents encounter a re...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 5. UniReason integrates text-to-image generation and image editing through a dual reasoning paradigm that enhances planning with world knowledge and uses editing for visual refinement, achieving superior performance on reasoning-intensive benchmarks.  					AI-generated summary 				 Unified multimodal m...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 6. WildGraphBench evaluates GraphRAG performance in realistic scenarios using Wikipedia's structured content to assess multi-fact aggregation and summarization capabilities across diverse document types.  					AI-generated summary 				 Graph-based Retrieval-Augmented Generation (GraphRAG) organizes ext...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 7. A file-system-based dual-agent framework enables large language model agents to perform extended research tasks beyond context window limitations by using persistent storage as external memory.  					AI-generated summary 				 Deep research is emerging as a representative long-horizon task for large ...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 8. A scalable framework for constructing real-world software engineering environments from GitHub pull requests using an efficient building agent with self-verification and hacking detection capabilities.  					AI-generated summary 				 We propose SWE-Universe, a scalable and efficient framework for au...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 9. Deep Research Agents demonstrate capabilities in autonomous information retrieval but show significant gaps when evaluated against expert-level Wikipedia articles using a new live benchmark and comprehensive evaluation framework.  					AI-generated summary 				 Deep Research Agents (DRAs) have demon...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 10. PixelGen is a pixel-space diffusion framework that uses perceptual supervision through LPIPS and DINO-based losses to generate high-quality images without requiring VAEs or latent representations.  					AI-generated summary 				 Pixel diffusion generates images directly in pixel space in an end-to-e...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 11. SLIME is a novel reference-free alignment objective for large language models that decouples preference learning from generation quality through a three-pronged approach combining likelihood maximization, probability stabilization, and dual-margin constraints.  					AI-generated summary 				 Direct ...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 12. RLAnything enhances reinforcement learning for LLMs and agents through dynamic model optimization and closed-loop feedback mechanisms that improve policy and reward model training.  					AI-generated summary 				 We propose RLAnything, a reinforcement learning framework that dynamically forges envir...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 13. PISCES is an annotation-free text-to-video generation method that uses dual optimal transport-aligned rewards to improve visual quality and semantic alignment without human preference annotations.  					AI-generated summary 				 Text-to-video (T2V) generation aims to synthesize videos with high visu...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 14. Mind-Brush presents a unified agentic framework for text-to-image generation that dynamically retrieves multimodal evidence and employs reasoning tools to improve understanding of implicit user intentions and complex knowledge reasoning.  					AI-generated summary 				 While text-to-image generation...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 15. A novel Causal Forcing method addresses the architectural gap in distilling bidirectional video diffusion models into autoregressive models by using AR teachers for ODE initialization, significantly improving video generation performance.  					AI-generated summary 				 To achieve real-time interact...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 16. Selective knowledge distillation in autoregressive language models using student-entropy-guided position selection improves accuracy and efficiency while reducing memory and storage requirements.  					AI-generated summary 				 Growing efforts to improve knowledge distillation (KD) in large language...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 17. Post-training of reasoning large language models can be improved by correcting distribution mismatches between supervised fine-tuning and reinforcement learning stages through importance sampling reweighting of the SFT loss.  					AI-generated summary 				 Post-training of reasoning LLMs is a holist...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 18. Autoregressive video diffusion models face efficiency challenges due to growing KV caches and redundant attention computations, which are addressed through TempCache, AnnCA, and AnnSA techniques that reduce computational demands while maintaining visual quality and stable performance.  					AI-gener...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 19. FSVideo is a fast transformer-based image-to-video diffusion framework that uses a compressed video autoencoder, diffusion transformer architecture with enhanced layer memory, and multi-resolution generation strategy to achieve high performance with significantly reduced computation time.  					AI-g...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 20. MLLMs equipped with Cognitive Supersensing and Latent Visual Imagery Prediction demonstrate enhanced cognitive reasoning capabilities through integrated visual and textual reasoning pathways.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved remarkable success in...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 21. Visual metaphor transfer enables creative AI systems to decompose abstract conceptual relationships from reference images and reapply them to new subjects through a multi-agent framework grounded in cognitive theory.  					AI-generated summary 				 A visual metaphor constitutes a high-order form of ...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 22. Visual Instruction Benchmark for Image Editing introduces a three-level interaction hierarchy for evaluating visual instruction following capabilities in generative models.  					AI-generated summary 				 Recent generative models have achieved remarkable progress in image editing. However, existing ...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 23. A dual-stream framework called InteractAvatar is presented for generating talking avatars that can interact with objects in their environment, addressing challenges in grounded human-object interaction through decoupled perception and planning modules.  					AI-generated summary 				 Generating talk...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 24. Re-TRAC is an agentic framework that enhances LLM-based research agents by enabling cross-trajectory exploration and iterative reflection through structured state representations, leading to more efficient and effective problem-solving compared to traditional ReAct approaches.  					AI-generated sum...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 25. Visual world models for mobile GUI agents are improved through renderable code generation using vision-language models, achieving better performance with reduced model size compared to existing approaches.  					AI-generated summary 				 Mobile Graphical User Interface (GUI) World Models (WMs) offer...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 26. SPARKLING is a framework for mid-stage width expansion in deep learning models that maintains signal preservation and breaks symmetry to stabilize training and reduce computational costs.  					AI-generated summary 				 Progressive Learning (PL) reduces pre-training computational overhead by gradual...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 27. Large language model control methods are unified under a dynamic weight update framework, revealing a preference-utility trade-off and enabling improved steering through SPLIT approach.  					AI-generated summary 				 Methods for controlling large language models (LLMs), including local weight fine-...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 28. LatentMorph integrates implicit latent reasoning into text-to-image generation through four lightweight components that enable adaptive self-refinement and improve both efficiency and cognitive alignment.  					AI-generated summary 				 Text-to-image (T2I) generation has achieved remarkable progress...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 29. A Japanese financial language understanding benchmark named Ebisu is introduced, featuring two expert-annotated tasks that evaluate implicit commitment recognition and hierarchical financial terminology extraction, revealing persistent challenges for current language models despite their advanced ca...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 30. Loop-ViT introduces a recursive vision transformer architecture that decouples reasoning depth from model capacity through weight-tied recurrence and dynamic exit mechanisms, achieving superior visual reasoning performance with fewer parameters.  					AI-generated summary 				 Recent advances in vis...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 31. PolySAE extends sparse autoencoders with polynomial decoding to capture feature interactions and compositional structure while maintaining linear encoders for interpretability.  					AI-generated summary 				 Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural netwo...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 32. AgentIF-OneDay evaluates AI agents' ability to handle diverse daily tasks through natural language instructions, requiring problem-solving, attachment understanding, and file-based outputs across three user-centric categories.  					AI-generated summary 				 The capacity of AI agents to effectively ...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 33. Thinking with Comics emerges as an effective visual reasoning approach that bridges images and videos by leveraging comic structures for improved multimodal reasoning efficiency and performance.  					AI-generated summary 				 Chain-of-Thought reasoning has driven large language models to extend fro...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 34. TRIP-Bench presents a comprehensive long-horizon benchmark for travel planning that evaluates LLM agents on complex multi-turn interactions, while GTPO offers an online reinforcement learning approach to enhance constraint satisfaction and robustness in extended dialogues.  					AI-generated summary...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 35. A novel framework called CoDiQ enables controllable difficulty generation for competition-level questions through test-time scaling, resulting in a corpus that significantly improves large reasoning model performance.  					AI-generated summary 				 Large Reasoning Models (LRMs) benefit substantiall...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 36. Rubric-ARM framework jointly optimizes rubric generation and judging through reinforcement learning to improve response quality assessment in creative and open-ended tasks.  					AI-generated summary 				 Standard reward models typically predict scalar scores that fail to capture the multifaceted na...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 37. Flow matching models for text-to-image generation are enhanced through a reinforcement learning framework that addresses sample inefficiency and prompt overfitting by incorporating language models for prompt refinement, achieving superior performance with reduced computational requirements.  					AI...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 38. Research identifies a sparse reward subsystem in LLM hidden states containing value neurons that represent internal state expectations and dopamine-like neurons encoding reward prediction errors.  					AI-generated summary 				 In this paper, we identify a sparse reward subsystem within the hidden s...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 39. Adaptive Ability Decomposing (A²D) enhances reinforcement learning with verifiable rewards by decomposing complex questions into simpler sub-questions, improving LLM reasoning through guided exploration without requiring a teacher model.  					AI-generated summary 				 Reinforcement learning with ve...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 40. Post-training quantization effects in world models reveal unique failure modes and trade-offs between accuracy, bit-width, and planning performance, particularly in encoder-predictor module asymmetries and low-bit rollout stability.  					AI-generated summary 				 World models learn an internal repr...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 41. Agentic large language models require investigatory intelligence for autonomous data analysis, demonstrated through the Deep Data Research benchmark that evaluates their ability to extract insights from databases without explicit queries.  					AI-generated summary 				 The agency expected of Agenti...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 42. Small language models can effectively evaluate outputs by leveraging internal representations rather than generating responses, enabling a more efficient and interpretable evaluation approach through a probing-based framework.  					AI-generated summary 				 Large language models (LLMs) are widely u...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 43. A new test-time scaling framework called Prism is introduced for discrete diffusion language models that improves reasoning performance through hierarchical trajectory search, local branching with partial remasking, and self-verified feedback mechanisms.  					AI-generated summary 				 Inference-tim...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 44. LiP-Map presents a line-plane joint optimization framework that explicitly models learnable line and planar primitives for accurate 3D line mapping in man-made environments.  					AI-generated summary 				 3D line mapping from multi-view RGB images provides a compact and structured visual representa...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 45. PISA is a novel sparse attention method that improves diffusion transformer efficiency by approximating non-critical attention blocks instead of discarding them, achieving faster processing with maintained quality.  					AI-generated summary 				 Diffusion Transformers are fundamental for video and ...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 46. VisionTrim is a training-free framework that accelerates multimodal large language models by selecting dominant visual tokens and merging them with text-guided complementation, improving efficiency without performance loss.  					AI-generated summary 				 Multimodal large language models (MLLMs) suf...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 47. Layer pruning compresses large language models while maintaining classification performance but causes significant degradation in generative reasoning tasks, with limited recovery possible through supervised finetuning on self-generated responses.  					AI-generated summary 				 Recent works have sh...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 48. Scaling hidden states of delimiter tokens in vision-language models reduces cross-image information leakage and improves multi-image reasoning performance.  					AI-generated summary 				 Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance dec...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 49. A training-free framework enables language model agents to automatically create and optimize tools during inference, improving their reasoning capabilities through self-evolution and memory consolidation.  					AI-generated summary 				 Existing Tool-Integrated Reasoning (TIR) models have effectivel...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 50. A novel optimizer called Mano is proposed that combines manifold optimization with momentum projection onto tangent spaces, achieving superior performance over AdamW and Muon while reducing memory and computational requirements.  					AI-generated summary 				 While large language models (LLMs) have...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 51. Effective dimension, an unsupervised geometric metric, strongly predicts neural network performance across different architectures and domains, showing bidirectional causality between representation geometry and accuracy.  					AI-generated summary 				 We investigate the relationship between repres...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 52. Implicit neural representations operate continuously over UV coordinate space, demonstrating good image quality while balancing memory usage and rendering time, with applications in real-time rendering and downstream tasks.  					AI-generated summary 				 Implicit neural representation (INR) has pro...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 53. Controlled cross-lingual evaluation reveals instability in LLM assessment methods when targeting morphologically rich languages, indicating unreliable zero-shot judge transfer for discourse-level tasks.  					AI-generated summary 				 Cross-lingual evaluation of large language models (LLMs) typicall...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 54. Generalizable Predictive Prompt Selection (GPS) uses Bayesian inference with a lightweight generative model to efficiently select informative prompts for reinforcement learning-enhanced language models, improving training efficiency and performance.  					AI-generated summary 				 Reinforcement lear...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 55. A two-phase diagnostic framework based on Item Response Theory and Graded Response Model is introduced to assess the reliability of LLM-as-a-Judge by examining intrinsic consistency and human alignment.  					AI-generated summary 				 While LLM-as-a-Judge is widely used in automated evaluation, exis...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 56. Clipping-Free Policy Optimization replaces heuristic clipping with convex quadratic penalty to stabilize reinforcement learning training for large language models without performance loss.  					AI-generated summary 				 Reinforcement learning has become central to post-training large language model...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 57. VAE-based inpainting creates spectral shifts that fool detection systems, which can be mitigated through Inpainting Exchange to improve content-aware detection performance.  					AI-generated summary 				 Modern deep learning-based inpainting enables realistic local image manipulation, raising criti...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 58. Automated pipeline for sound separation using high-purity single-event segments from in-the-wild datasets achieves competitive performance with significantly reduced data requirements.  					AI-generated summary 				 Query-based universal sound separation is fundamental to intelligent auditory syste...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 59. Parallel Echo State Network (ParalESN) addresses reservoir computing limitations by enabling parallel temporal processing through diagonal linear recurrence, maintaining theoretical guarantees while achieving significant computational efficiency gains.  					AI-generated summary 				 Reservoir Compu...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 60. On-policy Verbal Distillation (OVD) enables efficient knowledge transfer from teacher to student models by replacing token-level probability matching with trajectory matching using discrete verbal scores, reducing memory consumption and enabling free exploration without token alignment constraints. ...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 61. An reinforcement learning-based sampling framework adaptively reweights training datasets to improve embedding model performance while reducing GPU costs.  					AI-generated summary 				 General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 62. Large language models used as judges for agent performance evaluation are vulnerable to manipulation of reasoning traces, with content-based fabrications being more effective than style-based alterations.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as judges to...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 63. ReSID presents a novel recommendation-native framework that improves sequential recommendation by learning predictive item representations and optimizing quantization for information preservation and sequential predictability.  					AI-generated summary 				 Semantic ID (SID)-based recommendation is...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 64. Internal flow signatures analyze depthwise dynamics in large language models to enable self-checking and targeted refinement without modifying the base model.  					AI-generated summary 				 Large language models can generate fluent answers that are unfaithful to the provided context, while many saf...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 65. Multi-agent systems for molecular discovery that use individualized scientist profiles based on publication and molecular history outperform traditional role-based approaches.  					AI-generated summary 				 Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery....
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 66. Researchers developed a novel agentic data-generation framework to create culturally grounded safety datasets for Southeast Asia, resulting in multilingual safeguard models that outperform existing approaches in detecting regionally sensitive content while maintaining general safety performance.  		...
[03.02.2026 19:42] ********************************************************************************
[03.02.2026 19:42] Abstract 67. Parabolic Position Encoding (PaPE) is a novel position encoding method for vision modalities that improves upon existing approaches by incorporating translation invariance, rotation invariance, distance decay, directionality, and context awareness principles.  					AI-generated summary 				 We propo...
[03.02.2026 19:42] Read previous papers.
[03.02.2026 19:42] Generating reviews via LLM API.
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#data", "#rl", "#robotics", "#training"], "emoji": "🤖", "ru": {"title": "Универсальная политика действий для роботов разных конструкций через многоэтапное обучение с языком и подкреплением", "desc": "Green-VLA представляет собой пятиэтапную архитектуру Vision-Language
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#plp", "#cv", "#agents", "#reasoning", "#open_source", "#training"], "emoji": "🐝", "ru": {"title": "Многомодальный интеллект через роевую координацию агентов", "desc": "Представляем Kimi K2.5 — открытую мультимодальную модель-агента, которая оптимизирует обработку тек
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#rag", "#reasoning", "#optimization", "#rl", "#open_source"], "emoji": "🔍", "ru": {"title": "Мультимодальное глубокое исследование через многоходовой поиск с обучением", "desc": "Vision-DeepResearch представляет новую мультимодальную парадигму глубокого исследо
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#dataset", "#open_source"], "emoji": "🔍", "ru": {"title": "Реалистичная оценка визуального поиска в мультимодальных системах", "desc": "Статья представляет Vision-DeepResearch benchmark для оценки способностей мультимодальных больших языковых моделей в в
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#plp"], "emoji": "🔄", "ru": {"title": "Единый цикл понимания и генерации кода через высокоточные графовые представления", "desc": "Статья представляет RPG-Encoder — фреймворк, который преобразует понимание и генерацию кода в единый цикл через создание высоко
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#reasoning", "#synthetic"], "emoji": "🎨", "ru": {"title": "Единое рассуждение для совершенной генерации: планирование и утончение", "desc": "UniReason — это единая框架, которая объединяет генерацию изображений и их редактирование через двойную 
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#dataset", "#rag", "#benchmark", "#open_source", "#graphs", "#long_context"], "emoji": "🕸️", "ru": {"title": "Бенчмарк для оценки извлечения и агрегации знаний в реальных сценариях", "desc": "В статье представлен WildGraphBench — бенчмарк для оценки GraphRAG в реалистичных сценариях
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark", "#reasoning", "#open_source", "#long_context"], "emoji": "📚", "ru": {"title": "Преодоление контекстных ограничений: внешняя память для агентов LLM", "desc": "В статье предлагается FS-Researcher — двухагентная система, где один агент собирает инфор
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multilingual", "#plp", "#dataset", "#agents", "#benchmark", "#rl", "#training"], "emoji": "🏗️", "ru": {"title": "Миллионный масштаб верифицируемых задач разработки через умных агентов", "desc": "В статье представляется SWE-Universe — масштабируемая система для автоматического созда
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark", "#open_source", "#survey"], "emoji": "📚", "ru": {"title": "Измеряя дистанцию между AI-агентами и экспертными знаниями", "desc": "В статье представлена новая benchmark Wiki Live Challenge, которая использует статьи Wikipedia Good Articles как этало
[03.02.2026 19:42] Using data from previous issue: {"categories": [], "emoji": "🎨", "ru": {"title": "Прямая генерация изображений в пиксельном пространстве через перцептивный надзор", "desc": "PixelGen — это фреймворк диффузионных моделей, который генерирует изображения прямо в пиксельном пространстве, избегая артефактов, которые вносят VAE в традиц
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#training"], "emoji": "⚖️", "ru": {"title": "Стабильное выравнивание без компромисса между предпочтениями и качеством", "desc": "SLIME — это новый метод выравнивания больших языковых моделей, который решает проблему несоответствия целевых функций в методах пря
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#rl", "#agents", "#training"], "emoji": "🔄", "ru": {"title": "Динамическая оптимизация через замкнутую обратную связь в обучении с подкреплением", "desc": "RLAnything — это фреймворк обучения с подкреплением, который динамически оптимизирует модели окружения, политики и вознагражден
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#optimization", "#rl", "#video", "#training"], "emoji": "🎬", "ru": {"title": "Оптимальный транспорт для безаннотационной генерации видео из текста", "desc": "PISCES — это метод генерации видео из текста, который не требует аннотаций человека для улучшения качества. Авторы используют
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#benchmark", "#rag", "#reasoning"], "emoji": "🎨", "ru": {"title": "Агентный фреймворк для контекстно-осведомленной генерации изображений через динамический поиск знаний", "desc": "Mind-Brush представляет агентный фреймворк для генерации изображений из текст
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#architecture", "#video", "#training"], "emoji": "🎬", "ru": {"title": "Причинное принуждение: дистилляция видео диффузионных моделей через авторегрессивных учителей", "desc": "В статье предлагается новый метод Causal Forcing для дистилляции двунаправленных видео диффузионных моделей
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#inference", "#optimization", "#training", "#small_models"], "emoji": "🧠", "ru": {"title": "Умная дистилляция: выбираем правильные позиции для обучения моделей", "desc": "В работе предложен метод выборочной дистилляции знаний (knowledge distillation) для автогрессивных языковых моде
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#math", "#reasoning", "#optimization", "#rl", "#training"], "emoji": "⚖️", "ru": {"title": "Балансировка распределений для синергии SFT и RL в обучении моделей", "desc": "В статье предложен метод PEAR для улучшения совместного обучения моделей рассуждения на этапах supervised fine-t
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#video", "#inference", "#diffusion", "#optimization", "#long_context"], "emoji": "⚡", "ru": {"title": "Оптимизация внимания в видео диффузионных моделях через сжатие кеша и разреживание вычислений", "desc": "В работе предложены три техники оптимизации для авторегрессивных видео дифф
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#architecture", "#video", "#diffusion", "#optimization", "#open_source", "#training"], "emoji": "🎬", "ru": {"title": "Порядок величины быстрее: диффузионные трансформеры для генерации видео из изображений", "desc": "FSVideo представляет собой быстрый фреймворк для преобразования изо
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#reasoning", "#rl", "#open_source", "#training"], "emoji": "🧠", "ru": {"title": "Внутренняя визуальная память как основа когнитивного мышления мультимодальных моделей", "desc": "Исследование представляет новый подход Cognitive Supersensing дл
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#open_source"], "emoji": "🎨", "ru": {"title": "Творческий перенос визуальных метафор через многоагентную систему с когнитивным обоснованием", "desc": "В работе предлагается новая задача Visual Metaphor Transfer, которая заключается в извлечении абстрактной творческой сущности из эта
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#benchmark"], "emoji": "✏️", "ru": {"title": "Мультимодальное редактирование: от текста к визуальным инструкциям", "desc": "Статья представляет VIBE — Visual Instruction Benchmark for Image Editing, новый бенчмарк для оценки способности генеративных моделей сле
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#video", "#games", "#benchmark"], "emoji": "🎬", "ru": {"title": "Говорящие аватары, которые реально взаимодействуют с предметами", "desc": "В статье предлагается двухпоточная архитектура InteractAvatar для синтеза видео говорящих аватаров, которые мог
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#long_context", "#optimization", "#reasoning"], "emoji": "🔄", "ru": {"title": "Размышляющий агент: глобальное планирование через кросс-траекторную рефлексию", "desc": "Re-TRAC — это фреймворк для агентов на основе большых языковых моделей, который преодолевает ограничения линейного 
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#cv", "#agents", "#benchmark", "#small_models"], "emoji": "📱", "ru": {"title": "От пикселей к коду: визуальные мировые модели через генерацию рендерируемого веб-кода", "desc": "В статье предлагается новый подход к созданию мировых моделей для мобильных GUI
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "⚡", "ru": {"title": "Стабильное расширение ширины сети: баланс сохранения сигнала и нарушения симметрии", "desc": "SPARKLING — это фреймворк для расширения ширины нейросетей на промежуточных этапах обучения, который сохраняет 
[03.02.2026 19:42] Using data from previous issue: {"categories": [], "emoji": "⚖️", "ru": {"title": "Баланс между контролем и качеством: единая теория управления LLM", "desc": "Исследование объединяет различные методы контроля над большими языковыми моделями (LLM) - от дообучения весов до LoRA и активационных вмешательств - в единую концептуальную 
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#reasoning", "#optimization", "#rl", "#training"], "emoji": "🧠", "ru": {"title": "Скрытое рассуждение для более умной генерации изображений", "desc": "LatentMorph представляет новый подход к генерации изображений из текста, интегрируя неявное латентное рассужде
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#science", "#benchmark", "#low_resource", "#open_source"], "emoji": "📊", "ru": {"title": "Языковой барьер: почему LLM борются с японским финансовым текстом", "desc": "В статье представлен эталонный набор Ebisu для оценки понимания японского финансового т
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#architecture", "#cv", "#benchmark", "#reasoning", "#optimization", "#open_source", "#small_models"], "emoji": "🔄", "ru": {"title": "Эффективные рассуждения через адаптивную рекурсию вместо расширения сети", "desc": "Loop-ViT представляет рекурсивную архитектуру трансформера для зре
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#interpretability", "#architecture", "#training"], "emoji": "🧩", "ru": {"title": "Полиномиальное декодирование для интерпретации композиционных структур в нейросетях", "desc": "PolySAE улучшает разреженные автокодировщики (SAE) путём добавления полиномиального декодера, который позв
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset"], "emoji": "🤖", "ru": {"title": "Оценка способности AI агентов решать реальные повседневные задачи", "desc": "В работе представлен бенчмарк AgentIF-OneDay для оценки способности AI агентов выполнять разнообразные повседневные задачи, получаемые чер
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#video", "#cv", "#benchmark", "#reasoning", "#long_context"], "emoji": "🎨", "ru": {"title": "Комиксы как оптимальный мост между изображениями и видео для эффективного мультимодального рассуждения", "desc": "В работе предложен новый подход к мультимодальному рассуждени
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark", "#reasoning", "#optimization", "#rl", "#long_context"], "emoji": "✈️", "ru": {"title": "Длинные горизонты диалога: обучение агентов для надёжного планирования путешествий", "desc": "В статье представлен TRIP-Bench — комплексный бенчмарк для оценки
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#synthetic", "#open_source", "#reasoning"], "emoji": "🎯", "ru": {"title": "Генерация конкурсных задач с точным контролем сложности для обучения моделей рассуждений", "desc": "В статье предложен новый фреймворк CoDiQ для генерации задач с контролируемой сложностью на уровне соревнова
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#rl", "#rlhf", "#benchmark"], "emoji": "📋", "ru": {"title": "Динамические критерии для оценки творческих ответов через обучение с подкреплением", "desc": "Rubric-ARM — это фреймворк, который совместно оптимизирует генерацию критериев оценки и судью с помощью обучения с подкреплением
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#rl", "#rlhf", "#training"], "emoji": "🎨", "ru": {"title": "Пусть языковая модель улучшает подсказки в обучении с подкреплением", "desc": "В статье представлена PromptRL — фреймворк, который улучшает модели flow matching для генерации изображений по текс
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#transfer_learning", "#interpretability", "#training"], "emoji": "🧠", "ru": {"title": "Внутренняя система вознаграждения языковых моделей раскрыта", "desc": "Исследователи обнаружили разреженную подсистему вознаграждения в скрытых состояниях большых яз
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#optimization", "#rl", "#training", "#reasoning"], "emoji": "🧩", "ru": {"title": "Разложение сложности: от абстрактных вопросов к пошаговым решениям", "desc": "В работе предложен метод Adaptive Ability Decomposing (A²D) для улучшения обучения с подкреплением на основе проверяемых во
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#inference", "#rl", "#agents"], "emoji": "⚖️", "ru": {"title": "Компромиссы квантования в моделях мира: от точности к эффективности", "desc": "В статье исследуется влияние пост-тренировочного квантования на world models, которые обучаются компактно представлять динамику окружения дл
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset"], "emoji": "🔍", "ru": {"title": "Исследовательский интеллект: от выполнения команд к автономному анализу данных", "desc": "Статья представляет концепцию исследовательского интеллекта для агентивных языковых моделей, которые должны самостоятельно ст
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#benchmark", "#training", "#small_models"], "emoji": "🔍", "ru": {"title": "Оценка без генерации: скрытые представления маленьких моделей вместо больших судей", "desc": "Исследование показывает, что маленькие языковые модели могут эффективно оценивать качество ответов, используя внут
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#inference", "#benchmark", "#reasoning", "#diffusion", "#optimization", "#open_source", "#training"], "emoji": "🌳", "ru": {"title": "Умное ветвление: эффективный инферс дискретных диффузионных моделей через иерархический поиск", "desc": "В статье предложен фреймворк Prism для масшта
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#3d", "#cv"], "emoji": "📐", "ru": {"title": "Совместная оптимизация линий и плоскостей для структурированного 3D-отображения", "desc": "LiP-Map представляет собой фреймворк совместной оптимизации линий и плоскостей для восстановления трёхмерных линий из многовидовых RGB-изображений.
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#video", "#architecture", "#inference", "#diffusion", "#optimization"], "emoji": "⚡", "ru": {"title": "Точное вычисление важного, приближение остального: эффективное разреженное внимание", "desc": "В статье предложен метод PISA, который улучшает эффективность диффузио
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#inference", "#video"], "emoji": "⚡", "ru": {"title": "Ускорение мультимодальных моделей через умное прореживание визуальных токенов", "desc": "VisionTrim представляет фреймворк без обучения для ускорения мультимодальных больших языковых моделей путём выбора доминантн
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#inference", "#benchmark", "#reasoning", "#optimization", "#training"], "emoji": "✂️", "ru": {"title": "Пределы сжатия языковых моделей: почему генеративный интеллект страдает больше, чем классификация", "desc": "В работе исследуется влияние удаления слоёв на большие языковые модели
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#leakage", "#benchmark", "#reasoning"], "emoji": "🖼️", "ru": {"title": "Масштабирование разделителей для устранения утечки информации между изображениями", "desc": "Исследование выявляет проблему утечки информации между изображениями в больших видео-я
[03.02.2026 19:42] Using data from previous issue: {"categories": [], "emoji": "🛠️", "ru": {"title": "Агент, создающий свои собственные инструменты в процессе рассуждения", "desc": "В статье предложена框架UCT, которая превращает языковые модели из пользователей инструментов в их создателей. Система автоматически генерирует и оптимизирует специализиров
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#optimization"], "emoji": "🚀", "ru": {"title": "Манифольдная оптимизация с проекцией момента для эффективного обучения больших языковых моделей", "desc": "Авторы предлагают новый оптимизатор Mano, который применяет методы оптимизации на многообразиях с проекцией момента на касательн
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#architecture", "#cv", "#benchmark", "#interpretability", "#training"], "emoji": "📐", "ru": {"title": "Эффективная размерность как универсальный предиктор качества нейронных сетей", "desc": "В работе исследуется связь между геометрией представлений и производительностью нейронных се
[03.02.2026 19:42] Using data from previous issue: {"categories": [], "emoji": "🎨", "ru": {"title": "Непрерывные нейронные текстуры для эффективного рендеринга", "desc": "В работе предлагается использовать неявные нейронные представления (INR) для текстур, которые работают в непрерывном пространстве UV-координат вместо дискретного представления. Авт
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multilingual", "#machine_translation", "#dataset", "#benchmark", "#low_resource", "#data", "#open_source"], "emoji": "⚠️", "ru": {"title": "Когда судьи ошибаются: нестабильность кросс-лингвальной оценки LLM на морфологически сложных языках", "desc": "В работе исследуется надёжность
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#rl", "#training", "#small_models"], "emoji": "🎯", "ru": {"title": "Умный выбор подсказок через байесовское предсказание сложности", "desc": "В статье предлагается метод Generalizable Predictive Prompt Selection (GPS), который использует байесовский вы
[03.02.2026 19:42] Using data from previous issue: {"categories": [], "emoji": "⚖️", "ru": {"title": "Диагностика надёжности LLM-судей через теорию измерений и анализ согласованности", "desc": "В работе предлагается двухэтапная диагностическая система на основе теории ответов на задания и модели градуированных ответов для оценки надёжности LLM-as-a-
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#reasoning", "#alignment", "#optimization", "#rl", "#rlhf", "#training"], "emoji": "🎯", "ru": {"title": "Без отсечений — к стабильной оптимизации политики в LLM", "desc": "Авторы предлагают Clipping-Free Policy Optimization (CFPO) — новый метод для обучения больших языковых моделей 
[03.02.2026 19:42] Using data from previous issue: {"categories": [], "emoji": "🎨", "ru": {"title": "Спектральные ловушки VAE-инпейнтинга: от обнаружения артефактов к контент-ориентированной детекции", "desc": "В статье исследуется проблема детектирования изображений, отредактированных с помощью VAE-based инпейнтинга. Авторы обнаружили, что современ
[03.02.2026 19:42] Querying the API.
[03.02.2026 19:42] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Automated pipeline for sound separation using high-purity single-event segments from in-the-wild datasets achieves competitive performance with significantly reduced data requirements.  					AI-generated summary 				 Query-based universal sound separation is fundamental to intelligent auditory systems, aiming to isolate specific sources from mixtures. Despite recent advances, existing methods continue to suffer from residual interference in complex acoustic scenes. This performance limitation stems largely from a data bottleneck: in-the-wild datasets contain weak labels and severe co-occurrence of events. These flaws induce models to learn spurious correlations between background noise and target categories instead of robust acoustic features. To address this, we propose an automated pipeline that eliminates co-occurrence of events by mining high-purity single-event segments from in-the-wild datasets via a semantically consistent synthesis protocol. Utilizing this pipeline, we constructed Hive, a high-quality synthetic dataset comprising 2.4k hours of raw audio. Experimental results demonstrate that, compared with the state-of-the-art model SAM-Audio which was trained on a huge dataset sim500 times larger than Hive, certain open-source models trained on Hive achieve competitive separation accuracy and perceptual quality. Moreover, these models exhibited remarkable zero-shot generalization on out-of-distribution evaluation benchmarks. These findings highlight that prioritizing purity of supervised signals enables significant data efficiency, offering a new paradigm for training robust auditory foundation models with reduced computational costs. Code and dataset are available at https://shandaai.github.io/Hive.
[03.02.2026 19:42] Response: ```json
{
  "desc": "В работе предложен автоматизированный конвейер для разделения звуковых сигналов, который извлекает чистые монофонические фрагменты из реальных датасетов с помощью семантически согласованного синтеза. Авторы построили высококачественный синтетический датасет Hive объёмом 2.4 тысячи часов аудио, что позволило избежать проблемы совместного появления событий и слабых разметок в исходных данных. Модели, обученные на Hive, достигают конкурентной точности разделения источников звука, сопоставимой с моделью SAM-Audio, обученной на датасете в 500 раз большем размером. Такой подход демонстрирует, что приоритизация чистоты сигналов обучения обеспечивает значительную эффективность использования данных и открывает новую парадигму для обучения устойчивых моделей обработки звука с меньшими вычислительными затратами.",
  "emoji": "🎵",
  "title": "Чистые данные вместо больших объёмов: эффективное разделение звука через высокачественный синтез"
}
```
[03.02.2026 19:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Automated pipeline for sound separation using high-purity single-event segments from in-the-wild datasets achieves competitive performance with significantly reduced data requirements.  					AI-generated summary 				 Query-based universal sound separation is fundamental to intelligent auditory systems, aiming to isolate specific sources from mixtures. Despite recent advances, existing methods continue to suffer from residual interference in complex acoustic scenes. This performance limitation stems largely from a data bottleneck: in-the-wild datasets contain weak labels and severe co-occurrence of events. These flaws induce models to learn spurious correlations between background noise and target categories instead of robust acoustic features. To address this, we propose an automated pipeline that eliminates co-occurrence of events by mining high-purity single-event segments from in-the-wild datasets via a semantically consistent synthesis protocol. Utilizing this pipeline, we constructed Hive, a high-quality synthetic dataset comprising 2.4k hours of raw audio. Experimental results demonstrate that, compared with the state-of-the-art model SAM-Audio which was trained on a huge dataset sim500 times larger than Hive, certain open-source models trained on Hive achieve competitive separation accuracy and perceptual quality. Moreover, these models exhibited remarkable zero-shot generalization on out-of-distribution evaluation benchmarks. These findings highlight that prioritizing purity of supervised signals enables significant data efficiency, offering a new paradigm for training robust auditory foundation models with reduced computational costs. Code and dataset are available at https://shandaai.github.io/Hive."

[03.02.2026 19:42] Response: ```python
["AUDIO", "DATASET", "DATA"]
```
[03.02.2026 19:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Automated pipeline for sound separation using high-purity single-event segments from in-the-wild datasets achieves competitive performance with significantly reduced data requirements.  					AI-generated summary 				 Query-based universal sound separation is fundamental to intelligent auditory systems, aiming to isolate specific sources from mixtures. Despite recent advances, existing methods continue to suffer from residual interference in complex acoustic scenes. This performance limitation stems largely from a data bottleneck: in-the-wild datasets contain weak labels and severe co-occurrence of events. These flaws induce models to learn spurious correlations between background noise and target categories instead of robust acoustic features. To address this, we propose an automated pipeline that eliminates co-occurrence of events by mining high-purity single-event segments from in-the-wild datasets via a semantically consistent synthesis protocol. Utilizing this pipeline, we constructed Hive, a high-quality synthetic dataset comprising 2.4k hours of raw audio. Experimental results demonstrate that, compared with the state-of-the-art model SAM-Audio which was trained on a huge dataset sim500 times larger than Hive, certain open-source models trained on Hive achieve competitive separation accuracy and perceptual quality. Moreover, these models exhibited remarkable zero-shot generalization on out-of-distribution evaluation benchmarks. These findings highlight that prioritizing purity of supervised signals enables significant data efficiency, offering a new paradigm for training robust auditory foundation models with reduced computational costs. Code and dataset are available at https://shandaai.github.io/Hive."

[03.02.2026 19:42] Response: ```python
['SYNTHETIC', 'OPEN_SOURCE']
```
[03.02.2026 19:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents an automated pipeline for sound separation that focuses on using high-purity single-event segments from in-the-wild datasets. The authors address the issue of residual interference in complex acoustic scenes caused by weak labels and event co-occurrence in existing datasets. By mining high-quality audio segments, they created a synthetic dataset called Hive, which is significantly smaller yet competitive in performance compared to larger datasets. The results show that models trained on Hive achieve strong separation accuracy and generalization, demonstrating the importance of data purity in training effective auditory models.","title":"Purity Over Quantity: Revolutionizing Sound Separation with Hive"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents an automated pipeline for sound separation that focuses on using high-purity single-event segments from in-the-wild datasets. The authors address the issue of residual interference in complex acoustic scenes caused by weak labels and event co-occurrence in existing datasets. By mining high-quality audio segments, they created a synthetic dataset called Hive, which is significantly smaller yet competitive in performance compared to larger datasets. The results show that models trained on Hive achieve strong separation accuracy and generalization, demonstrating the importance of data purity in training effective auditory models.', title='Purity Over Quantity: Revolutionizing Sound Separation with Hive'))
[03.02.2026 19:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种自动化的声音分离管道，利用高纯度的单事件片段，从自然环境数据集中提取声音。该方法解决了现有技术在复杂声学场景中存在的残余干扰问题，主要是由于数据瓶颈导致的。通过语义一致的合成协议，构建了一个名为Hive的高质量合成数据集，包含2400小时的原始音频。实验结果表明，使用Hive训练的开源模型在分离准确性和感知质量上与大型数据集训练的最先进模型相当，且在零样本泛化能力上表现出色。","title":"高纯度数据驱动的声音分离新范式"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种自动化的声音分离管道，利用高纯度的单事件片段，从自然环境数据集中提取声音。该方法解决了现有技术在复杂声学场景中存在的残余干扰问题，主要是由于数据瓶颈导致的。通过语义一致的合成协议，构建了一个名为Hive的高质量合成数据集，包含2400小时的原始音频。实验结果表明，使用Hive训练的开源模型在分离准确性和感知质量上与大型数据集训练的最先进模型相当，且在零样本泛化能力上表现出色。', title='高纯度数据驱动的声音分离新范式'))
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#training"], "emoji": "⚡", "ru": {"title": "Параллельная обработка времени: экономия затрат без потери качества", "desc": "В статье представлена Parallel Echo State Network (ParalESN) — новый подход к reservoir computing, решающий проблему последовател
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#transfer_learning", "#optimization", "#rl", "#training"], "emoji": "🧠", "ru": {"title": "Вербальная дистилляция: передача знаний без ограничений токен-уровневого выравнивания", "desc": "Статья представляет метод On-policy Verbal Distillation (OVD), котор
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multilingual", "#data", "#rl", "#optimization", "#training", "#small_models"], "emoji": "🎯", "ru": {"title": "Умное переweighting: когда машина выбирает, какие данные учить", "desc": "В статье предложена новая система Inf-DDS, которая использует обучение с подкреплением для адаптив
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "⚠️", "ru": {"title": "Уязвимость LLM-судей перед манипуляциями в цепочках рассуждений агентов", "desc": "В работе показано, что большие языковые модели, используемые для оценки производительности агентов, уязвимы для манипуляций с цепочками рассужд
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#architecture", "#training"], "emoji": "🎯", "ru": {"title": "Предсказательные представления и адаптивная квантизация для рекомендаций", "desc": "ReSID представляет инновационный подход к последовательным рекомендациям, который обучает представления предметов, оптимизированные для пр
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#architecture", "#hallucinations", "#interpretability", "#open_source", "#training"], "emoji": "🔍", "ru": {"title": "Внутреннее видение LLM: самопроверка через анализ глубинной динамики активаций", "desc": "В статье предложен метод анализа внутренней динамики больших языковых моделе
[03.02.2026 19:42] Using data from previous issue: {"categories": [], "emoji": "🧬", "ru": {"title": "Научная ДНК агентов: индивидуализация для открытий в молекулярной химии", "desc": "В работе предложена система INDIBATOR для автоматизации молекулярных открытий, основанная на многоагентном подходе. Агенты в системе характеризуются индивидуальными пр
[03.02.2026 19:42] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#agents", "#benchmark", "#low_resource", "#alignment", "#synthetic", "#open_source"], "emoji": "🌏", "ru": {"title": "Культурно-адаптивные модели безопасности для Юго-Восточной Азии", "desc": "Исследователи разработали агентный фреймворк для автоматическо
[03.02.2026 19:42] Querying the API.
[03.02.2026 19:42] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Parabolic Position Encoding (PaPE) is a novel position encoding method for vision modalities that improves upon existing approaches by incorporating translation invariance, rotation invariance, distance decay, directionality, and context awareness principles.  					AI-generated summary 				 We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.
[03.02.2026 19:42] Response: ```json
{
  "desc": "В статье предлагается новый метод кодирования позиций (PaPE) для визуальных модальностей, основанный на параболических функциях. Метод incorporates пять ключевых принципов: инвариантность к трансляции, инвариантность к ротации, затухание расстояния, направленность и контекстную осведомлённость. Авторы тестировали подход на восьми датасетах, охватывающих четыре визуальные модальности (изображения, облака точек, видео, потоки событий), и продемонстрировали, что PaPE достигает лучших результатов в семи из восьми случаев. Особенно впечатляют результаты экстраполяции на ImageNet-1K, где метод превосходит альтернативные способы кодирования позиций на 10.5% в абсолютном выражении.",
  "emoji": "📍",
  "title": "Параболическое кодирование позиций для универсальных визуальных трансформеров"
}
```
[03.02.2026 19:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Parabolic Position Encoding (PaPE) is a novel position encoding method for vision modalities that improves upon existing approaches by incorporating translation invariance, rotation invariance, distance decay, directionality, and context awareness principles.  					AI-generated summary 				 We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding."

[03.02.2026 19:42] Response: ```python
["CV", "ARCHITECTURE", "MULTIMODAL"]
```
[03.02.2026 19:42] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Parabolic Position Encoding (PaPE) is a novel position encoding method for vision modalities that improves upon existing approaches by incorporating translation invariance, rotation invariance, distance decay, directionality, and context awareness principles.  					AI-generated summary 				 We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding."

[03.02.2026 19:42] Response: ```python
["OPTIMIZATION"]
```
[03.02.2026 19:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Parabolic Position Encoding (PaPE) is a new method designed to improve how positions are encoded in vision tasks using attention-based models. It incorporates key principles such as translation and rotation invariance, which help the model recognize objects regardless of their position or orientation. Additionally, PaPE considers factors like distance decay, directionality, and context awareness to better capture the unique characteristics of visual data. Evaluations on multiple datasets show that PaPE significantly outperforms existing position encoding methods, demonstrating its effectiveness across various vision modalities.","title":"Revolutionizing Vision with Parabolic Position Encoding"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Parabolic Position Encoding (PaPE) is a new method designed to improve how positions are encoded in vision tasks using attention-based models. It incorporates key principles such as translation and rotation invariance, which help the model recognize objects regardless of their position or orientation. Additionally, PaPE considers factors like distance decay, directionality, and context awareness to better capture the unique characteristics of visual data. Evaluations on multiple datasets show that PaPE significantly outperforms existing position encoding methods, demonstrating its effectiveness across various vision modalities.', title='Revolutionizing Vision with Parabolic Position Encoding'))
[03.02.2026 19:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的位置编码方法，称为抛物线位置编码（PaPE），旨在改善视觉模态中的位置编码。PaPE结合了平移不变性、旋转不变性、距离衰减、方向性和上下文感知等原则，以更好地适应视觉特征。我们在8个涵盖4种模态的数据集上评估了PaPE，结果显示PaPE或PaPE-RI在7个数据集上取得了最佳性能。实验结果表明，PaPE在ImageNet-1K上的外推能力显著，性能提升可达10.5%。","title":"抛物线位置编码：视觉模态的新突破"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新的位置编码方法，称为抛物线位置编码（PaPE），旨在改善视觉模态中的位置编码。PaPE结合了平移不变性、旋转不变性、距离衰减、方向性和上下文感知等原则，以更好地适应视觉特征。我们在8个涵盖4种模态的数据集上评估了PaPE，结果显示PaPE或PaPE-RI在7个数据集上取得了最佳性能。实验结果表明，PaPE在ImageNet-1K上的外推能力显著，性能提升可达10.5%。', title='抛物线位置编码：视觉模态的新突破'))
[03.02.2026 19:42] Renaming data file.
[03.02.2026 19:42] Renaming previous data. hf_papers.json to ./d/2026-02-03.json
[03.02.2026 19:42] Saving new data file.
[03.02.2026 19:42] Generating page.
[03.02.2026 19:42] Renaming previous page.
[03.02.2026 19:42] Renaming previous data. index.html to ./d/2026-02-03.html
[03.02.2026 19:42] Writing result.
[03.02.2026 19:42] Renaming log file.
[03.02.2026 19:42] Renaming previous data. log.txt to ./logs/2026-02-03_last_log.txt
