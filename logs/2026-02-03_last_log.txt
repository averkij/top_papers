[03.02.2026 20:29] Read previous papers.
[03.02.2026 20:29] Generating top page (month).
[03.02.2026 20:29] Writing top page (month).
[03.02.2026 21:28] Read previous papers.
[03.02.2026 21:28] Get feed.
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00919
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02276
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22060
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02185
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02084
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02437
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02053
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01566
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02361
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01590
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02493
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02383
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02488
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01624
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01756
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02214
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01395
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01801
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01058
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02092
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01541
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01335
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01851
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01538
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01479
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02486
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01576
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02472
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02343
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02227
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02156
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01322
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.20613
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02453
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01675
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01660
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01511
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01382
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00986
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00759
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02110
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02039
[03.02.2026 21:28] Extract page data from URL. URL: https://huggingface.co/papers/2602.00269
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22588
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01984
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01842
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01296
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01077
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22674
[03.02.2026 21:28] Extract page data from URL. URL: https://huggingface.co/papers/2602.02477
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01997
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01983
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.23000
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00130
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02354
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02287
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01970
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00521
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22801
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00192
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22599
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22296
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21968
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21759
[03.02.2026 21:28] Extract page data from URL. URL: https://huggingface.co/papers/2601.21123
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14691
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02338
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01897
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01815
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01618
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01418
[03.02.2026 21:28] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00168
[03.02.2026 21:28] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[03.02.2026 21:28] No deleted papers detected.
[03.02.2026 21:28] Downloading and parsing papers (pdf, html). Total: 72.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.00919.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.00919.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.00919.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02276.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02276.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02276.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2601.22060.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2601.22060.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2601.22060.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02185.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02185.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02185.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02084.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02084.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02084.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02437.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02437.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02437.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02053.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02053.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02053.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01566.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01566.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01566.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02361.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02361.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02361.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01590.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01590.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01590.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02493.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02493.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02493.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02383.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02383.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02383.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02488.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02488.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02488.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01624.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01624.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01624.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01756.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01756.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01756.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02214.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02214.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02214.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01395.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01395.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01395.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01801.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01801.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01801.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01058.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01058.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01058.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02092.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02092.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02092.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01541.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01541.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01541.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01335.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01335.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01335.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01851.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01851.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01851.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01538.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01538.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01538.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01479.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01479.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01479.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02486.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02486.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02486.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01576.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01576.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01576.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02472.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02472.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02472.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02343.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02343.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02343.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02227.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02227.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02227.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02156.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02156.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02156.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01322.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01322.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01322.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2601.20613.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2601.20613.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2601.20613.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02453.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02453.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02453.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01675.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01675.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01675.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01660.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01660.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01660.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01511.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01511.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01511.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01382.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01382.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01382.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.00986.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.00986.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.00986.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.00759.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.00759.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.00759.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02110.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02110.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02110.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02039.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02039.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02039.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.00269.
[03.02.2026 21:28] Downloading paper 2602.00269 from https://arxiv.org/pdf/2602.00269v1...
[03.02.2026 21:28] Extracting affiliations from text.
[03.02.2026 21:28] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"VOXSERVE: Streaming-Centric Serving System for Speech Language Models 6 2 0 2 0 3 ] . [ 1 9 6 2 0 0 . 2 0 6 2 : r Keisuke Kamahori 1 Wei-Tzu Lee 1 Atindra Jha 2 Rohan Kadekodi 1 Stephanie Wang 1 Arvind Krishnamurthy 1 Baris Kasikci 1 Abstract Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VOXSERVE, unified serving system for SpeechLMs that optimizes streaming performance. VOXSERVE introduces model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within single framework. Building on this abstraction, VOXSERVE implements streaming-aware scheduling and an asynchronous inference pipeline to improve endto-end efficiency. Evaluations across multiple modern SpeechLMs show that VOXSERVE achieves 1020 higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VOXSERVE is available at https://github .com/vox-serve/vox-serve. 1. Introduction In recent years, speech models built upon large language model (LLM) foundations have made substantial progress in tasks such as Text-to-Speech (TTS) and Speech-to-Speech (STS) (Arora et al., 2025). These Speech Language Models (SpeechLMs) leverage LLM backbones and neural audio codec models (Mousavi et al., 2025) to generate and understand speech representations. SpeechLMs are increasingly deployed at scale in real-world applications, including virtual assistants, content generation, and language access services (OpenAI, 2025; ElevenLabs, 2025; Yao, 2025). This widespread adoption demands serving systems that are both low-latency and cost-efficient. The 1University of Washington 2Stanford University. Correspondence to: Keisuke Kamahori <kamahori@cs.washingto"
[03.02.2026 21:28] Response: ```python
["University of Washington", "Stanford University"]
```
[03.02.2026 21:28] Deleting PDF ./assets/pdf/2602.00269.pdf.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2601.22588.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2601.22588.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2601.22588.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01984.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01984.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01984.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01842.
[03.02.2026 21:28] Downloading paper 2602.01842 from https://arxiv.org/pdf/2602.01842v1...
[03.02.2026 21:28] Failed to download and parse paper https://huggingface.co/papers/2602.01842: 'LTChar' object is not iterable
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01296.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01296.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01296.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01077.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01077.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01077.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2601.22674.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2601.22674.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2601.22674.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02477.
[03.02.2026 21:28] Downloading paper 2602.02477 from https://arxiv.org/pdf/2602.02477v1...
[03.02.2026 21:28] Extracting affiliations from text.
[03.02.2026 21:28] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 ] . [ 1 7 7 4 2 0 . 2 0 6 2 : r Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability Xiao Liang1, Zhong-Zhi Li2, Zhenghao Lin2, Eric Hancheng Jiang1, Hengyuan Zhang, Yelong Shen2, Kai-Wei Chang1, Ying Nian Wu1, Yeyun Gong2:, Weizhu Chen2: 1University of California, Los Angeles 2Microsoft : Corresponding Authors Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. potential alternative is divide-and-conquer (DAC) reasoning, which decomposes complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the models capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes problem into group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks. Date: February 3, Code: https://github.com/MasterVito/DAC-RL Correspondence: Yeyun Gong (yegong@microsoft.com); Weizhu Chen (wzchen@microsoft.com) 1. Introduction Large language models (LLMs) have shown remarkable capabilities in complex reasoning tasks, with chainof-thought (CoT) (Wei et al., 2022) enabled by large-scale reinforcement learni"
[03.02.2026 21:28] Response: ```python
[
    "University of California, Los Angeles",
    "Microsoft"
]
```
[03.02.2026 21:28] Deleting PDF ./assets/pdf/2602.02477.pdf.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01997.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01997.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01997.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01983.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01983.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01983.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2601.23000.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2601.23000.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2601.23000.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.00130.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.00130.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.00130.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02354.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02354.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02354.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02287.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02287.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02287.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01970.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01970.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01970.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.00521.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.00521.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.00521.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2601.22801.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2601.22801.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2601.22801.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.00192.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.00192.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.00192.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2601.22599.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2601.22599.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2601.22599.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2601.22296.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2601.22296.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2601.22296.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2601.21968.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2601.21968.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2601.21968.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2601.21759.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2601.21759.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2601.21759.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2601.21123.
[03.02.2026 21:28] Downloading paper 2601.21123 from https://arxiv.org/pdf/2601.21123v1...
[03.02.2026 21:28] Extracting affiliations from text.
[03.02.2026 21:28] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CUA-Skill: Develop Skills for Computer Using Agent Microsoft "
[03.02.2026 21:28] Response: ```python
[]
```
[03.02.2026 21:28] Extracting affiliations from text.
[03.02.2026 21:28] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CUA-Skill: Develop Skills for Computer Using Agent MicrosoftComputer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is large-scale library of carefully engineered skills spanning common Windows applications, serving as practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUASkill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft. github.io/cua_skill/. 6 2 0 2 8 2 ] . [ 1 3 2 1 1 2 . 1 0 6 2 : r 1. Introduction Computer-Using Agents (CUAs) aim to autonomously operate graphical user interfaces (GUIs) to complete real-world desktop tasks such as document editing, web navigation, data analysis, and system configuration (Xie et al., 2024; Zhang et al., 2025c; Yang et al., 2025b; Hui et al., 2025). Recent advances in large language models (LLMs) and 1See full author list in Appendix A. Copyright 2026 by the Microsoft. Figure 1. Success rate vs. execution steps on WAA. multimodal perception have substantially improved agents abilities to interpret user intent and visually ground actions on the screen, making CUAs promising pathway toward general-purpose digital assistants capable of interacting with complex desktop environments. Despite this progress, building reliable and scalable CUAs remains challenging. Existing systems often struggle with long-horizon tasks that require executing dozens of interdependent actions across dynamic UI states. Small errors in grounding, planning, or execution can quickly compound, leading to brittle behavior and low end-to-end success rates. More fundamentally, most current approaches lack an explicit representation of how humans use computers: desktop interaction is typically modeled as flat sequences of low-level actions, forcing agents to repeatedly rediscover common workflows from scratch. In contrast to current CUAs, human computer use is inherently structured around reusable procedural knowledge. Users rely on familiar skills, such as launching applications, navigating menus, or formatting documents, which are composed into higher-level workflows and adapted to the current UI context. The lack of such reusable and structured skill abstractions remains key bottleneck for existing CUAs, limiting their scalability, generalization, and robustness on CUA-Skill: Develop Skills for Computer Using Agent Figure 2. Overview of CUA-Skill and Associated Skill-Agent. complex real-world tasks. Concurrently, Anthropic introduced the notion of agent skills as reusable, filesystem-based resources that encapsulate domain expertise (Notov, 2025). While effective in code-centric environments (e.g., Linux or API-rich systems), these skills are primarily executed through scripts and tightly integrated with the Model Context Protocol (MCP) (Anthropic, 2024a). As result, they are less suited for desktop environments such as Windows, where many applications expose limited or inconsistent programmatic APIs and effective task execution fundamentally makes them difficult to leverage across applications. Therefore, question is naturally raised: How can we build scalable and transferable skill base for desktop environments that captures human procedural knowledge and enables reliable and capable CUAs? In this work, we answer this question by introducing CUASkill, the first systematic agentic skill library designed for desktop computer use. CUA-Skill encodes human computer-use knowledge as reusable skills coupled with parameterized execution and composition graphs, forming structured intermediate layer between high-level user intent and low-level interaction primitives. While GUI primitives serve as the primary, human-aligned substrate for skill execution, the execution graph abstraction flexibly supports scriptand code-based execution paths when they offer improved reliability or efficiency. This unified parameterization makes skills transferable across tasks, UI states, and applications, enabling strong generalization. Built on top of this skill base, we develop CUA-Skill Agent, an end-to-end computer-using agent that performs retrievalaugmented skill selection, configuration, and execution. At each step, an LLM-based planner retrieves relevant skills conditioned on the current UI state and user goal, re-ranks candidates using execution context and memory, instantiates skill arguments, and executes the selected skill via GUI grounding or direct script execution, depending on the instantiated execution path. This design supports scalable skill expansion, memory-aware recovery from failures, and robust long-horizon task completion without hard-coding tools into prompts or relying on monolithic plans (Huang et al., 2023; Schick et al., 2023). Our main contributions are summarized as follows: CUA-Skill. We introduce structured agentic skill library for desktop environments that encodes human computer-use knowledge as reusable, parameterized skills with explicit execution and composition graphs. This design enables strong transferability and generalization across tasks and UI states. The initial release contains hundreds of carefully engineered atomic skills spanning tens of popular applications. Through parameterization and composition, these skills can be instantiated into millions or more executable task variants, supporting wide range of downstream agent applications. CUA-Skill Agent. To effectively utilize CUA-Skill, we propose skill-centric, retrieval-augmented agent that performs "
[03.02.2026 21:28] Mistral response. {"id": "bb9387219cff415f9d5e4ca49784b202", "created": 1770154137, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1302, "total_tokens": 1310, "completion_tokens": 8, "num_cached_tokens": 1301}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Microsoft\"]\n```"}}]}
[03.02.2026 21:28] Response: ```python
["Microsoft"]
```
[03.02.2026 21:28] Deleting PDF ./assets/pdf/2601.21123.pdf.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2601.14691.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2601.14691.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2601.14691.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.02338.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.02338.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.02338.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01897.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01897.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01897.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01815.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01815.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01815.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01618.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01618.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01618.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.01418.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.01418.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.01418.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Downloading and parsing paper https://huggingface.co/papers/2602.00168.
[03.02.2026 21:28] Extra JSON file exists (./assets/json/2602.00168.json), skip PDF parsing.
[03.02.2026 21:28] Paper image links file exists (./assets/img_data/2602.00168.json), skip HTML parsing.
[03.02.2026 21:28] Success.
[03.02.2026 21:28] Enriching papers with extra data.
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 0. Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.  					AI-generated summary 				 We introduce Green-VLA, a staged Vision-Language-Action (...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 1. Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.  					AI-generated summary 				 We introduce Kimi K2.5, an open-source multimodal agentic model designed to adva...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 2. Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning.  					AI-generated summary 				 Multimodal large langu...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 3. Vision-DeepResearch benchmark addresses limitations in evaluating visual-textual search capabilities of multimodal models by introducing realistic evaluation conditions and improving visual retrieval through multi-round cropped-search workflow.  					AI-generated summary 				 Multimodal Large Langua...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 4. RPG-Encoder framework transforms repository comprehension and generation into a unified cycle by encoding code into high-fidelity Repository Planning Graph representations that improve understanding and reconstruction accuracy.  					AI-generated summary 				 Current repository agents encounter a re...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 5. UniReason integrates text-to-image generation and image editing through a dual reasoning paradigm that enhances planning with world knowledge and uses editing for visual refinement, achieving superior performance on reasoning-intensive benchmarks.  					AI-generated summary 				 Unified multimodal m...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 6. WildGraphBench evaluates GraphRAG performance in realistic scenarios using Wikipedia's structured content to assess multi-fact aggregation and summarization capabilities across diverse document types.  					AI-generated summary 				 Graph-based Retrieval-Augmented Generation (GraphRAG) organizes ext...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 7. A file-system-based dual-agent framework enables large language model agents to perform extended research tasks beyond context window limitations by using persistent storage as external memory.  					AI-generated summary 				 Deep research is emerging as a representative long-horizon task for large ...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 8. A scalable framework for constructing real-world software engineering environments from GitHub pull requests using an efficient building agent with self-verification and hacking detection capabilities.  					AI-generated summary 				 We propose SWE-Universe, a scalable and efficient framework for au...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 9. Deep Research Agents demonstrate capabilities in autonomous information retrieval but show significant gaps when evaluated against expert-level Wikipedia articles using a new live benchmark and comprehensive evaluation framework.  					AI-generated summary 				 Deep Research Agents (DRAs) have demon...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 10. PixelGen is a pixel-space diffusion framework that uses perceptual supervision through LPIPS and DINO-based losses to generate high-quality images without requiring VAEs or latent representations.  					AI-generated summary 				 Pixel diffusion generates images directly in pixel space in an end-to-e...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 11. SLIME is a novel reference-free alignment objective for large language models that decouples preference learning from generation quality through a three-pronged approach combining likelihood maximization, probability stabilization, and dual-margin constraints.  					AI-generated summary 				 Direct ...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 12. RLAnything enhances reinforcement learning for LLMs and agents through dynamic model optimization and closed-loop feedback mechanisms that improve policy and reward model training.  					AI-generated summary 				 We propose RLAnything, a reinforcement learning framework that dynamically forges envir...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 13. PISCES is an annotation-free text-to-video generation method that uses dual optimal transport-aligned rewards to improve visual quality and semantic alignment without human preference annotations.  					AI-generated summary 				 Text-to-video (T2V) generation aims to synthesize videos with high visu...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 14. Mind-Brush presents a unified agentic framework for text-to-image generation that dynamically retrieves multimodal evidence and employs reasoning tools to improve understanding of implicit user intentions and complex knowledge reasoning.  					AI-generated summary 				 While text-to-image generation...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 15. A novel Causal Forcing method addresses the architectural gap in distilling bidirectional video diffusion models into autoregressive models by using AR teachers for ODE initialization, significantly improving video generation performance.  					AI-generated summary 				 To achieve real-time interact...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 16. Selective knowledge distillation in autoregressive language models using student-entropy-guided position selection improves accuracy and efficiency while reducing memory and storage requirements.  					AI-generated summary 				 Growing efforts to improve knowledge distillation (KD) in large language...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 17. Autoregressive video diffusion models face efficiency challenges due to growing KV caches and redundant attention computations, which are addressed through TempCache, AnnCA, and AnnSA techniques that reduce computational demands while maintaining visual quality and stable performance.  					AI-gener...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 18. Post-training of reasoning large language models can be improved by correcting distribution mismatches between supervised fine-tuning and reinforcement learning stages through importance sampling reweighting of the SFT loss.  					AI-generated summary 				 Post-training of reasoning LLMs is a holist...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 19. FSVideo is a fast transformer-based image-to-video diffusion framework that uses a compressed video autoencoder, diffusion transformer architecture with enhanced layer memory, and multi-resolution generation strategy to achieve high performance with significantly reduced computation time.  					AI-g...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 20. MLLMs equipped with Cognitive Supersensing and Latent Visual Imagery Prediction demonstrate enhanced cognitive reasoning capabilities through integrated visual and textual reasoning pathways.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have achieved remarkable success in...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 21. Visual metaphor transfer enables creative AI systems to decompose abstract conceptual relationships from reference images and reapply them to new subjects through a multi-agent framework grounded in cognitive theory.  					AI-generated summary 				 A visual metaphor constitutes a high-order form of ...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 22. Visual Instruction Benchmark for Image Editing introduces a three-level interaction hierarchy for evaluating visual instruction following capabilities in generative models.  					AI-generated summary 				 Recent generative models have achieved remarkable progress in image editing. However, existing ...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 23. A dual-stream framework called InteractAvatar is presented for generating talking avatars that can interact with objects in their environment, addressing challenges in grounded human-object interaction through decoupled perception and planning modules.  					AI-generated summary 				 Generating talk...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 24. A Japanese financial language understanding benchmark named Ebisu is introduced, featuring two expert-annotated tasks that evaluate implicit commitment recognition and hierarchical financial terminology extraction, revealing persistent challenges for current language models despite their advanced ca...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 25. Re-TRAC is an agentic framework that enhances LLM-based research agents by enabling cross-trajectory exploration and iterative reflection through structured state representations, leading to more efficient and effective problem-solving compared to traditional ReAct approaches.  					AI-generated sum...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 26. Visual world models for mobile GUI agents are improved through renderable code generation using vision-language models, achieving better performance with reduced model size compared to existing approaches.  					AI-generated summary 				 Mobile Graphical User Interface (GUI) World Models (WMs) offer...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 27. SPARKLING is a framework for mid-stage width expansion in deep learning models that maintains signal preservation and breaks symmetry to stabilize training and reduce computational costs.  					AI-generated summary 				 Progressive Learning (PL) reduces pre-training computational overhead by gradual...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 28. Large language model control methods are unified under a dynamic weight update framework, revealing a preference-utility trade-off and enabling improved steering through SPLIT approach.  					AI-generated summary 				 Methods for controlling large language models (LLMs), including local weight fine-...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 29. LatentMorph integrates implicit latent reasoning into text-to-image generation through four lightweight components that enable adaptive self-refinement and improve both efficiency and cognitive alignment.  					AI-generated summary 				 Text-to-image (T2I) generation has achieved remarkable progress...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 30. Loop-ViT introduces a recursive vision transformer architecture that decouples reasoning depth from model capacity through weight-tied recurrence and dynamic exit mechanisms, achieving superior visual reasoning performance with fewer parameters.  					AI-generated summary 				 Recent advances in vis...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 31. PolySAE extends sparse autoencoders with polynomial decoding to capture feature interactions and compositional structure while maintaining linear encoders for interpretability.  					AI-generated summary 				 Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural netwo...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 32. AgentIF-OneDay evaluates AI agents' ability to handle diverse daily tasks through natural language instructions, requiring problem-solving, attachment understanding, and file-based outputs across three user-centric categories.  					AI-generated summary 				 The capacity of AI agents to effectively ...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 33. Thinking with Comics emerges as an effective visual reasoning approach that bridges images and videos by leveraging comic structures for improved multimodal reasoning efficiency and performance.  					AI-generated summary 				 Chain-of-Thought reasoning has driven large language models to extend fro...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 34. TRIP-Bench presents a comprehensive long-horizon benchmark for travel planning that evaluates LLM agents on complex multi-turn interactions, while GTPO offers an online reinforcement learning approach to enhance constraint satisfaction and robustness in extended dialogues.  					AI-generated summary...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 35. A novel framework called CoDiQ enables controllable difficulty generation for competition-level questions through test-time scaling, resulting in a corpus that significantly improves large reasoning model performance.  					AI-generated summary 				 Large Reasoning Models (LRMs) benefit substantiall...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 36. Rubric-ARM framework jointly optimizes rubric generation and judging through reinforcement learning to improve response quality assessment in creative and open-ended tasks.  					AI-generated summary 				 Standard reward models typically predict scalar scores that fail to capture the multifaceted na...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 37. Flow matching models for text-to-image generation are enhanced through a reinforcement learning framework that addresses sample inefficiency and prompt overfitting by incorporating language models for prompt refinement, achieving superior performance with reduced computational requirements.  					AI...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 38. Research identifies a sparse reward subsystem in LLM hidden states containing value neurons that represent internal state expectations and dopamine-like neurons encoding reward prediction errors.  					AI-generated summary 				 In this paper, we identify a sparse reward subsystem within the hidden s...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 39. Adaptive Ability Decomposing (AD) enhances reinforcement learning with verifiable rewards by decomposing complex questions into simpler sub-questions, improving LLM reasoning through guided exploration without requiring a teacher model.  					AI-generated summary 				 Reinforcement learning with ve...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 40. Post-training quantization effects in world models reveal unique failure modes and trade-offs between accuracy, bit-width, and planning performance, particularly in encoder-predictor module asymmetries and low-bit rollout stability.  					AI-generated summary 				 World models learn an internal repr...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 41. Agentic large language models require investigatory intelligence for autonomous data analysis, demonstrated through the Deep Data Research benchmark that evaluates their ability to extract insights from databases without explicit queries.  					AI-generated summary 				 The agency expected of Agenti...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 42. VoxServe is a unified serving system for Speech Language Models that enhances streaming performance through model-execution abstraction, streaming-aware scheduling, and asynchronous inference pipelines.  					AI-generated summary 				 Deploying modern Speech Language Models (SpeechLMs) in streaming ...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 43. Small language models can effectively evaluate outputs by leveraging internal representations rather than generating responses, enabling a more efficient and interpretable evaluation approach through a probing-based framework.  					AI-generated summary 				 Large language models (LLMs) are widely u...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 44. Scaling hidden states of delimiter tokens in vision-language models reduces cross-image information leakage and improves multi-image reasoning performance.  					AI-generated summary 				 Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance dec...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 45. A new test-time scaling framework called Prism is introduced for discrete diffusion language models that improves reasoning performance through hierarchical trajectory search, local branching with partial remasking, and self-verified feedback mechanisms.  					AI-generated summary 				 Inference-tim...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 46. LiP-Map presents a line-plane joint optimization framework that explicitly models learnable line and planar primitives for accurate 3D line mapping in man-made environments.  					AI-generated summary 				 3D line mapping from multi-view RGB images provides a compact and structured visual representa...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 47. PISA is a novel sparse attention method that improves diffusion transformer efficiency by approximating non-critical attention blocks instead of discarding them, achieving faster processing with maintained quality.  					AI-generated summary 				 Diffusion Transformers are fundamental for video and ...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 48. VisionTrim is a training-free framework that accelerates multimodal large language models by selecting dominant visual tokens and merging them with text-guided complementation, improving efficiency without performance loss.  					AI-generated summary 				 Multimodal large language models (MLLMs) suf...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 49. An end-to-end reinforcement learning framework enhances large language models' reasoning capabilities by implementing divide-and-conquer strategies that outperform traditional chain-of-thought reasoning on challenging benchmarks.  					AI-generated summary 				 Large language models (LLMs) have demo...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 50. Layer pruning compresses large language models while maintaining classification performance but causes significant degradation in generative reasoning tasks, with limited recovery possible through supervised finetuning on self-generated responses.  					AI-generated summary 				 Recent works have sh...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 51. A training-free framework enables language model agents to automatically create and optimize tools during inference, improving their reasoning capabilities through self-evolution and memory consolidation.  					AI-generated summary 				 Existing Tool-Integrated Reasoning (TIR) models have effectivel...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 52. A novel optimizer called Mano is proposed that combines manifold optimization with momentum projection onto tangent spaces, achieving superior performance over AdamW and Muon while reducing memory and computational requirements.  					AI-generated summary 				 While large language models (LLMs) have...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 53. Effective dimension, an unsupervised geometric metric, strongly predicts neural network performance across different architectures and domains, showing bidirectional causality between representation geometry and accuracy.  					AI-generated summary 				 We investigate the relationship between repres...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 54. Implicit neural representations operate continuously over UV coordinate space, demonstrating good image quality while balancing memory usage and rendering time, with applications in real-time rendering and downstream tasks.  					AI-generated summary 				 Implicit neural representation (INR) has pro...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 55. Controlled cross-lingual evaluation reveals instability in LLM assessment methods when targeting morphologically rich languages, indicating unreliable zero-shot judge transfer for discourse-level tasks.  					AI-generated summary 				 Cross-lingual evaluation of large language models (LLMs) typicall...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 56. Generalizable Predictive Prompt Selection (GPS) uses Bayesian inference with a lightweight generative model to efficiently select informative prompts for reinforcement learning-enhanced language models, improving training efficiency and performance.  					AI-generated summary 				 Reinforcement lear...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 57. A two-phase diagnostic framework based on Item Response Theory and Graded Response Model is introduced to assess the reliability of LLM-as-a-Judge by examining intrinsic consistency and human alignment.  					AI-generated summary 				 While LLM-as-a-Judge is widely used in automated evaluation, exis...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 58. Clipping-Free Policy Optimization replaces heuristic clipping with convex quadratic penalty to stabilize reinforcement learning training for large language models without performance loss.  					AI-generated summary 				 Reinforcement learning has become central to post-training large language model...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 59. VAE-based inpainting creates spectral shifts that fool detection systems, which can be mitigated through Inpainting Exchange to improve content-aware detection performance.  					AI-generated summary 				 Modern deep learning-based inpainting enables realistic local image manipulation, raising criti...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 60. Automated pipeline for sound separation using high-purity single-event segments from in-the-wild datasets achieves competitive performance with significantly reduced data requirements.  					AI-generated summary 				 Query-based universal sound separation is fundamental to intelligent auditory syste...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 61. Parallel Echo State Network (ParalESN) addresses reservoir computing limitations by enabling parallel temporal processing through diagonal linear recurrence, maintaining theoretical guarantees while achieving significant computational efficiency gains.  					AI-generated summary 				 Reservoir Compu...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 62. On-policy Verbal Distillation (OVD) enables efficient knowledge transfer from teacher to student models by replacing token-level probability matching with trajectory matching using discrete verbal scores, reducing memory consumption and enabling free exploration without token alignment constraints. ...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 63. An reinforcement learning-based sampling framework adaptively reweights training datasets to improve embedding model performance while reducing GPU costs.  					AI-generated summary 				 General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 64. CUA-Skill introduces a large-scale library of engineered computer-use skills that enhance agent performance and efficiency on Windows-based tasks.  					AI-generated summary 				 Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 65. Large language models used as judges for agent performance evaluation are vulnerable to manipulation of reasoning traces, with content-based fabrications being more effective than style-based alterations.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as judges to...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 66. ReSID presents a novel recommendation-native framework that improves sequential recommendation by learning predictive item representations and optimizing quantization for information preservation and sequential predictability.  					AI-generated summary 				 Semantic ID (SID)-based recommendation is...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 67. Internal flow signatures analyze depthwise dynamics in large language models to enable self-checking and targeted refinement without modifying the base model.  					AI-generated summary 				 Large language models can generate fluent answers that are unfaithful to the provided context, while many saf...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 68. Multi-agent systems for molecular discovery that use individualized scientist profiles based on publication and molecular history outperform traditional role-based approaches.  					AI-generated summary 				 Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery....
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 69. Researchers developed a novel agentic data-generation framework to create culturally grounded safety datasets for Southeast Asia, resulting in multilingual safeguard models that outperform existing approaches in detecting regionally sensitive content while maintaining general safety performance.  		...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 70. Parabolic Position Encoding (PaPE) is a novel position encoding method for vision modalities that improves upon existing approaches by incorporating translation invariance, rotation invariance, distance decay, directionality, and context awareness principles.  					AI-generated summary 				 We propo...
[03.02.2026 21:28] ********************************************************************************
[03.02.2026 21:28] Abstract 71. YOLOE-26 integrates YOLO26 architecture with open-vocabulary learning for real-time instance segmentation, utilizing convolutional backbones, end-to-end regression, and object embedding heads with text and visual prompting capabilities.  					AI-generated summary 				 This paper presents YOLOE-26, a...
[03.02.2026 21:28] Read previous papers.
[03.02.2026 21:28] Generating reviews via LLM API.
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multimodal", "#data", "#rl", "#robotics", "#training"], "emoji": "", "ru": {"title": "             ", "desc": "Green-VLA     Vision-Language
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multimodal", "#plp", "#cv", "#agents", "#reasoning", "#open_source", "#training"], "emoji": "", "ru": {"title": "     ", "desc": " Kimi K2.5    -,    
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#rag", "#reasoning", "#optimization", "#rl", "#open_source"], "emoji": "", "ru": {"title": "       ", "desc": "Vision-DeepResearch      
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#dataset", "#open_source"], "emoji": "", "ru": {"title": "      ", "desc": "  Vision-DeepResearch benchmark         
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#plp"], "emoji": "", "ru": {"title": "         ", "desc": "  RPG-Encoder  ,            
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#reasoning", "#synthetic"], "emoji": "", "ru": {"title": "    :   ", "desc": "UniReason   ,          
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#dataset", "#rag", "#benchmark", "#open_source", "#graphs", "#long_context"], "emoji": "", "ru": {"title": "         ", "desc": "   WildGraphBench     GraphRAG   
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark", "#reasoning", "#open_source", "#long_context"], "emoji": "", "ru": {"title": "  :     LLM", "desc": "   FS-Researcher   ,     
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multilingual", "#plp", "#dataset", "#agents", "#benchmark", "#rl", "#training"], "emoji": "", "ru": {"title": "       ", "desc": "   SWE-Universe      
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark", "#open_source", "#survey"], "emoji": "", "ru": {"title": "   AI-   ", "desc": "    benchmark Wiki Live Challenge,    Wikipedia Good Articles  
[03.02.2026 21:28] Using data from previous issue: {"categories": [], "emoji": "", "ru": {"title": "        ", "desc": "PixelGen     ,       ,  ,   VAE  
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#training"], "emoji": "", "ru": {"title": "       ", "desc": "SLIME        ,         
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#rl", "#agents", "#training"], "emoji": "", "ru": {"title": "         ", "desc": "RLAnything      ,     ,   
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#optimization", "#rl", "#video", "#training"], "emoji": "", "ru": {"title": "       ", "desc": "PISCES       ,        .  
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#benchmark", "#rag", "#reasoning"], "emoji": "", "ru": {"title": "   -      ", "desc": "Mind-Brush        
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#architecture", "#video", "#training"], "emoji": "", "ru": {"title": " :       ", "desc": "     Causal Forcing      
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#inference", "#optimization", "#training", "#small_models"], "emoji": "", "ru": {"title": " :      ", "desc": "       (knowledge distillation)    
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#video", "#inference", "#diffusion", "#optimization", "#long_context"], "emoji": "", "ru": {"title": "           ", "desc": "         
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#math", "#reasoning", "#optimization", "#rl", "#training"], "emoji": "", "ru": {"title": "    SFT  RL   ", "desc": "    PEAR         supervised fine-t
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#architecture", "#video", "#diffusion", "#optimization", "#open_source", "#training"], "emoji": "", "ru": {"title": "  :       ", "desc": "FSVideo       
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark", "#reasoning", "#rl", "#open_source", "#training"], "emoji": "", "ru": {"title": "        ", "desc": "    Cognitive Supersensing 
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#open_source"], "emoji": "", "ru": {"title": "         ", "desc": "     Visual Metaphor Transfer,         
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#benchmark"], "emoji": "", "ru": {"title": " :     ", "desc": "  VIBE  Visual Instruction Benchmark for Image Editing,        
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#video", "#games", "#benchmark"], "emoji": "", "ru": {"title": " ,     ", "desc": "     InteractAvatar     ,  
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#science", "#benchmark", "#low_resource", "#open_source"], "emoji": "", "ru": {"title": " :  LLM     ", "desc": "     Ebisu      
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#long_context", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": " :    - ", "desc": "Re-TRAC          ,     
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#cv", "#agents", "#benchmark", "#small_models"], "emoji": "", "ru": {"title": "   :       -", "desc": "           GUI
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "", "ru": {"title": "   :      ", "desc": "SPARKLING           ,   
[03.02.2026 21:28] Using data from previous issue: {"categories": [], "emoji": "", "ru": {"title": "    :    LLM", "desc": "         (LLM) -     LoRA    -    
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#reasoning", "#optimization", "#rl", "#training"], "emoji": "", "ru": {"title": "      ", "desc": "LatentMorph        ,    
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#architecture", "#cv", "#benchmark", "#reasoning", "#optimization", "#open_source", "#small_models"], "emoji": "", "ru": {"title": "       ", "desc": "Loop-ViT      
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#interpretability", "#architecture", "#training"], "emoji": "", "ru": {"title": "       ", "desc": "PolySAE    (SAE)    ,  
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset"], "emoji": "", "ru": {"title": "  AI     ", "desc": "    AgentIF-OneDay    AI     ,  
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multimodal", "#video", "#cv", "#benchmark", "#reasoning", "#long_context"], "emoji": "", "ru": {"title": "           ", "desc": "       
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#dataset", "#agents", "#benchmark", "#reasoning", "#optimization", "#rl", "#long_context"], "emoji": "", "ru": {"title": "  :      ", "desc": "   TRIP-Bench     
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#synthetic", "#open_source", "#reasoning"], "emoji": "", "ru": {"title": "          ", "desc": "     CoDiQ         
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#rl", "#rlhf", "#benchmark"], "emoji": "", "ru": {"title": "         ", "desc": "Rubric-ARM   ,             
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#rl", "#rlhf", "#training"], "emoji": "", "ru": {"title": "        ", "desc": "   PromptRL  ,    flow matching     
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#transfer_learning", "#interpretability", "#training"], "emoji": "", "ru": {"title": "     ", "desc": "         
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#optimization", "#rl", "#training", "#reasoning"], "emoji": "", "ru": {"title": " :      ", "desc": "    Adaptive Ability Decomposing (AD)         
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#inference", "#rl", "#agents"], "emoji": "", "ru": {"title": "    :    ", "desc": "    -   world models,       
[03.02.2026 21:28] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset"], "emoji": "", "ru": {"title": " :       ", "desc": "        ,    
[03.02.2026 21:28] Querying the API.
[03.02.2026 21:28] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

VoxServe is a unified serving system for Speech Language Models that enhances streaming performance through model-execution abstraction, streaming-aware scheduling, and asynchronous inference pipelines.  					AI-generated summary 				 Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.
[03.02.2026 21:29] Response: ```json
{
  "desc": "VoxServe         ,     .    -,       ,     SpeechLM   .   -     pipeline     .   ,  VoxServe   10-20            .",
  "emoji": "",
  "title": "     "
}
```
[03.02.2026 21:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VoxServe is a unified serving system for Speech Language Models that enhances streaming performance through model-execution abstraction, streaming-aware scheduling, and asynchronous inference pipelines.  					AI-generated summary 				 Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve."

[03.02.2026 21:29] Response: ```python
["AUDIO", "INFERENCE"]
```

**Justification:**
- **AUDIO**: The paper focuses on Speech Language Models (SpeechLMs) and their deployment, which directly relates to audio processing and generation.
- **INFERENCE**: The paper is explicitly about optimizing model deployment through a serving system that improves latency, throughput, and efficiency - core concerns of inference optimization.
[03.02.2026 21:29] Error. Failed to parse JSON from LLM. ["AUDIO", "INFERENCE"]


**Justification:**
- **AUDIO**: The paper focuses on Speech Language Models (SpeechLMs) and their deployment, which directly relates to audio processing and generation.
- **INFERENCE**: The paper is explicitly about optimizing model deployment through a serving system that improves latency, throughput, and efficiency - core concerns of inference optimization.
[03.02.2026 21:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VoxServe is a unified serving system for Speech Language Models that enhances streaming performance through model-execution abstraction, streaming-aware scheduling, and asynchronous inference pipelines.  					AI-generated summary 				 Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve."

[03.02.2026 21:29] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```

**Justification:**
- **OPTIMIZATION**: The paper focuses on optimizing serving systems for Speech Language Models through techniques like model-execution abstraction, streaming-aware scheduling, and asynchronous inference pipelines to achieve better throughput and latency.
- **OPEN_SOURCE**: The paper explicitly states "The code of VoxServe is available at https://github.com/vox-serve/vox-serve," indicating the authors are releasing their system as open-source.
[03.02.2026 21:29] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**
- **OPTIMIZATION**: The paper focuses on optimizing serving systems for Speech Language Models through techniques like model-execution abstraction, streaming-aware scheduling, and asynchronous inference pipelines to achieve better throughput and latency.
- **OPEN_SOURCE**: The paper explicitly states "The code of VoxServe is available at https://github.com/vox-serve/vox-serve," indicating the authors are releasing their system as open-source.
[03.02.2026 21:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VoxServe is a new system designed to improve the performance of Speech Language Models (SpeechLMs) in real-time applications. It achieves this by separating the model architecture from the system optimizations, allowing for a variety of SpeechLMs to be used efficiently. The system employs advanced scheduling techniques and an asynchronous inference pipeline to enhance processing speed and reduce delays. Tests show that VoxServe can process data 10-20 times faster than current systems while still ensuring effective streaming capabilities.","title":"VoxServe: Boosting Speech Model Performance in Real-Time"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VoxServe is a new system designed to improve the performance of Speech Language Models (SpeechLMs) in real-time applications. It achieves this by separating the model architecture from the system optimizations, allowing for a variety of SpeechLMs to be used efficiently. The system employs advanced scheduling techniques and an asynchronous inference pipeline to enhance processing speed and reduce delays. Tests show that VoxServe can process data 10-20 times faster than current systems while still ensuring effective streaming capabilities.', title='VoxServe: Boosting Speech Model Performance in Real-Time'))
[03.02.2026 21:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"VoxServeVoxServeVoxServe10-20","title":"VoxServe"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='VoxServeVoxServeVoxServe10-20', title='VoxServe'))
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#benchmark", "#training", "#small_models"], "emoji": "", "ru": {"title": "  :       ", "desc": " ,         ,  
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#leakage", "#benchmark", "#reasoning"], "emoji": "", "ru": {"title": "       ", "desc": "         -
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#inference", "#benchmark", "#reasoning", "#diffusion", "#optimization", "#open_source", "#training"], "emoji": "", "ru": {"title": " :        ", "desc": "    Prism  
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#3d", "#cv"], "emoji": "", "ru": {"title": "       3D-", "desc": "LiP-Map               RGB-.
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#multimodal", "#video", "#architecture", "#inference", "#diffusion", "#optimization"], "emoji": "", "ru": {"title": "  ,  :   ", "desc": "    PISA,    
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#multimodal", "#inference", "#video"], "emoji": "", "ru": {"title": "       ", "desc": "VisionTrim             
[03.02.2026 21:29] Querying the API.
[03.02.2026 21:29] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

An end-to-end reinforcement learning framework enhances large language models' reasoning capabilities by implementing divide-and-conquer strategies that outperform traditional chain-of-thought reasoning on challenging benchmarks.  					AI-generated summary 				 Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.
[03.02.2026 21:29] Response: ```json
{
  "desc": "                    .               ,    .  RL-       ,         .           8,6%   Pass@1  6,3%  Pass@32.",
  "emoji": "",
  "title": "       "
}
```
[03.02.2026 21:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"An end-to-end reinforcement learning framework enhances large language models' reasoning capabilities by implementing divide-and-conquer strategies that outperform traditional chain-of-thought reasoning on challenging benchmarks.  					AI-generated summary 				 Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks."

[03.02.2026 21:29] Response: ```python
['RL', 'BENCHMARK', 'TRAINING']
```
[03.02.2026 21:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"An end-to-end reinforcement learning framework enhances large language models' reasoning capabilities by implementing divide-and-conquer strategies that outperform traditional chain-of-thought reasoning on challenging benchmarks.  					AI-generated summary 				 Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks."

[03.02.2026 21:29] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[03.02.2026 21:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel reinforcement learning framework that improves the reasoning abilities of large language models (LLMs) by using a divide-and-conquer (DAC) approach. Unlike traditional chain-of-thought (CoT) reasoning, which processes tasks sequentially, DAC breaks down complex problems into smaller, manageable subproblems, allowing for more efficient exploration of solutions. The authors identify a misalignment between standard post-training methods and DAC inference, which limits the effectiveness of LLMs in challenging scenarios. By integrating DAC reasoning into an end-to-end RL training process, the proposed framework significantly enhances model performance, achieving better results on competitive benchmarks compared to CoT reasoning.","title":"Unlocking Reasoning Power with Divide-and-Conquer Reinforcement Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel reinforcement learning framework that improves the reasoning abilities of large language models (LLMs) by using a divide-and-conquer (DAC) approach. Unlike traditional chain-of-thought (CoT) reasoning, which processes tasks sequentially, DAC breaks down complex problems into smaller, manageable subproblems, allowing for more efficient exploration of solutions. The authors identify a misalignment between standard post-training methods and DAC inference, which limits the effectiveness of LLMs in challenging scenarios. By integrating DAC reasoning into an end-to-end RL training process, the proposed framework significantly enhances model performance, achieving better results on competitive benchmarks compared to CoT reasoning.', title='Unlocking Reasoning Power with Divide-and-Conquer Reinforcement Learning'))
[03.02.2026 21:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LLM","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LLM', title=''))
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#inference", "#benchmark", "#reasoning", "#optimization", "#training"], "emoji": "", "ru": {"title": "   :     ,  ", "desc": "         
[03.02.2026 21:29] Using data from previous issue: {"categories": [], "emoji": "", "ru": {"title": ",       ", "desc": "  UCT,          .      
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#optimization"], "emoji": "", "ru": {"title": "          ", "desc": "    Mano,           
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#architecture", "#cv", "#benchmark", "#interpretability", "#training"], "emoji": "", "ru": {"title": "       ", "desc": "          
[03.02.2026 21:29] Using data from previous issue: {"categories": [], "emoji": "", "ru": {"title": "     ", "desc": "       (INR)  ,      UV-   . 
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#multilingual", "#machine_translation", "#dataset", "#benchmark", "#low_resource", "#data", "#open_source"], "emoji": "", "ru": {"title": "  :  -  LLM    ", "desc": "   
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#rl", "#training", "#small_models"], "emoji": "", "ru": {"title": "      ", "desc": "    Generalizable Predictive Prompt Selection (GPS),    
[03.02.2026 21:29] Using data from previous issue: {"categories": [], "emoji": "", "ru": {"title": "  LLM-      ", "desc": "                   LLM-as-a-
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#reasoning", "#alignment", "#optimization", "#rl", "#rlhf", "#training"], "emoji": "", "ru": {"title": "        LLM", "desc": "  Clipping-Free Policy Optimization (CFPO)         
[03.02.2026 21:29] Using data from previous issue: {"categories": [], "emoji": "", "ru": {"title": "  VAE-:     - ", "desc": "     ,    VAE-based .  ,  
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#data", "#dataset", "#synthetic", "#open_source", "#audio"], "emoji": "", "ru": {"title": "    :      ", "desc": "        , 
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#training"], "emoji": "", "ru": {"title": "  :     ", "desc": "   Parallel Echo State Network (ParalESN)     reservoir computing,   
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#transfer_learning", "#optimization", "#rl", "#training"], "emoji": "", "ru": {"title": " :     - ", "desc": "   On-policy Verbal Distillation (OVD), 
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#multilingual", "#data", "#rl", "#optimization", "#training", "#small_models"], "emoji": "", "ru": {"title": " weighting:   ,   ", "desc": "     Inf-DDS,       
[03.02.2026 21:29] Querying the API.
[03.02.2026 21:29] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

CUA-Skill introduces a large-scale library of engineered computer-use skills that enhance agent performance and efficiency on Windows-based tasks.  					AI-generated summary 				 Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.
[03.02.2026 21:29] Response: ```json
{
  "desc": "   CUA-Skill       ,       .         Windows          .       CUA-Skill Agent  ,    ,         .            ,  57.5%   WindowsAgentArena        .",
  "emoji": "",
  "title": "     "
}
```
[03.02.2026 21:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CUA-Skill introduces a large-scale library of engineered computer-use skills that enhance agent performance and efficiency on Windows-based tasks.  					AI-generated summary 				 Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/."

[03.02.2026 21:29] Response: ```python
["AGENTS", "BENCHMARK"]
```

**Justification:**
- **AGENTS**: The paper explicitly focuses on Computer-Using Agents (CUAs), autonomous agents that operate computer systems, and introduces CUA-Skill Agent as an end-to-end computer-using agent with skill retrieval and execution capabilities.
- **BENCHMARK**: The paper evaluates the proposed approach on WindowsAgentArena, which is described as a challenging end-to-end agent benchmark, and reports performance metrics (57.5% success rate).
[03.02.2026 21:29] Error. Failed to parse JSON from LLM. ["AGENTS", "BENCHMARK"]


**Justification:**
- **AGENTS**: The paper explicitly focuses on Computer-Using Agents (CUAs), autonomous agents that operate computer systems, and introduces CUA-Skill Agent as an end-to-end computer-using agent with skill retrieval and execution capabilities.
- **BENCHMARK**: The paper evaluates the proposed approach on WindowsAgentArena, which is described as a challenging end-to-end agent benchmark, and reports performance metrics (57.5% success rate).
[03.02.2026 21:29] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CUA-Skill introduces a large-scale library of engineered computer-use skills that enhance agent performance and efficiency on Windows-based tasks.  					AI-generated summary 				 Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/."

[03.02.2026 21:29] Response: ```python
['OPEN_SOURCE']
```
[03.02.2026 21:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CUA-Skill is a comprehensive library designed to enhance the performance of computer-using agents (CUAs) in executing tasks on Windows systems. It addresses the challenge of scaling agentic systems by providing structured skill abstractions that mimic human interactions with graphical user interfaces. The library includes a wide range of engineered skills and supports dynamic skill retrieval and memory-aware failure recovery, making it a robust tool for agent development. The CUA-Skill Agent, built on this foundation, demonstrates significant improvements in execution success rates and efficiency compared to existing methods.","title":"Empowering Agents with Human-like Computer Skills"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CUA-Skill is a comprehensive library designed to enhance the performance of computer-using agents (CUAs) in executing tasks on Windows systems. It addresses the challenge of scaling agentic systems by providing structured skill abstractions that mimic human interactions with graphical user interfaces. The library includes a wide range of engineered skills and supports dynamic skill retrieval and memory-aware failure recovery, making it a robust tool for agent development. The CUA-Skill Agent, built on this foundation, demonstrates significant improvements in execution success rates and efficiency compared to existing methods.', title='Empowering Agents with Human-like Computer Skills'))
[03.02.2026 21:29] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CUA-SkillWindowsCUA-Skill AgentCUA-Skill","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CUA-SkillWindowsCUA-Skill AgentCUA-Skill', title=''))
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "", "ru": {"title": " LLM-      ", "desc": "  ,    ,     ,      
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#architecture", "#training"], "emoji": "", "ru": {"title": "      ", "desc": "ReSID      ,    ,   
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#architecture", "#hallucinations", "#interpretability", "#open_source", "#training"], "emoji": "", "ru": {"title": "  LLM:      ", "desc": "         
[03.02.2026 21:29] Using data from previous issue: {"categories": [], "emoji": "", "ru": {"title": "  :      ", "desc": "    INDIBATOR    ,    .      
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#agents", "#benchmark", "#low_resource", "#alignment", "#synthetic", "#open_source"], "emoji": "", "ru": {"title": "-    - ", "desc": "     
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#optimization", "#cv", "#multimodal", "#architecture"], "emoji": "", "ru": {"title": "      ", "desc": "       (PaPE)   ,   
[03.02.2026 21:29] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#cv", "#open_source"], "emoji": "", "ru": {"title": "YOLO26   :     ", "desc": "YOLOE-26   YOLO26         
[03.02.2026 21:29] Renaming data file.
[03.02.2026 21:29] Renaming previous data. hf_papers.json to ./d/2026-02-03.json
[03.02.2026 21:29] Saving new data file.
[03.02.2026 21:29] Generating page.
[03.02.2026 21:29] Renaming previous page.
[03.02.2026 21:29] Renaming previous data. index.html to ./d/2026-02-03.html
[03.02.2026 21:29] Writing result.
[03.02.2026 21:29] Renaming log file.
[03.02.2026 21:29] Renaming previous data. log.txt to ./logs/2026-02-03_last_log.txt
