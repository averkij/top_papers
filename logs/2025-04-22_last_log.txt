[22.04.2025 04:14] Read previous papers.
[22.04.2025 04:14] Generating top page (month).
[22.04.2025 04:14] Writing top page (month).
[22.04.2025 05:11] Read previous papers.
[22.04.2025 05:11] Get feed.
[22.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.14945
[22.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.14396
[22.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.13958
[22.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.14603
[22.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.13203
[22.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15257
[22.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15280
[22.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.13805
[22.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15217
[22.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.14655
[22.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.14239
[22.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15047
[22.04.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.13941
[22.04.2025 05:11] Extract page data from URL. URL: https://huggingface.co/papers/2504.14717
[22.04.2025 05:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.04.2025 05:11] No deleted papers detected.
[22.04.2025 05:11] Downloading and parsing papers (pdf, html). Total: 14.
[22.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.14945.
[22.04.2025 05:11] Extra JSON file exists (./assets/json/2504.14945.json), skip PDF parsing.
[22.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.14945.json), skip HTML parsing.
[22.04.2025 05:11] Success.
[22.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.14396.
[22.04.2025 05:11] Extra JSON file exists (./assets/json/2504.14396.json), skip PDF parsing.
[22.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.14396.json), skip HTML parsing.
[22.04.2025 05:11] Success.
[22.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.13958.
[22.04.2025 05:11] Extra JSON file exists (./assets/json/2504.13958.json), skip PDF parsing.
[22.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.13958.json), skip HTML parsing.
[22.04.2025 05:11] Success.
[22.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.14603.
[22.04.2025 05:11] Extra JSON file exists (./assets/json/2504.14603.json), skip PDF parsing.
[22.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.14603.json), skip HTML parsing.
[22.04.2025 05:11] Success.
[22.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.13203.
[22.04.2025 05:11] Extra JSON file exists (./assets/json/2504.13203.json), skip PDF parsing.
[22.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.13203.json), skip HTML parsing.
[22.04.2025 05:11] Success.
[22.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.15257.
[22.04.2025 05:11] Extra JSON file exists (./assets/json/2504.15257.json), skip PDF parsing.
[22.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.15257.json), skip HTML parsing.
[22.04.2025 05:11] Success.
[22.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.15280.
[22.04.2025 05:11] Extra JSON file exists (./assets/json/2504.15280.json), skip PDF parsing.
[22.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.15280.json), skip HTML parsing.
[22.04.2025 05:11] Success.
[22.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.13805.
[22.04.2025 05:11] Extra JSON file exists (./assets/json/2504.13805.json), skip PDF parsing.
[22.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.13805.json), skip HTML parsing.
[22.04.2025 05:11] Success.
[22.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.15217.
[22.04.2025 05:11] Extra JSON file exists (./assets/json/2504.15217.json), skip PDF parsing.
[22.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.15217.json), skip HTML parsing.
[22.04.2025 05:11] Success.
[22.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.14655.
[22.04.2025 05:11] Extra JSON file exists (./assets/json/2504.14655.json), skip PDF parsing.
[22.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.14655.json), skip HTML parsing.
[22.04.2025 05:11] Success.
[22.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.14239.
[22.04.2025 05:11] Extra JSON file exists (./assets/json/2504.14239.json), skip PDF parsing.
[22.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.14239.json), skip HTML parsing.
[22.04.2025 05:11] Success.
[22.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.15047.
[22.04.2025 05:11] Extra JSON file exists (./assets/json/2504.15047.json), skip PDF parsing.
[22.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.15047.json), skip HTML parsing.
[22.04.2025 05:11] Success.
[22.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.13941.
[22.04.2025 05:11] Extra JSON file exists (./assets/json/2504.13941.json), skip PDF parsing.
[22.04.2025 05:11] Paper image links file exists (./assets/img_data/2504.13941.json), skip HTML parsing.
[22.04.2025 05:11] Success.
[22.04.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2504.14717.
[22.04.2025 05:11] Downloading paper 2504.14717 from http://arxiv.org/pdf/2504.14717v1...
[22.04.2025 05:12] Extracting affiliations from text.
[22.04.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"TAPIP3D: Tracking Any Point in Persistent 3D Geometry Bowei Zhang1,2* 1Carnegie Mellon University Lei Ke1* Adam W. Harley3 2Peking University Katerina Fragkiadaki1 3Stanford University 5 2 0 2 0 ] . [ 1 7 1 7 4 1 . 4 0 5 2 : r a "
[22.04.2025 05:12] Response: ```python
["Carnegie Mellon University", "Peking University", "Stanford University"]
```
[22.04.2025 05:12] Deleting PDF ./assets/pdf/2504.14717.pdf.
[22.04.2025 05:12] Success.
[22.04.2025 05:12] Enriching papers with extra data.
[22.04.2025 05:12] ********************************************************************************
[22.04.2025 05:12] Abstract 0. Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning (RL) with simple rule-based rewards. However, existing zero-RL approaches are inherently ``on-policy'', limiting learning t...
[22.04.2025 05:12] ********************************************************************************
[22.04.2025 05:12] Abstract 1. The increasing demand for AR/VR applications has highlighted the need for high-quality 360-degree panoramic content. However, generating high-quality 360-degree panoramic images and videos remains a challenging task due to the severe distortions introduced by equirectangular projection (ERP). Existi...
[22.04.2025 05:12] ********************************************************************************
[22.04.2025 05:12] Abstract 2. Current Large Language Models (LLMs) often undergo supervised fine-tuning (SFT) to acquire tool use capabilities. However, SFT struggles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learning (RL), particularly with R1-like models, have demonstrated ...
[22.04.2025 05:12] ********************************************************************************
[22.04.2025 05:12] Abstract 3. Recent Computer-Using Agents (CUAs), powered by multimodal large language models (LLMs), offer a promising direction for automating complex desktop workflows through natural language. However, most existing CUAs remain conceptual prototypes, hindered by shallow OS integration, fragile screenshot-bas...
[22.04.2025 05:12] ********************************************************************************
[22.04.2025 05:12] Abstract 4. Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-te...
[22.04.2025 05:12] ********************************************************************************
[22.04.2025 05:12] Abstract 5. This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first en...
[22.04.2025 05:12] ********************************************************************************
[22.04.2025 05:12] Abstract 6. Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown imp...
[22.04.2025 05:12] ********************************************************************************
[22.04.2025 05:12] Abstract 7. Mobile GUI agents show promise in automating tasks but face generalization challenges in diverse real-world scenarios. Traditional approaches using pre-training or fine-tuning with massive datasets struggle with the diversity of mobile applications and user-specific tasks. We propose enhancing mobil...
[22.04.2025 05:12] ********************************************************************************
[22.04.2025 05:12] Abstract 8. We present Distributional RewArds for Generative OptimizatioN (DRAGON), a versatile framework for fine-tuning media generation models towards a desired outcome. Compared with traditional reinforcement learning with human feedback (RLHF) or pairwise preference approaches such as direct preference opt...
[22.04.2025 05:12] ********************************************************************************
[22.04.2025 05:12] Abstract 9. We introduce LeetCodeDataset, a high-quality benchmark for evaluating and training code-generation models, addressing two key challenges in LLM research: the lack of reasoning-focused coding benchmarks and self-contained training testbeds. By curating LeetCode Python problems with rich metadata, bro...
[22.04.2025 05:12] ********************************************************************************
[22.04.2025 05:12] Abstract 10. Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoni...
[22.04.2025 05:12] ********************************************************************************
[22.04.2025 05:12] Abstract 11. Large Language Models (LLMs) exhibit remarkable capabilities but are susceptible to adversarial prompts that exploit vulnerabilities to produce unsafe or biased outputs. Existing red-teaming methods often face scalability challenges, resource-intensive requirements, or limited diversity in attack st...
[22.04.2025 05:12] ********************************************************************************
[22.04.2025 05:12] Abstract 12. Large Language Models (LLMs) have shown strong reasoning capabilities, particularly when enhanced through Reinforcement Learning (RL). While prior work has successfully applied RL to mathematical reasoning -- where rules and correctness are well-defined -- generalizing these methods to broader reaso...
[22.04.2025 05:12] ********************************************************************************
[22.04.2025 05:12] Abstract 13. We introduce TAPIP3D, a novel approach for long-term 3D point tracking in monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized spatio-temporal feature clouds, leveraging depth and camera motion information to lift 2D video features into a 3D world space where camera motion ...
[22.04.2025 05:12] Read previous papers.
[22.04.2025 05:12] Generating reviews via LLM API.
[22.04.2025 05:12] Using data from previous issue: {"categories": ["#training", "#reasoning", "#math", "#optimization", "#rl"], "emoji": "üß†", "ru": {"title": "LUFFY: –æ–±—É—á–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —Å –±–∞–ª–∞–Ω—Å–æ–º –º–µ–∂–¥—É –∏–º–∏—Ç–∞—Ü–∏–µ–π –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º", "desc": "LUFFY - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏
[22.04.2025 05:12] Using data from previous issue: {"categories": ["#3d", "#diffusion", "#multimodal", "#open_source", "#video"], "emoji": "üåê", "ru": {"title": "SphereDiff: –ë–µ—Å—à–æ–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∞–Ω–æ—Ä–∞–º 360¬∞ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "SphereDiff - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ —Å –æ–±–∑–æ—Ä–æ–º 360 –≥—Ä–∞–¥—É—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑
[22.04.2025 05:12] Using data from previous issue: {"categories": ["#training", "#open_source", "#rlhf", "#survey", "#reasoning", "#optimization", "#benchmark", "#rl"], "emoji": "üõ†Ô∏è", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤ LLM –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤–æ–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–∞
[22.04.2025 05:12] Using data from previous issue: {"categories": ["#agents", "#architecture", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "UFO2: –ù–∞–¥–µ–∂–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è Windows —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –∏ –≥–ª—É–±–æ–∫–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –û–°", "desc": "UFO2 - —ç—Ç–æ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—á–µ–≥–æ —Å—Ç–æ–ª–∞ Windows, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω
[22.04.2025 05:12] Using data from previous issue: {"categories": ["#alignment", "#training", "#open_source", "#security", "#agents", "#dataset"], "emoji": "üõ°Ô∏è", "ru": {"title": "–£–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–º –¥–∏–∞–ª–æ–≥–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç X-Teaming - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –∞—Ç–∞–∫ –Ω–∞ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏. 
[22.04.2025 05:12] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#reasoning", "#rl", "#agents"], "emoji": "ü§ñ", "ru": {"title": "FlowReasoner: –£–º–Ω—ã–π –¥–∏–∑–∞–π–Ω–µ—Ä –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω FlowReasoner - –º–µ—Ç–∞-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ —É—Ä–æ–≤–Ω–µ –∑–∞–ø—Ä–æ—Å–æ–≤. –û
[22.04.2025 05:12] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#survey", "#reasoning", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –≤—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ All-Angles Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[22.04.2025 05:12] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#transfer_learning", "#agents"], "emoji": "üì±", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –º–æ–±–∏–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–º
[22.04.2025 05:12] Using data from previous issue: {"categories": ["#audio", "#rlhf", "#diffusion", "#optimization", "#training"], "emoji": "üêâ", "ru": {"title": "DRAGON: –≥–∏–±–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞", "desc": "DRAGON - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –æ–±–ª–∞—Å—Ç–∏ –º–µ–¥–∏–∞. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä
[22.04.2025 05:12] Using data from previous issue: {"categories": ["#training", "#open_source", "#reasoning", "#data", "#optimization", "#benchmark", "#dataset"], "emoji": "üß†", "ru": {"title": "LeetCodeDataset: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ò–ò –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "LeetCodeDataset –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥
[22.04.2025 05:12] Using data from previous issue: {"categories": ["#games", "#reasoning", "#multimodal", "#rl", "#agents", "#training"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç —Ä–µ–∞–∫—Ç–∏–≤–Ω—ã—Ö –∞–∫—Ç—ë—Ä–æ–≤ –∫ –æ–±–¥—É–º—ã–≤–∞—é—â–∏–º —Ä–∞—Å—Å—É–∂–¥–∞—Ç–µ–ª—è–º –≤ GUI-–∞–≥–µ–Ω—Ç–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ (GUI) 
[22.04.2025 05:12] Using data from previous issue: {"categories": ["#security", "#benchmark", "#data", "#open_source", "#dataset"], "emoji": "üåà", "ru": {"title": "RainbowPlus: –≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–≤—ã—à–µ–Ω–∏—é –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RainbowPlus - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[22.04.2025 05:12] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#data", "#rl", "#transfer_learning", "#math"], "emoji": "üß†", "ru": {"title": "–ú–Ω–æ–≥–æ–¥–æ–º–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç NEMOTRON-CROSSTHINK - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[22.04.2025 05:12] Querying the API.
[22.04.2025 05:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce TAPIP3D, a novel approach for long-term 3D point tracking in monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized spatio-temporal feature clouds, leveraging depth and camera motion information to lift 2D video features into a 3D world space where camera motion is effectively canceled. TAPIP3D iteratively refines multi-frame 3D motion estimates within this stabilized representation, enabling robust tracking over extended periods. To manage the inherent irregularities of 3D point distributions, we propose a Local Pair Attention mechanism. This 3D contextualization strategy effectively exploits spatial relationships in 3D, forming informative feature neighborhoods for precise 3D trajectory estimation. Our 3D-centric approach significantly outperforms existing 3D point tracking methods and even enhances 2D tracking accuracy compared to conventional 2D pixel trackers when accurate depth is available. It supports inference in both camera coordinates (i.e., unstabilized) and world coordinates, and our results demonstrate that compensating for camera motion improves tracking performance. Our approach replaces the conventional 2D square correlation neighborhoods used in prior 2D and 3D trackers, leading to more robust and accurate results across various 3D point tracking benchmarks. Project Page: https://tapip3d.github.io
[22.04.2025 05:12] Response: {
  "desc": "TAPIP3D - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è 3D —Ç–æ—á–µ–∫ –≤ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö RGB –∏ RGB-D –≤–∏–¥–µ–æ. –û–Ω –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≤–∏–¥–µ–æ –∫–∞–∫ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –∫–∞–º–µ—Ä–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–±–ª–∞–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥–ª—É–±–∏–Ω–µ –∏ –¥–≤–∏–∂–µ–Ω–∏–∏ –∫–∞–º–µ—Ä—ã. TAPIP3D –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É—Ç–æ—á–Ω—è–µ—Ç –æ—Ü–µ–Ω–∫–∏ 3D –¥–≤–∏–∂–µ–Ω–∏—è –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞–¥—Ä–∞—Ö, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç—ã –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ 3D –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.",
  "emoji": "üé•",
  "title": "–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ 3D –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Ç–æ—á–µ–∫ –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±–ª–∞–∫–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
}
[22.04.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce TAPIP3D, a novel approach for long-term 3D point tracking in monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized spatio-temporal feature clouds, leveraging depth and camera motion information to lift 2D video features into a 3D world space where camera motion is effectively canceled. TAPIP3D iteratively refines multi-frame 3D motion estimates within this stabilized representation, enabling robust tracking over extended periods. To manage the inherent irregularities of 3D point distributions, we propose a Local Pair Attention mechanism. This 3D contextualization strategy effectively exploits spatial relationships in 3D, forming informative feature neighborhoods for precise 3D trajectory estimation. Our 3D-centric approach significantly outperforms existing 3D point tracking methods and even enhances 2D tracking accuracy compared to conventional 2D pixel trackers when accurate depth is available. It supports inference in both camera coordinates (i.e., unstabilized) and world coordinates, and our results demonstrate that compensating for camera motion improves tracking performance. Our approach replaces the conventional 2D square correlation neighborhoods used in prior 2D and 3D trackers, leading to more robust and accurate results across various 3D point tracking benchmarks. Project Page: https://tapip3d.github.io"

[22.04.2025 05:12] Response: ```python
["3D", "BENCHMARK"]
```
[22.04.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce TAPIP3D, a novel approach for long-term 3D point tracking in monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized spatio-temporal feature clouds, leveraging depth and camera motion information to lift 2D video features into a 3D world space where camera motion is effectively canceled. TAPIP3D iteratively refines multi-frame 3D motion estimates within this stabilized representation, enabling robust tracking over extended periods. To manage the inherent irregularities of 3D point distributions, we propose a Local Pair Attention mechanism. This 3D contextualization strategy effectively exploits spatial relationships in 3D, forming informative feature neighborhoods for precise 3D trajectory estimation. Our 3D-centric approach significantly outperforms existing 3D point tracking methods and even enhances 2D tracking accuracy compared to conventional 2D pixel trackers when accurate depth is available. It supports inference in both camera coordinates (i.e., unstabilized) and world coordinates, and our results demonstrate that compensating for camera motion improves tracking performance. Our approach replaces the conventional 2D square correlation neighborhoods used in prior 2D and 3D trackers, leading to more robust and accurate results across various 3D point tracking benchmarks. Project Page: https://tapip3d.github.io"

[22.04.2025 05:12] Response: ```python
["LONG_CONTEXT"]
```
[22.04.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TAPIP3D is a new method designed for tracking 3D points over long periods using monocular RGB and RGB-D videos. It transforms videos into stabilized spatio-temporal feature clouds, which helps to eliminate the effects of camera motion. The method refines 3D motion estimates through a Local Pair Attention mechanism, allowing for better handling of irregular 3D point distributions. TAPIP3D not only improves the accuracy of 3D tracking but also enhances 2D tracking when depth information is available, outperforming traditional tracking methods.","title":"Revolutionizing 3D Point Tracking with TAPIP3D"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TAPIP3D is a new method designed for tracking 3D points over long periods using monocular RGB and RGB-D videos. It transforms videos into stabilized spatio-temporal feature clouds, which helps to eliminate the effects of camera motion. The method refines 3D motion estimates through a Local Pair Attention mechanism, allowing for better handling of irregular 3D point distributions. TAPIP3D not only improves the accuracy of 3D tracking but also enhances 2D tracking when depth information is available, outperforming traditional tracking methods.', title='Revolutionizing 3D Point Tracking with TAPIP3D'))
[22.04.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïTAPIP3DÔºåÁî®‰∫éÂú®ÂçïÁõÆRGBÂíåRGB-DËßÜÈ¢ë‰∏≠ËøõË°åÈïøÊúü3DÁÇπË∑üË∏™„ÄÇTAPIP3DÈÄöËøáÊ∑±Â∫¶ÂíåÁõ∏Êú∫ËøêÂä®‰ø°ÊÅØÔºåÂ∞ÜËßÜÈ¢ëË°®Á§∫‰∏∫Á®≥ÂÆöÁöÑÊó∂Á©∫ÁâπÂæÅ‰∫ëÔºå‰ªéËÄåÂ∞Ü2DËßÜÈ¢ëÁâπÂæÅÊèêÂçáÂà∞3D‰∏ñÁïåÁ©∫Èó¥„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ±ÄÈÉ®ÂØπÊ≥®ÊÑèÂäõÊú∫Âà∂Â§ÑÁêÜ3DÁÇπÂàÜÂ∏ÉÁöÑ‰∏çËßÑÂàôÊÄßÔºåÊúâÊïàÂà©Áî®3DÁ©∫Èó¥ÂÖ≥Á≥ªÔºåÂΩ¢Êàê‰ø°ÊÅØ‰∏∞ÂØåÁöÑÁâπÂæÅÈÇªÂüüÔºå‰ª•ÂÆûÁé∞Á≤æÁ°ÆÁöÑ3DËΩ®Ëøπ‰º∞ËÆ°„ÄÇÊàë‰ª¨ÁöÑ3D‰∏≠ÂøÉÊñπÊ≥ïÂú®Â§ö‰∏™3DÁÇπË∑üË∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Âú®ÂáÜÁ°ÆÊ∑±Â∫¶ÂèØÁî®Êó∂ÊèêÈ´ò‰∫Ü2DË∑üË∏™Á≤æÂ∫¶„ÄÇ","title":"TAPIP3DÔºöÊèêÂçá3DÁÇπË∑üË∏™ÁöÑÂàõÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïTAPIP3DÔºåÁî®‰∫éÂú®ÂçïÁõÆRGBÂíåRGB-DËßÜÈ¢ë‰∏≠ËøõË°åÈïøÊúü3DÁÇπË∑üË∏™„ÄÇTAPIP3DÈÄöËøáÊ∑±Â∫¶ÂíåÁõ∏Êú∫ËøêÂä®‰ø°ÊÅØÔºåÂ∞ÜËßÜÈ¢ëË°®Á§∫‰∏∫Á®≥ÂÆöÁöÑÊó∂Á©∫ÁâπÂæÅ‰∫ëÔºå‰ªéËÄåÂ∞Ü2DËßÜÈ¢ëÁâπÂæÅÊèêÂçáÂà∞3D‰∏ñÁïåÁ©∫Èó¥„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ±ÄÈÉ®ÂØπÊ≥®ÊÑèÂäõÊú∫Âà∂Â§ÑÁêÜ3DÁÇπÂàÜÂ∏ÉÁöÑ‰∏çËßÑÂàôÊÄßÔºåÊúâÊïàÂà©Áî®3DÁ©∫Èó¥ÂÖ≥Á≥ªÔºåÂΩ¢Êàê‰ø°ÊÅØ‰∏∞ÂØåÁöÑÁâπÂæÅÈÇªÂüüÔºå‰ª•ÂÆûÁé∞Á≤æÁ°ÆÁöÑ3DËΩ®Ëøπ‰º∞ËÆ°„ÄÇÊàë‰ª¨ÁöÑ3D‰∏≠ÂøÉÊñπÊ≥ïÂú®Â§ö‰∏™3DÁÇπË∑üË∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂Âú®ÂáÜÁ°ÆÊ∑±Â∫¶ÂèØÁî®Êó∂ÊèêÈ´ò‰∫Ü2DË∑üË∏™Á≤æÂ∫¶„ÄÇ', title='TAPIP3DÔºöÊèêÂçá3DÁÇπË∑üË∏™ÁöÑÂàõÊñ∞ÊñπÊ≥ï'))
[22.04.2025 05:12] Loading Chinese text from previous data.
[22.04.2025 05:12] Renaming data file.
[22.04.2025 05:12] Renaming previous data. hf_papers.json to ./d/2025-04-22.json
[22.04.2025 05:12] Saving new data file.
[22.04.2025 05:12] Generating page.
[22.04.2025 05:12] Renaming previous page.
[22.04.2025 05:12] Renaming previous data. index.html to ./d/2025-04-22.html
[22.04.2025 05:12] [Experimental] Generating Chinese page for reading.
[22.04.2025 05:12] Chinese vocab [{'word': 'ÊûÑÂª∫', 'pinyin': 'g√≤uji√†n', 'trans': 'construct'}, {'word': 'ÊúâÊïà', 'pinyin': 'y«íuxi√†o', 'trans': 'effective'}, {'word': 'Êåá‰ª§', 'pinyin': 'zh«êl√¨ng', 'trans': 'instruction'}, {'word': 'Ë∞ÉÊï¥', 'pinyin': 'ti√°ozhƒõng', 'trans': 'adjust'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√πj√πj√≠', 'trans': 'dataset'}, {'word': 'ÂÖ≥ÈîÆÂõ†Á¥†', 'pinyin': 'gu«énji√†n yƒ´ns√π', 'trans': 'key factors'}, {'word': 'Ë¥®Èáè', 'pinyin': 'zh√¨li√†ng', 'trans': 'quality'}, {'word': 'Â§öÊ†∑ÊÄß', 'pinyin': 'du≈çy√†ngx√¨ng', 'trans': 'diversity'}, {'word': 'ÂºÄÊ∫ê', 'pinyin': 'kƒÅiyu√°n', 'trans': 'open-source'}, {'word': 'Ëá™Âä®', 'pinyin': 'z√¨d√≤ng', 'trans': 'automatic'}, {'word': 'ÈÄâÊã©', 'pinyin': 'xu«énz√©', 'trans': 'select'}, {'word': 'Â≠êÈõÜ', 'pinyin': 'z«êj√≠', 'trans': 'subset'}, {'word': 'ÂèòÂæó', 'pinyin': 'bi√†nd√©', 'trans': 'become'}, {'word': 'ÈáçË¶Å', 'pinyin': 'zh√≤ngy√†o', 'trans': 'important'}, {'word': 'Áé∞Êúâ', 'pinyin': 'xi√†ny«íu', 'trans': 'existing'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': '‰∏ªË¶Å', 'pinyin': 'zh«îy√†o', 'trans': 'main'}, {'word': 'ÂÖ≥Ê≥®', 'pinyin': 'guƒÅnzh√π', 'trans': 'focus on'}, {'word': 'ÂÆû‰æã', 'pinyin': 'sh√≠l√¨', 'trans': 'instance'}, {'word': 'ÂêØÂèëÂºè', 'pinyin': 'q«êfƒÅsh√¨', 'trans': 'heuristic'}, {'word': 'ËßÑÂàô', 'pinyin': 'guƒ´z√©', 'trans': 'rule'}, {'word': 'Áª¥ÊåÅ', 'pinyin': 'w√©ich√≠', 'trans': 'maintain'}, {'word': 'ÊïàÊûú', 'pinyin': 'xi√†ogu«í', 'trans': 'effect'}, {'word': '‰∏ç‰Ω≥', 'pinyin': 'b√πjiƒÅ', 'trans': 'poor'}, {'word': '‰ΩúËÄÖ', 'pinyin': 'zu√≤zhƒõ', 'trans': 'author'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ch≈´', 'trans': 'propose'}, {'word': 'Êñ∞ÊñπÊ≥ï', 'pinyin': 'xƒ´n fƒÅngf«é', 'trans': 'new method'}, {'word': 'ÈÄöËøá', 'pinyin': 't≈çnggu√≤', 'trans': 'through'}, {'word': 'Ê†áÁ≠æ', 'pinyin': 'biƒÅoqiƒÅn', 'trans': 'label'}, {'word': 'Âõæ', 'pinyin': 't√∫', 'trans': 'graph'}, {'word': 'Ê®°Êãü', 'pinyin': 'm√≥n«ê', 'trans': 'simulate'}, {'word': 'ËØ≠‰πâ', 'pinyin': 'y«îy√¨', 'trans': 'semantic'}, {'word': 'Á©∫Èó¥', 'pinyin': 'k≈çngjiƒÅn', 'trans': 'space'}, {'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´y√∫', 'trans': 'based on'}, {'word': '‰ø°ÊÅØ', 'pinyin': 'x√¨nxƒ´', 'trans': 'information'}, {'word': 'ÂàÜÂ∏É', 'pinyin': 'fƒìnb√π', 'trans': 'distribution'}, {'word': 'ÈáèÂåñ', 'pinyin': 'li√†nghu√†', 'trans': 'quantify'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«énsh√¨', 'trans': 'show'}, {'word': '‰ºò‰∫é', 'pinyin': 'y≈çuy√∫', 'trans': 'superior to'}, {'word': 'Âü∫Á°Ä', 'pinyin': 'jƒ´ch«î', 'trans': 'foundation'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}]
[22.04.2025 05:12] Renaming previous Chinese page.
[22.04.2025 05:12] Renaming previous data. zh.html to ./d/2025-04-21_zh_reading_task.html
[22.04.2025 05:12] Writing Chinese reading task.
[22.04.2025 05:12] Writing result.
[22.04.2025 05:12] Renaming log file.
[22.04.2025 05:12] Renaming previous data. log.txt to ./logs/2025-04-22_last_log.txt
