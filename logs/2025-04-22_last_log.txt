[22.04.2025 07:12] Read previous papers.
[22.04.2025 07:12] Generating top page (month).
[22.04.2025 07:12] Writing top page (month).
[22.04.2025 08:15] Read previous papers.
[22.04.2025 08:15] Get feed.
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.14945
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15257
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.14396
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.13958
[22.04.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2504.15271
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.14603
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.13203
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15281
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15280
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.13805
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.14655
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.14239
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15217
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.08902
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15133
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.15047
[22.04.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2504.15270
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.14717
[22.04.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.13941
[22.04.2025 08:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.04.2025 08:15] No deleted papers detected.
[22.04.2025 08:15] Downloading and parsing papers (pdf, html). Total: 19.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.14945.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.14945.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.14945.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.15257.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.15257.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.15257.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.14396.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.14396.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.14396.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.13958.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.13958.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.13958.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.15271.
[22.04.2025 08:15] Downloading paper 2504.15271 from http://arxiv.org/pdf/2504.15271v1...
[22.04.2025 08:15] Extracting affiliations from text.
[22.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 1 7 2 5 1 . 4 0 5 2 : r 2025-4Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models Guo Chen1*, Zhiqi Li1*, Shihao Wang2*, Jindong Jiang3*, Yicheng Liu1, Lidong Lu1, De-An Huang, Wonmin Byeon, Matthieu Le, Tuomas Rintamaki, Tyler Poon, Max Ehrlich, Tong Lu1, Limin Wang1, Bryan Catanzaro, Jan Kautz, Andrew Tao, Zhiding Yu, Guilin Liu "
[22.04.2025 08:15] Response: ```python
[]
```
[22.04.2025 08:15] Extracting affiliations from text.
[22.04.2025 08:15] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 1 7 2 5 1 . 4 0 5 2 : r 2025-4Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models Guo Chen1*, Zhiqi Li1*, Shihao Wang2*, Jindong Jiang3*, Yicheng Liu1, Lidong Lu1, De-An Huang, Wonmin Byeon, Matthieu Le, Tuomas Rintamaki, Tyler Poon, Max Ehrlich, Tong Lu1, Limin Wang1, Bryan Catanzaro, Jan Kautz, Andrew Tao, Zhiding Yu, Guilin LiuWe introduce Eagle 2.5, family of frontier vision-language models (VLMs) for long-context multimodal learning. Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing generalist framework for both tasks. The proposed training framework incorporates Automatic Degrade Sampling and Image Area Preservation, two techniques that preserve contextual integrity and visual details. The framework also includes numerous efficiency optimizations in the pipeline for long-context data training. Finally, we propose Eagle-Video-110K, novel dataset that integrates both story-level and clip-level annotations, facilitating long-video understanding. Eagle 2.5 demonstrates substantial improvements on long-context multimodal benchmarks, providing robust solution to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B achieves 72.4% on Video-MME with 512 input frames, matching the results of top-tier commercial model such as GPT-4o and large-scale open-source models like Qwen2.5-VL-72B and InternVL2.5-78B. Links: Project Page 1. Introduction Despite the significant advances in multimodal learning Bai et al. (2023); Chen et al. (2023); Li et al. (2023,); Wang et al. (2024), many visionlanguage models (VLMs) remain focused on shortcontext tasks, with long-context understanding under-explored. This gap is particularly evident in both long video comprehension and highresolution image/video understanding, where the processing of extended visual contexts remains an open challenge. Such extended contexts encompass multiple images, extended video sequences, high-resolution media, or combinations thereof. However, the development of long-context VLMs is still in its early stages, hindered by fundamental challenges in dataset construction, architecture design, training strategies, and computation/memory bottlenecks. Figure 1: Performance comparison of Eagle 2.5 with leading vision-language models on the Video-MME benchmark. Eagle 2.5 demonstrates consistent improvement as the number of input frames increases. To enable long-context visual understanding, several approaches have been proposed to address the challenge of processing extended visual inputs by designing specialized compression or selection modules Jin et al. (2024); Korbar et al. (2024); Li et al. (2023,, 2024); Shen et al. (2024); Weng et al. (2024); Yu et al. (2024). While these methods effectively circumvent the need * Work Done during an internship at NVIDIA. Equal advising and corresponding authors: guilinl@nvidia.com, zhidingy@nvidia.com. Additional affiliations: 1 Nanjing University, 2 The Hong Kong Polytechnic University, 3 Rutgers University. 2025 NVIDIA. All rights reserved. Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models to extend the context length of VLMs, they often introduce additional computational overhead or capacity limitations, potentially constraining model performance. promising research direction is to extend the context length of LLMs to enable native long-context understanding. While prior studies Shen et al. (2025); Xue et al. (2024); Zhang et al. (2024) have explored this direction, challenges and key limitations still remain. First, the performance of existing methods is often suboptimal, generally falling behind proprietary models. Second, these approaches struggle to achieve consistent improvements as the amount of visual input increases. Lastly, the optimal training strategies for state-of-the-art long-context VLMs remain unclear, given the complex interplay of factors such as training strategies and data recipes. To this end, we present Eagle 2.5, versatile multimodal model designed to efficiently process extensive contextual information. Unlike models solely optimized for handling long multimodal sequences without improving performance, Eagle-2.5 benefits from increased input length, leading to consistent performance gains besides merely accommodating longer inputs. As shown in Fig. 1, our model achieves superior context coverage and exhibits consistent performance scaling with increasing frame counts. Notably, it attains competitive results compared to larger models such as GPT-4o OpenAI (2023) and Qwen2.5-VL-72B Bai et al. (2025), while maintaining significantly smaller parameter footprint. Eagle 2.5 is driven by both the advanced training strategy and data recipe. For training strategy, we introduce two core components for effective long-context learning: information-first sampling and progressive training. Information-first sampling. The information-first sampling strategy ensures the preservation of essential visual and semantic information through two mechanisms: (1) Image Area Preservation, which optimizes tiling to retain the majority of the original image area while maintaining aspect ratio fidelity, avoiding rigid aspect ratio constraints; and (2) Automatic Degradation Sampling (ADS), which dynamically balances visual and textual inputs by prioritizing complete text retention while adaptively optimizing visual content to maximize context length utilization and preserve multimodal information. Progressive training. We employ progressive mixed post-training approach, wherein context length is incrementally expanded during training, enhancing the models ability to process inputs of varying sizes. This integrated strategy significantly improves information density over static sampling methods while ensuring consistent performance across diverse input types and lengths. For data recipe, we embrace the diversity first, then quality principle in curating the training data pool. Our data recipe combines open-source data (including human-annotated data as well as synthetic video data) with our self-curated Eagle-Video-110K dataset, specifically designed to enhance long video understanding capabilities. We adopt diversity-driven collection strategy, using multiple video sources and similarity thresholding method to identify novel clips that maximize content diversity. Our dataset is distinguished by its dual annotation approach: top"
[22.04.2025 08:15] Mistral response. {"id": "d970b5ea74c046889752312b77e84789", "object": "chat.completion", "created": 1745309730, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['Nanjing University', 'The Hong Kong Polytechnic University', 'Rutgers University', 'NVIDIA']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1601, "total_tokens": 1634, "completion_tokens": 33}}
[22.04.2025 08:15] Response: ```python
['Nanjing University', 'The Hong Kong Polytechnic University', 'Rutgers University', 'NVIDIA']
```
[22.04.2025 08:15] Deleting PDF ./assets/pdf/2504.15271.pdf.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.14603.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.14603.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.14603.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.13203.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.13203.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.13203.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.15281.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.15281.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.15281.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.15280.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.15280.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.15280.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.13805.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.13805.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.13805.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.14655.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.14655.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.14655.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.14239.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.14239.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.14239.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.15217.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.15217.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.15217.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.08902.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.08902.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.08902.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.15133.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.15133.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.15133.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.15047.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.15047.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.15047.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.15270.
[22.04.2025 08:15] Downloading paper 2504.15270 from http://arxiv.org/pdf/2504.15270v1...
[22.04.2025 08:15] Extracting affiliations from text.
[22.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 0 7 2 5 1 . 4 0 5 2 : r Quicksviewer: An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes Ji Qi, Yuan Yao, Yushi Bai, Bin Xu, Juanzi Li, Zhiyuan Liu, Tat-Seng Chua Tsinghua University National Univesity of Singapore qijithu1@gmail.com, yaoyuanthu@gmail.com https://quicksviewer.github.io Figure 1: Quicksviewer involves cubing network that partitions video into nonuniform cubes, followed by 3D resampler to gather fixed number of visual tokens per cube. This efficiency enables Large Receptive Field (420 frames) with High Compression Rate (64) during all training stages, and scaling laws on extended frames in inference. "
[22.04.2025 08:15] Response: ```python
["Tsinghua University", "National University of Singapore"]
```
[22.04.2025 08:15] Deleting PDF ./assets/pdf/2504.15270.pdf.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.14717.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.14717.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.14717.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2504.13941.
[22.04.2025 08:15] Extra JSON file exists (./assets/json/2504.13941.json), skip PDF parsing.
[22.04.2025 08:15] Paper image links file exists (./assets/img_data/2504.13941.json), skip HTML parsing.
[22.04.2025 08:15] Success.
[22.04.2025 08:15] Enriching papers with extra data.
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 0. Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning (RL) with simple rule-based rewards. However, existing zero-RL approaches are inherently ``on-policy'', limiting learning t...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 1. This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first en...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 2. The increasing demand for AR/VR applications has highlighted the need for high-quality 360-degree panoramic content. However, generating high-quality 360-degree panoramic images and videos remains a challenging task due to the severe distortions introduced by equirectangular projection (ERP). Existi...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 3. Current Large Language Models (LLMs) often undergo supervised fine-tuning (SFT) to acquire tool use capabilities. However, SFT struggles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learning (RL), particularly with R1-like models, have demonstrated ...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 4. We introduce Eagle 2.5, a family of frontier vision-language models (VLMs) for long-context multimodal learning. Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing a generalist framework for both tasks. The proposed training framework i...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 5. Recent Computer-Using Agents (CUAs), powered by multimodal large language models (LLMs), offer a promising direction for automating complex desktop workflows through natural language. However, most existing CUAs remain conceptual prototypes, hindered by shallow OS integration, fragile screenshot-bas...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 6. Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-te...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 7. 3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented textures, semantic misalignment, and limited adaptability to abstract aesthetics. We propose StyleMe3D, a holistic framework for 3D GS style tran...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 8. Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown imp...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 9. Mobile GUI agents show promise in automating tasks but face generalization challenges in diverse real-world scenarios. Traditional approaches using pre-training or fine-tuning with massive datasets struggle with the diversity of mobile applications and user-specific tasks. We propose enhancing mobil...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 10. We introduce LeetCodeDataset, a high-quality benchmark for evaluating and training code-generation models, addressing two key challenges in LLM research: the lack of reasoning-focused coding benchmarks and self-contained training testbeds. By curating LeetCode Python problems with rich metadata, bro...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 11. Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoni...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 12. We present Distributional RewArds for Generative OptimizatioN (DRAGON), a versatile framework for fine-tuning media generation models towards a desired outcome. Compared with traditional reinforcement learning with human feedback (RLHF) or pairwise preference approaches such as direct preference opt...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 13. Anamorphosis refers to a category of images that are intentionally distorted, making them unrecognizable when viewed directly. Their true form only reveals itself when seen from a specific viewpoint, which can be through some catadioptric device like a mirror or a lens. While the construction of the...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 14. In this paper, we introduce EasyEdit2, a framework designed to enable plug-and-play adjustability for controlling Large Language Model (LLM) behaviors. EasyEdit2 supports a wide range of test-time interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language f...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 15. Large Language Models (LLMs) exhibit remarkable capabilities but are susceptible to adversarial prompts that exploit vulnerabilities to produce unsafe or biased outputs. Existing red-teaming methods often face scalability challenges, resource-intensive requirements, or limited diversity in attack st...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 16. Large Multimodal Models (LMMs) uniformly perceive video frames, creating computational inefficiency for videos with inherently varying temporal information density. This paper present Quicksviewer, an LMM with new perceiving paradigm that partitions a video of nonuniform density into varying cubes u...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 17. We introduce TAPIP3D, a novel approach for long-term 3D point tracking in monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized spatio-temporal feature clouds, leveraging depth and camera motion information to lift 2D video features into a 3D world space where camera motion ...
[22.04.2025 08:15] ********************************************************************************
[22.04.2025 08:15] Abstract 18. Large Language Models (LLMs) have shown strong reasoning capabilities, particularly when enhanced through Reinforcement Learning (RL). While prior work has successfully applied RL to mathematical reasoning -- where rules and correctness are well-defined -- generalizing these methods to broader reaso...
[22.04.2025 08:15] Read previous papers.
[22.04.2025 08:15] Generating reviews via LLM API.
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#training", "#reasoning", "#math", "#optimization", "#rl"], "emoji": "üß†", "ru": {"title": "LUFFY: –æ–±—É—á–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —Å –±–∞–ª–∞–Ω—Å–æ–º –º–µ–∂–¥—É –∏–º–∏—Ç–∞—Ü–∏–µ–π –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º", "desc": "LUFFY - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#reasoning", "#rl", "#agents"], "emoji": "ü§ñ", "ru": {"title": "FlowReasoner: –£–º–Ω—ã–π –¥–∏–∑–∞–π–Ω–µ—Ä –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω FlowReasoner - –º–µ—Ç–∞-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ —É—Ä–æ–≤–Ω–µ –∑–∞–ø—Ä–æ—Å–æ–≤. –û
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#3d", "#diffusion", "#multimodal", "#open_source", "#video"], "emoji": "üåê", "ru": {"title": "SphereDiff: –ë–µ—Å—à–æ–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∞–Ω–æ—Ä–∞–º 360¬∞ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "SphereDiff - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ —Å –æ–±–∑–æ—Ä–æ–º 360 –≥—Ä–∞–¥—É—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#training", "#open_source", "#rlhf", "#survey", "#reasoning", "#optimization", "#benchmark", "#rl"], "emoji": "üõ†Ô∏è", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤ LLM –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤–æ–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–∞
[22.04.2025 08:15] Querying the API.
[22.04.2025 08:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce Eagle 2.5, a family of frontier vision-language models (VLMs) for long-context multimodal learning. Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing a generalist framework for both tasks. The proposed training framework incorporates Automatic Degrade Sampling and Image Area Preservation, two techniques that preserve contextual integrity and visual details. The framework also includes numerous efficiency optimizations in the pipeline for long-context data training. Finally, we propose Eagle-Video-110K, a novel dataset that integrates both story-level and clip-level annotations, facilitating long-video understanding. Eagle 2.5 demonstrates substantial improvements on long-context multimodal benchmarks, providing a robust solution to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B achieves 72.4% on Video-MME with 512 input frames, matching the results of top-tier commercial model such as GPT-4o and large-scale open-source models like Qwen2.5-VL-72B and InternVL2.5-78B.
[22.04.2025 08:15] Response: {
  "desc": "Eagle 2.5 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ú–æ–¥–µ–ª—å —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–π —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π. –í —Ä–∞–±–æ—Ç–µ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç Eagle-Video-110K –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—é –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. Eagle 2.5 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, —Å–æ–ø–µ—Ä–Ω–∏—á–∞—è —Å –≤–µ–¥—É—â–∏–º–∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º–∏ –∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–º–∏ –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.",
  "emoji": "ü¶Ö",
  "title": "Eagle 2.5: –ü—Ä–æ—Ä—ã–≤ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"
}
[22.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce Eagle 2.5, a family of frontier vision-language models (VLMs) for long-context multimodal learning. Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing a generalist framework for both tasks. The proposed training framework incorporates Automatic Degrade Sampling and Image Area Preservation, two techniques that preserve contextual integrity and visual details. The framework also includes numerous efficiency optimizations in the pipeline for long-context data training. Finally, we propose Eagle-Video-110K, a novel dataset that integrates both story-level and clip-level annotations, facilitating long-video understanding. Eagle 2.5 demonstrates substantial improvements on long-context multimodal benchmarks, providing a robust solution to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B achieves 72.4% on Video-MME with 512 input frames, matching the results of top-tier commercial model such as GPT-4o and large-scale open-source models like Qwen2.5-VL-72B and InternVL2.5-78B."

[22.04.2025 08:15] Response: ```python
['DATASET', 'MULTIMODAL', 'TRAINING', 'BENCHMARK']
```
[22.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce Eagle 2.5, a family of frontier vision-language models (VLMs) for long-context multimodal learning. Our work addresses the challenges in long video comprehension and high-resolution image understanding, introducing a generalist framework for both tasks. The proposed training framework incorporates Automatic Degrade Sampling and Image Area Preservation, two techniques that preserve contextual integrity and visual details. The framework also includes numerous efficiency optimizations in the pipeline for long-context data training. Finally, we propose Eagle-Video-110K, a novel dataset that integrates both story-level and clip-level annotations, facilitating long-video understanding. Eagle 2.5 demonstrates substantial improvements on long-context multimodal benchmarks, providing a robust solution to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B achieves 72.4% on Video-MME with 512 input frames, matching the results of top-tier commercial model such as GPT-4o and large-scale open-source models like Qwen2.5-VL-72B and InternVL2.5-78B."

[22.04.2025 08:15] Response: ```python
["LONG_CONTEXT", "OPTIMIZATION"]
```
[22.04.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Eagle 2.5 is a new family of vision-language models designed to improve understanding of long videos and high-resolution images. It introduces a training framework that uses techniques like Automatic Degrade Sampling and Image Area Preservation to maintain important visual details and context. The model is optimized for efficiency when processing long-context data, making it faster and more effective. Additionally, the Eagle-Video-110K dataset provides comprehensive annotations for better training and evaluation, leading to significant performance gains over existing models.","title":"Eagle 2.5: Advancing Long-Context Vision-Language Understanding"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Eagle 2.5 is a new family of vision-language models designed to improve understanding of long videos and high-resolution images. It introduces a training framework that uses techniques like Automatic Degrade Sampling and Image Area Preservation to maintain important visual details and context. The model is optimized for efficiency when processing long-context data, making it faster and more effective. Additionally, the Eagle-Video-110K dataset provides comprehensive annotations for better training and evaluation, leading to significant performance gains over existing models.', title='Eagle 2.5: Advancing Long-Context Vision-Language Understanding'))
[22.04.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êàë‰ª¨‰ªãÁªç‰∫ÜEagle 2.5ÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éÈïø‰∏ä‰∏ãÊñáÂ§öÊ®°ÊÄÅÂ≠¶‰π†ÁöÑÂâçÊ≤øËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁ≥ªÂàó„ÄÇËØ•Á†îÁ©∂Ëß£ÂÜ≥‰∫ÜÈïøËßÜÈ¢ëÁêÜËß£ÂíåÈ´òÂàÜËæ®ÁéáÂõæÂÉèÁêÜËß£‰∏≠ÁöÑÊåëÊàòÔºåÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÈÄöÁî®Ê°ÜÊû∂Êù•Â§ÑÁêÜËøô‰∏§È°π‰ªªÂä°„ÄÇÊâÄÊèêÂá∫ÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÁªìÂêà‰∫ÜËá™Âä®ÈôçÁ∫ßÈááÊ†∑ÂíåÂõæÂÉèÂå∫Âüü‰øùÁïô‰∏§ÁßçÊäÄÊúØÔºå‰ª•‰øùÊåÅ‰∏ä‰∏ãÊñáÂÆåÊï¥ÊÄßÂíåËßÜËßâÁªÜËäÇ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜEagle-Video-110KÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊï∞ÊçÆÈõÜÔºåÈõÜÊàê‰∫ÜÊïÖ‰∫ãÁ∫ßÂíåÁâáÊÆµÁ∫ßÁöÑÊ≥®ÈáäÔºå‰øÉËøõ‰∫ÜÈïøËßÜÈ¢ëÁêÜËß£„ÄÇ","title":"Eagle 2.5ÔºöÈïø‰∏ä‰∏ãÊñáÂ§öÊ®°ÊÄÅÂ≠¶‰π†ÁöÑÊñ∞Á™ÅÁ†¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êàë‰ª¨‰ªãÁªç‰∫ÜEagle 2.5ÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éÈïø‰∏ä‰∏ãÊñáÂ§öÊ®°ÊÄÅÂ≠¶‰π†ÁöÑÂâçÊ≤øËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁ≥ªÂàó„ÄÇËØ•Á†îÁ©∂Ëß£ÂÜ≥‰∫ÜÈïøËßÜÈ¢ëÁêÜËß£ÂíåÈ´òÂàÜËæ®ÁéáÂõæÂÉèÁêÜËß£‰∏≠ÁöÑÊåëÊàòÔºåÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÈÄöÁî®Ê°ÜÊû∂Êù•Â§ÑÁêÜËøô‰∏§È°π‰ªªÂä°„ÄÇÊâÄÊèêÂá∫ÁöÑËÆ≠ÁªÉÊ°ÜÊû∂ÁªìÂêà‰∫ÜËá™Âä®ÈôçÁ∫ßÈááÊ†∑ÂíåÂõæÂÉèÂå∫Âüü‰øùÁïô‰∏§ÁßçÊäÄÊúØÔºå‰ª•‰øùÊåÅ‰∏ä‰∏ãÊñáÂÆåÊï¥ÊÄßÂíåËßÜËßâÁªÜËäÇ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜEagle-Video-110KÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊï∞ÊçÆÈõÜÔºåÈõÜÊàê‰∫ÜÊïÖ‰∫ãÁ∫ßÂíåÁâáÊÆµÁ∫ßÁöÑÊ≥®ÈáäÔºå‰øÉËøõ‰∫ÜÈïøËßÜÈ¢ëÁêÜËß£„ÄÇ', title='Eagle 2.5ÔºöÈïø‰∏ä‰∏ãÊñáÂ§öÊ®°ÊÄÅÂ≠¶‰π†ÁöÑÊñ∞Á™ÅÁ†¥'))
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#agents", "#architecture", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "UFO2: –ù–∞–¥–µ–∂–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è Windows —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –∏ –≥–ª—É–±–æ–∫–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –û–°", "desc": "UFO2 - —ç—Ç–æ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—á–µ–≥–æ —Å—Ç–æ–ª–∞ Windows, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#alignment", "#training", "#open_source", "#security", "#agents", "#dataset"], "emoji": "üõ°Ô∏è", "ru": {"title": "–£–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–º –¥–∏–∞–ª–æ–≥–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç X-Teaming - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –∞—Ç–∞–∫ –Ω–∞ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏. 
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#synthetic", "#3d", "#games", "#multimodal"], "emoji": "üé®", "ru": {"title": "StyleMe3D: –ú–æ—Å—Ç –º–µ–∂–¥—É —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º 3D –∏ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Å—Ç–∏–ª–∏–∑–∞—Ü–∏–µ–π", "desc": "StyleMe3D - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ 3D-—Å—Ü–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—é 3D Gaussian Splatting. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#survey", "#reasoning", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –≤—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ All-Angles Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#transfer_learning", "#agents"], "emoji": "üì±", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –º–æ–±–∏–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–º
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#training", "#open_source", "#reasoning", "#data", "#optimization", "#benchmark", "#dataset"], "emoji": "üß†", "ru": {"title": "LeetCodeDataset: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ò–ò –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "LeetCodeDataset –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#games", "#reasoning", "#multimodal", "#rl", "#agents", "#training"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç —Ä–µ–∞–∫—Ç–∏–≤–Ω—ã—Ö –∞–∫—Ç—ë—Ä–æ–≤ –∫ –æ–±–¥—É–º—ã–≤–∞—é—â–∏–º —Ä–∞—Å—Å—É–∂–¥–∞—Ç–µ–ª—è–º –≤ GUI-–∞–≥–µ–Ω—Ç–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ (GUI) 
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#audio", "#rlhf", "#diffusion", "#optimization", "#training"], "emoji": "üêâ", "ru": {"title": "DRAGON: –≥–∏–±–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞", "desc": "DRAGON - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –æ–±–ª–∞—Å—Ç–∏ –º–µ–¥–∏–∞. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#cv", "#multimodal"], "emoji": "üîç", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∞–Ω–∞–º–æ—Ä—Ñ–æ–∑—ã: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –æ–ø—Ç–∏—á–µ—Å–∫–∏–µ –∏–ª–ª—é–∑–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∞–º–æ—Ä—Ñ–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –º–æ–¥–µ–ª–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –≤—ã–ø—Ä—è–º–ª–µ–Ω–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞ 
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#training", "#alignment", "#architecture", "#open_source"], "emoji": "üéõÔ∏è", "ru": {"title": "–¢–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫", "desc": "EasyEdit2 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–∏–±–∫–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#security", "#benchmark", "#data", "#open_source", "#dataset"], "emoji": "üåà", "ru": {"title": "RainbowPlus: –≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–≤—ã—à–µ–Ω–∏—é –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RainbowPlus - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[22.04.2025 08:15] Querying the API.
[22.04.2025 08:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large Multimodal Models (LMMs) uniformly perceive video frames, creating computational inefficiency for videos with inherently varying temporal information density. This paper present Quicksviewer, an LMM with new perceiving paradigm that partitions a video of nonuniform density into varying cubes using Gumbel Softmax, followed by a unified resampling for each cube to achieve efficient video understanding. This simple and intuitive approach dynamically compress video online based on its temporal density, significantly reducing spatiotemporal redundancy (overall 45times compression rate), while enabling efficient training with large receptive field. We train the model from a language backbone through three progressive stages, each incorporating lengthy videos on average of 420s/1fps thanks to the perceiving efficiency. With only 0.8M total video-text samples for training, our model outperforms the direct baseline employing a fixed partitioning strategy by a maximum of 8.72 in accuracy, demonstrating the effectiveness in performance. On Video-MME, Quicksviewer achieves SOTA under modest sequence lengths using just up to 5\% of tokens per frame required by baselines. With this paradigm, scaling up the number of input frames reveals a clear power law of the model capabilities. It is also empirically verified that the segments generated by the cubing network can help for analyzing continuous events in videos.
[22.04.2025 08:15] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Quicksviewer - –Ω–æ–≤—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º Gumbel Softmax –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –≤–∏–¥–µ–æ –Ω–∞ –∫—É–±—ã –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö (–≤ 45 —Ä–∞–∑) –∏ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. Quicksviewer –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, –¥–æ—Å—Ç–∏–≥–∞—è state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ Video-MME.",
  "emoji": "üé¨",
  "title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –≤–∏–¥–µ–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"
}
[22.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large Multimodal Models (LMMs) uniformly perceive video frames, creating computational inefficiency for videos with inherently varying temporal information density. This paper present Quicksviewer, an LMM with new perceiving paradigm that partitions a video of nonuniform density into varying cubes using Gumbel Softmax, followed by a unified resampling for each cube to achieve efficient video understanding. This simple and intuitive approach dynamically compress video online based on its temporal density, significantly reducing spatiotemporal redundancy (overall 45times compression rate), while enabling efficient training with large receptive field. We train the model from a language backbone through three progressive stages, each incorporating lengthy videos on average of 420s/1fps thanks to the perceiving efficiency. With only 0.8M total video-text samples for training, our model outperforms the direct baseline employing a fixed partitioning strategy by a maximum of 8.72 in accuracy, demonstrating the effectiveness in performance. On Video-MME, Quicksviewer achieves SOTA under modest sequence lengths using just up to 5\% of tokens per frame required by baselines. With this paradigm, scaling up the number of input frames reveals a clear power law of the model capabilities. It is also empirically verified that the segments generated by the cubing network can help for analyzing continuous events in videos."

[22.04.2025 08:15] Response: ```python
['VIDEO', 'MULTIMODAL', 'TRAINING']
```
[22.04.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large Multimodal Models (LMMs) uniformly perceive video frames, creating computational inefficiency for videos with inherently varying temporal information density. This paper present Quicksviewer, an LMM with new perceiving paradigm that partitions a video of nonuniform density into varying cubes using Gumbel Softmax, followed by a unified resampling for each cube to achieve efficient video understanding. This simple and intuitive approach dynamically compress video online based on its temporal density, significantly reducing spatiotemporal redundancy (overall 45times compression rate), while enabling efficient training with large receptive field. We train the model from a language backbone through three progressive stages, each incorporating lengthy videos on average of 420s/1fps thanks to the perceiving efficiency. With only 0.8M total video-text samples for training, our model outperforms the direct baseline employing a fixed partitioning strategy by a maximum of 8.72 in accuracy, demonstrating the effectiveness in performance. On Video-MME, Quicksviewer achieves SOTA under modest sequence lengths using just up to 5\% of tokens per frame required by baselines. With this paradigm, scaling up the number of input frames reveals a clear power law of the model capabilities. It is also empirically verified that the segments generated by the cubing network can help for analyzing continuous events in videos."

[22.04.2025 08:15] Response: ```python
["OPTIMIZATION"]
```
[22.04.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Quicksviewer, a Large Multimodal Model (LMM) designed to efficiently process videos with varying temporal information density. By using a novel approach that partitions videos into dynamic cubes with Gumbel Softmax, it reduces computational redundancy and achieves a remarkable 45 times compression rate. The model is trained progressively on lengthy videos, allowing it to outperform traditional fixed partitioning methods by improving accuracy significantly. Quicksviewer demonstrates state-of-the-art performance on Video-MME while requiring only a fraction of the tokens typically needed, showcasing its efficiency and effectiveness in video understanding.","title":"Efficient Video Understanding with Dynamic Cubes"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Quicksviewer, a Large Multimodal Model (LMM) designed to efficiently process videos with varying temporal information density. By using a novel approach that partitions videos into dynamic cubes with Gumbel Softmax, it reduces computational redundancy and achieves a remarkable 45 times compression rate. The model is trained progressively on lengthy videos, allowing it to outperform traditional fixed partitioning methods by improving accuracy significantly. Quicksviewer demonstrates state-of-the-art performance on Video-MME while requiring only a fraction of the tokens typically needed, showcasing its efficiency and effectiveness in video understanding.', title='Efficient Video Understanding with Dynamic Cubes'))
[22.04.2025 08:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§ßËßÑÊ®°Â§öÊ®°ÊÄÅÊ®°ÂûãQuicksviewerÔºåËØ•Ê®°ÂûãÈÄöËøáGumbel SoftmaxÂ∞ÜËßÜÈ¢ëÂàÜÂâ≤Êàê‰∏çÂêåÂØÜÂ∫¶ÁöÑÁ´ãÊñπ‰ΩìÔºå‰ªéËÄåÊèêÈ´òËßÜÈ¢ëÁêÜËß£ÁöÑÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊ†πÊçÆËßÜÈ¢ëÁöÑÊó∂Èó¥ÂØÜÂ∫¶Âä®ÊÄÅÂéãÁº©ËßÜÈ¢ëÔºåÊòæËëóÂáèÂ∞ëÊó∂Á©∫ÂÜó‰ΩôÔºåÂéãÁº©ÁéáËææÂà∞45ÂÄç„ÄÇÊ®°ÂûãÈÄöËøáËØ≠Ë®ÄÂü∫Á°ÄËøõË°åËÆ≠ÁªÉÔºåËÉΩÂ§üÂ§ÑÁêÜÂπ≥ÂùáÊó∂Èïø‰∏∫420Áßí„ÄÅÂ∏ßÁéá‰∏∫1fpsÁöÑËßÜÈ¢ëÔºå‰∏î‰ªÖÈúÄ0.8MÁöÑËßÜÈ¢ë-ÊñáÊú¨Ê†∑Êú¨Âç≥ÂèØË∂ÖË∂äÂõ∫ÂÆöÂàÜÂå∫Á≠ñÁï•ÁöÑÂü∫Á∫ø„ÄÇQuicksviewerÂú®Video-MME‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩøÁî®ÁöÑtokenÊï∞Èáè‰ªÖ‰∏∫Âü∫Á∫øÁöÑ5%ÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®Â§ÑÁêÜËßÜÈ¢ëÊó∂ÁöÑÈ´òÊïàÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ","title":"QuicksviewerÔºöÈ´òÊïàËßÜÈ¢ëÁêÜËß£ÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§ßËßÑÊ®°Â§öÊ®°ÊÄÅÊ®°ÂûãQuicksviewerÔºåËØ•Ê®°ÂûãÈÄöËøáGumbel SoftmaxÂ∞ÜËßÜÈ¢ëÂàÜÂâ≤Êàê‰∏çÂêåÂØÜÂ∫¶ÁöÑÁ´ãÊñπ‰ΩìÔºå‰ªéËÄåÊèêÈ´òËßÜÈ¢ëÁêÜËß£ÁöÑÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÊ†πÊçÆËßÜÈ¢ëÁöÑÊó∂Èó¥ÂØÜÂ∫¶Âä®ÊÄÅÂéãÁº©ËßÜÈ¢ëÔºåÊòæËëóÂáèÂ∞ëÊó∂Á©∫ÂÜó‰ΩôÔºåÂéãÁº©ÁéáËææÂà∞45ÂÄç„ÄÇÊ®°ÂûãÈÄöËøáËØ≠Ë®ÄÂü∫Á°ÄËøõË°åËÆ≠ÁªÉÔºåËÉΩÂ§üÂ§ÑÁêÜÂπ≥ÂùáÊó∂Èïø‰∏∫420Áßí„ÄÅÂ∏ßÁéá‰∏∫1fpsÁöÑËßÜÈ¢ëÔºå‰∏î‰ªÖÈúÄ0.8MÁöÑËßÜÈ¢ë-ÊñáÊú¨Ê†∑Êú¨Âç≥ÂèØË∂ÖË∂äÂõ∫ÂÆöÂàÜÂå∫Á≠ñÁï•ÁöÑÂü∫Á∫ø„ÄÇQuicksviewerÂú®Video-MME‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩøÁî®ÁöÑtokenÊï∞Èáè‰ªÖ‰∏∫Âü∫Á∫øÁöÑ5%ÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®Â§ÑÁêÜËßÜÈ¢ëÊó∂ÁöÑÈ´òÊïàÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇ', title='QuicksviewerÔºöÈ´òÊïàËßÜÈ¢ëÁêÜËß£ÁöÑÊñ∞ÊñπÊ≥ï'))
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#3d", "#long_context"], "emoji": "üé•", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ 3D –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Ç–æ—á–µ–∫ –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±–ª–∞–∫–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", "desc": "TAPIP3D - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è 3D —Ç–æ—á–µ–∫ –≤ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö RGB –∏ RGB-D –≤–∏–¥–µ–æ. –û–Ω –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≤
[22.04.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#data", "#rl", "#transfer_learning", "#math"], "emoji": "üß†", "ru": {"title": "–ú–Ω–æ–≥–æ–¥–æ–º–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç NEMOTRON-CROSSTHINK - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[22.04.2025 08:15] Loading Chinese text from previous data.
[22.04.2025 08:15] Renaming data file.
[22.04.2025 08:15] Renaming previous data. hf_papers.json to ./d/2025-04-22.json
[22.04.2025 08:15] Saving new data file.
[22.04.2025 08:15] Generating page.
[22.04.2025 08:15] Renaming previous page.
[22.04.2025 08:15] Renaming previous data. index.html to ./d/2025-04-22.html
[22.04.2025 08:15] [Experimental] Generating Chinese page for reading.
[22.04.2025 08:15] Chinese vocab [{'word': 'ÊûÑÂª∫', 'pinyin': 'g√≤uji√†n', 'trans': 'construct'}, {'word': 'ÊúâÊïà', 'pinyin': 'y«íuxi√†o', 'trans': 'effective'}, {'word': 'Êåá‰ª§', 'pinyin': 'zh«êl√¨ng', 'trans': 'instruction'}, {'word': 'Ë∞ÉÊï¥', 'pinyin': 'ti√°ozhƒõng', 'trans': 'adjust'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√πj√πj√≠', 'trans': 'dataset'}, {'word': 'ÂÖ≥ÈîÆÂõ†Á¥†', 'pinyin': 'gu«énji√†n yƒ´ns√π', 'trans': 'key factors'}, {'word': 'Ë¥®Èáè', 'pinyin': 'zh√¨li√†ng', 'trans': 'quality'}, {'word': 'Â§öÊ†∑ÊÄß', 'pinyin': 'du≈çy√†ngx√¨ng', 'trans': 'diversity'}, {'word': 'ÂºÄÊ∫ê', 'pinyin': 'kƒÅiyu√°n', 'trans': 'open-source'}, {'word': 'Ëá™Âä®', 'pinyin': 'z√¨d√≤ng', 'trans': 'automatic'}, {'word': 'ÈÄâÊã©', 'pinyin': 'xu«énz√©', 'trans': 'select'}, {'word': 'Â≠êÈõÜ', 'pinyin': 'z«êj√≠', 'trans': 'subset'}, {'word': 'ÂèòÂæó', 'pinyin': 'bi√†nd√©', 'trans': 'become'}, {'word': 'ÈáçË¶Å', 'pinyin': 'zh√≤ngy√†o', 'trans': 'important'}, {'word': 'Áé∞Êúâ', 'pinyin': 'xi√†ny«íu', 'trans': 'existing'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': '‰∏ªË¶Å', 'pinyin': 'zh«îy√†o', 'trans': 'main'}, {'word': 'ÂÖ≥Ê≥®', 'pinyin': 'guƒÅnzh√π', 'trans': 'focus on'}, {'word': 'ÂÆû‰æã', 'pinyin': 'sh√≠l√¨', 'trans': 'instance'}, {'word': 'ÂêØÂèëÂºè', 'pinyin': 'q«êfƒÅsh√¨', 'trans': 'heuristic'}, {'word': 'ËßÑÂàô', 'pinyin': 'guƒ´z√©', 'trans': 'rule'}, {'word': 'Áª¥ÊåÅ', 'pinyin': 'w√©ich√≠', 'trans': 'maintain'}, {'word': 'ÊïàÊûú', 'pinyin': 'xi√†ogu«í', 'trans': 'effect'}, {'word': '‰∏ç‰Ω≥', 'pinyin': 'b√πjiƒÅ', 'trans': 'poor'}, {'word': '‰ΩúËÄÖ', 'pinyin': 'zu√≤zhƒõ', 'trans': 'author'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ch≈´', 'trans': 'propose'}, {'word': 'Êñ∞ÊñπÊ≥ï', 'pinyin': 'xƒ´n fƒÅngf«é', 'trans': 'new method'}, {'word': 'ÈÄöËøá', 'pinyin': 't≈çnggu√≤', 'trans': 'through'}, {'word': 'Ê†áÁ≠æ', 'pinyin': 'biƒÅoqiƒÅn', 'trans': 'label'}, {'word': 'Âõæ', 'pinyin': 't√∫', 'trans': 'graph'}, {'word': 'Ê®°Êãü', 'pinyin': 'm√≥n«ê', 'trans': 'simulate'}, {'word': 'ËØ≠‰πâ', 'pinyin': 'y«îy√¨', 'trans': 'semantic'}, {'word': 'Á©∫Èó¥', 'pinyin': 'k≈çngjiƒÅn', 'trans': 'space'}, {'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´y√∫', 'trans': 'based on'}, {'word': '‰ø°ÊÅØ', 'pinyin': 'x√¨nxƒ´', 'trans': 'information'}, {'word': 'ÂàÜÂ∏É', 'pinyin': 'fƒìnb√π', 'trans': 'distribution'}, {'word': 'ÈáèÂåñ', 'pinyin': 'li√†nghu√†', 'trans': 'quantify'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«énsh√¨', 'trans': 'show'}, {'word': '‰ºò‰∫é', 'pinyin': 'y≈çuy√∫', 'trans': 'superior to'}, {'word': 'Âü∫Á°Ä', 'pinyin': 'jƒ´ch«î', 'trans': 'foundation'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}]
[22.04.2025 08:15] Renaming previous Chinese page.
[22.04.2025 08:15] Renaming previous data. zh.html to ./d/2025-04-21_zh_reading_task.html
[22.04.2025 08:15] Writing Chinese reading task.
[22.04.2025 08:15] Writing result.
[22.04.2025 08:15] Renaming log file.
[22.04.2025 08:15] Renaming previous data. log.txt to ./logs/2025-04-22_last_log.txt
