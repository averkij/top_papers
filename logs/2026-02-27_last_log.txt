[27.02.2026 10:31] Read previous papers.
[27.02.2026 10:31] Generating top page (month).
[27.02.2026 10:31] Writing top page (month).
[27.02.2026 11:27] Read previous papers.
[27.02.2026 11:27] Get feed.
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22859
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23152
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22897
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22766
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23008
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23258
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22675
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23363
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21760
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17594
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23205
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22594
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22437
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23259
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23058
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23165
[27.02.2026 11:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22479
[27.02.2026 11:27] Extract page data from URL. URL: https://huggingface.co/papers/2602.20981
[27.02.2026 11:27] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.02.2026 11:27] No deleted papers detected.
[27.02.2026 11:27] Downloading and parsing papers (pdf, html). Total: 18.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.22859.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.22859.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.22859.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.23152.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.23152.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.23152.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.22897.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.22897.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.22897.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.22766.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.22766.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.22766.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.23008.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.23008.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.23008.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.23258.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.23258.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.23258.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.22675.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.22675.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.22675.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.23363.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.23363.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.23363.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.21760.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.21760.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.21760.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.17594.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.17594.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.17594.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.23205.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.23205.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.23205.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.22594.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.22594.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.22594.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.22437.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.22437.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.22437.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.23259.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.23259.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.23259.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.23058.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.23058.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.23058.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.23165.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.23165.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.23165.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.22479.
[27.02.2026 11:27] Extra JSON file exists (./assets/json/2602.22479.json), skip PDF parsing.
[27.02.2026 11:27] Paper image links file exists (./assets/img_data/2602.22479.json), skip HTML parsing.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Downloading and parsing paper https://huggingface.co/papers/2602.20981.
[27.02.2026 11:27] Downloading paper 2602.20981 from https://arxiv.org/pdf/2602.20981v2...
[27.02.2026 11:27] Extracting affiliations from text.
[27.02.2026 11:27] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models Christian Simon Masato Ishii Wei-Yao Wang Koichi Saito Shuyang Cui Akio Hayakawa Dongseok Shim Zhi Zhong Takashi Shibuya Yuki Mitsufuji, Shusuke Takahashi 6 2 0 2 5 ] . [ 2 1 8 9 0 2 . 2 0 6 2 : r Sony Group Corporation Sony AI {first name.last name}@sony.com "
[27.02.2026 11:27] Response: ```python
["Sony Group Corporation", "Sony AI"]
```
[27.02.2026 11:27] Deleting PDF ./assets/pdf/2602.20981.pdf.
[27.02.2026 11:27] Success.
[27.02.2026 11:27] Enriching papers with extra data.
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 0. Abstract Diagnostic-driven Progressive Evolution enables continuous improvement of large multimodal models through iterative diagnosis and targeted data generation guided by identified weaknesses.  					AI-generated summary As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) m...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 1. Abstract World Models require three consistency principles‚Äîmodal, spatial, and temporal‚Äîfor general artificial intelligence, with a proposed benchmark evaluating multimodal learning systems.  					AI-generated summary The construction of World Models capable of learning, simulating, and reasoning ab...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 2. Abstract OmniGAIA benchmark evaluates multi-modal agents on complex reasoning tasks across video, audio, and image modalities, while OmniAtlas agent improves tool-use capabilities through hindsight-guided tree exploration and OmniDPO fine-tuning.  					AI-generated summary Human intelligence natural...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 3. Abstract Research reveals that latent visual reasoning in multimodal models suffers from input-latent and latent-answer disconnects, leading to the proposal of CapImagine, a text-based approach that outperforms complex latent-space methods.  					AI-generated summary Latent visual reasoning aims to ...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 4. Abstract EMPO¬≤ is a hybrid reinforcement learning framework that enhances exploration for large language model agents by integrating memory mechanisms with on- and off-policy updates, demonstrating improved performance and adaptability in complex environments.  					AI-generated summary Exploration ...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 5. Abstract AgentDropoutV2 is a test-time framework that dynamically optimizes multi-agent system information flow through error correction and pruning mechanisms without requiring retraining.  					AI-generated summary While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the ca...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 6. Abstract A deep learning framework called SMTL improves efficient long-horizon agentic search by replacing sequential reasoning with parallel evidence acquisition, achieving state-of-the-art performance across multiple research benchmarks while reducing reasoning steps by 70.7%.  					AI-generated s...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 7. Abstract MediX-R1 presents an open-ended reinforcement learning framework for medical multimodal large language models that uses diverse reward signals and LLM-based evaluation to improve clinical reasoning beyond multiple-choice formats.  					AI-generated summary We introduce MediX-R1, an open-end...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 8. Abstract A hybrid parallelism framework for diffusion models that combines condition-based partitioning and adaptive pipeline scheduling to reduce inference latency while maintaining image quality across different architectures.  					AI-generated summary Diffusion models have achieved remarkable pr...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 9. Abstract AI systems were evaluated across a diverse set of human-designed games to assess general intelligence, revealing significant gaps in performance compared to human players, particularly in complex cognitive tasks.  					AI-generated summary Rigorously evaluating machine intelligence against ...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 10. Abstract A portable dual-iPhone system enables metric-scale human-scene reconstruction and supports embodied AI tasks including physics-based animation and robot motion control.  					AI-generated summary Human behaviors in the real world naturally encode rich, long-term contextual information that ...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 11. Abstract Causal Motion Diffusion Models introduce a unified framework for autoregressive motion generation using a causal diffusion transformer in a semantically aligned latent space, enabling fast, high-quality text-to-motion synthesis with improved temporal smoothness.  					AI-generated summary R...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 12. Abstract veScale-FSDP introduces a redesigned fully sharded data parallel system with flexible sharding and structure-aware planning to improve scalability and efficiency for large-scale model training.  					AI-generated summary Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 13. Abstract A risk-aware framework for autonomous driving that uses world modeling and risk evaluation to generalize beyond expert demonstrations without requiring explicit expert supervision.  					AI-generated summary With advances in imitation learning (IL) and large-scale driving datasets, end-to-e...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 14. Abstract GeoWorld addresses limitations in energy-based predictive world models by utilizing hyperbolic geometry to preserve latent state structures and improve long-horizon prediction performance.  					AI-generated summary Energy-based predictive world models provide a powerful approach for multi-...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 15. Abstract DyaDiT is a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals by capturing interaction dynamics between two speakers.  					AI-generated summary Generating realistic conversational gestures are essential for achieving natural, s...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 16. Abstract TRC¬≤ addresses continual learning challenges in language models through a sparse, chunk-parallel architectural design that enables rapid adaptation without catastrophic forgetting.  					AI-generated summary Continual learning is a core requirement for deployed language models, yet standard...
[27.02.2026 11:27] ********************************************************************************
[27.02.2026 11:27] Abstract 17. Abstract MMHNet enables long-form audio generation from video by integrating hierarchical methods and non-causal Mamba, achieving superior performance over existing video-to-audio approaches.  					AI-generated summary Scaling multimodal alignment between video and audio is challenging, particularly...
[27.02.2026 11:27] Read previous papers.
[27.02.2026 11:27] Generating reviews via LLM API.
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#rl", "#dataset", "#training", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–°–ø–∏—Ä–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –∏–∑ —Å–ª–∞–±–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –≤ —Å–∏–ª—É", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Diagnostic-driven Progressive Evolution (DPE) –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#benchmark", "#video", "#architecture", "#multimodal"], "emoji": "üåç", "ru": {"title": "–°–≤—è—Ç–∞—è –¢—Ä–æ–∏—Ü–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏: –ø—Ä–∏–Ω—Ü–∏–ø—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Ç—Ä—ë—Ö –ø—Ä–∏–Ω
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#agents", "#audio", "#video", "#training", "#multimodal", "#rlhf", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–û—Ç –±–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM –∫ –∏—Å—Ç–∏–Ω–Ω—ã–º –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω—ã–º –∞–≥–µ–Ω—Ç–∞–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã OmniGAIA ‚Äî —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#benchmark", "#interpretability", "#reasoning", "#multimodal"], "emoji": "üß†", "ru": {"title": "–Ø–≤–Ω–æ–µ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ª—É—á—à–µ —Å–∫—Ä—ã—Ç–æ–≥–æ: –æ—Ç –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM —Å –ø–æ–º–æ—â—å—é –ø—Ä–∏—á–∏–Ω–Ω
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#agents", "#rl", "#training"], "emoji": "üß†", "ru": {"title": "–ü–∞–º—è—Ç—å –∏ –≥–∏–±—Ä–∏–¥–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "EMPO¬≤ ‚Äî —ç—Ç–æ –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#rag", "#math", "#dataset"], "emoji": "üî•", "ru": {"title": "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—à–∏–±–æ–∫ –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "AgentDropoutV2 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ —ç—Ç–∞–ø–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Ç–æ–∫
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#training", "#synthetic", "#dataset", "#reasoning", "#rl", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ò—â–∏ –±–æ–ª—å—à–µ, –¥—É–º–∞–π –º–µ–Ω—å—à–µ: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤–º–µ—Å—Ç–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SMTL –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ 
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#training", "#healthcare", "#open_source", "#reasoning", "#rl", "#optimization", "#rlhf"], "emoji": "üè•", "ru": {"title": "–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Å–∏–≥–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "MediX-R1 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –≥–∏–±—Ä–∏–¥–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —É—Å–ª–æ–≤–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è 
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agi", "#multimodal", "#games"], "emoji": "üéÆ", "ru": {"title": "–û—Ç —É–∑–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ –∫ –æ–±—â–µ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É: –æ—Ü–µ–Ω–∫–∞ AI —á–µ—Ä–µ–∑ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –∏–≥—Ä—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Ü–µ–Ω–∫–∏ –æ–±—â–µ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ AI-—Å–∏—Å—Ç–µ–º —á–µ—Ä–µ–∑ –∏–≥—Ä—ã, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ª—é–¥—å–º–∏ –¥–ª—è –ª
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#3d", "#robotics", "#dataset", "#rl"], "emoji": "üì±", "ru": {"title": "–î–≤–µ –∫–∞–º–µ—Ä—ã –≤–º–µ—Å—Ç–æ —Å—Ç—É–¥–∏–∏: –ø–æ—Ä—Ç–∞—Ç–∏–≤–Ω—ã–π –∑–∞—Ö–≤–∞—Ç –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ –∏ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EmbodMocap ‚Äî –ø–æ—Ä—Ç–∞—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∑–∞—Ö–≤–∞—Ç–∞ –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –±–∞–∑–µ –¥–≤—É—Ö iPhone –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#video", "#training", "#multimodal", "#architecture", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ü—Ä–∏—á–∏–Ω–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è –ø–ª–∞–≤–Ω–æ–π –∏ –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Causal Motion Diffusion Models –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≤—Ç–æ—Ä–µ
[27.02.2026 11:27] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±–∫–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ Fully Sharded Data Parallel (FSDP) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º veScale-FSDP, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö 
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#agents", "#training", "#robotics"], "emoji": "üöó", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –≤–æ–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–∏—Å–∫–æ–≤ –±–µ–∑ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –Ω–∞–¥–∑–æ—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RaWMPC ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ –∏–º–∏—Ç–∞—Ü–∏–æ
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#optimization", "#video", "#architecture", "#rl", "#long_context"], "emoji": "üåç", "ru": {"title": "–ì–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—è –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –º–æ–¥–µ–ª—è—Ö –º–∏—Ä–∞", "desc": "GeoWorld ‚Äî —ç—Ç–æ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#audio", "#video", "#multimodal", "#architecture", "#diffusion"], "emoji": "ü§ù", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∂–µ—Å—Ç–æ–≤ —á–µ–ª–æ–≤–µ–∫–∞ —Å —É—á—ë—Ç–æ–º —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö", "desc": "DyaDiT ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π
[27.02.2026 11:27] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–µ–∑ –∑–∞–±—ã–≤–∞–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ TRC¬≤ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω
[27.02.2026 11:27] Querying the API.
[27.02.2026 11:27] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract MMHNet enables long-form audio generation from video by integrating hierarchical methods and non-causal Mamba, achieving superior performance over existing video-to-audio approaches.  					AI-generated summary Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations.
[27.02.2026 11:27] Response: ```json
{
  "desc": "MMHNet ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –∏ –Ω–µ–∫–∞—É–∑–∞–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Mamba –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å–µ–π. –ö–ª—é—á–µ–≤–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–∞ –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞—É–¥–∏–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –±–æ–ª–µ–µ 5 –º–∏–Ω—É—Ç, –æ–±—É—á–∞—è—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–µ–∂–¥—É –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ, –ø—Ä–µ–æ–¥–æ–ª–µ–≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MMHNet –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —Å–∏–Ω—Ç–µ–∑–∞ –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.",
  "emoji": "üé¨",
  "title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö"
}
```
[27.02.2026 11:27] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract MMHNet enables long-form audio generation from video by integrating hierarchical methods and non-causal Mamba, achieving superior performance over existing video-to-audio approaches.  					AI-generated summary Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations."

[27.02.2026 11:27] Response: ```python
['VIDEO', 'AUDIO', 'MULTIMODAL']
```
[27.02.2026 11:27] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract MMHNet enables long-form audio generation from video by integrating hierarchical methods and non-causal Mamba, achieving superior performance over existing video-to-audio approaches.  					AI-generated summary Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations."

[27.02.2026 11:28] Response: ```python
['LONG_CONTEXT']
```
[27.02.2026 11:28] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"The paper introduces MMHNet, a novel model designed for generating long-form audio from video inputs. It addresses the challenges of multimodal alignment, particularly the difficulties arising from limited data and mismatched information between text and video frames. By employing a hierarchical approach and a non-causal Mamba mechanism, MMHNet significantly enhances the performance of existing video-to-audio generation methods. The results demonstrate that the model can effectively generalize from short training instances to longer audio outputs, achieving superior results on benchmarks for long-duration audio generation.","title":"Transforming Video into Long Audio: MMHNet\'s Breakthrough!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces MMHNet, a novel model designed for generating long-form audio from video inputs. It addresses the challenges of multimodal alignment, particularly the difficulties arising from limited data and mismatched information between text and video frames. By employing a hierarchical approach and a non-causal Mamba mechanism, MMHNet significantly enhances the performance of existing video-to-audio generation methods. The results demonstrate that the model can effectively generalize from short training instances to longer audio outputs, achieving superior results on benchmarks for long-duration audio generation.', title="Transforming Video into Long Audio: MMHNet's Breakthrough!"))
[27.02.2026 11:28] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"MMHNetÊòØ‰∏ÄÁßçÊñ∞ÂûãÁöÑÂ§öÊ®°ÊÄÅÂ±ÇÊ¨°ÁΩëÁªúÔºåËÉΩÂ§ü‰ªéËßÜÈ¢ëÁîüÊàêÈïøÊó∂Èó¥ÁöÑÈü≥È¢ë„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫ÜÂ±ÇÊ¨°ÂåñÊäÄÊúØÂíåÈùûÂõ†ÊûúMambaÔºåÊòæËëóÊèêÂçá‰∫ÜËßÜÈ¢ëÂà∞Èü≥È¢ëÁîüÊàêÁöÑÊÄßËÉΩ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂç≥‰ΩøÂú®Áü≠ÂÆû‰æã‰∏äËÆ≠ÁªÉÔºåÊ®°Âûã‰πüËÉΩÂú®ÊµãËØïÊó∂ÊúâÊïàÂú∞Â§ÑÁêÜÈïøÂÆû‰æã„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåMMHNetÂú®ÈïøËßÜÈ¢ëÂà∞Èü≥È¢ëÁöÑÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊäÄÊúØÔºåËÉΩÂ§üÁîüÊàêË∂ÖËøá5ÂàÜÈíüÁöÑÈü≥È¢ë„ÄÇ","title":"MMHNetÔºöËßÜÈ¢ëÁîüÊàêÈïøÈü≥È¢ëÁöÑÊñ∞Á™ÅÁ†¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MMHNetÊòØ‰∏ÄÁßçÊñ∞ÂûãÁöÑÂ§öÊ®°ÊÄÅÂ±ÇÊ¨°ÁΩëÁªúÔºåËÉΩÂ§ü‰ªéËßÜÈ¢ëÁîüÊàêÈïøÊó∂Èó¥ÁöÑÈü≥È¢ë„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫ÜÂ±ÇÊ¨°ÂåñÊäÄÊúØÂíåÈùûÂõ†ÊûúMambaÔºåÊòæËëóÊèêÂçá‰∫ÜËßÜÈ¢ëÂà∞Èü≥È¢ëÁîüÊàêÁöÑÊÄßËÉΩ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂç≥‰ΩøÂú®Áü≠ÂÆû‰æã‰∏äËÆ≠ÁªÉÔºåÊ®°Âûã‰πüËÉΩÂú®ÊµãËØïÊó∂ÊúâÊïàÂú∞Â§ÑÁêÜÈïøÂÆû‰æã„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåMMHNetÂú®ÈïøËßÜÈ¢ëÂà∞Èü≥È¢ëÁöÑÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊäÄÊúØÔºåËÉΩÂ§üÁîüÊàêË∂ÖËøá5ÂàÜÈíüÁöÑÈü≥È¢ë„ÄÇ', title='MMHNetÔºöËßÜÈ¢ëÁîüÊàêÈïøÈü≥È¢ëÁöÑÊñ∞Á™ÅÁ†¥'))
[27.02.2026 11:28] Renaming data file.
[27.02.2026 11:28] Renaming previous data. hf_papers.json to ./d/2026-02-27.json
[27.02.2026 11:28] Saving new data file.
[27.02.2026 11:28] Generating page.
[27.02.2026 11:28] Renaming previous page.
[27.02.2026 11:28] Renaming previous data. index.html to ./d/2026-02-27.html
[27.02.2026 11:28] Writing result.
[27.02.2026 11:28] Renaming log file.
[27.02.2026 11:28] Renaming previous data. log.txt to ./logs/2026-02-27_last_log.txt
