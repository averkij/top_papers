[27.02.2026 21:23] Read previous papers.
[27.02.2026 21:23] Generating top page (month).
[27.02.2026 21:23] Writing top page (month).
[27.02.2026 22:16] Read previous papers.
[27.02.2026 22:16] Get feed.
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23152
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22859
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22638
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22897
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22766
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23008
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23258
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22675
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23363
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23361
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21760
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23205
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17594
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22594
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23339
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22437
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23058
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20332
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20300
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23259
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23165
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22953
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22045
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20423
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22479
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21420
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20981
[27.02.2026 22:16] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18253
[27.02.2026 22:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.02.2026 22:16] No deleted papers detected.
[27.02.2026 22:16] Downloading and parsing papers (pdf, html). Total: 28.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.23152.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.23152.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.23152.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.22859.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.22859.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.22859.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.22638.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.22638.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.22638.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.22897.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.22897.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.22897.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.22766.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.22766.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.22766.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.23008.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.23008.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.23008.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.23258.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.23258.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.23258.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.22675.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.22675.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.22675.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.23363.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.23363.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.23363.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.23361.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.23361.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.23361.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.21760.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.21760.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.21760.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.23205.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.23205.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.23205.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.17594.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.17594.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.17594.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.22594.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.22594.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.22594.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.23339.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.23339.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.23339.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.22437.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.22437.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.22437.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.23058.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.23058.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.23058.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.20332.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.20332.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.20332.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.20300.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.20300.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.20300.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.23259.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.23259.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.23259.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.23165.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.23165.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.23165.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.22953.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.22953.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.22953.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.22045.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.22045.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.22045.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.20423.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.20423.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.20423.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.22479.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.22479.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.22479.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.21420.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.21420.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.21420.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.20981.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.20981.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.20981.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Downloading and parsing paper https://huggingface.co/papers/2602.18253.
[27.02.2026 22:16] Extra JSON file exists (./assets/json/2602.18253.json), skip PDF parsing.
[27.02.2026 22:16] Paper image links file exists (./assets/img_data/2602.18253.json), skip HTML parsing.
[27.02.2026 22:16] Success.
[27.02.2026 22:16] Enriching papers with extra data.
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 0. Abstract World Models require three consistency principles‚Äîmodal, spatial, and temporal‚Äîfor general artificial intelligence, with a proposed benchmark evaluating multimodal learning systems.  					AI-generated summary The construction of World Models capable of learning, simulating, and reasoning ab...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 1. Abstract Diagnostic-driven Progressive Evolution enables continuous improvement of large multimodal models through iterative diagnosis and targeted data generation guided by identified weaknesses.  					AI-generated summary As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) m...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 2. Abstract MobileBench is a scalable benchmark for evaluating LLM-based route-planning agents in real-world scenarios, featuring anonymized user queries and a deterministic sandbox for reproducible testing.  					AI-generated summary Route-planning agents powered by large language models (LLMs) have e...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 3. Abstract OmniGAIA benchmark evaluates multi-modal agents on complex reasoning tasks across video, audio, and image modalities, while OmniAtlas agent improves tool-use capabilities through hindsight-guided tree exploration and OmniDPO fine-tuning.  					AI-generated summary Human intelligence natural...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 4. Abstract Research reveals that latent visual reasoning in multimodal models suffers from input-latent and latent-answer disconnects, leading to the proposal of CapImagine, a text-based approach that outperforms complex latent-space methods.  					AI-generated summary Latent visual reasoning aims to ...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 5. Abstract EMPO¬≤ is a hybrid reinforcement learning framework that enhances exploration for large language model agents by integrating memory mechanisms with on- and off-policy updates, demonstrating improved performance and adaptability in complex environments.  					AI-generated summary Exploration ...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 6. Abstract AgentDropoutV2 is a test-time framework that dynamically optimizes multi-agent system information flow through error correction and pruning mechanisms without requiring retraining.  					AI-generated summary While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the ca...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 7. Abstract A deep learning framework called SMTL improves efficient long-horizon agentic search by replacing sequential reasoning with parallel evidence acquisition, achieving state-of-the-art performance across multiple research benchmarks while reducing reasoning steps by 70.7%.  					AI-generated s...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 8. Abstract MediX-R1 presents an open-ended reinforcement learning framework for medical multimodal large language models that uses diverse reward signals and LLM-based evaluation to improve clinical reasoning beyond multiple-choice formats.  					AI-generated summary We introduce MediX-R1, an open-end...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 9. Abstract VGG-T¬≥ addresses scalability issues in 3D reconstruction by transforming variable-length key-value representations into fixed-size MLPs through test-time training, enabling linear scaling with input views and achieving significant speedup over traditional softmax attention methods.  					AI...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 10. Abstract A hybrid parallelism framework for diffusion models that combines condition-based partitioning and adaptive pipeline scheduling to reduce inference latency while maintaining image quality across different architectures.  					AI-generated summary Diffusion models have achieved remarkable pr...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 11. Abstract A portable dual-iPhone system enables metric-scale human-scene reconstruction and supports embodied AI tasks including physics-based animation and robot motion control.  					AI-generated summary Human behaviors in the real world naturally encode rich, long-term contextual information that ...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 12. Abstract AI systems were evaluated across a diverse set of human-designed games to assess general intelligence, revealing significant gaps in performance compared to human players, particularly in complex cognitive tasks.  					AI-generated summary Rigorously evaluating machine intelligence against ...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 13. Abstract Causal Motion Diffusion Models introduce a unified framework for autoregressive motion generation using a causal diffusion transformer in a semantically aligned latent space, enabling fast, high-quality text-to-motion synthesis with improved temporal smoothness.  					AI-generated summary R...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 14. Abstract Retrieval-augmented test-time adaptation with learned fusion of textual and visual features bridges the performance gap between zero-shot and supervised open-vocabulary segmentation.  					AI-generated summary Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 15. Abstract veScale-FSDP introduces a redesigned fully sharded data parallel system with flexible sharding and structure-aware planning to improve scalability and efficiency for large-scale model training.  					AI-generated summary Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 16. Abstract GeoWorld addresses limitations in energy-based predictive world models by utilizing hyperbolic geometry to preserve latent state structures and improve long-horizon prediction performance.  					AI-generated summary Energy-based predictive world models provide a powerful approach for multi-...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 17. Abstract A contextual bandit framework named QueryBandits is introduced to adaptively select optimal query-rewrite strategies for reducing hallucinations in large language models, demonstrating superior performance over static policies and enabling deployment with closed-source models.  					AI-gene...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 18. Abstract Analysis of 369,837 real-world queries reveals that specific linguistic features correlate with hallucination likelihood in large language models, identifying a risk landscape for query design.  					AI-generated summary Large Language Model (LLM) hallucinations are usually treated as defec...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 19. Abstract A risk-aware framework for autonomous driving that uses world modeling and risk evaluation to generalize beyond expert demonstrations without requiring explicit expert supervision.  					AI-generated summary With advances in imitation learning (IL) and large-scale driving datasets, end-to-e...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 20. Abstract DyaDiT is a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals by capturing interaction dynamics between two speakers.  					AI-generated summary Generating realistic conversational gestures are essential for achieving natural, s...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 21. Abstract General-purpose agents remain underdeveloped despite promising implementations, necessitating systematic evaluation frameworks and benchmarks to assess their true versatility across diverse environments.  					AI-generated summary The promise of general-purpose agents - systems that perform...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 22. Abstract The DLT-Corpus dataset, containing 2.98 billion tokens from diverse sources, enables analysis of technology emergence patterns and market-innovation correlations in the distributed ledger technology sector.  					AI-generated summary We introduce DLT-Corpus, the largest domain-specific text...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 23. Abstract MedCLIPSeg adapts CLIP for medical image segmentation by leveraging patch-level embeddings and probabilistic attention to achieve data-efficient, uncertain-aware segmentation with interpretability.  					AI-generated summary Medical image segmentation remains challenging due to limited anno...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 24. Abstract TRC¬≤ addresses continual learning challenges in language models through a sparse, chunk-parallel architectural design that enables rapid adaptation without catastrophic forgetting.  					AI-generated summary Continual learning is a core requirement for deployed language models, yet standard...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 25. Abstract Reinforcement learning with verifiable rewards suffers from reduced reasoning diversity due to uniform error penalization, which is addressed by a confidence-aware asymmetric error penalty method that dynamically modulates advantages based on rollout confidence.  					AI-generated summary R...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 26. Abstract MMHNet enables long-form audio generation from video by integrating hierarchical methods and non-causal Mamba, achieving superior performance over existing video-to-audio approaches.  					AI-generated summary Scaling multimodal alignment between video and audio is challenging, particularly...
[27.02.2026 22:16] ********************************************************************************
[27.02.2026 22:16] Abstract 27. Abstract Transfer learning enables efficient MEG-based speech decoding from perception to production tasks using a Conformer model with minimal fine-tuning data.  					AI-generated summary Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the firs...
[27.02.2026 22:16] Read previous papers.
[27.02.2026 22:16] Generating reviews via LLM API.
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#benchmark", "#video", "#architecture", "#multimodal"], "emoji": "üåç", "ru": {"title": "–°–≤—è—Ç–∞—è –¢—Ä–æ–∏—Ü–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏: –ø—Ä–∏–Ω—Ü–∏–ø—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Ç—Ä—ë—Ö –ø—Ä–∏–Ω
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#rl", "#dataset", "#training", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–°–ø–∏—Ä–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –∏–∑ —Å–ª–∞–±–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –≤ —Å–∏–ª—É", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Diagnostic-driven Progressive Evolution (DPE) –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#open_source", "#dataset"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ë–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞—Ä—à—Ä—É—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MobileBench ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞—Ä—à—Ä—É—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö 
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#agents", "#audio", "#video", "#training", "#multimodal", "#rlhf", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–û—Ç –±–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM –∫ –∏—Å—Ç–∏–Ω–Ω—ã–º –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω—ã–º –∞–≥–µ–Ω—Ç–∞–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã OmniGAIA ‚Äî —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#benchmark", "#interpretability", "#reasoning", "#multimodal"], "emoji": "üß†", "ru": {"title": "–Ø–≤–Ω–æ–µ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ª—É—á—à–µ —Å–∫—Ä—ã—Ç–æ–≥–æ: –æ—Ç –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM —Å –ø–æ–º–æ—â—å—é –ø—Ä–∏—á–∏–Ω–Ω
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#agents", "#rl", "#training"], "emoji": "üß†", "ru": {"title": "–ü–∞–º—è—Ç—å –∏ –≥–∏–±—Ä–∏–¥–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "EMPO¬≤ ‚Äî —ç—Ç–æ –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#rag", "#math", "#dataset"], "emoji": "üî•", "ru": {"title": "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—à–∏–±–æ–∫ –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "AgentDropoutV2 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ —ç—Ç–∞–ø–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Ç–æ–∫
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#training", "#synthetic", "#dataset", "#reasoning", "#rl", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ò—â–∏ –±–æ–ª—å—à–µ, –¥—É–º–∞–π –º–µ–Ω—å—à–µ: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤–º–µ—Å—Ç–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SMTL –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ 
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#training", "#healthcare", "#open_source", "#reasoning", "#rl", "#optimization", "#rlhf"], "emoji": "üè•", "ru": {"title": "–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Å–∏–≥–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "MediX-R1 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#training", "#architecture", "#3d"], "emoji": "üìê", "ru": {"title": "–û—Ç –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π –∫ –ª–∏–Ω–µ–π–Ω–æ–π: —Ç—Ä—ë—Ö–º–µ—Ä–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –Ω–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VGG-T¬≥, –º–µ—Ç–æ–¥ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –≥–∏–±—Ä–∏–¥–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —É—Å–ª–æ–≤–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è 
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#3d", "#robotics", "#dataset", "#rl"], "emoji": "üì±", "ru": {"title": "–î–≤–µ –∫–∞–º–µ—Ä—ã –≤–º–µ—Å—Ç–æ —Å—Ç—É–¥–∏–∏: –ø–æ—Ä—Ç–∞—Ç–∏–≤–Ω—ã–π –∑–∞—Ö–≤–∞—Ç –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ –∏ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EmbodMocap ‚Äî –ø–æ—Ä—Ç–∞—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∑–∞—Ö–≤–∞—Ç–∞ –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –±–∞–∑–µ –¥–≤—É—Ö iPhone –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agi", "#multimodal", "#games"], "emoji": "üéÆ", "ru": {"title": "–û—Ç —É–∑–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ –∫ –æ–±—â–µ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É: –æ—Ü–µ–Ω–∫–∞ AI —á–µ—Ä–µ–∑ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –∏–≥—Ä—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Ü–µ–Ω–∫–∏ –æ–±—â–µ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ AI-—Å–∏—Å—Ç–µ–º —á–µ—Ä–µ–∑ –∏–≥—Ä—ã, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ª—é–¥—å–º–∏ –¥–ª—è –ª
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#video", "#training", "#multimodal", "#architecture", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ü—Ä–∏—á–∏–Ω–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è –ø–ª–∞–≤–Ω–æ–π –∏ –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Causal Motion Diffusion Models –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≤—Ç–æ—Ä–µ
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#cv", "#rag", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–∞–µ–º–æ–µ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –∑–∞–¥–∞—á–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è, –≥–¥–µ –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç—ã –ª—é–±—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –æ–ø–∏—Å–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –∑
[27.02.2026 22:16] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±–∫–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ Fully Sharded Data Parallel (FSDP) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º veScale-FSDP, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö 
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#optimization", "#video", "#architecture", "#rl", "#long_context"], "emoji": "üåç", "ru": {"title": "–ì–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—è –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –º–æ–¥–µ–ª—è—Ö –º–∏—Ä–∞", "desc": "GeoWorld ‚Äî —ç—Ç–æ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#hallucinations", "#optimization"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø—Ä–æ—Ç–∏–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –±–∞–Ω–¥–∏—Ç—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ QueryBandits –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –º–Ω–æ–≥–æ—Ä—É–∫–æ–≥–æ –±–∞–Ω–¥–∏—Ç–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#hallucinations", "#interpretability"], "emoji": "üó∫Ô∏è", "ru": {"title": "–õ–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ä—Ç–∞ —Ä–∏—Å–∫–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç 369,837 —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#agents", "#training", "#robotics"], "emoji": "üöó", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –≤–æ–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–∏—Å–∫–æ–≤ –±–µ–∑ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –Ω–∞–¥–∑–æ—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RaWMPC ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ –∏–º–∏—Ç–∞—Ü–∏–æ
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#audio", "#video", "#multimodal", "#architecture", "#diffusion"], "emoji": "ü§ù", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∂–µ—Å—Ç–æ–≤ —á–µ–ª–æ–≤–µ–∫–∞ —Å —É—á—ë—Ç–æ–º —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö", "desc": "DyaDiT ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –æ—Ü–µ–Ω–∫–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ - —Å–∏—Å—Ç–µ–º, –∫–æ—Ç–æ—Ä—ã–µ —Å–ø–æ—Å–æ–±–Ω—ã —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏ –≤ –Ω–µ–∑–Ω–∞–∫–æ–º—ã—Ö –æ–∫—Ä—É
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#dataset", "#science", "#multilingual", "#open_source", "#data"], "emoji": "‚õìÔ∏è", "ru": {"title": "–û—Ç –Ω–∞—É–∫–∏ –∫ —Ä—ã–Ω–∫—É: –∫–∞–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –±–ª–æ–∫—á–µ–π–Ω–∞ –ø—Ä–æ—Ö–æ–¥—è—Ç –ø—É—Ç—å –∏–Ω–Ω–æ–≤–∞—Ü–∏–π", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç DLT-Corpus ‚Äî —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –Ω–∞ —Å–µ–≥–æ–¥–Ω—è –ø—Ä–µ–¥–º–µ—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∏–∑—É—á
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#healthcare"], "emoji": "üè•", "ru": {"title": "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∞—è vision-language –º–æ–¥–µ–ª—å –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "MedCLIPSeg –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å CLIP –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ 
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–µ–∑ –∑–∞–±—ã–≤–∞–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ TRC¬≤ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#rlhf", "#training", "#reasoning", "#optimization", "#rl"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ê—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π —à—Ç—Ä–∞—Ñ –æ—à–∏–±–æ–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#audio", "#video", "#long_context", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö", "desc": "MMHNet ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –∏ –Ω–µ–∫–∞—É–∑–∞–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ
[27.02.2026 22:16] Using data from previous issue: {"categories": ["#transfer_learning", "#training", "#audio", "#multimodal"], "emoji": "üß†", "ru": {"title": "–¢—Ä–∞–Ω—Å—Ñ–µ—Ä-–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏–∑ —Å–∏–≥–Ω–∞–ª–æ–≤ –º–æ–∑–≥–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏–∑ –º–∞–≥–Ω–∏—Ç–æ—ç–Ω—Ü–µ—Ñ–∞–ª–æ–≥—Ä–∞—Ñ–∏–∏ (–ú–≠
[27.02.2026 22:16] Renaming data file.
[27.02.2026 22:16] Renaming previous data. hf_papers.json to ./d/2026-02-27.json
[27.02.2026 22:16] Saving new data file.
[27.02.2026 22:16] Generating page.
[27.02.2026 22:16] Renaming previous page.
[27.02.2026 22:16] Renaming previous data. index.html to ./d/2026-02-27.html
[27.02.2026 22:16] Writing result.
[27.02.2026 22:16] Renaming log file.
[27.02.2026 22:16] Renaming previous data. log.txt to ./logs/2026-02-27_last_log.txt
