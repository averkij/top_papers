[27.02.2026 13:55] Read previous papers.
[27.02.2026 13:55] Generating top page (month).
[27.02.2026 13:55] Writing top page (month).
[27.02.2026 14:36] Read previous papers.
[27.02.2026 14:36] Get feed.
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23152
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22859
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22638
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22897
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22766
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23008
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23258
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22675
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23363
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21760
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23205
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17594
[27.02.2026 14:36] Extract page data from URL. URL: https://huggingface.co/papers/2602.23339
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22594
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22437
[27.02.2026 14:36] Extract page data from URL. URL: https://huggingface.co/papers/2602.20332
[27.02.2026 14:36] Extract page data from URL. URL: https://huggingface.co/papers/2602.20300
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23259
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23058
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22045
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23165
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22479
[27.02.2026 14:36] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20981
[27.02.2026 14:36] Extract page data from URL. URL: https://huggingface.co/papers/2602.18253
[27.02.2026 14:36] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.02.2026 14:36] No deleted papers detected.
[27.02.2026 14:36] Downloading and parsing papers (pdf, html). Total: 24.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.23152.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.23152.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.23152.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.22859.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.22859.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.22859.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.22638.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.22638.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.22638.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.22897.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.22897.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.22897.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.22766.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.22766.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.22766.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.23008.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.23008.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.23008.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.23258.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.23258.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.23258.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.22675.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.22675.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.22675.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.23363.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.23363.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.23363.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.21760.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.21760.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.21760.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.23205.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.23205.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.23205.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.17594.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.17594.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.17594.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.23339.
[27.02.2026 14:36] Downloading paper 2602.23339 from https://arxiv.org/pdf/2602.23339v1...
[27.02.2026 14:36] Extracting affiliations from text.
[27.02.2026 14:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 6 2 ] . [ 1 9 3 3 3 2 . 2 0 6 2 : r Retrieve and Segment: Are Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation? Tilemachos Aravanis1 Vladan Stojnic1 Bill Psomas1 Nikos Komodakis2 3 4 Giorgos Tolias1 1VRG, FEE, Czech Technical University in Prague 2University of Crete 3Archimedes, Athena RC 4IACM-FORTH Figure 1. Open-vocabulary segmentation (OVS) results. We compare three settings: (i) textual-only support (zero-shot OVS), (ii) simplified version of RNS using visual-only support, and (iii) the full RNS combining textual+visual support. Textual support: class name or description. Visual support: small set of pixel-annotated images for some classes. Initially, visual support includes images in and is later expanded to B, with B. Text-only support often yields ambiguous predictions (rider as motorcycle, background hallucinations). Visual-only support struggles when some classes lack support (person, car) and can confuse similar objects (motorcycle, bicycle) even when all classes have support. By retrieving information from images relevant to the test image and combining with the textual support, RNS is robust under missing visual support for some classes and achieves accurate segmentation. "
[27.02.2026 14:36] Response: ```python
[
    "VRG, FEE, Czech Technical University in Prague",
    "University of Crete",
    "Archimedes, Athena RC",
    "IACM-FORTH"
]
```
[27.02.2026 14:36] Deleting PDF ./assets/pdf/2602.23339.pdf.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.22594.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.22594.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.22594.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.22437.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.22437.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.22437.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.20332.
[27.02.2026 14:36] Downloading paper 2602.20332 from https://arxiv.org/pdf/2602.20332v1...
[27.02.2026 14:36] Extracting affiliations from text.
[27.02.2026 14:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 3 2 ] . [ 1 2 3 3 0 2 . 2 0 6 2 : r Published as conference paper at ICLR 2026 NO ONE SIZE FITS ALL: QUERYBANDITS FOR LLM HALLUCINATION MITIGATION Nicole Cho, William Watson, Alec Koppel, Sumitra Ganesh, Manuela Veloso JPMorgan AI Research New York, NY, USA nicole.cho@jpmorgan.com "
[27.02.2026 14:36] Response: ```python
["JPMorgan AI Research"]
```
[27.02.2026 14:36] Deleting PDF ./assets/pdf/2602.20332.pdf.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.20300.
[27.02.2026 14:36] Downloading paper 2602.20300 from https://arxiv.org/pdf/2602.20300v1...
[27.02.2026 14:36] Extracting affiliations from text.
[27.02.2026 14:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"What Makes Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance William Watson* Nicole Cho* J.P. Morgan AI Research nicole.cho@jpmorgan.com 6 2 0 2 3 2 ] . [ 1 0 0 3 0 2 . 2 0 6 2 : r a "
[27.02.2026 14:36] Response: ```python
["J.P. Morgan AI Research"]
```
[27.02.2026 14:36] Deleting PDF ./assets/pdf/2602.20300.pdf.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.23259.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.23259.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.23259.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.23058.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.23058.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.23058.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.22045.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.22045.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.22045.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.23165.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.23165.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.23165.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.22479.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.22479.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.22479.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.20981.
[27.02.2026 14:36] Extra JSON file exists (./assets/json/2602.20981.json), skip PDF parsing.
[27.02.2026 14:36] Paper image links file exists (./assets/img_data/2602.20981.json), skip HTML parsing.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Downloading and parsing paper https://huggingface.co/papers/2602.18253.
[27.02.2026 14:36] Downloading paper 2602.18253 from https://arxiv.org/pdf/2602.18253v1...
[27.02.2026 14:36] Extracting affiliations from text.
[27.02.2026 14:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data Xabier de Zuazo ID 1,, Vincenzo Verbeni ID 2, Eva Navas ID 1, Ibon Saratxaga ID 1, Mathieu Bourguignon ID 2,4,5,6, Nicola Molinaro ID 2,3 1 HiTZ Center, University of the Basque Country UPV/EHU, Spain 2 Basque Center on Cognition, Brain and Language BCBL, Spain 3 Ikerbasque, Basque Foundation for Science, Spain 4 Laboratory of Functional Anatomy, Faculty of Human Motor Sciences, Universite libre de Bruxelles (ULB), Belgium 5 Laboratoire de Neuroanatomie et Neuroimagerie translationnelles (LN2T), ULB Neuroscience Institute, Universite libre de Bruxelles (ULB), Belgium 6 WEL Research Institute, Belgium xabier.dezuazo@ehu.eus, v.verbeni@bcbl.eu, eva.navas@ehu.eus, ibon.saratxaga@ehu.eus, mabourgu@ulb.ac.be, n.molinaro@bcbl.eu "
[27.02.2026 14:36] Response: ```python
[
    "HiTZ Center, University of the Basque Country UPV/EHU, Spain",
    "Basque Center on Cognition, Brain and Language BCBL, Spain",
    "Ikerbasque, Basque Foundation for Science, Spain",
    "Laboratory of Functional Anatomy, Faculty of Human Motor Sciences, Universite libre de Bruxelles (ULB), Belgium",
    "Laboratoire de Neuroanatomie et Neuroimagerie translationnelles (LN2T), ULB Neuroscience Institute, Universite libre de Bruxelles (ULB), Belgium",
    "WEL Research Institute, Belgium"
]
```
[27.02.2026 14:36] Deleting PDF ./assets/pdf/2602.18253.pdf.
[27.02.2026 14:36] Success.
[27.02.2026 14:36] Enriching papers with extra data.
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 0. Abstract World Models require three consistency principles‚Äîmodal, spatial, and temporal‚Äîfor general artificial intelligence, with a proposed benchmark evaluating multimodal learning systems.  					AI-generated summary The construction of World Models capable of learning, simulating, and reasoning ab...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 1. Abstract Diagnostic-driven Progressive Evolution enables continuous improvement of large multimodal models through iterative diagnosis and targeted data generation guided by identified weaknesses.  					AI-generated summary As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) m...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 2. Abstract MobileBench is a scalable benchmark for evaluating LLM-based route-planning agents in real-world scenarios, featuring anonymized user queries and a deterministic sandbox for reproducible testing.  					AI-generated summary Route-planning agents powered by large language models (LLMs) have e...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 3. Abstract OmniGAIA benchmark evaluates multi-modal agents on complex reasoning tasks across video, audio, and image modalities, while OmniAtlas agent improves tool-use capabilities through hindsight-guided tree exploration and OmniDPO fine-tuning.  					AI-generated summary Human intelligence natural...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 4. Abstract Research reveals that latent visual reasoning in multimodal models suffers from input-latent and latent-answer disconnects, leading to the proposal of CapImagine, a text-based approach that outperforms complex latent-space methods.  					AI-generated summary Latent visual reasoning aims to ...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 5. Abstract EMPO¬≤ is a hybrid reinforcement learning framework that enhances exploration for large language model agents by integrating memory mechanisms with on- and off-policy updates, demonstrating improved performance and adaptability in complex environments.  					AI-generated summary Exploration ...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 6. Abstract AgentDropoutV2 is a test-time framework that dynamically optimizes multi-agent system information flow through error correction and pruning mechanisms without requiring retraining.  					AI-generated summary While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the ca...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 7. Abstract A deep learning framework called SMTL improves efficient long-horizon agentic search by replacing sequential reasoning with parallel evidence acquisition, achieving state-of-the-art performance across multiple research benchmarks while reducing reasoning steps by 70.7%.  					AI-generated s...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 8. Abstract MediX-R1 presents an open-ended reinforcement learning framework for medical multimodal large language models that uses diverse reward signals and LLM-based evaluation to improve clinical reasoning beyond multiple-choice formats.  					AI-generated summary We introduce MediX-R1, an open-end...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 9. Abstract A hybrid parallelism framework for diffusion models that combines condition-based partitioning and adaptive pipeline scheduling to reduce inference latency while maintaining image quality across different architectures.  					AI-generated summary Diffusion models have achieved remarkable pr...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 10. Abstract A portable dual-iPhone system enables metric-scale human-scene reconstruction and supports embodied AI tasks including physics-based animation and robot motion control.  					AI-generated summary Human behaviors in the real world naturally encode rich, long-term contextual information that ...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 11. Abstract AI systems were evaluated across a diverse set of human-designed games to assess general intelligence, revealing significant gaps in performance compared to human players, particularly in complex cognitive tasks.  					AI-generated summary Rigorously evaluating machine intelligence against ...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 12. Abstract Retrieval-augmented test-time adaptation with learned fusion of textual and visual features bridges the performance gap between zero-shot and supervised open-vocabulary segmentation.  					AI-generated summary Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 13. Abstract Causal Motion Diffusion Models introduce a unified framework for autoregressive motion generation using a causal diffusion transformer in a semantically aligned latent space, enabling fast, high-quality text-to-motion synthesis with improved temporal smoothness.  					AI-generated summary R...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 14. Abstract veScale-FSDP introduces a redesigned fully sharded data parallel system with flexible sharding and structure-aware planning to improve scalability and efficiency for large-scale model training.  					AI-generated summary Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 15. Abstract A contextual bandit framework named QueryBandits is introduced to adaptively select optimal query-rewrite strategies for reducing hallucinations in large language models, demonstrating superior performance over static policies and enabling deployment with closed-source models.  					AI-gene...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 16. Abstract Analysis of 369,837 real-world queries reveals that specific linguistic features correlate with hallucination likelihood in large language models, identifying a risk landscape for query design.  					AI-generated summary Large Language Model (LLM) hallucinations are usually treated as defec...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 17. Abstract A risk-aware framework for autonomous driving that uses world modeling and risk evaluation to generalize beyond expert demonstrations without requiring explicit expert supervision.  					AI-generated summary With advances in imitation learning (IL) and large-scale driving datasets, end-to-e...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 18. Abstract GeoWorld addresses limitations in energy-based predictive world models by utilizing hyperbolic geometry to preserve latent state structures and improve long-horizon prediction performance.  					AI-generated summary Energy-based predictive world models provide a powerful approach for multi-...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 19. Abstract The DLT-Corpus dataset, containing 2.98 billion tokens from diverse sources, enables analysis of technology emergence patterns and market-innovation correlations in the distributed ledger technology sector.  					AI-generated summary We introduce DLT-Corpus, the largest domain-specific text...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 20. Abstract DyaDiT is a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals by capturing interaction dynamics between two speakers.  					AI-generated summary Generating realistic conversational gestures are essential for achieving natural, s...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 21. Abstract TRC¬≤ addresses continual learning challenges in language models through a sparse, chunk-parallel architectural design that enables rapid adaptation without catastrophic forgetting.  					AI-generated summary Continual learning is a core requirement for deployed language models, yet standard...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 22. Abstract MMHNet enables long-form audio generation from video by integrating hierarchical methods and non-causal Mamba, achieving superior performance over existing video-to-audio approaches.  					AI-generated summary Scaling multimodal alignment between video and audio is challenging, particularly...
[27.02.2026 14:36] ********************************************************************************
[27.02.2026 14:36] Abstract 23. Abstract Transfer learning enables efficient MEG-based speech decoding from perception to production tasks using a Conformer model with minimal fine-tuning data.  					AI-generated summary Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the firs...
[27.02.2026 14:36] Read previous papers.
[27.02.2026 14:36] Generating reviews via LLM API.
[27.02.2026 14:36] Using data from previous issue: {"categories": ["#benchmark", "#video", "#architecture", "#multimodal"], "emoji": "üåç", "ru": {"title": "–°–≤—è—Ç–∞—è –¢—Ä–æ–∏—Ü–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏: –ø—Ä–∏–Ω—Ü–∏–ø—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Ç—Ä—ë—Ö –ø—Ä–∏–Ω
[27.02.2026 14:36] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#rl", "#dataset", "#training", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–°–ø–∏—Ä–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –∏–∑ —Å–ª–∞–±–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –≤ —Å–∏–ª—É", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Diagnostic-driven Progressive Evolution (DPE) –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[27.02.2026 14:36] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#open_source", "#dataset"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ë–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞—Ä—à—Ä—É—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MobileBench ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞—Ä—à—Ä—É—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö 
[27.02.2026 14:36] Using data from previous issue: {"categories": ["#agents", "#audio", "#video", "#training", "#multimodal", "#rlhf", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–û—Ç –±–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM –∫ –∏—Å—Ç–∏–Ω–Ω—ã–º –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω—ã–º –∞–≥–µ–Ω—Ç–∞–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã OmniGAIA ‚Äî —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω
[27.02.2026 14:36] Using data from previous issue: {"categories": ["#benchmark", "#interpretability", "#reasoning", "#multimodal"], "emoji": "üß†", "ru": {"title": "–Ø–≤–Ω–æ–µ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ª—É—á—à–µ —Å–∫—Ä—ã—Ç–æ–≥–æ: –æ—Ç –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM —Å –ø–æ–º–æ—â—å—é –ø—Ä–∏—á–∏–Ω–Ω
[27.02.2026 14:36] Using data from previous issue: {"categories": ["#agents", "#rl", "#training"], "emoji": "üß†", "ru": {"title": "–ü–∞–º—è—Ç—å –∏ –≥–∏–±—Ä–∏–¥–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "EMPO¬≤ ‚Äî —ç—Ç–æ –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º
[27.02.2026 14:36] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#rag", "#math", "#dataset"], "emoji": "üî•", "ru": {"title": "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—à–∏–±–æ–∫ –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "AgentDropoutV2 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ —ç—Ç–∞–ø–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Ç–æ–∫
[27.02.2026 14:36] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#training", "#synthetic", "#dataset", "#reasoning", "#rl", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ò—â–∏ –±–æ–ª—å—à–µ, –¥—É–º–∞–π –º–µ–Ω—å—à–µ: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤–º–µ—Å—Ç–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SMTL –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ 
[27.02.2026 14:36] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#training", "#healthcare", "#open_source", "#reasoning", "#rl", "#optimization", "#rlhf"], "emoji": "üè•", "ru": {"title": "–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Å–∏–≥–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "MediX-R1 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø
[27.02.2026 14:36] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –≥–∏–±—Ä–∏–¥–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —É—Å–ª–æ–≤–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è 
[27.02.2026 14:36] Using data from previous issue: {"categories": ["#3d", "#robotics", "#dataset", "#rl"], "emoji": "üì±", "ru": {"title": "–î–≤–µ –∫–∞–º–µ—Ä—ã –≤–º–µ—Å—Ç–æ —Å—Ç—É–¥–∏–∏: –ø–æ—Ä—Ç–∞—Ç–∏–≤–Ω—ã–π –∑–∞—Ö–≤–∞—Ç –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ –∏ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EmbodMocap ‚Äî –ø–æ—Ä—Ç–∞—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∑–∞—Ö–≤–∞—Ç–∞ –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –±–∞–∑–µ –¥–≤—É—Ö iPhone –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞
[27.02.2026 14:36] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agi", "#multimodal", "#games"], "emoji": "üéÆ", "ru": {"title": "–û—Ç —É–∑–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ –∫ –æ–±—â–µ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É: –æ—Ü–µ–Ω–∫–∞ AI —á–µ—Ä–µ–∑ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –∏–≥—Ä—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Ü–µ–Ω–∫–∏ –æ–±—â–µ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ AI-—Å–∏—Å—Ç–µ–º —á–µ—Ä–µ–∑ –∏–≥—Ä—ã, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ª—é–¥—å–º–∏ –¥–ª—è –ª
[27.02.2026 14:36] Querying the API.
[27.02.2026 14:36] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract Retrieval-augmented test-time adaptation with learned fusion of textual and visual features bridges the performance gap between zero-shot and supervised open-vocabulary segmentation.  					AI-generated summary Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.
[27.02.2026 14:36] Response: ```json
{
  "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –∑–∞–¥–∞—á–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è, –≥–¥–µ –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç—ã –ª—é–±—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –æ–ø–∏—Å–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π –Ω–∞–±–æ—Ä –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ö–ª—é—á–µ–≤–æ–π –≤–∫–ª–∞–¥ ‚Äî —ç—Ç–æ –æ–±—É—á–∞–µ–º–æ–µ —Å–ª–∏—è–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞, –≤–º–µ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∑–∞—Ä–∞–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª —Å–ª–∏—è–Ω–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –Ω—É–ª–µ–≤–æ–π –ø–æ–ø—ã—Ç–∫–æ–π –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏.",
  "emoji": "üéØ",
  "title": "–û–±—É—á–∞–µ–º–æ–µ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è"
}
```
[27.02.2026 14:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Retrieval-augmented test-time adaptation with learned fusion of textual and visual features bridges the performance gap between zero-shot and supervised open-vocabulary segmentation.  					AI-generated summary Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability."

[27.02.2026 14:36] Response: ```python
["RAG", "CV", "MULTIMODAL"]
```
[27.02.2026 14:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Retrieval-augmented test-time adaptation with learned fusion of textual and visual features bridges the performance gap between zero-shot and supervised open-vocabulary segmentation.  					AI-generated summary Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability."

[27.02.2026 14:36] Response: ```python
["TRANSFER_LEARNING"]
```

The paper discusses knowledge transfer between different modalities (textual and visual features) and adaptation techniques that leverage support sets to improve performance on open-vocabulary segmentation tasks. The core contribution involves learning to fuse features across domains/modalities for improved generalization, which is central to transfer learning concepts.
[27.02.2026 14:36] Error. Failed to parse JSON from LLM. ["TRANSFER_LEARNING"]


The paper discusses knowledge transfer between different modalities (textual and visual features) and adaptation techniques that leverage support sets to improve performance on open-vocabulary segmentation tasks. The core contribution involves learning to fuse features across domains/modalities for improved generalization, which is central to transfer learning concepts.
[27.02.2026 14:36] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper presents a new method for improving open-vocabulary segmentation (OVS) by combining textual and visual features at test time. It introduces a retrieval-augmented approach that uses a few-shot learning setting, where pixel-annotated images help refine the model\'s predictions based on text prompts. The key innovation is a learned fusion technique that adapts to each image, enhancing the interaction between the text and visual data. As a result, this method significantly improves segmentation performance while maintaining the flexibility of recognizing new categories without prior training.","title":"Bridging the Gap: Enhanced Open-Vocabulary Segmentation through Learned Fusion"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents a new method for improving open-vocabulary segmentation (OVS) by combining textual and visual features at test time. It introduces a retrieval-augmented approach that uses a few-shot learning setting, where pixel-annotated images help refine the model's predictions based on text prompts. The key innovation is a learned fusion technique that adapts to each image, enhancing the interaction between the text and visual data. As a result, this method significantly improves segmentation performance while maintaining the flexibility of recognizing new categories without prior training.", title='Bridging the Gap: Enhanced Open-Vocabulary Segmentation through Learned Fusion'))
[27.02.2026 14:36] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ£ÄÁ¥¢Â¢ûÂº∫ÁöÑÊµãËØïÊó∂ÈÄÇÂ∫îÊñπÊ≥ïÔºåÈÄöËøáÂ≠¶‰π†ÊñáÊú¨ÂíåËßÜËßâÁâπÂæÅÁöÑËûçÂêàÔºåÁº©Â∞è‰∫ÜÈõ∂-shotÂíåÁõëÁù£ÂºÄÊîæËØçÊ±áÂàÜÂâ≤‰πãÈó¥ÁöÑÊÄßËÉΩÂ∑ÆË∑ù„ÄÇÂºÄÊîæËØçÊ±áÂàÜÂâ≤ÔºàOVSÔºâ‰ΩøÂæóËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâËÉΩÂ§üËøõË°åÂÉèÁ¥†Á∫ßÁöÑ‰ªªÊÑèÁ±ªÂà´ÂàÜÂâ≤Ôºå‰ΩÜÁî±‰∫éËÆ≠ÁªÉÊó∂‰ΩøÁî®ÁöÑÁ≤óÁï•ÂõæÂÉèÁ∫ßÁõëÁù£ÂíåËá™ÁÑ∂ËØ≠Ë®ÄÁöÑËØ≠‰πâÊ®°Á≥äÊÄßÔºåOVSÁöÑË°®Áé∞‰ªç‰∏çÂèäÂÆåÂÖ®ÁõëÁù£ÁöÑÊñπÊ≥ï„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂ∞ëÊ†∑Êú¨ËÆæÁΩÆÔºåÈÄöËøáÂÉèÁ¥†Ê†áÊ≥®ÂõæÂÉèÁöÑÊîØÊåÅÈõÜÂ¢ûÂº∫ÊñáÊú¨ÊèêÁ§∫Ôºå‰ªéËÄåÂÖãÊúçËøô‰∫õÈôêÂà∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®‰øùÊåÅÂºÄÊîæËØçÊ±áËÉΩÂäõÁöÑÂêåÊó∂ÔºåÊòæËëóÁº©Â∞è‰∫ÜÈõ∂-shotÂíåÁõëÁù£ÂàÜÂâ≤‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ","title":"ËûçÂêàÊñáÊú¨‰∏éËßÜËßâÁâπÂæÅÔºåÊèêÂçáÂàÜÂâ≤ÊÄßËÉΩ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ£ÄÁ¥¢Â¢ûÂº∫ÁöÑÊµãËØïÊó∂ÈÄÇÂ∫îÊñπÊ≥ïÔºåÈÄöËøáÂ≠¶‰π†ÊñáÊú¨ÂíåËßÜËßâÁâπÂæÅÁöÑËûçÂêàÔºåÁº©Â∞è‰∫ÜÈõ∂-shotÂíåÁõëÁù£ÂºÄÊîæËØçÊ±áÂàÜÂâ≤‰πãÈó¥ÁöÑÊÄßËÉΩÂ∑ÆË∑ù„ÄÇÂºÄÊîæËØçÊ±áÂàÜÂâ≤ÔºàOVSÔºâ‰ΩøÂæóËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâËÉΩÂ§üËøõË°åÂÉèÁ¥†Á∫ßÁöÑ‰ªªÊÑèÁ±ªÂà´ÂàÜÂâ≤Ôºå‰ΩÜÁî±‰∫éËÆ≠ÁªÉÊó∂‰ΩøÁî®ÁöÑÁ≤óÁï•ÂõæÂÉèÁ∫ßÁõëÁù£ÂíåËá™ÁÑ∂ËØ≠Ë®ÄÁöÑËØ≠‰πâÊ®°Á≥äÊÄßÔºåOVSÁöÑË°®Áé∞‰ªç‰∏çÂèäÂÆåÂÖ®ÁõëÁù£ÁöÑÊñπÊ≥ï„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂ∞ëÊ†∑Êú¨ËÆæÁΩÆÔºåÈÄöËøáÂÉèÁ¥†Ê†áÊ≥®ÂõæÂÉèÁöÑÊîØÊåÅÈõÜÂ¢ûÂº∫ÊñáÊú¨ÊèêÁ§∫Ôºå‰ªéËÄåÂÖãÊúçËøô‰∫õÈôêÂà∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®‰øùÊåÅÂºÄÊîæËØçÊ±áËÉΩÂäõÁöÑÂêåÊó∂ÔºåÊòæËëóÁº©Â∞è‰∫ÜÈõ∂-shotÂíåÁõëÁù£ÂàÜÂâ≤‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ', title='ËûçÂêàÊñáÊú¨‰∏éËßÜËßâÁâπÂæÅÔºåÊèêÂçáÂàÜÂâ≤ÊÄßËÉΩ'))
[27.02.2026 14:36] Using data from previous issue: {"categories": ["#video", "#training", "#multimodal", "#architecture", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ü—Ä–∏—á–∏–Ω–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è –ø–ª–∞–≤–Ω–æ–π –∏ –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Causal Motion Diffusion Models –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≤—Ç–æ—Ä–µ
[27.02.2026 14:36] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±–∫–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ Fully Sharded Data Parallel (FSDP) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º veScale-FSDP, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö 
[27.02.2026 14:36] Querying the API.
[27.02.2026 14:36] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract A contextual bandit framework named QueryBandits is introduced to adaptively select optimal query-rewrite strategies for reducing hallucinations in large language models, demonstrating superior performance over static policies and enabling deployment with closed-source models.  					AI-generated summary Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.
[27.02.2026 14:36] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ QueryBandits –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –º–Ω–æ–≥–æ—Ä—É–∫–æ–≥–æ –±–∞–Ω–¥–∏—Ç–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Ü–µ–ª—å—é —É–º–µ–Ω—å—à–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–∞–µ—Ç—Å—è –æ–Ω–ª–∞–π–Ω —Å –ø–æ–º–æ—â—å—é –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –º–æ–¥–µ–ª–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ—ë —Å –∑–∞–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ Thompson Sampling –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 87.5% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ–±–µ–¥—ã –Ω–∞–¥ –±–∞–∑–æ–≤—ã–º –º–µ—Ç–æ–¥–æ–º –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—è –Ω–∞ 42.6-60.3%. –ö–ª—é—á–µ–≤–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –±–∞–Ω–¥–∏—Ç—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –±–∞–Ω–¥–∏—Ç—ã –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.",
  "emoji": "üéØ",
  "title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø—Ä–æ—Ç–∏–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –±–∞–Ω–¥–∏—Ç—ã"
}
```
[27.02.2026 14:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract A contextual bandit framework named QueryBandits is introduced to adaptively select optimal query-rewrite strategies for reducing hallucinations in large language models, demonstrating superior performance over static policies and enabling deployment with closed-source models.  					AI-generated summary Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation."

[27.02.2026 14:37] Response: ```python
['RL']
```

The paper introduces QueryBandits, a contextual bandit framework that adaptively learns to select optimal strategies. Contextual bandits are a core reinforcement learning technique, making this paper directly relevant to the RL topic.
[27.02.2026 14:37] Error. Failed to parse JSON from LLM. ["RL"]


The paper introduces QueryBandits, a contextual bandit framework that adaptively learns to select optimal strategies. Contextual bandits are a core reinforcement learning technique, making this paper directly relevant to the RL topic.
[27.02.2026 14:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract A contextual bandit framework named QueryBandits is introduced to adaptively select optimal query-rewrite strategies for reducing hallucinations in large language models, demonstrating superior performance over static policies and enabling deployment with closed-source models.  					AI-generated summary Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation."

[27.02.2026 14:37] Response: ```python
['HALLUCINATIONS', 'OPTIMIZATION']
```
[27.02.2026 14:37] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"The paper presents QueryBandits, a contextual bandit framework designed to improve query-rewrite strategies for large language models, particularly in reducing hallucinations. Unlike traditional static policies, QueryBandits adaptively learns optimal strategies in real-time, making it effective for both open-source and closed-source models. The framework utilizes a calibrated reward function to guide its learning process, achieving significant performance improvements over baseline methods. Results show that QueryBandits can outperform static policies and adapt to varying query contexts, highlighting the importance of flexibility in query rewriting.","title":"Adaptive Query-Rewriting to Combat Hallucinations in Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents QueryBandits, a contextual bandit framework designed to improve query-rewrite strategies for large language models, particularly in reducing hallucinations. Unlike traditional static policies, QueryBandits adaptively learns optimal strategies in real-time, making it effective for both open-source and closed-source models. The framework utilizes a calibrated reward function to guide its learning process, achieving significant performance improvements over baseline methods. Results show that QueryBandits can outperform static policies and adapt to varying query contexts, highlighting the importance of flexibility in query rewriting.', title='Adaptive Query-Rewriting to Combat Hallucinations in Language Models'))
[27.02.2026 14:37] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫QueryBanditsÁöÑ‰∏ä‰∏ãÊñáËµåÂçöÊ°ÜÊû∂ÔºåÊó®Âú®Ëá™ÈÄÇÂ∫îÈÄâÊã©ÊúÄ‰Ω≥ÁöÑÊü•ËØ¢ÈáçÂÜôÁ≠ñÁï•Ôºå‰ª•ÂáèÂ∞ëÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂπªËßâÁé∞Ë±°„ÄÇ‰∏éÈùôÊÄÅÁ≠ñÁï•Áõ∏ÊØîÔºåQueryBanditsÂú®16‰∏™ÈóÆÁ≠îÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂÖ∂È°∂Á∫ßÁ≠ñÁï•ÔºàÊ±§ÊôÆÊ£ÆÈááÊ†∑ÔºâÂú®Êó†ÈáçÂÜôÂü∫Á∫ø‰∏≠Ëé∑Âæó‰∫Ü87.5%ÁöÑËÉúÁéá„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊ≤°Êúâ‰∏ÄÁßçÈáçÂÜôÁ≠ñÁï•ÈÄÇÁî®‰∫éÊâÄÊúâÊü•ËØ¢ÔºåÊüê‰∫õÈùôÊÄÅÁ≠ñÁï•ÁîöËá≥ÂèØËÉΩÂØºËá¥Êõ¥È´òÁöÑÁ¥ØÁßØÈÅóÊÜæ„ÄÇÈÄöËøáQueryBanditsÂ≠¶‰π†Âú®Á∫øÁ≠ñÁï•ÔºåÂèØ‰ª•Âú®‰∏çÈúÄË¶ÅÈáçÊñ∞ËÆ≠ÁªÉÊàñÂü∫‰∫éÊ¢ØÂ∫¶ÁöÑÈÄÇÂ∫îÁöÑÊÉÖÂÜµ‰∏ãÔºåÊîπÂèòÊ®°ÂûãË°å‰∏∫ÔºåÈÄÇÁî®‰∫éÈó≠Ê∫êÊ®°Âûã„ÄÇ","title":"Ëá™ÈÄÇÂ∫îÊü•ËØ¢ÈáçÂÜôÔºåÂáèÂ∞ëÂπªËßâÁé∞Ë±°"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫QueryBanditsÁöÑ‰∏ä‰∏ãÊñáËµåÂçöÊ°ÜÊû∂ÔºåÊó®Âú®Ëá™ÈÄÇÂ∫îÈÄâÊã©ÊúÄ‰Ω≥ÁöÑÊü•ËØ¢ÈáçÂÜôÁ≠ñÁï•Ôºå‰ª•ÂáèÂ∞ëÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂπªËßâÁé∞Ë±°„ÄÇ‰∏éÈùôÊÄÅÁ≠ñÁï•Áõ∏ÊØîÔºåQueryBanditsÂú®16‰∏™ÈóÆÁ≠îÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂÖ∂È°∂Á∫ßÁ≠ñÁï•ÔºàÊ±§ÊôÆÊ£ÆÈááÊ†∑ÔºâÂú®Êó†ÈáçÂÜôÂü∫Á∫ø‰∏≠Ëé∑Âæó‰∫Ü87.5%ÁöÑËÉúÁéá„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊ≤°Êúâ‰∏ÄÁßçÈáçÂÜôÁ≠ñÁï•ÈÄÇÁî®‰∫éÊâÄÊúâÊü•ËØ¢ÔºåÊüê‰∫õÈùôÊÄÅÁ≠ñÁï•ÁîöËá≥ÂèØËÉΩÂØºËá¥Êõ¥È´òÁöÑÁ¥ØÁßØÈÅóÊÜæ„ÄÇÈÄöËøáQueryBanditsÂ≠¶‰π†Âú®Á∫øÁ≠ñÁï•ÔºåÂèØ‰ª•Âú®‰∏çÈúÄË¶ÅÈáçÊñ∞ËÆ≠ÁªÉÊàñÂü∫‰∫éÊ¢ØÂ∫¶ÁöÑÈÄÇÂ∫îÁöÑÊÉÖÂÜµ‰∏ãÔºåÊîπÂèòÊ®°ÂûãË°å‰∏∫ÔºåÈÄÇÁî®‰∫éÈó≠Ê∫êÊ®°Âûã„ÄÇ', title='Ëá™ÈÄÇÂ∫îÊü•ËØ¢ÈáçÂÜôÔºåÂáèÂ∞ëÂπªËßâÁé∞Ë±°'))
[27.02.2026 14:37] Querying the API.
[27.02.2026 14:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract Analysis of 369,837 real-world queries reveals that specific linguistic features correlate with hallucination likelihood in large language models, identifying a risk landscape for query design.  					AI-generated summary Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.
[27.02.2026 14:37] Response: ```json
{
  "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç 369,837 —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ 22 –∏–∑–º–µ—Ä–µ–Ω–∏–π, –≤–∫–ª—é—á–∞—é—â–∏–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞, —Ä–µ–¥–∫–æ—Å—Ç—å –ª–µ–∫—Å–∏–∫–∏, –∞–Ω–∞—Ñ–æ—Ä—É –∏ –¥—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –ø–æ –≤–ª–∏—è–Ω–∏—é –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —É –ª—é–¥–µ–π. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª —á–µ—Ç–∫—É—é ¬´–∫–∞—Ä—Ç—É —Ä–∏—Å–∫–∞¬ª: –≥–ª—É–±–æ–∫–∞—è –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –æ—à–∏–±–æ–∫ –º–æ–¥–µ–ª–∏, —Ç–æ–≥–¥–∞ –∫–∞–∫ —è—Å–Ω–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –∏ –æ—Ç–≤–µ—á–∞–µ–º–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞ —Å–Ω–∏–∂–∞—é—Ç —ç—Ç—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∑–≤–æ–ª—è—é—Ç –≤—ã—è–≤–ª—è—Ç—å —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.",
  "emoji": "üó∫Ô∏è",
  "title": "–õ–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ä—Ç–∞ —Ä–∏—Å–∫–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö"
}
```
[27.02.2026 14:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Analysis of 369,837 real-world queries reveals that specific linguistic features correlate with hallucination likelihood in large language models, identifying a risk landscape for query design.  					AI-generated summary Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies."

[27.02.2026 14:37] Response: ```python
["DATASET", "DATA", "BENCHMARK"]
```

**Justification:**

- **DATASET**: The paper introduces a large-scale analysis of 369,837 real-world queries with annotated features and hallucination labels, which constitutes a significant dataset contribution.

- **DATA**: The paper focuses on data analysis and feature engineering, constructing a 22-dimension query feature vector and analyzing linguistic properties of queries related to hallucination likelihood.

- **BENCHMARK**: The paper establishes an empirical framework for evaluating and understanding hallucination risk in LLMs based on query features, which relates to model evaluation and analysis methodology.
[27.02.2026 14:37] Error. Failed to parse JSON from LLM. ["DATASET", "DATA", "BENCHMARK"]


**Justification:**

- **DATASET**: The paper introduces a large-scale analysis of 369,837 real-world queries with annotated features and hallucination labels, which constitutes a significant dataset contribution.

- **DATA**: The paper focuses on data analysis and feature engineering, constructing a 22-dimension query feature vector and analyzing linguistic properties of queries related to hallucination likelihood.

- **BENCHMARK**: The paper establishes an empirical framework for evaluating and understanding hallucination risk in LLMs based on query features, which relates to model evaluation and analysis methodology.
[27.02.2026 14:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Analysis of 369,837 real-world queries reveals that specific linguistic features correlate with hallucination likelihood in large language models, identifying a risk landscape for query design.  					AI-generated summary Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies."

[27.02.2026 14:37] Response: ```python
['HALLUCINATIONS', 'INTERPRETABILITY']
```
[27.02.2026 14:37] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper investigates how the structure of queries affects the likelihood of hallucinations in large language models (LLMs). By analyzing 369,837 real-world queries, the authors identify specific linguistic features that correlate with higher or lower hallucination rates. They create a 22-dimensional feature vector that includes aspects like clause complexity and intention grounding, which influence how well the model understands the query. The findings suggest that certain query characteristics can be used to predict and potentially mitigate hallucinations, leading to improved query design and model performance.","title":"Understanding Query Design to Reduce Hallucinations in LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates how the structure of queries affects the likelihood of hallucinations in large language models (LLMs). By analyzing 369,837 real-world queries, the authors identify specific linguistic features that correlate with higher or lower hallucination rates. They create a 22-dimensional feature vector that includes aspects like clause complexity and intention grounding, which influence how well the model understands the query. The findings suggest that certain query characteristics can be used to predict and potentially mitigate hallucinations, leading to improved query design and model performance.', title='Understanding Query Design to Reduce Hallucinations in LLMs'))
[27.02.2026 14:37] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"ËøôÁØáËÆ∫ÊñáÂàÜÊûê‰∫Ü369,837‰∏™ÁúüÂÆûÊü•ËØ¢ÔºåÂèëÁé∞ÁâπÂÆöÁöÑËØ≠Ë®ÄÁâπÂæÅ‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂπªËßâÂèëÁîüÊ¶ÇÁéáÁõ∏ÂÖ≥„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊü•ËØ¢ÁöÑÂΩ¢ÂºèÂèØ‰ª•ÂΩ±ÂìçÊ®°ÂûãÁöÑÂìçÂ∫îÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™22Áª¥ÁöÑÊü•ËØ¢ÁâπÂæÅÂêëÈáèÔºåÊ∂µÁõñ‰∫Ü‰ªéÂè•Â§çÊùÇÊÄßÂà∞ÊÑèÂõæÂü∫Á°ÄÁ≠âÂ§ö‰∏™ÊñπÈù¢„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÊ∑±Â±ÇÂµåÂ•óÁöÑ‰ªéÂè•Âíå‰∏çÊòéÁ°ÆÊÄß‰∏éÊõ¥È´òÁöÑÂπªËßâÂÄæÂêëÁõ∏ÂÖ≥ÔºåËÄåÊòéÁ°ÆÁöÑÊÑèÂõæÂü∫Á°ÄÂíåÂèØÂõûÁ≠îÊÄßÂàô‰∏éËæÉ‰ΩéÁöÑÂπªËßâÁéáÁõ∏ÂÖ≥„ÄÇËØ•Á†îÁ©∂‰∏∫Êü•ËØ¢ÈáçÂÜôÂíåÊú™Êù•ÁöÑÂπ≤È¢ÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÂÆûËØÅÂü∫Á°Ä„ÄÇ","title":"Êü•ËØ¢ËÆæËÆ°‰∏éÂπªËßâÈ£éÈô©ÁöÑÂÖ≥ËÅî"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËøôÁØáËÆ∫ÊñáÂàÜÊûê‰∫Ü369,837‰∏™ÁúüÂÆûÊü•ËØ¢ÔºåÂèëÁé∞ÁâπÂÆöÁöÑËØ≠Ë®ÄÁâπÂæÅ‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂπªËßâÂèëÁîüÊ¶ÇÁéáÁõ∏ÂÖ≥„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊü•ËØ¢ÁöÑÂΩ¢ÂºèÂèØ‰ª•ÂΩ±ÂìçÊ®°ÂûãÁöÑÂìçÂ∫îÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™22Áª¥ÁöÑÊü•ËØ¢ÁâπÂæÅÂêëÈáèÔºåÊ∂µÁõñ‰∫Ü‰ªéÂè•Â§çÊùÇÊÄßÂà∞ÊÑèÂõæÂü∫Á°ÄÁ≠âÂ§ö‰∏™ÊñπÈù¢„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÊ∑±Â±ÇÂµåÂ•óÁöÑ‰ªéÂè•Âíå‰∏çÊòéÁ°ÆÊÄß‰∏éÊõ¥È´òÁöÑÂπªËßâÂÄæÂêëÁõ∏ÂÖ≥ÔºåËÄåÊòéÁ°ÆÁöÑÊÑèÂõæÂü∫Á°ÄÂíåÂèØÂõûÁ≠îÊÄßÂàô‰∏éËæÉ‰ΩéÁöÑÂπªËßâÁéáÁõ∏ÂÖ≥„ÄÇËØ•Á†îÁ©∂‰∏∫Êü•ËØ¢ÈáçÂÜôÂíåÊú™Êù•ÁöÑÂπ≤È¢ÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÂÆûËØÅÂü∫Á°Ä„ÄÇ', title='Êü•ËØ¢ËÆæËÆ°‰∏éÂπªËßâÈ£éÈô©ÁöÑÂÖ≥ËÅî'))
[27.02.2026 14:37] Using data from previous issue: {"categories": ["#agents", "#training", "#robotics"], "emoji": "üöó", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –≤–æ–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–∏—Å–∫–æ–≤ –±–µ–∑ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –Ω–∞–¥–∑–æ—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RaWMPC ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ –∏–º–∏—Ç–∞—Ü–∏–æ
[27.02.2026 14:37] Using data from previous issue: {"categories": ["#optimization", "#video", "#architecture", "#rl", "#long_context"], "emoji": "üåç", "ru": {"title": "–ì–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—è –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –º–æ–¥–µ–ª—è—Ö –º–∏—Ä–∞", "desc": "GeoWorld ‚Äî —ç—Ç–æ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç
[27.02.2026 14:37] Using data from previous issue: {"categories": ["#dataset", "#science", "#multilingual", "#open_source", "#data"], "emoji": "‚õìÔ∏è", "ru": {"title": "–û—Ç –Ω–∞—É–∫–∏ –∫ —Ä—ã–Ω–∫—É: –∫–∞–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –±–ª–æ–∫—á–µ–π–Ω–∞ –ø—Ä–æ—Ö–æ–¥—è—Ç –ø—É—Ç—å –∏–Ω–Ω–æ–≤–∞—Ü–∏–π", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç DLT-Corpus ‚Äî —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –Ω–∞ —Å–µ–≥–æ–¥–Ω—è –ø—Ä–µ–¥–º–µ—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∏–∑—É—á
[27.02.2026 14:37] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#audio", "#video", "#multimodal", "#architecture", "#diffusion"], "emoji": "ü§ù", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∂–µ—Å—Ç–æ–≤ —á–µ–ª–æ–≤–µ–∫–∞ —Å —É—á—ë—Ç–æ–º —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö", "desc": "DyaDiT ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π
[27.02.2026 14:37] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–µ–∑ –∑–∞–±—ã–≤–∞–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ TRC¬≤ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω
[27.02.2026 14:37] Using data from previous issue: {"categories": ["#audio", "#video", "#long_context", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö", "desc": "MMHNet ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –∏ –Ω–µ–∫–∞—É–∑–∞–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ
[27.02.2026 14:37] Querying the API.
[27.02.2026 14:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract Transfer learning enables efficient MEG-based speech decoding from perception to production tasks using a Conformer model with minimal fine-tuning data.  					AI-generated summary Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.
[27.02.2026 14:37] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏–∑ –º–∞–≥–Ω–∏—Ç–æ—ç–Ω—Ü–µ—Ñ–∞–ª–æ–≥—Ä–∞—Ñ–∏–∏ (–ú–≠–ì) —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ Conformer. –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ 50 —á–∞—Å–∞—Ö –¥–∞–Ω–Ω—ã—Ö —Å–ª—É—à–∞–Ω–∏—è –æ–¥–Ω–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞, –∞ –∑–∞—Ç–µ–º —Ç–æ–Ω–∫–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤—Å–µ–≥–æ –Ω–∞ 5 –º–∏–Ω—É—Ç–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ 18 —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤. –¢—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ 1-4% –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏ –∏ –Ω–∞ 5-6% –ø—Ä–∏ –ø–µ—Ä–µ–Ω–æ—Å–µ –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ä–µ—á–∏. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ—Ç—Ä–∞–∂–∞—é—Ç –æ–±—â–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã, –ø–æ–∑–≤–æ–ª—è—è –º–æ–¥–µ–ª—è–º, –æ–±—É—á–µ–Ω–Ω—ã–º –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ —Ä–µ—á–∏, —É—Å–ø–µ—à–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Å—Å–∏–≤–Ω–æ–µ —Å–ª—É—à–∞–Ω–∏–µ.",
  "emoji": "üß†",
  "title": "–¢—Ä–∞–Ω—Å—Ñ–µ—Ä-–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏–∑ —Å–∏–≥–Ω–∞–ª–æ–≤ –º–æ–∑–≥–∞"
}
```
[27.02.2026 14:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Transfer learning enables efficient MEG-based speech decoding from perception to production tasks using a Conformer model with minimal fine-tuning data.  					AI-generated summary Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity."

[27.02.2026 14:37] Response: ```python
["AUDIO", "TRAINING", "MULTIMODAL"]
```
[27.02.2026 14:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Transfer learning enables efficient MEG-based speech decoding from perception to production tasks using a Conformer model with minimal fine-tuning data.  					AI-generated summary Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity."

[27.02.2026 14:37] Response: ```python
['TRANSFER_LEARNING']
```
[27.02.2026 14:37] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper discusses the use of transfer learning to improve speech decoding from brain activity measured by MEG (magnetoencephalography). The authors utilize a Conformer model, pre-training it on a large dataset of listening tasks and then fine-tuning it with minimal data from speech production tasks. The results show that this approach leads to better accuracy in both the same task and when decoding between different tasks, indicating that the model captures shared neural processes. This work highlights the potential of transfer learning in enhancing brain-computer interfaces for speech applications.","title":"Enhancing Speech Decoding with Transfer Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the use of transfer learning to improve speech decoding from brain activity measured by MEG (magnetoencephalography). The authors utilize a Conformer model, pre-training it on a large dataset of listening tasks and then fine-tuning it with minimal data from speech production tasks. The results show that this approach leads to better accuracy in both the same task and when decoding between different tasks, indicating that the model captures shared neural processes. This work highlights the potential of transfer learning in enhancing brain-computer interfaces for speech applications.', title='Enhancing Speech Decoding with Transfer Learning'))
[27.02.2026 14:37] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçËΩ¨ÁßªÂ≠¶‰π†ÁöÑÊñπÊ≥ïÔºåËÉΩÂ§üÊúâÊïàÂú∞‰ªéÊÑüÁü•‰ªªÂä°Âà∞ÁîüÊàê‰ªªÂä°ËøõË°åMEGÔºàËÑëÁ£ÅÂõæÔºâÂü∫Á°ÄÁöÑËØ≠Èü≥Ëß£Á†Å„ÄÇÁ†îÁ©∂‰∏≠‰ΩøÁî®‰∫ÜConformerÊ®°ÂûãÔºåÁªèËøá50Â∞èÊó∂ÁöÑÂçï‰∏ÄÂèóËØïËÄÖÂê¨ËßâÊï∞ÊçÆÈ¢ÑËÆ≠ÁªÉÂêéÔºå‰ªÖÁî®5ÂàÜÈíüÁöÑÂæÆË∞ÉÊï∞ÊçÆÂ∞±ËÉΩÂú®18ÂêçÂèÇ‰∏éËÄÖ‰∏≠ËøõË°åËÆ≠ÁªÉ„ÄÇËΩ¨ÁßªÂ≠¶‰π†ÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÔºåÂú®Âêå‰∏Ä‰ªªÂä°‰∏≠ÁöÑÂáÜÁ°ÆÁéáÊèêÂçá‰∫Ü1-4%ÔºåËÄåË∑®‰ªªÂä°ÁöÑÂáÜÁ°ÆÁéáÊèêÂçáÂàôËææÂà∞5-6%„ÄÇËøôË°®ÊòéÔºåÈ¢ÑËÆ≠ÁªÉ‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÊØè‰∏™‰ªªÂä°ÁöÑÊÄßËÉΩÔºåËøò‰ΩøÂæóÊÑüÁü•‰∏éÁîüÊàê‰πãÈó¥ÁöÑËß£Á†ÅÂèòÂæóÂèØÈù†ÔºåÂèçÊò†‰∫ÜÂÖ±‰∫´ÁöÑÁ•ûÁªèËøáÁ®ã„ÄÇ","title":"ËΩ¨ÁßªÂ≠¶‰π†ÊèêÂçáËØ≠Èü≥Ëß£Á†ÅÊïàÁéá"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçËΩ¨ÁßªÂ≠¶‰π†ÁöÑÊñπÊ≥ïÔºåËÉΩÂ§üÊúâÊïàÂú∞‰ªéÊÑüÁü•‰ªªÂä°Âà∞ÁîüÊàê‰ªªÂä°ËøõË°åMEGÔºàËÑëÁ£ÅÂõæÔºâÂü∫Á°ÄÁöÑËØ≠Èü≥Ëß£Á†Å„ÄÇÁ†îÁ©∂‰∏≠‰ΩøÁî®‰∫ÜConformerÊ®°ÂûãÔºåÁªèËøá50Â∞èÊó∂ÁöÑÂçï‰∏ÄÂèóËØïËÄÖÂê¨ËßâÊï∞ÊçÆÈ¢ÑËÆ≠ÁªÉÂêéÔºå‰ªÖÁî®5ÂàÜÈíüÁöÑÂæÆË∞ÉÊï∞ÊçÆÂ∞±ËÉΩÂú®18ÂêçÂèÇ‰∏éËÄÖ‰∏≠ËøõË°åËÆ≠ÁªÉ„ÄÇËΩ¨ÁßªÂ≠¶‰π†ÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÔºåÂú®Âêå‰∏Ä‰ªªÂä°‰∏≠ÁöÑÂáÜÁ°ÆÁéáÊèêÂçá‰∫Ü1-4%ÔºåËÄåË∑®‰ªªÂä°ÁöÑÂáÜÁ°ÆÁéáÊèêÂçáÂàôËææÂà∞5-6%„ÄÇËøôË°®ÊòéÔºåÈ¢ÑËÆ≠ÁªÉ‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÊØè‰∏™‰ªªÂä°ÁöÑÊÄßËÉΩÔºåËøò‰ΩøÂæóÊÑüÁü•‰∏éÁîüÊàê‰πãÈó¥ÁöÑËß£Á†ÅÂèòÂæóÂèØÈù†ÔºåÂèçÊò†‰∫ÜÂÖ±‰∫´ÁöÑÁ•ûÁªèËøáÁ®ã„ÄÇ', title='ËΩ¨ÁßªÂ≠¶‰π†ÊèêÂçáËØ≠Èü≥Ëß£Á†ÅÊïàÁéá'))
[27.02.2026 14:37] Renaming data file.
[27.02.2026 14:37] Renaming previous data. hf_papers.json to ./d/2026-02-27.json
[27.02.2026 14:37] Saving new data file.
[27.02.2026 14:37] Generating page.
[27.02.2026 14:37] Renaming previous page.
[27.02.2026 14:37] Renaming previous data. index.html to ./d/2026-02-27.html
[27.02.2026 14:37] Writing result.
[27.02.2026 14:37] Renaming log file.
[27.02.2026 14:37] Renaming previous data. log.txt to ./logs/2026-02-27_last_log.txt
