[27.02.2026 09:34] Read previous papers.
[27.02.2026 09:34] Generating top page (month).
[27.02.2026 09:34] Writing top page (month).
[27.02.2026 10:30] Read previous papers.
[27.02.2026 10:30] Get feed.
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22859
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23152
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22897
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22766
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23008
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23258
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22675
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23363
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21760
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17594
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22594
[27.02.2026 10:30] Extract page data from URL. URL: https://huggingface.co/papers/2602.23205
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22437
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23058
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23259
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.23165
[27.02.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2602.22479
[27.02.2026 10:30] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.02.2026 10:30] No deleted papers detected.
[27.02.2026 10:30] Downloading and parsing papers (pdf, html). Total: 17.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.22859.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.22859.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.22859.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.23152.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.23152.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.23152.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.22897.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.22897.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.22897.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.22766.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.22766.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.22766.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.23008.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.23008.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.23008.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.23258.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.23258.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.23258.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.22675.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.22675.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.22675.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.23363.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.23363.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.23363.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.21760.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.21760.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.21760.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.17594.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.17594.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.17594.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.22594.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.22594.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.22594.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.23205.
[27.02.2026 10:30] Downloading paper 2602.23205 from https://arxiv.org/pdf/2602.23205v1...
[27.02.2026 10:30] Extracting affiliations from text.
[27.02.2026 10:30] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents Wenjia Wang1 Zhouyingcheng Liao1 Liang Pan1 Huaijin Pi1 Yuke Lou1 Xuqian Ren2 Yifan Wu1 Lei Yang3 Rishabh Dabral4 Christian Theobalt4 Taku Komura1 6 2 0 F 6 2 ] . [ 1 5 0 2 3 2 . 2 0 6 2 : r (*: Core contributor.) 1The University of Hong Kong 2Tampere University 3The Chinese University of Hong Kong 4Max-Planck Institute for Informatics Figure 1. Introducing EmbodMocap, portable and low-cost system for simultaneous 4D human and scene reconstruction, deployable anywhere using two moving iPhones. The dataset captured by EmbodMocap benefits three crucial embodied AI tasks: monocular human & scene reconstruction, physics-based character animation, and real-world humanoid motion control. Project page. "
[27.02.2026 10:30] Response: ```python
[
    "The University of Hong Kong",
    "Tampere University",
    "The Chinese University of Hong Kong",
    "Max-Planck Institute for Informatics"
]
```
[27.02.2026 10:30] Deleting PDF ./assets/pdf/2602.23205.pdf.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.22437.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.22437.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.22437.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.23058.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.23058.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.23058.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.23259.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.23259.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.23259.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.23165.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.23165.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.23165.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2602.22479.
[27.02.2026 10:30] Extra JSON file exists (./assets/json/2602.22479.json), skip PDF parsing.
[27.02.2026 10:30] Paper image links file exists (./assets/img_data/2602.22479.json), skip HTML parsing.
[27.02.2026 10:30] Success.
[27.02.2026 10:30] Enriching papers with extra data.
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 0. Abstract Diagnostic-driven Progressive Evolution enables continuous improvement of large multimodal models through iterative diagnosis and targeted data generation guided by identified weaknesses.  					AI-generated summary As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) m...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 1. Abstract World Models require three consistency principles‚Äîmodal, spatial, and temporal‚Äîfor general artificial intelligence, with a proposed benchmark evaluating multimodal learning systems.  					AI-generated summary The construction of World Models capable of learning, simulating, and reasoning ab...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 2. Abstract OmniGAIA benchmark evaluates multi-modal agents on complex reasoning tasks across video, audio, and image modalities, while OmniAtlas agent improves tool-use capabilities through hindsight-guided tree exploration and OmniDPO fine-tuning.  					AI-generated summary Human intelligence natural...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 3. Abstract Research reveals that latent visual reasoning in multimodal models suffers from input-latent and latent-answer disconnects, leading to the proposal of CapImagine, a text-based approach that outperforms complex latent-space methods.  					AI-generated summary Latent visual reasoning aims to ...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 4. Abstract EMPO¬≤ is a hybrid reinforcement learning framework that enhances exploration for large language model agents by integrating memory mechanisms with on- and off-policy updates, demonstrating improved performance and adaptability in complex environments.  					AI-generated summary Exploration ...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 5. Abstract AgentDropoutV2 is a test-time framework that dynamically optimizes multi-agent system information flow through error correction and pruning mechanisms without requiring retraining.  					AI-generated summary While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the ca...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 6. Abstract A deep learning framework called SMTL improves efficient long-horizon agentic search by replacing sequential reasoning with parallel evidence acquisition, achieving state-of-the-art performance across multiple research benchmarks while reducing reasoning steps by 70.7%.  					AI-generated s...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 7. Abstract MediX-R1 presents an open-ended reinforcement learning framework for medical multimodal large language models that uses diverse reward signals and LLM-based evaluation to improve clinical reasoning beyond multiple-choice formats.  					AI-generated summary We introduce MediX-R1, an open-end...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 8. Abstract A hybrid parallelism framework for diffusion models that combines condition-based partitioning and adaptive pipeline scheduling to reduce inference latency while maintaining image quality across different architectures.  					AI-generated summary Diffusion models have achieved remarkable pr...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 9. Abstract AI systems were evaluated across a diverse set of human-designed games to assess general intelligence, revealing significant gaps in performance compared to human players, particularly in complex cognitive tasks.  					AI-generated summary Rigorously evaluating machine intelligence against ...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 10. Abstract Causal Motion Diffusion Models introduce a unified framework for autoregressive motion generation using a causal diffusion transformer in a semantically aligned latent space, enabling fast, high-quality text-to-motion synthesis with improved temporal smoothness.  					AI-generated summary R...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 11. Abstract A portable dual-iPhone system enables metric-scale human-scene reconstruction and supports embodied AI tasks including physics-based animation and robot motion control.  					AI-generated summary Human behaviors in the real world naturally encode rich, long-term contextual information that ...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 12. Abstract veScale-FSDP introduces a redesigned fully sharded data parallel system with flexible sharding and structure-aware planning to improve scalability and efficiency for large-scale model training.  					AI-generated summary Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 13. Abstract GeoWorld addresses limitations in energy-based predictive world models by utilizing hyperbolic geometry to preserve latent state structures and improve long-horizon prediction performance.  					AI-generated summary Energy-based predictive world models provide a powerful approach for multi-...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 14. Abstract A risk-aware framework for autonomous driving that uses world modeling and risk evaluation to generalize beyond expert demonstrations without requiring explicit expert supervision.  					AI-generated summary With advances in imitation learning (IL) and large-scale driving datasets, end-to-e...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 15. Abstract DyaDiT is a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals by capturing interaction dynamics between two speakers.  					AI-generated summary Generating realistic conversational gestures are essential for achieving natural, s...
[27.02.2026 10:30] ********************************************************************************
[27.02.2026 10:30] Abstract 16. Abstract TRC¬≤ addresses continual learning challenges in language models through a sparse, chunk-parallel architectural design that enables rapid adaptation without catastrophic forgetting.  					AI-generated summary Continual learning is a core requirement for deployed language models, yet standard...
[27.02.2026 10:30] Read previous papers.
[27.02.2026 10:30] Generating reviews via LLM API.
[27.02.2026 10:30] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#rl", "#dataset", "#training", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–°–ø–∏—Ä–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –∏–∑ —Å–ª–∞–±–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –≤ —Å–∏–ª—É", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Diagnostic-driven Progressive Evolution (DPE) –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[27.02.2026 10:30] Using data from previous issue: {"categories": ["#benchmark", "#video", "#architecture", "#multimodal"], "emoji": "üåç", "ru": {"title": "–°–≤—è—Ç–∞—è –¢—Ä–æ–∏—Ü–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏: –ø—Ä–∏–Ω—Ü–∏–ø—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —Ç—Ä—ë—Ö –ø—Ä–∏–Ω
[27.02.2026 10:30] Using data from previous issue: {"categories": ["#agents", "#audio", "#video", "#training", "#multimodal", "#rlhf", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–û—Ç –±–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM –∫ –∏—Å—Ç–∏–Ω–Ω—ã–º –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω—ã–º –∞–≥–µ–Ω—Ç–∞–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã OmniGAIA ‚Äî —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω
[27.02.2026 10:30] Using data from previous issue: {"categories": ["#benchmark", "#interpretability", "#reasoning", "#multimodal"], "emoji": "üß†", "ru": {"title": "–Ø–≤–Ω–æ–µ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ª—É—á—à–µ —Å–∫—Ä—ã—Ç–æ–≥–æ: –æ—Ç –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM —Å –ø–æ–º–æ—â—å—é –ø—Ä–∏—á–∏–Ω–Ω
[27.02.2026 10:30] Using data from previous issue: {"categories": ["#agents", "#rl", "#training"], "emoji": "üß†", "ru": {"title": "–ü–∞–º—è—Ç—å –∏ –≥–∏–±—Ä–∏–¥–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "EMPO¬≤ ‚Äî —ç—Ç–æ –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º
[27.02.2026 10:30] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#rag", "#math", "#dataset"], "emoji": "üî•", "ru": {"title": "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—à–∏–±–æ–∫ –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "AgentDropoutV2 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ —ç—Ç–∞–ø–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Ç–æ–∫
[27.02.2026 10:30] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#training", "#synthetic", "#dataset", "#reasoning", "#rl", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ò—â–∏ –±–æ–ª—å—à–µ, –¥—É–º–∞–π –º–µ–Ω—å—à–µ: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤–º–µ—Å—Ç–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SMTL –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ 
[27.02.2026 10:30] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#training", "#healthcare", "#open_source", "#reasoning", "#rl", "#optimization", "#rlhf"], "emoji": "üè•", "ru": {"title": "–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Å–∏–≥–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "MediX-R1 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø
[27.02.2026 10:30] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –≥–∏–±—Ä–∏–¥–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —É—Å–ª–æ–≤–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è 
[27.02.2026 10:30] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agi", "#multimodal", "#games"], "emoji": "üéÆ", "ru": {"title": "–û—Ç —É–∑–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ –∫ –æ–±—â–µ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É: –æ—Ü–µ–Ω–∫–∞ AI —á–µ—Ä–µ–∑ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –∏–≥—Ä—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Ü–µ–Ω–∫–∏ –æ–±—â–µ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ AI-—Å–∏—Å—Ç–µ–º —á–µ—Ä–µ–∑ –∏–≥—Ä—ã, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ª—é–¥—å–º–∏ –¥–ª—è –ª
[27.02.2026 10:30] Using data from previous issue: {"categories": ["#video", "#training", "#multimodal", "#architecture", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ü—Ä–∏—á–∏–Ω–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è –ø–ª–∞–≤–Ω–æ–π –∏ –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Causal Motion Diffusion Models –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≤—Ç–æ—Ä–µ
[27.02.2026 10:30] Querying the API.
[27.02.2026 10:30] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract A portable dual-iPhone system enables metric-scale human-scene reconstruction and supports embodied AI tasks including physics-based animation and robot motion control.  					AI-generated summary Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.
[27.02.2026 10:31] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EmbodMocap ‚Äî –ø–æ—Ä—Ç–∞—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∑–∞—Ö–≤–∞—Ç–∞ –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –±–∞–∑–µ –¥–≤—É—Ö iPhone –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –∏ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã –≤ –µ–¥–∏–Ω–æ–π –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π —Å–∏—Å—Ç–µ–º–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ RGB-D –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ –¥–≤–∏–∂–µ–Ω–∏–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏—Ö —Å—Ç—É–¥–∏–π –∏ –º–∞—Ä–∫–µ—Ä–æ–≤. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –≤–æ–ø–ª–æ—â—ë–Ω–Ω–æ–≥–æ –ò–ò, –≤–∫–ª—é—á–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é —Å—Ü–µ–Ω, —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é –∞–Ω–∏–º–∞—Ü–∏—é –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –¥–≤–æ–π–Ω–∞—è —Ç–æ—á–∫–∞ –∑—Ä–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Å–Ω–∏–∂–∞–µ—Ç –≥–ª—É–±–∏–Ω–Ω—É—é –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏.",
  "emoji": "üì±",
  "title": "–î–≤–µ –∫–∞–º–µ—Ä—ã –≤–º–µ—Å—Ç–æ —Å—Ç—É–¥–∏–∏: –ø–æ—Ä—Ç–∞—Ç–∏–≤–Ω—ã–π –∑–∞—Ö–≤–∞—Ç –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ –∏ –ò–ò"
}
```
[27.02.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract A portable dual-iPhone system enables metric-scale human-scene reconstruction and supports embodied AI tasks including physics-based animation and robot motion control.  					AI-generated summary Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research."

[27.02.2026 10:31] Response: ```python
['DATASET', 'ROBOTICS', '3D', 'RL']
```
[27.02.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract A portable dual-iPhone system enables metric-scale human-scene reconstruction and supports embodied AI tasks including physics-based animation and robot motion control.  					AI-generated summary Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research."

[27.02.2026 10:31] Response: ```python
['SYNTHETIC']
```

**Reasoning:** The paper describes EmbodMocap, a data collection pipeline for generating synthetic training data using dual iPhones to capture human-scene interactions. This synthetic data is then used to train embodied AI models for various tasks (reconstruction, animation, robot control). The core contribution is a method for creating and leveraging artificial/synthetic data for training purposes, which directly matches the SYNTHETIC topic definition.
[27.02.2026 10:31] Error. Failed to parse JSON from LLM. ["SYNTHETIC"]


**Reasoning:** The paper describes EmbodMocap, a data collection pipeline for generating synthetic training data using dual iPhones to capture human-scene interactions. This synthetic data is then used to train embodied AI models for various tasks (reconstruction, animation, robot control). The core contribution is a method for creating and leveraging artificial/synthetic data for training purposes, which directly matches the SYNTHETIC topic definition.
[27.02.2026 10:31] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper introduces EmbodMocap, a novel system that uses two iPhones to capture human motion and scene geometry in everyday environments. By calibrating dual RGB-D sequences, the system reconstructs both humans and scenes in a unified metric coordinate frame, allowing for accurate data collection without the need for expensive equipment. The method significantly improves depth perception and alignment compared to traditional single-camera setups. The collected data supports various embodied AI applications, including human-scene reconstruction, physics-based animation, and robot motion control, demonstrating its potential to enhance AI understanding and interaction with the real world.","title":"Affordable Human-Scene Reconstruction with Dual iPhones"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces EmbodMocap, a novel system that uses two iPhones to capture human motion and scene geometry in everyday environments. By calibrating dual RGB-D sequences, the system reconstructs both humans and scenes in a unified metric coordinate frame, allowing for accurate data collection without the need for expensive equipment. The method significantly improves depth perception and alignment compared to traditional single-camera setups. The collected data supports various embodied AI applications, including human-scene reconstruction, physics-based animation, and robot motion control, demonstrating its potential to enhance AI understanding and interaction with the real world.', title='Affordable Human-Scene Reconstruction with Dual iPhones'))
[27.02.2026 10:31] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰æøÊê∫ÂºèÁöÑÂèåiPhoneÁ≥ªÁªüÔºåËÉΩÂ§üÂú®ÁúüÂÆûÂú∫ÊôØ‰∏≠ËøõË°å‰∫∫Á±ªÂíåÂú∫ÊôØÁöÑÈáçÂª∫„ÄÇËØ•Á≥ªÁªüÈÄöËøáËÅîÂêàÊ†°ÂáÜÂèåRGB-DÂ∫èÂàóÔºåÂÆûÁé∞Âú®Áªü‰∏ÄÁöÑÂ∫¶Èáè‰∏ñÁïåÂùêÊ†áÊ°ÜÊû∂ÂÜÖÊçïÊçâ‰∫∫Á±ªÂä®‰ΩúÂíåÂú∫ÊôØÂá†‰Ωï„ÄÇ‰∏é‰º†ÁªüÁöÑÈ´òÊàêÊú¨ÊçïÊçâÁ≥ªÁªüÁõ∏ÊØîÔºåËøôÁßçÊñπÊ≥ïÂú®Êó•Â∏∏ÁéØÂ¢É‰∏≠Êó†ÈúÄÈùôÊÄÅÁõ∏Êú∫ÊàñÊ†áËÆ∞ÔºåËÉΩÂ§üÊúâÊïàÊçïÊçâÂú∫ÊôØ‰∏ÄËá¥ÁöÑÂ∫¶ÈáèÂ∞∫Â∫¶Êï∞ÊçÆ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Ê∑±Â∫¶Ê®°Á≥äÁöÑÁºìËß£ÂíåÈáçÂª∫ÊÄßËÉΩ‰∏ä‰ºò‰∫éÂçï‰∏ÄiPhoneÊàñÂçïÁõÆÊ®°ÂûãÔºåÊé®Âä®‰∫ÜÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩÁöÑÁ†îÁ©∂„ÄÇ","title":"‰æøÊê∫ÂºèÂèåiPhoneÁ≥ªÁªüÔºöÈáçÂª∫‰∫∫Á±ª‰∏éÂú∫ÊôØÁöÑÊ°•Ê¢Å"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßç‰æøÊê∫ÂºèÁöÑÂèåiPhoneÁ≥ªÁªüÔºåËÉΩÂ§üÂú®ÁúüÂÆûÂú∫ÊôØ‰∏≠ËøõË°å‰∫∫Á±ªÂíåÂú∫ÊôØÁöÑÈáçÂª∫„ÄÇËØ•Á≥ªÁªüÈÄöËøáËÅîÂêàÊ†°ÂáÜÂèåRGB-DÂ∫èÂàóÔºåÂÆûÁé∞Âú®Áªü‰∏ÄÁöÑÂ∫¶Èáè‰∏ñÁïåÂùêÊ†áÊ°ÜÊû∂ÂÜÖÊçïÊçâ‰∫∫Á±ªÂä®‰ΩúÂíåÂú∫ÊôØÂá†‰Ωï„ÄÇ‰∏é‰º†ÁªüÁöÑÈ´òÊàêÊú¨ÊçïÊçâÁ≥ªÁªüÁõ∏ÊØîÔºåËøôÁßçÊñπÊ≥ïÂú®Êó•Â∏∏ÁéØÂ¢É‰∏≠Êó†ÈúÄÈùôÊÄÅÁõ∏Êú∫ÊàñÊ†áËÆ∞ÔºåËÉΩÂ§üÊúâÊïàÊçïÊçâÂú∫ÊôØ‰∏ÄËá¥ÁöÑÂ∫¶ÈáèÂ∞∫Â∫¶Êï∞ÊçÆ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Ê∑±Â∫¶Ê®°Á≥äÁöÑÁºìËß£ÂíåÈáçÂª∫ÊÄßËÉΩ‰∏ä‰ºò‰∫éÂçï‰∏ÄiPhoneÊàñÂçïÁõÆÊ®°ÂûãÔºåÊé®Âä®‰∫ÜÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩÁöÑÁ†îÁ©∂„ÄÇ', title='‰æøÊê∫ÂºèÂèåiPhoneÁ≥ªÁªüÔºöÈáçÂª∫‰∫∫Á±ª‰∏éÂú∫ÊôØÁöÑÊ°•Ê¢Å'))
[27.02.2026 10:31] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±–∫–æ–µ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ Fully Sharded Data Parallel (FSDP) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º veScale-FSDP, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö 
[27.02.2026 10:31] Using data from previous issue: {"categories": ["#optimization", "#video", "#architecture", "#rl", "#long_context"], "emoji": "üåç", "ru": {"title": "–ì–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—è –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –º–æ–¥–µ–ª—è—Ö –º–∏—Ä–∞", "desc": "GeoWorld ‚Äî —ç—Ç–æ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç
[27.02.2026 10:31] Using data from previous issue: {"categories": ["#agents", "#training", "#robotics"], "emoji": "üöó", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –≤–æ–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–∏—Å–∫–æ–≤ –±–µ–∑ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –Ω–∞–¥–∑–æ—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RaWMPC ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ –∏–º–∏—Ç–∞—Ü–∏–æ
[27.02.2026 10:31] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#audio", "#video", "#multimodal", "#architecture", "#diffusion"], "emoji": "ü§ù", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∂–µ—Å—Ç–æ–≤ —á–µ–ª–æ–≤–µ–∫–∞ —Å —É—á—ë—Ç–æ–º —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö", "desc": "DyaDiT ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π
[27.02.2026 10:31] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–µ–∑ –∑–∞–±—ã–≤–∞–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ TRC¬≤ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω
[27.02.2026 10:31] Renaming data file.
[27.02.2026 10:31] Renaming previous data. hf_papers.json to ./d/2026-02-27.json
[27.02.2026 10:31] Saving new data file.
[27.02.2026 10:31] Generating page.
[27.02.2026 10:31] Renaming previous page.
[27.02.2026 10:31] Renaming previous data. index.html to ./d/2026-02-27.html
[27.02.2026 10:31] Writing result.
[27.02.2026 10:31] Renaming log file.
[27.02.2026 10:31] Renaming previous data. log.txt to ./logs/2026-02-27_last_log.txt
