[20.02.2025 00:46] Read previous papers.
[20.02.2025 00:46] Generating top page (month).
[20.02.2025 00:46] Writing top page (month).
[20.02.2025 02:12] Read previous papers.
[20.02.2025 02:12] Get feed.
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12900
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13063
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11564
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11079
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13131
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13130
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13145
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13143
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12464
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09245
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11433
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12513
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11271
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12859
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12170
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12215
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13092
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12996
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09838
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12018
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12574
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12501
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12929
[20.02.2025 02:12] Extract page data from URL. URL: https://huggingface.co/papers/2502.12659
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10708
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12524
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08869
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12669
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13142
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10852
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12130
[20.02.2025 02:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10990
[20.02.2025 02:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[20.02.2025 02:12] No deleted papers detected.
[20.02.2025 02:12] Downloading and parsing papers (pdf, html). Total: 32.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12900.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.12900.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.12900.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.13063.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.13063.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.13063.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.11564.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.11564.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.11564.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.11079.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.11079.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.11079.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.13131.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.13131.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.13131.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.13130.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.13130.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.13130.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.13145.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.13145.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.13145.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.13143.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.13143.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.13143.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12464.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.12464.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.12464.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.09245.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.09245.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.09245.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.11433.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.11433.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.11433.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12513.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.12513.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.12513.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.11271.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.11271.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.11271.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12859.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.12859.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.12859.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12170.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.12170.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.12170.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12215.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.12215.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.12215.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.13092.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.13092.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.13092.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12996.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.12996.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.12996.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.09838.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.09838.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.09838.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12018.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.12018.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.12018.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12574.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.12574.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.12574.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12501.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.12501.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.12501.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12929.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.12929.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.12929.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12659.
[20.02.2025 02:12] Downloading paper 2502.12659 from http://arxiv.org/pdf/2502.12659v1...
[20.02.2025 02:12] Extracting affiliations from text.
[20.02.2025 02:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 9 5 6 2 1 . 2 0 5 2 : r The Hidden Risks of Large Reasoning Models: Safety Assessment of Kaiwen Zhou1, Chengzhi Liu1, Xuandong Zhao2, Shreedhar Jangam1, Jayanth Srinivasa 3, Gaowen Liu3, Dawn Song2, Xin Eric Wang1 1 University of California, Santa Cruz 2 University of California Berkeley 3 Cisco "
[20.02.2025 02:12] Response: ```python
["University of California, Santa Cruz", "University of California Berkeley", "Cisco"]
```
[20.02.2025 02:12] Deleting PDF ./assets/pdf/2502.12659.pdf.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.10708.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.10708.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.10708.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12524.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.12524.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.12524.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.08869.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.08869.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.08869.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12669.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.12669.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.12669.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.13142.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.13142.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.13142.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.10852.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.10852.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.10852.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.12130.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.12130.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.12130.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Downloading and parsing paper https://huggingface.co/papers/2502.10990.
[20.02.2025 02:12] Extra JSON file exists (./assets/json/2502.10990.json), skip PDF parsing.
[20.02.2025 02:12] Paper image links file exists (./assets/img_data/2502.10990.json), skip HTML parsing.
[20.02.2025 02:12] Success.
[20.02.2025 02:12] Enriching papers with extra data.
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 0. Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 1. A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches allow to reduce the amount of compute in existing language models. Despite relying o...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 2. Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discre...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 3. The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-con...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 4. Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collec...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 5. We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped wit...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 6. Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-c...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 7. Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand ...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 8. Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, bu...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 9. In contrast to RNNs, which compress previous tokens into a single hidden state, Transformers can attend to all previous tokens directly. However, standard Transformers only use representations from the immediately preceding layer. In this paper, we show that this design choice causes representation ...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 10. Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approach...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 11. After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language r...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 12. Solving complex reasoning tasks may involve visual understanding, domain knowledge retrieval, numerical calculation, and multi-step reasoning. Existing methods augment large language models (LLMs) with external tools but are restricted to specialized domains, limited tool types, or require additiona...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 13. While Large Language Models (LLMs) adapt well to downstream tasks after fine-tuning, this adaptability often compromises prompt robustness, as even minor prompt variations can significantly degrade performance. To address this, we propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective app...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 14. We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates conne...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 15. The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these mode...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 16. Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomne...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 17. Distributed optimization methods such as DiLoCo have been shown to be effective in training very large models across multiple distributed workers, such as datacenters. These methods split updates into two parts: an inner optimization phase, where the workers independently execute multiple optimizati...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 18. We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowled...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 19. Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumula...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 20. Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloa...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 21. LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly re...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 22. We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic system for autonomously so...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 23. The rapid development of large reasoning models, such as OpenAI-o3 and DeepSeek-R1, has led to significant improvements in complex reasoning over non-reasoning large language models~(LLMs). However, their enhanced capabilities, combined with the open-source access of models like DeepSeek-R1, raise s...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 24. Large Language Models (LLMs) have demonstrated remarkable success in various tasks such as natural language understanding, text summarization, and machine translation. However, their general-purpose nature often limits their effectiveness in domain-specific applications that require specialized know...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 25. Enhancing the network architecture of the YOLO framework has been crucial for a long time, but has focused on CNN-based improvements despite the proven superiority of attention mechanisms in modeling capabilities. This is because attention-based models cannot match the speed of CNN-based models. Thi...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 26. Time series analysis has witnessed the inspiring development from traditional autoregressive models, deep learning models, to recent Transformers and Large Language Models (LLMs). Efforts in leveraging vision models for time series analysis have also been made along the way but are less visible to t...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 27. The rapid advancement of perovskite solar cells (PSCs) has led to an exponential growth in research publications, creating an urgent need for efficient knowledge management and reasoning systems in this domain. We present a comprehensive knowledge-enhanced system for PSCs that integrates three key c...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 28. Foundation models pre-trained on massive unlabeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by ei...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 29. While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models n...
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 30. Large language models (LLMs) have demonstrated remarkable capabilities across a range of text-generation tasks. However, LLMs still struggle with problems requiring multi-step decision-making and environmental feedback, such as online shopping, scientific reasoning, and mathematical problem-solving....
[20.02.2025 02:12] ********************************************************************************
[20.02.2025 02:12] Abstract 31. Embedding models play a crucial role in representing and retrieving information across various NLP applications. Recent advances in large language models (LLMs) have further enhanced the performance of embedding models. While these models are often benchmarked on general-purpose datasets, real-world...
[20.02.2025 02:12] Read previous papers.
[20.02.2025 02:12] Generating reviews via LLM API.
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#training", "#audio", "#transfer_learning", "#open_source", "#optimization", "#data", "#architecture"], "emoji": "üéôÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –º–∏–Ω–∏–º—É–º–æ–º –¥–∞–Ω–Ω—ã—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Soundwave - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ—á–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#inference", "#optimization", "#training"], "emoji": "üóúÔ∏è", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Å–≤–µ—Ä—Ö—Å–∂–∞—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–∂–∞—Ç–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ç–æ–∫–µ–Ω–æ–≤ –≤ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —è–∑—ã–∫–æ–≤—ã—Ö
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#training", "#benchmark", "#optimization", "#dataset", "#diffusion", "#architecture"], "emoji": "üåä", "ru": {"title": "–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–º –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–∏ –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ —Å –∏—Å–ø
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#architecture", "#video", "#multimodal"], "emoji": "üé•", "ru": {"title": "Phantom: –±–∞–ª–∞–Ω—Å —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Phantom - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#training", "#dataset", "#interpretability"], "emoji": "üß©", "ru": {"title": "–†–∞–∑–ª–æ–∂–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏–∑ –±–∏–Ω–∞—Ä–Ω—ã—Ö
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#cv", "#robotics", "#agi", "#multimodal", "#agents", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "Magma: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ò–ò-–∞–≥–µ–Ω—Ç –¥–ª—è —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –º–∏—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Magma - –Ω–æ–≤—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, —Å–ø–æ—Å–æ–±–Ω—É—é –≤—ã–ø–æ–ª–Ω—è—Ç—å –∞–≥
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#transfer_learning", "#architecture", "#training", "#multimodal"], "emoji": "üöÄ", "ru": {"title": "mmMamba: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –ª–∏–Ω–µ–π–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç mmMamba - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#3d", "#games", "#dataset", "#reasoning", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É —Ä–æ–±–æ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —Ä–æ–±–æ—Ç–æ–≤. 
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#security", "#benchmark", "#training", "#inference", "#optimization"], "emoji": "üõ°Ô∏è", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∑–∞—â–∏—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ SafeRoute –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "LIMe: —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Layer-Integrated Memory (LIMe). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞–Ω–¥–∞
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#architecture", "#training", "#reasoning", "#rl", "#multimodal"], "emoji": "üìà", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ FLAG-Trader, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —è–∑—ã–∫–æ–≤—É
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#data", "#synthetic", "#dataset", "#multimodal"], "emoji": "üñºÔ∏è", "ru": {"title": "RealSyn: –£–ª—É—á—à–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —Ä–µ–∞–ª—å–Ω—ã—Ö –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#multimodal", "#open_source", "#training"], "emoji": "üêô", "ru": {"title": "OctoTools: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OctoTools - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#training", "#dataset", "#optimization", "#synthetic", "#inference"], "emoji": "üß†", "ru": {"title": "–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –ø—Ä–æ–º–ø—Ç–∞–º: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –Ω–∞–∑—ã–≤–∞–µ–º—ã–π P
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#open_source", "#architecture"], "emoji": "üîÄ", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ MUDD (MUltiway Dynamic Dense) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å–ª–æ—è–º–∏ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Transformer. M
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#training", "#optimization"], "emoji": "üîç", "ru": {"title": "–ö–æ—Ä–æ—á–µ - –ª—É—á—à–µ: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –±–æ
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#rl", "#games", "#reasoning", "#benchmark"], "emoji": "üåç", "ru": {"title": "Text2World: –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ LLM –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–∏—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Text2World –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#inference"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –º–µ—Ç–æ–¥—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–∞—Å–ø—Ä–µ–¥
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#data", "#training", "#cv", "#dataset", "#healthcare"], "emoji": "üè•", "ru": {"title": "HealthGPT: –ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "HealthGPT - —ç—Ç–æ –º–æ—â–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#training", "#benchmark", "#reasoning", "#inference"], "emoji": "üß†", "ru": {"title": "–ê—Ç–æ–º–∞—Ä–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Atom of Thoughts' (AoT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#inference", "#training", "#long_context"], "emoji": "üß†", "ru": {"title": "HEADINFER: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è LLM —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "HEADINFER - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#inference", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –ò–ò —á–µ—Ä–µ–∑ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –º–Ω–µ–Ω–∏–µ–º —Ç–æ–ª–ø—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Crowd-based Comparative Evaluation'.
[20.02.2025 02:12] Using data from previous issue: {"categories": ["#interpretability", "#training", "#rl", "#reasoning", "#cv", "#agents"], "emoji": "üß†", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: FoO –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è –ò–ò", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Flow-of-Options (FoO), –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ
[20.02.2025 02:12] Querying the API.
[20.02.2025 02:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The rapid development of large reasoning models, such as OpenAI-o3 and DeepSeek-R1, has led to significant improvements in complex reasoning over non-reasoning large language models~(LLMs). However, their enhanced capabilities, combined with the open-source access of models like DeepSeek-R1, raise serious safety concerns, particularly regarding their potential for misuse. In this work, we present a comprehensive safety assessment of these reasoning models, leveraging established safety benchmarks to evaluate their compliance with safety regulations. Furthermore, we investigate their susceptibility to adversarial attacks, such as jailbreaking and prompt injection, to assess their robustness in real-world applications. Through our multi-faceted analysis, we uncover four key findings: (1) There is a significant safety gap between the open-source R1 models and the o3-mini model, on both safety benchmark and attack, suggesting more safety effort on R1 is needed. (2) The distilled reasoning model shows poorer safety performance compared to its safety-aligned base models. (3) The stronger the model's reasoning ability, the greater the potential harm it may cause when answering unsafe questions. (4) The thinking process in R1 models pose greater safety concerns than their final answers. Our study provides insights into the security implications of reasoning models and highlights the need for further advancements in R1 models' safety to close the gap.
[20.02.2025 02:13] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[20.02.2025 02:13] Using data from previous issue: {"categories": ["#survey", "#dataset", "#healthcare", "#multilingual", "#benchmark", "#machine_translation", "#open_source"], "emoji": "üß†", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ LLM –¥–æ–º–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏: –∫–ª—é—á –∫ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–∞–¥–∞—á–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ
[20.02.2025 02:13] Using data from previous issue: {"categories": ["#cv", "#optimization", "#architecture"], "emoji": "üîç", "ru": {"title": "YOLOv12: –í–Ω–∏–º–∞–Ω–∏–µ —Å–æ —Å–∫–æ—Ä–æ—Å—Ç—å—é CNN", "desc": "YOLOv12 - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º–∞—Ö –≤–Ω–∏–º–∞–Ω–∏—è. –û–Ω–∞ —Å–æ—á–µ—Ç–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º –∏ —Å–∫–æ—Ä–æ—Å—Ç—å CNN-–º–æ–¥–µ–ª–µ–π
[20.02.2025 02:13] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#survey", "#data"], "emoji": "üìä", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –∞–Ω–∞–ª–∏–∑–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å
[20.02.2025 02:13] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#data", "#science", "#architecture", "#multimodal", "#training", "#graphs"], "emoji": "‚òÄÔ∏è", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö –ø–µ—Ä–æ–≤—Å–∫–∏—Ç–Ω—ã—Ö —Å–æ–ª–Ω–µ—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –ø–µ—Ä–æ–≤—Å–∫–∏
[20.02.2025 02:13] Using data from previous issue: {"categories": ["#training", "#dataset", "#transfer_learning", "#3d", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ –Ω–∞ –≤–∏–¥–µ–æ —Å –ª—é–¥—å–º–∏: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏ –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ARM4R - –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é 4D-–ø—Ä–µ–¥—Å—Ç–∞
[20.02.2025 02:13] Using data from previous issue: {"categories": ["#multilingual", "#low_resource"], "emoji": "üåç", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –±–∞—Ä—å–µ—Ä–∞: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –∫—Ä–∞–π–Ω–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ 
[20.02.2025 02:13] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#reasoning", "#optimization", "#rlhf"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–∏–Ω—è—Ç–∏—é —Ä–µ—à–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏
[20.02.2025 02:13] Using data from previous issue: {"categories": ["#dataset", "#synthetic", "#multilingual", "#transfer_learning", "#science", "#benchmark"], "emoji": "üíπ", "ru": {"title": "FinMTEB: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FinMTEB - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
[20.02.2025 02:13] Loading Chinese text from previous data.
[20.02.2025 02:13] Renaming data file.
[20.02.2025 02:13] Renaming previous data. hf_papers.json to ./d/2025-02-20.json
[20.02.2025 02:13] Saving new data file.
[20.02.2025 02:13] Generating page.
[20.02.2025 02:13] Renaming previous page.
[20.02.2025 02:13] Renaming previous data. index.html to ./d/2025-02-20.html
[20.02.2025 02:13] [Experimental] Generating Chinese page for reading.
[20.02.2025 02:13] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'Á´ØÂà∞Á´Ø', 'pinyin': 'duƒÅn d√†o duƒÅn', 'trans': 'end-to-end'}, {'word': 'ËØ≠Èü≥', 'pinyin': 'y«î yƒ´n', 'trans': 'speech'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': '‰æùËµñ', 'pinyin': 'yƒ´ l√†i', 'trans': 'rely on'}, {'word': 'Â§ßËßÑÊ®°', 'pinyin': 'd√† guƒ´ m√≥', 'trans': 'large-scale'}, {'word': 'Ê†áÊ≥®', 'pinyin': 'biƒÅo zh√π', 'trans': 'annotate'}, {'word': 'Êï∞ÊçÆ', 'pinyin': 'sh√π j√π', 'trans': 'data'}, {'word': 'ËøõË°å', 'pinyin': 'j√¨n x√≠ng', 'trans': 'carry out'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πn li√†n', 'trans': 'train'}, {'word': 'È´òÊïà', 'pinyin': 'gƒÅo xi√†o', 'trans': 'efficient'}, {'word': 'Ê∑±ÂÖ•', 'pinyin': 'shƒìn r√π', 'trans': 'in-depth'}, {'word': 'Êé¢ËÆ®', 'pinyin': 't√†n t«éo', 'trans': 'explore'}, {'word': 'ËÅöÁÑ¶', 'pinyin': 'j√π jiƒÅo', 'trans': 'focus on'}, {'word': 'Âü∫Êú¨', 'pinyin': 'jƒ´ bƒõn', 'trans': 'basic'}, {'word': 'ÈóÆÈ¢ò', 'pinyin': 'w√®n t√≠', 'trans': 'problem'}, {'word': 'Ë°®Á§∫', 'pinyin': 'bi«éo sh√¨', 'trans': 'represent'}, {'word': 'Á©∫Èó¥', 'pinyin': 'k≈çng jiƒÅn', 'trans': 'space'}, {'word': 'Â∑ÆË∑ù', 'pinyin': 'chƒÅ j√π', 'trans': 'gap'}, {'word': 'Â∫èÂàó', 'pinyin': 'x√π li√®', 'trans': 'sequence'}, {'word': 'ÈïøÂ∫¶', 'pinyin': 'ch√°ng d√π', 'trans': 'length'}, {'word': '‰∏ç‰∏ÄËá¥', 'pinyin': 'b√π yƒ´ zh√¨', 'trans': 'inconsistent'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'Êû∂ÊûÑ', 'pinyin': 'ji√† g√≤u', 'trans': 'architecture'}, {'word': 'Ëß£ÂÜ≥', 'pinyin': 'jiƒõ ju√©', 'trans': 'solve'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«én sh√¨', 'trans': 'show'}, {'word': '‰ºò‰∫é', 'pinyin': 'y≈çu y√∫', 'trans': 'superior to'}, {'word': 'ÂÖàËøõ', 'pinyin': 'xiƒÅn j√¨n', 'trans': 'advanced'}, {'word': 'ÂàÜÊûê', 'pinyin': 'fƒìn xƒ´', 'trans': 'analyze'}, {'word': 'Ë°®Êòé', 'pinyin': 'bi«éo m√≠ng', 'trans': 'indicate'}, {'word': '‰øùÊåÅ', 'pinyin': 'b«éo ch√≠', 'trans': 'maintain'}, {'word': 'Êô∫ËÉΩ', 'pinyin': 'zh√¨ n√©ng', 'trans': 'intelligence'}, {'word': 'È°πÁõÆ', 'pinyin': 'xi√†ng m√π', 'trans': 'project'}, {'word': '‰ª£Á†Å', 'pinyin': 'd√†i m«é', 'trans': 'code'}]
[20.02.2025 02:13] Renaming previous Chinese page.
[20.02.2025 02:13] Renaming previous data. zh.html to ./d/2025-02-19_zh_reading_task.html
[20.02.2025 02:13] Writing Chinese reading task.
[20.02.2025 02:13] Writing result.
[20.02.2025 02:13] Renaming log file.
[20.02.2025 02:13] Renaming previous data. log.txt to ./logs/2025-02-20_last_log.txt
