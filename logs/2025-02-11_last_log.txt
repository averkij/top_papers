[11.02.2025 05:10] Read previous papers.
[11.02.2025 05:10] Generating top page (month).
[11.02.2025 05:10] Writing top page (month).
[11.02.2025 06:14] Read previous papers.
[11.02.2025 06:14] Get feed.
[11.02.2025 06:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.06781
[11.02.2025 06:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.05609
[11.02.2025 06:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.03628
[11.02.2025 06:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.06772
[11.02.2025 06:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.06788
[11.02.2025 06:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.06635
[11.02.2025 06:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.06155
[11.02.2025 06:14] Extract page data from URL. URL: https://huggingface.co/papers/2502.06703
[11.02.2025 06:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.06049
[11.02.2025 06:14] Extract page data from URL. URL: https://huggingface.co/papers/2502.06786
[11.02.2025 06:14] Extract page data from URL. URL: https://huggingface.co/papers/2502.06527
[11.02.2025 06:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.06023
[11.02.2025 06:14] Get page data from previous paper. URL: https://huggingface.co/papers/2502.05431
[11.02.2025 06:14] Extract page data from URL. URL: https://huggingface.co/papers/2502.06782
[11.02.2025 06:14] Extract page data from URL. URL: https://huggingface.co/papers/2502.06764
[11.02.2025 06:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.02.2025 06:14] No deleted papers detected.
[11.02.2025 06:14] Downloading and parsing papers (pdf, html). Total: 15.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.06781.
[11.02.2025 06:14] Extra JSON file exists (./assets/json/2502.06781.json), skip PDF parsing.
[11.02.2025 06:14] Paper image links file exists (./assets/img_data/2502.06781.json), skip HTML parsing.
[11.02.2025 06:14] Success.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.05609.
[11.02.2025 06:14] Extra JSON file exists (./assets/json/2502.05609.json), skip PDF parsing.
[11.02.2025 06:14] Paper image links file exists (./assets/img_data/2502.05609.json), skip HTML parsing.
[11.02.2025 06:14] Success.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.03628.
[11.02.2025 06:14] Extra JSON file exists (./assets/json/2502.03628.json), skip PDF parsing.
[11.02.2025 06:14] Paper image links file exists (./assets/img_data/2502.03628.json), skip HTML parsing.
[11.02.2025 06:14] Success.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.06772.
[11.02.2025 06:14] Extra JSON file exists (./assets/json/2502.06772.json), skip PDF parsing.
[11.02.2025 06:14] Paper image links file exists (./assets/img_data/2502.06772.json), skip HTML parsing.
[11.02.2025 06:14] Success.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.06788.
[11.02.2025 06:14] Extra JSON file exists (./assets/json/2502.06788.json), skip PDF parsing.
[11.02.2025 06:14] Paper image links file exists (./assets/img_data/2502.06788.json), skip HTML parsing.
[11.02.2025 06:14] Success.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.06635.
[11.02.2025 06:14] Extra JSON file exists (./assets/json/2502.06635.json), skip PDF parsing.
[11.02.2025 06:14] Paper image links file exists (./assets/img_data/2502.06635.json), skip HTML parsing.
[11.02.2025 06:14] Success.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.06155.
[11.02.2025 06:14] Extra JSON file exists (./assets/json/2502.06155.json), skip PDF parsing.
[11.02.2025 06:14] Paper image links file exists (./assets/img_data/2502.06155.json), skip HTML parsing.
[11.02.2025 06:14] Success.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.06703.
[11.02.2025 06:14] Downloading paper 2502.06703 from http://arxiv.org/pdf/2502.06703v1...
[11.02.2025 06:14] Extracting affiliations from text.
[11.02.2025 06:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2025-2-11 Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling Runze Liu1,2,*, Junqi Gao1,3, Jian Zhao4, Kaiyan Zhang2, Xiu Li2, Biqing Qi1,, Wanli Ouyang1 and Bowen Zhou1,2, 1Shanghai AI Laboratory, 2Tsinghua University, 3Harbin Institute of Technology, 4BUPT Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, 1B LLM can exceed 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, 0.5B LLM outperforms GPT-4o, 3B LLM surpasses 405B LLM, and 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is promising approach for enhancing the reasoning abilities of LLMs. Our website is available at https://ryanliu112.github.io/compute-optimal-tts. 5 2 0 2 0 ] . [ 1 3 0 7 6 0 . 2 0 5 2 : r Figure 1: Comparison between the performance of smaller LLMs compute-optimal TTS and th"
[11.02.2025 06:14] Response: ```python
["Shanghai AI Laboratory", "Tsinghua University", "Harbin Institute of Technology", "BUPT"]
```
[11.02.2025 06:14] Deleting PDF ./assets/pdf/2502.06703.pdf.
[11.02.2025 06:14] Success.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.06049.
[11.02.2025 06:14] Extra JSON file exists (./assets/json/2502.06049.json), skip PDF parsing.
[11.02.2025 06:14] Paper image links file exists (./assets/img_data/2502.06049.json), skip HTML parsing.
[11.02.2025 06:14] Success.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.06786.
[11.02.2025 06:14] Downloading paper 2502.06786 from http://arxiv.org/pdf/2502.06786v1...
[11.02.2025 06:14] Extracting affiliations from text.
[11.02.2025 06:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2025-02- Pranav Nair*,1, Puranjay Datta*,1, Jeff Dean1, Prateek Jain1 and Aditya Kusupati1 1Google DeepMind, *Equal contribution Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models especially to low precisions like int4 or int2 requires trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. This paper proposes Matryoshka Quantization (MatQuant), novel multi-scale quantization technique that addresses the challenge of needing multiple quantized models. It allows training and maintaining just one model, which can then be served at different precision levels. Furthermore, due to the co-training and co-distillation regularization provided by MatQuant, the int2 precision models extracted by MatQuant can be up to 10% more accurate than standard int2 quantization (using techniques like QAT or OmniQuant). This represents significant progress in model quantization, demonstrated by the fact that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more accurate than an int8 FFN-quantized Gemma-2 2B model. 1. Introduction Due to their impressive performance, there is strong push to deploy deep learning models, particularly large language models (LLMs) (Achiam et al., 2023; Dubey et al., 2024; Team et al., 2024) in large number of scenarios. Due to autoregressive nature of LLMs, decode latency tends to dominate inference cost. Decode latency itself is dominated by communication cost of transferring model weights from high-bandwidth memory (HBM) to the SRAM or due to transferri"
[11.02.2025 06:14] Response: ```python
["Google DeepMind"]
```
[11.02.2025 06:14] Deleting PDF ./assets/pdf/2502.06786.pdf.
[11.02.2025 06:14] Success.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.06527.
[11.02.2025 06:14] Downloading paper 2502.06527 from http://arxiv.org/pdf/2502.06527v1...
[11.02.2025 06:14] Extracting affiliations from text.
[11.02.2025 06:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers D. She * 1 Mushui Liu * 2 Jingxuan Pang 2 Jin Wang 1 Zhen Yang 3 Wanggui He Guanghao Zhang Yi Wang 2 Qihan Huang 2 H. Tang 1 Yunlong Yu 2 Siming Fu 2 Project Page 5 2 0 2 0 1 ] . [ 1 7 2 5 6 0 . 2 0 5 2 : r Figure 1: CustomVideoX synthesizes natural motions while preserving the fine-grained object details. "
[11.02.2025 06:14] Response: ```python
[]
```
[11.02.2025 06:14] Extracting affiliations from text.
[11.02.2025 06:14] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers D. She * 1 Mushui Liu * 2 Jingxuan Pang 2 Jin Wang 1 Zhen Yang 3 Wanggui He Guanghao Zhang Yi Wang 2 Qihan Huang 2 H. Tang 1 Yunlong Yu 2 Siming Fu 2 Project Page 5 2 0 2 0 1 ] . [ 1 7 2 5 6 0 . 2 0 5 2 : r Figure 1: CustomVideoX synthesizes natural motions while preserving the fine-grained object details.Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffu- *Equal contribution 1University of Science and Technology of China 2Zhejiang Univerisity 3Hong Kong University of Science and Technology (Guangzhou). Correspondence to: Siming Fu <fusiming@zju.edu.cn>, Yunlong Yu <yuyunlong@zju.edu.cn>. sion transformer for personalized video generation from reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware 1 CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality. 1. Introduction Text-to-video (T2V) generation (Wu et al., 2023; Guo et al., 2024; Yang et al., 2024) have recently garnered extensive attention in the fields of computer vision. The ability to generate visual content from textual descriptions holds significant promise for applications, e.g., digital art creation, and advertising. In this domain, customized video generation (Wei et al., 2024a; Chefer et al., 2024; Jiang et al., 2024; Wu et al., 2024a) aims to generate videos that not only adhere to textual instructions but also maintain consistency with provided reference images. Although strongly in need, this topic remains underexplored unlike customized image generation (Ruiz et al., 2023; Gal et al., 2023; Ye et al., 2023; Chen et al., 2023; Wang et al., 2024; Tan et al., 2024). Existing work on customized video generation (Wei et al., 2024a;b; Jiang et al., 2024; Wu et al., 2024b;a) still struggles to maintain the subject identity of reference images across varying frames with high fidelity, often overlooking significant detail information. Additionally, the temporal coherence between consecutive frames remains unsatisfactory (Chefer et al., 2024; Wei et al., 2024a), exhibiting discontinuities. These limitations stem from the dual flaws of current frameworks: (1) inefficient exploitation of pixellevel reference guidance during spatial feature encoding, which undermines identity preservation by neglecting local structural cues, and (2) the uniform propagation of reference features throughout the denoising process fails to accommodate the distinct requirements of early-stage structural establishment versus late-stage motion dynamics refinement. This inflexible paradigm results in both identity degradation and temporal coherence artifacts, highlighting the need for temporally-aware reference adaptation mechanisms. we develop 3D Reference Attention mechanism that enables direct interaction between reference image features and each frame of the video within the VDiT framework. This strategy streamlines the interaction process by eliminating the need for separate temporal and spatial attention stages, thereby enhancing both efficiency and effectiveness. The reference image features are extracted using the same VDiT model, obviating the need for additional encoders and ensuring seamless integration. Second, we introduce Time-Aware Attention Bias mechanism to modulate the influence of reference image features throughout the denoising process inherent in diffusion models. By dynamically modulating the weight of reference image featuresinitiating with minimal influence when the input predominantly consists of random noise, escalating during intermediate phases, and diminishing in the final stages. This dynamic weighting promotes better temporal coherence and visual quality in the generated videos, allowing the model to capture both the overall structure and fine-grained temporal details effectively. Finally, we introduce an Entity Region-Aware Enhancement (ERAE) module that adaptively focuses on the key entity regions. After computing the attention score of the key entitys textual token with the latent, we adjust the attention bias by setting threshold to refine the focus. To tackle the challenge of limited paired reference video data, we introduce method for synthesizing high-quality reference video pairs, creating large-scale dataset of two million (2M) pairs. This enhances training and improves model generalization. We also propose comprehensive benchmark for personalized video generation, covering wide range of objects and scenes. As shown in Figure 1, our approach demonstrates strong personalization capabilities. Extensive experiments confirm that our method outperforms existing approaches in video quality and temporal coherence, setting new state-of-the-art on both existing and proposed benchmarks. The contributions of this paper can be summarized as follows: We present CustomVideoX, zero-shot personalized video generation framework based on the VDiT architecture, enabling physically consistent articulations without compromising high-resolution morphological details. The integration of 3D Reference Attentio"
[11.02.2025 06:14] Mistral response. {"id": "0a8e9fcb8b394ec0a1f8bc110608c50d", "object": "chat.completion", "created": 1739254484, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"University of Science and Technology of China\", \"Zhejiang Univerisity\", \"Hong Kong University of Science and Technology (Guangzhou)\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1555, "total_tokens": 1597, "completion_tokens": 42}}
[11.02.2025 06:14] Response: ```python
["University of Science and Technology of China", "Zhejiang Univerisity", "Hong Kong University of Science and Technology (Guangzhou)"]
```
[11.02.2025 06:14] Deleting PDF ./assets/pdf/2502.06527.pdf.
[11.02.2025 06:14] Success.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.06023.
[11.02.2025 06:14] Extra JSON file exists (./assets/json/2502.06023.json), skip PDF parsing.
[11.02.2025 06:14] Paper image links file exists (./assets/img_data/2502.06023.json), skip HTML parsing.
[11.02.2025 06:14] Success.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.05431.
[11.02.2025 06:14] Extra JSON file exists (./assets/json/2502.05431.json), skip PDF parsing.
[11.02.2025 06:14] Paper image links file exists (./assets/img_data/2502.05431.json), skip HTML parsing.
[11.02.2025 06:14] Success.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.06782.
[11.02.2025 06:14] Downloading paper 2502.06782 from http://arxiv.org/pdf/2502.06782v1...
[11.02.2025 06:14] Extracting affiliations from text.
[11.02.2025 06:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT Dongyang Liu * 1 2 Shicheng Li * 2 Yutong Liu * 2 Zhen Li * 1 Kai Wang * 2 Xinyue Li * 2 Qi Qin 2 Yufei Liu 2 Yi Xin 2 Zhongyu Li 2 3 Bin Fu 2 Chenyang Si 2 Yuewen Cao 2 Conghui He 2 Ziwei Liu 2 Yu Qiao 2 Qibin Hou 3 Hongsheng Li 1 2 Peng Gao 2 5 2 0 2 0 1 ] . [ 1 2 8 7 6 0 . 2 0 5 2 : r a "
[11.02.2025 06:14] Response: ```python
[]
```
[11.02.2025 06:14] Extracting affiliations from text.
[11.02.2025 06:14] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT Dongyang Liu * 1 2 Shicheng Li * 2 Yutong Liu * 2 Zhen Li * 1 Kai Wang * 2 Xinyue Li * 2 Qi Qin 2 Yufei Liu 2 Yi Xin 2 Zhongyu Li 2 3 Bin Fu 2 Chenyang Si 2 Yuewen Cao 2 Conghui He 2 Ziwei Liu 2 Yu Qiao 2 Qibin Hou 3 Hongsheng Li 1 2 Peng Gao 2 5 2 0 2 0 1 ] . [ 1 2 8 7 6 0 . 2 0 5 2 : r aRecent advancements have established Diffusion Transformers (DiTs) as dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data. To address this, we introduce Lumina-Video, framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis. LuminaVideo incorporates Multi-scale Next-DiT architecture, which jointly learns multiple patchifications to enhance both efficiency and flexibility. By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos dynamic degree. Combined with progressive training scheme with increasingly higher resolution and FPS, and multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency. We additionally propose Lumina-V2A, video-toaudio model based on Next-DiT, to create synchronized sounds for generated videos. Codes are released at https://www.github.com/ Alpha-VLLM/Lumina-Video. 1. Introduction The field of generative modeling has witnessed significant advancements in recent years, with Diffusion Transformers *Equal Contribution Corresponding Authors Project Lead 1The Chinese University of Hong Kong 2Shanghai Correspondence AI Laboratory Peng Gao <gaopeng@pjlab.org.cn>, Hongsheng Li to: <hsli@ee.cuhk.edu.hk>. 3Nankai University. (DiTs) emerging as powerful paradigm for creating highquality photorealistic content (Peebles & Xie, 2023; Esser et al., 2024; OpenAI, 2024). One notable innovation is NextDiT (Zhuo et al., 2024), an improved version of flow-based DiT that has shown strong performance in image generation. By combining architectural enhancements such as 3D RoPE for superior spatiotemporal representation, sandwich normalization for stabilized training, and grouped-query attention for efficient attention computation, Next-DiT has achieved remarkable success. The model consistently produces images that are not only visually compelling but also exhibit high diversity and fine-grained details. However, despite its advancements in image synthesis, the potential of Next-DiT for video generation remains underexplored. Video generation poses unique challenges that go beyond those encountered in image generation. The inherent complexity of modeling both spatial and temporal dimensions in coherent manner introduces significant computational and architectural challenges. While Next-DiT can be adapted for video tasks, its current design is not specifically tailored for the spatiotemporal intricacies of video data, leading to an excessive number of video tokens and low computational efficiency. These limitations underscore the need for tailored approach that fully leverages the capabilities of Next-DiT while addressing the unique demands of video synthesis. To bridge this gap, we introduce Lumina-Video, novel framework that excels at generating high-quality videos by building upon the strengths of Next-DiT. At the core of Lumina-Video is Multi-scale Next-DiT, an extension of Next-DiT into multi-scale architecture by introducing multiple patch sizes that share common DiT backbone and are trained jointly in unified manner. This straightforward yet elegant approach allows the model to learn video structures across different computational budgets simultaneously. By strategically allocating different patchifications to various sampling timesteps, Lumina-Video achieves notable improvements in inference efficiency with only minor sacrifice in quality. This design also enables users to dynamically adjust the computational cost based on resource constraints and specific requirements, offering greater flexibility during inference. Considering the importance of motion in Submission and Formatting Instructions for ICML 2025 Figure 1. Lumina-Video demonstrates strong ability to generate high-quality videos with rich details and remarkable temporal coherence, accurately following both simple and detailed text prompts. videos, we additionally derive motion scores from optical flow and incorporate them as an extra conditioning input to the DiT. By designing systematic strategy that separately manipulates the motion conditioning of positive and negative classifier-free guidance (CFG) (Ho & Salimans, 2022) samples, Lumina-Video provides an effective interface for controlling the extent of dynamics in generated videos. We further refine the training strategy by progressively training the model on videos with increasing spatiotemporal resolutions to improve training efficiency, leveraging joint image-video training to enhance frame quality and text comprehension, and incorporating multi-source training to fully utilize diverse real and synthetic data sources. These designs enable Lumina-Video to seamlessly tackle the challenging video generation task across wide range of scenarios. and visual quality. In addition, we design Lumina-V2A, Next-DiT-based video-to-audio framework to bring generated silent videos to real life with synchronized sounds. Furthermore, in line with our commitment to democratizing access to advanced video generation technologies, we open-source our training framework and model parameters, empowering the research community to explore, extend, and deploy Lumina-Video across diverse range of applications. Through this work, we aim to catalyze future innovations in video generation and pave the way for broader adoption of generative modeling techniques. 2. Related Work 2.1. Video Generation Our contributions establish Lumina-Video as new solution for video generation, offering researchers and practitioners powerful and flexible tool for creating video content. By fully unleashing the potential of Multi-Scale Next-DiT, Lumina-Video is capable of generating high-fidelity videos of varying resolution, excell"
[11.02.2025 06:14] Mistral response. {"id": "0f4a55f8fe25400787ccc58967b1d82b", "object": "chat.completion", "created": 1739254492, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"The Chinese University of Hong Kong\",\n    \"Shanghai Correspondence AI Laboratory\",\n    \"Nankai University\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1545, "total_tokens": 1585, "completion_tokens": 40}}
[11.02.2025 06:14] Response: ```python
[
    "The Chinese University of Hong Kong",
    "Shanghai Correspondence AI Laboratory",
    "Nankai University"
]
```
[11.02.2025 06:14] Deleting PDF ./assets/pdf/2502.06782.pdf.
[11.02.2025 06:14] Success.
[11.02.2025 06:14] Downloading and parsing paper https://huggingface.co/papers/2502.06764.
[11.02.2025 06:14] Downloading paper 2502.06764 from http://arxiv.org/pdf/2502.06764v1...
[11.02.2025 06:15] Extracting affiliations from text.
[11.02.2025 06:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 1 ] . [ 1 4 6 7 6 0 . 2 0 5 2 : r History-Guided Video Diffusion Kiwhan Song* 1 Boyuan Chen* 1 Max Simchowitz 1 Yilun Du 1 Russ Tedrake 1 Vincent Sitzmann "
[11.02.2025 06:15] Response: []
[11.02.2025 06:15] Extracting affiliations from text.
[11.02.2025 06:15] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 1 ] . [ 1 4 6 7 6 0 . 2 0 5 2 : r History-Guided Video Diffusion Kiwhan Song* 1 Boyuan Chen* 1 Max Simchowitz 1 Yilun Du 1 Russ Tedrake 1 Vincent SitzmannClassifier-free guidance (CFG) is key technique for improving conditional generation in diffusion models, enabling more accurate control while enhancing sample quality. It is natural to extend this technique to video diffusion, which generates video conditioned on variable number of context frames, collectively referred to as history. However, we find two key challenges to guiding with variable-length history: architectures that only support fixed-size conditioning, and the empirical observation that CFG-style history dropout performs poorly. To address this, we propose the Diffusion Forcing Transformer (DFoT), video diffusion architecture and theoretically grounded training objective that jointly enable conditioning on flexible number of history frames. We then introduce History Guidance, family of guidance methods uniquely enabled by DFoT. We show that its simplest form, vanilla history guidance, already significantly improves video generation quality and temporal consistency. more advanced method, history guidance across time and frequency further enhances motion dynamics, enables compositional generalization to out-ofdistribution history, and can stably roll out extremely long videos. Website: this URLDiffusion models are effective generative models in domains such as image, sound, and video. Critical to their success is classifier-free guidance (CFG) (Ho & Salimans, 2022), which trades off between sample quality and diversity by jointly training conditional and an unconditional diffusion model and combining their score estimates when sampling. In the realm of video generative models, CFG commonly relies on either text or image prompts as conditioning variables. Yet, another conditioning variable, namely the entire collection of previous video frames, or history, deserves further exploration. In this paper, we investigate the following *Equal contribution 1MIT. Correspondence to: Kiwhan Song <kiwhan@mit.edu>, Boyuan Chen <boyuanc@mit.edu>. Preprint. Under review. question: Can we use different portions of history - variable lengths, subsets of frames, and even different image-domain frequencies - as form of guidance for video generation? Importantly, CFG with flexible history is incompatible with existing diffusion model architectures and the most obvious fix significantly degrades sample quality (see Section 3). To address these limitations, we propose the Diffusion Forcing Transformer (DFoT), video diffusion framework that enables flexible conditioning on any portion of the input history. Extending the noising-as-masking paradigm in Diffusion Forcing (Chen et al., 2024) to non-causal transformers, DFoT trains video diffusion models by applying independent noise levels to each frame. During sampling, portions of the history can be selectively masked with noise, enabling flexible conditioning and guidance. For instance, in CFG, the unconditional score corresponds to our model with the entire history masked out. Notably, DFoT is compatible with existing architectures such as DiT (Peebles & Xie, 2023) and U-ViT (Hoogeboom et al., 2023; 2024) and can be efficiently implemented through fine-tuning of pre-trained video diffusion models. At sampling time, the DFoT facilitates family of historyconditioned guidance methods, collectively referred to as History Guidance (HG). The simplest of these, Vanilla History Guidance (HG-v), uses an arbitrary length of history as the conditioning variable for CFG. Notably, even this simple method significantly enhances video quality. We further introduce two advanced methods enabled by the DFoT: Temporal History Guidance (HG-t) and Fractional History Guidance (HG-f) . These extend history guidance beyond special case of CFG. Temporal History Guidance combines scores from different history windows. Fractional History Guidance conditions on history windows corrupted by varying levels of noise, effectively acting as low-pass filter on historical frames. With minor modifications, it can also target specific frequency bandwidths to enhance the dynamic degree of generated videos (hence the frequency-based terminology). Together, we compose HG-t and HG-f to create comprehensive history guidance paradigm, which we term history guidance across time and frequency (HG-tf). The Diffusion Forcing Transformer and associated History Guidance methods dramatically improve the quality and consistency of video generation, enabling the creation of exceptionally long videos through autoregressive extension, History-Guided Video Diffusion Figure 1. Diffusion Forcing Transformer with history guidance enables stable rollout of extremely long videos. We visualize 21 frames from an 862-frame long navigation video generated by our DFoT model from single image in test set video that the model has never seen before. Best viewed as videos on our project website. outperforming the de facto standard DiT diffusion and performing on par with industry models trained with an order of magnitude more compute. In Fig. 1, we showcase our method by using history guidance across time and frequency with DFoT to generate an 862-frame navigation video from single imagemany times longer than prior results and the maximum video length in the training set. Our contributions can be summarized as follows: 1. We propose the Diffusion Forcing Transformer (DFoT), competitive video diffusion framework that enables sampling-time conditioning using any portion of history, capability that is difficult to achieve with existing models. 2. We introduce History Guidance (HG), family of history-conditioned guidance methods enabled by DFoT that significantly enhance sample consistency, motion dynamics, and visual quality in video diffusion. 3. We empirically demonstrate the state-of-the-art performance and new capabilities enabled by our method, especially in long video generation. Additionally, we provide theoretical justification of the training objective through variational lower bound.Diffusion Models. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020a; Song et al., 2021) define forward process that transforms data distribution into white noise via stochastic process over increasing noise levels [0, 1]: xk = αkx0+σkϵ, where ϵ (0, I). The goal of the model is to reverse this process by learning to estimate the log pk(xk) (Vincent"
[11.02.2025 06:15] Mistral response. {"id": "766fff26168440a29310b48c4435cc49", "object": "chat.completion", "created": 1739254513, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\"MIT\"]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1546, "total_tokens": 1549, "completion_tokens": 3}}
[11.02.2025 06:15] Response: ["MIT"]
[11.02.2025 06:15] Deleting PDF ./assets/pdf/2502.06764.pdf.
[11.02.2025 06:15] Success.
[11.02.2025 06:15] Enriching papers with extra data.
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 0. Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrev...
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 1. Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as they have been widely incorporated into real-world services. Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, ther...
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 2. Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits rankings througho...
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 3. We present that hierarchical LLM reasoning via scaling thought templates can effectively optimize the reasoning search space and outperform the mathematical reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3. We train our ReasonFlux-32B model with only 8 GPUs and introduc...
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 4. Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap ...
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 5. Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency...
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 6. Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for ge...
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 7. Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty infl...
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 8. This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long con...
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 9. Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models -- especially to low precisions like int4 or int2 -- requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequentl...
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 10. Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffusion transformer for ...
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 11. Recent advancements in human preference optimization, originally developed for Large Language Models (LLMs), have shown significant potential in improving text-to-image diffusion models. These methods aim to learn the distribution of preferred samples while distinguishing them from less preferred on...
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 12. Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of ...
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 13. Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains larg...
[11.02.2025 06:15] ********************************************************************************
[11.02.2025 06:15] Abstract 14. Classifier-free guidance (CFG) is a key technique for improving conditional generation in diffusion models, enabling more accurate control while enhancing sample quality. It is natural to extend this technique to video diffusion, which generates video conditioned on a variable number of context fram...
[11.02.2025 06:15] Read previous papers.
[11.02.2025 06:15] Generating reviews via LLM API.
[11.02.2025 06:15] Using data from previous issue: {"categories": ["#training", "#open_source", "#rl", "#reasoning", "#optimization", "#math"], "emoji": "🧮", "ru": {"title": "OREAL: Прорыв в обучении с подкреплением для математических рассуждений", "desc": "Статья представляет новый фреймворк обучения с подкреплением под названием OREAL для решения 
[11.02.2025 06:15] Using data from previous issue: {"categories": ["#training", "#architecture", "#inference", "#optimization"], "emoji": "🚀", "ru": {"title": "Иерархическая черновая генерация: новый подход к ускорению вывода в LLM", "desc": "Статья представляет новый метод ускорения вывода в больших языковых моделях (LLM) под названием Hierarchy Dr
[11.02.2025 06:15] Using data from previous issue: {"categories": ["#cv", "#training", "#hallucinations", "#benchmark", "#inference", "#interpretability", "#multimodal"], "emoji": "🔍", "ru": {"title": "Борьба с галлюцинациями в визуально-языковых моделях: метод VISTA", "desc": "Эта статья исследует проблему галлюцинаций в крупных визуально-языковых 
[11.02.2025 06:15] Using data from previous issue: {"categories": ["#rl", "#optimization", "#training", "#reasoning", "#math", "#benchmark", "#inference"], "emoji": "🧠", "ru": {"title": "Революция в математическом мышлении ИИ: иерархические рассуждения на новом уровне", "desc": "Представлена модель ReasonFlux-32B, использующая иерархическое рассужде
[11.02.2025 06:15] Using data from previous issue: {"categories": ["#optimization", "#training", "#architecture", "#agi", "#multimodal"], "emoji": "🔬", "ru": {"title": "Революция в мультимодальном обучении: EVEv2.0 - эффективность без энкодеров", "desc": "Статья представляет новое семейство моделей машинного обучения EVEv2.0, работающих с изображени
[11.02.2025 06:15] Using data from previous issue: {"categories": ["#small_models", "#multilingual", "#training", "#data", "#benchmark", "#open_source", "#dataset", "#low_resource"], "emoji": "🇨🇳", "ru": {"title": "Создание эффективной китайскоязычной LLM с открытым исходным кодом", "desc": "Steel-LLM - это языковая модель, ориентированная на китайс
[11.02.2025 06:15] Using data from previous issue: {"categories": ["#training", "#diffusion", "#optimization", "#inference", "#video"], "emoji": "🎬", "ru": {"title": "Ускорение генерации видео: эффективность без потери качества", "desc": "Статья представляет новый подход к ускорению генерации видео с помощью Диффузионных Трансформеров (DiTs). Авторы
[11.02.2025 06:15] Querying the API.
[11.02.2025 06:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is a promising approach for enhancing the reasoning abilities of LLMs.
[11.02.2025 06:15] Response: {
  "desc": "Это исследование анализирует влияние метода Test-Time Scaling (TTS) на производительность больших языковых моделей (LLM) при решении сложных математических задач. Авторы изучают, как выбор политики модели, модели вознаграждения процесса (PRM) и сложность задачи влияют на оптимальную стратегию TTS. Результаты показывают, что даже небольшие модели могут превзойти более крупные при использовании оптимальной стратегии TTS. Исследование демонстрирует потенциал TTS для улучшения способностей LLM к рассуждению и решению сложных задач.",
  "emoji": "🧮",
  "title": "Маленькие модели побеждают гигантов: сила масштабирования во время теста"
}
[11.02.2025 06:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is a promising approach for enhancing the reasoning abilities of LLMs."

[11.02.2025 06:15] Response: ```python
['INFERENCE', 'MATH', 'SMALL_MODELS', 'TRAINING']
```
[11.02.2025 06:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Test-Time Scaling (TTS) is an important method for improving the performance of Large Language Models (LLMs) by using additional computation during the inference phase. However, current studies do not systematically analyze how policy models, Process Reward Models (PRMs), and problem difficulty influence TTS. This lack of analysis limits the understanding and practical use of TTS methods. In this paper, we focus on two core questions: (1) What is the optimal approach to scale test-time computation across different policy models, PRMs, and problem difficulty levels? (2) To what extent can extended computation improve the performance of LLMs on complex tasks, and can smaller language models outperform larger ones through this approach? Through comprehensive experiments on MATH-500 and challenging AIME24 tasks, we have the following observations: (1) The compute-optimal TTS strategy is highly dependent on the choice of policy model, PRM, and problem difficulty. (2) With our compute-optimal TTS strategy, extremely small policy models can outperform larger models. For example, a 1B LLM can exceed a 405B LLM on MATH-500. Moreover, on both MATH-500 and AIME24, a 0.5B LLM outperforms GPT-4o, a 3B LLM surpasses a 405B LLM, and a 7B LLM beats o1 and DeepSeek-R1, while with higher inference efficiency. These findings show the significance of adapting TTS strategies to the specific characteristics of each task and model and indicate that TTS is a promising approach for enhancing the reasoning abilities of LLMs."

[11.02.2025 06:15] Response: ```python
['OPTIMIZATION', 'REASONING']
```
[11.02.2025 06:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates Test-Time Scaling (TTS), a technique that enhances the performance of Large Language Models (LLMs) by adjusting computation during inference. It addresses how different policy models, Process Reward Models (PRMs), and the difficulty of problems affect the effectiveness of TTS. The authors conduct experiments on MATH-500 and AIME24 tasks, revealing that smaller models can outperform larger ones when using an optimal TTS strategy. The results emphasize the importance of tailoring TTS methods to specific models and tasks to improve reasoning capabilities in LLMs.","title":"Unlocking LLM Potential: Small Models, Big Gains with TTS!"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper investigates Test-Time Scaling (TTS), a technique that enhances the performance of Large Language Models (LLMs) by adjusting computation during inference. It addresses how different policy models, Process Reward Models (PRMs), and the difficulty of problems affect the effectiveness of TTS. The authors conduct experiments on MATH-500 and AIME24 tasks, revealing that smaller models can outperform larger ones when using an optimal TTS strategy. The results emphasize the importance of tailoring TTS methods to specific models and tasks to improve reasoning capabilities in LLMs.', title='Unlocking LLM Potential: Small Models, Big Gains with TTS!'))
[11.02.2025 06:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"测试时间扩展（TTS）是一种通过在推理阶段增加计算量来提高大型语言模型（LLMs）性能的方法。本文系统分析了策略模型、过程奖励模型（PRMs）和问题难度如何影响TTS的效果。研究表明，计算最优的TTS策略依赖于所选的策略模型、PRM和问题难度，且小型模型在某些情况下可以超越大型模型。通过在MATH-500和AIME24任务上的实验，我们发现适应特定任务和模型的TTS策略对于提升LLMs的推理能力至关重要。","title":"优化测试时间扩展，提升小型模型性能！"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='测试时间扩展（TTS）是一种通过在推理阶段增加计算量来提高大型语言模型（LLMs）性能的方法。本文系统分析了策略模型、过程奖励模型（PRMs）和问题难度如何影响TTS的效果。研究表明，计算最优的TTS策略依赖于所选的策略模型、PRM和问题难度，且小型模型在某些情况下可以超越大型模型。通过在MATH-500和AIME24任务上的实验，我们发现适应特定任务和模型的TTS策略对于提升LLMs的推理能力至关重要。', title='优化测试时间扩展，提升小型模型性能！'))
[11.02.2025 06:15] Using data from previous issue: {"categories": ["#dataset", "#architecture", "#benchmark", "#interpretability", "#long_context", "#reasoning"], "emoji": "🧠", "ru": {"title": "LM2: Трансформер с памятью для улучшенных рассуждений", "desc": "В статье представлена модель Large Memory Model (LM2), архитектура декодер-трансформер с доп
[11.02.2025 06:15] Querying the API.
[11.02.2025 06:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models -- especially to low precisions like int4 or int2 -- requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve a single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess a nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. This paper proposes Matryoshka Quantization (MatQuant), a novel multi-scale quantization technique that addresses the challenge of needing multiple quantized models. It allows training and maintaining just one model, which can then be served at different precision levels. Furthermore, due to the co-training and co-distillation regularization provided by MatQuant, the int2 precision models extracted by MatQuant can be up to 10% more accurate than standard int2 quantization (using techniques like QAT or OmniQuant). This represents significant progress in model quantization, demonstrated by the fact that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more accurate than an int8 FFN-quantized Gemma-2 2B model.
[11.02.2025 06:15] Response: {
  "desc": "MatQuant - это новый метод многомасштабной квантизации для моделей машинного обучения. Он позволяет обучать и обслуживать одну модель, которую затем можно использовать с разными уровнями точности. Благодаря совместному обучению и ко-дистилляции, модели int2, полученные с помощью MatQuant, могут быть до 10% точнее, чем при стандартной квантизации int2. Этот метод значительно улучшает квантизацию моделей, что демонстрируется тем, что модель Gemma-2 9B с квантизацией FFN до int2 оказывается точнее, чем модель Gemma-2 2B с квантизацией FFN до int8.",
  "emoji": "🪆",
  "title": "MatQuant: Одна модель - множество уровней точности"
}
[11.02.2025 06:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models -- especially to low precisions like int4 or int2 -- requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve a single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess a nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. This paper proposes Matryoshka Quantization (MatQuant), a novel multi-scale quantization technique that addresses the challenge of needing multiple quantized models. It allows training and maintaining just one model, which can then be served at different precision levels. Furthermore, due to the co-training and co-distillation regularization provided by MatQuant, the int2 precision models extracted by MatQuant can be up to 10% more accurate than standard int2 quantization (using techniques like QAT or OmniQuant). This represents significant progress in model quantization, demonstrated by the fact that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more accurate than an int8 FFN-quantized Gemma-2 2B model."

[11.02.2025 06:15] Response: ```python
["INFERENCE", "TRAINING"]
```
[11.02.2025 06:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models -- especially to low precisions like int4 or int2 -- requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve a single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess a nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. This paper proposes Matryoshka Quantization (MatQuant), a novel multi-scale quantization technique that addresses the challenge of needing multiple quantized models. It allows training and maintaining just one model, which can then be served at different precision levels. Furthermore, due to the co-training and co-distillation regularization provided by MatQuant, the int2 precision models extracted by MatQuant can be up to 10% more accurate than standard int2 quantization (using techniques like QAT or OmniQuant). This represents significant progress in model quantization, demonstrated by the fact that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more accurate than an int8 FFN-quantized Gemma-2 2B model."

[11.02.2025 06:15] Response: ```python
["OPTIMIZATION"]
```
[11.02.2025 06:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Matryoshka Quantization (MatQuant), a new technique for quantizing model weights that allows for multiple precision levels without sacrificing model quality. Traditional low-precision quantization, especially to int2, often leads to significant degradation in performance, forcing practitioners to manage several models. MatQuant leverages the nested structure of integer data types to enable a single model to be trained and served at various precision levels. The method also enhances the accuracy of int2 models by up to 10% compared to standard quantization techniques, showcasing its effectiveness in reducing the need for multiple quantized models.","title":"One Model, Multiple Precision: Revolutionizing Quantization with MatQuant"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper introduces Matryoshka Quantization (MatQuant), a new technique for quantizing model weights that allows for multiple precision levels without sacrificing model quality. Traditional low-precision quantization, especially to int2, often leads to significant degradation in performance, forcing practitioners to manage several models. MatQuant leverages the nested structure of integer data types to enable a single model to be trained and served at various precision levels. The method also enhances the accuracy of int2 models by up to 10% compared to standard quantization techniques, showcasing its effectiveness in reducing the need for multiple quantized models.', title='One Model, Multiple Precision: Revolutionizing Quantization with MatQuant'))
[11.02.2025 06:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"量化模型权重对于减少大型模型的通信和推理成本至关重要。然而，将模型量化到低精度（如int4或int2）时，模型质量会受到影响，尤其是int2会显著降低模型性能。为了解决这个问题，本文提出了一种新的多尺度量化技术——Matryoshka量化（MatQuant），它允许只训练和维护一个模型，并在不同精度级别下进行服务。通过MatQuant的共同训练和共同蒸馏正则化，提取的int2精度模型的准确性比标准的int2量化高出10%。","title":"Matryoshka量化：单模型多精度服务的创新"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='量化模型权重对于减少大型模型的通信和推理成本至关重要。然而，将模型量化到低精度（如int4或int2）时，模型质量会受到影响，尤其是int2会显著降低模型性能。为了解决这个问题，本文提出了一种新的多尺度量化技术——Matryoshka量化（MatQuant），它允许只训练和维护一个模型，并在不同精度级别下进行服务。通过MatQuant的共同训练和共同蒸馏正则化，提取的int2精度模型的准确性比标准的int2量化高出10%。', title='Matryoshka量化：单模型多精度服务的创新'))
[11.02.2025 06:15] Querying the API.
[11.02.2025 06:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffusion transformer for personalized video generation from a reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish a new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality.
[11.02.2025 06:15] Response: {
  "desc": "CustomVideoX - это инновационная система для персонализированной генерации видео на основе референсного изображения, использующая видео-диффузионный трансформер. Система применяет предобученные видеосети и обучает только параметры LoRA для извлечения признаков из референса, что обеспечивает эффективность и адаптивность. Предложенное 3D Reference Attention позволяет взаимодействовать признакам референсного изображения со всеми кадрами видео в пространственном и временном измерениях. Для улучшения качества генерации используются стратегии Time-Aware Reference Attention Bias и Entity Region-Aware Enhancement.",
  "emoji": "🎬",
  "title": "CustomVideoX: Персонализированная генерация видео нового уровня"
}
[11.02.2025 06:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffusion transformer for personalized video generation from a reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish a new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality."

[11.02.2025 06:15] Response: ```python
['VIDEO', 'BENCHMARK', '3D']
```
[11.02.2025 06:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffusion transformer for personalized video generation from a reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish a new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality."

[11.02.2025 06:15] Response: ```python
["DIFFUSION"]
```
[11.02.2025 06:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents CustomVideoX, a new framework designed for generating personalized videos from a reference image. It utilizes a video diffusion transformer and focuses on training LoRA parameters to efficiently extract features from the reference image. The framework introduces 3D Reference Attention to enhance the interaction between the reference image and video frames, addressing temporal inconsistencies. Additionally, it employs the Time-Aware Reference Attention Bias and the Entity Region-Aware Enhancement modules to improve video quality and consistency, validated by a new benchmark called VideoBench.","title":"Revolutionizing Personalized Video Generation with CustomVideoX"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper presents CustomVideoX, a new framework designed for generating personalized videos from a reference image. It utilizes a video diffusion transformer and focuses on training LoRA parameters to efficiently extract features from the reference image. The framework introduces 3D Reference Attention to enhance the interaction between the reference image and video frames, addressing temporal inconsistencies. Additionally, it employs the Time-Aware Reference Attention Bias and the Entity Region-Aware Enhancement modules to improve video quality and consistency, validated by a new benchmark called VideoBench.', title='Revolutionizing Personalized Video Generation with CustomVideoX'))
[11.02.2025 06:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"个性化视频生成在图像合成领域取得了显著进展，但由于时间不一致性和质量下降，仍然面临挑战。本文提出了CustomVideoX，一个创新框架，利用视频扩散变换器从参考图像生成个性化视频。CustomVideoX通过专门训练LoRA参数来提取参考特征，确保了效率和适应性。我们还提出了3D参考注意力机制，以便在空间和时间维度上直接和同时地将参考图像特征与所有视频帧进行交互。","title":"个性化视频生成的新突破"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='个性化视频生成在图像合成领域取得了显著进展，但由于时间不一致性和质量下降，仍然面临挑战。本文提出了CustomVideoX，一个创新框架，利用视频扩散变换器从参考图像生成个性化视频。CustomVideoX通过专门训练LoRA参数来提取参考特征，确保了效率和适应性。我们还提出了3D参考注意力机制，以便在空间和时间维度上直接和同时地将参考图像特征与所有视频帧进行交互。', title='个性化视频生成的新突破'))
[11.02.2025 06:15] Using data from previous issue: {"categories": ["#optimization", "#training", "#rlhf", "#dataset", "#diffusion", "#cv"], "emoji": "🖼️", "ru": {"title": "Двойные подписи для улучшения генерации изображений по текстовым описаниям", "desc": "Эта статья описывает новый метод под названием Dual Caption Preference Optimization (DCPO) дл
[11.02.2025 06:15] Using data from previous issue: {"categories": ["#rag", "#optimization", "#inference", "#long_context"], "emoji": "⚡", "ru": {"title": "Ускорение генерации с контекстом: адаптивное параллельное кодирование", "desc": "Статья представляет новый метод адаптивного параллельного кодирования (APE) для эффективной обработки множественных
[11.02.2025 06:15] Querying the API.
[11.02.2025 06:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data. To address this, we introduce Lumina-Video, a framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis. Lumina-Video incorporates a Multi-scale Next-DiT architecture, which jointly learns multiple patchifications to enhance both efficiency and flexibility. By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos' dynamic degree. Combined with a progressive training scheme with increasingly higher resolution and FPS, and a multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency. We additionally propose Lumina-V2A, a video-to-audio model based on Next-DiT, to create synchronized sounds for generated videos. Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.
[11.02.2025 06:15] Response: {
  "desc": "Статья представляет Lumina-Video - новую архитектуру для генерации видео, основанную на Diffusion Transformers (DiT). Авторы предлагают мультимасштабную архитектуру Next-DiT, которая обучается на нескольких уровнях детализации одновременно. Модель использует условие движения для контроля динамики генерируемого видео. Благодаря прогрессивной схеме обучения и использованию смешанных данных, Lumina-Video достигает высокого качества и плавности движения при эффективном обучении и инференсе.",
  "emoji": "🎬",
  "title": "Lumina-Video: новый уровень генерации видео с помощью диффузионных трансформеров"
}
[11.02.2025 06:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data. To address this, we introduce Lumina-Video, a framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis. Lumina-Video incorporates a Multi-scale Next-DiT architecture, which jointly learns multiple patchifications to enhance both efficiency and flexibility. By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos' dynamic degree. Combined with a progressive training scheme with increasingly higher resolution and FPS, and a multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency. We additionally propose Lumina-V2A, a video-to-audio model based on Next-DiT, to create synchronized sounds for generated videos. Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video."

[11.02.2025 06:15] Response: ```python
['VIDEO', 'ARCHITECTURE', 'TRAINING', 'AUDIO']
```
[11.02.2025 06:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data. To address this, we introduce Lumina-Video, a framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis. Lumina-Video incorporates a Multi-scale Next-DiT architecture, which jointly learns multiple patchifications to enhance both efficiency and flexibility. By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos' dynamic degree. Combined with a progressive training scheme with increasingly higher resolution and FPS, and a multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency. We additionally propose Lumina-V2A, a video-to-audio model based on Next-DiT, to create synchronized sounds for generated videos. Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video."

[11.02.2025 06:15] Response: ```python
['DIFFUSION', 'SYNTHETIC']
```
[11.02.2025 06:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Lumina-Video, a new framework for generating videos using the strengths of Diffusion Transformers, specifically the Next-DiT model. It addresses the challenges of capturing the complex movements and changes in video data by using a Multi-scale Next-DiT architecture that learns from different segments of the video. The framework also incorporates a motion score to allow for better control over the dynamics of the generated videos. Additionally, Lumina-Video employs a progressive training approach to improve the quality and smoothness of the output while maintaining efficiency in both training and inference.","title":"Revolutionizing Video Generation with Lumina-Video"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper introduces Lumina-Video, a new framework for generating videos using the strengths of Diffusion Transformers, specifically the Next-DiT model. It addresses the challenges of capturing the complex movements and changes in video data by using a Multi-scale Next-DiT architecture that learns from different segments of the video. The framework also incorporates a motion score to allow for better control over the dynamics of the generated videos. Additionally, Lumina-Video employs a progressive training approach to improve the quality and smoothness of the output while maintaining efficiency in both training and inference.', title='Revolutionizing Video Generation with Lumina-Video'))
[11.02.2025 06:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"最近的研究表明，扩散变换器（DiTs）在生成建模中表现出色。基于这一成功，Lumina-Next在生成逼真图像方面取得了卓越的性能，但在视频生成方面仍面临挑战。为了解决这一问题，我们提出了Lumina-Video框架，它结合了Next-DiT的优势，并针对视频合成引入了定制化的解决方案。通过多尺度Next-DiT架构和运动评分的引入，Lumina-Video实现了高效、灵活的视频生成，并在训练和推理效率上表现出色。","title":"Lumina-Video：高效生成视频的新框架"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='最近的研究表明，扩散变换器（DiTs）在生成建模中表现出色。基于这一成功，Lumina-Next在生成逼真图像方面取得了卓越的性能，但在视频生成方面仍面临挑战。为了解决这一问题，我们提出了Lumina-Video框架，它结合了Next-DiT的优势，并针对视频合成引入了定制化的解决方案。通过多尺度Next-DiT架构和运动评分的引入，Lumina-Video实现了高效、灵活的视频生成，并在训练和推理效率上表现出色。', title='Lumina-Video：高效生成视频的新框架'))
[11.02.2025 06:15] Querying the API.
[11.02.2025 06:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Classifier-free guidance (CFG) is a key technique for improving conditional generation in diffusion models, enabling more accurate control while enhancing sample quality. It is natural to extend this technique to video diffusion, which generates video conditioned on a variable number of context frames, collectively referred to as history. However, we find two key challenges to guiding with variable-length history: architectures that only support fixed-size conditioning, and the empirical observation that CFG-style history dropout performs poorly. To address this, we propose the Diffusion Forcing Transformer (DFoT), a video diffusion architecture and theoretically grounded training objective that jointly enable conditioning on a flexible number of history frames. We then introduce History Guidance, a family of guidance methods uniquely enabled by DFoT. We show that its simplest form, vanilla history guidance, already significantly improves video generation quality and temporal consistency. A more advanced method, history guidance across time and frequency further enhances motion dynamics, enables compositional generalization to out-of-distribution history, and can stably roll out extremely long videos. Website: https://boyuan.space/history-guidance
[11.02.2025 06:15] Response: {
  "desc": "Статья представляет новый подход к улучшению генерации видео с помощью диффузионных моделей. Авторы предлагают архитектуру Diffusion Forcing Transformer (DFoT), которая позволяет использовать переменное количество кадров истории для обусловливания генерации. Они также вводят концепцию History Guidance - семейство методов, которые улучшают качество и временную согласованность генерируемого видео. Эксперименты показывают, что предложенный подход значительно улучшает динамику движения и позволяет генерировать очень длинные видео.",
  "emoji": "🎬",
  "title": "Улучшение генерации видео с помощью гибкого обусловливания историей"
}
[11.02.2025 06:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Classifier-free guidance (CFG) is a key technique for improving conditional generation in diffusion models, enabling more accurate control while enhancing sample quality. It is natural to extend this technique to video diffusion, which generates video conditioned on a variable number of context frames, collectively referred to as history. However, we find two key challenges to guiding with variable-length history: architectures that only support fixed-size conditioning, and the empirical observation that CFG-style history dropout performs poorly. To address this, we propose the Diffusion Forcing Transformer (DFoT), a video diffusion architecture and theoretically grounded training objective that jointly enable conditioning on a flexible number of history frames. We then introduce History Guidance, a family of guidance methods uniquely enabled by DFoT. We show that its simplest form, vanilla history guidance, already significantly improves video generation quality and temporal consistency. A more advanced method, history guidance across time and frequency further enhances motion dynamics, enables compositional generalization to out-of-distribution history, and can stably roll out extremely long videos. Website: https://boyuan.space/history-guidance"

[11.02.2025 06:15] Response: ```python
['VIDEO', 'ARCHITECTURE', 'TRAINING']
```
[11.02.2025 06:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Classifier-free guidance (CFG) is a key technique for improving conditional generation in diffusion models, enabling more accurate control while enhancing sample quality. It is natural to extend this technique to video diffusion, which generates video conditioned on a variable number of context frames, collectively referred to as history. However, we find two key challenges to guiding with variable-length history: architectures that only support fixed-size conditioning, and the empirical observation that CFG-style history dropout performs poorly. To address this, we propose the Diffusion Forcing Transformer (DFoT), a video diffusion architecture and theoretically grounded training objective that jointly enable conditioning on a flexible number of history frames. We then introduce History Guidance, a family of guidance methods uniquely enabled by DFoT. We show that its simplest form, vanilla history guidance, already significantly improves video generation quality and temporal consistency. A more advanced method, history guidance across time and frequency further enhances motion dynamics, enables compositional generalization to out-of-distribution history, and can stably roll out extremely long videos. Website: https://boyuan.space/history-guidance"

[11.02.2025 06:15] Response: ```python
["DIFFUSION"]
```
[11.02.2025 06:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces the Diffusion Forcing Transformer (DFoT), a novel architecture designed for video diffusion that allows for flexible conditioning on a variable number of context frames. The authors address challenges related to fixed-size conditioning architectures and the inefficacy of traditional classifier-free guidance (CFG) methods when applied to variable-length history. They propose History Guidance, a set of techniques that leverage DFoT to improve video generation quality and temporal consistency. The results demonstrate that these methods enhance motion dynamics and enable the generation of longer videos with better compositional generalization.","title":"Enhancing Video Diffusion with Flexible History Guidance"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper introduces the Diffusion Forcing Transformer (DFoT), a novel architecture designed for video diffusion that allows for flexible conditioning on a variable number of context frames. The authors address challenges related to fixed-size conditioning architectures and the inefficacy of traditional classifier-free guidance (CFG) methods when applied to variable-length history. They propose History Guidance, a set of techniques that leverage DFoT to improve video generation quality and temporal consistency. The results demonstrate that these methods enhance motion dynamics and enable the generation of longer videos with better compositional generalization.', title='Enhancing Video Diffusion with Flexible History Guidance'))
[11.02.2025 06:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文提出了一种新的视频扩散模型架构，称为Diffusion Forcing Transformer（DFoT），旨在解决在可变长度历史帧条件下进行视频生成的挑战。我们发现，传统的分类器无关引导（CFG）方法在处理可变长度历史时效果不佳，因此我们设计了新的引导方法，称为历史引导。DFoT允许灵活地使用历史帧进行条件生成，从而显著提高视频生成的质量和时间一致性。通过引入更高级的历史引导方法，我们进一步增强了运动动态，并实现了对超出分布历史的组合泛化。","title":"灵活历史引导，提升视频生成质量"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='本论文提出了一种新的视频扩散模型架构，称为Diffusion Forcing Transformer（DFoT），旨在解决在可变长度历史帧条件下进行视频生成的挑战。我们发现，传统的分类器无关引导（CFG）方法在处理可变长度历史时效果不佳，因此我们设计了新的引导方法，称为历史引导。DFoT允许灵活地使用历史帧进行条件生成，从而显著提高视频生成的质量和时间一致性。通过引入更高级的历史引导方法，我们进一步增强了运动动态，并实现了对超出分布历史的组合泛化。', title='灵活历史引导，提升视频生成质量'))
[11.02.2025 06:16] Loading Chinese text from previous data.
[11.02.2025 06:16] Renaming data file.
[11.02.2025 06:16] Renaming previous data. hf_papers.json to ./d/2025-02-11.json
[11.02.2025 06:16] Saving new data file.
[11.02.2025 06:16] Generating page.
[11.02.2025 06:16] Renaming previous page.
[11.02.2025 06:16] Renaming previous data. index.html to ./d/2025-02-11.html
[11.02.2025 06:16] [Experimental] Generating Chinese page for reading.
[11.02.2025 06:16] Chinese vocab [{'word': '注意力', 'pinyin': 'zhùyìlì', 'trans': 'attention'}, {'word': '机制', 'pinyin': 'jīzhì', 'trans': 'mechanism'}, {'word': '滑动', 'pinyin': 'huádòng', 'trans': 'sliding'}, {'word': '磁贴', 'pinyin': 'cítiē', 'trans': 'magnetic tile'}, {'word': '扩散', 'pinyin': 'kuòsàn', 'trans': 'diffusion'}, {'word': '变压器', 'pinyin': 'biànyāqì', 'trans': 'transformer'}, {'word': '计算', 'pinyin': 'jìsuàn', 'trans': 'computation'}, {'word': '成本', 'pinyin': 'chéngběn', 'trans': 'cost'}, {'word': '传统', 'pinyin': 'chuántǒng', 'trans': 'traditional'}, {'word': '全', 'pinyin': 'quán', 'trans': 'full'}, {'word': '耗时', 'pinyin': 'hàoshí', 'trans': 'time-consuming'}, {'word': '极长', 'pinyin': 'jícháng', 'trans': 'extremely long'}, {'word': '局部', 'pinyin': 'júbù', 'trans': 'local'}, {'word': '3D', 'pinyin': '', 'trans': '3D'}, {'word': '窗口', 'pinyin': 'chuāngkǒu', 'trans': 'window'}, {'word': '冗余', 'pinyin': 'rǒngyú', 'trans': 'redundancy'}, {'word': '硬件', 'pinyin': 'yìngjiàn', 'trans': 'hardware'}, {'word': '感知', 'pinyin': 'gǎnzhī', 'trans': 'perception'}, {'word': '设计', 'pinyin': 'shèjì', 'trans': 'design'}, {'word': '效率', 'pinyin': 'xiàolǜ', 'trans': 'efficiency'}, {'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'}, {'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'display'}, {'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'}, {'word': '减少', 'pinyin': 'jiǎnshǎo', 'trans': 'reduce'}, {'word': '延迟', 'pinyin': 'yánchí', 'trans': 'latency'}]
[11.02.2025 06:16] Renaming previous Chinese page.
[11.02.2025 06:16] Renaming previous data. zh.html to ./d/2025-02-10_zh_reading_task.html
[11.02.2025 06:16] Writing Chinese reading task.
[11.02.2025 06:16] Writing result.
[11.02.2025 06:16] Renaming log file.
[11.02.2025 06:16] Renaming previous data. log.txt to ./logs/2025-02-11_last_log.txt
