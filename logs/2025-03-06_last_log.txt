[06.03.2025 04:13] Read previous papers.
[06.03.2025 04:13] Generating top page (month).
[06.03.2025 04:13] Writing top page (month).
[06.03.2025 05:11] Read previous papers.
[06.03.2025 05:11] Get feed.
[06.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00865
[06.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00329
[06.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02951
[06.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.03751
[06.03.2025 05:11] Extract page data from URL. URL: https://huggingface.co/papers/2503.01449
[06.03.2025 05:11] Extract page data from URL. URL: https://huggingface.co/papers/2503.01378
[06.03.2025 05:11] Extract page data from URL. URL: https://huggingface.co/papers/2503.01372
[06.03.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00502
[06.03.2025 05:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.03.2025 05:11] No deleted papers detected.
[06.03.2025 05:11] Downloading and parsing papers (pdf, html). Total: 8.
[06.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.00865.
[06.03.2025 05:11] Extra JSON file exists (./assets/json/2503.00865.json), skip PDF parsing.
[06.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.00865.json), skip HTML parsing.
[06.03.2025 05:11] Success.
[06.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.00329.
[06.03.2025 05:11] Extra JSON file exists (./assets/json/2503.00329.json), skip PDF parsing.
[06.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.00329.json), skip HTML parsing.
[06.03.2025 05:11] Success.
[06.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.02951.
[06.03.2025 05:11] Extra JSON file exists (./assets/json/2503.02951.json), skip PDF parsing.
[06.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.02951.json), skip HTML parsing.
[06.03.2025 05:11] Success.
[06.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.03751.
[06.03.2025 05:11] Extra JSON file exists (./assets/json/2503.03751.json), skip PDF parsing.
[06.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.03751.json), skip HTML parsing.
[06.03.2025 05:11] Success.
[06.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.01449.
[06.03.2025 05:11] Downloading paper 2503.01449 from http://arxiv.org/pdf/2503.01449v1...
[06.03.2025 05:11] Extracting affiliations from text.
[06.03.2025 05:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection Ting Zhang1,*, Chengran Yang1,*, Yindu Su1, Martin Weyssow1, Hung Nguyen1, Tan Bui1, Hong Jin Kang2, Yikun Li1, Eng Lieh Ouh1, Lwin Khin Shar1, David Lo1 1School of Computing and Information Systems, Singapore Management University, Singapore Email: {tingzhang.2019, cryang, yindusu, mweyssow, huuhungn, ngoctanbui, yikunli, elouh, lkshar, davidlo}@smu.edu.sg 2School of Computer Science, University of Sydney, Australia Email: hongjin.kang@sydney.edu.au . *Both authors contributed equally to this research. "
[06.03.2025 05:11] Response: ```python
[
    "School of Computing and Information Systems, Singapore Management University, Singapore",
    "School of Computer Science, University of Sydney, Australia"
]
```
[06.03.2025 05:11] Deleting PDF ./assets/pdf/2503.01449.pdf.
[06.03.2025 05:11] Success.
[06.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.01378.
[06.03.2025 05:11] Downloading paper 2503.01378 from http://arxiv.org/pdf/2503.01378v1...
[06.03.2025 05:11] Extracting affiliations from text.
[06.03.2025 05:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CognitiveDrone: VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs Artem Lykov, Valerii Serpiva, Muhammad Haris Khan, Oleg Sautenkov, Artyom Myshlyaev, Grik Tadevosyan, Yasheerah Yaqoot, and Dzmitry Tsetserukou 5 2 0 2 3 ] . [ 1 8 7 3 1 0 . 3 0 5 2 : r Abstract This paper introduces CognitiveDrone, novel tailored for complex Vision-Language-Action (VLA) model Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on dataset comprising over 8,000 simulated flight trajectories across three key categoriesHuman Recognition, Symbol Understanding, and Reasoningthe model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional VisionLanguage Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while racing-oriented model (RaceVLA) achieves an overall success rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and CognitiveDrone-R1 attains success rate of 77.2%. These results demonstrate improvements of up to 30% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of stateof-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at https://cognitivedrone.github.io. I. INTRODUCTION In an era marked by rapid advancements in robotics and artificial intelligence, enabling robots to perform wide range of complex tasks in dynamically changing environments has emerged as critical challenge. Cognitive robotics strives not only to endow machines with precise control but also t"
[06.03.2025 05:11] Response: ```python
[]
```
[06.03.2025 05:11] Extracting affiliations from text.
[06.03.2025 05:11] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CognitiveDrone: VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs Artem Lykov, Valerii Serpiva, Muhammad Haris Khan, Oleg Sautenkov, Artyom Myshlyaev, Grik Tadevosyan, Yasheerah Yaqoot, and Dzmitry Tsetserukou 5 2 0 2 3 ] . [ 1 8 7 3 1 0 . 3 0 5 2 : r Abstract This paper introduces CognitiveDrone, novel tailored for complex Vision-Language-Action (VLA) model Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on dataset comprising over 8,000 simulated flight trajectories across three key categoriesHuman Recognition, Symbol Understanding, and Reasoningthe model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional VisionLanguage Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while racing-oriented model (RaceVLA) achieves an overall success rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and CognitiveDrone-R1 attains success rate of 77.2%. These results demonstrate improvements of up to 30% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of stateof-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at https://cognitivedrone.github.io. I. INTRODUCTION In an era marked by rapid advancements in robotics and artificial intelligence, enabling robots to perform wide range of complex tasks in dynamically changing environments has emerged as critical challenge. Cognitive robotics strives not only to endow machines with precise control but also to equip them with high-level reasoning and decisionmaking capabilities that allow them to adapt to unpredictable real-world scenarios. Despite notable progress in various domains of robotics, one persistent challenge remains: how to objectively evaluate and compare cognitive robotic systems, particularly when they are expected to tackle multitude of intricate tasks. The scarcity of standardized, open-source benchmarks and datasets is especially evident in the realm of Unmanned Aerial Vehicles (UAVs), where existing evaluation frameworks are largely confined to racing or basic navigation tasks. This limitation not only hinders fair comparison among different cognitive UAV systems but also restricts the exThese authors contributed equally to this work. The authors are with the Intelligent Space Robotics Laboratory, Science Center {Artem.Lykov, Valerii.Serpiva, and haris.khan, Artyom.Myshlyaev, oleg.sautenkov, grik.tadevosyan, yasheerah.yaqoot, d.tsetserukou}@skoltech.ru for Digital Engineering, Technology. Skolkovo Institute of Fig. 1. CognitiveDrone is VLA system for UAVs that generates smooth 4D control commands from first-person visual inputs and natural language instructions. It combines 7B-parameter VLA model trained on an extensive open-source dataset of cognitive tasksincluding reasoning, human recognition, and symbol understandingwith 7B-parameter VLM reasoning module that refines task directives. The system is evaluated within CognitiveDroneBenchthe first evaluation benchmark for VLA systems tailored to cognitive UAVswhere the drone must navigate track with gates by selecting the appropriate gate through solving cognitive tasks. We have released the complete dataset, benchmark environment, model weights, and training/inference code as open source. ploration of more sophisticated cognitive functions such as reasoning, human recognition, and symbolic understanding. To address these challenges, we introduce CognitiveDrone novel VLA model designed for real-time cognitive task solving and reasoning in UAVs. In conjunction with the model, we propose CognitiveDroneBench, an open-source benchmark built upon Gazebo-based physical simulation environment that integrates drone racing track with cognitive checkpoints. At each stage of the track, the UAV is required to select specific gates by solving cognitive task, thus providing comprehensive performance evaluation that transcends traditional racing metrics. Furthermore, we augment our system with an auxiliary reasoning module based on the VLM model Qwen2.5VL, yielding the CognitiveDrone-R1 variant. This reasoning module, operating at lower frequency than the primary VLA component, is intended to enhance task comprehension and facilitate more robust decision-making. By seamlessly integrating these components, our work paves the way for more rigorous evaluations and innovative applications in cognitive UAV research. II. RELATED WORKS substantial body of research has focused on developing cognitive systems for robotics, where the integration of visual perception, language understanding, and action planning is paramount. For robotic manipulators, state-of-the-art models such as PaLM-E [1], RT-1 [2], RT-2 [3], and RT-X [4] have significantly advanced the field by incorporating VLA frameworks that enable these robots to handle complex manipulation tasks in dynamic environments. Complementary to these efforts, VLA-based systems like OpenVLA [5], MiniVLA [6], and Octo [7] have been developed for diverse robotic platforms, extending the application spectrum of cognitive robotics. The evaluation of such systems has been further supported by simulation benchmarks. For instance, LIBERO [8] provides simulation environment that standardizes the assessment of cognitive performance in robotic manipulators. In parallel, for robots aimed at human-robot collaboration and domestic assistance including humanoids such as Tesla Optimus, Agility Robotics [9], and FIGURE, as well as quadrupedal platforms like CognitiveDog [10] and DoggyBot [11] the open-source PARTNR benchmark [12] has been introduced to facilitate objective comparisons. Within the realm of cognitive UAVs, various transformerbased and VLA approaches have been explored. UAVs Meet LLMs [13] presents comprehensive overview of such methods, highlighting applications in drone navigation [14], [15], [16], [17], [18], flight control [19], [20], and mission planning [21], [22]. Additionally, models for drone swarm control, such as Swar"
[06.03.2025 05:11] Mistral response. {"id": "15e0cdc36abb4a948a799ba2e39f45c0", "object": "chat.completion", "created": 1741237899, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Intelligent Space Robotics Laboratory, Science Center for Digital Engineering, Technology. Skolkovo Institute\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1594, "total_tokens": 1624, "completion_tokens": 30}}
[06.03.2025 05:11] Response: ```python
["Intelligent Space Robotics Laboratory, Science Center for Digital Engineering, Technology. Skolkovo Institute"]
```
[06.03.2025 05:11] Deleting PDF ./assets/pdf/2503.01378.pdf.
[06.03.2025 05:11] Success.
[06.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.01372.
[06.03.2025 05:11] Downloading paper 2503.01372 from http://arxiv.org/pdf/2503.01372v1...
[06.03.2025 05:11] Extracting affiliations from text.
[06.03.2025 05:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SwiLTra-Bench: The Swiss Legal Translation Benchmark Joel NiklausH Jakob MeraneE,L,M Luka Nenadic Sina Ahmadi Yingqiang Gao Cyrill A. H. Chevalley Claude Humbel Christophe G√∂sken Lorenzo Tanzi Thomas L√ºthi Stefan Palombo Spencer Poff Boling Yang Nan Wu Matthew Guillod Robin Mami√© Daniel Brunner C HHarvey EETH Zurich CSwiss Federal Supreme Court ZUniversity of Zurich BUniversity of Basel GUniversity of Geneva LUniversity of Lausanne SCanton of Solothurn Max Planck Institute for Research on Collective Goods 5 2 0 2 M 3 ] . [ 1 2 7 3 1 0 . 3 0 5 2 : r a "
[06.03.2025 05:11] Response: ```python
[
    "ETH Zurich",
    "Swiss Federal Supreme Court",
    "University of Zurich",
    "University of Basel",
    "University of Geneva",
    "University of Lausanne",
    "Canton of Solothurn",
    "Max Planck Institute for Research on Collective Goods"
]
```
[06.03.2025 05:11] Deleting PDF ./assets/pdf/2503.01372.pdf.
[06.03.2025 05:11] Success.
[06.03.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2503.00502.
[06.03.2025 05:11] Extra JSON file exists (./assets/json/2503.00502.json), skip PDF parsing.
[06.03.2025 05:11] Paper image links file exists (./assets/img_data/2503.00502.json), skip HTML parsing.
[06.03.2025 05:11] Success.
[06.03.2025 05:11] Enriching papers with extra data.
[06.03.2025 05:11] ********************************************************************************
[06.03.2025 05:11] Abstract 0. Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced languages, while widely spoken but under-resourced languages a...
[06.03.2025 05:11] ********************************************************************************
[06.03.2025 05:11] Abstract 1. Visual embedding models excel at zero-shot tasks like visual retrieval and classification. However, these models cannot be used for tasks that contain ambiguity or require user instruction. These tasks necessitate a multimodal embedding model, which outputs embeddings that combine visual and natural...
[06.03.2025 05:11] ********************************************************************************
[06.03.2025 05:11] Abstract 2. We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of ...
[06.03.2025 05:11] ********************************************************************************
[06.03.2025 05:11] Abstract 3. We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if i...
[06.03.2025 05:11] ********************************************************************************
[06.03.2025 05:11] Abstract 4. Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspec...
[06.03.2025 05:11] ********************************************************************************
[06.03.2025 05:11] Abstract 5. This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition...
[06.03.2025 05:11] ********************************************************************************
[06.03.2025 05:11] Abstract 6. In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and im...
[06.03.2025 05:11] ********************************************************************************
[06.03.2025 05:11] Abstract 7. Autonomous Vehicles (AVs) have entered the commercialization stage, but their limited ability to interact and express intentions still poses challenges in interactions with Human-driven Vehicles (HVs). Recent advances in large language models (LLMs) enable bidirectional human-machine communication, ...
[06.03.2025 05:11] Read previous papers.
[06.03.2025 05:11] Generating reviews via LLM API.
[06.03.2025 05:11] Using data from previous issue: {"categories": ["#low_resource", "#architecture", "#open_source", "#training", "#multilingual"], "emoji": "üåç", "ru": {"title": "Babel: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å Babel, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∞—è 25 —Å–∞–º—ã—Ö —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –º–∏—Ä–∞. –ú–æ–¥–µ
[06.03.2025 05:11] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#open_source", "#dataset"], "emoji": "üß†", "ru": {"title": "ABC: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è —Å –≥–∏–±–∫–∏–º —è–∑—ã–∫–æ–≤—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ABC, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥
[06.03.2025 05:11] Using data from previous issue: {"categories": ["#dataset", "#rl", "#optimization", "#synthetic", "#training"], "emoji": "üßë‚Äçüíª", "ru": {"title": "KodCode: –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "KodCode - —ç—Ç–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é. –û–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä
[06.03.2025 05:11] Using data from previous issue: {"categories": ["#3d", "#video"], "emoji": "üé•", "ru": {"title": "–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞–º–µ—Ä—ã –∏ 3D-—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "GEN3C - —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –≤–∏–¥–µ–æ —Å —Ç–æ—á–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞–º–µ—Ä—ã –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π 3D-—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 3D-–∫—ç—à –≤ –≤–∏–¥–µ –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ –≥–ª—É–±–∏–Ω–Ω
[06.03.2025 05:11] Querying the API.
[06.03.2025 05:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices.
[06.03.2025 05:11] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[06.03.2025 05:11] Querying the API.
[06.03.2025 05:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition, Symbol Understanding, and Reasoning-the model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while a racing-oriented model (RaceVLA) achieves an overall success rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and CognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate improvements of up to 30% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of a state-of-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at cognitivedrone.github.io
[06.03.2025 05:12] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[06.03.2025 05:12] Querying the API.
[06.03.2025 05:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.
[06.03.2025 05:12] Error getting data: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}
[06.03.2025 05:12] Using data from previous issue: {"categories": ["#rl", "#robotics", "#inference", "#optimization", "#agents", "#reasoning"], "emoji": "üöó", "ru": {"title": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∏ –æ–±—ã—á–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Actor-Reasoner –¥–ª—è —É–ª—É—á
[06.03.2025 05:12] Loading Chinese text from previous data.
[06.03.2025 05:12] Renaming data file.
[06.03.2025 05:12] Renaming previous data. hf_papers.json to ./d/2025-03-06.json
[06.03.2025 05:12] Saving new data file.
[06.03.2025 05:12] Generating page.
[06.03.2025 05:12] Renaming previous page.
[06.03.2025 05:12] Renaming previous data. index.html to ./d/2025-03-06.html
[06.03.2025 05:12] [Experimental] Generating Chinese page for reading.
[06.03.2025 05:12] Chinese vocab [{'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«îy√°n m√≥x√≠ng', 'trans': 'large language model'}, {'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´y√∫', 'trans': 'based on'}, {'word': '‰ª£ÁêÜ', 'pinyin': 'd√†il«ê', 'trans': 'agent'}, {'word': '‰∫íÂä®Âºè', 'pinyin': 'h√πd√≤ngsh√¨', 'trans': 'interactive'}, {'word': 'ËßÑÂàí', 'pinyin': 'guƒ´hu√†', 'trans': 'planning'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®nw√π', 'trans': 'task'}, {'word': 'ÂπªËßâ', 'pinyin': 'hu√†nju√©', 'trans': 'hallucination'}, {'word': 'Âõ∞Êâ∞', 'pinyin': 'k√πnr«éo', 'trans': 'trouble'}, {'word': 'ÈáçÊñ∞', 'pinyin': 'ch√≥ngxƒ´n', 'trans': 'renew'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πnli√†n', 'trans': 'training'}, {'word': 'ÊåëÊàò', 'pinyin': 'ti«éozh√†n', 'trans': 'challenge'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ch≈´', 'trans': 'propose'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'ÊòæÂºè', 'pinyin': 'xi«énsh√¨', 'trans': 'explicit'}, {'word': 'ÊåáÂØº', 'pinyin': 'zh«êd«éo', 'trans': 'guidance'}, {'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìngqi√°ng', 'trans': 'enhance'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability'}, {'word': 'Âà©Áî®', 'pinyin': 'l√¨y√≤ng', 'trans': 'utilize'}, {'word': 'Êèê‰æõ', 'pinyin': 't√≠g≈çng', 'trans': 'provide'}, {'word': 'È´òÂ±ÇÊ¨°', 'pinyin': 'gƒÅo c√©ngc√¨', 'trans': 'high-level'}, {'word': 'ÈÄöÁî®', 'pinyin': 't≈çngy√≤ng', 'trans': 'general'}, {'word': 'Â∏ÆÂä©', 'pinyin': 'bƒÅngzh√π', 'trans': 'help'}, {'word': 'ÊâßË°å', 'pinyin': 'zh√≠x√≠ng', 'trans': 'execute'}, {'word': 'ÂèçÈ¶à', 'pinyin': 'f«énku√¨', 'trans': 'feedback'}, {'word': 'ÊåÅÁª≠', 'pinyin': 'ch√≠x√π', 'trans': 'continuous'}, {'word': '‰ºòÂåñ', 'pinyin': 'y≈çuhu√†', 'trans': 'optimize'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'}, {'word': 'Ë°®Êòé', 'pinyin': 'bi«éom√≠ng', 'trans': 'indicate'}, {'word': '‰ª£Ë°®ÊÄß', 'pinyin': 'd√†ibi«éox√¨ng', 'trans': 'representative'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'}, {'word': '‰ºò‰∫é', 'pinyin': 'y≈çuy√∫', 'trans': 'superior to'}, {'word': 'Áé∞Êúâ', 'pinyin': 'xi√†ny«íu', 'trans': 'existing'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´zh«în', 'trans': 'benchmark'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'}, {'word': 'ÂÆåÊàê', 'pinyin': 'w√°nch√©ng', 'trans': 'complete'}, {'word': 'ÊïàÁéá', 'pinyin': 'xi√†ol«ú', 'trans': 'efficiency'}, {'word': 'Ê≥õÂåñ', 'pinyin': 'f√†nhu√†', 'trans': 'generalize'}]
[06.03.2025 05:12] Renaming previous Chinese page.
[06.03.2025 05:12] Renaming previous data. zh.html to ./d/2025-03-05_zh_reading_task.html
[06.03.2025 05:12] Writing Chinese reading task.
[06.03.2025 05:12] Writing result.
[06.03.2025 05:12] Renaming log file.
[06.03.2025 05:12] Renaming previous data. log.txt to ./logs/2025-03-06_last_log.txt
