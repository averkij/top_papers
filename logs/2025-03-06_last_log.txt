[06.03.2025 21:10] Read previous papers.
[06.03.2025 21:10] Generating top page (month).
[06.03.2025 21:10] Writing top page (month).
[06.03.2025 22:09] Read previous papers.
[06.03.2025 22:09] Get feed.
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00865
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.03746
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00329
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.03751
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02951
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.03278
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01836
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01933
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.20317
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.03044
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00307
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18860
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01763
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01729
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01449
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01378
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01372
[06.03.2025 22:09] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00502
[06.03.2025 22:09] Extract page data from URL. URL: https://huggingface.co/papers/2503.02954
[06.03.2025 22:09] Extract page data from URL. URL: https://huggingface.co/papers/2503.02924
[06.03.2025 22:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.03.2025 22:09] No deleted papers detected.
[06.03.2025 22:09] Downloading and parsing papers (pdf, html). Total: 20.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.00865.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.00865.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.00865.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.03746.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.03746.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.03746.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.00329.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.00329.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.00329.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.03751.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.03751.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.03751.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.02951.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.02951.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.02951.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.03278.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.03278.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.03278.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.01836.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.01836.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.01836.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.01933.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.01933.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.01933.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2502.20317.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2502.20317.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2502.20317.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.03044.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.03044.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.03044.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.00307.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.00307.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.00307.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2502.18860.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2502.18860.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2502.18860.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.01763.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.01763.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.01763.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.01729.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.01729.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.01729.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.01449.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.01449.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.01449.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.01378.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.01378.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.01378.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.01372.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.01372.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.01372.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.00502.
[06.03.2025 22:09] Extra JSON file exists (./assets/json/2503.00502.json), skip PDF parsing.
[06.03.2025 22:09] Paper image links file exists (./assets/img_data/2503.00502.json), skip HTML parsing.
[06.03.2025 22:09] Success.
[06.03.2025 22:09] Downloading and parsing paper https://huggingface.co/papers/2503.02954.
[06.03.2025 22:09] Downloading paper 2503.02954 from http://arxiv.org/pdf/2503.02954v1...
[06.03.2025 22:09] Extracting affiliations from text.
[06.03.2025 22:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders Yue Meng1, Nathalie Majcherczyk2, Wenliang Liu2, Scott Kiesel2, Chuchu Fan1 and Federico Pecora2 5 2 0 2 4 ] . [ 1 4 5 9 2 0 . 3 0 5 2 : r Abstract Multi-agent coordination is crucial for reliable multi-robot navigation in shared spaces such as automated warehouses. In regions of dense robot traffic, local coordination methods may fail to find deadlock-free solution. In these scenarios, it is appropriate to let central unit generate global schedule that decides the passing order of robots. However, the runtime of such centralized coordination methods increases significantly with the problem scale. In this paper, we propose to leverage Graph Neural Network Variational Autoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale faster than through centralized optimization. We formulate the coordination problem as graph problem and collect ground truth data using Mixed-Integer Linear Program (MILP) solver. During training, our learning framework encodes good quality solutions of the graph problem into latent space. At inference time, solution samples are decoded from the sampled latent variables, and the lowest-cost sample is selected for coordination. By construction, our GNN-VAE framework returns solutions that always respect the constraints of the considered coordination problem. Numerical results show that our approach trained on small-scale problems can achieve high-quality solutions even for large-scale problems with 250 robots, being much faster than other baselines. I. INTRODUCTION Multi-agent coordination is essential to ensure that fleet of robots can navigate shared spaces, such as warehouse floors [33] and public roads [1]. Effective coordination avoids collisions, reduces delays, and optimizes resource usage. Coordination between robots can either be achieved implicitly by each robot acting to avoid conflicts based on its local information, o"
[06.03.2025 22:09] Response: ```python
[]
```
[06.03.2025 22:09] Extracting affiliations from text.
[06.03.2025 22:09] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders Yue Meng1, Nathalie Majcherczyk2, Wenliang Liu2, Scott Kiesel2, Chuchu Fan1 and Federico Pecora2 5 2 0 2 4 ] . [ 1 4 5 9 2 0 . 3 0 5 2 : r Abstract Multi-agent coordination is crucial for reliable multi-robot navigation in shared spaces such as automated warehouses. In regions of dense robot traffic, local coordination methods may fail to find deadlock-free solution. In these scenarios, it is appropriate to let central unit generate global schedule that decides the passing order of robots. However, the runtime of such centralized coordination methods increases significantly with the problem scale. In this paper, we propose to leverage Graph Neural Network Variational Autoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale faster than through centralized optimization. We formulate the coordination problem as graph problem and collect ground truth data using Mixed-Integer Linear Program (MILP) solver. During training, our learning framework encodes good quality solutions of the graph problem into latent space. At inference time, solution samples are decoded from the sampled latent variables, and the lowest-cost sample is selected for coordination. By construction, our GNN-VAE framework returns solutions that always respect the constraints of the considered coordination problem. Numerical results show that our approach trained on small-scale problems can achieve high-quality solutions even for large-scale problems with 250 robots, being much faster than other baselines. I. INTRODUCTION Multi-agent coordination is essential to ensure that fleet of robots can navigate shared spaces, such as warehouse floors [33] and public roads [1]. Effective coordination avoids collisions, reduces delays, and optimizes resource usage. Coordination between robots can either be achieved implicitly by each robot acting to avoid conflicts based on its local information, or explicitly via distributed or centralized decision-making. The former category of methods implies pre-determined mutual understanding between robots (e.g. set of rules or reciprocal policies). Their myopic nature is ill-suited for solving complex scenarios with many agents. The latter methods can plan ahead to optimize fleet operation, allowing robots to achieve common goals safely and efficiently in challenging settings. However, existing explicit coordination methods face fundamental trade-off between optimality and computational tractability, particularly as the number of robots increases or the task objectives become more complex. While heuristicbased methods [9] and sampling-based methods [27] are fast in computation, they often struggle to provide high-quality *This research was done during Yues internship at Amazon. Project page: https://mengyuest.github.io/gnn-vae-coord/ 1Yue Meng and Chuchu the Massachusetts of Technology, Cambridge, MA 02139 USA. Email: are with Fan Institute {mengyue,chuchu}@mit.edu 2Authors are with Amazon Robotics, North Reading, MA USA. Email: {majcherc,liuwll,skkiesel,fpecora}@amazon.com solutions for large graphs and require carefully crafted designs tailored to specific objectives. On the other hand, exact methods such as optimization-based approaches [22] and search-based methods [23] can deliver better quality results, but their exponential complexity makes them impractical for large-scale problems. In light of the recent advances in deep generative models [21], we leverage Graph Neural Networks (GNN) and Variational Autoencoders (VAE) to learn the distribution of the high-quality solutions for multi-agent coordination problems. This approach offers several key advantages: (i) GNN are well-suited for embedding the inherent graphical structure of multi-agent coordination problems, enabling them to capture complex interactions among robots. (ii) VAE incorporate uncertainties in the problem, opening the possibility to generate multiple candidate solutions. (iii) Neural Networks are efficient in evaluation, leveraging GPU parallel computation for faster performance, and (iv) deep generative models based on graphs can generalize effectively to larger-scale problems. In this paper, we propose GNN-VAE based framework to achieve reliable and efficient multi-agent coordination. Framing the multi-agent coordination problem as an optimization problem on graph, we collect optimal solutions using Mixed-Integer Linear Program (MILP) solver. During training, the GNN-VAE encodes these solutions into latent space. At the inference stage, latent embeddings are sampled from the latent space and are further decoded to the solution samples, with the solution sample having the lowest cost selected for the coordination problem. Rather than predicting pure solution labels, GNN-VAE learns node ranks and edge modes in semi-supervised manner to construct the solution, ensuring the prediction satisfies formal constraints of the coordination problem. Our contributions can be summarized as follows: 1) We propose novel learning framework that utilizes GNN-VAEs to tackle the particular application of multi-agent navigation in shared space, leveraging the generative nature of the model to sample from the set of feasible problem solutions. 2) We propose two-branch learning framework that guarantees, by construction, the satisfaction of two types of constraints of the coordination problem when inferring solutions. 3) We perform an extensive evaluation of our approach, benchmarking it against strong baselines for problems involving up to 250 robots. II. RELATED WORK This paper considers centralized, explicit coordination problems, which belong to resource-constrained project scheduling problems (RCPSP) [29] known to be NPhard [2]. Related work can be divided into heuristic-based methods [6], optimization-based methods [22], search-based methods [23] and sampling-based methods [34]. An extensive comparison in [30] shows that meta-heuristic methods such as Tabu search outperforms other algorithms, and optimization-based methods work well on small-scale problems. Our approach does not require handcrafted heuristics designs, nor does it require time-consuming search or optimization processes. Instead, our method is akin to the sampling-based methods as it learns the underlying solution distribution from the demonstrated data, enabling it to scale and generalize to large-scale unseen scenarios. [35], to new scenarios [2"
[06.03.2025 22:10] Mistral response. {"id": "277a75dd863247fa9d799a037cbcfa55", "object": "chat.completion", "created": 1741298999, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Massachusetts Institute of Technology, Cambridge, MA 02139 USA\", \"Amazon Robotics, North Reading, MA USA\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1524, "total_tokens": 1561, "completion_tokens": 37}}
[06.03.2025 22:10] Response: ```python
["Massachusetts Institute of Technology, Cambridge, MA 02139 USA", "Amazon Robotics, North Reading, MA USA"]
```
[06.03.2025 22:10] Deleting PDF ./assets/pdf/2503.02954.pdf.
[06.03.2025 22:10] Success.
[06.03.2025 22:10] Downloading and parsing paper https://huggingface.co/papers/2503.02924.
[06.03.2025 22:10] Downloading paper 2503.02924 from http://arxiv.org/pdf/2503.02924v1...
[06.03.2025 22:10] Extracting affiliations from text.
[06.03.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JULY Yue Meng1 and Chuchu Fan1, Member, IEEE 5 2 0 2 4 ] . [ 1 4 2 9 2 0 . 3 0 5 2 : r AbstractGenerating realistic simulations is critical for autonomous system applications such as self-driving and humanrobot interactions. However, driving simulators nowadays still have difficulty in generating controllable, diverse, and rulecompliant behaviors for road participants: Rule-based models cannot produce diverse behaviors and require careful tuning, whereas learning-based methods imitate the policy from data but are not designed to follow the rules explicitly. Besides, the real-world datasets are by nature single-outcome, making the learning method hard to generate diverse behaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion Models to learn controllable, diverse, and rule-aware policy. We first calibrate the STL on the real-world data, then generate diverse synthetic data using trajectory optimization, and finally learn the rectified diffusion policy on the augmented dataset. We test on the NuScenes dataset and our approach can achieve the most diverse rule-compliant trajectories compared to other baselines, with runtime 1/17X to the second-best approach. In the closed-loop testing, our approach reaches the highest diversity, rule satisfaction rate, and the least collision rate. Our method can generate varied characteristics conditional on different STL parameters in testing. case study on human-robot encounter scenarios shows our approach can generate diverse and closed-to-oracle trajectories. The annotation tool, augmented dataset, and code are available at https://github.com/mengyuest/pSTL-diffusion-policy. Index TermsAutonomous Agents; Autonomous Vehicle Navigation; Machine Learning for Robot Control I. INTRODUCTION simulators and studying intelligent systems such as autonomous driving and warehouse ground robots [1], [2]. To close the sim-to-real gap for the agents, is crit"
[06.03.2025 22:10] Response: ```python
[]
```
[06.03.2025 22:10] Extracting affiliations from text.
[06.03.2025 22:10] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JULYYue Meng1 and Chuchu Fan1, Member, IEEE 5 2 0 2 4 ] . [ 1 4 2 9 2 0 . 3 0 5 2 : r AbstractGenerating realistic simulations is critical for autonomous system applications such as self-driving and humanrobot interactions. However, driving simulators nowadays still have difficulty in generating controllable, diverse, and rulecompliant behaviors for road participants: Rule-based models cannot produce diverse behaviors and require careful tuning, whereas learning-based methods imitate the policy from data but are not designed to follow the rules explicitly. Besides, the real-world datasets are by nature single-outcome, making the learning method hard to generate diverse behaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion Models to learn controllable, diverse, and rule-aware policy. We first calibrate the STL on the real-world data, then generate diverse synthetic data using trajectory optimization, and finally learn the rectified diffusion policy on the augmented dataset. We test on the NuScenes dataset and our approach can achieve the most diverse rule-compliant trajectories compared to other baselines, with runtime 1/17X to the second-best approach. In the closed-loop testing, our approach reaches the highest diversity, rule satisfaction rate, and the least collision rate. Our method can generate varied characteristics conditional on different STL parameters in testing. case study on human-robot encounter scenarios shows our approach can generate diverse and closed-to-oracle trajectories. The annotation tool, augmented dataset, and code are available at https://github.com/mengyuest/pSTL-diffusion-policy. Index TermsAutonomous Agents; Autonomous Vehicle Navigation; Machine Learning for Robot Control I. INTRODUCTIONsimulators and studying intelligent systems such as autonomous driving and warehouse ground robots [1], [2]. To close the sim-to-real gap for the agents, is critical to model the uncertainty and rule adherence properties that naturally arise from human behaviors. For example, human drivers have different characteristics (aggressiveness, conservativeness), which affect their decision-making in challenging scenarios (e.g., going through roundabout with dense traffic). Besides, low-level driving commands (steering the wheel, accelerating, braking) are also driven by high-level maneuvers lane-changing) and traffic rules (e.g., speed (lane-keeping, it Manuscript received: March 26, 2024; Revised: June 22, 2024; Accepted: July 31, 2024. This paper was recommended for publication by Editor Jens Kober upon evaluation of the Associate Editor and Reviewers comments. This work was partly supported by the National Science Foundation (NSF) CAREER Award #CCF-2238030 and the MIT-Ford Alliance program. 1Yue Meng and Chuchu Fan are with the Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, MA 02139 USA (mengyue@mit.edu; chuchu@mit.edu) Digital Object Identifier (DOI): see top of this page. limit). Thus, it is of paramount importance to endow agent models with diversity, controllability, and rule-awareness. However, driving simulators up-to-date [3], [4] still struggle in delivering diverse and rule-compliance agent behaviors. They either use recorded trajectories or utilize rule-based or imitation-based methods to generate policy. Rule-based approaches (IDM [5], MOBIL [6]) directly encode rules into mathematical models thus can provide safety and goalreaching performance. Still, they assume simplified driving scenarios and require careful parameter tuning, lacking diversity and realisticness. Imitation-based methods [7], [8] learn from real-world driving data, being more akin to human behaviors, but are prone to violate the rules. Besides, since there is only one outcome (out of many possible future trajectories) per scene in the ground truth, only limited diversity is achieved by these imitation-based approaches [9]. Impeding the advancement of learning realistic behaviors are three challenges: (1) flexible rule representation, (2) the scarcity of multiple-outcome datasets, and (3) the trade-off between rule compliance and diversity. Our paper systematically addresses these problems by leveraging formal language termed Signal Temporal Logic (STL) [10], [11]. STL is known for modeling complicated rules [12], and there are increasing works recently studying controller synthesis under STL specifications via trajectory optimization [13], deep learning [14], [15] and reinforcement learning [16]. Inspired by these works and recent breakthroughs in diffusion models [17] for policy learning [18], we proposed parametric-STL approach to flexibly encode traffic rules, augment the dataset, and learn controllable diffusion policy to balance quality and diversity1. The whole pipeline is: We first specify rules via parameterSTL and use demonstrations to calibrate the parameters. The parameters involve both discrete and continuous values, adding the capacity to form multi-modal and diverse policy distributions. Based on the STL, the parameters, and the original data, we generate the multiple-outcome data via trajectory optimization. Next, we use Denoising Diffusion Probabilistic Model (DDPM [17]) to learn from the augmented data. Finally, different from other diffusion-based policies [18], [19], an additional neural network is designed to regulate the trajectories to be rule-compliant and diverse. We conduct experiments on NuScenes [20], large-scale autonomous driving dataset. We first label the dataset using our annotation tool and generate the augmented dataset. Then we train our approach on the augmented dataset and evaluate on the validation set and in closed-loop testing. Our approach 1Diversity refers to generate different trajectories for the same STL rule. IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JULY 2024 results in the highest STL satisfaction rate on the validation set and generates the most diverse trajectories compared to baselines. In closed-loop testing, our approach reaches the highest overall performance regarding diversity, STL satisfaction, collision, out-of-lane, and progress. We also show that with varied STL parameters, our approach can reflect different driver characteristics in challenging roundabout scenario, which is valuable for diverse behavior modeling in simulators. case study on human-robot encounter scenarios "
[06.03.2025 22:10] Mistral response. {"id": "60346c6ef70848a0bd9009cc4ed019ab", "object": "chat.completion", "created": 1741299005, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\"Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, MA 02139 USA\"]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1579, "total_tokens": 1609, "completion_tokens": 30}}
[06.03.2025 22:10] Response: ["Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, MA 02139 USA"]
[06.03.2025 22:10] Deleting PDF ./assets/pdf/2503.02924.pdf.
[06.03.2025 22:10] Success.
[06.03.2025 22:10] Enriching papers with extra data.
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 0. Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced languages, while widely spoken but under-resourced languages a...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 1. Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' performance, which is constrained by the upper limit of human performance....
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 2. Visual embedding models excel at zero-shot tasks like visual retrieval and classification. However, these models cannot be used for tasks that contain ambiguity or require user instruction. These tasks necessitate a multimodal embedding model, which outputs embeddings that combine visual and natural...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 3. We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if i...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 4. We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of ...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 5. Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains underexplored. A major challenge is the complex and abstract n...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 6. Distilling advanced Large Language Models' instruction-following capabilities into smaller models using a selected subset has become a mainstream approach in model training. While existing synthetic instruction data selection strategies rely mainly on single-dimensional signals (i.e., reward scores,...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 7. Deploying large scale language models on edge devices faces inherent challenges such as high computational demands, energy consumption, and potential data privacy risks. This paper introduces the Shakti Small Language Models (SLMs) Shakti-100M, Shakti-250M, and Shakti-500M which target these constra...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 8. Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid me...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 9. Word-level quality estimation (QE) detects erroneous spans in machine translations, which can direct and facilitate human post-editing. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of hum...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 10. Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an ...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 11. Conversational assistants often require a question rewriting algorithm that leverages a subset of past interactions to provide a more meaningful (accurate) answer to the user's question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks ...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 12. Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 13. Recent progress in robotic manipulation has been fueled by large-scale datasets collected across diverse environments. Training robotic manipulation policies on these datasets is traditionally performed in a centralized manner, raising concerns regarding scalability, adaptability, and data privacy. ...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 14. Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspec...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 15. This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 16. In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and im...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 17. Autonomous Vehicles (AVs) have entered the commercialization stage, but their limited ability to interact and express intentions still poses challenges in interactions with Human-driven Vehicles (HVs). Recent advances in large language models (LLMs) enable bidirectional human-machine communication, ...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 18. Multi-agent coordination is crucial for reliable multi-robot navigation in shared spaces such as automated warehouses. In regions of dense robot traffic, local coordination methods may fail to find a deadlock-free solution. In these scenarios, it is appropriate to let a central unit generate a globa...
[06.03.2025 22:10] ********************************************************************************
[06.03.2025 22:10] Abstract 19. Generating realistic simulations is critical for autonomous system applications such as self-driving and human-robot interactions. However, driving simulators nowadays still have difficulty in generating controllable, diverse, and rule-compliant behaviors for road participants: Rule-based models can...
[06.03.2025 22:10] Read previous papers.
[06.03.2025 22:10] Generating reviews via LLM API.
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#low_resource", "#architecture", "#open_source", "#training", "#multilingual"], "emoji": "üåç", "ru": {"title": "Babel: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å Babel, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∞—è 25 —Å–∞–º—ã—Ö —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –º–∏—Ä–∞. –ú–æ–¥–µ
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#math", "#alignment", "#rlhf", "#reasoning", "#training"], "emoji": "üßÆ", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –ò–ò –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ: —à–∞–≥ –∑–∞ —à–∞–≥–æ–º –∫ —Å–≤–µ—Ä—Ö—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#open_source", "#dataset"], "emoji": "üß†", "ru": {"title": "ABC: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è —Å –≥–∏–±–∫–∏–º —è–∑—ã–∫–æ–≤—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ABC, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#3d", "#video"], "emoji": "üé•", "ru": {"title": "–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞–º–µ—Ä—ã –∏ 3D-—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "GEN3C - —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –≤–∏–¥–µ–æ —Å —Ç–æ—á–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞–º–µ—Ä—ã –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π 3D-—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 3D-–∫—ç—à –≤ –≤–∏–¥–µ –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ –≥–ª—É–±–∏–Ω–Ω
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#dataset", "#rl", "#optimization", "#synthetic", "#training"], "emoji": "üßë‚Äçüíª", "ru": {"title": "KodCode: –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "KodCode - —ç—Ç–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é. –û–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#healthcare", "#alignment", "#transfer_learning"], "emoji": "üî¨", "ru": {"title": "–î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ VLM –≤ –∞–Ω–∞–ª–∏–∑–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞–±–æ—Ç—ã –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —è–∑—ã
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#small_models", "#training", "#synthetic", "#optimization"], "emoji": "üß†", "ru": {"title": "CrowdSelect: —É–º–Ω—ã–π –æ—Ç–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π CrowdSelect. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#healthcare", "#benchmark", "#training", "#ethics", "#inference", "#small_models", "#optimization"], "emoji": "üì±", "ru": {"title": "–ú–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –Ω–∞ –∫—Ä–∞—é —Å–µ—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ—Ä–∏—é –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π Shakti, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–∏–º–µ–Ω
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#benchmark", "#data", "#graphs", "#dataset", "#multimodal", "#reasoning"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ì–∞—Ä–º–æ–Ω–∏—á–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ –≥—Ä–∞—Ñ–æ–≤—ã—Ö –±–∞–∑–∞—Ö –∑–Ω–∞–Ω–∏–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º MoR (Mixture of Structural-and-Textual
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#machine_translation", "#data", "#multilingual", "#healthcare"], "emoji": "üîç", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: –º–æ—Å—Ç –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ—Å—Ç—å—é", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤ (word-level QE) –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å –ø–æ—Å—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#open_source", "#inference", "#diffusion", "#science", "#multimodal"], "emoji": "üîÑ", "ru": {"title": "–ü–µ—Ä–µ–º–∞—Å–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ ReMDM (remasking diffusion model), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –¥–∏—Å–∫—Ä
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#dataset", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –ò–ò: –æ–¥–∏–Ω –º–µ—Ç–æ–¥ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞ –∫ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞—Ö: –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –∏ —Å–ª–∏—è–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å 
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#training", "#optimization", "#benchmark", "#dataset", "#data"], "emoji": "üîç", "ru": {"title": "ToolRet: –ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–µ–π –ø–æ–∏—Å–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –ò–ò", "desc": "ToolRet - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–∏—Å–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –æ–±—É—á–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#dataset"], "emoji": "ü§ñ", "ru": {"title": "–§–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–π –∏ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FLAME - –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏. FLAME –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –±–æ
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#open_source", "#plp", "#training", "#security", "#benchmark", "#dataset", "#data"], "emoji": "üõ°Ô∏è", "ru": {"title": "LLM –Ω–∞ —Å—Ç—Ä–∞–∂–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#benchmark", "#dataset", "#multimodal", "#cv"], "emoji": "üöÅ", "ru": {"title": "–£–º–Ω—ã–µ –¥—Ä–æ–Ω—ã: –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ë–ü–õ–ê —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å CognitiveDrone - –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –ó—Ä–µ–Ω–∏–µ-–Ø–∑—ã–∫-–î–µ–π—Å—Ç–≤–∏–µ (VLA) –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –±–µ—Å–ø–∏
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#multilingual", "#open_source", "#benchmark", "#dataset", "#machine_translation"], "emoji": "‚öñÔ∏è", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ: –ò–ò –ø–æ–∫–æ—Ä—è–µ—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—É—é –®–≤–µ–π—Ü–∞—Ä–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SwiLTra-Bench - –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º –º–∞—à–∏–Ω–Ω–æ–≥
[06.03.2025 22:10] Using data from previous issue: {"categories": ["#rl", "#robotics", "#inference", "#optimization", "#agents", "#reasoning"], "emoji": "üöó", "ru": {"title": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∏ –æ–±—ã—á–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Actor-Reasoner –¥–ª—è —É–ª—É—á
[06.03.2025 22:10] Querying the API.
[06.03.2025 22:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Multi-agent coordination is crucial for reliable multi-robot navigation in shared spaces such as automated warehouses. In regions of dense robot traffic, local coordination methods may fail to find a deadlock-free solution. In these scenarios, it is appropriate to let a central unit generate a global schedule that decides the passing order of robots. However, the runtime of such centralized coordination methods increases significantly with the problem scale. In this paper, we propose to leverage Graph Neural Network Variational Autoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale faster than through centralized optimization. We formulate the coordination problem as a graph problem and collect ground truth data using a Mixed-Integer Linear Program (MILP) solver. During training, our learning framework encodes good quality solutions of the graph problem into a latent space. At inference time, solution samples are decoded from the sampled latent variables, and the lowest-cost sample is selected for coordination. Finally, the feasible proposal with the highest performance index is selected for the deployment. By construction, our GNN-VAE framework returns solutions that always respect the constraints of the considered coordination problem. Numerical results show that our approach trained on small-scale problems can achieve high-quality solutions even for large-scale problems with 250 robots, being much faster than other baselines. Project page: https://mengyuest.github.io/gnn-vae-coord
[06.03.2025 22:10] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ä–æ–±–æ—Ç–æ–≤ –≤ –æ–±—â–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∫–ª–∞–¥—ã. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞—Ñ–æ–≤—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π (GNN-VAE) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –≤ –º–∞—Å—à—Ç–∞–±–µ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º —Å –ø–æ–º–æ—â—å—é —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ—à–∞—Ç–µ–ª–µ–º –∑–∞–¥–∞—á —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è (MILP). –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–æ–¥—Ö–æ–¥ –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –¥–∞–∂–µ –¥–ª—è –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å 250 —Ä–æ–±–æ—Ç–∞–º–∏, —Ä–∞–±–æ—Ç–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ –¥—Ä—É–≥–∏—Ö –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤.",
  "emoji": "ü§ñ",
  "title": "GNN-VAE: –ë—ã—Å—Ç—Ä–∞—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ä–æ–±–æ—Ç–æ–≤"
}
[06.03.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multi-agent coordination is crucial for reliable multi-robot navigation in shared spaces such as automated warehouses. In regions of dense robot traffic, local coordination methods may fail to find a deadlock-free solution. In these scenarios, it is appropriate to let a central unit generate a global schedule that decides the passing order of robots. However, the runtime of such centralized coordination methods increases significantly with the problem scale. In this paper, we propose to leverage Graph Neural Network Variational Autoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale faster than through centralized optimization. We formulate the coordination problem as a graph problem and collect ground truth data using a Mixed-Integer Linear Program (MILP) solver. During training, our learning framework encodes good quality solutions of the graph problem into a latent space. At inference time, solution samples are decoded from the sampled latent variables, and the lowest-cost sample is selected for coordination. Finally, the feasible proposal with the highest performance index is selected for the deployment. By construction, our GNN-VAE framework returns solutions that always respect the constraints of the considered coordination problem. Numerical results show that our approach trained on small-scale problems can achieve high-quality solutions even for large-scale problems with 250 robots, being much faster than other baselines. Project page: https://mengyuest.github.io/gnn-vae-coord"

[06.03.2025 22:10] Response: ```python
['AGENTS', 'INFERENCE', 'TRAINING']
```
[06.03.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multi-agent coordination is crucial for reliable multi-robot navigation in shared spaces such as automated warehouses. In regions of dense robot traffic, local coordination methods may fail to find a deadlock-free solution. In these scenarios, it is appropriate to let a central unit generate a global schedule that decides the passing order of robots. However, the runtime of such centralized coordination methods increases significantly with the problem scale. In this paper, we propose to leverage Graph Neural Network Variational Autoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale faster than through centralized optimization. We formulate the coordination problem as a graph problem and collect ground truth data using a Mixed-Integer Linear Program (MILP) solver. During training, our learning framework encodes good quality solutions of the graph problem into a latent space. At inference time, solution samples are decoded from the sampled latent variables, and the lowest-cost sample is selected for coordination. Finally, the feasible proposal with the highest performance index is selected for the deployment. By construction, our GNN-VAE framework returns solutions that always respect the constraints of the considered coordination problem. Numerical results show that our approach trained on small-scale problems can achieve high-quality solutions even for large-scale problems with 250 robots, being much faster than other baselines. Project page: https://mengyuest.github.io/gnn-vae-coord"

[06.03.2025 22:10] Response: ```python
["GAMES", "GRAPHS", "OPTIMIZATION"]
```
[06.03.2025 22:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenge of coordinating multiple robots in shared environments, particularly in high-traffic areas where traditional local methods may lead to deadlocks. The authors propose a novel approach using Graph Neural Network Variational Autoencoders (GNN-VAE) to efficiently generate coordination schedules at scale. By framing the coordination problem as a graph problem and utilizing a Mixed-Integer Linear Program (MILP) for data collection, the framework learns to encode effective solutions into a latent space. During inference, it decodes these solutions to select the most optimal coordination strategy, ensuring compliance with all operational constraints while significantly reducing computation time compared to centralized methods.","title":"Efficient Multi-Robot Coordination with GNN-VAE"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenge of coordinating multiple robots in shared environments, particularly in high-traffic areas where traditional local methods may lead to deadlocks. The authors propose a novel approach using Graph Neural Network Variational Autoencoders (GNN-VAE) to efficiently generate coordination schedules at scale. By framing the coordination problem as a graph problem and utilizing a Mixed-Integer Linear Program (MILP) for data collection, the framework learns to encode effective solutions into a latent space. During inference, it decodes these solutions to select the most optimal coordination strategy, ensuring compliance with all operational constraints while significantly reducing computation time compared to centralized methods.', title='Efficient Multi-Robot Coordination with GNN-VAE'))
[06.03.2025 22:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Â§öÊô∫ËÉΩ‰ΩìÂçèË∞ÉÂú®ÂÖ±‰∫´Á©∫Èó¥‰∏≠ÁöÑÂèØÈù†Â§öÊú∫Âô®‰∫∫ÂØºËà™‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂ∞§ÂÖ∂ÊòØÂú®Êú∫Âô®‰∫∫‰∫§ÈÄöÂØÜÈõÜÁöÑÂå∫Âüü„ÄÇ‰º†ÁªüÁöÑÂ±ÄÈÉ®ÂçèË∞ÉÊñπÊ≥ïÂèØËÉΩÊó†Ê≥ïÊâæÂà∞Êó†Ê≠ªÈîÅÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÂõ†Ê≠§ÈúÄË¶Å‰∏Ä‰∏™‰∏≠Â§ÆÂçïÂÖÉÁîüÊàêÂÖ®Â±ÄË∞ÉÂ∫¶Êù•ÂÜ≥ÂÆöÊú∫Âô®‰∫∫ÁöÑÈÄöË°åÈ°∫Â∫è„ÄÇÊú¨ÊñáÊèêÂá∫Âà©Áî®ÂõæÁ•ûÁªèÁΩëÁªúÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàGNN-VAEÔºâÊù•Êõ¥Âø´Âú∞Ëß£ÂÜ≥Â§ßËßÑÊ®°ÁöÑÂ§öÊô∫ËÉΩ‰ΩìÂçèË∞ÉÈóÆÈ¢òÔºåÈÅøÂÖç‰∫ÜÈõÜ‰∏≠‰ºòÂåñÊñπÊ≥ïÁöÑÈ´òËøêË°åÊó∂Èó¥„ÄÇÈÄöËøáÂ∞ÜÂçèË∞ÉÈóÆÈ¢òÂΩ¢ÂºèÂåñ‰∏∫ÂõæÈóÆÈ¢òÔºåÂπ∂‰ΩøÁî®Ê∑∑ÂêàÊï¥Êï∞Á∫øÊÄßËßÑÂàíÔºàMILPÔºâÊ±ÇËß£Âô®Êî∂ÈõÜÁúüÂÆûÊï∞ÊçÆÔºåÊàë‰ª¨ÁöÑÂ≠¶‰π†Ê°ÜÊû∂ËÉΩÂ§üÂú®ÊΩúÂú®Á©∫Èó¥‰∏≠ÁºñÁ†ÅÈ´òË¥®ÈáèÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ","title":"È´òÊïàÂ§öÊô∫ËÉΩ‰ΩìÂçèË∞ÉÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Â§öÊô∫ËÉΩ‰ΩìÂçèË∞ÉÂú®ÂÖ±‰∫´Á©∫Èó¥‰∏≠ÁöÑÂèØÈù†Â§öÊú∫Âô®‰∫∫ÂØºËà™‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂ∞§ÂÖ∂ÊòØÂú®Êú∫Âô®‰∫∫‰∫§ÈÄöÂØÜÈõÜÁöÑÂå∫Âüü„ÄÇ‰º†ÁªüÁöÑÂ±ÄÈÉ®ÂçèË∞ÉÊñπÊ≥ïÂèØËÉΩÊó†Ê≥ïÊâæÂà∞Êó†Ê≠ªÈîÅÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÂõ†Ê≠§ÈúÄË¶Å‰∏Ä‰∏™‰∏≠Â§ÆÂçïÂÖÉÁîüÊàêÂÖ®Â±ÄË∞ÉÂ∫¶Êù•ÂÜ≥ÂÆöÊú∫Âô®‰∫∫ÁöÑÈÄöË°åÈ°∫Â∫è„ÄÇÊú¨ÊñáÊèêÂá∫Âà©Áî®ÂõæÁ•ûÁªèÁΩëÁªúÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàGNN-VAEÔºâÊù•Êõ¥Âø´Âú∞Ëß£ÂÜ≥Â§ßËßÑÊ®°ÁöÑÂ§öÊô∫ËÉΩ‰ΩìÂçèË∞ÉÈóÆÈ¢òÔºåÈÅøÂÖç‰∫ÜÈõÜ‰∏≠‰ºòÂåñÊñπÊ≥ïÁöÑÈ´òËøêË°åÊó∂Èó¥„ÄÇÈÄöËøáÂ∞ÜÂçèË∞ÉÈóÆÈ¢òÂΩ¢ÂºèÂåñ‰∏∫ÂõæÈóÆÈ¢òÔºåÂπ∂‰ΩøÁî®Ê∑∑ÂêàÊï¥Êï∞Á∫øÊÄßËßÑÂàíÔºàMILPÔºâÊ±ÇËß£Âô®Êî∂ÈõÜÁúüÂÆûÊï∞ÊçÆÔºåÊàë‰ª¨ÁöÑÂ≠¶‰π†Ê°ÜÊû∂ËÉΩÂ§üÂú®ÊΩúÂú®Á©∫Èó¥‰∏≠ÁºñÁ†ÅÈ´òË¥®ÈáèÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ', title='È´òÊïàÂ§öÊô∫ËÉΩ‰ΩìÂçèË∞ÉÁöÑÊñ∞ÊñπÊ≥ï'))
[06.03.2025 22:10] Querying the API.
[06.03.2025 22:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Generating realistic simulations is critical for autonomous system applications such as self-driving and human-robot interactions. However, driving simulators nowadays still have difficulty in generating controllable, diverse, and rule-compliant behaviors for road participants: Rule-based models cannot produce diverse behaviors and require careful tuning, whereas learning-based methods imitate the policy from data but are not designed to follow the rules explicitly. Besides, the real-world datasets are by nature "single-outcome", making the learning method hard to generate diverse behaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion Models to learn controllable, diverse, and rule-aware policy. We first calibrate the STL on the real-world data, then generate diverse synthetic data using trajectory optimization, and finally learn the rectified diffusion policy on the augmented dataset. We test on the NuScenes dataset and our approach can achieve the most diverse rule-compliant trajectories compared to other baselines, with a runtime 1/17X to the second-best approach. In the closed-loop testing, our approach reaches the highest diversity, rule satisfaction rate, and the least collision rate. Our method can generate varied characteristics conditional on different STL parameters in testing. A case study on human-robot encounter scenarios shows our approach can generate diverse and closed-to-oracle trajectories. The annotation tool, augmented dataset, and code are available at https://github.com/mengyuest/pSTL-diffusion-policy.
[06.03.2025 22:10] Response: {
  "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å–∏–º—É–ª—è—Ü–∏–π –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, —Ç–∞–∫–∏—Ö –∫–∞–∫ –±–µ—Å–ø–∏–ª–æ—Ç–Ω—ã–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —á–µ–ª–æ–≤–µ–∫–∞ —Å —Ä–æ–±–æ—Ç–æ–º. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –°–∏–≥–Ω–∞–ª—å–Ω–æ–π –¢–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–π –õ–æ–≥–∏–∫–∏ (STL) –∏ –î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –ú–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–π –∏ —Å–æ–±–ª—é–¥–∞—é—â–µ–π –ø—Ä–∞–≤–∏–ª–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –¥–æ—Ä–æ–∂–Ω–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ —Å–Ω–∞—á–∞–ª–∞ –∫–∞–ª–∏–±—Ä—É–µ—Ç STL –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∑–∞—Ç–µ–º –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –∏ –Ω–∞–∫–æ–Ω–µ—Ü –æ–±—É—á–∞–µ—Ç —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—é, —Å–æ–±–ª—é–¥–µ–Ω–∏—é –ø—Ä–∞–≤–∏–ª –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –∞ —Ç–∞–∫–∂–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ STL.",

  "emoji": "üöó",

  "title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Å–∏–º—É–ª—è—Ü–∏–∏ –¥–æ—Ä–æ–∂–Ω–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é STL –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[06.03.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generating realistic simulations is critical for autonomous system applications such as self-driving and human-robot interactions. However, driving simulators nowadays still have difficulty in generating controllable, diverse, and rule-compliant behaviors for road participants: Rule-based models cannot produce diverse behaviors and require careful tuning, whereas learning-based methods imitate the policy from data but are not designed to follow the rules explicitly. Besides, the real-world datasets are by nature "single-outcome", making the learning method hard to generate diverse behaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion Models to learn controllable, diverse, and rule-aware policy. We first calibrate the STL on the real-world data, then generate diverse synthetic data using trajectory optimization, and finally learn the rectified diffusion policy on the augmented dataset. We test on the NuScenes dataset and our approach can achieve the most diverse rule-compliant trajectories compared to other baselines, with a runtime 1/17X to the second-best approach. In the closed-loop testing, our approach reaches the highest diversity, rule satisfaction rate, and the least collision rate. Our method can generate varied characteristics conditional on different STL parameters in testing. A case study on human-robot encounter scenarios shows our approach can generate diverse and closed-to-oracle trajectories. The annotation tool, augmented dataset, and code are available at https://github.com/mengyuest/pSTL-diffusion-policy."

[06.03.2025 22:10] Response: ```python
['DATASET', 'DATA', 'AGENTS', 'RL', 'TRAINING']
```
[06.03.2025 22:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generating realistic simulations is critical for autonomous system applications such as self-driving and human-robot interactions. However, driving simulators nowadays still have difficulty in generating controllable, diverse, and rule-compliant behaviors for road participants: Rule-based models cannot produce diverse behaviors and require careful tuning, whereas learning-based methods imitate the policy from data but are not designed to follow the rules explicitly. Besides, the real-world datasets are by nature "single-outcome", making the learning method hard to generate diverse behaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion Models to learn controllable, diverse, and rule-aware policy. We first calibrate the STL on the real-world data, then generate diverse synthetic data using trajectory optimization, and finally learn the rectified diffusion policy on the augmented dataset. We test on the NuScenes dataset and our approach can achieve the most diverse rule-compliant trajectories compared to other baselines, with a runtime 1/17X to the second-best approach. In the closed-loop testing, our approach reaches the highest diversity, rule satisfaction rate, and the least collision rate. Our method can generate varied characteristics conditional on different STL parameters in testing. A case study on human-robot encounter scenarios shows our approach can generate diverse and closed-to-oracle trajectories. The annotation tool, augmented dataset, and code are available at https://github.com/mengyuest/pSTL-diffusion-policy."

[06.03.2025 22:10] Response: ```python
["DIFFUSION", "SYNTHETIC"]
```
[06.03.2025 22:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenge of generating realistic simulations for autonomous systems, particularly in driving scenarios. It combines Signal Temporal Logic (STL) with Diffusion Models to create a policy that is both controllable and diverse while adhering to traffic rules. By calibrating STL on real-world data and generating synthetic data through trajectory optimization, the authors enhance the learning process to produce varied and rule-compliant behaviors. The results demonstrate that their method outperforms existing approaches in terms of diversity and safety in simulated environments.","title":"Diverse and Rule-Compliant Simulations for Autonomous Systems"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenge of generating realistic simulations for autonomous systems, particularly in driving scenarios. It combines Signal Temporal Logic (STL) with Diffusion Models to create a policy that is both controllable and diverse while adhering to traffic rules. By calibrating STL on real-world data and generating synthetic data through trajectory optimization, the authors enhance the learning process to produce varied and rule-compliant behaviors. The results demonstrate that their method outperforms existing approaches in terms of diversity and safety in simulated environments.', title='Diverse and Rule-Compliant Simulations for Autonomous Systems'))
[06.03.2025 22:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÈÄöËøá‰ø°Âè∑Êó∂Â∫èÈÄªËæëÔºàSTLÔºâÂíåÊâ©Êï£Ê®°ÂûãÊù•ÁîüÊàêÂèØÊéß„ÄÅÂ§öÊ†∑‰∏îÈÅµÂæ™ËßÑÂàôÁöÑË°å‰∏∫Á≠ñÁï•Ôºå‰ª•Ëß£ÂÜ≥ÂΩìÂâçÈ©æÈ©∂Ê®°ÊãüÂô®Âú®ÁîüÊàêÈÅìË∑ØÂèÇ‰∏éËÄÖË°å‰∏∫Êó∂ÁöÑÂ±ÄÈôêÊÄß„ÄÇÊàë‰ª¨È¶ñÂÖàÂú®ÁúüÂÆûÊï∞ÊçÆ‰∏äÊ†°ÂáÜSTLÔºåÁÑ∂ÂêéÂà©Áî®ËΩ®Ëøπ‰ºòÂåñÁîüÊàêÂ§öÊ†∑ÁöÑÂêàÊàêÊï∞ÊçÆÔºåÊúÄÂêéÂú®Â¢ûÂº∫Êï∞ÊçÆÈõÜ‰∏äÂ≠¶‰π†‰øÆÊ≠£ÁöÑÊâ©Êï£Á≠ñÁï•„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®NuScenesÊï∞ÊçÆÈõÜ‰∏äËÉΩÂ§üÁîüÊàêÊúÄÂÖ∑Â§öÊ†∑ÊÄß‰∏îÁ¨¶ÂêàËßÑÂàôÁöÑËΩ®ËøπÔºå‰∏îËøêË°åÊó∂Èó¥ÊòæËëó‰ºò‰∫éÂÖ∂‰ªñÂü∫Á∫øÊñπÊ≥ï„ÄÇÈÄöËøáÈó≠ÁéØÊµãËØïÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â§öÊ†∑ÊÄß„ÄÅËßÑÂàôÊª°Ë∂≥ÁéáÂíåÁ¢∞ÊíûÁéáÊñπÈù¢ÂùáË°®Áé∞ÊúÄ‰Ω≥ÔºåËÉΩÂ§üÊ†πÊçÆ‰∏çÂêåÁöÑSTLÂèÇÊï∞ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÁâπÂæÅ„ÄÇ","title":"ÁîüÊàêÂ§öÊ†∑Âåñ‰∏îÈÅµÂæ™ËßÑÂàôÁöÑË°å‰∏∫Á≠ñÁï•"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÈÄöËøá‰ø°Âè∑Êó∂Â∫èÈÄªËæëÔºàSTLÔºâÂíåÊâ©Êï£Ê®°ÂûãÊù•ÁîüÊàêÂèØÊéß„ÄÅÂ§öÊ†∑‰∏îÈÅµÂæ™ËßÑÂàôÁöÑË°å‰∏∫Á≠ñÁï•Ôºå‰ª•Ëß£ÂÜ≥ÂΩìÂâçÈ©æÈ©∂Ê®°ÊãüÂô®Âú®ÁîüÊàêÈÅìË∑ØÂèÇ‰∏éËÄÖË°å‰∏∫Êó∂ÁöÑÂ±ÄÈôêÊÄß„ÄÇÊàë‰ª¨È¶ñÂÖàÂú®ÁúüÂÆûÊï∞ÊçÆ‰∏äÊ†°ÂáÜSTLÔºåÁÑ∂ÂêéÂà©Áî®ËΩ®Ëøπ‰ºòÂåñÁîüÊàêÂ§öÊ†∑ÁöÑÂêàÊàêÊï∞ÊçÆÔºåÊúÄÂêéÂú®Â¢ûÂº∫Êï∞ÊçÆÈõÜ‰∏äÂ≠¶‰π†‰øÆÊ≠£ÁöÑÊâ©Êï£Á≠ñÁï•„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®NuScenesÊï∞ÊçÆÈõÜ‰∏äËÉΩÂ§üÁîüÊàêÊúÄÂÖ∑Â§öÊ†∑ÊÄß‰∏îÁ¨¶ÂêàËßÑÂàôÁöÑËΩ®ËøπÔºå‰∏îËøêË°åÊó∂Èó¥ÊòæËëó‰ºò‰∫éÂÖ∂‰ªñÂü∫Á∫øÊñπÊ≥ï„ÄÇÈÄöËøáÈó≠ÁéØÊµãËØïÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â§öÊ†∑ÊÄß„ÄÅËßÑÂàôÊª°Ë∂≥ÁéáÂíåÁ¢∞ÊíûÁéáÊñπÈù¢ÂùáË°®Áé∞ÊúÄ‰Ω≥ÔºåËÉΩÂ§üÊ†πÊçÆ‰∏çÂêåÁöÑSTLÂèÇÊï∞ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÁâπÂæÅ„ÄÇ', title='ÁîüÊàêÂ§öÊ†∑Âåñ‰∏îÈÅµÂæ™ËßÑÂàôÁöÑË°å‰∏∫Á≠ñÁï•'))
[06.03.2025 22:10] Loading Chinese text from previous data.
[06.03.2025 22:10] Renaming data file.
[06.03.2025 22:10] Renaming previous data. hf_papers.json to ./d/2025-03-06.json
[06.03.2025 22:10] Saving new data file.
[06.03.2025 22:10] Generating page.
[06.03.2025 22:10] Renaming previous page.
[06.03.2025 22:10] Renaming previous data. index.html to ./d/2025-03-06.html
[06.03.2025 22:10] [Experimental] Generating Chinese page for reading.
[06.03.2025 22:10] Chinese vocab [{'word': 'Â§ßÂûã', 'pinyin': 'd√†x√≠ng', 'trans': 'large-scale'}, {'word': 'ÂΩªÂ∫ï', 'pinyin': 'ch√®d«ê', 'trans': 'thoroughly'}, {'word': 'Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ', 'pinyin': 'z√¨r√°n y«îy√°n ch«îl«ê', 'trans': 'Natural Language Processing'}, {'word': 'Á®ÄÁº∫', 'pinyin': 'xƒ´quƒì', 'trans': 'scarce'}, {'word': 'Ë¶ÜÁõñ', 'pinyin': 'f√πg√†i', 'trans': 'cover'}, {'word': 'ÊúâÈôê', 'pinyin': 'y«íuxi√†n', 'trans': 'limited'}, {'word': '‰ºòÂÖà', 'pinyin': 'y≈çuxiƒÅn', 'trans': 'prioritize'}, {'word': 'ËµÑÊ∫ê', 'pinyin': 'zƒ´yu√°n', 'trans': 'resources'}, {'word': '‰∏∞ÂØå', 'pinyin': 'fƒìngf√π', 'trans': 'abundant'}, {'word': 'ÂåÆ‰πè', 'pinyin': 'ku√¨f√°', 'trans': 'scarce'}, {'word': 'Â∑ÆË∑ù', 'pinyin': 'chƒÅj√π', 'trans': 'gap'}, {'word': '‰ªãÁªç', 'pinyin': 'ji√®sh√†o', 'trans': 'introduce'}, {'word': 'Ê∂µÁõñ', 'pinyin': 'h√°ng√†i', 'trans': 'cover'}, {'word': 'Êåâ', 'pinyin': '√†n', 'trans': 'according to'}, {'word': 'ÊéíÂêç', 'pinyin': 'p√°im√≠ng', 'trans': 'ranking'}, {'word': 'ÊîØÊåÅ', 'pinyin': 'zhƒ´ch√≠', 'trans': 'support'}, {'word': 'ÂÖ®ÁêÉ', 'pinyin': 'qu√°nqi√∫', 'trans': 'global'}, {'word': '‰∫∫Âè£', 'pinyin': 'r√©nk«íu', 'trans': 'population'}, {'word': 'ÁªßÁª≠', 'pinyin': 'j√¨x√π', 'trans': 'continue'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πnli√†n', 'trans': 'pre-training'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': 'Â±Ç', 'pinyin': 'c√©ng', 'trans': 'layer'}, {'word': 'Êâ©Â±ï', 'pinyin': 'ku√≤zh«én', 'trans': 'expand'}, {'word': 'ÊäÄÊúØ', 'pinyin': 'j√¨sh√π', 'trans': 'technology'}, {'word': 'ÂèÇÊï∞', 'pinyin': 'cƒÅnsh«î', 'trans': 'parameters'}, {'word': 'Êï∞Èáè', 'pinyin': 'sh√πli√†ng', 'trans': 'quantity'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ngn√©ng', 'trans': 'performance'}, {'word': '‰∏äÈôê', 'pinyin': 'sh√†ngxi√†n', 'trans': 'upper limit'}, {'word': 'ÂºïÂÖ•', 'pinyin': 'y«ênr√π', 'trans': 'introduce'}, {'word': 'Âèò‰Ωì', 'pinyin': 'bi√†nt«ê', 'trans': 'variants'}, {'word': 'È´òÊïà', 'pinyin': 'gƒÅoxi√†o', 'trans': 'efficient'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'inference'}, {'word': 'ÂæÆË∞É', 'pinyin': 'wƒìiti√°o', 'trans': 'fine-tuning'}, {'word': 'ËÆæÂÆö', 'pinyin': 'sh√®d√¨ng', 'trans': 'set'}, {'word': 'Ê†áÂáÜ', 'pinyin': 'biƒÅozh«în', 'trans': 'standard'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ngg≈´', 'trans': 'evaluation'}, {'word': 'ËØÅÊòé', 'pinyin': 'zh√®ngm√≠ng', 'trans': 'prove'}, {'word': '‰ºòË∂ä', 'pinyin': 'y≈çuyu√®', 'trans': 'superior'}, {'word': 'ÁõëÁù£', 'pinyin': 'ji√†nd≈´', 'trans': 'supervised'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√πj√π j√≠', 'trans': 'dataset'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'}, {'word': 'È¢ÜÂÖà', 'pinyin': 'l«êngxiƒÅn', 'trans': 'lead'}, {'word': 'ÂïÜ‰∏ö', 'pinyin': 'shƒÅngy√®', 'trans': 'commercial'}, {'word': 'Ê∞¥Âπ≥', 'pinyin': 'shu«êp√≠ng', 'trans': 'level'}]
[06.03.2025 22:10] Renaming previous Chinese page.
[06.03.2025 22:10] Renaming previous data. zh.html to ./d/2025-03-05_zh_reading_task.html
[06.03.2025 22:10] Writing Chinese reading task.
[06.03.2025 22:10] Writing result.
[06.03.2025 22:10] Renaming log file.
[06.03.2025 22:10] Renaming previous data. log.txt to ./logs/2025-03-06_last_log.txt
