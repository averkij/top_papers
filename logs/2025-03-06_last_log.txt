[06.03.2025 08:15] Read previous papers.
[06.03.2025 08:15] Generating top page (month).
[06.03.2025 08:15] Writing top page (month).
[06.03.2025 09:11] Read previous papers.
[06.03.2025 09:11] Get feed.
[06.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00865
[06.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00329
[06.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.03278
[06.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.03751
[06.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02951
[06.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01836
[06.03.2025 09:11] Extract page data from URL. URL: https://huggingface.co/papers/2502.20317
[06.03.2025 09:11] Extract page data from URL. URL: https://huggingface.co/papers/2502.18860
[06.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.03044
[06.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01729
[06.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01378
[06.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00502
[06.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01763
[06.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01449
[06.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01372
[06.03.2025 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.03.2025 09:11] No deleted papers detected.
[06.03.2025 09:11] Downloading and parsing papers (pdf, html). Total: 15.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.00865.
[06.03.2025 09:11] Extra JSON file exists (./assets/json/2503.00865.json), skip PDF parsing.
[06.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.00865.json), skip HTML parsing.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.00329.
[06.03.2025 09:11] Extra JSON file exists (./assets/json/2503.00329.json), skip PDF parsing.
[06.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.00329.json), skip HTML parsing.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.03278.
[06.03.2025 09:11] Extra JSON file exists (./assets/json/2503.03278.json), skip PDF parsing.
[06.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.03278.json), skip HTML parsing.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.03751.
[06.03.2025 09:11] Extra JSON file exists (./assets/json/2503.03751.json), skip PDF parsing.
[06.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.03751.json), skip HTML parsing.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.02951.
[06.03.2025 09:11] Extra JSON file exists (./assets/json/2503.02951.json), skip PDF parsing.
[06.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.02951.json), skip HTML parsing.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01836.
[06.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01836.json), skip PDF parsing.
[06.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01836.json), skip HTML parsing.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.20317.
[06.03.2025 09:11] Downloading paper 2502.20317 from http://arxiv.org/pdf/2502.20317v2...
[06.03.2025 09:11] Extracting affiliations from text.
[06.03.2025 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases Yongjia Lei1, Haoyu Han2, Ryan A. Rossi3, Franck Dernoncourt3, Nedim Lipka3, Mahantesh Halappanavar4, Jiliang Tang2, Yu Wang1 1University of Oregon, 2Michigan State University, 3Adobe Research, 4Pacific Northwest National Laboratory {yuwang, yongjia}@uoregon.edu, {hanhaoy1, tangjili}@msu.edu {ryrossi, dernonco, lipka}@adobe.com, hala@pnnl.gov 5 2 0 2 1 ] . [ 2 7 1 3 0 2 . 2 0 5 2 : r a "
[06.03.2025 09:11] Response: ```python
[
    "University of Oregon",
    "Michigan State University",
    "Adobe Research",
    "Pacific Northwest National Laboratory"
]
```
[06.03.2025 09:11] Deleting PDF ./assets/pdf/2502.20317.pdf.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18860.
[06.03.2025 09:11] Downloading paper 2502.18860 from http://arxiv.org/pdf/2502.18860v2...
[06.03.2025 09:11] Extracting affiliations from text.
[06.03.2025 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Md Mehrab Tanjim, Ryan A. Rossi, Mike Rimer, Xiang Chen, Sungchul Kim, Vaishnavi Muppala, Tong Yu, Zhengmian Hu, Ritwik Sinha Wei Zhang, Iftikhar Ahamath Burhanuddin, Franck Dernoncourt {tanjim, ryrossi, mrimer, xiangche, sukim, mvaishna, tyu, zhengmianh, risinha, wzhang, burhanud, dernonco}@adobe.com Adobe Research "
[06.03.2025 09:11] Response: ```python
["Adobe Research"]
```
[06.03.2025 09:11] Deleting PDF ./assets/pdf/2502.18860.pdf.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.03044.
[06.03.2025 09:11] Extra JSON file exists (./assets/json/2503.03044.json), skip PDF parsing.
[06.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.03044.json), skip HTML parsing.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01729.
[06.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01729.json), skip PDF parsing.
[06.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01729.json), skip HTML parsing.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01378.
[06.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01378.json), skip PDF parsing.
[06.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01378.json), skip HTML parsing.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.00502.
[06.03.2025 09:11] Extra JSON file exists (./assets/json/2503.00502.json), skip PDF parsing.
[06.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.00502.json), skip HTML parsing.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01763.
[06.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01763.json), skip PDF parsing.
[06.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01763.json), skip HTML parsing.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01449.
[06.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01449.json), skip PDF parsing.
[06.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01449.json), skip HTML parsing.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01372.
[06.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01372.json), skip PDF parsing.
[06.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01372.json), skip HTML parsing.
[06.03.2025 09:11] Success.
[06.03.2025 09:11] Enriching papers with extra data.
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 0. Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced languages, while widely spoken but under-resourced languages a...
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 1. Visual embedding models excel at zero-shot tasks like visual retrieval and classification. However, these models cannot be used for tasks that contain ambiguity or require user instruction. These tasks necessitate a multimodal embedding model, which outputs embeddings that combine visual and natural...
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 2. Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains underexplored. A major challenge is the complex and abstract n...
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 3. We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if i...
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 4. We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of ...
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 5. Distilling advanced Large Language Models' instruction-following capabilities into smaller models using a selected subset has become a mainstream approach in model training. While existing synthetic instruction data selection strategies rely mainly on single-dimensional signals (i.e., reward scores,...
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 6. Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid me...
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 7. Conversational assistants often require a question rewriting algorithm that leverages a subset of past interactions to provide a more meaningful (accurate) answer to the user's question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks ...
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 8. Word-level quality estimation (QE) detects erroneous spans in machine translations, which can direct and facilitate human post-editing. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of hum...
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 9. Recent progress in robotic manipulation has been fueled by large-scale datasets collected across diverse environments. Training robotic manipulation policies on these datasets is traditionally performed in a centralized manner, raising concerns regarding scalability, adaptability, and data privacy. ...
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 10. This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition...
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 11. Autonomous Vehicles (AVs) have entered the commercialization stage, but their limited ability to interact and express intentions still poses challenges in interactions with Human-driven Vehicles (HVs). Recent advances in large language models (LLMs) enable bidirectional human-machine communication, ...
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 12. Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial...
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 13. Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspec...
[06.03.2025 09:11] ********************************************************************************
[06.03.2025 09:11] Abstract 14. In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and im...
[06.03.2025 09:11] Read previous papers.
[06.03.2025 09:11] Generating reviews via LLM API.
[06.03.2025 09:11] Using data from previous issue: {"categories": ["#low_resource", "#architecture", "#open_source", "#training", "#multilingual"], "emoji": "🌍", "ru": {"title": "Babel: революция в многоязычном машинном обучении", "desc": "Представлена новая многоязычная языковая модель Babel, охватывающая 25 самых распространенных языков мира. Моде
[06.03.2025 09:11] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#open_source", "#dataset"], "emoji": "🧠", "ru": {"title": "ABC: Мультимодальные встраивания с гибким языковым контролем", "desc": "Статья представляет новую мультимодальную модель встраивания под названием ABC, которая объединяет визуальные и текстовые д
[06.03.2025 09:11] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#healthcare", "#alignment", "#transfer_learning"], "emoji": "🔬", "ru": {"title": "Декомпозиция медицинских знаний для повышения эффективности VLM в анализе медицинских изображений", "desc": "Эта статья представляет новый подход к улучшению работы визуальных язы
[06.03.2025 09:11] Using data from previous issue: {"categories": ["#3d", "#video"], "emoji": "🎥", "ru": {"title": "Точный контроль камеры и 3D-согласованность в генерации видео", "desc": "GEN3C - это генеративная модель видео с точным контролем камеры и временной 3D-согласованностью. Она использует 3D-кэш в виде облаков точек, полученных из глубинн
[06.03.2025 09:11] Using data from previous issue: {"categories": ["#dataset", "#rl", "#optimization", "#synthetic", "#training"], "emoji": "🧑‍💻", "ru": {"title": "KodCode: Синтетические данные для обучения ИИ программированию", "desc": "KodCode - это синтетический набор данных для обучения больших языковых моделей программированию. Он состоит из тр
[06.03.2025 09:11] Using data from previous issue: {"categories": ["#small_models", "#training", "#synthetic", "#optimization"], "emoji": "🧠", "ru": {"title": "CrowdSelect: умный отбор инструкций для обучения языковых моделей", "desc": "Статья описывает новый метод отбора инструкций для обучения языковых моделей, названный CrowdSelect. Он использует
[06.03.2025 09:11] Querying the API.
[06.03.2025 09:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR.
[06.03.2025 09:11] Response: {
  "desc": "Эта статья представляет новый метод под названием MoR (Mixture of Structural-and-Textual Retrieval) для работы с графовыми базами знаний, содержащими текстовую информацию. MoR использует трехэтапный подход: планирование, рассуждение и организация, чтобы эффективно объединить структурный и текстовый поиск. Метод генерирует текстовые графы планирования, затем переплетает структурный обход и текстовое сопоставление, и наконец, переранжирует кандидатов на основе их структурных траекторий. Эксперименты показывают превосходство MoR в гармонизации структурного и текстового поиска.",

  "emoji": "🕸️",

  "title": "Гармоничное слияние структурного и текстового поиска в графовых базах знаний"
}
[06.03.2025 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR."

[06.03.2025 09:11] Response: ```python
["DATASET", "DATA", "BENCHMARK", "MULTIMODAL"]
```
[06.03.2025 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR."

[06.03.2025 09:11] Response: ```python
["GRAPHS", "REASONING"]
```
[06.03.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new method called Mixture of Structural-and-Textual Retrieval (MoR) for improving the retrieval of knowledge from Text-rich Graph Knowledge Bases (TG-KBs). Unlike traditional methods that treat textual and structural knowledge separately, MoR integrates both types through a three-stage framework: Planning, Reasoning, and Organizing. In the Planning stage, it creates graphs that outline the logic for query responses, while the Reasoning stage combines structural traversal with textual matching to gather relevant candidates. Finally, the Organizing stage enhances the selection process by reranking candidates based on their structural paths, leading to better retrieval performance across various query types.","title":"Harmonizing Text and Structure for Better Knowledge Retrieval"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new method called Mixture of Structural-and-Textual Retrieval (MoR) for improving the retrieval of knowledge from Text-rich Graph Knowledge Bases (TG-KBs). Unlike traditional methods that treat textual and structural knowledge separately, MoR integrates both types through a three-stage framework: Planning, Reasoning, and Organizing. In the Planning stage, it creates graphs that outline the logic for query responses, while the Reasoning stage combines structural traversal with textual matching to gather relevant candidates. Finally, the Organizing stage enhances the selection process by reranking candidates based on their structural paths, leading to better retrieval performance across various query types.', title='Harmonizing Text and Structure for Better Knowledge Retrieval'))
[06.03.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的混合检索方法，称为结构与文本检索的混合体（MoR），旨在同时利用文本和结构知识来回答查询。MoR通过规划-推理-组织的框架来实现这一目标，在规划阶段生成文本规划图，明确回答查询的逻辑。接着，在推理阶段，MoR结合结构遍历和文本匹配，从文本丰富的图知识库中获取候选答案。最后，在组织阶段，MoR根据结构轨迹对获取的候选答案进行重新排序，实验结果表明该方法在结构和文本检索的协调性上具有显著优势。","title":"结构与文本知识的完美结合"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新的混合检索方法，称为结构与文本检索的混合体（MoR），旨在同时利用文本和结构知识来回答查询。MoR通过规划-推理-组织的框架来实现这一目标，在规划阶段生成文本规划图，明确回答查询的逻辑。接着，在推理阶段，MoR结合结构遍历和文本匹配，从文本丰富的图知识库中获取候选答案。最后，在组织阶段，MoR根据结构轨迹对获取的候选答案进行重新排序，实验结果表明该方法在结构和文本检索的协调性上具有显著优势。', title='结构与文本知识的完美结合'))
[06.03.2025 09:12] Querying the API.
[06.03.2025 09:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Conversational assistants often require a question rewriting algorithm that leverages a subset of past interactions to provide a more meaningful (accurate) answer to the user's question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks supported by the conversational assistant, among other constraints. In this paper, we systematically investigate two different approaches, denoted as rewriting and fusion, on two fundamentally different generation tasks, including a text-to-text generation task and a multimodal generative task that takes as input text and generates a visualization or data table that answers the user's question. Our results indicate that the specific rewriting or fusion approach highly depends on the underlying use case and generative task. In particular, we find that for a conversational question-answering assistant, the query rewriting approach performs best, whereas for a data analysis assistant that generates visualizations and data tables based on the user's conversation with the assistant, the fusion approach works best. Notably, we explore two datasets for the data analysis assistant use case, for short and long conversations, and we find that query fusion always performs better, whereas for the conversational text-based question-answering, the query rewrite approach performs best.
[06.03.2025 09:12] Response: {
  "desc": "В статье исследуются два подхода к переписыванию запросов в разговорных ассистентах: переписывание и слияние. Эксперименты проводились на двух различных задачах генерации: текст-в-текст и мультимодальной генерации визуализаций. Результаты показывают, что эффективность подхода зависит от конкретного случая использования и задачи. Для текстового вопросно-ответного ассистента лучше работает переписывание запросов, а для ассистента по анализу данных - слияние запросов.",
  "emoji": "🤖",
  "title": "Переписывание запросов в разговорных ИИ: один метод не подходит для всех задач"
}
[06.03.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Conversational assistants often require a question rewriting algorithm that leverages a subset of past interactions to provide a more meaningful (accurate) answer to the user's question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks supported by the conversational assistant, among other constraints. In this paper, we systematically investigate two different approaches, denoted as rewriting and fusion, on two fundamentally different generation tasks, including a text-to-text generation task and a multimodal generative task that takes as input text and generates a visualization or data table that answers the user's question. Our results indicate that the specific rewriting or fusion approach highly depends on the underlying use case and generative task. In particular, we find that for a conversational question-answering assistant, the query rewriting approach performs best, whereas for a data analysis assistant that generates visualizations and data tables based on the user's conversation with the assistant, the fusion approach works best. Notably, we explore two datasets for the data analysis assistant use case, for short and long conversations, and we find that query fusion always performs better, whereas for the conversational text-based question-answering, the query rewrite approach performs best."

[06.03.2025 09:12] Response: ```python
['MULTIMODAL', 'DATASET']
```
[06.03.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Conversational assistants often require a question rewriting algorithm that leverages a subset of past interactions to provide a more meaningful (accurate) answer to the user's question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks supported by the conversational assistant, among other constraints. In this paper, we systematically investigate two different approaches, denoted as rewriting and fusion, on two fundamentally different generation tasks, including a text-to-text generation task and a multimodal generative task that takes as input text and generates a visualization or data table that answers the user's question. Our results indicate that the specific rewriting or fusion approach highly depends on the underlying use case and generative task. In particular, we find that for a conversational question-answering assistant, the query rewriting approach performs best, whereas for a data analysis assistant that generates visualizations and data tables based on the user's conversation with the assistant, the fusion approach works best. Notably, we explore two datasets for the data analysis assistant use case, for short and long conversations, and we find that query fusion always performs better, whereas for the conversational text-based question-answering, the query rewrite approach performs best."

[06.03.2025 09:12] Response: ```python
[]
```
[06.03.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how conversational assistants can improve their responses by using two methods: rewriting and fusion. The rewriting method modifies user questions to enhance accuracy, while the fusion method combines information from past interactions to generate answers. The study shows that the effectiveness of these methods varies based on the task; rewriting is better for text-based question answering, while fusion excels in generating visualizations from data analysis tasks. The findings highlight the importance of tailoring the approach to the specific use case for optimal performance.","title":"Tailoring Response Strategies for Conversational Assistants"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores how conversational assistants can improve their responses by using two methods: rewriting and fusion. The rewriting method modifies user questions to enhance accuracy, while the fusion method combines information from past interactions to generate answers. The study shows that the effectiveness of these methods varies based on the task; rewriting is better for text-based question answering, while fusion excels in generating visualizations from data analysis tasks. The findings highlight the importance of tailoring the approach to the specific use case for optimal performance.', title='Tailoring Response Strategies for Conversational Assistants'))
[06.03.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文探讨了对话助手中问题重写算法的两种不同方法：重写和融合。这两种方法在文本生成和多模态生成任务中表现不同，具体取决于应用场景。研究发现，对于对话问答助手，查询重写方法效果最佳；而对于生成可视化和数据表的数据分析助手，查询融合方法更为有效。我们还分析了短对话和长对话的数据集，结果表明查询融合在数据分析任务中始终表现更好。","title":"对话助手中的问题重写与融合方法的最佳选择"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文探讨了对话助手中问题重写算法的两种不同方法：重写和融合。这两种方法在文本生成和多模态生成任务中表现不同，具体取决于应用场景。研究发现，对于对话问答助手，查询重写方法效果最佳；而对于生成可视化和数据表的数据分析助手，查询融合方法更为有效。我们还分析了短对话和长对话的数据集，结果表明查询融合在数据分析任务中始终表现更好。', title='对话助手中的问题重写与融合方法的最佳选择'))
[06.03.2025 09:12] Using data from previous issue: {"categories": ["#translation", "#data", "#multilingual", "#healthcare"], "emoji": "🔍", "ru": {"title": "Оценка качества перевода: мост между точностью и практичностью", "desc": "Статья исследует влияние оценки качества перевода на уровне слов (word-level QE) на процесс постредактирования машинного 
[06.03.2025 09:12] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#dataset"], "emoji": "🤖", "ru": {"title": "Федеративное обучение для масштабируемой и конфиденциальной робототехники", "desc": "Статья представляет FLAME - первый бенчмарк для федеративного обучения в робототехнической манипуляции. FLAME включает в себя бо
[06.03.2025 09:12] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#benchmark", "#dataset", "#multimodal", "#cv"], "emoji": "🚁", "ru": {"title": "Умные дроны: когнитивное управление БПЛА с помощью ИИ", "desc": "В статье представлена модель CognitiveDrone - новая модель Зрение-Язык-Действие (VLA) для сложных задач беспи
[06.03.2025 09:12] Using data from previous issue: {"categories": ["#rl", "#robotics", "#inference", "#optimization", "#agents", "#reasoning"], "emoji": "🚗", "ru": {"title": "Интеллектуальное взаимодействие автономных и обычных автомобилей с помощью больших языковых моделей", "desc": "Эта статья представляет новую архитектуру Actor-Reasoner для улуч
[06.03.2025 09:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#benchmark", "#dataset", "#data"], "emoji": "🔍", "ru": {"title": "ToolRet: Новый вызов для моделей поиска инструментов ИИ", "desc": "ToolRet - это новый эталонный тест для оценки поиска инструментов в контексте обучения инструментам для больших языковых
[06.03.2025 09:12] Using data from previous issue: {"categories": ["#open_source", "#plp", "#training", "#security", "#benchmark", "#dataset", "#data"], "emoji": "🛡️", "ru": {"title": "LLM на страже безопасности кода: новые горизонты в обнаружении уязвимостей", "desc": "Статья представляет комплексное исследование возможностей больших языковых модел
[06.03.2025 09:12] Using data from previous issue: {"categories": ["#multilingual", "#open_source", "#benchmark", "#dataset", "#machine_translation"], "emoji": "⚖️", "ru": {"title": "Революция в юридическом переводе: ИИ покоряет многоязычную Швейцарию", "desc": "Статья представляет SwiLTra-Bench - многоязычный набор данных для оценки систем машинног
[06.03.2025 09:12] Trying to get texts in Chinese.
[06.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced languages, while widely spoken but under-resourced languages are often overlooked. To address this disparity, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supports over 90% of the global population, and includes many languages neglected by other open multilingual LLMs. Unlike traditional continue pretraining approaches, Babel expands its parameter count through a layer extension technique that elevates Babel's performance ceiling. We introduce two variants: Babel-9B, designed for efficient inference and fine-tuning, and Babel-83B, which sets a new standard for open multilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its superior performance compared to open LLMs of comparable size. In addition, using open-source supervised fine-tuning datasets, Babel achieves remarkable performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting a new standard for multilingual tasks, reaching the same level of commercial models.
[06.03.2025 09:12] Mistral response. {"id": "5588c72f9192488bb877a355dcb1513f", "object": "chat.completion", "created": 1741252335, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5f7b\u5e95\u6539\u53d8\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\uff0c\u4f46\u5f00\u6e90\u7684\u591a\u8bed\u8a00LLMs\u4ecd\u7136\u7a00\u7f3a\uff0c\u73b0\u6709\u6a21\u578b\u901a\u5e38\u8bed\u8a00\u8986\u76d6\u6709\u9650\u3002\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u4f18\u5148\u8003\u8651\u8d44\u6e90\u4e30\u5bcc\u7684\u8bed\u8a00\uff0c\u800c\u5ffd\u7565\u4e86\u5e7f\u6cdb\u4f7f\u7528\u4f46\u8d44\u6e90\u532e\u4e4f\u7684\u8bed\u8a00\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86Babel\uff0c\u4e00\u4e2a\u5f00\u653e\u7684\u591a\u8bed\u8a00LLM\uff0c\u6db5\u76d6\u4e86\u6309\u4f7f\u7528\u4eba\u6570\u6392\u540d\u524d25\u7684\u8bed\u8a00\uff0c\u652f\u6301\u8d85\u8fc790%\u7684\u5168\u7403\u4eba\u53e3\uff0c\u5e76\u5305\u62ec\u8bb8\u591a\u5176\u4ed6\u5f00\u653e\u591a\u8bed\u8a00LLMs\u5ffd\u7565\u7684\u8bed\u8a00\u3002\u4e0e\u4f20\u7edf\u7684\u7ee7\u7eed\u9884\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u540c\uff0cBabel\u901a\u8fc7\u4e00\u79cd\u5c42\u6269\u5c55\u6280\u672f\u589e\u52a0\u5176\u53c2\u6570\u6570\u91cf\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86Babel\u7684\u6027\u80fd\u4e0a\u9650\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u4e2a\u53d8\u4f53\uff1aBabel-9B\uff0c\u7528\u4e8e\u9ad8\u6548\u63a8\u7406\u548c\u5fae\u8c03\uff0c\u4ee5\u53caBabel-83B\uff0c\u4e3a\u5f00\u653e\u591a\u8bed\u8a00LLMs\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002\u5e7f\u6cdb\u7684\u591a\u8bed\u8a00\u4efb\u52a1\u8bc4\u4f30\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u5f00\u6e90\u76d1\u7763\u5fae\u8c03\u6570\u636e\u96c6\uff0cBabel\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\uff0cBabel-9B-Chat\u572810B\u5927\u5c0f\u7684LLMs\u4e2d\u9886\u5148\uff0cBabel-83B-Chat\u5728\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u8fbe\u5230\u4e86\u5546\u4e1a\u6a21\u578b\u7684\u6c34\u5e73\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 313, "total_tokens": 691, "completion_tokens": 378}}
[06.03.2025 09:12] Response: 大型语言模型（LLMs）彻底改变了自然语言处理（NLP），但开源的多语言LLMs仍然稀缺，现有模型通常语言覆盖有限。这些模型通常优先考虑资源丰富的语言，而忽略了广泛使用但资源匮乏的语言。为了解决这一差距，我们介绍了Babel，一个开放的多语言LLM，涵盖了按使用人数排名前25的语言，支持超过90%的全球人口，并包括许多其他开放多语言LLMs忽略的语言。与传统的继续预训练方法不同，Babel通过一种层扩展技术增加其参数数量，从而提高了Babel的性能上限。我们引入了两个变体：Babel-9B，用于高效推理和微调，以及Babel-83B，为开放多语言LLMs设定了新标准。广泛的多语言任务评估证明了其优越的性能。此外，使用开源监督微调数据集，Babel取得了显著的性能，Babel-9B-Chat在10B大小的LLMs中领先，Babel-83B-Chat在多语言任务中设定了新标准，达到了商业模型的水平。
[06.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

大型语言模型（LLMs）彻底改变了自然语言处理（NLP），但开源的多语言LLMs仍然稀缺，现有模型通常语言覆盖有限。这些模型通常优先考虑资源丰富的语言，而忽略了广泛使用但资源匮乏的语言。为了解决这一差距，我们介绍了Babel，一个开放的多语言LLM，涵盖了按使用人数排名前25的语言，支持超过90%的全球人口，并包括许多其他开放多语言LLMs忽略的语言。与传统的继续预训练方法不同，Babel通过一种层扩展技术增加其参数数量，从而提高了Babel的性能上限。我们引入了两个变体：Babel-9B，用于高效推理和微调，以及Babel-83B，为开放多语言LLMs设定了新标准。广泛的多语言任务评估证明了其优越的性能。此外，使用开源监督微调数据集，Babel取得了显著的性能，Babel-9B-Chat在10B大小的LLMs中领先，Babel-83B-Chat在多语言任务中设定了新标准，达到了商业模型的水平。
[06.03.2025 09:12] Mistral response. {"id": "58d7632987fb455da8c95f3577c710f9", "object": "chat.completion", "created": 1741252342, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "D\u00e0x\u00edng y\u01d4y\u00e1n m\u00f3x\u00edng (LLMs) ch\u00e8d\u01d0 g\u01ceibi\u00e0nle z\u00ecr\u00e1n y\u01d4y\u00e1n ch\u01d4l\u01d0 (NLP), d\u00e0n k\u0101iyu\u00e1n de du\u014dy\u01d4y\u00e1n LLMs r\u00e9ngr\u00e1n x\u012bqu\u0113, xi\u00e0ny\u01d2u m\u00f3x\u00edng t\u014dngch\u00e1ng y\u01d4y\u00e1n f\u00fag\u00e0i y\u01d2uxi\u00e0n. Zh\u00e8xi\u0113 m\u00f3x\u00edng t\u014dngch\u00e1ng y\u014duxi\u0101n k\u01ceol\u01dc z\u012byu\u00e1n f\u0113ngf\u00f9 de y\u01d4y\u00e1n, \u00e9r h\u016bl\u00fce le gu\u01cengf\u00e0n sh\u01d0y\u00f2ng d\u00e0n z\u012byu\u00e1n ku\u00ecf\u00e1 de y\u01d4y\u00e1n. W\u00e8ile ji\u011bju\u00e9 zh\u00e8 y\u012b ch\u0101j\u00f9, w\u01d2men ji\u00e8sh\u00e0o le Babel, y\u012bg\u00e8 k\u0101if\u00e0ng de du\u014dy\u01d4y\u00e1n LLM, h\u00e0nhu\u00f2le \u00e0n sh\u01d0y\u00f2ng r\u00e9nsh\u00f9 p\u00e1im\u00edng qi\u00e1n 25 de y\u01d4y\u00e1n, zh\u012bch\u00ed ch\u0101ogu\u00f2 90% de qu\u00e1nqi\u00fa r\u00e9nk\u01d2u, b\u00ecng b\u0101oku\u00f2 x\u01d4du\u014d q\u00edt\u0101 k\u0101if\u00e0ng du\u014dy\u01d4y\u00e1n LLMs h\u016bl\u00fce de y\u01d4y\u00e1n. Y\u01d4 chu\u00e1nt\u01d2ng de j\u00ecx\u00f9 y\u00f9x\u00f9n f\u0101ngf\u01ce b\u00f9t\u00f3ng, Babel t\u014dnggu\u00f2 y\u012bzh\u01d2ng c\u00e9ng ku\u00f2zh\u01cen j\u00ecsh\u00f9 z\u0113ngji\u0101 q\u00ed c\u0101nsh\u00f9 sh\u00f9li\u00e0ng, d\u00e0ngr\u00e1n t\u00edg\u0101ole Babel de x\u00ecngn\u00e9ng sh\u00e0ngxi\u00e0n. W\u01d2men y\u01d0nr\u00f9le li\u01cengg\u00e8 bi\u00e0nt\u01d0: Babel-9B, y\u00f2ngy\u00fa g\u0101oxi\u00e0o tu\u012bl\u01d0 h\u00e9 w\u0113iti\u00e1o, y\u01d0ji\u01ce Babel-83B, w\u00e8i k\u0101if\u00e0ng du\u014dy\u01d4y\u00e1n LLMs sh\u00e8d\u00ecngle x\u012bn bi\u0101ozh\u01d4n. Gu\u01cengf\u00e0n de du\u014dy\u01d4y\u00e1n r\u00e8nw\u00f9 p\u00edngg\u016b zh\u00e8ngm\u00edngle q\u00ed y\u014dubi\u00e8 de x\u00ecngn\u00e9ng. C\u01d0w\u00e0i, sh\u01d0y\u00f2ng k\u0101iyu\u00e1n ji\u00e0nsh\u01d0 w\u0113iti\u00e1o sh\u00f9j\u00faj\u00ed, Babel qu\u00e8d\u00e9le xi\u01cenzh\u00f9 de x\u00ecngn\u00e9ng, Babel-9B-Chat z\u00e0i 10B d\u00e0x\u00ecng de LLMs zh\u014dng l\u01d0ngxi\u0101n, Babel-83B-Chat z\u00e0i du\u014dy\u01d4y\u00e1n r\u00e8nw\u00f9 zh\u014dng sh\u00e8d\u00ecngle x\u012bn bi\u0101ozh\u01d4n, d\u00e1le sh\u0101ngy\u00e8 m\u00f3x\u00edng de shu\u01d0p\u00edng."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 393, "total_tokens": 1104, "completion_tokens": 711}}
[06.03.2025 09:12] Response: Dàxíng yǔyán móxíng (LLMs) chèdǐ gǎibiànle zìrán yǔyán chǔlǐ (NLP), dàn kāiyuán de duōyǔyán LLMs réngrán xīquē, xiànyǒu móxíng tōngcháng yǔyán fúgài yǒuxiàn. Zhèxiē móxíng tōngcháng yōuxiān kǎolǜ zīyuán fēngfù de yǔyán, ér hūlüe le guǎngfàn shǐyòng dàn zīyuán kuìfá de yǔyán. Wèile jiějué zhè yī chājù, wǒmen jièshào le Babel, yīgè kāifàng de duōyǔyán LLM, hànhuòle àn shǐyòng rénshù páimíng qián 25 de yǔyán, zhīchí chāoguò 90% de quánqiú rénkǒu, bìng bāokuò xǔduō qítā kāifàng duōyǔyán LLMs hūlüe de yǔyán. Yǔ chuántǒng de jìxù yùxùn fāngfǎ bùtóng, Babel tōngguò yīzhǒng céng kuòzhǎn jìshù zēngjiā qí cānshù shùliàng, dàngrán tígāole Babel de xìngnéng shàngxiàn. Wǒmen yǐnrùle liǎnggè biàntǐ: Babel-9B, yòngyú gāoxiào tuīlǐ hé wēitiáo, yǐjiǎ Babel-83B, wèi kāifàng duōyǔyán LLMs shèdìngle xīn biāozhǔn. Guǎngfàn de duōyǔyán rènwù pínggū zhèngmíngle qí yōubiè de xìngnéng. Cǐwài, shǐyòng kāiyuán jiànshǐ wēitiáo shùjújí, Babel quèdéle xiǎnzhù de xìngnéng, Babel-9B-Chat zài 10B dàxìng de LLMs zhōng lǐngxiān, Babel-83B-Chat zài duōyǔyán rènwù zhōng shèdìngle xīn biāozhǔn, dále shāngyè móxíng de shuǐpíng.
[06.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

大型语言模型（LLMs）彻底改变了自然语言处理（NLP），但开源的多语言LLMs仍然稀缺，现有模型通常语言覆盖有限。这些模型通常优先考虑资源丰富的语言，而忽略了广泛使用但资源匮乏的语言。为了解决这一差距，我们介绍了Babel，一个开放的多语言LLM，涵盖了按使用人数排名前25的语言，支持超过90%的全球人口，并包括许多其他开放多语言LLMs忽略的语言。与传统的继续预训练方法不同，Babel通过一种层扩展技术增加其参数数量，从而提高了Babel的性能上限。我们引入了两个变体：Babel-9B，用于高效推理和微调，以及Babel-83B，为开放多语言LLMs设定了新标准。广泛的多语言任务评估证明了其优越的性能。此外，使用开源监督微调数据集，Babel取得了显著的性能，Babel-9B-Chat在10B大小的LLMs中领先，Babel-83B-Chat在多语言任务中设定了新标准，达到了商业模型的水平。
[06.03.2025 09:13] Mistral response. {"id": "ca4da88ea657460cb6ad3a3576ef9a56", "object": "chat.completion", "created": 1741252355, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u5927\u578b\", \"pinyin\": \"d\u00e0x\u00edng\", \"trans\": \"large-scale\"},\n    {\"word\": \"\u5f7b\u5e95\", \"pinyin\": \"ch\u00e8d\u01d0\", \"trans\": \"thoroughly\"},\n    {\"word\": \"\u81ea\u7136\u8bed\u8a00\u5904\u7406\", \"pinyin\": \"z\u00ecr\u00e1n y\u01d4y\u00e1n ch\u01d4l\u01d0\", \"trans\": \"Natural Language Processing\"},\n    {\"word\": \"\u7a00\u7f3a\", \"pinyin\": \"x\u012bqu\u0113\", \"trans\": \"scarce\"},\n    {\"word\": \"\u8986\u76d6\", \"pinyin\": \"f\u00f9g\u00e0i\", \"trans\": \"cover\"},\n    {\"word\": \"\u6709\u9650\", \"pinyin\": \"y\u01d2uxi\u00e0n\", \"trans\": \"limited\"},\n    {\"word\": \"\u4f18\u5148\", \"pinyin\": \"y\u014duxi\u0101n\", \"trans\": \"prioritize\"},\n    {\"word\": \"\u8d44\u6e90\", \"pinyin\": \"z\u012byu\u00e1n\", \"trans\": \"resources\"},\n    {\"word\": \"\u4e30\u5bcc\", \"pinyin\": \"f\u0113ngf\u00f9\", \"trans\": \"abundant\"},\n    {\"word\": \"\u532e\u4e4f\", \"pinyin\": \"ku\u00ecf\u00e1\", \"trans\": \"scarce\"},\n    {\"word\": \"\u5dee\u8ddd\", \"pinyin\": \"ch\u0101j\u00f9\", \"trans\": \"gap\"},\n    {\"word\": \"\u4ecb\u7ecd\", \"pinyin\": \"ji\u00e8sh\u00e0o\", \"trans\": \"introduce\"},\n    {\"word\": \"\u6db5\u76d6\", \"pinyin\": \"h\u00e1ng\u00e0i\", \"trans\": \"cover\"},\n    {\"word\": \"\u6309\", \"pinyin\": \"\u00e0n\", \"trans\": \"according to\"},\n    {\"word\": \"\u6392\u540d\", \"pinyin\": \"p\u00e1im\u00edng\", \"trans\": \"ranking\"},\n    {\"word\": \"\u652f\u6301\", \"pinyin\": \"zh\u012bch\u00ed\", \"trans\": \"support\"},\n    {\"word\": \"\u5168\u7403\", \"pinyin\": \"qu\u00e1nqi\u00fa\", \"trans\": \"global\"},\n    {\"word\": \"\u4eba\u53e3\", \"pinyin\": \"r\u00e9nk\u01d2u\", \"trans\": \"population\"},\n    {\"word\": \"\u7ee7\u7eed\", \"pinyin\": \"j\u00ecx\u00f9\", \"trans\": \"continue\"},\n    {\"word\": \"\u9884\u8bad\u7ec3\", \"pinyin\": \"y\u00f9 x\u00f9nli\u00e0n\", \"trans\": \"pre-training\"},\n    {\"word\": \"\u65b9\u6cd5\", \"pinyin\": \"f\u0101ngf\u01ce\", \"trans\": \"method\"},\n    {\"word\": \"\u5c42\", \"pinyin\": \"c\u00e9ng\", \"trans\": \"layer\"},\n    {\"word\": \"\u6269\u5c55\", \"pinyin\": \"ku\u00f2zh\u01cen\", \"trans\": \"expand\"},\n    {\"word\": \"\u6280\u672f\", \"pinyin\": \"j\u00ecsh\u00f9\", \"trans\": \"technology\"},\n    {\"word\": \"\u53c2\u6570\", \"pinyin\": \"c\u0101nsh\u01d4\", \"trans\": \"parameters\"},\n    {\"word\": \"\u6570\u91cf\", \"pinyin\": \"sh\u00f9li\u00e0ng\", \"trans\": \"quantity\"},\n    {\"word\": \"\u63d0\u9ad8\", \"pinyin\": \"t\u00edg\u0101o\", \"trans\": \"improve\"},\n    {\"word\": \"\u6027\u80fd\", \"pinyin\": \"x\u00ecngn\u00e9ng\", \"trans\": \"performance\"},\n    {\"word\": \"\u4e0a\u9650\", \"pinyin\": \"sh\u00e0ngxi\u00e0n\", \"trans\": \"upper limit\"},\n    {\"word\": \"\u5f15\u5165\", \"pinyin\": \"y\u01d0nr\u00f9\", \"trans\": \"introduce\"},\n    {\"word\": \"\u53d8\u4f53\", \"pinyin\": \"bi\u00e0nt\u01d0\", \"trans\": \"variants\"},\n    {\"word\": \"\u9ad8\u6548\", \"pinyin\": \"g\u0101oxi\u00e0o\", \"trans\": \"efficient\"},\n    {\"word\": \"\u63a8\u7406\", \"pinyin\": \"tu\u012bl\u01d0\", \"trans\": \"inference\"},\n    {\"word\": \"\u5fae\u8c03\", \"pinyin\": \"w\u0113iti\u00e1o\", \"trans\": \"fine-tuning\"},\n    {\"word\": \"\u8bbe\u5b9a\", \"pinyin\": \"sh\u00e8d\u00ecng\", \"trans\": \"set\"},\n    {\"word\": \"\u6807\u51c6\", \"pinyin\": \"bi\u0101ozh\u01d4n\", \"trans\": \"standard\"},\n    {\"word\": \"\u8bc4\u4f30\", \"pinyin\": \"p\u00edngg\u016b\", \"trans\": \"evaluation\"},\n    {\"word\": \"\u8bc1\u660e\", \"pinyin\": \"zh\u00e8ngm\u00edng\", \"trans\": \"prove\"},\n    {\"word\": \"\u4f18\u8d8a\", \"pinyin\": \"y\u014duyu\u00e8\", \"trans\": \"superior\"},\n    {\"word\": \"\u76d1\u7763\", \"pinyin\": \"ji\u00e0nd\u016b\", \"trans\": \"supervised\"},\n    {\"word\": \"\u6570\u636e\u96c6\", \"pinyin\": \"sh\u00f9j\u00f9 j\u00ed\", \"trans\": \"dataset\"},\n    {\"word\": \"\u663e\u8457\", \"pinyin\": \"xi\u01cenzh\u00f9\", \"trans\": \"significant\"},\n    {\"word\": \"\u9886\u5148\", \"pinyin\": \"l\u01d0ngxi\u0101n\", \"trans\": \"lead\"},\n    {\"word\": \"\u5546\u4e1a\", \"pinyin\": \"sh\u0101ngy\u00e8\", \"trans\": \"commercial\"},\n    {\"word\": \"\u6c34\u5e73\", \"pinyin\": \"shu\u01d0p\u00edng\", \"trans\": \"level\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 425, "total_tokens": 1708, "completion_tokens": 1283}}
[06.03.2025 09:13] Response: [
    {"word": "大型", "pinyin": "dàxíng", "trans": "large-scale"},
    {"word": "彻底", "pinyin": "chèdǐ", "trans": "thoroughly"},
    {"word": "自然语言处理", "pinyin": "zìrán yǔyán chǔlǐ", "trans": "Natural Language Processing"},
    {"word": "稀缺", "pinyin": "xīquē", "trans": "scarce"},
    {"word": "覆盖", "pinyin": "fùgài", "trans": "cover"},
    {"word": "有限", "pinyin": "yǒuxiàn", "trans": "limited"},
    {"word": "优先", "pinyin": "yōuxiān", "trans": "prioritize"},
    {"word": "资源", "pinyin": "zīyuán", "trans": "resources"},
    {"word": "丰富", "pinyin": "fēngfù", "trans": "abundant"},
    {"word": "匮乏", "pinyin": "kuìfá", "trans": "scarce"},
    {"word": "差距", "pinyin": "chājù", "trans": "gap"},
    {"word": "介绍", "pinyin": "jièshào", "trans": "introduce"},
    {"word": "涵盖", "pinyin": "hángài", "trans": "cover"},
    {"word": "按", "pinyin": "àn", "trans": "according to"},
    {"word": "排名", "pinyin": "páimíng", "trans": "ranking"},
    {"word": "支持", "pinyin": "zhīchí", "trans": "support"},
    {"word": "全球", "pinyin": "quánqiú", "trans": "global"},
    {"word": "人口", "pinyin": "rénkǒu", "trans": "population"},
    {"word": "继续", "pinyin": "jìxù", "trans": "continue"},
    {"word": "预训练", "pinyin": "yù xùnliàn", "trans": "pre-training"},
    {"word": "方法", "pinyin": "fāngfǎ", "trans": "method"},
    {"word": "层", "pinyin": "céng", "trans": "layer"},
    {"word": "扩展", "pinyin": "kuòzhǎn", "trans": "expand"},
    {"word": "技术", "pinyin": "jìshù", "trans": "technology"},
    {"word": "参数", "pinyin": "cānshǔ", "trans": "parameters"},
    {"word": "数量", "pinyin": "shùliàng", "trans": "quantity"},
    {"word": "提高", "pinyin": "tígāo", "trans": "improve"},
    {"word": "性能", "pinyin": "xìngnéng", "trans": "performance"},
    {"word": "上限", "pinyin": "shàngxiàn", "trans": "upper limit"},
    {"word": "引入", "pinyin": "yǐnrù", "trans": "introduce"},
    {"word": "变体", "pinyin": "biàntǐ", "trans": "variants"},
    {"word": "高效", "pinyin": "gāoxiào", "trans": "efficient"},
    {"word": "推理", "pinyin": "tuīlǐ", "trans": "inference"},
    {"word": "微调", "pinyin": "wēitiáo", "trans": "fine-tuning"},
    {"word": "设定", "pinyin": "shèdìng", "trans": "set"},
    {"word": "标准", "pinyin": "biāozhǔn", "trans": "standard"},
    {"word": "评估", "pinyin": "pínggū", "trans": "evaluation"},
    {"word": "证明", "pinyin": "zhèngmíng", "trans": "prove"},
    {"word": "优越", "pinyin": "yōuyuè", "trans": "superior"},
    {"word": "监督", "pinyin": "jiàndū", "trans": "supervised"},
    {"word": "数据集", "pinyin": "shùjù jí", "trans": "dataset"},
    {"word": "显著", "pinyin": "xiǎnzhù", "trans": "significant"},
    {"word": "领先", "pinyin": "lǐngxiān", "trans": "lead"},
    {"word": "商业", "pinyin": "shāngyè", "trans": "commercial"},
    {"word": "水平", "pinyin": "shuǐpíng", "trans": "level"}
]
[06.03.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

大型语言模型（LLMs）彻底改变了自然语言处理（NLP），但开源的多语言LLMs仍然稀缺，现有模型通常语言覆盖有限。这些模型通常优先考虑资源丰富的语言，而忽略了广泛使用但资源匮乏的语言。为了解决这一差距，我们介绍了Babel，一个开放的多语言LLM，涵盖了按使用人数排名前25的语言，支持超过90%的全球人口，并包括许多其他开放多语言LLMs忽略的语言。与传统的继续预训练方法不同，Babel通过一种层扩展技术增加其参数数量，从而提高了Babel的性能上限。我们引入了两个变体：Babel-9B，用于高效推理和微调，以及Babel-83B，为开放多语言LLMs设定了新标准。广泛的多语言任务评估证明了其优越的性能。此外，使用开源监督微调数据集，Babel取得了显著的性能，Babel-9B-Chat在10B大小的LLMs中领先，Babel-83B-Chat在多语言任务中设定了新标准，达到了商业模型的水平。
[06.03.2025 09:13] Mistral response. {"id": "d954199dbeb64f4eae07bdeee05cd058", "object": "chat.completion", "created": 1741252380, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Large language models (LLMs) have revolutionized natural language processing (NLP), but open-source multilingual LLMs remain scarce, with existing models often having limited language coverage. These models typically prioritize resource-rich languages while neglecting widely used but resource-scarce languages. To address this gap, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supporting over 90% of the global population and including many languages overlooked by other open multilingual LLMs. Unlike traditional continued pre-training methods, Babel enhances its parameter count through a layer expansion technique, raising Babel's performance ceiling. We introduce two variants: Babel-9B for efficient inference and fine-tuning, and Babel-83B, setting a new standard for open multilingual LLMs. Extensive multilingual task evaluations demonstrate its superior performance. Additionally, using open-source supervised fine-tuning datasets, Babel achieves significant performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting new standards in multilingual tasks, reaching the level of commercial models."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 391, "total_tokens": 653, "completion_tokens": 262}}
[06.03.2025 09:13] Response: Large language models (LLMs) have revolutionized natural language processing (NLP), but open-source multilingual LLMs remain scarce, with existing models often having limited language coverage. These models typically prioritize resource-rich languages while neglecting widely used but resource-scarce languages. To address this gap, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supporting over 90% of the global population and including many languages overlooked by other open multilingual LLMs. Unlike traditional continued pre-training methods, Babel enhances its parameter count through a layer expansion technique, raising Babel's performance ceiling. We introduce two variants: Babel-9B for efficient inference and fine-tuning, and Babel-83B, setting a new standard for open multilingual LLMs. Extensive multilingual task evaluations demonstrate its superior performance. Additionally, using open-source supervised fine-tuning datasets, Babel achieves significant performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting new standards in multilingual tasks, reaching the level of commercial models.
[06.03.2025 09:13] Renaming data file.
[06.03.2025 09:13] Renaming previous data. hf_papers.json to ./d/2025-03-06.json
[06.03.2025 09:13] Saving new data file.
[06.03.2025 09:13] Generating page.
[06.03.2025 09:13] Renaming previous page.
[06.03.2025 09:13] Renaming previous data. index.html to ./d/2025-03-06.html
[06.03.2025 09:13] [Experimental] Generating Chinese page for reading.
[06.03.2025 09:13] Chinese vocab [{'word': '大型', 'pinyin': 'dàxíng', 'trans': 'large-scale'}, {'word': '彻底', 'pinyin': 'chèdǐ', 'trans': 'thoroughly'}, {'word': '自然语言处理', 'pinyin': 'zìrán yǔyán chǔlǐ', 'trans': 'Natural Language Processing'}, {'word': '稀缺', 'pinyin': 'xīquē', 'trans': 'scarce'}, {'word': '覆盖', 'pinyin': 'fùgài', 'trans': 'cover'}, {'word': '有限', 'pinyin': 'yǒuxiàn', 'trans': 'limited'}, {'word': '优先', 'pinyin': 'yōuxiān', 'trans': 'prioritize'}, {'word': '资源', 'pinyin': 'zīyuán', 'trans': 'resources'}, {'word': '丰富', 'pinyin': 'fēngfù', 'trans': 'abundant'}, {'word': '匮乏', 'pinyin': 'kuìfá', 'trans': 'scarce'}, {'word': '差距', 'pinyin': 'chājù', 'trans': 'gap'}, {'word': '介绍', 'pinyin': 'jièshào', 'trans': 'introduce'}, {'word': '涵盖', 'pinyin': 'hángài', 'trans': 'cover'}, {'word': '按', 'pinyin': 'àn', 'trans': 'according to'}, {'word': '排名', 'pinyin': 'páimíng', 'trans': 'ranking'}, {'word': '支持', 'pinyin': 'zhīchí', 'trans': 'support'}, {'word': '全球', 'pinyin': 'quánqiú', 'trans': 'global'}, {'word': '人口', 'pinyin': 'rénkǒu', 'trans': 'population'}, {'word': '继续', 'pinyin': 'jìxù', 'trans': 'continue'}, {'word': '预训练', 'pinyin': 'yù xùnliàn', 'trans': 'pre-training'}, {'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'}, {'word': '层', 'pinyin': 'céng', 'trans': 'layer'}, {'word': '扩展', 'pinyin': 'kuòzhǎn', 'trans': 'expand'}, {'word': '技术', 'pinyin': 'jìshù', 'trans': 'technology'}, {'word': '参数', 'pinyin': 'cānshǔ', 'trans': 'parameters'}, {'word': '数量', 'pinyin': 'shùliàng', 'trans': 'quantity'}, {'word': '提高', 'pinyin': 'tígāo', 'trans': 'improve'}, {'word': '性能', 'pinyin': 'xìngnéng', 'trans': 'performance'}, {'word': '上限', 'pinyin': 'shàngxiàn', 'trans': 'upper limit'}, {'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'}, {'word': '变体', 'pinyin': 'biàntǐ', 'trans': 'variants'}, {'word': '高效', 'pinyin': 'gāoxiào', 'trans': 'efficient'}, {'word': '推理', 'pinyin': 'tuīlǐ', 'trans': 'inference'}, {'word': '微调', 'pinyin': 'wēitiáo', 'trans': 'fine-tuning'}, {'word': '设定', 'pinyin': 'shèdìng', 'trans': 'set'}, {'word': '标准', 'pinyin': 'biāozhǔn', 'trans': 'standard'}, {'word': '评估', 'pinyin': 'pínggū', 'trans': 'evaluation'}, {'word': '证明', 'pinyin': 'zhèngmíng', 'trans': 'prove'}, {'word': '优越', 'pinyin': 'yōuyuè', 'trans': 'superior'}, {'word': '监督', 'pinyin': 'jiàndū', 'trans': 'supervised'}, {'word': '数据集', 'pinyin': 'shùjù jí', 'trans': 'dataset'}, {'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'}, {'word': '领先', 'pinyin': 'lǐngxiān', 'trans': 'lead'}, {'word': '商业', 'pinyin': 'shāngyè', 'trans': 'commercial'}, {'word': '水平', 'pinyin': 'shuǐpíng', 'trans': 'level'}]
[06.03.2025 09:13] Renaming previous Chinese page.
[06.03.2025 09:13] Renaming previous data. zh.html to ./d/2025-03-05_zh_reading_task.html
[06.03.2025 09:13] Writing Chinese reading task.
[06.03.2025 09:13] Writing result.
[06.03.2025 09:13] Renaming log file.
[06.03.2025 09:13] Renaming previous data. log.txt to ./logs/2025-03-06_last_log.txt
