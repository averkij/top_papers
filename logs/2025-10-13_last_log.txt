[13.10.2025 07:13] Read previous papers.
[13.10.2025 07:13] Generating top page (month).
[13.10.2025 07:13] Writing top page (month).
[13.10.2025 08:17] Read previous papers.
[13.10.2025 08:17] Get feed.
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08673
[13.10.2025 08:17] Extract page data from URL. URL: https://huggingface.co/papers/2510.05684
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04533
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09201
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09558
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06499
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08189
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09606
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08457
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09608
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08696
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09517
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04759
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09510
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09577
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08697
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08047
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07745
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09592
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08867
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05608
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09507
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08872
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07861
[13.10.2025 08:17] Extract page data from URL. URL: https://huggingface.co/papers/2510.02898
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09561
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08994
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07959
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07896
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07319
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09535
[13.10.2025 08:17] Extract page data from URL. URL: https://huggingface.co/papers/2510.08492
[13.10.2025 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01119
[13.10.2025 08:17] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.10.2025 08:17] No deleted papers detected.
[13.10.2025 08:17] Downloading and parsing papers (pdf, html). Total: 33.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.08673.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.08673.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.08673.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.05684.
[13.10.2025 08:17] Downloading paper 2510.05684 from http://arxiv.org/pdf/2510.05684v1...
[13.10.2025 08:17] Extracting affiliations from text.
[13.10.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 ] . [ 1 4 8 6 5 0 . 0 1 5 2 : r a D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI Suhwan Choi MAUM.AI Jaeyoon Jung MAUM.AI Haebin Seong MAUM.AI Minchan Kim MAUM.AI Yongjun Cho MAUM.AI Yoonshik Kim MAUM.AI Yubeen Park MAUM.AI Youngjae Yu Seoul National University Yunsung Lee MAUM.AI "
[13.10.2025 08:17] Response: ```python
["MAUM.AI", "Seoul National University"]
```
[13.10.2025 08:17] Deleting PDF ./assets/pdf/2510.05684.pdf.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.04533.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.04533.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.04533.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.09201.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.09201.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.09201.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.09558.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.09558.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.09558.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.06499.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.06499.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.06499.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.08189.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.08189.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.08189.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.09606.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.09606.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.09606.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.08457.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.08457.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.08457.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.09608.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.09608.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.09608.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.08696.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.08696.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.08696.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.09517.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.09517.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.09517.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.04759.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.04759.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.04759.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.09510.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.09510.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.09510.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.09577.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.09577.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.09577.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.08697.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.08697.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.08697.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.08047.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.08047.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.08047.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.07745.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.07745.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.07745.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.09592.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.09592.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.09592.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.08867.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.08867.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.08867.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.05608.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.05608.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.05608.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.09507.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.09507.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.09507.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.08872.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.08872.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.08872.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.07861.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.07861.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.07861.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.02898.
[13.10.2025 08:17] Downloading paper 2510.02898 from http://arxiv.org/pdf/2510.02898v2...
[13.10.2025 08:17] Extracting affiliations from text.
[13.10.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ONE PATCH TO CAPTION THEM ALL: UNIFIED ZEROSHOT CAPTIONING FRAMEWORK Lorenzo Bianchi*1,2 Giacomo Pacini*1,2 Fabio Carrara1 Nicola Messina1 Giuseppe Amato1 1CNR-ISTI Fabrizio Falchi1 2Università di Pisa 5 2 0 O 6 ] . [ 2 8 9 8 2 0 . 0 1 5 2 : r *Equal contribution "
[13.10.2025 08:17] Response: ```python
["CNR-ISTI", "Università di Pisa"]
```
[13.10.2025 08:17] Deleting PDF ./assets/pdf/2510.02898.pdf.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.09561.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.09561.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.09561.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.08994.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.08994.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.08994.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.07959.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.07959.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.07959.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.07896.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.07896.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.07896.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.07319.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.07319.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.07319.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.09535.
[13.10.2025 08:17] Extra JSON file exists (./assets/json/2510.09535.json), skip PDF parsing.
[13.10.2025 08:17] Paper image links file exists (./assets/img_data/2510.09535.json), skip HTML parsing.
[13.10.2025 08:17] Success.
[13.10.2025 08:17] Downloading and parsing paper https://huggingface.co/papers/2510.08492.
[13.10.2025 08:17] Downloading paper 2510.08492 from http://arxiv.org/pdf/2510.08492v1...
[13.10.2025 08:17] Extracting affiliations from text.
[13.10.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 ] . [ 1 2 9 4 8 0 . 0 1 5 2 : r Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models Sharut Gupta, Shobhita Sundaram, Chenyu Wang, Stefanie Jegelka , Phillip Isola MIT CSAIL, TU Munich {sharut, shobhita, wangchy, stefje, phillipi}@mit.edu Figure 1: Text provides complementary information beyond images, even when not paired directly; We introduce Unpaired Multimodal Learner (Uml) whereby sharing model weights across modalities (e.g., image and text) extracts synergies and enhances unimodal representations, outperforming methods that rely only on single modality (such as images above). "
[13.10.2025 08:17] Response: ```python
["MIT CSAIL", "TU Munich"]
```
[13.10.2025 08:17] Deleting PDF ./assets/pdf/2510.08492.pdf.
[13.10.2025 08:18] Success.
[13.10.2025 08:18] Downloading and parsing paper https://huggingface.co/papers/2510.01119.
[13.10.2025 08:18] Extra JSON file exists (./assets/json/2510.01119.json), skip PDF parsing.
[13.10.2025 08:18] Paper image links file exists (./assets/img_data/2510.01119.json), skip HTML parsing.
[13.10.2025 08:18] Success.
[13.10.2025 08:18] Enriching papers with extra data.
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 0. Puffin, a unified multimodal model, integrates language regression and diffusion-based generation to enhance camera-centric spatial understanding and generation by treating camera parameters as language.  					AI-generated summary 				 Camera-centric understanding and generation are two cornerstones...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 1. D2E framework uses desktop interactions to pretrain embodied AI, achieving high success rates in physical manipulation and navigation tasks.  					AI-generated summary 				 Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physic...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 2. Tangential Amplifying Guidance (TAG) improves diffusion model sample quality by directly amplifying tangential components of estimated scores without modifying the model architecture.  					AI-generated summary 				 Recent diffusion models achieve the state-of-the-art performance in image generation...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 3. Multimodal Prompt Optimizer (MPO) extends prompt optimization to handle multiple data types, improving performance over text-only methods in various applications.  					AI-generated summary 				 Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) furth...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 4. AutoPR, a multi-agent framework, automates the promotion of research papers by transforming them into engaging public content, significantly improving engagement metrics compared to direct LLM pipelines.  					AI-generated summary 				 As the volume of peer-reviewed research surges, scholars increas...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 5. A scalable data engine converts large-scale pre-training documents into diverse question-answer pairs for reinforcement learning, significantly improving model performance and efficiency.  					AI-generated summary 				 Large Language Models (LLMs) have achieved remarkable success through imitation ...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 6. R-HORIZON, a method using query composition, improves long-horizon reasoning in Large Reasoning Models through a benchmark of complex multi-step tasks, enhancing performance and accuracy.  					AI-generated summary 				 Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSe...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 7. A spatial reasoning model using scale-aware experts and progressive rewards demonstrates competitive performance across diverse tasks and scales using a large, curated dataset.  					AI-generated summary 				 With the current surge in spatial reasoning explorations, researchers have made significant...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 8. ARES, a unified framework for adaptive reasoning, dynamically adjusts exploration effort based on task difficulty using high window-entropy tokens and hierarchical entropy rewards, improving performance and efficiency across various benchmarks.  					AI-generated summary 				 Recent advances in mult...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 9. StreamingVLM is a real-time vision-language model that efficiently processes infinite video streams using a compact KV cache and supervised fine-tuning, achieving high performance on long videos and diverse benchmarks.  					AI-generated summary 				 Vision-language models (VLMs) could power real-ti...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 10. LENS modifies GRPO by assigning confidence-dependent rewards to incorrect responses, improving efficiency and performance in reinforcement learning with verifiable rewards.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) has become a standard recipe for improvin...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 11. StatEval is a comprehensive benchmark for statistical reasoning, covering foundational and research-level problems, and highlights the limitations of current LLMs in this domain.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable advances in mathematical and lo...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 12. PG-Occ, a Progressive Gaussian Transformer Framework, enhances 3D occupancy prediction with progressive densification and anisotropy-aware sampling, achieving state-of-the-art performance.  					AI-generated summary 				 The 3D occupancy prediction task has witnessed remarkable progress in recent ye...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 13. MRMR is a benchmark for expert-level multidisciplinary multimodal retrieval that includes reasoning-intensive tasks, contradiction retrieval, and image-text interleaved sequences, highlighting the need for improved multimodal models.  					AI-generated summary 				 We introduce MRMR, the first exper...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 14. Introducing Dyna-Mind, a two-stage training framework that enhances AI agents' reasoning and planning abilities through simulation, leading to improved performance in complex interactive environments.  					AI-generated summary 				 Reasoning models have recently shown remarkable progress in domains...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 15. BigCodeArena is an open human evaluation platform for code generation that enables real-time execution and interaction, revealing preferences and capabilities of LLMs in coding tasks.  					AI-generated summary 				 Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time eva...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 16. A parameter-space correction method reduces Word Error Rate in ASR systems by addressing pseudo-label biases without target ground truth.  					AI-generated summary 				 Robust ASR under domain shift is crucial because real-world systems encounter unseen accents and domains with limited labeled data...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 17. Parallel test-time scaling is enabled for latent reasoning models using uncertainty-inspired sampling strategies and a Latent Reward Model for effective trajectory selection.  					AI-generated summary 				 Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (L...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 18. Mind-Paced Speaking (MPS) is a brain-inspired framework that enables real-time reasoning and fluent speech generation by dividing the process into a "Formulation Brain" for reasoning and an "Articulation Brain" for speech, achieving high accuracy with low latency.  					AI-generated summary 				 Rea...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 19. ReviewerToo, a modular AI-assisted peer review framework, complements human judgment with systematic assessments, achieving high accuracy and quality in specific domains while highlighting areas where human expertise remains essential.  					AI-generated summary 				 Peer review is the cornerstone o...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 20. A plan-and-execute framework with EAGLET enhances LLM-based agents' planning abilities, achieving state-of-the-art performance in long-horizon tasks with reduced training costs.  					AI-generated summary 				 Agents based on large language models (LLMs) struggle with brainless trial-and-error and g...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 21. PhysToolBench evaluates MLLMs' comprehension of physical tools through a VQA dataset, revealing significant deficiencies in tool understanding.  					AI-generated summary 				 The ability to use, understand, and create tools is a hallmark of human intelligence, enabling sophisticated interaction wit...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 22. Game-Theoretic Alignment (GTAlign) improves Large Language Model (LLM) performance by integrating game-theoretic decision making into reasoning and training, enhancing efficiency, answer quality, and mutual welfare.  					AI-generated summary 				 Large Language Models (LLMs) have achieved remarkabl...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 23. A framework evaluates DeepResearch systems by assessing the quality, redundancy, and factuality of their research reports using an LLM-as-a-Judge methodology.  					AI-generated summary 				 DeepResearch agents represent a transformative AI paradigm, conducting expert-level research through sophisti...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 24. A patch-centric framework for zero-shot captioning achieves state-of-the-art performance by using dense visual features from models like DINO to caption arbitrary image regions.  					AI-generated summary 				 Zero-shot captioners are recently proposed models that utilize common-space vision-languag...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 25. TC-LoRA enhances generative fidelity and adherence to spatial conditions by dynamically conditioning model weights through a hypernetwork, improving upon static activation-based methods in diffusion models.  					AI-generated summary 				 Current controllable diffusion models typically rely on fixed...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 26. Speculative Jacobi-Denoising Decoding accelerates autoregressive text-to-image generation by enabling parallel token prediction and reducing model forward passes.  					AI-generated summary 				 As a new paradigm of visual content generation, autoregressive text-to-image models suffer from slow infe...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 27. A method called DISCO selects samples with the greatest model disagreements to predict performance, achieving state-of-the-art results across various benchmarks with reduced computational cost.  					AI-generated summary 				 Evaluating modern machine learning models has become prohibitively expensi...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 28. ACE, a framework using neuron-level attribution, enhances multi-hop factual recall in LLMs by editing critical query-value pathways, outperforming existing methods.  					AI-generated summary 				 Large Language Models (LLMs) require efficient knowledge editing (KE) to update factual information, ye...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 29. The Tenet framework decomposes the RVOS task into referring, video, and segmentation factors, using temporal prompts and prompt preference learning to adapt image-based foundation segmentation models for efficient RVOS.  					AI-generated summary 				 Referring Video Object Segmentation (RVOS) aims ...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 30. Group Relative Segment Penalization (GRSP) improves token efficiency in large reasoning models without significantly reducing accuracy, especially for complex problems, by regularizing reasoning at the step level.  					AI-generated summary 				 Large reasoning models (LRMs) boosted by Reinforcement...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 31. UML, an unpaired multimodal learner, enhances representation learning in a target modality by leveraging auxiliary unpaired data from different modalities.  					AI-generated summary 				 Traditional multimodal learners find unified representations for tasks like visual question answering, but rely ...
[13.10.2025 08:18] ********************************************************************************
[13.10.2025 08:18] Abstract 32. Instant4D uses deep visual SLAM and a 4D Gaussian representation to efficiently reconstruct scenes from uncalibrated video sequences in minutes.  					AI-generated summary 				 Dynamic view synthesis has seen significant advances, yet reconstructing scenes from uncalibrated, casual video remains cha...
[13.10.2025 08:18] Read previous papers.
[13.10.2025 08:18] Generating reviews via LLM API.
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#dataset", "#multimodal", "#benchmark", "#alignment", "#open_source"], "emoji": "📸", "ru": {"title": "Камера как язык: единая модель для понимания и генерации сцен", "desc": "Puffin — это мультимодальная модель, которая объединяет понимание и генерацию изображен
[13.10.2025 08:18] Querying the API.
[13.10.2025 08:18] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

D2E framework uses desktop interactions to pretrain embodied AI, achieving high success rates in physical manipulation and navigation tasks.  					AI-generated summary 				 Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/
[13.10.2025 08:18] Response: ```json
{
  "title": "От игр на компьютере к роботам: новый подход к обучению ИИ",
  "emoji": "🎮",
  "desc": "Исследователи разработали фреймворк D2E, который использует данные взаимодействия человека с компьютерными играми для предобучения embodied AI систем. Вместо дорогостоящего сбора физических траекторий роботов, модель обучается на 1300+ часах игрового процесса, включая человеческие демонстрации и автоматически размеченные данные. Ключевые компоненты включают OWA Toolkit для стандартизации данных, Generalist-IDM для zero-shot обобщения и VAPT для переноса знаний на физические задачи. Система достигла 96.6% успешности в манипуляциях (LIBERO) и 83.3% в навигации (CANVAS), доказав, что сенсомоторные паттерны из цифровых взаимодействий эффективно переносятся на реальных роботов."
}
```
[13.10.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"D2E framework uses desktop interactions to pretrain embodied AI, achieving high success rates in physical manipulation and navigation tasks.  					AI-generated summary 				 Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/"

[13.10.2025 08:18] Response: ```python
['AGENTS', 'ROBOTICS', 'DATASET', 'BENCHMARK']
```
[13.10.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"D2E framework uses desktop interactions to pretrain embodied AI, achieving high success rates in physical manipulation and navigation tasks.  					AI-generated summary 				 Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/"

[13.10.2025 08:18] Response: ```python
['GAMES', 'TRANSFER_LEARNING', 'OPEN_SOURCE']
```
[13.10.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The D2E framework introduces a novel approach to pretraining embodied AI by utilizing desktop interactions, particularly from gaming environments. This method allows for the collection of rich sensorimotor data at scale, which is crucial for training AI in physical manipulation and navigation tasks. The framework includes components like the OWA Toolkit for data standardization, the Generalist-IDM for zero-shot generalization, and VAPT for transferring learned representations to real-world applications. Results show high success rates in benchmark tasks, demonstrating the effectiveness of desktop pretraining for robotics.","title":"Transforming Desktop Interactions into Real-World AI Success"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The D2E framework introduces a novel approach to pretraining embodied AI by utilizing desktop interactions, particularly from gaming environments. This method allows for the collection of rich sensorimotor data at scale, which is crucial for training AI in physical manipulation and navigation tasks. The framework includes components like the OWA Toolkit for data standardization, the Generalist-IDM for zero-shot generalization, and VAPT for transferring learned representations to real-world applications. Results show high success rates in benchmark tasks, demonstrating the effectiveness of desktop pretraining for robotics.', title='Transforming Desktop Interactions into Real-World AI Success'))
[13.10.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"D2E框架利用桌面交互来预训练具身人工智能，在物理操作和导航任务中取得了高成功率。该框架通过游戏等桌面环境提供丰富的传感器运动交互，克服了物理轨迹收集的高成本问题。D2E包括三个主要组件：OWA工具包、通用IDM和VAPT，形成了从桌面数据收集到具身领域验证转移的完整流程。实验结果显示，数字交互中的传感器运动原语具有足够的不变性，可以有效转移到物理任务中，确立了桌面预训练作为机器人技术的实用范式。","title":"桌面交互助力具身AI的成功预训练"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='D2E框架利用桌面交互来预训练具身人工智能，在物理操作和导航任务中取得了高成功率。该框架通过游戏等桌面环境提供丰富的传感器运动交互，克服了物理轨迹收集的高成本问题。D2E包括三个主要组件：OWA工具包、通用IDM和VAPT，形成了从桌面数据收集到具身领域验证转移的完整流程。实验结果显示，数字交互中的传感器运动原语具有足够的不变性，可以有效转移到物理任务中，确立了桌面预训练作为机器人技术的实用范式。', title='桌面交互助力具身AI的成功预训练'))
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#inference", "#optimization", "#hallucinations"], "emoji": "📐", "ru": {"title": "Улучшение генерации через усиление касательных компонент", "desc": "Статья предлагает метод Tangential Amplifying Guidance (TAG) для улучшения качества генерации диффузионных моделе
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#optimization", "#training", "#multimodal"], "emoji": "🎨", "ru": {"title": "Оптимизация промптов для всех модальностей одновременно", "desc": "Статья представляет Multimodal Prompt Optimizer (MPO) — новый подход к оптимизации промптов для мультимодальных языковых моделей (MLLM). В о
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#optimization", "#multimodal", "#alignment"], "emoji": "📢", "ru": {"title": "Автоматическое продвижение научных статей с помощью мультиагентной системы", "desc": "Исследователи представили AutoPR — новую задачу автоматического превращения научных статей в пр
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#rl", "#data", "#reasoning", "#optimization"], "emoji": "🔄", "ru": {"title": "От текстов к вопросам: масштабирование RL для языковых моделей", "desc": "Исследователи разработали Webscale-RL pipeline — масштабируемый движок данных, который преобразует больши
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#long_context", "#training", "#reasoning", "#benchmark", "#rl"], "emoji": "🔭", "ru": {"title": "R-HORIZON: Обучение AI мыслить на дальние дистанции", "desc": "Статья представляет R-HORIZON - метод для оценки и улучшения способности больших языковых моделей к долгосрочному многоэтапн
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#data", "#cv", "#training", "#dataset", "#reasoning", "#survey", "#benchmark", "#optimization"], "emoji": "🔭", "ru": {"title": "Пространственное мышление на всех масштабах: от объектов до городских сцен", "desc": "Исследователи представили SpaceVista-7B — модель для пространственног
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#training", "#reasoning", "#benchmark", "#optimization", "#open_source", "#multimodal", "#dataset"], "emoji": "🎯", "ru": {"title": "ARES: Адаптивное мышление AI в зависимости от сложности задачи", "desc": "Статья представляет фреймворк ARES, который решает проблему неэффективного ра
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#cv", "#video", "#long_context", "#training", "#multimodal", "#benchmark", "#optimization", "#agents"], "emoji": "🎬", "ru": {"title": "Бесконечные видеопотоки без перегрузки памяти", "desc": "StreamingVLM - это модель для обработки бесконечных видеопотоков в реальном времени, котора
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#training", "#reasoning", "#benchmark", "#rl", "#optimization", "#rlhf"], "emoji": "🔍", "ru": {"title": "Превращая ошибки в уроки: как извлечь пользу из неправильных ответов LLM", "desc": "Статья представляет метод LENS, который улучшает алгоритм GRPO для обучения языковых моделей н
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#survey", "#reasoning"], "emoji": "📊", "ru": {"title": "StatEval: бенчмарк, который показал слабость LLM в статистике", "desc": "Статья представляет StatEval — первый комплексный бенчмарк для оценки статистического мышления LLM, включающий 13,817 базовых за
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#3d", "#cv", "#open_source", "#optimization"], "emoji": "🎯", "ru": {"title": "Прогрессивное уплотнение гауссиан для понимания 3D-сцен", "desc": "Статья представляет PG-Occ — фреймворк на основе Gaussian Transformer для предсказания 3D occupancy с поддержкой open-vocabulary запросов.
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#benchmark", "#games", "#reasoning", "#multimodal"], "emoji": "🔬", "ru": {"title": "Мультимодальный поиск экспертного уровня с глубоким reasoning", "desc": "MRMR — это первый бенчмарк для мультимодального retrieval экспертного уровня, требующий интенсивного reasoning. Он содержит 15
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#synthetic", "#training", "#reasoning", "#rl", "#optimization", "#agents"], "emoji": "🎮", "ru": {"title": "Учим AI-агентов думать перед действием через симуляцию", "desc": "Dyna-Mind — это двухэтапный фреймворк для обучения AI-агентов, который улучшает их способность к рассуждению и
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#games", "#dataset", "#multilingual", "#benchmark", "#open_source"], "emoji": "⚔️", "ru": {"title": "Арена для кода: краудсорсинговая оценка способностей LLM в программировании", "desc": "BigCodeArena — это открытая платформа для оценки генерации кода с помощью людей, которая позвол
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#data", "#training", "#low_resource", "#optimization", "#audio"], "emoji": "🗣️", "ru": {"title": "Коррекция параметров для улучшения распознавания речи", "desc": "В статье рассматривается метод коррекции параметров для снижения уровня ошибок в системах распознавания речи (ASR) без и
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#inference", "#architecture", "#reasoning", "#optimization", "#training"], "emoji": "🔍", "ru": {"title": "Новые горизонты масштабирования в латентных моделях рассуждения", "desc": "В статье рассматривается метод параллельного масштабирования во время тестирования для моделей латентн
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#audio", "#reasoning", "#training", "#multimodal"], "emoji": "🧠", "ru": {"title": "Думай и говори одновременно: архитектура двух мозгов для разговорного AI", "desc": "Исследователи представили Mind-Paced Speaking (MPS) — фреймворк, вдохновлённый работой человеческого мозга, который 
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#data", "#dataset", "#multimodal", "#ethics", "#benchmark", "#science"], "emoji": "🔍", "ru": {"title": "AI-рецензент: систематичность машины плюс экспертиза человека", "desc": "В статье представлен ReviewerToo — модульный фреймворк для AI-ассистированного научного рецензирования, ко
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#rl", "#agents", "#hallucinations", "#reasoning", "#optimization", "#training", "#long_context"], "emoji": "🦅", "ru": {"title": "EAGLET: умный планировщик для LLM-агентов без лишних затрат", "desc": "Исследователи предложили фреймворк plan-and-execute с методом EAGLET для улучшения 
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agi", "#multimodal", "#interpretability", "#open_source"], "emoji": "🔧", "ru": {"title": "Проверка понимания физических инструментов у мультимодальных LLM", "desc": "PhysToolBench — это первый бенчмарк для оценки того, насколько хорошо мультимодальные LLM 
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#rlhf", "#alignment", "#training", "#reasoning"], "emoji": "🎮", "ru": {"title": "Когда LLM играет в игры с пользователем: теоретико-игровой alignment", "desc": "Статья представляет GTAlign — новый подход к alignment LLM, основанный на теории игр. Авторы решают проблему, когда модель
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning", "#optimization", "#survey"], "emoji": "🔬", "ru": {"title": "Оценка AI-исследователей: как измерить качество глубокого анализа", "desc": "Статья представляет DeepResearch-ReportEval — фреймворк для оценки AI-систем, способных проводить глубокие и
[13.10.2025 08:18] Querying the API.
[13.10.2025 08:18] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A patch-centric framework for zero-shot captioning achieves state-of-the-art performance by using dense visual features from models like DINO to caption arbitrary image regions.  					AI-generated summary 				 Zero-shot captioners are recently proposed models that utilize common-space vision-language representations to caption images without relying on paired image-text data. To caption an image, they proceed by textually decoding a text-aligned image feature, but they limit their scope to global representations and whole-image captions. We present , a unified framework for zero-shot captioning that shifts from an image-centric to a patch-centric paradigm, enabling the captioning of arbitrary regions without the need of region-level supervision. Instead of relying on global image representations, we treat individual patches as atomic captioning units and aggregate them to describe arbitrary regions, from single patches to non-contiguous areas and entire images. We analyze the key ingredients that enable current latent captioners to work in our novel proposed framework. Experiments demonstrate that backbones producing meaningful, dense visual features, such as DINO, are key to achieving state-of-the-art performance in multiple region-based captioning tasks. Compared to other baselines and state-of-the-art competitors, our models achieve better performance on zero-shot dense, region-set, and a newly introduced trace captioning task, highlighting the effectiveness of patch-wise semantic representations for scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .
[13.10.2025 08:18] Response: ```json
{
  "title": "От целого изображения к патчам: новая эра zero-shot описаний",
  "desc": "Исследователи предложили новый подход к генерации описаний изображений без обучения на парах картинка-текст, сместив фокус с целого изображения на отдельные патчи. Вместо описания только всей картины целиком, модель может создавать описания произвольных областей, используя патчи как базовые единицы. Ключевым элементом успеха оказались визуальные энкодеры вроде DINO, которые создают плотные и осмысленные представления изображений. Система достигла state-of-the-art результатов в задачах описания регионов, включая новую задачу trace captioning.",
  "emoji": "🧩",
  "desc_en": ""
}
```
[13.10.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A patch-centric framework for zero-shot captioning achieves state-of-the-art performance by using dense visual features from models like DINO to caption arbitrary image regions.  					AI-generated summary 				 Zero-shot captioners are recently proposed models that utilize common-space vision-language representations to caption images without relying on paired image-text data. To caption an image, they proceed by textually decoding a text-aligned image feature, but they limit their scope to global representations and whole-image captions. We present , a unified framework for zero-shot captioning that shifts from an image-centric to a patch-centric paradigm, enabling the captioning of arbitrary regions without the need of region-level supervision. Instead of relying on global image representations, we treat individual patches as atomic captioning units and aggregate them to describe arbitrary regions, from single patches to non-contiguous areas and entire images. We analyze the key ingredients that enable current latent captioners to work in our novel proposed framework. Experiments demonstrate that backbones producing meaningful, dense visual features, such as DINO, are key to achieving state-of-the-art performance in multiple region-based captioning tasks. Compared to other baselines and state-of-the-art competitors, our models achieve better performance on zero-shot dense, region-set, and a newly introduced trace captioning task, highlighting the effectiveness of patch-wise semantic representations for scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ ."

[13.10.2025 08:18] Response: ```python
['CV', 'MULTIMODAL']
```
[13.10.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A patch-centric framework for zero-shot captioning achieves state-of-the-art performance by using dense visual features from models like DINO to caption arbitrary image regions.  					AI-generated summary 				 Zero-shot captioners are recently proposed models that utilize common-space vision-language representations to caption images without relying on paired image-text data. To caption an image, they proceed by textually decoding a text-aligned image feature, but they limit their scope to global representations and whole-image captions. We present , a unified framework for zero-shot captioning that shifts from an image-centric to a patch-centric paradigm, enabling the captioning of arbitrary regions without the need of region-level supervision. Instead of relying on global image representations, we treat individual patches as atomic captioning units and aggregate them to describe arbitrary regions, from single patches to non-contiguous areas and entire images. We analyze the key ingredients that enable current latent captioners to work in our novel proposed framework. Experiments demonstrate that backbones producing meaningful, dense visual features, such as DINO, are key to achieving state-of-the-art performance in multiple region-based captioning tasks. Compared to other baselines and state-of-the-art competitors, our models achieve better performance on zero-shot dense, region-set, and a newly introduced trace captioning task, highlighting the effectiveness of patch-wise semantic representations for scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ ."

[13.10.2025 08:18] Response: ```python
["GAMES"]
```
[13.10.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a patch-centric framework for zero-shot captioning, which allows models to generate captions for specific regions of images without needing paired image-text data. Instead of focusing on the entire image, the framework treats individual patches as the basic units for captioning, enabling more flexible and detailed descriptions. By utilizing dense visual features from models like DINO, the approach enhances performance across various region-based captioning tasks. The results show that this method outperforms existing models, demonstrating the advantages of using patch-wise semantic representations for scalable caption generation.","title":"Revolutionizing Image Captioning with Patch-Centric Insights"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a patch-centric framework for zero-shot captioning, which allows models to generate captions for specific regions of images without needing paired image-text data. Instead of focusing on the entire image, the framework treats individual patches as the basic units for captioning, enabling more flexible and detailed descriptions. By utilizing dense visual features from models like DINO, the approach enhances performance across various region-based captioning tasks. The results show that this method outperforms existing models, demonstrating the advantages of using patch-wise semantic representations for scalable caption generation.', title='Revolutionizing Image Captioning with Patch-Centric Insights'))
[13.10.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种基于补丁的零-shot图像描述框架，能够利用密集的视觉特征生成任意图像区域的描述。与传统的全图描述方法不同，该框架将图像划分为多个补丁，并将每个补丁视为独立的描述单元，从而实现对任意区域的描述。通过使用像DINO这样的模型，框架在多个区域描述任务中达到了最先进的性能。实验结果表明，补丁级的语义表示在可扩展的描述生成中具有显著的有效性。","title":"补丁驱动的零-shot图像描述新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种基于补丁的零-shot图像描述框架，能够利用密集的视觉特征生成任意图像区域的描述。与传统的全图描述方法不同，该框架将图像划分为多个补丁，并将每个补丁视为独立的描述单元，从而实现对任意区域的描述。通过使用像DINO这样的模型，框架在多个区域描述任务中达到了最先进的性能。实验结果表明，补丁级的语义表示在可扩展的描述生成中具有显著的有效性。', title='补丁驱动的零-shot图像描述新方法'))
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#cv", "#training", "#diffusion", "#optimization", "#architecture"], "emoji": "🎛️", "ru": {"title": "Динамическая настройка весов для адаптивного контроля диффузионных моделей", "desc": "TC-LoRA представляет новый подход к управлению диффузионными моделями через динамическое изменени
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#inference", "#optimization", "#cv", "#video"], "emoji": "🎨", "ru": {"title": "Параллельная генерация изображений через шумоподавление", "desc": "Авторегрессионные модели генерации изображений из текста работают медленно, так как создают токены последовательно, требуя тысячи проходо
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#data", "#optimization", "#benchmark", "#training"], "emoji": "🎯", "ru": {"title": "Умная выборка через разногласия моделей вместо дорогого полного тестирования", "desc": "Статья представляет метод DISCO для эффективной оценки современных ML-моделей, который значительно снижает вычи
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#architecture", "#data", "#interpretability"], "emoji": "🔗", "ru": {"title": "Редактирование знаний через управление нейронными путями", "desc": "Статья представляет фреймворк ACE для улучшения редактирования знаний в LLM, особенно для мно
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#video", "#optimization"], "emoji": "🎯", "ru": {"title": "Декомпозиция задачи видеосегментации через временные промпты", "desc": "Статья представляет фреймворк Tenet для задачи сегментации объектов в видео по текстовому описанию (RVOS). Авторы декомпозируют зада
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#training", "#optimization"], "emoji": "✂️", "ru": {"title": "Эффективное мышление: как научить AI рассуждать короче, но не глупее", "desc": "Большие модели рассуждений (LRM) часто страдают от «overthinking» — избыточных рассуждений, которые увеличивают вычислит
[13.10.2025 08:18] Querying the API.
[13.10.2025 08:18] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

UML, an unpaired multimodal learner, enhances representation learning in a target modality by leveraging auxiliary unpaired data from different modalities.  					AI-generated summary 				 Traditional multimodal learners find unified representations for tasks like visual question answering, but rely heavily on paired datasets. However, an overlooked yet potentially powerful question is: can one leverage auxiliary unpaired multimodal data to directly enhance representation learning in a target modality? We introduce UML: Unpaired Multimodal Learner, a modality-agnostic training paradigm in which a single model alternately processes inputs from different modalities while sharing parameters across them. This design exploits the assumption that different modalities are projections of a shared underlying reality, allowing the model to benefit from cross-modal structure without requiring explicit pairs. Theoretically, under linear data-generating assumptions, we show that unpaired auxiliary data can yield representations strictly more informative about the data-generating process than unimodal training. Empirically, we show that using unpaired data from auxiliary modalities -- such as text, audio, or images -- consistently improves downstream performance across diverse unimodal targets such as image and audio. Our project page: https://unpaired-multimodal.github.io/
[13.10.2025 08:18] Response: ```json
{
  "desc": "Статья представляет UML (Unpaired Multimodal Learner) — новый подход к обучению представлений, который использует непарные данные из разных модальностей для улучшения целевой модальности. В отличие от традиционных мультимодальных методов, требующих парные датасеты, UML работает с одной моделью, которая поочередно обрабатывает входы из разных модальностей с общими параметрами. Теоретически доказано, что непарные вспомогательные данные могут давать более информативные представления, чем одномодальное обучение. Эксперименты показывают стабильное улучшение качества при использовании непарных текстовых, аудио или визуальных данных для различных задач.",
  "emoji": "🔀",
  "title": "Учимся на разных модальностях без парных данных"
}
```
[13.10.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"UML, an unpaired multimodal learner, enhances representation learning in a target modality by leveraging auxiliary unpaired data from different modalities.  					AI-generated summary 				 Traditional multimodal learners find unified representations for tasks like visual question answering, but rely heavily on paired datasets. However, an overlooked yet potentially powerful question is: can one leverage auxiliary unpaired multimodal data to directly enhance representation learning in a target modality? We introduce UML: Unpaired Multimodal Learner, a modality-agnostic training paradigm in which a single model alternately processes inputs from different modalities while sharing parameters across them. This design exploits the assumption that different modalities are projections of a shared underlying reality, allowing the model to benefit from cross-modal structure without requiring explicit pairs. Theoretically, under linear data-generating assumptions, we show that unpaired auxiliary data can yield representations strictly more informative about the data-generating process than unimodal training. Empirically, we show that using unpaired data from auxiliary modalities -- such as text, audio, or images -- consistently improves downstream performance across diverse unimodal targets such as image and audio. Our project page: https://unpaired-multimodal.github.io/"

[13.10.2025 08:18] Response: ```python
['MULTIMODAL']
```
[13.10.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"UML, an unpaired multimodal learner, enhances representation learning in a target modality by leveraging auxiliary unpaired data from different modalities.  					AI-generated summary 				 Traditional multimodal learners find unified representations for tasks like visual question answering, but rely heavily on paired datasets. However, an overlooked yet potentially powerful question is: can one leverage auxiliary unpaired multimodal data to directly enhance representation learning in a target modality? We introduce UML: Unpaired Multimodal Learner, a modality-agnostic training paradigm in which a single model alternately processes inputs from different modalities while sharing parameters across them. This design exploits the assumption that different modalities are projections of a shared underlying reality, allowing the model to benefit from cross-modal structure without requiring explicit pairs. Theoretically, under linear data-generating assumptions, we show that unpaired auxiliary data can yield representations strictly more informative about the data-generating process than unimodal training. Empirically, we show that using unpaired data from auxiliary modalities -- such as text, audio, or images -- consistently improves downstream performance across diverse unimodal targets such as image and audio. Our project page: https://unpaired-multimodal.github.io/"

[13.10.2025 08:18] Response: ```python
["TRANSFER_LEARNING"]
```
[13.10.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces UML, an Unpaired Multimodal Learner that improves representation learning by using unpaired data from various modalities. Unlike traditional multimodal learners that depend on paired datasets, UML processes different modalities alternately while sharing model parameters. This approach allows the model to leverage the inherent relationships between modalities, enhancing its understanding of the target modality. The results demonstrate that incorporating unpaired auxiliary data significantly boosts performance in tasks involving unimodal data like images and audio.","title":"Unlocking Multimodal Learning with Unpaired Data"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces UML, an Unpaired Multimodal Learner that improves representation learning by using unpaired data from various modalities. Unlike traditional multimodal learners that depend on paired datasets, UML processes different modalities alternately while sharing model parameters. This approach allows the model to leverage the inherent relationships between modalities, enhancing its understanding of the target modality. The results demonstrate that incorporating unpaired auxiliary data significantly boosts performance in tasks involving unimodal data like images and audio.', title='Unlocking Multimodal Learning with Unpaired Data'))
[13.10.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"UML（无配对多模态学习器）通过利用来自不同模态的辅助无配对数据，增强目标模态的表示学习。传统的多模态学习器依赖于配对数据，而UML则提出了一种新的训练范式，允许模型交替处理不同模态的输入，并共享参数。该方法利用了不同模态是共享底层现实的投影这一假设，从而在不需要显式配对的情况下，获得跨模态结构的好处。实验结果表明，使用来自辅助模态的无配对数据可以显著提高在图像和音频等单模态任务上的表现。","title":"无配对数据助力多模态学习"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='UML（无配对多模态学习器）通过利用来自不同模态的辅助无配对数据，增强目标模态的表示学习。传统的多模态学习器依赖于配对数据，而UML则提出了一种新的训练范式，允许模型交替处理不同模态的输入，并共享参数。该方法利用了不同模态是共享底层现实的投影这一假设，从而在不需要显式配对的情况下，获得跨模态结构的好处。实验结果表明，使用来自辅助模态的无配对数据可以显著提高在图像和音频等单模态任务上的表现。', title='无配对数据助力多模态学习'))
[13.10.2025 08:18] Using data from previous issue: {"categories": ["#games", "#cv", "#video", "#dataset", "#inference", "#benchmark", "#optimization"], "emoji": "⚡", "ru": {"title": "Мгновенная 4D реконструкция сцен из обычного видео за минуты", "desc": "Instant4D — это система для реконструкции динамических сцен из некалиброванного видео, которая р
[13.10.2025 08:18] Renaming data file.
[13.10.2025 08:18] Renaming previous data. hf_papers.json to ./d/2025-10-13.json
[13.10.2025 08:18] Saving new data file.
[13.10.2025 08:18] Generating page.
[13.10.2025 08:18] Renaming previous page.
[13.10.2025 08:18] Renaming previous data. index.html to ./d/2025-10-13.html
[13.10.2025 08:18] Writing result.
[13.10.2025 08:18] Renaming log file.
[13.10.2025 08:18] Renaming previous data. log.txt to ./logs/2025-10-13_last_log.txt
