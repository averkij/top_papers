[13.10.2025 11:11] Read previous papers.
[13.10.2025 11:11] Generating top page (month).
[13.10.2025 11:11] Writing top page (month).
[13.10.2025 12:22] Read previous papers.
[13.10.2025 12:22] Get feed.
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08673
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05684
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04533
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09201
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09558
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08189
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06499
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09606
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08457
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09608
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08696
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09426
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06274
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04759
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09517
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09510
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07959
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09577
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08867
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08697
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08525
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08047
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07745
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09592
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05608
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09507
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09462
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08872
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07861
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02898
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09561
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09535
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08994
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07896
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07319
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01119
[13.10.2025 12:22] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08492
[13.10.2025 12:22] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.10.2025 12:22] No deleted papers detected.
[13.10.2025 12:22] Downloading and parsing papers (pdf, html). Total: 37.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.08673.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.08673.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.08673.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.05684.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.05684.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.05684.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.04533.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.04533.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.04533.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.09201.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.09201.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.09201.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.09558.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.09558.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.09558.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.08189.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.08189.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.08189.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.06499.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.06499.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.06499.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.09606.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.09606.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.09606.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.08457.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.08457.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.08457.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.09608.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.09608.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.09608.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.08696.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.08696.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.08696.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.09426.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.09426.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.09426.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.06274.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.06274.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.06274.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.04759.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.04759.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.04759.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.09517.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.09517.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.09517.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.09510.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.09510.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.09510.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.07959.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.07959.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.07959.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.09577.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.09577.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.09577.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.08867.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.08867.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.08867.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.08697.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.08697.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.08697.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.08525.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.08525.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.08525.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.08047.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.08047.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.08047.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.07745.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.07745.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.07745.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.09592.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.09592.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.09592.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.05608.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.05608.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.05608.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.09507.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.09507.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.09507.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.09462.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.09462.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.09462.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.08872.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.08872.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.08872.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.07861.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.07861.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.07861.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.02898.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.02898.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.02898.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.09561.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.09561.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.09561.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.09535.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.09535.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.09535.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.08994.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.08994.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.08994.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.07896.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.07896.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.07896.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.07319.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.07319.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.07319.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.01119.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.01119.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.01119.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Downloading and parsing paper https://huggingface.co/papers/2510.08492.
[13.10.2025 12:22] Extra JSON file exists (./assets/json/2510.08492.json), skip PDF parsing.
[13.10.2025 12:22] Paper image links file exists (./assets/img_data/2510.08492.json), skip HTML parsing.
[13.10.2025 12:22] Success.
[13.10.2025 12:22] Enriching papers with extra data.
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 0. Puffin, a unified multimodal model, integrates language regression and diffusion-based generation to enhance camera-centric spatial understanding and generation by treating camera parameters as language.  					AI-generated summary 				 Camera-centric understanding and generation are two cornerstones...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 1. D2E framework uses desktop interactions to pretrain embodied AI, achieving high success rates in physical manipulation and navigation tasks.  					AI-generated summary 				 Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physic...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 2. Tangential Amplifying Guidance (TAG) improves diffusion model sample quality by directly amplifying tangential components of estimated scores without modifying the model architecture.  					AI-generated summary 				 Recent diffusion models achieve the state-of-the-art performance in image generation...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 3. Multimodal Prompt Optimizer (MPO) extends prompt optimization to handle multiple data types, improving performance over text-only methods in various applications.  					AI-generated summary 				 Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) furth...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 4. AutoPR, a multi-agent framework, automates the promotion of research papers by transforming them into engaging public content, significantly improving engagement metrics compared to direct LLM pipelines.  					AI-generated summary 				 As the volume of peer-reviewed research surges, scholars increas...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 5. R-HORIZON, a method using query composition, improves long-horizon reasoning in Large Reasoning Models through a benchmark of complex multi-step tasks, enhancing performance and accuracy.  					AI-generated summary 				 Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSe...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 6. A scalable data engine converts large-scale pre-training documents into diverse question-answer pairs for reinforcement learning, significantly improving model performance and efficiency.  					AI-generated summary 				 Large Language Models (LLMs) have achieved remarkable success through imitation ...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 7. A spatial reasoning model using scale-aware experts and progressive rewards demonstrates competitive performance across diverse tasks and scales using a large, curated dataset.  					AI-generated summary 				 With the current surge in spatial reasoning explorations, researchers have made significant...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 8. ARES, a unified framework for adaptive reasoning, dynamically adjusts exploration effort based on task difficulty using high window-entropy tokens and hierarchical entropy rewards, improving performance and efficiency across various benchmarks.  					AI-generated summary 				 Recent advances in mult...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 9. StreamingVLM is a real-time vision-language model that efficiently processes infinite video streams using a compact KV cache and supervised fine-tuning, achieving high performance on long videos and diverse benchmarks.  					AI-generated summary 				 Vision-language models (VLMs) could power real-ti...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 10. LENS modifies GRPO by assigning confidence-dependent rewards to incorrect responses, improving efficiency and performance in reinforcement learning with verifiable rewards.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) has become a standard recipe for improvin...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 11. A large-scale investigation into constructing a fully open bilingual LLM for Korean using synthetic data demonstrates that such data can sustain pretraining and achieve performance comparable to multilingual baselines.  					AI-generated summary 				 This work presents the first large-scale investig...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 12. A framework called Complexity Out of Distribution (Complexity OoD) is proposed to define and measure reasoning ability in AI models by evaluating their performance on test instances requiring higher solution complexity than training examples.  					AI-generated summary 				 Recent progress has pushe...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 13. PG-Occ, a Progressive Gaussian Transformer Framework, enhances 3D occupancy prediction with progressive densification and anisotropy-aware sampling, achieving state-of-the-art performance.  					AI-generated summary 				 The 3D occupancy prediction task has witnessed remarkable progress in recent ye...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 14. StatEval is a comprehensive benchmark for statistical reasoning, covering foundational and research-level problems, and highlights the limitations of current LLMs in this domain.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable advances in mathematical and lo...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 15. MRMR is a benchmark for expert-level multidisciplinary multimodal retrieval that includes reasoning-intensive tasks, contradiction retrieval, and image-text interleaved sequences, highlighting the need for improved multimodal models.  					AI-generated summary 				 We introduce MRMR, the first exper...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 16. A method called DISCO selects samples with the greatest model disagreements to predict performance, achieving state-of-the-art results across various benchmarks with reduced computational cost.  					AI-generated summary 				 Evaluating modern machine learning models has become prohibitively expensi...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 17. Introducing Dyna-Mind, a two-stage training framework that enhances AI agents' reasoning and planning abilities through simulation, leading to improved performance in complex interactive environments.  					AI-generated summary 				 Reasoning models have recently shown remarkable progress in domains...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 18. ReviewerToo, a modular AI-assisted peer review framework, complements human judgment with systematic assessments, achieving high accuracy and quality in specific domains while highlighting areas where human expertise remains essential.  					AI-generated summary 				 Peer review is the cornerstone o...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 19. BigCodeArena is an open human evaluation platform for code generation that enables real-time execution and interaction, revealing preferences and capabilities of LLMs in coding tasks.  					AI-generated summary 				 Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time eva...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 20. A reinforcement learning framework identifies and prioritizes critical attention heads for efficient KV cache compression in large language models, maintaining reasoning quality with reduced overhead.  					AI-generated summary 				 Reasoning large language models exhibit complex reasoning behaviors...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 21. A parameter-space correction method reduces Word Error Rate in ASR systems by addressing pseudo-label biases without target ground truth.  					AI-generated summary 				 Robust ASR under domain shift is crucial because real-world systems encounter unseen accents and domains with limited labeled data...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 22. Parallel test-time scaling is enabled for latent reasoning models using uncertainty-inspired sampling strategies and a Latent Reward Model for effective trajectory selection.  					AI-generated summary 				 Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (L...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 23. Mind-Paced Speaking (MPS) is a brain-inspired framework that enables real-time reasoning and fluent speech generation by dividing the process into a "Formulation Brain" for reasoning and an "Articulation Brain" for speech, achieving high accuracy with low latency.  					AI-generated summary 				 Rea...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 24. A plan-and-execute framework with EAGLET enhances LLM-based agents' planning abilities, achieving state-of-the-art performance in long-horizon tasks with reduced training costs.  					AI-generated summary 				 Agents based on large language models (LLMs) struggle with brainless trial-and-error and g...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 25. PhysToolBench evaluates MLLMs' comprehension of physical tools through a VQA dataset, revealing significant deficiencies in tool understanding.  					AI-generated summary 				 The ability to use, understand, and create tools is a hallmark of human intelligence, enabling sophisticated interaction wit...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 26. Adaptive attacks on AI control protocols using prompt injections can evade monitors and complete malicious tasks, highlighting a significant vulnerability in current security mechanisms.  					AI-generated summary 				 AI control protocols serve as a defense mechanism to stop untrusted LLM agents fr...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 27. Game-Theoretic Alignment (GTAlign) improves Large Language Model (LLM) performance by integrating game-theoretic decision making into reasoning and training, enhancing efficiency, answer quality, and mutual welfare.  					AI-generated summary 				 Large Language Models (LLMs) have achieved remarkabl...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 28. A framework evaluates DeepResearch systems by assessing the quality, redundancy, and factuality of their research reports using an LLM-as-a-Judge methodology.  					AI-generated summary 				 DeepResearch agents represent a transformative AI paradigm, conducting expert-level research through sophisti...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 29. A patch-centric framework for zero-shot captioning achieves state-of-the-art performance by using dense visual features from models like DINO to caption arbitrary image regions.  					AI-generated summary 				 Zero-shot captioners are recently proposed models that utilize common-space vision-languag...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 30. TC-LoRA enhances generative fidelity and adherence to spatial conditions by dynamically conditioning model weights through a hypernetwork, improving upon static activation-based methods in diffusion models.  					AI-generated summary 				 Current controllable diffusion models typically rely on fixed...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 31. Group Relative Segment Penalization (GRSP) improves token efficiency in large reasoning models without significantly reducing accuracy, especially for complex problems, by regularizing reasoning at the step level.  					AI-generated summary 				 Large reasoning models (LRMs) boosted by Reinforcement...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 32. Speculative Jacobi-Denoising Decoding accelerates autoregressive text-to-image generation by enabling parallel token prediction and reducing model forward passes.  					AI-generated summary 				 As a new paradigm of visual content generation, autoregressive text-to-image models suffer from slow infe...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 33. ACE, a framework using neuron-level attribution, enhances multi-hop factual recall in LLMs by editing critical query-value pathways, outperforming existing methods.  					AI-generated summary 				 Large Language Models (LLMs) require efficient knowledge editing (KE) to update factual information, ye...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 34. The Tenet framework decomposes the RVOS task into referring, video, and segmentation factors, using temporal prompts and prompt preference learning to adapt image-based foundation segmentation models for efficient RVOS.  					AI-generated summary 				 Referring Video Object Segmentation (RVOS) aims ...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 35. Instant4D uses deep visual SLAM and a 4D Gaussian representation to efficiently reconstruct scenes from uncalibrated video sequences in minutes.  					AI-generated summary 				 Dynamic view synthesis has seen significant advances, yet reconstructing scenes from uncalibrated, casual video remains cha...
[13.10.2025 12:22] ********************************************************************************
[13.10.2025 12:22] Abstract 36. UML, an unpaired multimodal learner, enhances representation learning in a target modality by leveraging auxiliary unpaired data from different modalities.  					AI-generated summary 				 Traditional multimodal learners find unified representations for tasks like visual question answering, but rely ...
[13.10.2025 12:22] Read previous papers.
[13.10.2025 12:22] Generating reviews via LLM API.
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#dataset", "#multimodal", "#benchmark", "#alignment", "#open_source"], "emoji": "üì∏", "ru": {"title": "–ö–∞–º–µ—Ä–∞ –∫–∞–∫ —è–∑—ã–∫: –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ü–µ–Ω", "desc": "Puffin ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#agents", "#transfer_learning", "#robotics", "#games"], "emoji": "üéÆ", "ru": {"title": "–û—Ç –∏–≥—Ä –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ –∫ —Ä–æ–±–æ—Ç–∞–º: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ D2E, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#inference", "#optimization", "#hallucinations"], "emoji": "üìê", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ —É—Å–∏–ª–µ–Ω–∏–µ –∫–∞—Å–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ Tangential Amplifying Guidance (TAG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#optimization", "#training", "#multimodal"], "emoji": "üé®", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Multimodal Prompt Optimizer (MPO) ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –í –æ
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#optimization", "#multimodal", "#alignment"], "emoji": "üì¢", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ AutoPR ‚Äî –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–≤—Ä–∞—â–µ–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –≤ –ø—Ä
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#long_context", "#training", "#reasoning", "#benchmark", "#rl"], "emoji": "üî≠", "ru": {"title": "R-HORIZON: –û–±—É—á–µ–Ω–∏–µ AI –º—ã—Å–ª–∏—Ç—å –Ω–∞ –¥–∞–ª—å–Ω–∏–µ –¥–∏—Å—Ç–∞–Ω—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç R-HORIZON - –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–º—É –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#rl", "#data", "#reasoning", "#optimization"], "emoji": "üîÑ", "ru": {"title": "–û—Ç —Ç–µ–∫—Å—Ç–æ–≤ –∫ –≤–æ–ø—Ä–æ—Å–∞–º: –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ RL –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ Webscale-RL pipeline ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –¥–≤–∏–∂–æ–∫ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –±–æ–ª—å—à–∏
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#data", "#cv", "#training", "#dataset", "#reasoning", "#survey", "#benchmark", "#optimization"], "emoji": "üî≠", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö: –æ—Ç –æ–±—ä–µ–∫—Ç–æ–≤ –¥–æ –≥–æ—Ä–æ–¥—Å–∫–∏—Ö —Å—Ü–µ–Ω", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ SpaceVista-7B ‚Äî –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#training", "#reasoning", "#benchmark", "#optimization", "#open_source", "#multimodal", "#dataset"], "emoji": "üéØ", "ru": {"title": "ARES: –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ AI –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ ARES, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#cv", "#video", "#long_context", "#training", "#multimodal", "#benchmark", "#optimization", "#agents"], "emoji": "üé¨", "ru": {"title": "–ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∏ –±–µ–∑ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ –ø–∞–º—è—Ç–∏", "desc": "StreamingVLM - —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, –∫–æ—Ç–æ—Ä–∞
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#training", "#reasoning", "#benchmark", "#rl", "#optimization", "#rlhf"], "emoji": "üîç", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–∞—è –æ—à–∏–±–∫–∏ –≤ —É—Ä–æ–∫–∏: –∫–∞–∫ –∏–∑–≤–ª–µ—á—å –ø–æ–ª—å–∑—É –∏–∑ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ LENS, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º GRPO –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#low_resource", "#open_source", "#synthetic", "#data", "#training", "#dataset", "#multilingual"], "emoji": "üá∞üá∑", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö LLM —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–µ —Ö—É–∂–µ —Ä–µ–∞–ª—å–Ω—ã—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ KORMo-10B ‚Äî –ø–µ—Ä–≤—É—é –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—É—é –±–∏–ª–∏
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#reasoning", "#training", "#math", "#benchmark", "#architecture"], "emoji": "üß©", "ru": {"title": "–ö–æ–≥–¥–∞ AI —É—á–∏—Ç—Å—è –¥—É–º–∞—Ç—å —Å–ª–æ–∂–Ω–µ–µ, —á–µ–º –µ–≥–æ —É—á–∏–ª–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Complexity Out of Distribution –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ AI-–º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –ö–ª—é
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#3d", "#cv", "#open_source", "#optimization"], "emoji": "üéØ", "ru": {"title": "–ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —É–ø–ª–æ—Ç–Ω–µ–Ω–∏–µ –≥–∞—É—Å—Å–∏–∞–Ω –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è 3D-—Å—Ü–µ–Ω", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PG-Occ ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ Gaussian Transformer –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è 3D occupancy —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π open-vocabulary –∑–∞–ø—Ä–æ—Å–æ–≤.
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#survey", "#reasoning"], "emoji": "üìä", "ru": {"title": "StatEval: –±–µ–Ω—á–º–∞—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∫–∞–∑–∞–ª —Å–ª–∞–±–æ—Å—Ç—å LLM –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç StatEval ‚Äî –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è LLM, –≤–∫–ª—é—á–∞—é—â–∏–π 13,817 –±–∞–∑–æ–≤—ã—Ö –∑–∞
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#benchmark", "#games", "#reasoning", "#multimodal"], "emoji": "üî¨", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è —Å –≥–ª—É–±–æ–∫–∏–º reasoning", "desc": "MRMR ‚Äî —ç—Ç–æ –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ retrieval —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è, —Ç—Ä–µ–±—É—é—â–∏–π –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–≥–æ reasoning. –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç 15
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#data", "#optimization", "#benchmark", "#training"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏—è –º–æ–¥–µ–ª–µ–π –≤–º–µ—Å—Ç–æ –¥–æ—Ä–æ–≥–æ–≥–æ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ DISCO –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö ML-–º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#synthetic", "#training", "#reasoning", "#rl", "#optimization", "#agents"], "emoji": "üéÆ", "ru": {"title": "–£—á–∏–º AI-–∞–≥–µ–Ω—Ç–æ–≤ –¥—É–º–∞—Ç—å –ø–µ—Ä–µ–¥ –¥–µ–π—Å—Ç–≤–∏–µ–º —á–µ—Ä–µ–∑ —Å–∏–º—É–ª—è—Ü–∏—é", "desc": "Dyna-Mind ‚Äî —ç—Ç–æ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è AI-–∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#data", "#dataset", "#multimodal", "#ethics", "#benchmark", "#science"], "emoji": "üîç", "ru": {"title": "AI-—Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç: —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω–æ—Å—Ç—å –º–∞—à–∏–Ω—ã –ø–ª—é—Å —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞ —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω ReviewerToo ‚Äî –º–æ–¥—É–ª—å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è AI-–∞—Å—Å–∏—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ —Ä–µ—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#games", "#dataset", "#multilingual", "#benchmark", "#open_source"], "emoji": "‚öîÔ∏è", "ru": {"title": "–ê—Ä–µ–Ω–∞ –¥–ª—è –∫–æ–¥–∞: –∫—Ä–∞—É–¥—Å–æ—Ä—Å–∏–Ω–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "BigCodeArena ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é –ª—é–¥–µ–π, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#rl", "#training", "#inference"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –ø–∞–º—è—Ç–∏: –Ω–∞—Ö–æ–¥–∏–º –≤–∞–∂–Ω—ã–µ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "Reasoning-–º–æ–¥–µ–ª–∏ —Å —Ü–µ–ø–æ—á–∫–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å–æ–∑–¥–∞—é—Ç –æ–≥—Ä–æ–º–Ω—ã–π KV-–∫—ç—à –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#data", "#training", "#low_resource", "#optimization", "#audio"], "emoji": "üó£Ô∏è", "ru": {"title": "–ö–æ—Ä—Ä–µ–∫—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —É—Ä–æ–≤–Ω—è –æ—à–∏–±–æ–∫ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ (ASR) –±–µ–∑ –∏
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#inference", "#architecture", "#reasoning", "#optimization", "#training"], "emoji": "üîç", "ru": {"title": "–ù–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–µ–π –ª–∞—Ç–µ–Ω—Ç–Ω
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#audio", "#reasoning", "#training", "#multimodal"], "emoji": "üß†", "ru": {"title": "–î—É–º–∞–π –∏ –≥–æ–≤–æ—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ: –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–≤—É—Ö –º–æ–∑–≥–æ–≤ –¥–ª—è —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–≥–æ AI", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Mind-Paced Speaking (MPS) ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ä–∞–±–æ—Ç–æ–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –º–æ–∑–≥–∞, –∫–æ—Ç–æ—Ä—ã–π 
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#rl", "#agents", "#hallucinations", "#reasoning", "#optimization", "#training", "#long_context"], "emoji": "ü¶Ö", "ru": {"title": "EAGLET: —É–º–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è LLM-–∞–≥–µ–Ω—Ç–æ–≤ –±–µ–∑ –ª–∏—à–Ω–∏—Ö –∑–∞—Ç—Ä–∞—Ç", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ plan-and-execute —Å –º–µ—Ç–æ–¥–æ–º EAGLET –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è 
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agi", "#multimodal", "#interpretability", "#open_source"], "emoji": "üîß", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —É –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM", "desc": "PhysToolBench ‚Äî —ç—Ç–æ –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ LLM 
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#agents", "#security", "#benchmark"], "emoji": "üé≠", "ru": {"title": "Prompt injection –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ—Ä—É–∂–∏–µ –ø—Ä–æ—Ç–∏–≤ AI-–º–æ–Ω–∏—Ç–æ—Ä–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞—Ö –∫–æ–Ω—Ç—Ä–æ–ª—è AI, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç LLM-–º–æ–Ω–∏—Ç–æ—Ä—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#rlhf", "#alignment", "#training", "#reasoning"], "emoji": "üéÆ", "ru": {"title": "–ö–æ–≥–¥–∞ LLM –∏–≥—Ä–∞–µ—Ç –≤ –∏–≥—Ä—ã —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º: —Ç–µ–æ—Ä–µ—Ç–∏–∫–æ-–∏–≥—Ä–æ–≤–æ–π alignment", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GTAlign ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ alignment LLM, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ç–µ–æ—Ä–∏–∏ –∏–≥—Ä. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning", "#optimization", "#survey"], "emoji": "üî¨", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ AI-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π: –∫–∞–∫ –∏–∑–º–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DeepResearch-ReportEval ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI-—Å–∏—Å—Ç–µ–º, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –ø—Ä–æ–≤–æ–¥–∏—Ç—å –≥–ª—É–±–æ–∫–∏–µ –∏
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#games", "#multimodal", "#cv"], "emoji": "üß©", "ru": {"title": "–û—Ç —Ü–µ–ª–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫ –ø–∞—Ç—á–∞–º: –Ω–æ–≤–∞—è —ç—Ä–∞ zero-shot –æ–ø–∏—Å–∞–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø–∞—Ä–∞—Ö –∫–∞—Ä—Ç–∏–Ω–∫–∞-—Ç–µ–∫—Å—Ç, —Å–º–µ—Å—Ç–∏–≤ —Ñ–æ–∫—É—Å —Å —Ü–µ–ª–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –æ
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#cv", "#training", "#diffusion", "#optimization", "#architecture"], "emoji": "üéõÔ∏è", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ—Å–æ–≤ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "TC-LoRA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#training", "#optimization"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å AI —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –∫–æ—Ä–æ—á–µ, –Ω–æ –Ω–µ –≥–ª—É–ø–µ–µ", "desc": "–ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) —á–∞—Å—Ç–æ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç ¬´overthinking¬ª ‚Äî –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç –≤—ã—á–∏—Å–ª–∏—Ç
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#inference", "#optimization", "#cv", "#video"], "emoji": "üé®", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ", "desc": "–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Ä–∞–±–æ—Ç–∞—é—Ç –º–µ–¥–ª–µ–Ω–Ω–æ, —Ç–∞–∫ –∫–∞–∫ —Å–æ–∑–¥–∞—é—Ç —Ç–æ–∫–µ–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, —Ç—Ä–µ–±—É—è —Ç—ã—Å—è—á–∏ –ø—Ä–æ—Ö–æ–¥–æ
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#architecture", "#data", "#interpretability"], "emoji": "üîó", "ru": {"title": "–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π —á–µ—Ä–µ–∑ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ –ø—É—Ç—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ ACE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –≤ LLM, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –º–Ω–æ
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#video", "#optimization"], "emoji": "üéØ", "ru": {"title": "–î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –∑–∞–¥–∞—á–∏ –≤–∏–¥–µ–æ—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Tenet –¥–ª—è –∑–∞–¥–∞—á–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é (RVOS). –ê–≤—Ç–æ—Ä—ã –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä—É—é—Ç –∑–∞–¥–∞
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#games", "#cv", "#video", "#dataset", "#inference", "#benchmark", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è 4D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å—Ü–µ–Ω –∏–∑ –æ–±—ã—á–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∑–∞ –º–∏–Ω—É—Ç—ã", "desc": "Instant4D ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω –∏–∑ –Ω–µ–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è —Ä
[13.10.2025 12:22] Using data from previous issue: {"categories": ["#transfer_learning", "#multimodal"], "emoji": "üîÄ", "ru": {"title": "–£—á–∏–º—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö –±–µ–∑ –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UML (Unpaired Multimodal Learner) ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–ø–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è
[13.10.2025 12:22] Renaming data file.
[13.10.2025 12:22] Renaming previous data. hf_papers.json to ./d/2025-10-13.json
[13.10.2025 12:22] Saving new data file.
[13.10.2025 12:22] Generating page.
[13.10.2025 12:22] Renaming previous page.
[13.10.2025 12:22] Renaming previous data. index.html to ./d/2025-10-13.html
[13.10.2025 12:22] Writing result.
[13.10.2025 12:22] Renaming log file.
[13.10.2025 12:22] Renaming previous data. log.txt to ./logs/2025-10-13_last_log.txt
