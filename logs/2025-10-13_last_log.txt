[13.10.2025 08:18] Read previous papers.
[13.10.2025 08:18] Generating top page (month).
[13.10.2025 08:18] Writing top page (month).
[13.10.2025 09:15] Read previous papers.
[13.10.2025 09:15] Get feed.
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08673
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05684
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09201
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04533
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09558
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06499
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08189
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09606
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08457
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09608
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08696
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09517
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09510
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04759
[13.10.2025 09:15] Extract page data from URL. URL: https://huggingface.co/papers/2510.06274
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09577
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08697
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08047
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07745
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09592
[13.10.2025 09:15] Extract page data from URL. URL: https://huggingface.co/papers/2510.09426
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08867
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07959
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05608
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09507
[13.10.2025 09:15] Extract page data from URL. URL: https://huggingface.co/papers/2510.09462
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08872
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07861
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02898
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09561
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08994
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07896
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07319
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09535
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08492
[13.10.2025 09:15] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01119
[13.10.2025 09:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.10.2025 09:15] No deleted papers detected.
[13.10.2025 09:15] Downloading and parsing papers (pdf, html). Total: 36.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.08673.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.08673.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.08673.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.05684.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.05684.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.05684.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.09201.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.09201.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.09201.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.04533.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.04533.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.04533.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.09558.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.09558.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.09558.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.06499.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.06499.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.06499.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.08189.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.08189.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.08189.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.09606.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.09606.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.09606.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.08457.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.08457.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.08457.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.09608.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.09608.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.09608.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.08696.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.08696.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.08696.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.09517.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.09517.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.09517.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.09510.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.09510.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.09510.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.04759.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.04759.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.04759.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.06274.
[13.10.2025 09:15] Downloading paper 2510.06274 from http://arxiv.org/pdf/2510.06274v1...
[13.10.2025 09:15] Extracting affiliations from text.
[13.10.2025 09:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 ] . [ 1 4 7 2 6 0 . 0 1 5 2 : r Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out-of-Distribution Generalization Arman Tahmasebi-Zadeh Department of Computer Engineering Sharif University of Technology Mohamadreza Fereydooni Department of Computer Engineering Sharif University of Technology Mahdi Ghaznavai Department of Computer Engineering Sharif University of Technology Mahdieh Soleymani Baghshah Department of Computer Engineering Sharif University of Technology mm.samiei@sharif.edu arashmarioriyad@gmail.com arman.tahmasebi345@sharif.edu mrezafereydooni@gmail.com mahdi.ghaznavi@ce.sharif.edu soleymani@sharif.edu "
[13.10.2025 09:15] Response: ```python
["Department of Computer Engineering Sharif University of Technology"]
```
[13.10.2025 09:15] Deleting PDF ./assets/pdf/2510.06274.pdf.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.09577.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.09577.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.09577.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.08697.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.08697.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.08697.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.08047.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.08047.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.08047.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.07745.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.07745.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.07745.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.09592.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.09592.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.09592.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.09426.
[13.10.2025 09:15] Downloading paper 2510.09426 from http://arxiv.org/pdf/2510.09426v1...
[13.10.2025 09:15] Extracting affiliations from text.
[13.10.2025 09:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"KORMo: Korean Open Reasoning Model for Everyone Minjun Kim1,4 Hyeonseok Lim1,4 Hangyeol Yoo1,4 Inho Won1 Junghun Yuk1 Changsu Choi1,4 Seungwoo Song1,4 Minkyung Cho1 5 2 0 2 0 ] . [ 1 6 2 4 9 0 . 0 1 5 2 : r Dongjae Shin1,4 Huije Lee2 Hoyun Song1 Alice Oh3 KyungTae Lim1 1KAIST MLP Lab 3KAIST U&I Lab Corresponding Author: ktlim@kaist.ac.kr 2KAIST NLPCL Lab 4SeoulTech "
[13.10.2025 09:15] Response: ```python
["KAIST MLP Lab", "KAIST U&I Lab", "KAIST NLPCL Lab", "SeoulTech"]
```
[13.10.2025 09:15] Deleting PDF ./assets/pdf/2510.09426.pdf.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.08867.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.08867.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.08867.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.07959.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.07959.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.07959.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.05608.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.05608.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.05608.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.09507.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.09507.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.09507.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.09462.
[13.10.2025 09:15] Downloading paper 2510.09462 from http://arxiv.org/pdf/2510.09462v1...
[13.10.2025 09:15] Extracting affiliations from text.
[13.10.2025 09:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Mikhail Terekhov1,2* Alexander Panfilov3,4* Daniil Dzenhaliou2* Caglar Gulcehre2 Maksym Andriushchenko3,4 Ameya Prabhu4,5 Jonas Geiping3,4 1MATS 2EPFL 3ELLIS Institute Tübingen & Max Planck Institute for Intelligent Systems 4Tübingen AI Center 5University of Tübingen 5 2 0 2 0 1 ] . [ 1 2 6 4 9 0 . 0 1 5 2 : r a "
[13.10.2025 09:15] Response: ```python
["MATS", "EPFL", "ELLIS Institute Tübingen & Max Planck Institute for Intelligent Systems", "Tübingen AI Center", "University of Tübingen"]
```
[13.10.2025 09:15] Deleting PDF ./assets/pdf/2510.09462.pdf.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.08872.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.08872.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.08872.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.07861.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.07861.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.07861.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.02898.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.02898.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.02898.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.09561.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.09561.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.09561.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.08994.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.08994.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.08994.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.07896.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.07896.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.07896.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.07319.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.07319.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.07319.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.09535.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.09535.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.09535.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.08492.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.08492.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.08492.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Downloading and parsing paper https://huggingface.co/papers/2510.01119.
[13.10.2025 09:15] Extra JSON file exists (./assets/json/2510.01119.json), skip PDF parsing.
[13.10.2025 09:15] Paper image links file exists (./assets/img_data/2510.01119.json), skip HTML parsing.
[13.10.2025 09:15] Success.
[13.10.2025 09:15] Enriching papers with extra data.
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 0. Puffin, a unified multimodal model, integrates language regression and diffusion-based generation to enhance camera-centric spatial understanding and generation by treating camera parameters as language.  					AI-generated summary 				 Camera-centric understanding and generation are two cornerstones...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 1. D2E framework uses desktop interactions to pretrain embodied AI, achieving high success rates in physical manipulation and navigation tasks.  					AI-generated summary 				 Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physic...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 2. Multimodal Prompt Optimizer (MPO) extends prompt optimization to handle multiple data types, improving performance over text-only methods in various applications.  					AI-generated summary 				 Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) furth...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 3. Tangential Amplifying Guidance (TAG) improves diffusion model sample quality by directly amplifying tangential components of estimated scores without modifying the model architecture.  					AI-generated summary 				 Recent diffusion models achieve the state-of-the-art performance in image generation...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 4. AutoPR, a multi-agent framework, automates the promotion of research papers by transforming them into engaging public content, significantly improving engagement metrics compared to direct LLM pipelines.  					AI-generated summary 				 As the volume of peer-reviewed research surges, scholars increas...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 5. A scalable data engine converts large-scale pre-training documents into diverse question-answer pairs for reinforcement learning, significantly improving model performance and efficiency.  					AI-generated summary 				 Large Language Models (LLMs) have achieved remarkable success through imitation ...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 6. R-HORIZON, a method using query composition, improves long-horizon reasoning in Large Reasoning Models through a benchmark of complex multi-step tasks, enhancing performance and accuracy.  					AI-generated summary 				 Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSe...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 7. A spatial reasoning model using scale-aware experts and progressive rewards demonstrates competitive performance across diverse tasks and scales using a large, curated dataset.  					AI-generated summary 				 With the current surge in spatial reasoning explorations, researchers have made significant...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 8. ARES, a unified framework for adaptive reasoning, dynamically adjusts exploration effort based on task difficulty using high window-entropy tokens and hierarchical entropy rewards, improving performance and efficiency across various benchmarks.  					AI-generated summary 				 Recent advances in mult...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 9. StreamingVLM is a real-time vision-language model that efficiently processes infinite video streams using a compact KV cache and supervised fine-tuning, achieving high performance on long videos and diverse benchmarks.  					AI-generated summary 				 Vision-language models (VLMs) could power real-ti...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 10. LENS modifies GRPO by assigning confidence-dependent rewards to incorrect responses, improving efficiency and performance in reinforcement learning with verifiable rewards.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) has become a standard recipe for improvin...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 11. StatEval is a comprehensive benchmark for statistical reasoning, covering foundational and research-level problems, and highlights the limitations of current LLMs in this domain.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable advances in mathematical and lo...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 12. MRMR is a benchmark for expert-level multidisciplinary multimodal retrieval that includes reasoning-intensive tasks, contradiction retrieval, and image-text interleaved sequences, highlighting the need for improved multimodal models.  					AI-generated summary 				 We introduce MRMR, the first exper...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 13. PG-Occ, a Progressive Gaussian Transformer Framework, enhances 3D occupancy prediction with progressive densification and anisotropy-aware sampling, achieving state-of-the-art performance.  					AI-generated summary 				 The 3D occupancy prediction task has witnessed remarkable progress in recent ye...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 14. A framework called Complexity Out of Distribution (Complexity OoD) is proposed to define and measure reasoning ability in AI models by evaluating their performance on test instances requiring higher solution complexity than training examples.  					AI-generated summary 				 Recent progress has pushe...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 15. Introducing Dyna-Mind, a two-stage training framework that enhances AI agents' reasoning and planning abilities through simulation, leading to improved performance in complex interactive environments.  					AI-generated summary 				 Reasoning models have recently shown remarkable progress in domains...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 16. BigCodeArena is an open human evaluation platform for code generation that enables real-time execution and interaction, revealing preferences and capabilities of LLMs in coding tasks.  					AI-generated summary 				 Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time eva...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 17. A parameter-space correction method reduces Word Error Rate in ASR systems by addressing pseudo-label biases without target ground truth.  					AI-generated summary 				 Robust ASR under domain shift is crucial because real-world systems encounter unseen accents and domains with limited labeled data...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 18. Parallel test-time scaling is enabled for latent reasoning models using uncertainty-inspired sampling strategies and a Latent Reward Model for effective trajectory selection.  					AI-generated summary 				 Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (L...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 19. Mind-Paced Speaking (MPS) is a brain-inspired framework that enables real-time reasoning and fluent speech generation by dividing the process into a "Formulation Brain" for reasoning and an "Articulation Brain" for speech, achieving high accuracy with low latency.  					AI-generated summary 				 Rea...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 20. A large-scale investigation into constructing a fully open bilingual LLM for Korean using synthetic data demonstrates that such data can sustain pretraining and achieve performance comparable to multilingual baselines.  					AI-generated summary 				 This work presents the first large-scale investig...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 21. ReviewerToo, a modular AI-assisted peer review framework, complements human judgment with systematic assessments, achieving high accuracy and quality in specific domains while highlighting areas where human expertise remains essential.  					AI-generated summary 				 Peer review is the cornerstone o...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 22. A method called DISCO selects samples with the greatest model disagreements to predict performance, achieving state-of-the-art results across various benchmarks with reduced computational cost.  					AI-generated summary 				 Evaluating modern machine learning models has become prohibitively expensi...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 23. A plan-and-execute framework with EAGLET enhances LLM-based agents' planning abilities, achieving state-of-the-art performance in long-horizon tasks with reduced training costs.  					AI-generated summary 				 Agents based on large language models (LLMs) struggle with brainless trial-and-error and g...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 24. PhysToolBench evaluates MLLMs' comprehension of physical tools through a VQA dataset, revealing significant deficiencies in tool understanding.  					AI-generated summary 				 The ability to use, understand, and create tools is a hallmark of human intelligence, enabling sophisticated interaction wit...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 25. Adaptive attacks on AI control protocols using prompt injections can evade monitors and complete malicious tasks, highlighting a significant vulnerability in current security mechanisms.  					AI-generated summary 				 AI control protocols serve as a defense mechanism to stop untrusted LLM agents fr...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 26. Game-Theoretic Alignment (GTAlign) improves Large Language Model (LLM) performance by integrating game-theoretic decision making into reasoning and training, enhancing efficiency, answer quality, and mutual welfare.  					AI-generated summary 				 Large Language Models (LLMs) have achieved remarkabl...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 27. A framework evaluates DeepResearch systems by assessing the quality, redundancy, and factuality of their research reports using an LLM-as-a-Judge methodology.  					AI-generated summary 				 DeepResearch agents represent a transformative AI paradigm, conducting expert-level research through sophisti...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 28. A patch-centric framework for zero-shot captioning achieves state-of-the-art performance by using dense visual features from models like DINO to caption arbitrary image regions.  					AI-generated summary 				 Zero-shot captioners are recently proposed models that utilize common-space vision-languag...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 29. TC-LoRA enhances generative fidelity and adherence to spatial conditions by dynamically conditioning model weights through a hypernetwork, improving upon static activation-based methods in diffusion models.  					AI-generated summary 				 Current controllable diffusion models typically rely on fixed...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 30. Speculative Jacobi-Denoising Decoding accelerates autoregressive text-to-image generation by enabling parallel token prediction and reducing model forward passes.  					AI-generated summary 				 As a new paradigm of visual content generation, autoregressive text-to-image models suffer from slow infe...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 31. ACE, a framework using neuron-level attribution, enhances multi-hop factual recall in LLMs by editing critical query-value pathways, outperforming existing methods.  					AI-generated summary 				 Large Language Models (LLMs) require efficient knowledge editing (KE) to update factual information, ye...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 32. The Tenet framework decomposes the RVOS task into referring, video, and segmentation factors, using temporal prompts and prompt preference learning to adapt image-based foundation segmentation models for efficient RVOS.  					AI-generated summary 				 Referring Video Object Segmentation (RVOS) aims ...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 33. Group Relative Segment Penalization (GRSP) improves token efficiency in large reasoning models without significantly reducing accuracy, especially for complex problems, by regularizing reasoning at the step level.  					AI-generated summary 				 Large reasoning models (LRMs) boosted by Reinforcement...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 34. UML, an unpaired multimodal learner, enhances representation learning in a target modality by leveraging auxiliary unpaired data from different modalities.  					AI-generated summary 				 Traditional multimodal learners find unified representations for tasks like visual question answering, but rely ...
[13.10.2025 09:15] ********************************************************************************
[13.10.2025 09:15] Abstract 35. Instant4D uses deep visual SLAM and a 4D Gaussian representation to efficiently reconstruct scenes from uncalibrated video sequences in minutes.  					AI-generated summary 				 Dynamic view synthesis has seen significant advances, yet reconstructing scenes from uncalibrated, casual video remains cha...
[13.10.2025 09:15] Read previous papers.
[13.10.2025 09:15] Generating reviews via LLM API.
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#dataset", "#multimodal", "#benchmark", "#alignment", "#open_source"], "emoji": "📸", "ru": {"title": "Камера как язык: единая модель для понимания и генерации сцен", "desc": "Puffin — это мультимодальная модель, которая объединяет понимание и генерацию изображен
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#agents", "#transfer_learning", "#robotics", "#games"], "emoji": "🎮", "ru": {"title": "От игр на компьютере к роботам: новый подход к обучению ИИ", "desc": "Исследователи разработали фреймворк D2E, который использует данные взаимодействия че
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#optimization", "#training", "#multimodal"], "emoji": "🎨", "ru": {"title": "Оптимизация промптов для всех модальностей одновременно", "desc": "Статья представляет Multimodal Prompt Optimizer (MPO) — новый подход к оптимизации промптов для мультимодальных языковых моделей (MLLM). В о
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#inference", "#optimization", "#hallucinations"], "emoji": "📐", "ru": {"title": "Улучшение генерации через усиление касательных компонент", "desc": "Статья предлагает метод Tangential Amplifying Guidance (TAG) для улучшения качества генерации диффузионных моделе
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#optimization", "#multimodal", "#alignment"], "emoji": "📢", "ru": {"title": "Автоматическое продвижение научных статей с помощью мультиагентной системы", "desc": "Исследователи представили AutoPR — новую задачу автоматического превращения научных статей в пр
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#rl", "#data", "#reasoning", "#optimization"], "emoji": "🔄", "ru": {"title": "От текстов к вопросам: масштабирование RL для языковых моделей", "desc": "Исследователи разработали Webscale-RL pipeline — масштабируемый движок данных, который преобразует больши
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#long_context", "#training", "#reasoning", "#benchmark", "#rl"], "emoji": "🔭", "ru": {"title": "R-HORIZON: Обучение AI мыслить на дальние дистанции", "desc": "Статья представляет R-HORIZON - метод для оценки и улучшения способности больших языковых моделей к долгосрочному многоэтапн
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#data", "#cv", "#training", "#dataset", "#reasoning", "#survey", "#benchmark", "#optimization"], "emoji": "🔭", "ru": {"title": "Пространственное мышление на всех масштабах: от объектов до городских сцен", "desc": "Исследователи представили SpaceVista-7B — модель для пространственног
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#training", "#reasoning", "#benchmark", "#optimization", "#open_source", "#multimodal", "#dataset"], "emoji": "🎯", "ru": {"title": "ARES: Адаптивное мышление AI в зависимости от сложности задачи", "desc": "Статья представляет фреймворк ARES, который решает проблему неэффективного ра
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#cv", "#video", "#long_context", "#training", "#multimodal", "#benchmark", "#optimization", "#agents"], "emoji": "🎬", "ru": {"title": "Бесконечные видеопотоки без перегрузки памяти", "desc": "StreamingVLM - это модель для обработки бесконечных видеопотоков в реальном времени, котора
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#training", "#reasoning", "#benchmark", "#rl", "#optimization", "#rlhf"], "emoji": "🔍", "ru": {"title": "Превращая ошибки в уроки: как извлечь пользу из неправильных ответов LLM", "desc": "Статья представляет метод LENS, который улучшает алгоритм GRPO для обучения языковых моделей н
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#survey", "#reasoning"], "emoji": "📊", "ru": {"title": "StatEval: бенчмарк, который показал слабость LLM в статистике", "desc": "Статья представляет StatEval — первый комплексный бенчмарк для оценки статистического мышления LLM, включающий 13,817 базовых за
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#benchmark", "#games", "#reasoning", "#multimodal"], "emoji": "🔬", "ru": {"title": "Мультимодальный поиск экспертного уровня с глубоким reasoning", "desc": "MRMR — это первый бенчмарк для мультимодального retrieval экспертного уровня, требующий интенсивного reasoning. Он содержит 15
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#3d", "#cv", "#open_source", "#optimization"], "emoji": "🎯", "ru": {"title": "Прогрессивное уплотнение гауссиан для понимания 3D-сцен", "desc": "Статья представляет PG-Occ — фреймворк на основе Gaussian Transformer для предсказания 3D occupancy с поддержкой open-vocabulary запросов.
[13.10.2025 09:15] Querying the API.
[13.10.2025 09:15] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A framework called Complexity Out of Distribution (Complexity OoD) is proposed to define and measure reasoning ability in AI models by evaluating their performance on test instances requiring higher solution complexity than training examples.  					AI-generated summary 				 Recent progress has pushed AI frontiers from pattern recognition tasks toward problems that require step by step, System2 style reasoning, especially with large language models. Yet, unlike learning, where generalization and out of distribution (OoD) evaluation concepts are well formalized, there is no clear, consistent definition or metric for reasoning ability. We propose Complexity Out of Distribution (Complexity OoD) generalization as a framework and problem setting to define and measure reasoning. A model exhibits Complexity OoD generalization when it maintains performance on test instances whose minimal required solution complexity, either representational (richer solution structure) or computational (more reasoning steps/program length), exceeds that of all training examples. We formalize complexity via solution description Kolmogorov complexity and operational proxies (e.g., object/relation counts; reasoning step counts), clarifying how Complexity OoD differs from length and compositional OoD. This lens unifies learning and reasoning: many cases solvable with System1 like processing at low complexity become System2 like under complexity pressure, while System2 can be viewed as generalization over solution structures. We translate this perspective into practice with recommendations for operationalizing Complexity OoD across the stack: incorporating complexity into benchmark and evaluation metric design, rethinking supervision to target solution traces, seeking and designing inductive biases for Complexity OoD generalization, addressing learning to reason spillovers such as spurious shortcuts, semantic robustness, catastrophic forgetting, and step wise calibration. Because Complexity OoD cannot be solved by scaling data alone, progress toward robust reasoning will require architectures and training regimes that explicitly model and allocate computation with respect to complexity.
[13.10.2025 09:15] Response: ```json
{
  "desc": "Авторы предлагают фреймворк Complexity Out of Distribution для определения и измерения способности AI-моделей к рассуждению. Ключевая идея: модель демонстрирует настоящее reasoning, когда она справляется с задачами, требующими более сложных решений (больше шагов или более богатой структуры), чем любые примеры из обучающей выборки. Сложность формализуется через колмогоровскую сложность и практические метрики вроде количества объектов, отношений или шагов рассуждения. Исследователи утверждают, что масштабирование данных не решит проблему - нужны новые архитектуры и методы обучения, явно моделирующие вычислительную сложность.",
  "emoji": "🧩",
  "title": "Когда AI учится думать сложнее, чем его учили"
}
```
[13.10.2025 09:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework called Complexity Out of Distribution (Complexity OoD) is proposed to define and measure reasoning ability in AI models by evaluating their performance on test instances requiring higher solution complexity than training examples.  					AI-generated summary 				 Recent progress has pushed AI frontiers from pattern recognition tasks toward problems that require step by step, System2 style reasoning, especially with large language models. Yet, unlike learning, where generalization and out of distribution (OoD) evaluation concepts are well formalized, there is no clear, consistent definition or metric for reasoning ability. We propose Complexity Out of Distribution (Complexity OoD) generalization as a framework and problem setting to define and measure reasoning. A model exhibits Complexity OoD generalization when it maintains performance on test instances whose minimal required solution complexity, either representational (richer solution structure) or computational (more reasoning steps/program length), exceeds that of all training examples. We formalize complexity via solution description Kolmogorov complexity and operational proxies (e.g., object/relation counts; reasoning step counts), clarifying how Complexity OoD differs from length and compositional OoD. This lens unifies learning and reasoning: many cases solvable with System1 like processing at low complexity become System2 like under complexity pressure, while System2 can be viewed as generalization over solution structures. We translate this perspective into practice with recommendations for operationalizing Complexity OoD across the stack: incorporating complexity into benchmark and evaluation metric design, rethinking supervision to target solution traces, seeking and designing inductive biases for Complexity OoD generalization, addressing learning to reason spillovers such as spurious shortcuts, semantic robustness, catastrophic forgetting, and step wise calibration. Because Complexity OoD cannot be solved by scaling data alone, progress toward robust reasoning will require architectures and training regimes that explicitly model and allocate computation with respect to complexity."

[13.10.2025 09:15] Response: ```python
["BENCHMARK", "MATH", "ARCHITECTURE", "TRAINING"]
```
[13.10.2025 09:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework called Complexity Out of Distribution (Complexity OoD) is proposed to define and measure reasoning ability in AI models by evaluating their performance on test instances requiring higher solution complexity than training examples.  					AI-generated summary 				 Recent progress has pushed AI frontiers from pattern recognition tasks toward problems that require step by step, System2 style reasoning, especially with large language models. Yet, unlike learning, where generalization and out of distribution (OoD) evaluation concepts are well formalized, there is no clear, consistent definition or metric for reasoning ability. We propose Complexity Out of Distribution (Complexity OoD) generalization as a framework and problem setting to define and measure reasoning. A model exhibits Complexity OoD generalization when it maintains performance on test instances whose minimal required solution complexity, either representational (richer solution structure) or computational (more reasoning steps/program length), exceeds that of all training examples. We formalize complexity via solution description Kolmogorov complexity and operational proxies (e.g., object/relation counts; reasoning step counts), clarifying how Complexity OoD differs from length and compositional OoD. This lens unifies learning and reasoning: many cases solvable with System1 like processing at low complexity become System2 like under complexity pressure, while System2 can be viewed as generalization over solution structures. We translate this perspective into practice with recommendations for operationalizing Complexity OoD across the stack: incorporating complexity into benchmark and evaluation metric design, rethinking supervision to target solution traces, seeking and designing inductive biases for Complexity OoD generalization, addressing learning to reason spillovers such as spurious shortcuts, semantic robustness, catastrophic forgetting, and step wise calibration. Because Complexity OoD cannot be solved by scaling data alone, progress toward robust reasoning will require architectures and training regimes that explicitly model and allocate computation with respect to complexity."

[13.10.2025 09:15] Response: ```python
["REASONING"]
```
[13.10.2025 09:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces a new framework called Complexity Out of Distribution (Complexity OoD) to assess the reasoning capabilities of AI models. It focuses on evaluating how well models perform on complex tasks that require more sophisticated reasoning than what they encountered during training. The authors define complexity using concepts like Kolmogorov complexity and propose metrics to measure both representational and computational complexity. This framework aims to bridge the gap between learning and reasoning, suggesting that models need to be designed to handle complexity explicitly rather than relying solely on data scaling.","title":"Measuring AI Reasoning: Complexity Out of Distribution"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces a new framework called Complexity Out of Distribution (Complexity OoD) to assess the reasoning capabilities of AI models. It focuses on evaluating how well models perform on complex tasks that require more sophisticated reasoning than what they encountered during training. The authors define complexity using concepts like Kolmogorov complexity and propose metrics to measure both representational and computational complexity. This framework aims to bridge the gap between learning and reasoning, suggesting that models need to be designed to handle complexity explicitly rather than relying solely on data scaling.', title='Measuring AI Reasoning: Complexity Out of Distribution'))
[13.10.2025 09:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种名为复杂性超出分布（Complexity OoD）的框架，用于定义和测量人工智能模型的推理能力。该框架通过评估模型在需要比训练示例更高解决复杂性的测试实例上的表现来进行评估。我们通过解决描述的Kolmogorov复杂性和操作性代理来形式化复杂性，明确复杂性超出分布与长度和组合超出分布的区别。为了实现复杂性超出分布的推广，本文还提出了在基准和评估指标设计中融入复杂性的建议，以及重新思考监督以针对解决方案轨迹。","title":"推理能力的新标准：复杂性超出分布"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种名为复杂性超出分布（Complexity OoD）的框架，用于定义和测量人工智能模型的推理能力。该框架通过评估模型在需要比训练示例更高解决复杂性的测试实例上的表现来进行评估。我们通过解决描述的Kolmogorov复杂性和操作性代理来形式化复杂性，明确复杂性超出分布与长度和组合超出分布的区别。为了实现复杂性超出分布的推广，本文还提出了在基准和评估指标设计中融入复杂性的建议，以及重新思考监督以针对解决方案轨迹。', title='推理能力的新标准：复杂性超出分布'))
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#synthetic", "#training", "#reasoning", "#rl", "#optimization", "#agents"], "emoji": "🎮", "ru": {"title": "Учим AI-агентов думать перед действием через симуляцию", "desc": "Dyna-Mind — это двухэтапный фреймворк для обучения AI-агентов, который улучшает их способность к рассуждению и
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#games", "#dataset", "#multilingual", "#benchmark", "#open_source"], "emoji": "⚔️", "ru": {"title": "Арена для кода: краудсорсинговая оценка способностей LLM в программировании", "desc": "BigCodeArena — это открытая платформа для оценки генерации кода с помощью людей, которая позвол
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#data", "#training", "#low_resource", "#optimization", "#audio"], "emoji": "🗣️", "ru": {"title": "Коррекция параметров для улучшения распознавания речи", "desc": "В статье рассматривается метод коррекции параметров для снижения уровня ошибок в системах распознавания речи (ASR) без и
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#inference", "#architecture", "#reasoning", "#optimization", "#training"], "emoji": "🔍", "ru": {"title": "Новые горизонты масштабирования в латентных моделях рассуждения", "desc": "В статье рассматривается метод параллельного масштабирования во время тестирования для моделей латентн
[13.10.2025 09:15] Using data from previous issue: {"categories": ["#audio", "#reasoning", "#training", "#multimodal"], "emoji": "🧠", "ru": {"title": "Думай и говори одновременно: архитектура двух мозгов для разговорного AI", "desc": "Исследователи представили Mind-Paced Speaking (MPS) — фреймворк, вдохновлённый работой человеческого мозга, который 
[13.10.2025 09:15] Querying the API.
[13.10.2025 09:15] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A large-scale investigation into constructing a fully open bilingual LLM for Korean using synthetic data demonstrates that such data can sustain pretraining and achieve performance comparable to multilingual baselines.  					AI-generated summary 				 This work presents the first large-scale investigation into constructing a fully open bilingual large language model (LLM) for a non-English language, specifically Korean, trained predominantly on synthetic data. We introduce KORMo-10B, a 10.8B-parameter model trained from scratch on a Korean-English corpus in which 68.74% of the Korean portion is synthetic. Through systematic experimentation, we demonstrate that synthetic data, when carefully curated with balanced linguistic coverage and diverse instruction styles, does not cause instability or degradation during large-scale pretraining. Furthermore, the model achieves performance comparable to that of contemporary open-weight multilingual baselines across a wide range of reasoning, knowledge, and instruction-following benchmarks. Our experiments reveal two key findings: (1) synthetic data can reliably sustain long-horizon pretraining without model collapse, and (2) bilingual instruction tuning enables near-native reasoning and discourse coherence in Korean. By fully releasing all components including data, code, training recipes, and logs, this work establishes a transparent framework for developing synthetic data-driven fully open models (FOMs) in low-resource settings and sets a reproducible precedent for future multilingual LLM research.
[13.10.2025 09:16] Response: ```json
{
  "desc": "Исследователи создали KORMo-10B — первую полностью открытую билингвальную языковую модель для корейского и английского языков с 10.8 миллиардами параметров. Модель обучалась преимущественно на синтетических данных, при этом 68.74% корейской части корпуса было сгенерировано искусственно. Эксперименты показали, что качественные синтетические данные не вызывают нестабильности при длительном pretraining и позволяют достичь производительности, сравнимой с современными мультиязычными baseline моделями. Все компоненты проекта — данные, код, рецепты обучения и логи — полностью открыты, что создаёт прозрачную основу для разработки LLM в условиях ограниченных ресурсов.",
  "emoji": "🇰🇷",
  "title": "Синтетические данные для обучения билингвальных LLM работают не хуже реальных"
}
```
[13.10.2025 09:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A large-scale investigation into constructing a fully open bilingual LLM for Korean using synthetic data demonstrates that such data can sustain pretraining and achieve performance comparable to multilingual baselines.  					AI-generated summary 				 This work presents the first large-scale investigation into constructing a fully open bilingual large language model (LLM) for a non-English language, specifically Korean, trained predominantly on synthetic data. We introduce KORMo-10B, a 10.8B-parameter model trained from scratch on a Korean-English corpus in which 68.74% of the Korean portion is synthetic. Through systematic experimentation, we demonstrate that synthetic data, when carefully curated with balanced linguistic coverage and diverse instruction styles, does not cause instability or degradation during large-scale pretraining. Furthermore, the model achieves performance comparable to that of contemporary open-weight multilingual baselines across a wide range of reasoning, knowledge, and instruction-following benchmarks. Our experiments reveal two key findings: (1) synthetic data can reliably sustain long-horizon pretraining without model collapse, and (2) bilingual instruction tuning enables near-native reasoning and discourse coherence in Korean. By fully releasing all components including data, code, training recipes, and logs, this work establishes a transparent framework for developing synthetic data-driven fully open models (FOMs) in low-resource settings and sets a reproducible precedent for future multilingual LLM research."

[13.10.2025 09:16] Response: ```python
['DATASET', 'DATA', 'MULTILINGUAL', 'TRAINING']
```
[13.10.2025 09:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A large-scale investigation into constructing a fully open bilingual LLM for Korean using synthetic data demonstrates that such data can sustain pretraining and achieve performance comparable to multilingual baselines.  					AI-generated summary 				 This work presents the first large-scale investigation into constructing a fully open bilingual large language model (LLM) for a non-English language, specifically Korean, trained predominantly on synthetic data. We introduce KORMo-10B, a 10.8B-parameter model trained from scratch on a Korean-English corpus in which 68.74% of the Korean portion is synthetic. Through systematic experimentation, we demonstrate that synthetic data, when carefully curated with balanced linguistic coverage and diverse instruction styles, does not cause instability or degradation during large-scale pretraining. Furthermore, the model achieves performance comparable to that of contemporary open-weight multilingual baselines across a wide range of reasoning, knowledge, and instruction-following benchmarks. Our experiments reveal two key findings: (1) synthetic data can reliably sustain long-horizon pretraining without model collapse, and (2) bilingual instruction tuning enables near-native reasoning and discourse coherence in Korean. By fully releasing all components including data, code, training recipes, and logs, this work establishes a transparent framework for developing synthetic data-driven fully open models (FOMs) in low-resource settings and sets a reproducible precedent for future multilingual LLM research."

[13.10.2025 09:16] Response: ```python
['SYNTHETIC', 'OPEN_SOURCE', 'LOW_RESOURCE']
```
[13.10.2025 09:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores the creation of a fully open bilingual large language model (LLM) for Korean, named KORMo-10B, which is trained mainly on synthetic data. The research shows that synthetic data, when well-curated, can effectively support pretraining without causing instability in the model. The model\'s performance is on par with existing multilingual models, demonstrating its capability in reasoning and instruction-following tasks. By sharing all components of the project, this work provides a clear framework for developing similar models in low-resource languages using synthetic data.","title":"Building a Bilingual LLM for Korean with Synthetic Data Success!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper explores the creation of a fully open bilingual large language model (LLM) for Korean, named KORMo-10B, which is trained mainly on synthetic data. The research shows that synthetic data, when well-curated, can effectively support pretraining without causing instability in the model. The model's performance is on par with existing multilingual models, demonstrating its capability in reasoning and instruction-following tasks. By sharing all components of the project, this work provides a clear framework for developing similar models in low-resource languages using synthetic data.", title='Building a Bilingual LLM for Korean with Synthetic Data Success!'))
[13.10.2025 09:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本研究首次大规模探讨了如何构建一个完全开放的双语大型语言模型（LLM），专注于非英语语言——韩语。我们介绍了KORMo-10B，这是一个拥有10.8亿参数的模型，主要基于合成数据进行训练，其中68.74%的韩语部分为合成数据。通过系统实验，我们证明了经过精心策划的合成数据能够在大规模预训练中保持稳定性，并且在推理、知识和指令遵循等基准测试中表现出与现有多语言基线相当的性能。我们的研究结果表明，合成数据可以可靠地支持长期预训练，而双语指令调优则使得模型在韩语中的推理和话语连贯性接近母语水平。","title":"合成数据驱动的开放双语模型构建新纪元"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本研究首次大规模探讨了如何构建一个完全开放的双语大型语言模型（LLM），专注于非英语语言——韩语。我们介绍了KORMo-10B，这是一个拥有10.8亿参数的模型，主要基于合成数据进行训练，其中68.74%的韩语部分为合成数据。通过系统实验，我们证明了经过精心策划的合成数据能够在大规模预训练中保持稳定性，并且在推理、知识和指令遵循等基准测试中表现出与现有多语言基线相当的性能。我们的研究结果表明，合成数据可以可靠地支持长期预训练，而双语指令调优则使得模型在韩语中的推理和话语连贯性接近母语水平。', title='合成数据驱动的开放双语模型构建新纪元'))
[13.10.2025 09:16] Using data from previous issue: {"categories": ["#data", "#dataset", "#multimodal", "#ethics", "#benchmark", "#science"], "emoji": "🔍", "ru": {"title": "AI-рецензент: систематичность машины плюс экспертиза человека", "desc": "В статье представлен ReviewerToo — модульный фреймворк для AI-ассистированного научного рецензирования, ко
[13.10.2025 09:16] Using data from previous issue: {"categories": ["#data", "#optimization", "#benchmark", "#training"], "emoji": "🎯", "ru": {"title": "Умная выборка через разногласия моделей вместо дорогого полного тестирования", "desc": "Статья представляет метод DISCO для эффективной оценки современных ML-моделей, который значительно снижает вычи
[13.10.2025 09:16] Using data from previous issue: {"categories": ["#rl", "#agents", "#hallucinations", "#reasoning", "#optimization", "#training", "#long_context"], "emoji": "🦅", "ru": {"title": "EAGLET: умный планировщик для LLM-агентов без лишних затрат", "desc": "Исследователи предложили фреймворк plan-and-execute с методом EAGLET для улучшения 
[13.10.2025 09:16] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agi", "#multimodal", "#interpretability", "#open_source"], "emoji": "🔧", "ru": {"title": "Проверка понимания физических инструментов у мультимодальных LLM", "desc": "PhysToolBench — это первый бенчмарк для оценки того, насколько хорошо мультимодальные LLM 
[13.10.2025 09:16] Querying the API.
[13.10.2025 09:16] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Adaptive attacks on AI control protocols using prompt injections can evade monitors and complete malicious tasks, highlighting a significant vulnerability in current security mechanisms.  					AI-generated summary 				 AI control protocols serve as a defense mechanism to stop untrusted LLM agents from causing harm in autonomous settings. Prior work treats this as a security problem, stress testing with exploits that use the deployment context to subtly complete harmful side tasks, such as backdoor insertion. In practice, most AI control protocols are fundamentally based on LLM monitors, which can become a central point of failure. We study adaptive attacks by an untrusted model that knows the protocol and the monitor model, which is plausible if the untrusted model was trained with a later knowledge cutoff or can search for this information autonomously. We instantiate a simple adaptive attack vector by which the attacker embeds publicly known or zero-shot prompt injections in the model outputs. Using this tactic, frontier models consistently evade diverse monitors and complete malicious tasks on two main AI control benchmarks. The attack works universally against current protocols that rely on a monitor. Furthermore, the recent Defer-to-Resample protocol even backfires, as its resampling amplifies the prompt injection and effectively reframes it as a best-of-n attack. In general, adaptive attacks on monitor models represent a major blind spot in current control protocols and should become a standard component of evaluations for future AI control mechanisms.
[13.10.2025 09:16] Response: ```json
{
  "desc": "Исследование выявило критическую уязвимость в протоколах контроля AI, которые используют LLM-мониторы для предотвращения вредоносных действий автономных агентов. Атакующие модели, знающие архитектуру защиты, могут внедрять prompt injection в свои ответы и успешно обходить мониторинг. Эксперименты показали, что передовые модели систематически уклоняются от различных типов мониторов на двух основных бенчмарках AI control. Особенно опасен протокол Defer-to-Resample, который парадоксально усиливает атаку через повторную выборку, превращая её в best-of-n attack.",
  "emoji": "🎭",
  "title": "Prompt injection как универсальное оружие против AI-мониторов"
}
```
[13.10.2025 09:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Adaptive attacks on AI control protocols using prompt injections can evade monitors and complete malicious tasks, highlighting a significant vulnerability in current security mechanisms.  					AI-generated summary 				 AI control protocols serve as a defense mechanism to stop untrusted LLM agents from causing harm in autonomous settings. Prior work treats this as a security problem, stress testing with exploits that use the deployment context to subtly complete harmful side tasks, such as backdoor insertion. In practice, most AI control protocols are fundamentally based on LLM monitors, which can become a central point of failure. We study adaptive attacks by an untrusted model that knows the protocol and the monitor model, which is plausible if the untrusted model was trained with a later knowledge cutoff or can search for this information autonomously. We instantiate a simple adaptive attack vector by which the attacker embeds publicly known or zero-shot prompt injections in the model outputs. Using this tactic, frontier models consistently evade diverse monitors and complete malicious tasks on two main AI control benchmarks. The attack works universally against current protocols that rely on a monitor. Furthermore, the recent Defer-to-Resample protocol even backfires, as its resampling amplifies the prompt injection and effectively reframes it as a best-of-n attack. In general, adaptive attacks on monitor models represent a major blind spot in current control protocols and should become a standard component of evaluations for future AI control mechanisms."

[13.10.2025 09:16] Response: ```python
['AGENTS', 'BENCHMARK', 'SECURITY']
```
[13.10.2025 09:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Adaptive attacks on AI control protocols using prompt injections can evade monitors and complete malicious tasks, highlighting a significant vulnerability in current security mechanisms.  					AI-generated summary 				 AI control protocols serve as a defense mechanism to stop untrusted LLM agents from causing harm in autonomous settings. Prior work treats this as a security problem, stress testing with exploits that use the deployment context to subtly complete harmful side tasks, such as backdoor insertion. In practice, most AI control protocols are fundamentally based on LLM monitors, which can become a central point of failure. We study adaptive attacks by an untrusted model that knows the protocol and the monitor model, which is plausible if the untrusted model was trained with a later knowledge cutoff or can search for this information autonomously. We instantiate a simple adaptive attack vector by which the attacker embeds publicly known or zero-shot prompt injections in the model outputs. Using this tactic, frontier models consistently evade diverse monitors and complete malicious tasks on two main AI control benchmarks. The attack works universally against current protocols that rely on a monitor. Furthermore, the recent Defer-to-Resample protocol even backfires, as its resampling amplifies the prompt injection and effectively reframes it as a best-of-n attack. In general, adaptive attacks on monitor models represent a major blind spot in current control protocols and should become a standard component of evaluations for future AI control mechanisms."

[13.10.2025 09:16] Response: ```python
["SECURITY"]
```
[13.10.2025 09:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the vulnerabilities in AI control protocols that are designed to prevent harmful actions by untrusted language models (LLMs). It highlights how adaptive attacks, where an attacker understands the control protocol and the monitoring system, can successfully bypass security measures. The authors demonstrate that these attacks can use prompt injections to manipulate outputs, allowing malicious tasks to be completed without detection. The findings suggest that current security mechanisms are inadequate and need to incorporate defenses against such adaptive attacks in future evaluations.","title":"Exposing Vulnerabilities: Adaptive Attacks on AI Control Protocols"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the vulnerabilities in AI control protocols that are designed to prevent harmful actions by untrusted language models (LLMs). It highlights how adaptive attacks, where an attacker understands the control protocol and the monitoring system, can successfully bypass security measures. The authors demonstrate that these attacks can use prompt injections to manipulate outputs, allowing malicious tasks to be completed without detection. The findings suggest that current security mechanisms are inadequate and need to incorporate defenses against such adaptive attacks in future evaluations.', title='Exposing Vulnerabilities: Adaptive Attacks on AI Control Protocols'))
[13.10.2025 09:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文探讨了针对AI控制协议的自适应攻击，这种攻击利用提示注入技术，可以绕过监控系统并完成恶意任务，揭示了当前安全机制的重大漏洞。AI控制协议旨在防止不可信的大型语言模型（LLM）代理在自主环境中造成伤害，但大多数协议依赖于LLM监控，这可能成为单点故障。研究表明，攻击者可以通过嵌入已知的或零样本的提示注入，成功绕过多种监控系统并在两个主要的AI控制基准上完成恶意任务。自适应攻击对依赖监控的当前协议普遍有效，未来的AI控制机制评估中应将其作为标准组成部分。","title":"自适应攻击：AI控制协议的安全盲点"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文探讨了针对AI控制协议的自适应攻击，这种攻击利用提示注入技术，可以绕过监控系统并完成恶意任务，揭示了当前安全机制的重大漏洞。AI控制协议旨在防止不可信的大型语言模型（LLM）代理在自主环境中造成伤害，但大多数协议依赖于LLM监控，这可能成为单点故障。研究表明，攻击者可以通过嵌入已知的或零样本的提示注入，成功绕过多种监控系统并在两个主要的AI控制基准上完成恶意任务。自适应攻击对依赖监控的当前协议普遍有效，未来的AI控制机制评估中应将其作为标准组成部分。', title='自适应攻击：AI控制协议的安全盲点'))
[13.10.2025 09:16] Using data from previous issue: {"categories": ["#rlhf", "#alignment", "#training", "#reasoning"], "emoji": "🎮", "ru": {"title": "Когда LLM играет в игры с пользователем: теоретико-игровой alignment", "desc": "Статья представляет GTAlign — новый подход к alignment LLM, основанный на теории игр. Авторы решают проблему, когда модель
[13.10.2025 09:16] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning", "#optimization", "#survey"], "emoji": "🔬", "ru": {"title": "Оценка AI-исследователей: как измерить качество глубокого анализа", "desc": "Статья представляет DeepResearch-ReportEval — фреймворк для оценки AI-систем, способных проводить глубокие и
[13.10.2025 09:16] Using data from previous issue: {"categories": ["#games", "#multimodal", "#cv"], "emoji": "🧩", "ru": {"title": "От целого изображения к патчам: новая эра zero-shot описаний", "desc": "Исследователи предложили новый подход к генерации описаний изображений без обучения на парах картинка-текст, сместив фокус с целого изображения на о
[13.10.2025 09:16] Using data from previous issue: {"categories": ["#cv", "#training", "#diffusion", "#optimization", "#architecture"], "emoji": "🎛️", "ru": {"title": "Динамическая настройка весов для адаптивного контроля диффузионных моделей", "desc": "TC-LoRA представляет новый подход к управлению диффузионными моделями через динамическое изменени
[13.10.2025 09:16] Using data from previous issue: {"categories": ["#inference", "#optimization", "#cv", "#video"], "emoji": "🎨", "ru": {"title": "Параллельная генерация изображений через шумоподавление", "desc": "Авторегрессионные модели генерации изображений из текста работают медленно, так как создают токены последовательно, требуя тысячи проходо
[13.10.2025 09:16] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#architecture", "#data", "#interpretability"], "emoji": "🔗", "ru": {"title": "Редактирование знаний через управление нейронными путями", "desc": "Статья представляет фреймворк ACE для улучшения редактирования знаний в LLM, особенно для мно
[13.10.2025 09:16] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#video", "#optimization"], "emoji": "🎯", "ru": {"title": "Декомпозиция задачи видеосегментации через временные промпты", "desc": "Статья представляет фреймворк Tenet для задачи сегментации объектов в видео по текстовому описанию (RVOS). Авторы декомпозируют зада
[13.10.2025 09:16] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#training", "#optimization"], "emoji": "✂️", "ru": {"title": "Эффективное мышление: как научить AI рассуждать короче, но не глупее", "desc": "Большие модели рассуждений (LRM) часто страдают от «overthinking» — избыточных рассуждений, которые увеличивают вычислит
[13.10.2025 09:16] Using data from previous issue: {"categories": ["#transfer_learning", "#multimodal"], "emoji": "🔀", "ru": {"title": "Учимся на разных модальностях без парных данных", "desc": "Статья представляет UML (Unpaired Multimodal Learner) — новый подход к обучению представлений, который использует непарные данные из разных модальностей для
[13.10.2025 09:16] Using data from previous issue: {"categories": ["#games", "#cv", "#video", "#dataset", "#inference", "#benchmark", "#optimization"], "emoji": "⚡", "ru": {"title": "Мгновенная 4D реконструкция сцен из обычного видео за минуты", "desc": "Instant4D — это система для реконструкции динамических сцен из некалиброванного видео, которая р
[13.10.2025 09:16] Renaming data file.
[13.10.2025 09:16] Renaming previous data. hf_papers.json to ./d/2025-10-13.json
[13.10.2025 09:16] Saving new data file.
[13.10.2025 09:16] Generating page.
[13.10.2025 09:16] Renaming previous page.
[13.10.2025 09:16] Renaming previous data. index.html to ./d/2025-10-13.html
[13.10.2025 09:16] Writing result.
[13.10.2025 09:16] Renaming log file.
[13.10.2025 09:16] Renaming previous data. log.txt to ./logs/2025-10-13_last_log.txt
