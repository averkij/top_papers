[28.02.2025 06:15] Read previous papers.
[28.02.2025 06:15] Generating top page (month).
[28.02.2025 06:15] Writing top page (month).
[28.02.2025 07:10] Read previous papers.
[28.02.2025 07:10] Get feed.
[28.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.19613
[28.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.20395
[28.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.20082
[28.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16645
[28.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.20238
[28.02.2025 07:10] Extract page data from URL. URL: https://huggingface.co/papers/2502.16944
[28.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.20321
[28.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.20127
[28.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.20126
[28.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.20307
[28.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.19735
[28.02.2025 07:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.02.2025 07:10] No deleted papers detected.
[28.02.2025 07:10] Downloading and parsing papers (pdf, html). Total: 11.
[28.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.19613.
[28.02.2025 07:10] Extra JSON file exists (./assets/json/2502.19613.json), skip PDF parsing.
[28.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.19613.json), skip HTML parsing.
[28.02.2025 07:10] Success.
[28.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.20395.
[28.02.2025 07:10] Extra JSON file exists (./assets/json/2502.20395.json), skip PDF parsing.
[28.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.20395.json), skip HTML parsing.
[28.02.2025 07:10] Success.
[28.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.20082.
[28.02.2025 07:10] Extra JSON file exists (./assets/json/2502.20082.json), skip PDF parsing.
[28.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.20082.json), skip HTML parsing.
[28.02.2025 07:10] Success.
[28.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.16645.
[28.02.2025 07:10] Extra JSON file exists (./assets/json/2502.16645.json), skip PDF parsing.
[28.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.16645.json), skip HTML parsing.
[28.02.2025 07:10] Success.
[28.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.20238.
[28.02.2025 07:10] Extra JSON file exists (./assets/json/2502.20238.json), skip PDF parsing.
[28.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.20238.json), skip HTML parsing.
[28.02.2025 07:10] Success.
[28.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.16944.
[28.02.2025 07:10] Downloading paper 2502.16944 from http://arxiv.org/pdf/2502.16944v1...
[28.02.2025 07:10] Extracting affiliations from text.
[28.02.2025 07:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 4 4 9 6 1 . 2 0 5 2 : r Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance Chenghua Huang * Lu Wang Fangkai Yang Pu Zhao Zhixu Li Qingwei Lin Dongmei Zhang Saravan Rajmohan Qi Zhang School of Computer Science, Fudan University Microsoft huangch22@m.fudan.edu.cn, {wlu, fangkaiyang, puzhao, dongmeiz}@microsoft.com "
[28.02.2025 07:10] Response: ```python
["School of Computer Science, Fudan University", "Microsoft"]
```
[28.02.2025 07:10] Deleting PDF ./assets/pdf/2502.16944.pdf.
[28.02.2025 07:10] Success.
[28.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.20321.
[28.02.2025 07:10] Extra JSON file exists (./assets/json/2502.20321.json), skip PDF parsing.
[28.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.20321.json), skip HTML parsing.
[28.02.2025 07:10] Success.
[28.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.20127.
[28.02.2025 07:10] Extra JSON file exists (./assets/json/2502.20127.json), skip PDF parsing.
[28.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.20127.json), skip HTML parsing.
[28.02.2025 07:10] Success.
[28.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.20126.
[28.02.2025 07:10] Extra JSON file exists (./assets/json/2502.20126.json), skip PDF parsing.
[28.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.20126.json), skip HTML parsing.
[28.02.2025 07:10] Success.
[28.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.20307.
[28.02.2025 07:10] Extra JSON file exists (./assets/json/2502.20307.json), skip PDF parsing.
[28.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.20307.json), skip HTML parsing.
[28.02.2025 07:10] Success.
[28.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.19735.
[28.02.2025 07:10] Extra JSON file exists (./assets/json/2502.19735.json), skip PDF parsing.
[28.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.19735.json), skip HTML parsing.
[28.02.2025 07:10] Success.
[28.02.2025 07:10] Enriching papers with extra data.
[28.02.2025 07:10] ********************************************************************************
[28.02.2025 07:10] Abstract 0. We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reason...
[28.02.2025 07:10] ********************************************************************************
[28.02.2025 07:10] Abstract 1. In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on challenging downstream tasks. This weakness has been recently mitig...
[28.02.2025 07:10] ********************************************************************************
[28.02.2025 07:10] Abstract 2. LongRoPE2 is a novel approach that extends the effective context window of pre-trained large language models (LLMs) to the target length, while preserving the performance on the original shorter context window. This is achieved by three contributions: (1) a hypothesis that insufficient training in h...
[28.02.2025 07:10] ********************************************************************************
[28.02.2025 07:10] Abstract 3. Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, of...
[28.02.2025 07:10] ********************************************************************************
[28.02.2025 07:10] Abstract 4. Many challenging reasoning tasks require not just rapid, intuitive responses, but a more deliberate, multi-step approach. Recent progress in large language models (LLMs) highlights an important shift from the "System 1" way of quick reactions to the "System 2" style of reflection-and-correction prob...
[28.02.2025 07:10] ********************************************************************************
[28.02.2025 07:10] Abstract 5. Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases comp...
[28.02.2025 07:10] ********************************************************************************
[28.02.2025 07:10] Abstract 6. The representation disparity between visual generation and understanding imposes a critical gap in integrating these capabilities into a single framework. To bridge this gap, we introduce UniTok, a discrete visual tokenizer that encodes fine-grained details for generation while also capturing high-l...
[28.02.2025 07:10] ********************************************************************************
[28.02.2025 07:10] Abstract 7. Mainstream issue-resolving frameworks predominantly rely on commercial models, leading to high costs and privacy concerns. Existing training approaches for issue resolving struggle with poor generalization and fail to fully leverage open-source development resources. We propose Subtask-oriented Rein...
[28.02.2025 07:10] ********************************************************************************
[28.02.2025 07:10] Abstract 8. Despite their remarkable performance, modern Diffusion Transformers are hindered by substantial resource requirements during inference, stemming from the fixed and large amount of compute needed for each denoising step. In this work, we revisit the conventional static paradigm that allocates a fixed...
[28.02.2025 07:10] ********************************************************************************
[28.02.2025 07:10] Abstract 9. We present Mobius, a novel method to generate seamlessly looping videos from text descriptions directly without any user annotations, thereby creating new visual materials for the multi-media presentation. Our method repurposes the pre-trained video latent diffusion model for generating looping vide...
[28.02.2025 07:10] ********************************************************************************
[28.02.2025 07:10] Abstract 10. Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existin...
[28.02.2025 07:10] Read previous papers.
[28.02.2025 07:10] Generating reviews via LLM API.
[28.02.2025 07:10] Using data from previous issue: {"categories": ["#reasoning", "#training", "#inference", "#optimization", "#rl"], "emoji": "🤖", "ru": {"title": "Самокорректирующиеся языковые модели: новый шаг к автономному ИИ", "desc": "Исследователи изучают языковые модели с самовознаграждением, способные генерировать пошаговые рассуждения и оце
[28.02.2025 07:10] Using data from previous issue: {"categories": ["#multimodal", "#training", "#architecture", "#optimization", "#benchmark"], "emoji": "🔀", "ru": {"title": "Оптимизация маршрутизации на лету для повышения эффективности мультимодальных моделей", "desc": "В статье представлен метод Re-Routing in Test-Time (R2-T2) для улучшения работы
[28.02.2025 07:10] Using data from previous issue: {"categories": ["#training", "#architecture", "#benchmark", "#long_context"], "emoji": "🔬", "ru": {"title": "Расширение контекста языковых моделей без потери качества", "desc": "LongRoPE2 - это новый подход к расширению эффективного контекстного окна предобученных больших языковых моделей (LLM) до ц
[28.02.2025 07:10] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#data", "#optimization", "#benchmark"], "emoji": "🔄", "ru": {"title": "Синхронизация языковых моделей с эволюцией кода", "desc": "Статья представляет CODESYNC - инструмент для выявления устаревших паттернов кода и сбора обновлений знаний о коде из сторонн
[28.02.2025 07:10] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#dataset", "#math"], "emoji": "🧩", "ru": {"title": "FINEREASON: Новый подход к оценке рассуждений искусственного интеллекта", "desc": "Статья представляет новый бенчмарк FINEREASON для оценки способностей больших языковых моделей (LLM) к многоступенчатому
[28.02.2025 07:10] Querying the API.
[28.02.2025 07:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining a value model or a reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback. To address these issues, we propose Decoupled Value Policy Optimization (DVPO), a lean framework that replaces traditional reward modeling with a pretrained global value model (GVM). The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40\% and training time by 35\% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance.
[28.02.2025 07:10] Response: {
  "desc": "Статья представляет новый метод обучения языковых моделей с подкреплением на основе обратной связи от человека, называемый Decoupled Value Policy Optimization (DVPO). DVPO заменяет традиционное моделирование вознаграждений предобученной глобальной моделью ценности (GVM), которая предсказывает оценки return-to-go на уровне токенов. Этот подход устраняет взаимозависимость актора и критика, что приводит к значительному снижению использования памяти GPU и времени обучения по сравнению с обычными методами RLHF. Эксперименты показывают, что DVPO превосходит эффективные методы RLHF и соответствует современным методам PPO по производительности.",
  "emoji": "🤖",
  "title": "DVPO: Эффективное обучение языковых моделей с подкреплением без взаимозависимости актора и критика"
}
[28.02.2025 07:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining a value model or a reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback. To address these issues, we propose Decoupled Value Policy Optimization (DVPO), a lean framework that replaces traditional reward modeling with a pretrained global value model (GVM). The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40\% and training time by 35\% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance."

[28.02.2025 07:10] Response: ```python
["RLHF", "RL", "TRAINING"]
```
[28.02.2025 07:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining a value model or a reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback. To address these issues, we propose Decoupled Value Policy Optimization (DVPO), a lean framework that replaces traditional reward modeling with a pretrained global value model (GVM). The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40\% and training time by 35\% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance."

[28.02.2025 07:10] Response: ```python
["ALIGNMENT", "OPTIMIZATION"]
```
[28.02.2025 07:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Decoupled Value Policy Optimization (DVPO), a new framework for improving Reinforcement Learning from Human Feedback (RLHF) in large language models. DVPO uses a pretrained global value model (GVM) to provide fixed supervisory signals, which helps to decouple the value model from policy training. This decoupling reduces the computational complexity and instability associated with traditional actor-critic methods, leading to significant reductions in GPU memory usage and training time. Experimental results demonstrate that DVPO not only outperforms existing efficient RLHF methods but also achieves performance comparable to state-of-the-art Proximal Policy Optimization (PPO).","title":"Decoupling for Efficiency: DVPO Revolutionizes RLHF"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Decoupled Value Policy Optimization (DVPO), a new framework for improving Reinforcement Learning from Human Feedback (RLHF) in large language models. DVPO uses a pretrained global value model (GVM) to provide fixed supervisory signals, which helps to decouple the value model from policy training. This decoupling reduces the computational complexity and instability associated with traditional actor-critic methods, leading to significant reductions in GPU memory usage and training time. Experimental results demonstrate that DVPO not only outperforms existing efficient RLHF methods but also achieves performance comparable to state-of-the-art Proximal Policy Optimization (PPO).', title='Decoupling for Efficiency: DVPO Revolutionizes RLHF'))
[28.02.2025 07:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的强化学习框架，称为解耦值策略优化（DVPO），旨在解决传统的基于人类反馈的强化学习方法中的一些问题。DVPO使用预训练的全局价值模型（GVM），而不是传统的奖励模型，从而减少了演员和评论家之间的相互依赖。通过这种方式，DVPO显著降低了GPU内存使用量和训练时间，同时在多个基准测试中表现优于现有的高效强化学习方法。该方法为大型语言模型的训练提供了更稳定和高效的解决方案。","title":"解耦值策略优化：提升强化学习效率的新方法"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新的强化学习框架，称为解耦值策略优化（DVPO），旨在解决传统的基于人类反馈的强化学习方法中的一些问题。DVPO使用预训练的全局价值模型（GVM），而不是传统的奖励模型，从而减少了演员和评论家之间的相互依赖。通过这种方式，DVPO显著降低了GPU内存使用量和训练时间，同时在多个基准测试中表现优于现有的高效强化学习方法。该方法为大型语言模型的训练提供了更稳定和高效的解决方案。', title='解耦值策略优化：提升强化学习效率的新方法'))
[28.02.2025 07:11] Using data from previous issue: {"categories": ["#training", "#architecture", "#multimodal", "#cv", "#optimization"], "emoji": "🖼️", "ru": {"title": "UniTok: Единый токенизатор для генерации и понимания изображений", "desc": "UniTok - это дискретный визуальный токенизатор, который объединяет возможности генерации и понимания изобр
[28.02.2025 07:11] Using data from previous issue: {"categories": ["#open_source", "#training", "#rlhf", "#rl", "#optimization"], "emoji": "🔧", "ru": {"title": "SoRFT: Эффективное обучение ЯМ для автоматического исправления кода", "desc": "Авторы предлагают новый подход к обучению языковых моделей для решения проблем в программном коде - Subtask-ori
[28.02.2025 07:11] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#inference", "#video", "#optimization"], "emoji": "🚀", "ru": {"title": "FlexiDiT: Эффективная генерация без компромиссов", "desc": "Статья представляет новый подход к оптимизации работы моделей диффузионных трансформеров (DiT). Авторы предлагают динамическую стр
[28.02.2025 07:11] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#video", "#open_source"], "emoji": "🔄", "ru": {"title": "Бесшовная генерация зацикленных видео из текста с помощью латентной диффузии", "desc": "Метод Mobius предлагает новый подход к генерации зацикленных видео на основе текстовых описаний без дополните
[28.02.2025 07:11] Using data from previous issue: {"categories": ["#reasoning", "#training", "#machine_translation", "#rl", "#multilingual"], "emoji": "🧠", "ru": {"title": "Разумный перевод: как научить ИИ думать как переводчик", "desc": "Эта статья представляет R1-Translator (R1-T1) - новый фреймворк для машинного перевода с использованием рассужд
[28.02.2025 07:11] Loading Chinese text from previous data.
[28.02.2025 07:11] Renaming data file.
[28.02.2025 07:11] Renaming previous data. hf_papers.json to ./d/2025-02-28.json
[28.02.2025 07:11] Saving new data file.
[28.02.2025 07:11] Generating page.
[28.02.2025 07:11] Renaming previous page.
[28.02.2025 07:11] Renaming previous data. index.html to ./d/2025-02-28.html
[28.02.2025 07:11] [Experimental] Generating Chinese page for reading.
[28.02.2025 07:11] Chinese vocab [{'word': '介绍', 'pinyin': 'jièshào', 'trans': 'introduce'}, {'word': '双语', 'pinyin': 'shuāngyǔ', 'trans': 'bilingual'}, {'word': '计算成本', 'pinyin': 'jìsuàn chéngběn', 'trans': 'computational cost'}, {'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'}, {'word': '预训练', 'pinyin': 'yù xùnliàn', 'trans': 'pre-training'}, {'word': '高质量', 'pinyin': 'gāo zhìliàng', 'trans': 'high-quality'}, {'word': '过滤', 'pinyin': 'guòlǜ', 'trans': 'filter'}, {'word': '分阶段', 'pinyin': 'fēn jiēduàn', 'trans': 'phased'}, {'word': '深度扩展', 'pinyin': 'shēndù kuòzhǎn', 'trans': 'deep expansion'}, {'word': '剪枝', 'pinyin': 'jiǎnzhī', 'trans': 'pruning'}, {'word': '蒸馏', 'pinyin': 'zhēngliú', 'trans': 'distillation'}, {'word': '监督微调', 'pinyin': 'jiàndū wēitiáo', 'trans': 'supervised fine-tuning'}, {'word': '偏好优化', 'pinyin': 'piānhào yōuhuà', 'trans': 'preference optimization'}, {'word': '互动', 'pinyin': 'hùdòng', 'trans': 'interaction'}, {'word': '嵌入', 'pinyin': 'qiànrù', 'trans': 'embedding'}, {'word': '检索增强生成', 'pinyin': 'jiǎnsuǒ zēngqiáng shēngchéng', 'trans': 'retrieval-augmented generation'}, {'word': '函数调用', 'pinyin': 'hánshù diàoyòng', 'trans': 'function call'}, {'word': '参数', 'pinyin': 'cānshù', 'trans': 'parameters'}, {'word': '公开发布', 'pinyin': 'gōngkāi fābù', 'trans': 'publicly released'}, {'word': '促进', 'pinyin': 'cùjìn', 'trans': 'promote'}]
[28.02.2025 07:11] Renaming previous Chinese page.
[28.02.2025 07:11] Renaming previous data. zh.html to ./d/2025-02-27_zh_reading_task.html
[28.02.2025 07:11] Writing Chinese reading task.
[28.02.2025 07:11] Writing result.
[28.02.2025 07:11] Renaming log file.
[28.02.2025 07:11] Renaming previous data. log.txt to ./logs/2025-02-28_last_log.txt
