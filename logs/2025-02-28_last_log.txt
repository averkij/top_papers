[28.02.2025 02:16] Read previous papers.
[28.02.2025 02:16] Generating top page (month).
[28.02.2025 02:16] Writing top page (month).
[28.02.2025 03:19] Read previous papers.
[28.02.2025 03:19] Get feed.
[28.02.2025 03:19] Extract page data from URL. URL: https://huggingface.co/papers/2502.19613
[28.02.2025 03:19] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.02.2025 03:19] Downloading and parsing papers (pdf, html). Total: 1.
[28.02.2025 03:19] Downloading and parsing paper https://huggingface.co/papers/2502.19613.
[28.02.2025 03:19] Downloading paper 2502.19613 from http://arxiv.org/pdf/2502.19613v1...
[28.02.2025 03:19] Extracting affiliations from text.
[28.02.2025 03:19] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Self-rewarding correction for mathematical reasoning Wei Xiong * 1 Hanning Zhang * 1 Chenlu Ye * 1 Lichang Chen 2 Nan Jiang 1 Tong Zhang "
[28.02.2025 03:19] Response: []
[28.02.2025 03:19] Extracting affiliations from text.
[28.02.2025 03:19] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Self-rewarding correction for mathematical reasoning Wei Xiong * 1 Hanning Zhang * 1 Chenlu Ye * 1 Lichang Chen 2 Nan Jiang 1 Tong ZhangWe study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference timewithout external feedback. This integrated approach allows single model to independently guide its reasoning process, offering computational advantages for model deployment. We particularly focus on the representative task of self-correction, where models autonomously detect errors in their responses, revise outputs, and decide when to terminate iterative refinement loops. To enable this, we propose twostaged algorithmic framework for constructing self-rewarding reasoning models using only selfIn the first stage, we employ generated data. sequential rejection sampling to synthesize long chain-of-thought trajectories that incorporate both self-rewarding and self-correction mechanisms. Fine-tuning models on these curated data allows them to learn the patterns of self-rewarding and self-correction. In the second stage, we further enhance the models ability to assess response accuracy and refine outputs through reinforcement learning with rule-based signals. Experiments with Llama-3 and Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction capabilities and achieves performance comparable to systems that rely on external reward models. 5 2 0 2 6 2 ] . [ 1 3 1 6 9 1 . 2 0 5 2 : r 1. Introduction Large language models (LLMs) have demonstrated remarkable capabilities in reasoning-related tasks such as mathematics and coding. Notable examples include ChatGPT (OpenAI, 2023), Claude (Anthropic, 2023), and Gemini (Team et al., 2023). Following the release of GPT4-o1, LLMs with strong reasoning abilities have attracted even *Equal contribution 1University of Illinois Urbana-Champaign 2University of Maryland, College Park. Correspondence to: Wei Xiong <wx13@illinois.edu>. 1 more attention, along with inference methods that enhance reasoning. particularly desirable property of such models is their ability to detect inconsistencies and errors in self-generated responsesbased on feedback to their prior outputsand correct these errors to produce improved responses. This process is often referred to as self-correction in the literature (Welleck et al., 2022; Madaan et al., 2024; Kim et al., 2024). When an external ground-truth reward model is available, studies (Kim et al., 2024; Qu et al., 2024; Shinn et al., 2024) have shown that LLMs can refine their initial responses based on external gold reward feedback and determine when to terminate the self-correction loop. These approaches have proven effective for both mathematical reasoning and general agent tasks. Moreover, even when relying on imperfect proxy rewards, models can still achieve higher accuracy in revised responses by leveraging feedback from an outcomebased reward model (see Section 5 for empirical results). However, since these reward models are often themselves LLMs, deploying them requires running multiple models during inference, which increases computational costs and deployment complexity. In contrast, without external reward feedback, current LLMs struggle to refine their initial responses solely based on their intrinsic capabilitiesa limitation known as intrinsic self-correction (Huang et al., 2023). While reward models are traditionally trained with an additional scalar head for general-purpose chat (Ouyang et al., 2022; Bai et al., 2022; Touvron et al., 2023) and reasoning tasks (Cobbe et al., 2021a; Lightman et al., 2023), recent work suggests that LLMs themselves can generate reward signals in generative way. For example, the LLM-as-ajudge approach (Zheng et al., 2023; Dubois et al., 2023) prompts the LLM to evaluate text outputs, effectively serving as surrogate for human feedback. Another emerging direction explores generative reward models (Zhao et al., 2023; Dong et al., 2024; Zhang et al., 2024b; Mahan et al., 2024; Zhang et al., 2024a), which formulate evaluation tasks as instruction-following problems, using the probability of generating specific tokens as the reward value. These methods leverage LLMs next-token prediction capabilities, integrate the generation and evaluation into unified framework. Self-rewarding correction for mathematical reasoning Building on these insights, this work investigates selfrewarding reasoning models that can incorporate three abilities within single LLM: (i) generating step-by-step reasoning paths for given prompts, (ii) evaluating the correctness of generated responses, and (iii) revising and enhancing previous responses based on self-rewarding signals. Our key contributions are as follows: contrast, our approach does not focus on self-improvement during training. Instead, we rely on an external ground-truth reward model to provide learning signals in training. Our study emphasizes inference-time alignment for reasoningfocused LLMs, where self-rewarding signals are employed solely to guide inference rather than training. 1. Self-rewarding reasoning framework. We introduce self-rewarding reasoning framework for LLMs, which integrates the generator and reward model into single LLM, enabling autonomous reasoning, evaluation, and correction. This unification simplifies the models decision-making process and reduces computational overhead compared to external reward-based approaches. 2. Algorithmic framework for self-correction. We focus on the self-correction in mathematical reasoning and propose two-stage framework that relies only on self-generated data. In the first stage, we use sequential rejection sampling to construct long chain-of-thought (CoT) trajectories that encode both self-rewarding and self-correction behaviors. Fine-tuning models on these trajectories enables them to detect the error in the selfgenerated responses and revise the previous attempts. In the second stage, we further enhance these patterns through reinforcement learning with rule-based signals. 3. Empirical validation and analysis. Through extensive experiments, we show that self-rewarding correction significantly outperforms intrinsic self-correction. Additionally, we conduct ablation studies to investigate the learning dynamics of the proposed framework, providing deeper insights into its behavior and effectiveness. The training codes and datasets are publicly avai"
[28.02.2025 03:19] Mistral response. {"id": "b7ce38ec98b44914bef1d1f0d087857f", "object": "chat.completion", "created": 1740712772, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"University of Illinois Urbana-Champaign\", \"University of Maryland, College Park\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1565, "total_tokens": 1593, "completion_tokens": 28}}
[28.02.2025 03:19] Response: ```python
["University of Illinois Urbana-Champaign", "University of Maryland, College Park"]
```
[28.02.2025 03:19] Deleting PDF ./assets/pdf/2502.19613.pdf.
[28.02.2025 03:19] Success.
[28.02.2025 03:19] Enriching papers with extra data.
[28.02.2025 03:19] ********************************************************************************
[28.02.2025 03:19] Abstract 0. We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reason...
[28.02.2025 03:19] Read previous papers.
[28.02.2025 03:19] Generating reviews via LLM API.
[28.02.2025 03:19] Querying the API.
[28.02.2025 03:19] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reasoning process, offering computational advantages for model deployment. We particularly focus on the representative task of self-correction, where models autonomously detect errors in their responses, revise outputs, and decide when to terminate iterative refinement loops. To enable this, we propose a two-staged algorithmic framework for constructing self-rewarding reasoning models using only self-generated data. In the first stage, we employ sequential rejection sampling to synthesize long chain-of-thought trajectories that incorporate both self-rewarding and self-correction mechanisms. Fine-tuning models on these curated data allows them to learn the patterns of self-rewarding and self-correction. In the second stage, we further enhance the models' ability to assess response accuracy and refine outputs through reinforcement learning with rule-based signals. Experiments with Llama-3 and Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction capabilities and achieves performance comparable to systems that rely on external reward models.
[28.02.2025 03:19] Response: {
  "desc": "Исследователи изучают языковые модели с самовознаграждением, способные генерировать пошаговые рассуждения и оценивать их корректность без внешней обратной связи. Предложен двухэтапный алгоритмический подход для создания таких моделей, использующий только самогенерируемые данные. На первом этапе применяется последовательная выборка с отклонением для синтеза длинных цепочек рассуждений, включающих механизмы самовознаграждения и самокоррекции. Второй этап усиливает способность моделей оценивать точность ответов и улучшать выходные данные с помощью обучения с подкреплением.",

  "emoji": "🤖",

  "title": "Самокорректирующиеся языковые модели: новый шаг к автономному ИИ"
}
[28.02.2025 03:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reasoning process, offering computational advantages for model deployment. We particularly focus on the representative task of self-correction, where models autonomously detect errors in their responses, revise outputs, and decide when to terminate iterative refinement loops. To enable this, we propose a two-staged algorithmic framework for constructing self-rewarding reasoning models using only self-generated data. In the first stage, we employ sequential rejection sampling to synthesize long chain-of-thought trajectories that incorporate both self-rewarding and self-correction mechanisms. Fine-tuning models on these curated data allows them to learn the patterns of self-rewarding and self-correction. In the second stage, we further enhance the models' ability to assess response accuracy and refine outputs through reinforcement learning with rule-based signals. Experiments with Llama-3 and Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction capabilities and achieves performance comparable to systems that rely on external reward models."

[28.02.2025 03:19] Response: ```python
["INFERENCE", "RL", "TRAINING"]
```
[28.02.2025 03:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reasoning process, offering computational advantages for model deployment. We particularly focus on the representative task of self-correction, where models autonomously detect errors in their responses, revise outputs, and decide when to terminate iterative refinement loops. To enable this, we propose a two-staged algorithmic framework for constructing self-rewarding reasoning models using only self-generated data. In the first stage, we employ sequential rejection sampling to synthesize long chain-of-thought trajectories that incorporate both self-rewarding and self-correction mechanisms. Fine-tuning models on these curated data allows them to learn the patterns of self-rewarding and self-correction. In the second stage, we further enhance the models' ability to assess response accuracy and refine outputs through reinforcement learning with rule-based signals. Experiments with Llama-3 and Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction capabilities and achieves performance comparable to systems that rely on external reward models."

[28.02.2025 03:19] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[28.02.2025 03:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores self-rewarding reasoning in large language models (LLMs), enabling them to generate and evaluate their own reasoning without needing outside feedback. The focus is on self-correction, where models can identify and fix their mistakes independently. The authors introduce a two-stage framework that first uses sequential rejection sampling to create data for training the models on self-rewarding and self-correction. The second stage enhances the models\' accuracy assessment and output refinement through reinforcement learning, showing that their method outperforms traditional self-correction techniques.","title":"Empowering LLMs with Self-Rewarding Reasoning and Self-Correction"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper explores self-rewarding reasoning in large language models (LLMs), enabling them to generate and evaluate their own reasoning without needing outside feedback. The focus is on self-correction, where models can identify and fix their mistakes independently. The authors introduce a two-stage framework that first uses sequential rejection sampling to create data for training the models on self-rewarding and self-correction. The second stage enhances the models' accuracy assessment and output refinement through reinforcement learning, showing that their method outperforms traditional self-correction techniques.", title='Empowering LLMs with Self-Rewarding Reasoning and Self-Correction'))
[28.02.2025 03:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"我们研究了自我奖励推理的大型语言模型（LLMs），这些模型能够在推理过程中同时生成逐步推理并评估输出的正确性，而无需外部反馈。这种集成方法使得单一模型能够独立引导其推理过程，为模型部署提供了计算优势。我们特别关注自我修正的任务，模型能够自主检测响应中的错误，修正输出，并决定何时终止迭代优化循环。为此，我们提出了一种两阶段的算法框架，利用自生成的数据构建自我奖励推理模型。","title":"自我奖励推理：模型的独立思考与修正"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='我们研究了自我奖励推理的大型语言模型（LLMs），这些模型能够在推理过程中同时生成逐步推理并评估输出的正确性，而无需外部反馈。这种集成方法使得单一模型能够独立引导其推理过程，为模型部署提供了计算优势。我们特别关注自我修正的任务，模型能够自主检测响应中的错误，修正输出，并决定何时终止迭代优化循环。为此，我们提出了一种两阶段的算法框架，利用自生成的数据构建自我奖励推理模型。', title='自我奖励推理：模型的独立思考与修正'))
[28.02.2025 03:19] Loading Chinese text from previous data.
[28.02.2025 03:19] Renaming data file.
[28.02.2025 03:19] Renaming previous data. hf_papers.json to ./d/2025-02-28.json
[28.02.2025 03:19] Saving new data file.
[28.02.2025 03:19] Generating page.
[28.02.2025 03:19] Renaming previous page.
[28.02.2025 03:19] Renaming previous data. index.html to ./d/2025-02-28.html
[28.02.2025 03:19] [Experimental] Generating Chinese page for reading.
[28.02.2025 03:19] Chinese vocab [{'word': '介绍', 'pinyin': 'jièshào', 'trans': 'introduce'}, {'word': '双语', 'pinyin': 'shuāngyǔ', 'trans': 'bilingual'}, {'word': '计算成本', 'pinyin': 'jìsuàn chéngběn', 'trans': 'computational cost'}, {'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'}, {'word': '预训练', 'pinyin': 'yù xùnliàn', 'trans': 'pre-training'}, {'word': '高质量', 'pinyin': 'gāo zhìliàng', 'trans': 'high-quality'}, {'word': '过滤', 'pinyin': 'guòlǜ', 'trans': 'filter'}, {'word': '分阶段', 'pinyin': 'fēn jiēduàn', 'trans': 'phased'}, {'word': '深度扩展', 'pinyin': 'shēndù kuòzhǎn', 'trans': 'deep expansion'}, {'word': '剪枝', 'pinyin': 'jiǎnzhī', 'trans': 'pruning'}, {'word': '蒸馏', 'pinyin': 'zhēngliú', 'trans': 'distillation'}, {'word': '监督微调', 'pinyin': 'jiàndū wēitiáo', 'trans': 'supervised fine-tuning'}, {'word': '偏好优化', 'pinyin': 'piānhào yōuhuà', 'trans': 'preference optimization'}, {'word': '互动', 'pinyin': 'hùdòng', 'trans': 'interaction'}, {'word': '嵌入', 'pinyin': 'qiànrù', 'trans': 'embedding'}, {'word': '检索增强生成', 'pinyin': 'jiǎnsuǒ zēngqiáng shēngchéng', 'trans': 'retrieval-augmented generation'}, {'word': '函数调用', 'pinyin': 'hánshù diàoyòng', 'trans': 'function call'}, {'word': '参数', 'pinyin': 'cānshù', 'trans': 'parameters'}, {'word': '公开发布', 'pinyin': 'gōngkāi fābù', 'trans': 'publicly released'}, {'word': '促进', 'pinyin': 'cùjìn', 'trans': 'promote'}]
[28.02.2025 03:19] Renaming previous Chinese page.
[28.02.2025 03:19] Renaming previous data. zh.html to ./d/2025-02-27_zh_reading_task.html
[28.02.2025 03:19] Writing Chinese reading task.
[28.02.2025 03:19] Writing result.
[28.02.2025 03:19] Renaming log file.
[28.02.2025 03:19] Renaming previous data. log.txt to ./logs/2025-02-28_last_log.txt
