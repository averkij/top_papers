[09.04.2025 17:10] Read previous papers.
[09.04.2025 17:10] Generating top page (month).
[09.04.2025 17:10] Writing top page (month).
[09.04.2025 18:15] Read previous papers.
[09.04.2025 18:15] Get feed.
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.06263
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.06261
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05599
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05979
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05535
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02160
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02810
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.06148
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05594
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20533
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.06232
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05897
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00043
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05520
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.06122
[09.04.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03755
[09.04.2025 18:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.04.2025 18:15] No deleted papers detected.
[09.04.2025 18:15] Downloading and parsing papers (pdf, html). Total: 16.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.06263.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.06263.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.06263.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.06261.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.06261.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.06261.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.05599.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.05599.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.05599.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.05979.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.05979.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.05979.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.05535.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.05535.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.05535.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.02160.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.02160.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.02160.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.02810.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.02810.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.02810.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.06148.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.06148.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.06148.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.05594.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.05594.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.05594.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2503.20533.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2503.20533.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2503.20533.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.06232.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.06232.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.06232.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.05897.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.05897.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.05897.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.00043.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.00043.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.00043.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.05520.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.05520.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.05520.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.06122.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.06122.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.06122.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2504.03755.
[09.04.2025 18:15] Extra JSON file exists (./assets/json/2504.03755.json), skip PDF parsing.
[09.04.2025 18:15] Paper image links file exists (./assets/img_data/2504.03755.json), skip HTML parsing.
[09.04.2025 18:15] Success.
[09.04.2025 18:15] Enriching papers with extra data.
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 0. Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existin...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 1. Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 2. We introduce Skywork R1V, a multimodal reasoning model extending the an R1-series Large language models (LLM) to visual modalities via an efficient multimodal transfer method. Leveraging a lightweight visual projector, Skywork R1V facilitates seamless multimodal adaptation without necessitating retr...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 3. The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 4. Aligning large language models (LLMs) with human preferences has achieved remarkable success. However, existing Chinese preference datasets are limited by small scale, narrow domain coverage, and lack of rigorous data validation. Additionally, the reliance on human annotators for instruction and res...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 5. Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 6. With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once inc...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 7. Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 8. Balancing fidelity and editability is essential in text-based image editing (TIE), where failures commonly lead to over- or under-editing issues. Existing methods typically rely on attention injections for structure preservation and leverage the inherent text alignment capabilities of pre-trained te...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 9. Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive a...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 10. Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. To this end,...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 11. The Mixture of Experts (MoE) architecture has demonstrated significant advantages as it enables to increase the model capacity without a proportional increase in computation. However, the large MoE model size still introduces substantial memory demands, which usually requires expert offloading on re...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 12. Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this li...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 13. Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuni...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 14. Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entir...
[09.04.2025 18:15] ********************************************************************************
[09.04.2025 18:15] Abstract 15. Generalized category discovery (GCD) is a pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveragin...
[09.04.2025 18:15] Read previous papers.
[09.04.2025 18:15] Generating reviews via LLM API.
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#dataset", "#cv", "#benchmark", "#multimodal"], "emoji": "üé®", "ru": {"title": "OmniSVG: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "OmniSVG - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–∏ SVG —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–∏–∑—É–∞–ª—å
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#long_context", "#optimization", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–°–∞–º–æ–æ—Ä–≥–∞–Ω–∏–∑—É—é—â–µ–µ—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∑–∞–¥–∞—á –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ 
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#transfer_learning", "#benchmark", "#inference", "#architecture", "#multimodal", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Skywork R1V 
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#architecture", "#multimodal", "#open_source", "#benchmark"], "emoji": "üñºÔ∏è", "ru": {"title": "GPT-4o: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–æ–¥–µ–ª—å—é GPT-4o. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç 
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#alignment", "#data", "#rlhf", "#open_source"], "emoji": "üá®üá≥", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç COIG-P - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –∫–∏—Ç–∞–π—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#dataset", "#data", "#synthetic", "#multimodal", "#diffusion", "#cv"], "emoji": "üé®", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Å–∏
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#benchmark", "#interpretability", "#reasoning", "#multimodal"], "emoji": "üß†", "ru": {"title": "KUMO: –Ω–æ–≤—ã–π —Å–ø–æ—Å–æ–± –æ—Ü–µ–Ω–∏—Ç—å –∏—Å—Ç–∏–Ω–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ò–ò –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ KUMO - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#benchmark", "#games", "#multimodal", "#reasoning", "#agents"], "emoji": "üéÆ", "ru": {"title": "V-MAGE: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò —á–µ—Ä–µ–∑ –∏–≥—Ä–æ–≤—ã–µ –∑–∞–¥–∞—á–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#optimization", "#multimodal"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–∞–ª–∞–Ω—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UnifyEdit - –º–µ—Ç–æ–¥ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –º–µ–∂–¥
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#training", "#math"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—é –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#diffusion", "#training", "#optimization", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "HiFlow: —Ä–∞—Å–∫—Ä—ã—Ç–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "HiFlow - —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥–æ–±—É—á–µ
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization", "#inference", "#open_source"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –≤—ã–≤–æ–¥ MoE-–º–æ–¥–µ–ª–µ–π –Ω–∞ CPU –∏ GPU", "desc": "HybriMoE - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–µ–π Mixture of Experts (MoE) –Ω–∞ CPU –∏ GPU. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#reasoning"], "emoji": "üß©", "ru": {"title": "–ö—Ä–æ—Å—Å–≤–æ—Ä–¥—ã –∫–∞–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –æ—Ü–µ–Ω–∫–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "CrossWordBench - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏ –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LVLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rl", "#math", "#optimization"], "emoji": "üß†", "ru": {"title": "AdaRFT: –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ AdaRFT (–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—á–µ–±–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã), –∫–æ
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#data", "#optimization", "#dataset", "#training", "#reasoning", "#rl"], "emoji": "üß†", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–µ —Ç–µ–æ—Ä–µ–º: –æ—Ç —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º (
[09.04.2025 18:15] Using data from previous issue: {"categories": ["#transfer_learning", "#training", "#data", "#benchmark", "#optimization", "#dataset"], "emoji": "üß†", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±–æ–±—â–µ–Ω–Ω–æ–º—É –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é –∫–∞—Ç–µ–≥–æ—Ä–∏–π (GCD
[09.04.2025 18:15] Loading Chinese text from previous data.
[09.04.2025 18:15] Renaming data file.
[09.04.2025 18:15] Renaming previous data. hf_papers.json to ./d/2025-04-09.json
[09.04.2025 18:15] Saving new data file.
[09.04.2025 18:15] Generating page.
[09.04.2025 18:15] Renaming previous page.
[09.04.2025 18:15] Renaming previous data. index.html to ./d/2025-04-09.html
[09.04.2025 18:15] [Experimental] Generating Chinese page for reading.
[09.04.2025 18:15] Chinese vocab [{'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'inference'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': 'ËΩ¨Áßª', 'pinyin': 'zhu«én y√≠', 'trans': 'transfer'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'Êâ©Â±ï', 'pinyin': 'ku√≤ zh«én', 'trans': 'extend'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'visual'}, {'word': 'ÊäïÂΩ±Âô®', 'pinyin': 't√≥u y«êng q√¨', 'trans': 'projector'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}, {'word': 'ÈáçÊñ∞', 'pinyin': 'ch√≥ng xƒ´n', 'trans': 're-'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πn li√†n', 'trans': 'train'}, {'word': 'ÁºñÁ†ÅÂô®', 'pinyin': 'biƒÅn m«é q√¨', 'trans': 'encoder'}, {'word': 'ÈÄÇÂ∫î', 'pinyin': 'sh√¨ y√¨ng', 'trans': 'adapt'}, {'word': 'Ê∑∑Âêà', 'pinyin': 'h√πn h√©', 'trans': 'hybrid'}, {'word': '‰ºòÂåñ', 'pinyin': 'y≈çu hu√†', 'trans': 'optimization'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'Ëá™ÈÄÇÂ∫î', 'pinyin': 'z√¨ sh√¨ y√¨ng', 'trans': 'adaptive'}, {'word': 'ÈïøÂ∫¶', 'pinyin': 'ch√°ng d√π', 'trans': 'length'}, {'word': 'ÊÄùÁª¥Èìæ', 'pinyin': 'sƒ´ w√©i li√°n', 'trans': 'chain of thought'}, {'word': 'ÊèêÁÇº', 'pinyin': 't√≠ li√†n', 'trans': 'extract'}, {'word': 'ÊïàÁéá', 'pinyin': 'xi√†o l«ú', 'trans': 'efficiency'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«én sh√¨', 'trans': 'show'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ÊµãËØï', 'pinyin': 'c√® sh√¨', 'trans': 'test'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´ s√®', 'trans': 'outstanding'}, {'word': 'ÊùÉÈáç', 'pinyin': 'qu√°n zh√≤ng', 'trans': 'weights'}, {'word': 'ÂÖ¨ÂºÄ', 'pinyin': 'g≈çng kƒÅi', 'trans': 'public'}]
[09.04.2025 18:15] Renaming previous Chinese page.
[09.04.2025 18:15] Renaming previous data. zh.html to ./d/2025-04-08_zh_reading_task.html
[09.04.2025 18:15] Writing Chinese reading task.
[09.04.2025 18:15] Writing result.
[09.04.2025 18:15] Renaming log file.
[09.04.2025 18:15] Renaming previous data. log.txt to ./logs/2025-04-09_last_log.txt
