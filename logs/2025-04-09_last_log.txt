[09.04.2025 15:14] Read previous papers.
[09.04.2025 15:14] Generating top page (month).
[09.04.2025 15:14] Writing top page (month).
[09.04.2025 16:13] Read previous papers.
[09.04.2025 16:13] Get feed.
[09.04.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.06263
[09.04.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05599
[09.04.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.06261
[09.04.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05979
[09.04.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05535
[09.04.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02160
[09.04.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02810
[09.04.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05594
[09.04.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.06148
[09.04.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.06232
[09.04.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00043
[09.04.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.20533
[09.04.2025 16:13] Extract page data from URL. URL: https://huggingface.co/papers/2504.05897
[09.04.2025 16:13] Extract page data from URL. URL: https://huggingface.co/papers/2504.05520
[09.04.2025 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2504.06122
[09.04.2025 16:13] Extract page data from URL. URL: https://huggingface.co/papers/2504.03755
[09.04.2025 16:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.04.2025 16:13] No deleted papers detected.
[09.04.2025 16:13] Downloading and parsing papers (pdf, html). Total: 16.
[09.04.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2504.06263.
[09.04.2025 16:13] Extra JSON file exists (./assets/json/2504.06263.json), skip PDF parsing.
[09.04.2025 16:13] Paper image links file exists (./assets/img_data/2504.06263.json), skip HTML parsing.
[09.04.2025 16:13] Success.
[09.04.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2504.05599.
[09.04.2025 16:13] Extra JSON file exists (./assets/json/2504.05599.json), skip PDF parsing.
[09.04.2025 16:13] Paper image links file exists (./assets/img_data/2504.05599.json), skip HTML parsing.
[09.04.2025 16:13] Success.
[09.04.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2504.06261.
[09.04.2025 16:13] Extra JSON file exists (./assets/json/2504.06261.json), skip PDF parsing.
[09.04.2025 16:13] Paper image links file exists (./assets/img_data/2504.06261.json), skip HTML parsing.
[09.04.2025 16:13] Success.
[09.04.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2504.05979.
[09.04.2025 16:13] Extra JSON file exists (./assets/json/2504.05979.json), skip PDF parsing.
[09.04.2025 16:13] Paper image links file exists (./assets/img_data/2504.05979.json), skip HTML parsing.
[09.04.2025 16:13] Success.
[09.04.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2504.05535.
[09.04.2025 16:13] Extra JSON file exists (./assets/json/2504.05535.json), skip PDF parsing.
[09.04.2025 16:13] Paper image links file exists (./assets/img_data/2504.05535.json), skip HTML parsing.
[09.04.2025 16:13] Success.
[09.04.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2504.02160.
[09.04.2025 16:13] Extra JSON file exists (./assets/json/2504.02160.json), skip PDF parsing.
[09.04.2025 16:13] Paper image links file exists (./assets/img_data/2504.02160.json), skip HTML parsing.
[09.04.2025 16:13] Success.
[09.04.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2504.02810.
[09.04.2025 16:13] Extra JSON file exists (./assets/json/2504.02810.json), skip PDF parsing.
[09.04.2025 16:13] Paper image links file exists (./assets/img_data/2504.02810.json), skip HTML parsing.
[09.04.2025 16:13] Success.
[09.04.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2504.05594.
[09.04.2025 16:13] Extra JSON file exists (./assets/json/2504.05594.json), skip PDF parsing.
[09.04.2025 16:13] Paper image links file exists (./assets/img_data/2504.05594.json), skip HTML parsing.
[09.04.2025 16:13] Success.
[09.04.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2504.06148.
[09.04.2025 16:13] Extra JSON file exists (./assets/json/2504.06148.json), skip PDF parsing.
[09.04.2025 16:13] Paper image links file exists (./assets/img_data/2504.06148.json), skip HTML parsing.
[09.04.2025 16:13] Success.
[09.04.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2504.06232.
[09.04.2025 16:13] Extra JSON file exists (./assets/json/2504.06232.json), skip PDF parsing.
[09.04.2025 16:13] Paper image links file exists (./assets/img_data/2504.06232.json), skip HTML parsing.
[09.04.2025 16:13] Success.
[09.04.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2504.00043.
[09.04.2025 16:13] Extra JSON file exists (./assets/json/2504.00043.json), skip PDF parsing.
[09.04.2025 16:13] Paper image links file exists (./assets/img_data/2504.00043.json), skip HTML parsing.
[09.04.2025 16:13] Success.
[09.04.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2503.20533.
[09.04.2025 16:13] Extra JSON file exists (./assets/json/2503.20533.json), skip PDF parsing.
[09.04.2025 16:13] Paper image links file exists (./assets/img_data/2503.20533.json), skip HTML parsing.
[09.04.2025 16:13] Success.
[09.04.2025 16:13] Downloading and parsing paper https://huggingface.co/papers/2504.05897.
[09.04.2025 16:13] Downloading paper 2504.05897 from http://arxiv.org/pdf/2504.05897v1...
[09.04.2025 16:14] Extracting affiliations from text.
[09.04.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient MoE Inference Shuzhang Zhong1,2, Yanfan Sun5, Ling Liang2, Runsheng Wang2,3,4, Ru Huang2,3,4, Meng Li1,2,4 1Institute for Artificial Intelligence, Peking University, Beijing, China 2School of Integrated Circuits, Peking University, Beijing, China 3Institute of Electronic Design Automation, Peking University, Wuxi, China 4Beijing Advanced Innovation Center for Integrated Circuits, Beijing, China 5School of Computer Science and Engineering, Beihang University, Beijing, China 5 2 0 2 8 ] . [ 1 7 9 8 5 0 . 4 0 5 2 : r AbstractThe Mixture of Experts (MoE) architecture has demonstrated significant advantages as it enables to increase the model capacity without proportional increase in computation. However, the large MoE model size still introduces substantial memory demands, which usually requires expert offloading on resource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU inference has been proposed to leverage CPU computation to reduce expert loading overhead but faces major challenges: on one hand, the expert activation patterns of MoE models are highly unstable, rendering the fixed mapping strategies in existing works inefficient; on the other hand, the hybrid CPU-GPU schedule for MoE is inherently complex due to the diverse expert sizes, structures, uneven workload distribution, etc. To address these challenges, in this paper, we propose HybriMoE, hybrid CPU-GPU inference framework that improves resource utilization through novel CPU-GPU scheduling and cache management system. HybriMoE introduces (i) dynamic intra-layer scheduling strategy to balance workloads across CPU and GPU, (ii) an impact-driven inter-layer prefetching algorithm, and (iii) score-based caching algorithm to mitigate expert activation instability. We implement HybriMoE on top of the kTransformers framework and evaluate it on three widely used MoE-based LLMs. Experimental results demonstrate that HybriMoE "
[09.04.2025 16:14] Response: ```python
[
    "Institute for Artificial Intelligence, Peking University, Beijing, China",
    "School of Integrated Circuits, Peking University, Beijing, China",
    "Institute of Electronic Design Automation, Peking University, Wuxi, China",
    "Beijing Advanced Innovation Center for Integrated Circuits, Beijing, China",
    "School of Computer Science and Engineering, Beihang University, Beijing, China"
]
```
[09.04.2025 16:14] Deleting PDF ./assets/pdf/2504.05897.pdf.
[09.04.2025 16:14] Success.
[09.04.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2504.05520.
[09.04.2025 16:14] Extra JSON file exists (./assets/json/2504.05520.json), skip PDF parsing.
[09.04.2025 16:14] Paper image links file exists (./assets/img_data/2504.05520.json), skip HTML parsing.
[09.04.2025 16:14] Success.
[09.04.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2504.06122.
[09.04.2025 16:14] Extra JSON file exists (./assets/json/2504.06122.json), skip PDF parsing.
[09.04.2025 16:14] Paper image links file exists (./assets/img_data/2504.06122.json), skip HTML parsing.
[09.04.2025 16:14] Success.
[09.04.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2504.03755.
[09.04.2025 16:14] Downloading paper 2504.03755 from http://arxiv.org/pdf/2504.03755v1...
[09.04.2025 16:14] Extracting affiliations from text.
[09.04.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 ] . [ 1 5 5 7 3 0 . 4 0 5 2 : r IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery Shijie Ma, Fei Zhu, Xu-Yao Zhang, Senior Member, IEEE, and Cheng-Lin Liu, Fellow, IEEE AbstractGeneralized category discovery (GCD) is pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveraging pseudo-labeling with parametric classifiers handle old and new classes separately, which brings about imbalanced accuracy between them. Recent methods employing contrastive learning neglect potential positives and are decoupled from the clustering objective, leading to biased representations and sub-optimal results. To address these issues, we introduce unified and unbiased prototype learning framework, namely ProtoGCD, wherein old and new classes are modeled with joint prototypes and unified learning objectives, enabling unified modeling between old and new classes. Specifically, we propose dual-level adaptive pseudo-labeling mechanism to mitigate confirmation bias, together with two regularization terms to collectively help learn more suitable representations for GCD. Moreover, for practical considerations, we devise criterion to estimate the number of new classes. Furthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level unification. Comprehensive experiments show that ProtoGCD achieves state-of-the-art performance on both generic and fine-grained datasets. Index TermsGeneralized Category Discovery, Open-World Learning, Semi-Supervised Learning, Prototype Learning. concepts based on what they have learned [1], [2], [3]. Consider that kid has been taught to recognize some species (e.g., cat, panda, car) and gradually grasp some general knowledge, i.e., what constitut"
[09.04.2025 16:14] Response: ```python
[]
```
[09.04.2025 16:14] Extracting affiliations from text.
[09.04.2025 16:14] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 ] . [ 1 5 5 7 3 0 . 4 0 5 2 : r IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery Shijie Ma, Fei Zhu, Xu-Yao Zhang, Senior Member, IEEE, and Cheng-Lin Liu, Fellow, IEEE AbstractGeneralized category discovery (GCD) is pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveraging pseudo-labeling with parametric classifiers handle old and new classes separately, which brings about imbalanced accuracy between them. Recent methods employing contrastive learning neglect potential positives and are decoupled from the clustering objective, leading to biased representations and sub-optimal results. To address these issues, we introduce unified and unbiased prototype learning framework, namely ProtoGCD, wherein old and new classes are modeled with joint prototypes and unified learning objectives, enabling unified modeling between old and new classes. Specifically, we propose dual-level adaptive pseudo-labeling mechanism to mitigate confirmation bias, together with two regularization terms to collectively help learn more suitable representations for GCD. Moreover, for practical considerations, we devise criterion to estimate the number of new classes. Furthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level unification. Comprehensive experiments show that ProtoGCD achieves state-of-the-art performance on both generic and fine-grained datasets. Index TermsGeneralized Category Discovery, Open-World Learning, Semi-Supervised Learning, Prototype Learning.concepts based on what they have learned [1], [2], [3]. Consider that kid has been taught to recognize some species (e.g., cat, panda, car) and gradually grasp some general knowledge, i.e., what constitutes class. Then, the kid could cluster some tiger images together and regard them as novel category even without learning them before, as shown in Fig. 1. Accordingly, it is important to empower such ability to deep learning and make it more applicable in the open-world [3], [4], [5], [6], where samples from new classes might emerge and models are expected to discover them by transferring the knowledge from old classes. Recently, novel category discovery (NCD) [1], [2], [7], [8], [9], [10], [11] has been introduced to solve the aforementioned problem. Formally, NCD aims to automatically cluster the unlabeled novel classes by leveraging the knowledge learned from old classes in the labeled dataset. It assumes that unlabeled data exclusively comprises samples from novel categories, which often fails to hold in reality. By relaxing such strong assumption, Vaze et al. [12] extended NCD to more pragmatic setting, called generalized category This work has been supported by the National Science and Technology Major Project (2022ZD0116500), National Natural Science Foundation of China (62222609, 62076236), CAS Project for Young Scientists in Basic Research (YSBR-083), Key Research Program of Frontier Sciences of CAS (ZDBS-LY-7004), and the InnoHK program. (Corresponding author: Xu-Yao Zhang.) Shijie Ma, Xu-Yao Zhang and Cheng-Lin Liu are with the State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, 95 Zhongguancun East Road, Beijing 100190, P.R. China, and also with the School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China. Email: mashijie2021@ia.ac.cn, {xyz, liucl}@nlpr.ia.ac.cn. Fei Zhu is with the Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science and Innovation, Chinese Academy of Sciences, Hong Kong 999077, P.R. China. Email: zhfei2018@gmail.com. Code is available at https://github.com/mashijie1028/ProtoGCD. Fig. 1: Generalized category discovery. Given dataset with labeled data from old classes and unlabeled data from both old and novel categories. The objective is to classify old classes and cluster new categories in the unlabeled data. discovery (GCD). In GCD, images from unlabeled data could contain both old and new classes. In this paper, we tackle the task of GCD [12], [13], [14], [15] as illustrated in Fig. 1, which is challenging openworld [3], [5], [6] setting in that models need to simultaneously discover novel categories and recognize old classes coexisting in the unlabeled data. Pioneer works [12], [13] resort to the supervised [16] and unsupervised contrastive learning [17] on labeled data and unlabeled data, respectively. And non-parametric semi-supervised K-means [12], [18] is employed upon the learned features for clustering. However, contrastive learning alone ignores underlying positives and is susceptible to class collision [19]. Furthermore, pure contrastive learning is essentially decoupled with the clustering objective of GCD, leading to biased representations and sub-optimal performance. Another line of works [2], [20] use pseudo-labels and handle old and novel classes with separate classification heads and learning objectives. These methods tend to be biased toward old classes [12], and bring about IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2 Fig. 2: The unified and unbiased characteristics of ProtoGCD, which contribute to addressing the issues of prior methods. imbalanced accuracies between old and new classes. The problems of preceding methods are summarized in Fig. 2. To solve the issues above, we propose unified and unbiased Prototype Learning framework for Generalized Category Discovery (ProtoGCD), which handles old and novel categories jointly in shared feature space with the same set of learnable prototypes. There are two key insights to solve the issues of prior methods: (1) The first is the unification between old and new classes (Fig. 2 (a)). We model old and new classes with joint classifier and unified learning objectives, which helps alleviate the imbalanced performance of prior parametric methods [2], [20], as shown in Fig. 3 (a) and (b). (2) Secondly, the model is equipped with parametric prototypical classifier and self-trained with pseudo-labels, which aligns more closely with the clustering objective and learns more suitable representations for GCD (Fig. 2 (c)) than contrastive learning-based methods [12], [13], [14], ["
[09.04.2025 16:14] Mistral response. {"id": "38290e5a699b4cfdb133825967daf450", "object": "chat.completion", "created": 1744215254, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, 95 Zhongguancun East Road, Beijing 100190, P.R. China\",\n    \"School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China\",\n    \"Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science and Innovation, Chinese Academy of Sciences, Hong Kong 999077, P.R. China\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1719, "total_tokens": 1855, "completion_tokens": 136}}
[09.04.2025 16:14] Response: ```python
[
    "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, 95 Zhongguancun East Road, Beijing 100190, P.R. China",
    "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China",
    "Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science and Innovation, Chinese Academy of Sciences, Hong Kong 999077, P.R. China"
]
```
[09.04.2025 16:14] Deleting PDF ./assets/pdf/2504.03755.pdf.
[09.04.2025 16:14] Success.
[09.04.2025 16:14] Enriching papers with extra data.
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 0. Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existin...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 1. We introduce Skywork R1V, a multimodal reasoning model extending the an R1-series Large language models (LLM) to visual modalities via an efficient multimodal transfer method. Leveraging a lightweight visual projector, Skywork R1V facilitates seamless multimodal adaptation without necessitating retr...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 2. Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 3. The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 4. Aligning large language models (LLMs) with human preferences has achieved remarkable success. However, existing Chinese preference datasets are limited by small scale, narrow domain coverage, and lack of rigorous data validation. Additionally, the reliance on human annotators for instruction and res...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 5. Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 6. With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once inc...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 7. Balancing fidelity and editability is essential in text-based image editing (TIE), where failures commonly lead to over- or under-editing issues. Existing methods typically rely on attention injections for structure preservation and leverage the inherent text alignment capabilities of pre-trained te...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 8. Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 9. Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. To this end,...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 10. Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this li...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 11. Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive a...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 12. The Mixture of Experts (MoE) architecture has demonstrated significant advantages as it enables to increase the model capacity without a proportional increase in computation. However, the large MoE model size still introduces substantial memory demands, which usually requires expert offloading on re...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 13. Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuni...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 14. Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entir...
[09.04.2025 16:14] ********************************************************************************
[09.04.2025 16:14] Abstract 15. Generalized category discovery (GCD) is a pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveragin...
[09.04.2025 16:14] Read previous papers.
[09.04.2025 16:14] Generating reviews via LLM API.
[09.04.2025 16:14] Using data from previous issue: {"categories": ["#dataset", "#cv", "#benchmark", "#multimodal"], "emoji": "🎨", "ru": {"title": "OmniSVG: Революция в генерации векторной графики с помощью ИИ", "desc": "OmniSVG - это новая структура для генерации высококачественной векторной графики SVG с использованием предобученных моделей визуаль
[09.04.2025 16:14] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#transfer_learning", "#benchmark", "#inference", "#architecture", "#multimodal", "#training", "#optimization"], "emoji": "🧠", "ru": {"title": "Эффективная мультимодальная адаптация для улучшения рассуждений ИИ", "desc": "Статья представляет Skywork R1V 
[09.04.2025 16:14] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#long_context", "#optimization", "#architecture"], "emoji": "🤖", "ru": {"title": "Самоорганизующееся параллельное сотрудничество языковых моделей", "desc": "Эта статья представляет новый подход к параллельному выполнению задач большими языковыми моделями 
[09.04.2025 16:14] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#architecture", "#multimodal", "#open_source", "#benchmark"], "emoji": "🖼️", "ru": {"title": "GPT-4o: Новый рубеж в унифицированной генерации изображений", "desc": "Статья посвящена исследованию возможностей генерации изображений моделью GPT-4o. Авторы проводят 
[09.04.2025 16:14] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#alignment", "#data", "#rlhf", "#open_source"], "emoji": "🇨🇳", "ru": {"title": "Автоматизированное создание масштабного китайского датасета предпочтений для улучшения LLM", "desc": "Статья представляет COIG-P - крупномасштабный китайский датасет предпочтени
[09.04.2025 16:14] Using data from previous issue: {"categories": ["#dataset", "#data", "#synthetic", "#multimodal", "#diffusion", "#cv"], "emoji": "🎨", "ru": {"title": "Универсальная генерация изображений с множеством объектов", "desc": "Статья описывает новый подход к генерации изображений с несколькими объектами. Авторы предлагают пайплайн для си
[09.04.2025 16:14] Using data from previous issue: {"categories": ["#benchmark", "#interpretability", "#reasoning", "#multimodal"], "emoji": "🧠", "ru": {"title": "KUMO: новый способ оценить истинные способности ИИ к рассуждению", "desc": "Исследователи представили KUMO - новую систему для оценки способностей больших языковых моделей (LLM) к рассужде
[09.04.2025 16:14] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#optimization", "#multimodal"], "emoji": "⚖️", "ru": {"title": "Баланс точности и редактируемости в редактировании изображений на основе текста", "desc": "Статья представляет UnifyEdit - метод редактирования изображений на основе текста, который балансирует межд
[09.04.2025 16:14] Using data from previous issue: {"categories": ["#benchmark", "#games", "#multimodal", "#reasoning", "#agents"], "emoji": "🎮", "ru": {"title": "V-MAGE: Новый подход к оценке визуальных способностей ИИ через игровые задачи", "desc": "В статье представлена новая система оценки визуальных способностей мультимодальных больших языковых
[09.04.2025 16:14] Using data from previous issue: {"categories": ["#diffusion", "#training", "#optimization", "#cv"], "emoji": "🖼️", "ru": {"title": "HiFlow: раскрытие потенциала высокого разрешения в генерации изображений", "desc": "HiFlow - это инновационный подход к улучшению качества генерации изображений высокого разрешения с помощью предобуче
[09.04.2025 16:14] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#reasoning"], "emoji": "🧩", "ru": {"title": "Кроссворды как инструмент оценки искусственного интеллекта", "desc": "CrossWordBench - это новый метод оценки способностей больших языковых моделей (LLM) и больших визуально-языковых моделей (LVLM) к рассужден
[09.04.2025 16:14] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#training", "#math"], "emoji": "🚀", "ru": {"title": "Ускорение рассуждений без потери качества", "desc": "Статья описывает метод ускорения процесса рассуждений в моделях машинного обучения. Авторы предлагают использовать параллелизацию для обработки не
[09.04.2025 16:14] Querying the API.
[09.04.2025 16:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The Mixture of Experts (MoE) architecture has demonstrated significant advantages as it enables to increase the model capacity without a proportional increase in computation. However, the large MoE model size still introduces substantial memory demands, which usually requires expert offloading on resource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU inference has been proposed to leverage CPU computation to reduce expert loading overhead but faces major challenges: on one hand, the expert activation patterns of MoE models are highly unstable, rendering the fixed mapping strategies in existing works inefficient; on the other hand, the hybrid CPU-GPU schedule for MoE is inherently complex due to the diverse expert sizes, structures, uneven workload distribution, etc. To address these challenges, in this paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that improves resource utilization through a novel CPU-GPU scheduling and cache management system. HybriMoE introduces (i) a dynamic intra-layer scheduling strategy to balance workloads across CPU and GPU, (ii) an impact-driven inter-layer prefetching algorithm, and (iii) a score-based caching algorithm to mitigate expert activation instability. We implement HybriMoE on top of the kTransformers framework and evaluate it on three widely used MoE-based LLMs. Experimental results demonstrate that HybriMoE achieves an average speedup of 1.33times in the prefill stage and 1.70times in the decode stage compared to state-of-the-art hybrid MoE inference framework. Our code is available at: https://github.com/PKU-SEC-Lab/HybriMoE.
[09.04.2025 16:14] Response: {
  "desc": "HybriMoE - это фреймворк для гибридного вывода моделей Mixture of Experts (MoE) на CPU и GPU. Он решает проблемы нестабильных паттернов активации экспертов и сложности планирования вычислений между CPU и GPU. Фреймворк использует динамическое планирование внутри слоев, алгоритм предварительной загрузки между слоями и кэширование на основе оценок. Эксперименты показывают значительное ускорение по сравнению с существующими решениями для вывода MoE-моделей.",
  "emoji": "🧠",
  "title": "Эффективный гибридный вывод MoE-моделей на CPU и GPU"
}
[09.04.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The Mixture of Experts (MoE) architecture has demonstrated significant advantages as it enables to increase the model capacity without a proportional increase in computation. However, the large MoE model size still introduces substantial memory demands, which usually requires expert offloading on resource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU inference has been proposed to leverage CPU computation to reduce expert loading overhead but faces major challenges: on one hand, the expert activation patterns of MoE models are highly unstable, rendering the fixed mapping strategies in existing works inefficient; on the other hand, the hybrid CPU-GPU schedule for MoE is inherently complex due to the diverse expert sizes, structures, uneven workload distribution, etc. To address these challenges, in this paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that improves resource utilization through a novel CPU-GPU scheduling and cache management system. HybriMoE introduces (i) a dynamic intra-layer scheduling strategy to balance workloads across CPU and GPU, (ii) an impact-driven inter-layer prefetching algorithm, and (iii) a score-based caching algorithm to mitigate expert activation instability. We implement HybriMoE on top of the kTransformers framework and evaluate it on three widely used MoE-based LLMs. Experimental results demonstrate that HybriMoE achieves an average speedup of 1.33times in the prefill stage and 1.70times in the decode stage compared to state-of-the-art hybrid MoE inference framework. Our code is available at: https://github.com/PKU-SEC-Lab/HybriMoE."

[09.04.2025 16:14] Response: ```python
["ARCHITECTURE", "INFERENCE", "TRAINING"]
```
[09.04.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The Mixture of Experts (MoE) architecture has demonstrated significant advantages as it enables to increase the model capacity without a proportional increase in computation. However, the large MoE model size still introduces substantial memory demands, which usually requires expert offloading on resource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU inference has been proposed to leverage CPU computation to reduce expert loading overhead but faces major challenges: on one hand, the expert activation patterns of MoE models are highly unstable, rendering the fixed mapping strategies in existing works inefficient; on the other hand, the hybrid CPU-GPU schedule for MoE is inherently complex due to the diverse expert sizes, structures, uneven workload distribution, etc. To address these challenges, in this paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that improves resource utilization through a novel CPU-GPU scheduling and cache management system. HybriMoE introduces (i) a dynamic intra-layer scheduling strategy to balance workloads across CPU and GPU, (ii) an impact-driven inter-layer prefetching algorithm, and (iii) a score-based caching algorithm to mitigate expert activation instability. We implement HybriMoE on top of the kTransformers framework and evaluate it on three widely used MoE-based LLMs. Experimental results demonstrate that HybriMoE achieves an average speedup of 1.33times in the prefill stage and 1.70times in the decode stage compared to state-of-the-art hybrid MoE inference framework. Our code is available at: https://github.com/PKU-SEC-Lab/HybriMoE."

[09.04.2025 16:14] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```
[09.04.2025 16:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents HybriMoE, a hybrid CPU-GPU inference framework designed to optimize the performance of Mixture of Experts (MoE) models. It addresses the challenges of unstable expert activation patterns and complex scheduling due to varying expert sizes and workloads. HybriMoE introduces innovative strategies such as dynamic intra-layer scheduling, impact-driven inter-layer prefetching, and score-based caching to enhance resource utilization. Experimental results show that HybriMoE significantly speeds up the inference process, achieving notable improvements over existing frameworks.","title":"Optimizing MoE Inference with HybriMoE: Speed and Efficiency Unleashed!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents HybriMoE, a hybrid CPU-GPU inference framework designed to optimize the performance of Mixture of Experts (MoE) models. It addresses the challenges of unstable expert activation patterns and complex scheduling due to varying expert sizes and workloads. HybriMoE introduces innovative strategies such as dynamic intra-layer scheduling, impact-driven inter-layer prefetching, and score-based caching to enhance resource utilization. Experimental results show that HybriMoE significantly speeds up the inference process, achieving notable improvements over existing frameworks.', title='Optimizing MoE Inference with HybriMoE: Speed and Efficiency Unleashed!'))
[09.04.2025 16:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"混合专家（MoE）架构在提高模型容量方面表现出显著优势，但其大规模模型仍然对内存提出了高要求。为了解决这一问题，本文提出了HybriMoE，一个混合CPU-GPU推理框架，通过新颖的调度和缓存管理系统来提高资源利用率。HybriMoE引入了动态的层内调度策略、影响驱动的层间预取算法和基于评分的缓存算法，以减轻专家激活的不稳定性。实验结果表明，HybriMoE在预填充阶段和解码阶段的速度分别提高了1.33倍和1.70倍。","title":"HybriMoE：提升混合推理效率的创新框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='混合专家（MoE）架构在提高模型容量方面表现出显著优势，但其大规模模型仍然对内存提出了高要求。为了解决这一问题，本文提出了HybriMoE，一个混合CPU-GPU推理框架，通过新颖的调度和缓存管理系统来提高资源利用率。HybriMoE引入了动态的层内调度策略、影响驱动的层间预取算法和基于评分的缓存算法，以减轻专家激活的不稳定性。实验结果表明，HybriMoE在预填充阶段和解码阶段的速度分别提高了1.33倍和1.70倍。', title='HybriMoE：提升混合推理效率的创新框架'))
[09.04.2025 16:14] Querying the API.
[09.04.2025 16:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces the number of training steps by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.
[09.04.2025 16:14] Response: {
  "desc": "В этой статье представлен метод AdaRFT (Адаптивное обучение с подкреплением на основе учебной программы), который значительно улучшает эффективность и точность обучения с подкреплением для больших языковых моделей в задачах математического рассуждения. AdaRFT динамически корректирует сложность обучающих задач на основе недавних сигналов вознаграждения модели, обеспечивая постоянное обучение на задачах, которые являются сложными, но решаемыми. Эксперименты на наборах данных уровня соревнований показывают, что AdaRFT значительно улучшает как эффективность обучения, так и производительность рассуждений. Метод сокращает количество шагов обучения до 2 раз и значительно повышает точность, предлагая более масштабируемую и эффективную структуру обучения с подкреплением.",
  "emoji": "🧠",
  "title": "AdaRFT: Адаптивное обучение для улучшения математических способностей ИИ"
}
[09.04.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces the number of training steps by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework."

[09.04.2025 16:14] Response: ```python
["RL", "TRAINING", "MATH"]
```
[09.04.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces the number of training steps by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework."

[09.04.2025 16:14] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[09.04.2025 16:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents AdaRFT, a new method that enhances Reinforcement Finetuning (RFT) for large language models (LLMs) by using adaptive curriculum learning. AdaRFT adjusts the difficulty of training tasks based on the model\'s performance, ensuring that it learns from problems that are appropriately challenging. This approach improves training efficiency by reducing unnecessary computations on tasks that are too easy or too hard. Experiments show that AdaRFT can cut training steps in half while significantly boosting the model\'s accuracy on complex math problems.","title":"Adaptive Learning for Efficient Reinforcement Finetuning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents AdaRFT, a new method that enhances Reinforcement Finetuning (RFT) for large language models (LLMs) by using adaptive curriculum learning. AdaRFT adjusts the difficulty of training tasks based on the model's performance, ensuring that it learns from problems that are appropriately challenging. This approach improves training efficiency by reducing unnecessary computations on tasks that are too easy or too hard. Experiments show that AdaRFT can cut training steps in half while significantly boosting the model's accuracy on complex math problems.", title='Adaptive Learning for Efficient Reinforcement Finetuning'))
[09.04.2025 16:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"强化微调（RFT）在提升大型语言模型的数学推理能力方面展现了巨大潜力，但通常需要大量样本和计算资源。我们提出了AdaRFT（自适应课程强化微调），通过自适应课程学习显著提高了RFT的效率和最终准确性。AdaRFT根据模型最近的奖励信号动态调整训练问题的难度，确保模型始终在具有挑战性但可解决的任务上进行训练。这种自适应采样策略加速了学习，避免了在过于简单或过于困难的问题上浪费计算资源。","title":"自适应课程强化微调：提升效率与准确性"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='强化微调（RFT）在提升大型语言模型的数学推理能力方面展现了巨大潜力，但通常需要大量样本和计算资源。我们提出了AdaRFT（自适应课程强化微调），通过自适应课程学习显著提高了RFT的效率和最终准确性。AdaRFT根据模型最近的奖励信号动态调整训练问题的难度，确保模型始终在具有挑战性但可解决的任务上进行训练。这种自适应采样策略加速了学习，避免了在过于简单或过于困难的问题上浪费计算资源。', title='自适应课程强化微调：提升效率与准确性'))
[09.04.2025 16:14] Using data from previous issue: {"categories": ["#data", "#optimization", "#dataset", "#training", "#reasoning", "#rl"], "emoji": "🧠", "ru": {"title": "Революция в автоматическом доказательстве теорем: от языковых моделей к формальной логике", "desc": "Статья описывает исследование в области автоматического доказательства теорем (
[09.04.2025 16:14] Querying the API.
[09.04.2025 16:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Generalized category discovery (GCD) is a pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveraging pseudo-labeling with parametric classifiers handle old and new classes separately, which brings about imbalanced accuracy between them. Recent methods employing contrastive learning neglect potential positives and are decoupled from the clustering objective, leading to biased representations and sub-optimal results. To address these issues, we introduce a unified and unbiased prototype learning framework, namely ProtoGCD, wherein old and new classes are modeled with joint prototypes and unified learning objectives, {enabling unified modeling between old and new classes}. Specifically, we propose a dual-level adaptive pseudo-labeling mechanism to mitigate confirmation bias, together with two regularization terms to collectively help learn more suitable representations for GCD. Moreover, for practical considerations, we devise a criterion to estimate the number of new classes. Furthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level unification. Comprehensive experiments show that ProtoGCD achieves state-of-the-art performance on both generic and fine-grained datasets. The code is available at https://github.com/mashijie1028/ProtoGCD.
[09.04.2025 16:14] Response: {
  "desc": "Статья представляет новый подход к обобщенному обнаружению категорий (GCD) в машинном обучении. Авторы предлагают унифицированную систему обучения прототипов под названием ProtoGCD, которая моделирует старые и новые классы с помощью общих прототипов и единых целей обучения. Метод включает двухуровневый адаптивный механизм псевдомаркировки и регуляризационные термы для улучшения представлений данных. ProtoGCD также способен оценивать количество новых классов и обнаруживать выбросы, демонстрируя лучшие результаты на различных наборах данных.",
  "emoji": "🧠",
  "title": "Унифицированное обучение прототипов для эффективного обнаружения новых категорий"
}
[09.04.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generalized category discovery (GCD) is a pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveraging pseudo-labeling with parametric classifiers handle old and new classes separately, which brings about imbalanced accuracy between them. Recent methods employing contrastive learning neglect potential positives and are decoupled from the clustering objective, leading to biased representations and sub-optimal results. To address these issues, we introduce a unified and unbiased prototype learning framework, namely ProtoGCD, wherein old and new classes are modeled with joint prototypes and unified learning objectives, {enabling unified modeling between old and new classes}. Specifically, we propose a dual-level adaptive pseudo-labeling mechanism to mitigate confirmation bias, together with two regularization terms to collectively help learn more suitable representations for GCD. Moreover, for practical considerations, we devise a criterion to estimate the number of new classes. Furthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level unification. Comprehensive experiments show that ProtoGCD achieves state-of-the-art performance on both generic and fine-grained datasets. The code is available at https://github.com/mashijie1028/ProtoGCD."

[09.04.2025 16:15] Response: ```python
["DATASET", "DATA", "BENCHMARK", "TRAINING"]
```
[09.04.2025 16:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generalized category discovery (GCD) is a pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveraging pseudo-labeling with parametric classifiers handle old and new classes separately, which brings about imbalanced accuracy between them. Recent methods employing contrastive learning neglect potential positives and are decoupled from the clustering objective, leading to biased representations and sub-optimal results. To address these issues, we introduce a unified and unbiased prototype learning framework, namely ProtoGCD, wherein old and new classes are modeled with joint prototypes and unified learning objectives, {enabling unified modeling between old and new classes}. Specifically, we propose a dual-level adaptive pseudo-labeling mechanism to mitigate confirmation bias, together with two regularization terms to collectively help learn more suitable representations for GCD. Moreover, for practical considerations, we devise a criterion to estimate the number of new classes. Furthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level unification. Comprehensive experiments show that ProtoGCD achieves state-of-the-art performance on both generic and fine-grained datasets. The code is available at https://github.com/mashijie1028/ProtoGCD."

[09.04.2025 16:15] Response: ```python
["TRANSFER_LEARNING", "OPTIMIZATION"]
```
[09.04.2025 16:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenge of Generalized Category Discovery (GCD), where models must identify new categories using existing labeled data. The authors introduce ProtoGCD, a framework that uses joint prototypes to represent both old and new classes, allowing for a more balanced learning process. They implement a dual-level adaptive pseudo-labeling mechanism to reduce bias and improve representation learning. Additionally, ProtoGCD includes a method for estimating new class counts and detecting unseen outliers, demonstrating superior performance on various datasets.","title":"Unified Learning for Discovering New Categories in Machine Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenge of Generalized Category Discovery (GCD), where models must identify new categories using existing labeled data. The authors introduce ProtoGCD, a framework that uses joint prototypes to represent both old and new classes, allowing for a more balanced learning process. They implement a dual-level adaptive pseudo-labeling mechanism to reduce bias and improve representation learning. Additionally, ProtoGCD includes a method for estimating new class counts and detecting unseen outliers, demonstrating superior performance on various datasets.', title='Unified Learning for Discovering New Categories in Machine Learning'))
[09.04.2025 16:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的框架，称为ProtoGCD，用于解决广义类别发现（GCD）问题。该框架通过联合原型学习来同时建模旧类别和新类别，克服了以往方法中存在的偏差和不平衡问题。我们引入了双层自适应伪标签机制，以减少确认偏差，并通过正则化项来优化表示学习。实验结果表明，ProtoGCD在多个数据集上表现出色，达到了最先进的性能。","title":"统一建模，发现新类别的力量"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新的框架，称为ProtoGCD，用于解决广义类别发现（GCD）问题。该框架通过联合原型学习来同时建模旧类别和新类别，克服了以往方法中存在的偏差和不平衡问题。我们引入了双层自适应伪标签机制，以减少确认偏差，并通过正则化项来优化表示学习。实验结果表明，ProtoGCD在多个数据集上表现出色，达到了最先进的性能。', title='统一建模，发现新类别的力量'))
[09.04.2025 16:15] Loading Chinese text from previous data.
[09.04.2025 16:15] Renaming data file.
[09.04.2025 16:15] Renaming previous data. hf_papers.json to ./d/2025-04-09.json
[09.04.2025 16:15] Saving new data file.
[09.04.2025 16:15] Generating page.
[09.04.2025 16:15] Renaming previous page.
[09.04.2025 16:15] Renaming previous data. index.html to ./d/2025-04-09.html
[09.04.2025 16:15] [Experimental] Generating Chinese page for reading.
[09.04.2025 16:15] Chinese vocab [{'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '转移', 'pinyin': 'zhuǎn yí', 'trans': 'transfer'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '扩展', 'pinyin': 'kuò zhǎn', 'trans': 'extend'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '投影器', 'pinyin': 'tóu yǐng qì', 'trans': 'projector'}, {'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'}, {'word': '重新', 'pinyin': 'chóng xīn', 'trans': 're-'}, {'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'train'}, {'word': '编码器', 'pinyin': 'biān mǎ qì', 'trans': 'encoder'}, {'word': '适应', 'pinyin': 'shì yìng', 'trans': 'adapt'}, {'word': '混合', 'pinyin': 'hùn hé', 'trans': 'hybrid'}, {'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimization'}, {'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'}, {'word': '自适应', 'pinyin': 'zì shì yìng', 'trans': 'adaptive'}, {'word': '长度', 'pinyin': 'cháng dù', 'trans': 'length'}, {'word': '思维链', 'pinyin': 'sī wéi lián', 'trans': 'chain of thought'}, {'word': '提炼', 'pinyin': 'tí liàn', 'trans': 'extract'}, {'word': '效率', 'pinyin': 'xiào lǜ', 'trans': 'efficiency'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '显示', 'pinyin': 'xiǎn shì', 'trans': 'show'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '测试', 'pinyin': 'cè shì', 'trans': 'test'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}, {'word': '权重', 'pinyin': 'quán zhòng', 'trans': 'weights'}, {'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'}]
[09.04.2025 16:15] Renaming previous Chinese page.
[09.04.2025 16:15] Renaming previous data. zh.html to ./d/2025-04-08_zh_reading_task.html
[09.04.2025 16:15] Writing Chinese reading task.
[09.04.2025 16:15] Writing result.
[09.04.2025 16:15] Renaming log file.
[09.04.2025 16:15] Renaming previous data. log.txt to ./logs/2025-04-09_last_log.txt
