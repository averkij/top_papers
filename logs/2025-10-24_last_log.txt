[24.10.2025 16:16] Read previous papers.
[24.10.2025 16:16] Generating top page (month).
[24.10.2025 16:16] Writing top page (month).
[24.10.2025 17:12] Read previous papers.
[24.10.2025 17:12] Get feed.
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19600
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19779
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20579
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20822
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19304
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20766
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20187
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19365
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16917
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16893
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18821
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20820
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20470
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19944
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12487
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20803
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20771
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20270
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20733
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20668
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17853
[24.10.2025 17:12] Extract page data from URL. URL: https://huggingface.co/papers/2510.18245
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20362
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19995
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18413
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17896
[24.10.2025 17:12] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15804
[24.10.2025 17:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[24.10.2025 17:12] No deleted papers detected.
[24.10.2025 17:12] Downloading and parsing papers (pdf, html). Total: 27.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.19600.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.19600.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.19600.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.19779.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.19779.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.19779.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.20579.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.20579.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.20579.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.20822.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.20822.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.20822.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.19304.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.19304.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.19304.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.20766.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.20766.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.20766.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.20187.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.20187.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.20187.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.19365.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.19365.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.19365.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.16917.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.16917.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.16917.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.16893.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.16893.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.16893.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.18821.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.18821.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.18821.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.20820.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.20820.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.20820.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.20470.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.20470.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.20470.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.19944.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.19944.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.19944.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.12487.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.12487.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.12487.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.20803.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.20803.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.20803.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.20771.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.20771.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.20771.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.20270.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.20270.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.20270.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.20733.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.20733.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.20733.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.20668.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.20668.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.20668.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.17853.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.17853.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.17853.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.18245.
[24.10.2025 17:12] Downloading paper 2510.18245 from http://arxiv.org/pdf/2510.18245v1...
[24.10.2025 17:12] Extracting affiliations from text.
[24.10.2025 17:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 2 ] . [ 1 5 4 2 8 1 . 0 1 5 2 : r Preprint SCALING LAWS MEET MODEL ARCHITECTURE: TOWARD INFERENCE-EFFICIENT LLMS Song Bian UW-Madison Tao Yu Amazon Web Services Shivaram Venkataraman UW-Madison Youngsuk Park Amazon Web Services "
[24.10.2025 17:12] Response: ```python
["UW-Madison", "Amazon Web Services"]
```
[24.10.2025 17:12] Deleting PDF ./assets/pdf/2510.18245.pdf.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.20362.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.20362.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.20362.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.19995.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.19995.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.19995.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.18413.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.18413.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.18413.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.17896.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.17896.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.17896.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Downloading and parsing paper https://huggingface.co/papers/2510.15804.
[24.10.2025 17:12] Extra JSON file exists (./assets/json/2510.15804.json), skip PDF parsing.
[24.10.2025 17:12] Paper image links file exists (./assets/img_data/2510.15804.json), skip HTML parsing.
[24.10.2025 17:12] Success.
[24.10.2025 17:12] Enriching papers with extra data.
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 0. AutoPage, a multi-agent system, automates the creation of interactive research webpages through a hierarchical process, ensuring high-quality and efficient results.  					AI-generated summary 				 In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, ...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 1. AdaSPEC enhances speculative decoding by selectively filtering tokens during knowledge distillation, improving token acceptance rates without sacrificing generation quality.  					AI-generated summary 				 Speculative Decoding (SD) accelerates large language model inference by employing a small draf...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 2. Open-o3 Video integrates spatio-temporal evidence into video reasoning, achieving state-of-the-art performance on multiple benchmarks and providing valuable reasoning traces for test-time scaling.  					AI-generated summary 				 Most video reasoning models only generate textual reasoning traces with...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 3. HoloCine generates coherent multi-shot narratives using a Window Cross-Attention mechanism and Sparse Inter-Shot Self-Attention, enabling end-to-end cinematic creation.  					AI-generated summary 				 State-of-the-art text-to-video models excel at generating isolated clips but fall short of creating...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 4. Loopholing Discrete Diffusion Models (LDDMs) enhance text generation by preserving distributional information through a deterministic latent pathway, reducing perplexity and improving coherence and performance on reasoning tasks.  					AI-generated summary 				 Discrete diffusion models offer a prom...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 5. Dynamic Position Extrapolation (DyPE) enhances ultra-high-resolution image generation by dynamically adjusting positional encodings in pre-trained diffusion transformers, achieving state-of-the-art fidelity without additional sampling cost.  					AI-generated summary 				 Diffusion Transformer model...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 6. RLEV, a reinforcement learning method, aligns LLM optimization with human value signals, improving value-weighted accuracy and learning value-sensitive termination policies.  					AI-generated summary 				 We propose Reinforcement Learning with Explicit Human Values (RLEV), a method that aligns Larg...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 7. MLEB is the largest open-source benchmark for legal information retrieval, encompassing multiple jurisdictions, document types, and task types.  					AI-generated summary 				 We present the Massive Legal Embedding Benchmark (MLEB), the largest, most diverse, and most comprehensive open-source bench...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 8. SAKE is a benchmark for editing auditory attribute knowledge in Large Audio-Language Models, addressing challenges in reliability, generality, locality, and portability.  					AI-generated summary 				 Knowledge editing offers an efficient way to update model knowledge without full retraining, but p...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 9. Research investigates the impact of speaker emotion on the safety of large audio-language models, revealing inconsistencies and vulnerabilities that require targeted alignment strategies.  					AI-generated summary 				 Large audio-language models (LALMs) extend text-based LLMs with auditory underst...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 10. Self-play training for deep search agents improves performance through co-evolution of task generation and problem solving without supervision.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) has become the mainstream technique for training LLM agents. However, ...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 11. LayerComposer provides interactive control over spatial composition and scalability in multi-subject text-to-image generation through a layered canvas and locking mechanism.  					AI-generated summary 				 Despite their impressive visual fidelity, existing personalized generative models lack interac...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 12. Conan, a framework for evidence-grounded multi-step video reasoning, enhances visual grounding and reasoning accuracy through a multi-stage training strategy and outperforms existing models on various benchmarks.  					AI-generated summary 				 Video reasoning, which requires multi-step deduction ac...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 13. Seed3D 1.0 generates scalable, physics-accurate 3D assets from images for use in simulation environments, enhancing both content diversity and real-time physics feedback.  					AI-generated summary 				 Developing embodied AI agents requires scalable training environments that balance content divers...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 14. A benchmark for code-diff understanding with tasks including apply, anti-apply, and diff generation, revealing optimal diff formats based on model size and use case.  					AI-generated summary 				 Reliable handling of code diffs is central to agents that edit and refactor repositories at scale. We ...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 15. A novel AutoRegressive Generation-based paradigm for image segmentation leverages multimodal large language models and VQ-VAE for dense mask generation, achieving high performance and fast inference.  					AI-generated summary 				 We propose a novel AutoRegressive Generation-based paradigm for imag...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 16. The $\alpha$-Flow framework improves few-step generative modeling by unifying and disentangling conflicting objectives, leading to better convergence and state-of-the-art performance on ImageNet-1K.  					AI-generated summary 				 MeanFlow has recently emerged as a powerful framework for few-step ge...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 17. ImpossibleBench is a benchmark framework that measures and mitigates LLMs' tendency to exploit test cases by introducing impossible task variants, thereby enhancing model reliability.  					AI-generated summary 				 The tendency to find and exploit "shortcuts" to complete tasks poses significant ris...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 18. Thought communication enables direct mind-to-mind interaction between agents, uncovering latent thoughts and improving collaborative intelligence beyond natural language.  					AI-generated summary 				 Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect natur...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 19. The guide outlines a progression from early masked models to memory-augmented systems, emphasizing generative capabilities, interactive loops, and memory for building world models.  					AI-generated summary 				 This is not a typical survey of world models; it is a guide for those who want to build...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 20. CiteGuard, a retrieval-aware agent framework, enhances citation accuracy in LLM-generated text by aligning citations with human choices, achieving near-human performance.  					AI-generated summary 				 Large Language Models (LLMs) have emerged as promising assistants for scientific writing. However...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 21. A conditional scaling law is introduced to optimize architectural choices for large language models, balancing accuracy and inference efficiency.  					AI-generated summary 				 Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large ...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 22. ComProScanner, an autonomous multi-agent platform, extracts, validates, classifies, and visualizes chemical compositions and properties from scientific literature, outperforming various LLMs in accuracy.  					AI-generated summary 				 Since the advent of various pre-trained large language models, e...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 23. C2C, a scalable framework for multi-agent LLM systems, improves task completion time through the Alignment Factor and Sequential Action Framework, enabling cost-aware communication and dynamic task understanding.  					AI-generated summary 				 Teamwork in workspace for complex tasks requires divers...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 24. Adamas, a sparse attention mechanism, achieves high accuracy and speed in long-context inference by using Hadamard transform, bucketization, 2-bit compression, and Manhattan-distance estimation.  					AI-generated summary 				 Large language models (LLMs) now support context windows of hundreds of t...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 25. A unified benchmark evaluates attention mechanisms in transformer-based LLMs, focusing on efficiency, scalability, and performance across different attention mask patterns and sequence lengths.  					AI-generated summary 				 Transformer-based large language models (LLMs) have achieved remarkable su...
[24.10.2025 17:12] ********************************************************************************
[24.10.2025 17:12] Abstract 26. A one-layer transformer model demonstrates how linear subspaces separating true from false statements can emerge in language models through memorization and subsequent linear separation.  					AI-generated summary 				 Recent probing studies reveal that large language models exhibit linear subspaces...
[24.10.2025 17:12] Read previous papers.
[24.10.2025 17:12] Generating reviews via LLM API.
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#hallucinations", "#open_source", "#science", "#agents"], "emoji": "üåê", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã", "desc": "AutoPage ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Å
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#alignment", "#inference", "#training"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞", "desc": "Speculative Decoding —É—Å–∫–æ—Ä—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –º–∞–ª–µ–Ω—å–∫—É—é draft-–º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#dataset", "#video", "#training", "#benchmark", "#reasoning", "#games"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏", "desc": "Open-o3 Video ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –æ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –ø—Ä–æ—Å—Ç–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#architecture", "#story_generation", "#video", "#cv"], "emoji": "üé¨", "ru": {"title": "–û—Ç –∫–ª–∏–ø–æ–≤ –∫ –∫–∏–Ω–æ: —Ü–µ–ª–æ—Å—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ-–Ω–∞—Ä—Ä–∞—Ç–∏–≤–æ–≤", "desc": "HoloCine ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤—è–∑–Ω—ã—Ö –≤–∏–¥–µ–æ-–Ω–∞—Ä—Ä–∞—Ç–∏–≤–æ–≤, —Å–æ—Å—Ç–æ—è—â–∏—Ö –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∫–∞–¥—Ä–æ–≤, —á—Ç–æ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö text-t
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#data", "#multimodal", "#training", "#reasoning", "#diffusion"], "emoji": "üîÑ", "ru": {"title": "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Loopholing Discrete Diffusion Models (LDDMs) ‚Äî —É–ª—É—á—à–µ–Ω–Ω—ã–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –¥–∏—Ñ—Ñ
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#cv", "#benchmark", "#architecture"], "emoji": "üî≠", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–µ—Ä—Ö–≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —á–µ—Ä–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—é –ø–æ–∑–∏—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Dynamic Position Extrapolation (DyPE) - –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–≤–µ—Ä
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#optimization", "#rl", "#alignment", "#rlhf"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ AI —Å —É—á—ë—Ç–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ RLEV, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç reinforcement learning –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á—ë—Ç–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π –∏ –ø—Ä
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#data"], "emoji": "‚öñÔ∏è", "ru": {"title": "–≠—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤–æ –≤—Å–µ—Ö –ø—Ä–∞–≤–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MLEB ‚Äî –∫—Ä—É–ø–Ω–µ–π—à–∏–π –æ—Ç–∫—Ä—ã—Ç—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Å—Ñ–µ—Ä–µ. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –¥–µ—Å—è—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#audio", "#benchmark", "#multimodal"], "emoji": "üîä", "ru": {"title": "–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–≤—É–∫–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π –≤ AI-–º–æ–¥–µ–ª—è—Ö –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω SAKE ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –æ–± –∞—É–¥–∏–∞–ª—å–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–∞—Ö –≤ –±–æ–ª—å—à–∏—Ö –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LALM)
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#audio", "#dataset", "#security", "#alignment", "#multimodal"], "emoji": "üò†", "ru": {"title": "–≠–º–æ—Ü–∏–∏ –∫–∞–∫ —É—è–∑–≤–∏–º–æ—Å—Ç—å: –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ-LLM –ø–æ–¥ —É–≥—Ä–æ–∑–æ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç, –∫–∞–∫ —ç–º–æ—Ü–∏–∏ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –≤–ª–∏—è—é—Ç –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LALMs). –£—á—ë–Ω—ã–µ —Å–æ–∑–¥–∞
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#agents", "#rag", "#games", "#rl"], "emoji": "üîÑ", "ru": {"title": "Self-play –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤: —É—á–∏–º—Å—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ self-play, –≥–¥–µ LLM –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#synthetic", "#multimodal", "#games", "#cv"], "emoji": "üé®", "ru": {"title": "–ü–æ—Å–ª–æ–π–Ω–∞—è –∫–æ–º–ø–æ–∑–∏—Ü–∏—è: –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "LayerComposer ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏, –∫–æ—Ç–æ
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#reasoning", "#long_context", "#dataset", "#rl", "#video"], "emoji": "üîç", "ru": {"title": "Conan: –ü–æ—à–∞–≥–æ–≤–æ–µ –≤–∏–¥–µ–æ—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Conan ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞–¥ –≤–∏–¥–µ–æ —Å –æ–ø–æ—Ä–æ–π 
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#agents", "#games", "#3d", "#robotics", "#synthetic"], "emoji": "üé≤", "ru": {"title": "–û—Ç –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∫ —Å–∏–º—É–ª—è—Ü–∏–∏: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —Ç–æ—á–Ω—ã—Ö 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è AI-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "Seed3D 1.0 ‚Äî —ç—Ç–æ foundation –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–æ—Ç–æ–≤—ã–µ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç—ã –∏–∑ –æ–¥–Ω–æ–≥–æ 
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#optimization", "#open_source"], "emoji": "üîÑ", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã diff –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –∑–∞–¥–∞—á", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Diff-XYZ ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è code diff –º–æ–¥–µ–ª—è–º–∏ —Å —Ç—Ä–µ–º—è –∑–∞–¥–∞—á–∞–º–∏: –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–∞—Ç—á–∞, –æ–±—Ä–∞—Ç–Ω–æ–µ
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#inference", "#games", "#cv"], "emoji": "üé≠", "ru": {"title": "–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –º–∞—Å–æ–∫", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ARGenSeg, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#data", "#optimization", "#training", "#cv", "#diffusion"], "emoji": "üåä", "ru": {"title": "Œ±-Flow: —Ä–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π –≤ –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Œ±-Flow ‚Äî —É–ª—É—á—à–µ–Ω–Ω—ã–π framework –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ç—Ä–µ–±—É–µ—Ç –≤—Å–µ–≥–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ 
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#security"], "emoji": "üö´", "ru": {"title": "–õ–æ–≤—É—à–∫–∞ –¥–ª—è —á–∏—Ç–µ—Ä–æ–≤: –∫–∞–∫ –ø–æ–π–º–∞—Ç—å LLM –Ω–∞ –Ω–µ—á–µ—Å—Ç–Ω–æ–º —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á", "desc": "ImpossibleBench ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è LLM, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–º–µ—Ä—è–µ—Ç —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –Ω–µ—á–µ—Å—Ç–Ω—ã—Ö —Å–ø–æ—Å–æ–±–æ–≤ —Ä–µ—à–µ
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#benchmark", "#reasoning", "#interpretability", "#synthetic"], "emoji": "üß†", "ru": {"title": "–¢–µ–ª–µ–ø–∞—Ç–∏—è –¥–ª—è –ò–ò: –æ–±—â–µ–Ω–∏–µ –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ —Å–∫—Ä—ã—Ç—ã–µ –º—ã—Å–ª–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –º–µ–∂–¥—É AI-–∞–≥–µ–Ω—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º –æ–±–º
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#architecture", "#multimodal", "#games", "#diffusion"], "emoji": "üåç", "ru": {"title": "–ü—É—Ç–µ–≤–æ–¥–∏—Ç–µ–ª—å –ø–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤—É –º–∏—Ä–æ–≤: –æ—Ç –º–∞—Å–æ–∫ –∫ –ø–∞–º—è—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é world models, –∞ –Ω–µ –æ–±—ã—á–Ω—ã–π –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ—Å–ª–µ–∂–∏–≤–∞—é—Ç 
[24.10.2025 17:12] Using data from previous issue: {"categories": ["#rag", "#science", "#benchmark", "#agents", "#alignment"], "emoji": "üìö", "ru": {"title": "CiteGuard: —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ —á–µ–ª–æ–≤–µ–∫–∞ –¥–ª—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –≤ –Ω–∞—É—á–Ω–æ–º –ø–∏—Å—å–º–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CiteGuard ‚Äî –∞–≥–µ–Ω—Ç—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–∞—Ö, –≥–µ–Ω–µ—Ä–∏
[24.10.2025 17:12] Querying the API.
[24.10.2025 17:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A conditional scaling law is introduced to optimize architectural choices for large language models, balancing accuracy and inference efficiency.  					AI-generated summary 				 Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large language model (LLM) performance. Yet, as these models grow increasingly powerful and widely deployed, the cost of inference has become a pressing concern. Despite its importance, the trade-off between model accuracy and inference efficiency remains underexplored. In this work, we examine how key architectural factors, hidden size, the allocation of parameters between MLP and attention (mlp-to-attention ratio), and grouped-query attention (GQA), influence both inference cost and accuracy. We introduce a conditional scaling law that augments the Chinchilla framework with architectural information, along with a search framework for identifying architectures that are simultaneously inference-efficient and accurate. To validate our approach, we train more than 200 models spanning 80M to 3B parameters and 8B to 100B training tokens, and fit the proposed conditional scaling law. Our results show that the conditional scaling law reliably predicts optimal architectural choices and that the resulting models outperform existing open-source baselines. Under the same training budget, optimized architectures achieve up to 2.1% higher accuracy and 42% greater inference throughput compared to LLaMA-3.2.
[24.10.2025 17:12] Response: ```json
{
  "title": "–£—Å–ª–æ–≤–Ω—ã–π –∑–∞–∫–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã LLM",
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —É—Å–ª–æ–≤–Ω—ã–π –∑–∞–∫–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –≤—ã–±—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á—ë—Ç–æ–º –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. –û–Ω–∏ –∏–∑—É—á–∏–ª–∏ –≤–ª–∏—è–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: —Ä–∞–∑–º–µ—Ä–∞ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—ë–≤, —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–µ–∂–¥—É MLP –∏ attention –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏, –∞ —Ç–∞–∫–∂–µ grouped-query attention –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–¥—Ö–æ–¥–∞ –æ–±—É—á–∏–ª–∏ –±–æ–ª–µ–µ 200 –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–º –æ—Ç 80M –¥–æ 3B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –æ—Ç 8B –¥–æ 100B —Ç–æ–∫–µ–Ω–æ–≤. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–æ–∫–∞–∑–∞–ª–∏ –Ω–∞ 2.1% –≤—ã—à–µ —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –Ω–∞ 42% –±–æ–ª—å—à–µ throughput –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å LLaMA-3.2 –ø—Ä–∏ —Ç–æ–º –∂–µ –±—é–¥–∂–µ—Ç–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ.",
  "emoji": "‚öñÔ∏è"
}
```
[24.10.2025 17:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A conditional scaling law is introduced to optimize architectural choices for large language models, balancing accuracy and inference efficiency.  					AI-generated summary 				 Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large language model (LLM) performance. Yet, as these models grow increasingly powerful and widely deployed, the cost of inference has become a pressing concern. Despite its importance, the trade-off between model accuracy and inference efficiency remains underexplored. In this work, we examine how key architectural factors, hidden size, the allocation of parameters between MLP and attention (mlp-to-attention ratio), and grouped-query attention (GQA), influence both inference cost and accuracy. We introduce a conditional scaling law that augments the Chinchilla framework with architectural information, along with a search framework for identifying architectures that are simultaneously inference-efficient and accurate. To validate our approach, we train more than 200 models spanning 80M to 3B parameters and 8B to 100B training tokens, and fit the proposed conditional scaling law. Our results show that the conditional scaling law reliably predicts optimal architectural choices and that the resulting models outperform existing open-source baselines. Under the same training budget, optimized architectures achieve up to 2.1% higher accuracy and 42% greater inference throughput compared to LLaMA-3.2."

[24.10.2025 17:12] Response: ```python
['ARCHITECTURE', 'INFERENCE', 'TRAINING']
```
[24.10.2025 17:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A conditional scaling law is introduced to optimize architectural choices for large language models, balancing accuracy and inference efficiency.  					AI-generated summary 				 Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large language model (LLM) performance. Yet, as these models grow increasingly powerful and widely deployed, the cost of inference has become a pressing concern. Despite its importance, the trade-off between model accuracy and inference efficiency remains underexplored. In this work, we examine how key architectural factors, hidden size, the allocation of parameters between MLP and attention (mlp-to-attention ratio), and grouped-query attention (GQA), influence both inference cost and accuracy. We introduce a conditional scaling law that augments the Chinchilla framework with architectural information, along with a search framework for identifying architectures that are simultaneously inference-efficient and accurate. To validate our approach, we train more than 200 models spanning 80M to 3B parameters and 8B to 100B training tokens, and fit the proposed conditional scaling law. Our results show that the conditional scaling law reliably predicts optimal architectural choices and that the resulting models outperform existing open-source baselines. Under the same training budget, optimized architectures achieve up to 2.1% higher accuracy and 42% greater inference throughput compared to LLaMA-3.2."

[24.10.2025 17:12] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```
[24.10.2025 17:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a conditional scaling law aimed at optimizing the architecture of large language models (LLMs) to enhance both accuracy and inference efficiency. It explores how different architectural elements, such as hidden size and the distribution of parameters between multi-layer perceptrons (MLP) and attention mechanisms, affect model performance and inference costs. The authors propose a framework that integrates architectural insights into the Chinchilla scaling laws, allowing for the identification of architectures that maximize efficiency without sacrificing accuracy. Their experiments with over 200 models demonstrate that the proposed scaling law can predict optimal configurations, leading to significant improvements in performance compared to existing models.","title":"Optimizing Large Language Models for Efficiency and Accuracy"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a conditional scaling law aimed at optimizing the architecture of large language models (LLMs) to enhance both accuracy and inference efficiency. It explores how different architectural elements, such as hidden size and the distribution of parameters between multi-layer perceptrons (MLP) and attention mechanisms, affect model performance and inference costs. The authors propose a framework that integrates architectural insights into the Chinchilla scaling laws, allowing for the identification of architectures that maximize efficiency without sacrificing accuracy. Their experiments with over 200 models demonstrate that the proposed scaling law can predict optimal configurations, leading to significant improvements in performance compared to existing models.', title='Optimizing Large Language Models for Efficiency and Accuracy'))
[24.10.2025 17:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊù°‰ª∂Áº©ÊîæÊ≥ïÂàôÔºåÁî®‰∫é‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊû∂ÊûÑÈÄâÊã©Ôºå‰ª•Âπ≥Ë°°ÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜÊïàÁéá„ÄÇÈöèÁùÄÊ®°ÂûãÂèÇÊï∞ÂíåËÆ≠ÁªÉÊï∞ÊçÆËßÑÊ®°ÁöÑÂ¢ûÂä†ÔºåÊé®ÁêÜÊàêÊú¨Êàê‰∏∫‰∏Ä‰∏™ÈáçË¶ÅÈóÆÈ¢ò„ÄÇÊàë‰ª¨Á†îÁ©∂‰∫ÜÈöêËóèÂ±ÇÂ§ßÂ∞è„ÄÅMLP‰∏éÊ≥®ÊÑèÂäõÂèÇÊï∞ÂàÜÈÖçÊØî‰æã‰ª•ÂèäÂàÜÁªÑÊü•ËØ¢Ê≥®ÊÑèÂäõÁ≠âÂÖ≥ÈîÆÊû∂ÊûÑÂõ†Á¥†Â¶Ç‰ΩïÂΩ±ÂìçÊé®ÁêÜÊàêÊú¨ÂíåÂáÜÁ°ÆÊÄß„ÄÇÈÄöËøáËÆ≠ÁªÉË∂ÖËøá200‰∏™Ê®°ÂûãÔºåÊàë‰ª¨È™åËØÅ‰∫ÜÊù°‰ª∂Áº©ÊîæÊ≥ïÂàôÁöÑÊúâÊïàÊÄßÔºå‰ºòÂåñÂêéÁöÑÊû∂ÊûÑÂú®Áõ∏ÂêåËÆ≠ÁªÉÈ¢ÑÁÆó‰∏ãÂÆûÁé∞‰∫ÜÊõ¥È´òÁöÑÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜÂêûÂêêÈáè„ÄÇ","title":"‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊû∂ÊûÑÈÄâÊã©"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊù°‰ª∂Áº©ÊîæÊ≥ïÂàôÔºåÁî®‰∫é‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊû∂ÊûÑÈÄâÊã©Ôºå‰ª•Âπ≥Ë°°ÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜÊïàÁéá„ÄÇÈöèÁùÄÊ®°ÂûãÂèÇÊï∞ÂíåËÆ≠ÁªÉÊï∞ÊçÆËßÑÊ®°ÁöÑÂ¢ûÂä†ÔºåÊé®ÁêÜÊàêÊú¨Êàê‰∏∫‰∏Ä‰∏™ÈáçË¶ÅÈóÆÈ¢ò„ÄÇÊàë‰ª¨Á†îÁ©∂‰∫ÜÈöêËóèÂ±ÇÂ§ßÂ∞è„ÄÅMLP‰∏éÊ≥®ÊÑèÂäõÂèÇÊï∞ÂàÜÈÖçÊØî‰æã‰ª•ÂèäÂàÜÁªÑÊü•ËØ¢Ê≥®ÊÑèÂäõÁ≠âÂÖ≥ÈîÆÊû∂ÊûÑÂõ†Á¥†Â¶Ç‰ΩïÂΩ±ÂìçÊé®ÁêÜÊàêÊú¨ÂíåÂáÜÁ°ÆÊÄß„ÄÇÈÄöËøáËÆ≠ÁªÉË∂ÖËøá200‰∏™Ê®°ÂûãÔºåÊàë‰ª¨È™åËØÅ‰∫ÜÊù°‰ª∂Áº©ÊîæÊ≥ïÂàôÁöÑÊúâÊïàÊÄßÔºå‰ºòÂåñÂêéÁöÑÊû∂ÊûÑÂú®Áõ∏ÂêåËÆ≠ÁªÉÈ¢ÑÁÆó‰∏ãÂÆûÁé∞‰∫ÜÊõ¥È´òÁöÑÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜÂêûÂêêÈáè„ÄÇ', title='‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊû∂ÊûÑÈÄâÊã©'))
[24.10.2025 17:13] Using data from previous issue: {"categories": ["#data", "#dataset", "#agents", "#benchmark", "#open_source", "#science"], "emoji": "üî¨", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π", "desc": "ComProScanner ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –∫–ª–∞—Å—Å
[24.10.2025 17:13] Using data from previous issue: {"categories": ["#alignment", "#agents", "#multimodal"], "emoji": "ü§ù", "ru": {"title": "–£–º–Ω–∞—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥–Ω–æ–π —Ä–∞–±–æ—Ç—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ C2C ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è ‚Äî –º–µ—Ç—Ä–∏–∫–∞ Alignment Fa
[24.10.2025 17:13] Using data from previous issue: {"categories": ["#inference", "#long_context", "#optimization", "#training", "#architecture"], "emoji": "üíé", "ru": {"title": "Adamas: —Å–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "Adamas ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è LLM, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±
[24.10.2025 17:13] Using data from previous issue: {"categories": ["#benchmark", "#long_context", "#optimization", "#architecture"], "emoji": "üîç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –µ–¥–∏–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö LLM, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç—Ä–∞–¥–∞—é—Ç 
[24.10.2025 17:13] Using data from previous issue: {"categories": ["#architecture", "#training", "#reasoning", "#interpretability"], "emoji": "üéØ", "ru": {"title": "–ö–∞–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —É—á–∞—Ç—Å—è –æ—Ç–ª–∏—á–∞—Ç—å –ø—Ä–∞–≤–¥—É –æ—Ç –ª–∂–∏ —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –ø—Ä–æ—Å—Ç—É—é –æ–¥–Ω–æ—Å–ª–æ–π–Ω—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, –∫–∞–∫ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥
[24.10.2025 17:13] Renaming data file.
[24.10.2025 17:13] Renaming previous data. hf_papers.json to ./d/2025-10-24.json
[24.10.2025 17:13] Saving new data file.
[24.10.2025 17:13] Generating page.
[24.10.2025 17:13] Renaming previous page.
[24.10.2025 17:13] Renaming previous data. index.html to ./d/2025-10-24.html
[24.10.2025 17:13] Writing result.
[24.10.2025 17:13] Renaming log file.
[24.10.2025 17:13] Renaming previous data. log.txt to ./logs/2025-10-24_last_log.txt
