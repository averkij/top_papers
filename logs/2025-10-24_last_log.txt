[24.10.2025 21:10] Read previous papers.
[24.10.2025 21:10] Generating top page (month).
[24.10.2025 21:10] Writing top page (month).
[24.10.2025 22:11] Read previous papers.
[24.10.2025 22:11] Get feed.
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19600
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19779
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20579
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20822
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19304
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20766
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20187
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19365
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16917
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16893
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18821
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20820
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20470
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19944
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.12487
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20803
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20771
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20733
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20270
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20668
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17853
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19423
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18245
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20362
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.19995
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.18413
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.17896
[24.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15804
[24.10.2025 22:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[24.10.2025 22:11] No deleted papers detected.
[24.10.2025 22:11] Downloading and parsing papers (pdf, html). Total: 28.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.19600.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.19600.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.19600.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.19779.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.19779.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.19779.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.20579.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.20579.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.20579.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.20822.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.20822.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.20822.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.19304.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.19304.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.19304.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.20766.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.20766.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.20766.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.20187.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.20187.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.20187.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.19365.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.19365.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.19365.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.16917.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.16917.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.16917.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.16893.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.16893.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.16893.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.18821.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.18821.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.18821.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.20820.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.20820.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.20820.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.20470.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.20470.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.20470.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.19944.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.19944.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.19944.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.12487.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.12487.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.12487.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.20803.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.20803.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.20803.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.20771.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.20771.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.20771.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.20733.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.20733.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.20733.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.20270.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.20270.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.20270.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.20668.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.20668.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.20668.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.17853.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.17853.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.17853.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.19423.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.19423.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.19423.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.18245.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.18245.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.18245.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.20362.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.20362.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.20362.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.19995.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.19995.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.19995.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.18413.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.18413.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.18413.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.17896.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.17896.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.17896.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.15804.
[24.10.2025 22:11] Extra JSON file exists (./assets/json/2510.15804.json), skip PDF parsing.
[24.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.15804.json), skip HTML parsing.
[24.10.2025 22:11] Success.
[24.10.2025 22:11] Enriching papers with extra data.
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 0. AutoPage, a multi-agent system, automates the creation of interactive research webpages through a hierarchical process, ensuring high-quality and efficient results.  					AI-generated summary 				 In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, ...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 1. AdaSPEC enhances speculative decoding by selectively filtering tokens during knowledge distillation, improving token acceptance rates without sacrificing generation quality.  					AI-generated summary 				 Speculative Decoding (SD) accelerates large language model inference by employing a small draf...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 2. Open-o3 Video integrates spatio-temporal evidence into video reasoning, achieving state-of-the-art performance on multiple benchmarks and providing valuable reasoning traces for test-time scaling.  					AI-generated summary 				 Most video reasoning models only generate textual reasoning traces with...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 3. HoloCine generates coherent multi-shot narratives using a Window Cross-Attention mechanism and Sparse Inter-Shot Self-Attention, enabling end-to-end cinematic creation.  					AI-generated summary 				 State-of-the-art text-to-video models excel at generating isolated clips but fall short of creating...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 4. Loopholing Discrete Diffusion Models (LDDMs) enhance text generation by preserving distributional information through a deterministic latent pathway, reducing perplexity and improving coherence and performance on reasoning tasks.  					AI-generated summary 				 Discrete diffusion models offer a prom...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 5. Dynamic Position Extrapolation (DyPE) enhances ultra-high-resolution image generation by dynamically adjusting positional encodings in pre-trained diffusion transformers, achieving state-of-the-art fidelity without additional sampling cost.  					AI-generated summary 				 Diffusion Transformer model...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 6. RLEV, a reinforcement learning method, aligns LLM optimization with human value signals, improving value-weighted accuracy and learning value-sensitive termination policies.  					AI-generated summary 				 We propose Reinforcement Learning with Explicit Human Values (RLEV), a method that aligns Larg...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 7. MLEB is the largest open-source benchmark for legal information retrieval, encompassing multiple jurisdictions, document types, and task types.  					AI-generated summary 				 We present the Massive Legal Embedding Benchmark (MLEB), the largest, most diverse, and most comprehensive open-source bench...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 8. SAKE is a benchmark for editing auditory attribute knowledge in Large Audio-Language Models, addressing challenges in reliability, generality, locality, and portability.  					AI-generated summary 				 Knowledge editing offers an efficient way to update model knowledge without full retraining, but p...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 9. Research investigates the impact of speaker emotion on the safety of large audio-language models, revealing inconsistencies and vulnerabilities that require targeted alignment strategies.  					AI-generated summary 				 Large audio-language models (LALMs) extend text-based LLMs with auditory underst...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 10. Self-play training for deep search agents improves performance through co-evolution of task generation and problem solving without supervision.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) has become the mainstream technique for training LLM agents. However, ...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 11. LayerComposer provides interactive control over spatial composition and scalability in multi-subject text-to-image generation through a layered canvas and locking mechanism.  					AI-generated summary 				 Despite their impressive visual fidelity, existing personalized generative models lack interac...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 12. Conan, a framework for evidence-grounded multi-step video reasoning, enhances visual grounding and reasoning accuracy through a multi-stage training strategy and outperforms existing models on various benchmarks.  					AI-generated summary 				 Video reasoning, which requires multi-step deduction ac...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 13. Seed3D 1.0 generates scalable, physics-accurate 3D assets from images for use in simulation environments, enhancing both content diversity and real-time physics feedback.  					AI-generated summary 				 Developing embodied AI agents requires scalable training environments that balance content divers...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 14. A benchmark for code-diff understanding with tasks including apply, anti-apply, and diff generation, revealing optimal diff formats based on model size and use case.  					AI-generated summary 				 Reliable handling of code diffs is central to agents that edit and refactor repositories at scale. We ...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 15. A novel AutoRegressive Generation-based paradigm for image segmentation leverages multimodal large language models and VQ-VAE for dense mask generation, achieving high performance and fast inference.  					AI-generated summary 				 We propose a novel AutoRegressive Generation-based paradigm for imag...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 16. The $\alpha$-Flow framework improves few-step generative modeling by unifying and disentangling conflicting objectives, leading to better convergence and state-of-the-art performance on ImageNet-1K.  					AI-generated summary 				 MeanFlow has recently emerged as a powerful framework for few-step ge...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 17. Thought communication enables direct mind-to-mind interaction between agents, uncovering latent thoughts and improving collaborative intelligence beyond natural language.  					AI-generated summary 				 Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect natur...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 18. ImpossibleBench is a benchmark framework that measures and mitigates LLMs' tendency to exploit test cases by introducing impossible task variants, thereby enhancing model reliability.  					AI-generated summary 				 The tendency to find and exploit "shortcuts" to complete tasks poses significant ris...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 19. The guide outlines a progression from early masked models to memory-augmented systems, emphasizing generative capabilities, interactive loops, and memory for building world models.  					AI-generated summary 				 This is not a typical survey of world models; it is a guide for those who want to build...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 20. CiteGuard, a retrieval-aware agent framework, enhances citation accuracy in LLM-generated text by aligning citations with human choices, achieving near-human performance.  					AI-generated summary 				 Large Language Models (LLMs) have emerged as promising assistants for scientific writing. However...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 21. MSC-Bench evaluates multi-hop tool orchestration by LLM agents in a hierarchical ecosystem, addressing challenges like functional overlap and cross-server planning with a five-level curriculum and objective metrics.  					AI-generated summary 				 We introduce MSC-Bench, a large-scale benchmark for ...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 22. A conditional scaling law is introduced to optimize architectural choices for large language models, balancing accuracy and inference efficiency.  					AI-generated summary 				 Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large ...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 23. ComProScanner, an autonomous multi-agent platform, extracts, validates, classifies, and visualizes chemical compositions and properties from scientific literature, outperforming various LLMs in accuracy.  					AI-generated summary 				 Since the advent of various pre-trained large language models, e...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 24. C2C, a scalable framework for multi-agent LLM systems, improves task completion time through the Alignment Factor and Sequential Action Framework, enabling cost-aware communication and dynamic task understanding.  					AI-generated summary 				 Teamwork in workspace for complex tasks requires divers...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 25. Adamas, a sparse attention mechanism, achieves high accuracy and speed in long-context inference by using Hadamard transform, bucketization, 2-bit compression, and Manhattan-distance estimation.  					AI-generated summary 				 Large language models (LLMs) now support context windows of hundreds of t...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 26. A unified benchmark evaluates attention mechanisms in transformer-based LLMs, focusing on efficiency, scalability, and performance across different attention mask patterns and sequence lengths.  					AI-generated summary 				 Transformer-based large language models (LLMs) have achieved remarkable su...
[24.10.2025 22:11] ********************************************************************************
[24.10.2025 22:11] Abstract 27. A one-layer transformer model demonstrates how linear subspaces separating true from false statements can emerge in language models through memorization and subsequent linear separation.  					AI-generated summary 				 Recent probing studies reveal that large language models exhibit linear subspaces...
[24.10.2025 22:11] Read previous papers.
[24.10.2025 22:11] Generating reviews via LLM API.
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#hallucinations", "#open_source", "#science", "#agents"], "emoji": "üåê", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã", "desc": "AutoPage ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Å
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#alignment", "#inference", "#training"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞", "desc": "Speculative Decoding —É—Å–∫–æ—Ä—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –º–∞–ª–µ–Ω—å–∫—É—é draft-–º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#dataset", "#video", "#training", "#benchmark", "#reasoning", "#games"], "emoji": "üé¨", "ru": {"title": "–í–∏–¥–µ–æ-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏", "desc": "Open-o3 Video ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –æ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –ø—Ä–æ—Å—Ç–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#architecture", "#story_generation", "#video", "#cv"], "emoji": "üé¨", "ru": {"title": "–û—Ç –∫–ª–∏–ø–æ–≤ –∫ –∫–∏–Ω–æ: —Ü–µ–ª–æ—Å—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ-–Ω–∞—Ä—Ä–∞—Ç–∏–≤–æ–≤", "desc": "HoloCine ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤—è–∑–Ω—ã—Ö –≤–∏–¥–µ–æ-–Ω–∞—Ä—Ä–∞—Ç–∏–≤–æ–≤, —Å–æ—Å—Ç–æ—è—â–∏—Ö –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∫–∞–¥—Ä–æ–≤, —á—Ç–æ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö text-t
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#data", "#multimodal", "#training", "#reasoning", "#diffusion"], "emoji": "üîÑ", "ru": {"title": "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Loopholing Discrete Diffusion Models (LDDMs) ‚Äî —É–ª—É—á—à–µ–Ω–Ω—ã–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –¥–∏—Ñ—Ñ
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#cv", "#benchmark", "#architecture"], "emoji": "üî≠", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–µ—Ä—Ö–≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —á–µ—Ä–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—é –ø–æ–∑–∏—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Dynamic Position Extrapolation (DyPE) - –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–≤–µ—Ä
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#rl", "#alignment", "#rlhf"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ AI —Å —É—á—ë—Ç–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ RLEV, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç reinforcement learning –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á—ë—Ç–æ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π –∏ –ø—Ä
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#open_source", "#data"], "emoji": "‚öñÔ∏è", "ru": {"title": "–≠—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤–æ –≤—Å–µ—Ö –ø—Ä–∞–≤–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MLEB ‚Äî –∫—Ä—É–ø–Ω–µ–π—à–∏–π –æ—Ç–∫—Ä—ã—Ç—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Å—Ñ–µ—Ä–µ. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –¥–µ—Å—è—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#audio", "#benchmark", "#multimodal"], "emoji": "üîä", "ru": {"title": "–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–≤—É–∫–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π –≤ AI-–º–æ–¥–µ–ª—è—Ö –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω SAKE ‚Äî –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –æ–± –∞—É–¥–∏–∞–ª—å–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–∞—Ö –≤ –±–æ–ª—å—à–∏—Ö –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LALM)
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#audio", "#dataset", "#security", "#alignment", "#multimodal"], "emoji": "üò†", "ru": {"title": "–≠–º–æ—Ü–∏–∏ –∫–∞–∫ —É—è–∑–≤–∏–º–æ—Å—Ç—å: –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ-LLM –ø–æ–¥ —É–≥—Ä–æ–∑–æ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç, –∫–∞–∫ —ç–º–æ—Ü–∏–∏ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –≤–ª–∏—è—é—Ç –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LALMs). –£—á—ë–Ω—ã–µ —Å–æ–∑–¥–∞
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#agents", "#rag", "#games", "#rl"], "emoji": "üîÑ", "ru": {"title": "Self-play –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤: —É—á–∏–º—Å—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ self-play, –≥–¥–µ LLM –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#synthetic", "#multimodal", "#games", "#cv"], "emoji": "üé®", "ru": {"title": "–ü–æ—Å–ª–æ–π–Ω–∞—è –∫–æ–º–ø–æ–∑–∏—Ü–∏—è: –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "LayerComposer ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏, –∫–æ—Ç–æ
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#reasoning", "#long_context", "#dataset", "#rl", "#video"], "emoji": "üîç", "ru": {"title": "Conan: –ü–æ—à–∞–≥–æ–≤–æ–µ –≤–∏–¥–µ–æ—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Conan ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞–¥ –≤–∏–¥–µ–æ —Å –æ–ø–æ—Ä–æ–π 
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#agents", "#games", "#3d", "#robotics", "#synthetic"], "emoji": "üé≤", "ru": {"title": "–û—Ç –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∫ —Å–∏–º—É–ª—è—Ü–∏–∏: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —Ç–æ—á–Ω—ã—Ö 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è AI-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "Seed3D 1.0 ‚Äî —ç—Ç–æ foundation –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–æ—Ç–æ–≤—ã–µ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç—ã –∏–∑ –æ–¥–Ω–æ–≥–æ 
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#optimization", "#open_source"], "emoji": "üîÑ", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã diff –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –∑–∞–¥–∞—á", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Diff-XYZ ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è code diff –º–æ–¥–µ–ª—è–º–∏ —Å —Ç—Ä–µ–º—è –∑–∞–¥–∞—á–∞–º–∏: –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–∞—Ç—á–∞, –æ–±—Ä–∞—Ç–Ω–æ–µ
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#inference", "#games", "#cv"], "emoji": "üé≠", "ru": {"title": "–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –º–∞—Å–æ–∫", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ARGenSeg, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#data", "#optimization", "#training", "#cv", "#diffusion"], "emoji": "üåä", "ru": {"title": "Œ±-Flow: —Ä–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π –≤ –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Œ±-Flow ‚Äî —É–ª—É—á—à–µ–Ω–Ω—ã–π framework –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ç—Ä–µ–±—É–µ—Ç –≤—Å–µ–≥–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ 
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#benchmark", "#reasoning", "#interpretability", "#synthetic"], "emoji": "üß†", "ru": {"title": "–¢–µ–ª–µ–ø–∞—Ç–∏—è –¥–ª—è –ò–ò: –æ–±—â–µ–Ω–∏–µ –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ —Å–∫—Ä—ã—Ç—ã–µ –º—ã—Å–ª–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –º–µ–∂–¥—É AI-–∞–≥–µ–Ω—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º –æ–±–º
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#security"], "emoji": "üö´", "ru": {"title": "–õ–æ–≤—É—à–∫–∞ –¥–ª—è —á–∏—Ç–µ—Ä–æ–≤: –∫–∞–∫ –ø–æ–π–º–∞—Ç—å LLM –Ω–∞ –Ω–µ—á–µ—Å—Ç–Ω–æ–º —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á", "desc": "ImpossibleBench ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è LLM, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–º–µ—Ä—è–µ—Ç —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –Ω–µ—á–µ—Å—Ç–Ω—ã—Ö —Å–ø–æ—Å–æ–±–æ–≤ —Ä–µ—à–µ
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#architecture", "#multimodal", "#games", "#diffusion"], "emoji": "üåç", "ru": {"title": "–ü—É—Ç–µ–≤–æ–¥–∏—Ç–µ–ª—å –ø–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤—É –º–∏—Ä–æ–≤: –æ—Ç –º–∞—Å–æ–∫ –∫ –ø–∞–º—è—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é world models, –∞ –Ω–µ –æ–±—ã—á–Ω—ã–π –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ—Å–ª–µ–∂–∏–≤–∞—é—Ç 
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#rag", "#science", "#benchmark", "#agents", "#alignment"], "emoji": "üìö", "ru": {"title": "CiteGuard: —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ —á–µ–ª–æ–≤–µ–∫–∞ –¥–ª—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –≤ –Ω–∞—É—á–Ω–æ–º –ø–∏—Å—å–º–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CiteGuard ‚Äî –∞–≥–µ–Ω—Ç—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–∞—Ö, –≥–µ–Ω–µ—Ä–∏
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#survey", "#agents", "#alignment"], "emoji": "üîß", "ru": {"title": "MSC-Bench: –ø—Ä–æ–≤–µ—Ä–∫–∞ AI-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Å–ª–æ–∂–Ω—É—é –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—é –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MSC-Bench ‚Äî –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤—ã–≤–∞—Ç—å –º–Ω–æ–≥–æ—à–∞
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#training", "#inference", "#architecture"], "emoji": "‚öñÔ∏è", "ru": {"title": "–£—Å–ª–æ–≤–Ω—ã–π –∑–∞–∫–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —É—Å–ª–æ–≤–Ω—ã–π –∑–∞–∫–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –≤—ã–±—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#data", "#dataset", "#agents", "#benchmark", "#open_source", "#science"], "emoji": "üî¨", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π", "desc": "ComProScanner ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –∫–ª–∞—Å—Å
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#alignment", "#agents", "#multimodal"], "emoji": "ü§ù", "ru": {"title": "–£–º–Ω–∞—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥–Ω–æ–π —Ä–∞–±–æ—Ç—ã", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ C2C ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è ‚Äî –º–µ—Ç—Ä–∏–∫–∞ Alignment Fa
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#inference", "#long_context", "#optimization", "#training", "#architecture"], "emoji": "üíé", "ru": {"title": "Adamas: —Å–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "Adamas ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è LLM, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#benchmark", "#long_context", "#optimization", "#architecture"], "emoji": "üîç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –µ–¥–∏–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö LLM, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç—Ä–∞–¥–∞—é—Ç 
[24.10.2025 22:11] Using data from previous issue: {"categories": ["#architecture", "#training", "#reasoning", "#interpretability"], "emoji": "üéØ", "ru": {"title": "–ö–∞–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —É—á–∞—Ç—Å—è –æ—Ç–ª–∏—á–∞—Ç—å –ø—Ä–∞–≤–¥—É –æ—Ç –ª–∂–∏ —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –ø—Ä–æ—Å—Ç—É—é –æ–¥–Ω–æ—Å–ª–æ–π–Ω—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, –∫–∞–∫ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥
[24.10.2025 22:11] Renaming data file.
[24.10.2025 22:11] Renaming previous data. hf_papers.json to ./d/2025-10-24.json
[24.10.2025 22:11] Saving new data file.
[24.10.2025 22:11] Generating page.
[24.10.2025 22:11] Renaming previous page.
[24.10.2025 22:11] Renaming previous data. index.html to ./d/2025-10-24.html
[24.10.2025 22:11] Writing result.
[24.10.2025 22:11] Renaming log file.
[24.10.2025 22:11] Renaming previous data. log.txt to ./logs/2025-10-24_last_log.txt
