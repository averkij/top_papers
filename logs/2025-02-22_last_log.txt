[22.02.2025 06:31] Read previous papers.
[22.02.2025 06:31] Generating top page (month).
[22.02.2025 06:31] Writing top page (month).
[22.02.2025 12:38] Read previous papers.
[22.02.2025 12:38] Get feed.
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14499
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14786
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14739
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14502
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14382
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14768
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14372
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14834
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12853
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14258
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14282
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14844
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14678
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14638
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14846
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14802
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14377
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14669
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14866
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14044
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11262
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14541
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14191
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14854
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12769
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14409
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13759
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13928
[22.02.2025 12:38] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14842
[22.02.2025 12:38] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.02.2025 12:38] No deleted papers detected.
[22.02.2025 12:38] Downloading and parsing papers (pdf, html). Total: 29.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14499.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14499.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14499.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14786.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14786.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14786.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14739.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14739.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14739.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14502.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14502.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14502.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14382.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14382.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14382.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14768.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14768.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14768.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14372.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14372.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14372.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14834.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14834.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14834.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.12853.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.12853.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.12853.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14258.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14258.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14258.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14282.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14282.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14282.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14844.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14844.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14844.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14678.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14678.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14678.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14638.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14638.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14638.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14846.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14846.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14846.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14802.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14802.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14802.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14377.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14377.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14377.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14669.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14669.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14669.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14866.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14866.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14866.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14044.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14044.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14044.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.11262.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.11262.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.11262.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14541.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14541.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14541.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14191.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14191.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14191.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14854.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14854.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14854.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.12769.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.12769.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.12769.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14409.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14409.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14409.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.13759.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.13759.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.13759.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.13928.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.13928.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.13928.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Downloading and parsing paper https://huggingface.co/papers/2502.14842.
[22.02.2025 12:38] Extra JSON file exists (./assets/json/2502.14842.json), skip PDF parsing.
[22.02.2025 12:38] Paper image links file exists (./assets/img_data/2502.14842.json), skip HTML parsing.
[22.02.2025 12:38] Success.
[22.02.2025 12:38] Enriching papers with extra data.
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 0. We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench c...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 1. We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed techniques into a unified recipe -- this includes ...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 2. Large language models (LLMs) have demonstrated remarkable proficiency in mainstream academic disciplines such as mathematics, physics, and computer science. However, human knowledge encompasses over 200 specialized disciplines, far exceeding the scope of existing benchmarks. The capabilities of LLMs...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 3. The performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model's parameters. Low-rank adaptation (LoRA) is a popular and efficient training technique for updating or domain-specific adaptation of LLMs. In this stu...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 4. Increasing test-time compute for LLMs shows promise across domains but remains underexplored in code generation, despite extensive study in math. In this paper, we propose S*, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of generated co...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 5. Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification. We make s...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 6. The realization of scalable fault-tolerant quantum computing is expected to hinge on quantum error-correcting codes. In the quest for more efficient quantum fault tolerance, a critical code parameter is the weight of measurements that extract information about errors to enable error correction: as h...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 7. Existing Large Vision-Language Models (LVLMs) can process inputs with context lengths up to 128k visual and text tokens, yet they struggle to generate coherent outputs beyond 1,000 words. We find that the primary limitation is the absence of long output examples during supervised fine-tuning (SFT). ...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 8. Recent studies have demonstrated the effectiveness of LLM test-time scaling. However, existing approaches to incentivize LLMs' deep thinking abilities generally require large-scale data or significant training efforts. Meanwhile, it remains unclear how to improve the thinking abilities of less power...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 9. While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads primarily responsible for processing temporal knowledge through circuit analysis. We confirm that t...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 10. In the field of MLLM-based GUI agents, compared to smartphones, the PC scenario not only features a more complex interactive environment, but also involves more intricate intra- and inter-app workflows. To address these issues, we propose a hierarchical agent framework named PC-Agent. Specifically, ...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 11. Personalizing generative text-to-image models has seen remarkable progress, but extending this personalization to text-to-video models presents unique challenges. Unlike static concepts, personalizing text-to-video models has the potential to capture dynamic concepts, i.e., entities defined not only...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 12. The pace of evolution of Large Language Models (LLMs) necessitates new approaches for rigorous and comprehensive evaluation. Traditional human annotation is increasingly impracticable due to the complexities and costs involved in generating high-quality, challenging problems. In this work, we introd...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 13. Image geo-localization is the task of predicting the specific location of an image and requires complex reasoning across visual, geographical, and cultural contexts. While prior Vision Language Models (VLMs) have the best accuracy at this task, there is a dearth of high-quality datasets and models f...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 14. Reasoning about images with rich text, such as charts and documents, is a critical application of vision-language models (VLMs). However, VLMs often struggle in these domains due to the scarcity of diverse text-rich vision-language data. To address this challenge, we present CoSyn, a framework that ...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 15. Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become ...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 16. The Diffusion Transformer plays a pivotal role in advancing text-to-image and text-to-video generation, owing primarily to its inherent scalability. However, existing controlled diffusion transformer methods incur significant parameter and computational overheads and suffer from inefficient resource...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 17. Large Language Models (LLMs) have demonstrated impressive capabilities in language processing, yet they often struggle with tasks requiring genuine visual spatial reasoning. In this paper, we introduce a novel two-stage training framework designed to equip standard LLMs with visual reasoning abiliti...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 18. Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the de...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 19. Large multimodal models (LMMs) have shown impressive capabilities in a wide range of visual tasks. However, they often struggle with fine-grained visual reasoning, failing to identify domain-specific objectives and provide justifiable explanations for their predictions. To address this, we propose a...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 20. Preparing high-quality datasets required by various data-driven AI and machine learning models has become a cornerstone task in data-driven analysis. Conventional data discovery methods typically integrate datasets towards a single pre-defined quality measure that may lead to bias for downstream tas...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 21. The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvemen...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 22. Reward models play an essential role in training vision-language models (VLMs) by assessing output quality to enable aligning with human preferences. Despite their importance, the research community lacks comprehensive open benchmarks for evaluating multimodal reward models in VLMs. To address this ...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 23. LLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging. We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires r...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 24. In the age of misinformation, hallucination -- the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses -- represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quantifyin...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 25. Large language models (LLMs) are capable of generating coherent summaries from very long contexts given a user query. Extracting and properly citing evidence spans could help improve the transparency and reliability of these summaries. At the same time, LLMs suffer from positional biases in terms of...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 26. Geolocation, the task of identifying an image's location, requires complex reasoning and is crucial for navigation, monitoring, and cultural preservation. However, current methods often produce coarse, imprecise, and non-interpretable localization. A major challenge lies in the quality and scale of ...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 27. Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises because existing VLMs are not explicitly trained to genera...
[22.02.2025 12:38] ********************************************************************************
[22.02.2025 12:38] Abstract 28. Generating novel molecules with out-of-distribution properties is a major challenge in molecular discovery. While supervised learning methods generate high-quality molecules similar to those in a dataset, they struggle to generalize to out-of-distribution properties. Reinforcement learning can explo...
[22.02.2025 12:38] Read previous papers.
[22.02.2025 12:38] Generating reviews via LLM API.
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#training", "#dataset", "#agents", "#open_source", "#synthetic", "#benchmark", "#games", "#rl"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞ¾Ğ²Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Meta MLGym Ğ¸ MLGym-Bench - Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½Ğº
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#training", "#transfer_learning", "#architecture", "#ethics", "#multilingual"], "emoji": "ğŸŒ", "ru": {"title": "SigLIP 2: ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸", "desc": "SigLIP 2 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼Ğµ
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#survey", "#reasoning", "#agi", "#benchmark", "#rlhf", "#data"], "emoji": "ğŸ“", "ru": {"title": "SuperGPQA: ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ğ² 285 Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SuperGPQA - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#rlhf", "#training", "#transfer_learning", "#optimization", "#hallucinations"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ LLM Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LoRA", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆ
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#reasoning", "#architecture", "#training", "#optimization"], "emoji": "ğŸš€", "ru": {"title": "S*: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ S* - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#rl", "#training", "#transfer_learning", "#math", "#reasoning"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ˜Ğ˜ Ğ² Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒ
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#math", "#optimization", "#rl"], "emoji": "ğŸ§¬", "ru": {"title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ°Ğ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¾Ğ² ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#benchmark", "#rlhf", "#training", "#long_context"], "emoji": "ğŸ“", "ru": {"title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#inference", "#rl", "#training", "#data", "#reasoning", "#optimization"], "emoji": "ğŸ§ ", "ru": {"title": "S^2R: Ğ¡Ğ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº S^2R Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#interpretability", "#data", "#architecture", "#reasoning"], "emoji": "â³", "ru": {"title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚Ğ°Ğ¹Ğ½ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 'Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‚ 
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#agents", "#open_source", "#architecture", "#agi"], "emoji": "ğŸ–¥ï¸", "ru": {"title": "Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° ĞŸĞš", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² PC-Agent Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ½Ğ° ĞŸĞš Ñ Ğ¸Ñ
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#benchmark", "#video", "#training", "#architecture", "#diffusion"], "emoji": "ğŸ¬", "ru": {"title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Set-and-Sequence", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Set-and-Sequence Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#science", "#math", "#synthetic"], "emoji": "ğŸ¤–", "ru": {"title": "CHASE: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CHASE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#reasoning", "#dataset", "#cv", "#games", "#training"], "emoji": "ğŸ—ºï¸", "ru": {"title": "Ğ“ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑÑ‚Ğ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒ
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#benchmark", "#data", "#agents", "#multimodal", "#synthetic"], "emoji": "ğŸ¤–", "ru": {"title": "CoSyn: ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CoSyn - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#continual_learning", "#rag", "#optimization", "#dataset", "#graphs"], "emoji": "ğŸ§ ", "ru": {"title": "HippoRAG 2: Ğ¨Ğ°Ğ³ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ HippoRAG 2 - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ° ÑĞ¸
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#optimization", "#video", "#architecture", "#inference", "#diffusion", "#cv"], "emoji": "ğŸ¨", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾ĞµĞ²", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ RelaCtrl Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#training", "#reasoning", "#multimodal", "#optimization", "#robotics"], "emoji": "ğŸ§ ", "ru": {"title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ»Ğ°Ğ±Ğ¸Ñ€Ğ¸Ğ½Ñ‚Ğ°Ñ…", "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#inference", "#architecture", "#long_context", "#optimization", "#training"], "emoji": "ğŸš€", "ru": {"title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ", "desc": "LServe - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒÑĞºĞ¾Ñ€ÑÑÑ‰Ğ°Ñ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğµ
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#cv", "#training", "#synthetic"], "emoji": "ğŸ”", "ru": {"title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… 
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#synthetic", "#data"], "emoji": "ğŸ“Š", "ru": {"title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ñƒ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MODis - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#data", "#benchmark", "#long_context", "#dataset", "#multimodal", "#recommendation"], "emoji": "ğŸ“š", "ru": {"title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#benchmark", "#dataset", "#alignment", "#open_source"], "emoji": "ğŸ¯", "ru": {"title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ±Ñ€Ğ¾ÑĞ°ĞµÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Multimodal RewardBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#data", "#long_context", "#small_models", "#training", "#synthetic"], "emoji": "ğŸ“š", "ru": {"title": "CLIPPER: ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºĞ½Ğ¸Ğ³ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "CLIPPER - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#low_resource", "#machine_translation", "#dataset", "#multilingual", "#open_source", "#data", "#hallucinations"], "emoji": "ğŸŒ", "ru": {"title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ĞºÑ€Ñƒ
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#data", "#multimodal", "#long_context", "#synthetic", "#dataset"], "emoji": "ğŸ”", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸Ğ· Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#data", "#games", "#dataset", "#cv", "#benchmark", "#interpretability", "#reasoning"], "emoji": "ğŸŒ", "ru": {"title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº ÑƒĞ¼Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#multimodal", "#dataset", "#optimization", "#hallucinations", "#training"], "emoji": "ğŸ‘ï¸", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (V
[22.02.2025 12:38] Using data from previous issue: {"categories": ["#open_source", "#science", "#dataset", "#data", "#rl", "#training"], "emoji": "ğŸ§ª", "ru": {"title": "ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ñ Ğ½ĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ STGG+
[22.02.2025 12:38] Trying to get texts in Chinese.
[22.02.2025 12:38] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.
[22.02.2025 12:38] Mistral response. {"id": "17c58d914ae44b7794b91a749cf6eab5", "object": "chat.completion", "created": 1740227882, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6211\u4eec\u4ecb\u7ecd\u4e86Meta MLGym\u548cMLGym-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u5f00\u53d1LLM\u4ee3\u7406\u5728AI\u7814\u7a76\u4efb\u52a1\u4e0a\u7684\u65b0\u6846\u67b6\u548c\u57fa\u51c6\u3002\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684Gym\u73af\u5883\uff0c\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u7814\u7a76\u3002MLGym-Bench\u5305\u542b13\u4e2a\u591a\u6837\u4e14\u5f00\u653e\u7684AI\u7814\u7a76\u4efb\u52a1\uff0c\u6db5\u76d6\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u535a\u5f08\u8bba\u7b49\u9886\u57df\u3002\u89e3\u51b3\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u5b9e\u9645\u7684AI\u7814\u7a76\u6280\u80fd\uff0c\u5982\u4ea7\u751f\u65b0\u60f3\u6cd5\u548c\u5047\u8bbe\u3001\u521b\u5efa\u548c\u5904\u7406\u6570\u636e\u3001\u5b9e\u73b0ML\u65b9\u6cd5\u3001\u8bad\u7ec3\u6a21\u578b\u3001\u8fd0\u884c\u5b9e\u9a8c\u3001\u5206\u6790\u7ed3\u679c\u5e76\u8fed\u4ee3\u6539\u8fdb\u4efb\u52a1\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u4e2a\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5982Claude-3.5-Sonnet\u548cGPT-4o\u3002\u6211\u4eec\u7684MLGym\u6846\u67b6\u4f7f\u5f97\u6dfb\u52a0\u65b0\u4efb\u52a1\u3001\u96c6\u6210\u548c\u8bc4\u4f30\u6a21\u578b\u3001\u5927\u89c4\u6a21\u751f\u6210\u5408\u6210\u6570\u636e\u4ee5\u53ca\u5f00\u53d1\u65b0\u7684\u5b66\u4e60\u7b97\u6cd5\u53d8\u5f97\u5bb9\u6613\u3002\u6211\u4eec\u53d1\u73b0\u5f53\u524d\u7684\u524d\u6cbf\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u627e\u5230\u66f4\u597d\u7684\u8d85\u53c2\u6570\u6765\u6539\u8fdb\u57fa\u7ebf\uff0c\u4f46\u4e0d\u4f1a\u4ea7\u751f\u65b0\u7684\u5047\u8bbe\u3001\u7b97\u6cd5\u3001\u67b6\u6784\u6216\u663e\u8457\u6539\u8fdb\u3002\u6211\u4eec\u5f00\u6e90\u4e86\u6211\u4eec\u7684\u6846\u67b6\u548c\u57fa\u51c6\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u5728\u63d0\u9ad8LLM\u4ee3\u7406\u7684AI\u7814\u7a76\u80fd\u529b\u65b9\u9762\u7684\u7814\u7a76\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 349, "total_tokens": 722, "completion_tokens": 373}}
[22.02.2025 12:38] Response: æˆ‘ä»¬ä»‹ç»äº†Meta MLGymå’ŒMLGym-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å’Œå¼€å‘LLMä»£ç†åœ¨AIç ”ç©¶ä»»åŠ¡ä¸Šçš„æ–°æ¡†æ¶å’ŒåŸºå‡†ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºæœºå™¨å­¦ä¹ ä»»åŠ¡çš„Gymç¯å¢ƒï¼Œæ”¯æŒå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç ”ç©¶ã€‚MLGym-BenchåŒ…å«13ä¸ªå¤šæ ·ä¸”å¼€æ”¾çš„AIç ”ç©¶ä»»åŠ¡ï¼Œæ¶µç›–è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€å¼ºåŒ–å­¦ä¹ å’Œåšå¼ˆè®ºç­‰é¢†åŸŸã€‚è§£å†³è¿™äº›ä»»åŠ¡éœ€è¦å®é™…çš„AIç ”ç©¶æŠ€èƒ½ï¼Œå¦‚äº§ç”Ÿæ–°æƒ³æ³•å’Œå‡è®¾ã€åˆ›å»ºå’Œå¤„ç†æ•°æ®ã€å®ç°MLæ–¹æ³•ã€è®­ç»ƒæ¨¡å‹ã€è¿è¡Œå®éªŒã€åˆ†æç»“æœå¹¶è¿­ä»£æ”¹è¿›ä»»åŠ¡ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªå‰æ²¿å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚Claude-3.5-Sonnetå’ŒGPT-4oã€‚æˆ‘ä»¬çš„MLGymæ¡†æ¶ä½¿å¾—æ·»åŠ æ–°ä»»åŠ¡ã€é›†æˆå’Œè¯„ä¼°æ¨¡å‹ã€å¤§è§„æ¨¡ç”Ÿæˆåˆæˆæ•°æ®ä»¥åŠå¼€å‘æ–°çš„å­¦ä¹ ç®—æ³•å˜å¾—å®¹æ˜“ã€‚æˆ‘ä»¬å‘ç°å½“å‰çš„å‰æ²¿æ¨¡å‹å¯ä»¥é€šè¿‡æ‰¾åˆ°æ›´å¥½çš„è¶…å‚æ•°æ¥æ”¹è¿›åŸºçº¿ï¼Œä½†ä¸ä¼šäº§ç”Ÿæ–°çš„å‡è®¾ã€ç®—æ³•ã€æ¶æ„æˆ–æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬å¼€æºäº†æˆ‘ä»¬çš„æ¡†æ¶å’ŒåŸºå‡†ï¼Œä»¥ä¿ƒè¿›æœªæ¥åœ¨æé«˜LLMä»£ç†çš„AIç ”ç©¶èƒ½åŠ›æ–¹é¢çš„ç ”ç©¶ã€‚
[22.02.2025 12:38] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

æˆ‘ä»¬ä»‹ç»äº†Meta MLGymå’ŒMLGym-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å’Œå¼€å‘LLMä»£ç†åœ¨AIç ”ç©¶ä»»åŠ¡ä¸Šçš„æ–°æ¡†æ¶å’ŒåŸºå‡†ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºæœºå™¨å­¦ä¹ ä»»åŠ¡çš„Gymç¯å¢ƒï¼Œæ”¯æŒå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç ”ç©¶ã€‚MLGym-BenchåŒ…å«13ä¸ªå¤šæ ·ä¸”å¼€æ”¾çš„AIç ”ç©¶ä»»åŠ¡ï¼Œæ¶µç›–è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€å¼ºåŒ–å­¦ä¹ å’Œåšå¼ˆè®ºç­‰é¢†åŸŸã€‚è§£å†³è¿™äº›ä»»åŠ¡éœ€è¦å®é™…çš„AIç ”ç©¶æŠ€èƒ½ï¼Œå¦‚äº§ç”Ÿæ–°æƒ³æ³•å’Œå‡è®¾ã€åˆ›å»ºå’Œå¤„ç†æ•°æ®ã€å®ç°MLæ–¹æ³•ã€è®­ç»ƒæ¨¡å‹ã€è¿è¡Œå®éªŒã€åˆ†æç»“æœå¹¶è¿­ä»£æ”¹è¿›ä»»åŠ¡ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªå‰æ²¿å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚Claude-3.5-Sonnetå’ŒGPT-4oã€‚æˆ‘ä»¬çš„MLGymæ¡†æ¶ä½¿å¾—æ·»åŠ æ–°ä»»åŠ¡ã€é›†æˆå’Œè¯„ä¼°æ¨¡å‹ã€å¤§è§„æ¨¡ç”Ÿæˆåˆæˆæ•°æ®ä»¥åŠå¼€å‘æ–°çš„å­¦ä¹ ç®—æ³•å˜å¾—å®¹æ˜“ã€‚æˆ‘ä»¬å‘ç°å½“å‰çš„å‰æ²¿æ¨¡å‹å¯ä»¥é€šè¿‡æ‰¾åˆ°æ›´å¥½çš„è¶…å‚æ•°æ¥æ”¹è¿›åŸºçº¿ï¼Œä½†ä¸ä¼šäº§ç”Ÿæ–°çš„å‡è®¾ã€ç®—æ³•ã€æ¶æ„æˆ–æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬å¼€æºäº†æˆ‘ä»¬çš„æ¡†æ¶å’ŒåŸºå‡†ï¼Œä»¥ä¿ƒè¿›æœªæ¥åœ¨æé«˜LLMä»£ç†çš„AIç ”ç©¶èƒ½åŠ›æ–¹é¢çš„ç ”ç©¶ã€‚
[22.02.2025 12:38] Mistral response. {"id": "81716961bb5b49f788bcfaaa739c8c57", "object": "chat.completion", "created": 1740227890, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "W\u01d2men ji\u00e8sh\u00e0o le Meta MLGym h\u00e9 MLGym-Bench, zh\u00e8 sh\u00ec y\u012bg\u00e8 y\u00f2ngy\u00fa p\u00edngg\u01d4 h\u00e9 k\u0101if\u0101 LLM d\u00e0il\u01d0 z\u00e0i AI y\u00e1nji\u016b r\u00e8nw\u00f9 sh\u00e0ng de x\u012bn ku\u00e0ngji\u00e0 h\u00e9 j\u012bzh\u01d4n. Zh\u00e8 sh\u00ec d\u00ec-y\u012bg\u00e8 y\u00f2ngy\u00fa j\u012bq\u01d0 xu\u00e9x\u00ed r\u00e8nw\u00f9 de Gym hu\u00e1nj\u00ecng, zh\u012bch\u00ed qi\u00e1ngzh\u00f9 xu\u00e9x\u00ed su\u00e0nf\u01ce de y\u00e1nji\u016b. MLGym-Bench b\u0101oh\u00e1n 13 g\u00e8 du\u014dy\u00e0ng qi\u011b k\u0101if\u00e0ng de AI y\u00e1nji\u016b r\u00e8nw\u00f9, h\u00e1nf\u00f9 j\u00ecsu\u00e0nj\u012b sh\u00ecji\u00e8, z\u00ecr\u00e1n y\u01d4y\u00e1n ch\u01d4l\u01d0, qi\u00e1ngzh\u00f9 xu\u00e9x\u00ed h\u00e9 b\u00f3y\u00ecl\u00f9n d\u011bng l\u01d0ngy\u00f9. Ji\u011bju\u00e9 zh\u00e8xi\u0113 r\u00e8nw\u00f9 x\u016by\u00e0o sh\u00edj\u00ec de AI y\u00e1nji\u016b j\u00ecn\u00e9ng, r\u00fa ch\u01censh\u0113ng x\u012bn y\u00ecxi\u01ceng h\u00e9 ji\u01cesh\u00e8, chu\u00e0ngji\u00e0n h\u00e9 ch\u01d4l\u01d0 sh\u00f9j\u00f9, sh\u00edxi\u00e0n ML f\u0101ngf\u01ce, x\u00f9nli\u00e0n m\u00f3x\u00edng, y\u00f9nx\u00edng sh\u00ecy\u00e0n, f\u0113nxi ji\u00e9gu\u01d2 b\u00ecng di\u00e9d\u01cei g\u01ceij\u00ecn r\u00e8nw\u00f9. W\u01d2men p\u00edngg\u01d4 le du\u014d g\u00e8 qi\u00e1nx\u012bn d\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng, r\u00fa Claude-3.5-Sonnet h\u00e9 GPT-4o. W\u01d2men de MLGym ku\u00e0ngji\u00e0 sh\u01d0d\u00e9 ti\u0101nji\u0101 x\u012bn r\u00e8nw\u00f9, j\u00edch\u00e9ng h\u00e9 p\u00edngg\u01d4 m\u00f3x\u00edng, d\u00e0gu\u012bm\u00f3 sh\u0113ngch\u00e9ng h\u00e9ch\u00e9ng sh\u00f9j\u00f9 y\u01d0j\u00ed k\u0101if\u0101 x\u012bn de xu\u00e9x\u00ed su\u00e0nf\u01ce bi\u00e0nd\u00e9 r\u00f3ngy\u00ec. W\u01d2men f\u0101xi\u00e0n d\u0101ngqi\u00e1n de qi\u00e1nx\u012bn m\u00f3x\u00edng k\u011by\u01d0 t\u014dnggu\u00f2 zh\u01ceod\u00e0o g\u00e8ng h\u01ceo de ch\u0101oc\u0101nsh\u00f9 l\u00e1i g\u01ceij\u00ecn j\u012bch\u01d4, d\u00e0n b\u00f9hu\u00ec ch\u01censh\u0113ng x\u012bn de ji\u01cesh\u00e8, su\u00e0nf\u01ce, ji\u00e0g\u00f2u hu\u00f2 xi\u01cenzh\u00f9 g\u01ceij\u00ecn. W\u01d2men k\u0101iyu\u00e1n le w\u01d2men de ku\u00e0ngji\u00e0 h\u00e9 j\u012bzh\u01d4n, y\u01d0 c\u00f9j\u00ecn w\u00e8il\u01cei z\u00e0i t\u00edg\u0101o LLM d\u00e0il\u01d0 de AI y\u00e1nji\u016b n\u00e9ngl\u00ec f\u0101ngmi\u00e0n de y\u00e1nji\u016b."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 388, "total_tokens": 1113, "completion_tokens": 725}}
[22.02.2025 12:38] Response: WÇ’men jiÃ¨shÃ o le Meta MLGym hÃ© MLGym-Bench, zhÃ¨ shÃ¬ yÄ«gÃ¨ yÃ²ngyÃº pÃ­nggÇ” hÃ© kÄifÄ LLM dÃ ilÇ zÃ i AI yÃ¡njiÅ« rÃ¨nwÃ¹ shÃ ng de xÄ«n kuÃ ngjiÃ  hÃ© jÄ«zhÇ”n. ZhÃ¨ shÃ¬ dÃ¬-yÄ«gÃ¨ yÃ²ngyÃº jÄ«qÇ xuÃ©xÃ­ rÃ¨nwÃ¹ de Gym huÃ¡njÃ¬ng, zhÄ«chÃ­ qiÃ¡ngzhÃ¹ xuÃ©xÃ­ suÃ nfÇ de yÃ¡njiÅ«. MLGym-Bench bÄohÃ¡n 13 gÃ¨ duÅyÃ ng qiÄ› kÄifÃ ng de AI yÃ¡njiÅ« rÃ¨nwÃ¹, hÃ¡nfÃ¹ jÃ¬suÃ njÄ« shÃ¬jiÃ¨, zÃ¬rÃ¡n yÇ”yÃ¡n chÇ”lÇ, qiÃ¡ngzhÃ¹ xuÃ©xÃ­ hÃ© bÃ³yÃ¬lÃ¹n dÄ›ng lÇngyÃ¹. JiÄ›juÃ© zhÃ¨xiÄ“ rÃ¨nwÃ¹ xÅ«yÃ o shÃ­jÃ¬ de AI yÃ¡njiÅ« jÃ¬nÃ©ng, rÃº chÇnshÄ“ng xÄ«n yÃ¬xiÇng hÃ© jiÇshÃ¨, chuÃ ngjiÃ n hÃ© chÇ”lÇ shÃ¹jÃ¹, shÃ­xiÃ n ML fÄngfÇ, xÃ¹nliÃ n mÃ³xÃ­ng, yÃ¹nxÃ­ng shÃ¬yÃ n, fÄ“nxi jiÃ©guÇ’ bÃ¬ng diÃ©dÇi gÇijÃ¬n rÃ¨nwÃ¹. WÇ’men pÃ­nggÇ” le duÅ gÃ¨ qiÃ¡nxÄ«n dÃ  yÇ”yÃ¡n mÃ³xÃ­ng, rÃº Claude-3.5-Sonnet hÃ© GPT-4o. WÇ’men de MLGym kuÃ ngjiÃ  shÇdÃ© tiÄnjiÄ xÄ«n rÃ¨nwÃ¹, jÃ­chÃ©ng hÃ© pÃ­nggÇ” mÃ³xÃ­ng, dÃ guÄ«mÃ³ shÄ“ngchÃ©ng hÃ©chÃ©ng shÃ¹jÃ¹ yÇjÃ­ kÄifÄ xÄ«n de xuÃ©xÃ­ suÃ nfÇ biÃ ndÃ© rÃ³ngyÃ¬. WÇ’men fÄxiÃ n dÄngqiÃ¡n de qiÃ¡nxÄ«n mÃ³xÃ­ng kÄ›yÇ tÅngguÃ² zhÇodÃ o gÃ¨ng hÇo de chÄocÄnshÃ¹ lÃ¡i gÇijÃ¬n jÄ«chÇ”, dÃ n bÃ¹huÃ¬ chÇnshÄ“ng xÄ«n de jiÇshÃ¨, suÃ nfÇ, jiÃ gÃ²u huÃ² xiÇnzhÃ¹ gÇijÃ¬n. WÇ’men kÄiyuÃ¡n le wÇ’men de kuÃ ngjiÃ  hÃ© jÄ«zhÇ”n, yÇ cÃ¹jÃ¬n wÃ¨ilÇi zÃ i tÃ­gÄo LLM dÃ ilÇ de AI yÃ¡njiÅ« nÃ©nglÃ¬ fÄngmiÃ n de yÃ¡njiÅ«.
[22.02.2025 12:38] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

æˆ‘ä»¬ä»‹ç»äº†Meta MLGymå’ŒMLGym-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å’Œå¼€å‘LLMä»£ç†åœ¨AIç ”ç©¶ä»»åŠ¡ä¸Šçš„æ–°æ¡†æ¶å’ŒåŸºå‡†ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºæœºå™¨å­¦ä¹ ä»»åŠ¡çš„Gymç¯å¢ƒï¼Œæ”¯æŒå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç ”ç©¶ã€‚MLGym-BenchåŒ…å«13ä¸ªå¤šæ ·ä¸”å¼€æ”¾çš„AIç ”ç©¶ä»»åŠ¡ï¼Œæ¶µç›–è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€å¼ºåŒ–å­¦ä¹ å’Œåšå¼ˆè®ºç­‰é¢†åŸŸã€‚è§£å†³è¿™äº›ä»»åŠ¡éœ€è¦å®é™…çš„AIç ”ç©¶æŠ€èƒ½ï¼Œå¦‚äº§ç”Ÿæ–°æƒ³æ³•å’Œå‡è®¾ã€åˆ›å»ºå’Œå¤„ç†æ•°æ®ã€å®ç°MLæ–¹æ³•ã€è®­ç»ƒæ¨¡å‹ã€è¿è¡Œå®éªŒã€åˆ†æç»“æœå¹¶è¿­ä»£æ”¹è¿›ä»»åŠ¡ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªå‰æ²¿å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚Claude-3.5-Sonnetå’ŒGPT-4oã€‚æˆ‘ä»¬çš„MLGymæ¡†æ¶ä½¿å¾—æ·»åŠ æ–°ä»»åŠ¡ã€é›†æˆå’Œè¯„ä¼°æ¨¡å‹ã€å¤§è§„æ¨¡ç”Ÿæˆåˆæˆæ•°æ®ä»¥åŠå¼€å‘æ–°çš„å­¦ä¹ ç®—æ³•å˜å¾—å®¹æ˜“ã€‚æˆ‘ä»¬å‘ç°å½“å‰çš„å‰æ²¿æ¨¡å‹å¯ä»¥é€šè¿‡æ‰¾åˆ°æ›´å¥½çš„è¶…å‚æ•°æ¥æ”¹è¿›åŸºçº¿ï¼Œä½†ä¸ä¼šäº§ç”Ÿæ–°çš„å‡è®¾ã€ç®—æ³•ã€æ¶æ„æˆ–æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬å¼€æºäº†æˆ‘ä»¬çš„æ¡†æ¶å’ŒåŸºå‡†ï¼Œä»¥ä¿ƒè¿›æœªæ¥åœ¨æé«˜LLMä»£ç†çš„AIç ”ç©¶èƒ½åŠ›æ–¹é¢çš„ç ”ç©¶ã€‚
[22.02.2025 12:38] Mistral response. {"id": "e7e9deccc988477ba1ffdf135b05b4e6", "object": "chat.completion", "created": 1740227905, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u4ecb\u7ecd\", \"pinyin\": \"ji\u00e8 sh\u00e0o\", \"trans\": \"introduce\"},\n    {\"word\": \"\u6846\u67b6\", \"pinyin\": \"ku\u00e0ng ji\u00e0\", \"trans\": \"framework\"},\n    {\"word\": \"\u57fa\u51c6\", \"pinyin\": \"j\u012b zh\u01d4n\", \"trans\": \"benchmark\"},\n    {\"word\": \"\u8bc4\u4f30\", \"pinyin\": \"p\u00edng g\u016b\", \"trans\": \"evaluate\"},\n    {\"word\": \"\u5f00\u53d1\", \"pinyin\": \"k\u0101i f\u0101\", \"trans\": \"develop\"},\n    {\"word\": \"\u4ee3\u7406\", \"pinyin\": \"d\u00e0i l\u01d0\", \"trans\": \"agent\"},\n    {\"word\": \"\u4efb\u52a1\", \"pinyin\": \"r\u00e8n wu\", \"trans\": \"task\"},\n    {\"word\": \"\u652f\u6301\", \"pinyin\": \"zh\u012b ch\u00ed\", \"trans\": \"support\"},\n    {\"word\": \"\u5f3a\u5316\", \"pinyin\": \"qi\u00e1ng hu\u00e0\", \"trans\": \"reinforce\"},\n    {\"word\": \"\u7b97\u6cd5\", \"pinyin\": \"su\u00e0n f\u01ce\", \"trans\": \"algorithm\"},\n    {\"word\": \"\u591a\u6837\", \"pinyin\": \"du\u014d y\u00e0ng\", \"trans\": \"diverse\"},\n    {\"word\": \"\u5f00\u653e\", \"pinyin\": \"k\u0101i f\u00e0ng\", \"trans\": \"open\"},\n    {\"word\": \"\u6db5\u76d6\", \"pinyin\": \"h\u00e1n g\u00e0i\", \"trans\": \"cover\"},\n    {\"word\": \"\u8ba1\u7b97\u673a\u89c6\u89c9\", \"pinyin\": \"j\u00ec su\u00e0n j\u012b sh\u00ec ju\u00e9\", \"trans\": \"computer vision\"},\n    {\"word\": \"\u81ea\u7136\u8bed\u8a00\u5904\u7406\", \"pinyin\": \"z\u00ec r\u00e1n y\u01d4 y\u00e1n ch\u01d4 l\u01d0\", \"trans\": \"natural language processing\"},\n    {\"word\": \"\u535a\u5f08\u8bba\", \"pinyin\": \"b\u00f3 y\u00ec l\u00f9n\", \"trans\": \"game theory\"},\n    {\"word\": \"\u6280\u80fd\", \"pinyin\": \"j\u00ec n\u00e9ng\", \"trans\": \"skill\"},\n    {\"word\": \"\u5047\u8bbe\", \"pinyin\": \"ji\u01ce sh\u00e8\", \"trans\": \"hypothesis\"},\n    {\"word\": \"\u521b\u5efa\", \"pinyin\": \"chu\u00e0ng ji\u00e0n\", \"trans\": \"create\"},\n    {\"word\": \"\u5904\u7406\", \"pinyin\": \"ch\u01d4 l\u01d0\", \"trans\": \"process\"},\n    {\"word\": \"\u5b9e\u73b0\", \"pinyin\": \"sh\u00ed xi\u00e0n\", \"trans\": \"implement\"},\n    {\"word\": \"\u8bad\u7ec3\", \"pinyin\": \"x\u00f9n li\u00e0n\", \"trans\": \"train\"},\n    {\"word\": \"\u5b9e\u9a8c\", \"pinyin\": \"sh\u00ed y\u00e0n\", \"trans\": \"experiment\"},\n    {\"word\": \"\u5206\u6790\", \"pinyin\": \"f\u0113n x\u012b\", \"trans\": \"analyze\"},\n    {\"word\": \"\u8fed\u4ee3\", \"pinyin\": \"di\u00e9 d\u00e0i\", \"trans\": \"iterate\"},\n    {\"word\": \"\u6539\u8fdb\", \"pinyin\": \"g\u01cei j\u00ecn\", \"trans\": \"improve\"},\n    {\"word\": \"\u524d\u6cbf\", \"pinyin\": \"qi\u00e1n y\u00e1n\", \"trans\": \"frontier\"},\n    {\"word\": \"\u8d85\u53c2\u6570\", \"pinyin\": \"ch\u0101o c\u0101n sh\u00f9\", \"trans\": \"hyperparameter\"},\n    {\"word\": \"\u57fa\u7ebf\", \"pinyin\": \"j\u012b xi\u00e0n\", \"trans\": \"baseline\"},\n    {\"word\": \"\u751f\u6210\", \"pinyin\": \"sh\u0113ng ch\u00e9ng\", \"trans\": \"generate\"},\n    {\"word\": \"\u5408\u6210\", \"pinyin\": \"h\u00e9 ch\u00e9ng\", \"trans\": \"synthetic\"},\n    {\"word\": \"\u5f00\u6e90\", \"pinyin\": \"k\u0101i yu\u00e1n\", \"trans\": \"open source\"},\n    {\"word\": \"\u4fc3\u8fdb\", \"pinyin\": \"c\u00f9 j\u00ecn\", \"trans\": \"promote\"},\n    {\"word\": \"\u80fd\u529b\", \"pinyin\": \"n\u00e9ng l\u00ec\", \"trans\": \"ability\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 420, "total_tokens": 1412, "completion_tokens": 992}}
[22.02.2025 12:38] Response: [
    {"word": "ä»‹ç»", "pinyin": "jiÃ¨ shÃ o", "trans": "introduce"},
    {"word": "æ¡†æ¶", "pinyin": "kuÃ ng jiÃ ", "trans": "framework"},
    {"word": "åŸºå‡†", "pinyin": "jÄ« zhÇ”n", "trans": "benchmark"},
    {"word": "è¯„ä¼°", "pinyin": "pÃ­ng gÅ«", "trans": "evaluate"},
    {"word": "å¼€å‘", "pinyin": "kÄi fÄ", "trans": "develop"},
    {"word": "ä»£ç†", "pinyin": "dÃ i lÇ", "trans": "agent"},
    {"word": "ä»»åŠ¡", "pinyin": "rÃ¨n wu", "trans": "task"},
    {"word": "æ”¯æŒ", "pinyin": "zhÄ« chÃ­", "trans": "support"},
    {"word": "å¼ºåŒ–", "pinyin": "qiÃ¡ng huÃ ", "trans": "reinforce"},
    {"word": "ç®—æ³•", "pinyin": "suÃ n fÇ", "trans": "algorithm"},
    {"word": "å¤šæ ·", "pinyin": "duÅ yÃ ng", "trans": "diverse"},
    {"word": "å¼€æ”¾", "pinyin": "kÄi fÃ ng", "trans": "open"},
    {"word": "æ¶µç›–", "pinyin": "hÃ¡n gÃ i", "trans": "cover"},
    {"word": "è®¡ç®—æœºè§†è§‰", "pinyin": "jÃ¬ suÃ n jÄ« shÃ¬ juÃ©", "trans": "computer vision"},
    {"word": "è‡ªç„¶è¯­è¨€å¤„ç†", "pinyin": "zÃ¬ rÃ¡n yÇ” yÃ¡n chÇ” lÇ", "trans": "natural language processing"},
    {"word": "åšå¼ˆè®º", "pinyin": "bÃ³ yÃ¬ lÃ¹n", "trans": "game theory"},
    {"word": "æŠ€èƒ½", "pinyin": "jÃ¬ nÃ©ng", "trans": "skill"},
    {"word": "å‡è®¾", "pinyin": "jiÇ shÃ¨", "trans": "hypothesis"},
    {"word": "åˆ›å»º", "pinyin": "chuÃ ng jiÃ n", "trans": "create"},
    {"word": "å¤„ç†", "pinyin": "chÇ” lÇ", "trans": "process"},
    {"word": "å®ç°", "pinyin": "shÃ­ xiÃ n", "trans": "implement"},
    {"word": "è®­ç»ƒ", "pinyin": "xÃ¹n liÃ n", "trans": "train"},
    {"word": "å®éªŒ", "pinyin": "shÃ­ yÃ n", "trans": "experiment"},
    {"word": "åˆ†æ", "pinyin": "fÄ“n xÄ«", "trans": "analyze"},
    {"word": "è¿­ä»£", "pinyin": "diÃ© dÃ i", "trans": "iterate"},
    {"word": "æ”¹è¿›", "pinyin": "gÇi jÃ¬n", "trans": "improve"},
    {"word": "å‰æ²¿", "pinyin": "qiÃ¡n yÃ¡n", "trans": "frontier"},
    {"word": "è¶…å‚æ•°", "pinyin": "chÄo cÄn shÃ¹", "trans": "hyperparameter"},
    {"word": "åŸºçº¿", "pinyin": "jÄ« xiÃ n", "trans": "baseline"},
    {"word": "ç”Ÿæˆ", "pinyin": "shÄ“ng chÃ©ng", "trans": "generate"},
    {"word": "åˆæˆ", "pinyin": "hÃ© chÃ©ng", "trans": "synthetic"},
    {"word": "å¼€æº", "pinyin": "kÄi yuÃ¡n", "trans": "open source"},
    {"word": "ä¿ƒè¿›", "pinyin": "cÃ¹ jÃ¬n", "trans": "promote"},
    {"word": "èƒ½åŠ›", "pinyin": "nÃ©ng lÃ¬", "trans": "ability"}
]
[22.02.2025 12:38] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

æˆ‘ä»¬ä»‹ç»äº†Meta MLGymå’ŒMLGym-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å’Œå¼€å‘LLMä»£ç†åœ¨AIç ”ç©¶ä»»åŠ¡ä¸Šçš„æ–°æ¡†æ¶å’ŒåŸºå‡†ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºæœºå™¨å­¦ä¹ ä»»åŠ¡çš„Gymç¯å¢ƒï¼Œæ”¯æŒå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç ”ç©¶ã€‚MLGym-BenchåŒ…å«13ä¸ªå¤šæ ·ä¸”å¼€æ”¾çš„AIç ”ç©¶ä»»åŠ¡ï¼Œæ¶µç›–è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€å¼ºåŒ–å­¦ä¹ å’Œåšå¼ˆè®ºç­‰é¢†åŸŸã€‚è§£å†³è¿™äº›ä»»åŠ¡éœ€è¦å®é™…çš„AIç ”ç©¶æŠ€èƒ½ï¼Œå¦‚äº§ç”Ÿæ–°æƒ³æ³•å’Œå‡è®¾ã€åˆ›å»ºå’Œå¤„ç†æ•°æ®ã€å®ç°MLæ–¹æ³•ã€è®­ç»ƒæ¨¡å‹ã€è¿è¡Œå®éªŒã€åˆ†æç»“æœå¹¶è¿­ä»£æ”¹è¿›ä»»åŠ¡ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªå‰æ²¿å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚Claude-3.5-Sonnetå’ŒGPT-4oã€‚æˆ‘ä»¬çš„MLGymæ¡†æ¶ä½¿å¾—æ·»åŠ æ–°ä»»åŠ¡ã€é›†æˆå’Œè¯„ä¼°æ¨¡å‹ã€å¤§è§„æ¨¡ç”Ÿæˆåˆæˆæ•°æ®ä»¥åŠå¼€å‘æ–°çš„å­¦ä¹ ç®—æ³•å˜å¾—å®¹æ˜“ã€‚æˆ‘ä»¬å‘ç°å½“å‰çš„å‰æ²¿æ¨¡å‹å¯ä»¥é€šè¿‡æ‰¾åˆ°æ›´å¥½çš„è¶…å‚æ•°æ¥æ”¹è¿›åŸºçº¿ï¼Œä½†ä¸ä¼šäº§ç”Ÿæ–°çš„å‡è®¾ã€ç®—æ³•ã€æ¶æ„æˆ–æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬å¼€æºäº†æˆ‘ä»¬çš„æ¡†æ¶å’ŒåŸºå‡†ï¼Œä»¥ä¿ƒè¿›æœªæ¥åœ¨æé«˜LLMä»£ç†çš„AIç ”ç©¶èƒ½åŠ›æ–¹é¢çš„ç ”ç©¶ã€‚
[22.02.2025 12:38] Mistral response. {"id": "160ad744177843ddb9326422f797e1c5", "object": "chat.completion", "created": 1740227924, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "We introduced Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning tasks that supports research on reinforcement learning algorithms. MLGym-Bench includes 13 diverse and open AI research tasks, covering areas such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires practical AI research skills, such as generating new ideas and hypotheses, creating and handling data, implementing ML methods, training models, running experiments, analyzing results, and iteratively improving tasks. We evaluated several state-of-the-art large language models, such as Claude-3.5-Sonnet and GPT-4o. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models, generate synthetic data at scale, and develop new learning algorithms. We found that current state-of-the-art models can improve baselines by finding better hyperparameters but do not generate new hypotheses, algorithms, architectures, or significant improvements. We have open-sourced our framework and benchmark to promote future research in enhancing the AI research capabilities of LLM agents."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 386, "total_tokens": 646, "completion_tokens": 260}}
[22.02.2025 12:38] Response: We introduced Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning tasks that supports research on reinforcement learning algorithms. MLGym-Bench includes 13 diverse and open AI research tasks, covering areas such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires practical AI research skills, such as generating new ideas and hypotheses, creating and handling data, implementing ML methods, training models, running experiments, analyzing results, and iteratively improving tasks. We evaluated several state-of-the-art large language models, such as Claude-3.5-Sonnet and GPT-4o. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models, generate synthetic data at scale, and develop new learning algorithms. We found that current state-of-the-art models can improve baselines by finding better hyperparameters but do not generate new hypotheses, algorithms, architectures, or significant improvements. We have open-sourced our framework and benchmark to promote future research in enhancing the AI research capabilities of LLM agents.
[22.02.2025 12:38] Renaming data file.
[22.02.2025 12:38] Renaming previous data. hf_papers.json to ./d/2025-02-21.json
[22.02.2025 12:38] Saving new data file.
[22.02.2025 12:38] Generating page.
[22.02.2025 12:38] Renaming previous page.
[22.02.2025 12:38] Renaming previous data. index.html to ./d/2025-02-21.html
[22.02.2025 12:38] [Experimental] Generating Chinese page for reading.
[22.02.2025 12:38] Chinese vocab [{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨ shÃ o', 'trans': 'introduce'}, {'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'}, {'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluate'}, {'word': 'å¼€å‘', 'pinyin': 'kÄi fÄ', 'trans': 'develop'}, {'word': 'ä»£ç†', 'pinyin': 'dÃ i lÇ', 'trans': 'agent'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wu', 'trans': 'task'}, {'word': 'æ”¯æŒ', 'pinyin': 'zhÄ« chÃ­', 'trans': 'support'}, {'word': 'å¼ºåŒ–', 'pinyin': 'qiÃ¡ng huÃ ', 'trans': 'reinforce'}, {'word': 'ç®—æ³•', 'pinyin': 'suÃ n fÇ', 'trans': 'algorithm'}, {'word': 'å¤šæ ·', 'pinyin': 'duÅ yÃ ng', 'trans': 'diverse'}, {'word': 'å¼€æ”¾', 'pinyin': 'kÄi fÃ ng', 'trans': 'open'}, {'word': 'æ¶µç›–', 'pinyin': 'hÃ¡n gÃ i', 'trans': 'cover'}, {'word': 'è®¡ç®—æœºè§†è§‰', 'pinyin': 'jÃ¬ suÃ n jÄ« shÃ¬ juÃ©', 'trans': 'computer vision'}, {'word': 'è‡ªç„¶è¯­è¨€å¤„ç†', 'pinyin': 'zÃ¬ rÃ¡n yÇ” yÃ¡n chÇ” lÇ', 'trans': 'natural language processing'}, {'word': 'åšå¼ˆè®º', 'pinyin': 'bÃ³ yÃ¬ lÃ¹n', 'trans': 'game theory'}, {'word': 'æŠ€èƒ½', 'pinyin': 'jÃ¬ nÃ©ng', 'trans': 'skill'}, {'word': 'å‡è®¾', 'pinyin': 'jiÇ shÃ¨', 'trans': 'hypothesis'}, {'word': 'åˆ›å»º', 'pinyin': 'chuÃ ng jiÃ n', 'trans': 'create'}, {'word': 'å¤„ç†', 'pinyin': 'chÇ” lÇ', 'trans': 'process'}, {'word': 'å®ç°', 'pinyin': 'shÃ­ xiÃ n', 'trans': 'implement'}, {'word': 'è®­ç»ƒ', 'pinyin': 'xÃ¹n liÃ n', 'trans': 'train'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'åˆ†æ', 'pinyin': 'fÄ“n xÄ«', 'trans': 'analyze'}, {'word': 'è¿­ä»£', 'pinyin': 'diÃ© dÃ i', 'trans': 'iterate'}, {'word': 'æ”¹è¿›', 'pinyin': 'gÇi jÃ¬n', 'trans': 'improve'}, {'word': 'å‰æ²¿', 'pinyin': 'qiÃ¡n yÃ¡n', 'trans': 'frontier'}, {'word': 'è¶…å‚æ•°', 'pinyin': 'chÄo cÄn shÃ¹', 'trans': 'hyperparameter'}, {'word': 'åŸºçº¿', 'pinyin': 'jÄ« xiÃ n', 'trans': 'baseline'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generate'}, {'word': 'åˆæˆ', 'pinyin': 'hÃ© chÃ©ng', 'trans': 'synthetic'}, {'word': 'å¼€æº', 'pinyin': 'kÄi yuÃ¡n', 'trans': 'open source'}, {'word': 'ä¿ƒè¿›', 'pinyin': 'cÃ¹ jÃ¬n', 'trans': 'promote'}, {'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'}]
[22.02.2025 12:38] Renaming previous Chinese page.
[22.02.2025 12:38] Renaming previous data. zh.html to ./d/2025-02-21_zh_reading_task.html
[22.02.2025 12:38] Writing Chinese reading task.
[22.02.2025 12:38] Writing result.
[22.02.2025 12:38] Renaming log file.
[22.02.2025 12:38] Renaming previous data. log.txt to ./logs/2025-02-22_last_log.txt
