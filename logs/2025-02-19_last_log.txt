[18.02.2025 23:10] Read previous papers.
[18.02.2025 23:10] Generating top page (month).
[18.02.2025 23:10] Writing top page (month).
[19.02.2025 00:46] Read previous papers.
[19.02.2025 00:46] Get feed.
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11089
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12152
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12115
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11190
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09061
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08745
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12148
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11196
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10458
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11167
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12146
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11357
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11831
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11438
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12135
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11275
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11157
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11330
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11775
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11098
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11901
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09509
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11748
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10550
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12054
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10454
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08820
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11085
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11574
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08826
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09969
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.08441
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09083
[19.02.2025 00:46] Extract page data from URL. URL: https://huggingface.co/papers/2502.12154
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11177
[19.02.2025 00:46] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11578
[19.02.2025 00:46] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.02.2025 00:46] No deleted papers detected.
[19.02.2025 00:46] Downloading and parsing papers (pdf, html). Total: 36.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11089.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11089.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11089.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.12152.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.12152.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.12152.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.12115.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.12115.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.12115.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11190.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11190.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11190.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.09061.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.09061.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.09061.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.08745.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.08745.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.08745.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.12148.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.12148.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.12148.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11196.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11196.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11196.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.10458.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.10458.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.10458.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11167.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11167.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11167.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.12146.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.12146.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.12146.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11357.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11357.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11357.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11831.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11831.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11831.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11438.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11438.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11438.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.12135.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.12135.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.12135.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11275.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11275.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11275.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11157.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11157.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11157.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11330.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11330.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11330.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11775.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11775.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11775.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11098.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11098.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11098.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11901.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11901.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11901.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.09509.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.09509.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.09509.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11748.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11748.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11748.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.10550.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.10550.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.10550.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.12054.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.12054.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.12054.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.10454.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.10454.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.10454.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.08820.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.08820.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.08820.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11085.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11085.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11085.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11574.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11574.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11574.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.08826.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.08826.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.08826.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.09969.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.09969.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.09969.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.08441.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.08441.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.08441.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.09083.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.09083.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.09083.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.12154.
[19.02.2025 00:46] Downloading paper 2502.12154 from http://arxiv.org/pdf/2502.12154v1...
[19.02.2025 00:46] Extracting affiliations from text.
[19.02.2025 00:46] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Diffusion Models without Classifier-free Guidance Zhicong Tang 1 Jianmin Bao 2 Dong Chen 2 Baining Guo 2 5 2 0 2 7 1 ] . [ 1 4 5 1 2 1 . 2 0 5 2 : r a "
[19.02.2025 00:46] Response: ```python
[]
```
[19.02.2025 00:46] Extracting affiliations from text.
[19.02.2025 00:46] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Diffusion Models without Classifier-free Guidance Zhicong Tang 1 Jianmin Bao 2 Dong Chen 2 Baining Guo 2 5 2 0 2 7 1 ] . [ 1 4 5 1 2 1 . 2 0 5 2 : r aThis paper presents Model-guidance (MG), novel objective for training diffusion model that addresses and removes of the commonly used Classifier-free guidance (CFG). Our innovative approach transcends the standard modeling of solely data distribution to incorporating the posterior probability of conditions. The proposed technique originates from the idea of CFG and is easy yet effective, making it plug-and-play module for existing models. Our method significantly accelerates the training process, doubles the inference speed, and achieve exceptional quality that parallel and even surpass concurrent diffusion models with CFG. Extensive experiments demonstrate the effectiveness, efficiency, scalability on different models and datasets. Finally, we establish state-of-theart performance on ImageNet 256 benchmarks with an FID of 1.34. Our code is available at github.com/tzco/Diffusion-wo-CFG. 1. Introduction Diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021a;b) have become the cornerstone of many successful generative models, e.g. image generation (Dhariwal & Nichol, 2021; Nichol et al., 2022; Rombach et al., 2022; Podell et al., 2024; Chen et al., 2024) and video generation (Ho et al., 2022; Blattmann et al., 2023; Gupta et al., 2025; Polyak et al., 2024; Wang et al., 2024) tasks. However, diffusion models also struggle to generate low temperature samples (Ho & Salimans, 2021; Karras et al., 2024) due to the nature of training objectives, and techniques such as Classifier guidance (Dhariwal & Nichol, 2021) and Classifier-free guidance (CFG) (Ho & Salimans, 2021) are proposed to improve performances. Despite its advantage and ubiquity, CFG has several drawbacks (Karras et al., 2024) and poses challenges to effective implementations (Kynkaanniemi et al., 2024) of diffusion 1Tsinghua University 2Microsoft Research Asia. Figure 1: We propose Model-guidance (MG), removing Classifier-free guidance (CFG) for diffusion models and achieving state-of-the-art on ImageNet with FID of 1.34. (a) Instead of running models twice during inference (green and red), MG directly learns the final distribution (blue). (b) MG requires only one line of code modification while providing excellent improvements. (c) Comparing to concurrent methods, MG yields lowest FID even without CFG. models. One critical limitation is the simultaneous training of unconditional model apart from the main diffusion model. The unconditional model is typically implemented by randomly dropping the condition of training pairs and replacing with an manually defined empty label. The introduction of additional tasks may reduce network capabilities and lead to skewed sampling distributions (Karras et al., 2024; Kynkaanniemi et al., 2024). Furthermore, CFG requires two forward passes per denoising step during inference, one for the conditioned and another for the unconditioned model, thereby significantly escalating the computational costs. In this work, we propose Model-guidance (MG), an innovative method for diffusion models to effectively circumvent CFG and boost performances, thereby eliminating the limitations above. We propose novel objective that transcends from simply modeling the data distribution to incorporating the posterior probability of conditions. Specifically, we leverage the model itself as an implicit classifier and directly learn the score of calibrated distribution during training. As depicted in Figure 1, our proposed method confers mul1 Diffusion Models without Classifier-Free Guidance tiple substantial breakthroughs. First, it significantly refines generation quality and accelerates training processes, with experiments showcasing 6.5 convergence speedup than vanilla diffusion models with excellent quality. Second, the inference speed is doubled with our method, as each denoising step needs only one network forward in contrast to two in CFG. Besides, it is easy to implement and requires only one line of code modification, making it plug-and-play module of existing diffusion models with instant improvements. Finally, it is an end-to-end method that excels traditional two-stage distillation-based approaches and even outperforms CFG in generation performances. We conduct comprehensive experiments on the prevalent Imagenet (Deng et al., 2009; Russakovsky et al., 2015) benchmarks with 256256 and 512512 resolution and compare with wide variates of concurrent models to attest the effectiveness of our proposed method. The evaluation results demonstrate that our method not only parallels and even outperforms other approaches with CFG, but also scales to different models and datasets, making it promising enhancement for diffusion models. In conclusion, we make the following contribution in this work: We proposed novel and effective method, Modelguidance (MG), for training diffusion models. MG removes CFG for diffusion models and greatly accelerates both training and inference process. Extensive experiments with SOTA results on ImageNet demonstrate the usefulness and advantages of MG. 2. Background 2.1. Diffusion and Flow Models Diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021a;b) are class of generative models that utilize forward and reverse stochastic processes to model complex data distributions. The forward process adds noise and transforms data samples into Gaussian distributions as q(xtx0) = (cid:0)xt; Œ±tx0, (1 Œ±t)I(cid:1) , (1) where xt represents the noised data at timestep and Œ±t = (cid:81)t s=1 Œ±s is the noise schedule. Conversely, the reverse process learns to denoise and finally recover the original data distribution, which aims to reconstruct score (Sohl-Dickstein et al., 2015; Song et al., 2021b) from the noisy samples xt by learning pŒ∏(xt1xt) = (xt1; ¬µŒ∏(xt, t), Œ£Œ∏(xt, t)) , (2) where ¬µŒ∏ and Œ£Œ∏ are mean and variance and commonly predicted by neural networks. In common implementations, the training of diffusion mod2 els leverages re-parameterized objective that directly predicts the noise at each step (Ho et al., 2020) Lsimple = Et,x0,œµœµŒ∏(xt, t) œµ2, where xt is derived from the forward process in Equation (1) with x0 and œµ drawn from dataset and Gaussian noises. (3) Conditional diffusion models allow users to generate samples aligned with specified demands and"
[19.02.2025 00:46] Mistral response. {"id": "82f60f3c9a8f4abfabd15a178dc0faaf", "object": "chat.completion", "created": 1739925971, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Tsinghua University\", \"Microsoft Research Asia\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1765, "total_tokens": 1783, "completion_tokens": 18}}
[19.02.2025 00:46] Response: ```python
["Tsinghua University", "Microsoft Research Asia"]
```
[19.02.2025 00:46] Deleting PDF ./assets/pdf/2502.12154.pdf.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11177.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11177.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11177.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Downloading and parsing paper https://huggingface.co/papers/2502.11578.
[19.02.2025 00:46] Extra JSON file exists (./assets/json/2502.11578.json), skip PDF parsing.
[19.02.2025 00:46] Paper image links file exists (./assets/img_data/2502.11578.json), skip HTML parsing.
[19.02.2025 00:46] Success.
[19.02.2025 00:46] Enriching papers with extra data.
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 0. Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present N...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 1. Automatic fall recovery is a crucial prerequisite before humanoid robots can be reliably deployed. Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 2. We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from 50 bug fixes to \$32,000 feature implementations--and managerial tasks, w...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 3. Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize ...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 4. Code generation, symbolic math reasoning, and other tasks require LLMs to produce outputs that are both syntactically and semantically correct. Constrained LLM generation is a promising direction to enforce adherence to formal grammar, but prior works have empirically observed that strict enforcemen...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 5. The instruction hierarchy, which establishes a priority order from system messages to user messages, conversation history, and tool outputs, is essential for ensuring consistent and safe behavior in language models (LMs). Despite its importance, this topic receives limited attention, and there is a ...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 6. The remarkable success of the autoregressive paradigm has made significant advancement in Multimodal Large Language Models (MLLMs), with powerful models like Show-o, Transfusion and Emu3 achieving notable progress in unified image understanding and generation. For the first time, we uncover a common...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 7. Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowl...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 8. This paper presents ThinkDiff, a novel alignment paradigm that empowers text-to-image diffusion models with multimodal in-context understanding and reasoning capabilities by integrating the strengths of vision-language models (VLMs). Existing multimodal diffusion finetuning methods largely focus on ...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 9. Large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks, such as code understanding and code generation. However, an equally important yet underexplored question is whether LLMs can serve as general-purpose surrogate code executors, to predict the output and beha...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 10. We propose Diffusion-Sharpening, a fine-tuning approach that enhances downstream alignment by optimizing sampling trajectories. Existing RL-based fine-tuning methods focus on single training timesteps and neglect trajectory-level alignment, while recent sampling trajectory optimization methods incur...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 11. Recent success in large multimodal models (LMMs) has sparked promising applications of agents capable of autonomously completing complex web tasks. While open-source LMM agents have made significant advances in offline evaluation benchmarks, their performance still falls substantially short of human...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 12. We investigate the emergence of intuitive physics understanding in general-purpose deep neural network models trained to predict masked regions in natural videos. Leveraging the violation-of-expectation framework, we find that video prediction models trained to predict outcomes in a learned represen...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 13. Text-to-SQL aims to convert natural language questions into executable SQL queries. While previous approaches, such as skeleton-masked selection, have demonstrated strong performance by retrieving similar training examples to guide large language models (LLMs), they struggle in real-world scenarios ...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 14. With the explosive growth of 3D content creation, there is an increasing demand for automatically converting static 3D models into articulation-ready versions that support realistic animation. Traditional approaches rely heavily on manual annotation, which is both time-consuming and labor-intensive....
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 15. Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE m...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 16. We present Dyve, a dynamic process verifier that enhances reasoning error detection in large language models by integrating fast and slow thinking, inspired by Kahneman's Systems Theory. Dyve adaptively applies immediate token-level confirmation System 1 for straightforward steps and comprehensive a...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 17. System messages play a crucial role in interactions with large language models (LLMs), often serving as prompts to initiate conversations. Through system messages, users can assign specific roles, perform intended tasks, incorporate background information, specify various output formats and communic...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 18. While recent advancements in reasoning optimization have significantly enhanced the capabilities of large language models (LLMs), existing efforts to improve reasoning have been limited to solving mathematical problems and focusing on visual graphical inputs, neglecting broader applications in gener...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 19. Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose Talk Structurally, Act Hierarchically (TalkHier), a novel framework that in...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 20. Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model ...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 21. Latent generative models have emerged as a leading approach for high-quality image synthesis. These models rely on an autoencoder to compress images into a latent space, followed by a generative model to learn the latent distribution. We identify that existing autoencoders lack equivariance to seman...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 22. This work introduces ILIAS, a new test dataset for Instance-Level Image retrieval At Scale. It is designed to evaluate the ability of current and future foundation models and retrieval techniques to recognize particular objects. The key benefits over existing datasets include large scale, domain div...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 23. Memory is crucial for enabling agents to tackle complex tasks with temporal and spatial dependencies. While many reinforcement learning (RL) algorithms incorporate memory, the field lacks a universal benchmark to assess an agent's memory capabilities across diverse scenarios. This gap is particularl...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 24. Large language models demonstrate remarkable capabilities across various domains, especially mathematics and logic reasoning. However, current evaluations overlook physics-based reasoning - a complex task requiring physics theorems and constraints. We present PhysReason, a 1,200-problem benchmark co...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 25. Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their dee...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 26. Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 27. This paper challenges the recent paradigm in atomic property prediction that links progress to growing dataset sizes and computational resources. We show that pretraining on a carefully selected, task-relevant dataset can match or even surpass large-scale pretraining, while using as little as 1/24th...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 28. This paper investigates the mathematical reasoning capabilities of large language models (LLMs) using 50 newly constructed high-school-level word problems. Unlike prior studies that focus solely on answer correctness, we rigorously analyze both final answers and solution steps to identify reasoning ...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 29. Large Language Models (LLMs) struggle with hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information enhancing factual and updated grounding. Recent advances in multimod...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 30. Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scal...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 31. Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitiga...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 32. The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-ch...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 33. This paper presents Model-guidance (MG), a novel objective for training diffusion model that addresses and removes of the commonly used Classifier-free guidance (CFG). Our innovative approach transcends the standard modeling of solely data distribution to incorporating the posterior probability of c...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 34. Despite near-perfect results in artificial evaluations, the effectiveness of model editing in real-world applications remains unexplored. To bridge this gap, we propose to study model editing in question answering (QA) by establishing a rigorous evaluation practice to assess the effectiveness of edi...
[19.02.2025 00:46] ********************************************************************************
[19.02.2025 00:46] Abstract 35. Large Language Models (LLMs) have made significant strides in natural language generation but often face challenges in tasks requiring precise calculations and structural analysis. This paper investigates the performance of state-of-the-art LLMs on language complexity measurement tasks, through the ...
[19.02.2025 00:46] Read previous papers.
[19.02.2025 00:46] Generating reviews via LLM API.
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#long_context", "#training", "#optimization", "#architecture"], "emoji": "üöÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç NSA - –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –Ω–∞—Ç–∏–≤–Ω–æ –æ–±—É—á–∞–µ–º–æ–≥–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#training", "#games", "#robotics", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–†–æ–±–æ—Ç—ã —É—á–∞—Ç—Å—è –≤—Å—Ç–∞–≤–∞—Ç—å: –ø—Ä–æ—Ä—ã–≤ –≤ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –≥—É–º–∞–Ω–æ–∏–¥–∞–º–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É —Å–∏—Å—Ç–µ–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–æ–≤, –ø–æ–∑–≤–æ–ª—è—é—â–∏—Ö –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã–º —Ä–æ–±–æ—Ç–∞–º –≤—Å—Ç
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#dataset", "#science", "#benchmark", "#open_source"], "emoji": "üíª", "ru": {"title": "SWE-Lancer: –ò–∑–º–µ—Ä—è–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ò–ò –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ü–û", "desc": "SWE-Lancer - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ –æ–±–ª–∞—Å—Ç–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#data", "#training", "#hallucinations", "#open_source", "#benchmark", "#optimization"], "emoji": "üß†", "ru": {"title": "ReLearn: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ ReLearn –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –í –æ—Ç
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#benchmark", "#reasoning", "#training", "#architecture"], "emoji": "üß†", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#multimodal", "#alignment", "#benchmark"], "emoji": "üé≠", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π: –∞—Ö–∏–ª–ª–µ—Å–æ–≤–∞ –ø—è—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ IHEval –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#training", "#multimodal", "#dataset", "#alignment"], "emoji": "üåâ", "ru": {"title": "HermesFlow: –º–æ—Å—Ç –º–µ–∂–¥—É –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ HermesFlow –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, 
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#data", "#training", "#architecture", "#transfer_learning", "#optimization"], "emoji": "üß†", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞—è —Ç–∞–π–Ω—ã –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: —ç–≤–æ–ª—é—Ü–∏—è —Ü–µ–ø–µ–π –∑–Ω–∞–Ω–∏–π –≤ LLM", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ —É—Å–≤–æ–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (L
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#multimodal", "#diffusion", "#benchmark", "#alignment"], "emoji": "üß†", "ru": {"title": "ThinkDiff: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ThinkDiff - —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–¥–µ–ª—è–µ—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ç–µ–∫—Å—Ç-–∏–∑–æ–±—Ä
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#training", "#plp", "#dataset", "#agi", "#open_source", "#benchmark", "#optimization"], "emoji": "üß†", "ru": {"title": "LLM –∫–∞–∫ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–∏ –∫–æ–¥–∞: –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ SURGE - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#training", "#rlhf", "#optimization", "#diffusion", "#alignment", "#rl"], "emoji": "üéØ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Diffusion-Sharpening –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#synthetic", "#dataset", "#agents", "#benchmark"], "emoji": "üåê", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –∫–ª—é—á –∫ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—é –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#reasoning", "#video", "#architecture", "#multimodal", "#agi"], "emoji": "üß†", "ru": {"title": "–ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–∞—è —Ñ–∏–∑–∏–∫–∞ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞–ª–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ–∏–∑–∏–∫–∏ –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö, –æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#data", "#dataset", "#transfer_learning", "#optimization", "#training"], "emoji": "üîç", "ru": {"title": "–°–∞–º–æ—É—Å–∏–ª–µ–Ω–∏–µ –ò–ò –≤ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≤ SQL", "desc": "SAFE-SQL - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –≤ SQL-–∑–∞–ø—Ä–æ—Å—ã. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è 
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#dataset", "#architecture"], "emoji": "ü¶æ", "ru": {"title": "–ú–∞–≥–∏—è –æ–∂–∏–≤–ª–µ–Ω–∏—è 3D: –æ—Ç —Å—Ç–∞—Ç–∏–∫–∏ –∫ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏", "desc": "MagicArticulate - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π –≤ –≥–æ—Ç–æ–≤—ã–µ –∫ –∞–Ω–∏–º–∞—Ü–∏–∏ –≤–µ—Ä—Å–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ 
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#optimization", "#training", "#dataset", "#data", "#transfer_learning"], "emoji": "üê£", "ru": {"title": "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ –ø–ª–µ—á–∞—Ö –≥–∏–≥–∞–Ω—Ç–æ–≤: –∫–∞–∫ IE –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ—Å—É—Ä—Å—ã LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (IE) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#reasoning", "#math", "#training", "#optimization", "#data", "#benchmark"], "emoji": "üß†", "ru": {"title": "Dyve: —É–º–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ –ò–ò-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Dyve - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, —É–ª—É—á—à–∞—é—â–∏–π –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö 
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#open_source", "#alignment", "#training", "#benchmark", "#dataset"], "emoji": "ü§ñ", "ru": {"title": "SysGen: —É–ª—É—á—à–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ—Ç–≤–µ—Ç–æ–≤ LLM —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SysGen - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#reasoning", "#video", "#training", "#open_source", "#optimization", "#benchmark", "#multimodal", "#dataset"], "emoji": "üé•", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç video-SALMONN-o1 - –ø–µ—Ä–≤—É—é –æ—Ç–∫—Ä—ã—Ç—É
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#agents", "#multimodal", "#alignment"], "emoji": "ü§ñ", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ –∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ TalkHier –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è 
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#reasoning", "#training", "#dataset", "#data", "#plp", "#transfer_learning", "#synthetic"], "emoji": "üß†", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é,
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#diffusion", "#architecture", "#training", "#cv", "#optimization"], "emoji": "üñºÔ∏è", "ru": {"title": "–≠–∫–≤–∏–≤–∞—Ä–∏–∞–Ω—Ç–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EQ-VAE - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π 
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#cv", "#dataset", "#benchmark"], "emoji": "üîç", "ru": {"title": "ILIAS: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "ILIAS - —ç—Ç–æ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –≤ –∫—Ä—É–ø–Ω–æ–º –º–∞—Å—à—Ç–∞–±–µ. –û–Ω —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#rl", "#optimization", "#benchmark", "#agents", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "MIKASA: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–∞–º—è—Ç–∏ RL-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MIKASA - –Ω–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø–∞–º—è—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#math", "#benchmark", "#reasoning"], "emoji": "üß†", "ru": {"title": "PhysReason: –∏—Å–ø—ã—Ç–∞–Ω–∏–µ —Ñ–∏–∑–∏–∫–æ–π –¥–ª—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PhysReason - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ—Å—Ç–æ
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#reasoning", "#training", "#math", "#optimization", "#dataset"], "emoji": "üßÆ", "ru": {"title": "–ö–æ–Ω—Ç—Ä–ø—Ä–∏–º–µ—Ä—ã –∫–∞–∫ –∫–ª—é—á –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞. –ê–≤—Ç–æ—Ä—ã 
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#benchmark", "#multimodal", "#dataset", "#agi", "#agents"], "emoji": "ü§ñ", "ru": {"title": "CALM: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ CALM (Conversational Agentic Language 
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#training", "#dataset", "#benchmark", "#data"], "emoji": "üß™", "ru": {"title": "–ö–∞—á–µ—Å—Ç–≤–æ –≤–∞–∂–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞—Ç–æ–º–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Å—Ç–∞–≤–∏—Ç –ø–æ–¥ —Å–æ–º–Ω–µ–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –∞—Ç–æ
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#training", "#dataset", "#reasoning", "#math"], "emoji": "üßÆ", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ 50 –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á —É—Ä–æ–≤–Ω—è —Å—Ç–∞—Ä—à–µ–π —à–∫–æ–ª—ã. –ê–≤
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#optimization", "#training", "#rag", "#multimodal", "#survey", "#benchmark", "#hallucinations"], "emoji": "ü§ñ", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π RAG: –ù–æ–≤—ã–π —Ñ—Ä–æ–Ω—Ç–∏—Ä –≤ –æ–±–æ–≥–∞—â–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä —Å–∏—Å—Ç–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#optimization", "#interpretability", "#training", "#small_models"], "emoji": "üöÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–ª–∏—è–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –º–∞–ª—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –≤–ª–∏—è–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Influe
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#training", "#optimization", "#dataset"], "emoji": "üî¨", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤–ª–æ–∂–µ–Ω–∏–π —Å–ª–æ–≤ —Å –ø–æ–º–æ—â—å—é Coupled Adam", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∞–Ω–∏–∑–æ—Ç—Ä–æ–ø–∏–∏ –≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö —Å–ª–æ–≤, –æ–±—É—á–∞–µ–º—ã—Ö –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –≤—Ç–æ—Ä–æ–π –º–æ–º–µ–Ω—Ç –≤ –æ–ø—Ç–∏–º–∏–∑
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#ethics", "#healthcare", "#multimodal", "#data"], "emoji": "üîç", "ru": {"title": "–û–±—ä—è—Å–Ω–∏–º–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–µ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤ –≤ —ç–ø–æ—Ö—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[19.02.2025 00:46] Querying the API.
[19.02.2025 00:46] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

This paper presents Model-guidance (MG), a novel objective for training diffusion model that addresses and removes of the commonly used Classifier-free guidance (CFG). Our innovative approach transcends the standard modeling of solely data distribution to incorporating the posterior probability of conditions. The proposed technique originates from the idea of CFG and is easy yet effective, making it a plug-and-play module for existing models. Our method significantly accelerates the training process, doubles the inference speed, and achieve exceptional quality that parallel and even surpass concurrent diffusion models with CFG. Extensive experiments demonstrate the effectiveness, efficiency, scalability on different models and datasets. Finally, we establish state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34. Our code is available at https://github.com/tzco/Diffusion-wo-CFG.
[19.02.2025 00:46] Response: {
  "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Model-guidance (MG) - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ Classifier-free guidance (CFG). MG —É–ª—É—á—à–∞–µ—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ª–æ–≤–∏–π. –ú–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥–∞, –¥–æ—Å—Ç–∏–≥–∞—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å MG –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ ImageNet 256 —Å FID 1.34.",
  "emoji": "üöÄ",
  "title": "Model-guidance: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ CFG"
}
[19.02.2025 00:46] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper presents Model-guidance (MG), a novel objective for training diffusion model that addresses and removes of the commonly used Classifier-free guidance (CFG). Our innovative approach transcends the standard modeling of solely data distribution to incorporating the posterior probability of conditions. The proposed technique originates from the idea of CFG and is easy yet effective, making it a plug-and-play module for existing models. Our method significantly accelerates the training process, doubles the inference speed, and achieve exceptional quality that parallel and even surpass concurrent diffusion models with CFG. Extensive experiments demonstrate the effectiveness, efficiency, scalability on different models and datasets. Finally, we establish state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34. Our code is available at https://github.com/tzco/Diffusion-wo-CFG."

[19.02.2025 00:46] Response: ```python
['TRAINING', 'BENCHMARK', 'CV', 'DATASET']
```
[19.02.2025 00:46] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper presents Model-guidance (MG), a novel objective for training diffusion model that addresses and removes of the commonly used Classifier-free guidance (CFG). Our innovative approach transcends the standard modeling of solely data distribution to incorporating the posterior probability of conditions. The proposed technique originates from the idea of CFG and is easy yet effective, making it a plug-and-play module for existing models. Our method significantly accelerates the training process, doubles the inference speed, and achieve exceptional quality that parallel and even surpass concurrent diffusion models with CFG. Extensive experiments demonstrate the effectiveness, efficiency, scalability on different models and datasets. Finally, we establish state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34. Our code is available at https://github.com/tzco/Diffusion-wo-CFG."

[19.02.2025 00:46] Response: ```python
['DIFFUSION', 'OPTIMIZATION']
```
[19.02.2025 00:46] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Model-guidance (MG), a new objective for training diffusion models that eliminates the need for Classifier-free guidance (CFG). MG enhances the training process by integrating the posterior probability of conditions, rather than just focusing on data distribution. The method is designed to be easily implemented as a plug-and-play module, improving both training speed and inference efficiency. Experimental results show that MG not only accelerates training and doubles inference speed but also achieves superior quality, setting new benchmarks on ImageNet 256 with an FID of 1.34.","title":"Revolutionizing Diffusion Models with Model-Guidance"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Model-guidance (MG), a new objective for training diffusion models that eliminates the need for Classifier-free guidance (CFG). MG enhances the training process by integrating the posterior probability of conditions, rather than just focusing on data distribution. The method is designed to be easily implemented as a plug-and-play module, improving both training speed and inference efficiency. Experimental results show that MG not only accelerates training and doubles inference speed but also achieves superior quality, setting new benchmarks on ImageNet 256 with an FID of 1.34.', title='Revolutionizing Diffusion Models with Model-Guidance'))
[19.02.2025 00:46] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁõÆÊ†áÂáΩÊï∞ÔºåÁß∞‰∏∫Ê®°ÂûãÂºïÂØºÔºàModel-guidance, MGÔºâÔºåÁî®‰∫éËÆ≠ÁªÉÊâ©Êï£Ê®°ÂûãÔºåÊó®Âú®Ëß£ÂÜ≥ÂíåÂéªÈô§Â∏∏Áî®ÁöÑÊó†ÂàÜÁ±ªÂô®ÂºïÂØºÔºàClassifier-free guidance, CFGÔºâ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïË∂ÖË∂ä‰∫Ü‰ªÖ‰ªÖÂª∫Ê®°Êï∞ÊçÆÂàÜÂ∏ÉÁöÑÊ†áÂáÜÊñπÂºèÔºåËûçÂÖ•‰∫ÜÊù°‰ª∂ÁöÑÂêéÈ™åÊ¶ÇÁéá„ÄÇËØ•ÊäÄÊúØÊ∫ê‰∫éCFGÁöÑÁêÜÂøµÔºåÁÆÄÂçïËÄåÊúâÊïàÔºåËÉΩÂ§ü‰Ωú‰∏∫Áé∞ÊúâÊ®°ÂûãÁöÑÂç≥ÊèíÂç≥Áî®Ê®°Âùó„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÊòæËëóÂä†Âø´‰∫ÜËÆ≠ÁªÉËøáÁ®ãÔºåÊèêÂçá‰∫ÜÊé®ÁêÜÈÄüÂ∫¶ÔºåÂπ∂Âú®Ë¥®Èáè‰∏ä‰∏éÁîöËá≥Ë∂ÖË∂ä‰∫Ü‰ΩøÁî®CFGÁöÑÊâ©Êï£Ê®°ÂûãÔºåÊúÄÁªàÂú®ImageNet 256Âü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫Ü1.34ÁöÑFIDÔºåÂ±ïÁé∞‰∫ÜÂçìË∂äÁöÑÊÄßËÉΩ„ÄÇ","title":"Ê®°ÂûãÂºïÂØºÔºöË∂ÖË∂ä‰º†ÁªüÁöÑÊâ©Êï£Ê®°ÂûãËÆ≠ÁªÉ"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁõÆÊ†áÂáΩÊï∞ÔºåÁß∞‰∏∫Ê®°ÂûãÂºïÂØºÔºàModel-guidance, MGÔºâÔºåÁî®‰∫éËÆ≠ÁªÉÊâ©Êï£Ê®°ÂûãÔºåÊó®Âú®Ëß£ÂÜ≥ÂíåÂéªÈô§Â∏∏Áî®ÁöÑÊó†ÂàÜÁ±ªÂô®ÂºïÂØºÔºàClassifier-free guidance, CFGÔºâ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïË∂ÖË∂ä‰∫Ü‰ªÖ‰ªÖÂª∫Ê®°Êï∞ÊçÆÂàÜÂ∏ÉÁöÑÊ†áÂáÜÊñπÂºèÔºåËûçÂÖ•‰∫ÜÊù°‰ª∂ÁöÑÂêéÈ™åÊ¶ÇÁéá„ÄÇËØ•ÊäÄÊúØÊ∫ê‰∫éCFGÁöÑÁêÜÂøµÔºåÁÆÄÂçïËÄåÊúâÊïàÔºåËÉΩÂ§ü‰Ωú‰∏∫Áé∞ÊúâÊ®°ÂûãÁöÑÂç≥ÊèíÂç≥Áî®Ê®°Âùó„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÊòæËëóÂä†Âø´‰∫ÜËÆ≠ÁªÉËøáÁ®ãÔºåÊèêÂçá‰∫ÜÊé®ÁêÜÈÄüÂ∫¶ÔºåÂπ∂Âú®Ë¥®Èáè‰∏ä‰∏éÁîöËá≥Ë∂ÖË∂ä‰∫Ü‰ΩøÁî®CFGÁöÑÊâ©Êï£Ê®°ÂûãÔºåÊúÄÁªàÂú®ImageNet 256Âü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫Ü1.34ÁöÑFIDÔºåÂ±ïÁé∞‰∫ÜÂçìË∂äÁöÑÊÄßËÉΩ„ÄÇ', title='Ê®°ÂûãÂºïÂØºÔºöË∂ÖË∂ä‰º†ÁªüÁöÑÊâ©Êï£Ê®°ÂûãËÆ≠ÁªÉ'))
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#optimization", "#reasoning", "#training", "#dataset"], "emoji": "üîç", "ru": {"title": "–ü–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ç –∏–ª–ª—é–∑–∏–∏ –∫ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –≤
[19.02.2025 00:46] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#interpretability", "#science"], "emoji": "üìä", "ru": {"title": "–ò–∑–º–µ—Ä–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–∞ –∫–∞–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–∞, –≤ —á–∞
[19.02.2025 00:46] Loading Chinese text from previous data.
[19.02.2025 00:46] Renaming data file.
[19.02.2025 00:46] Renaming previous data. hf_papers.json to ./d/2025-02-19.json
[19.02.2025 00:46] Saving new data file.
[19.02.2025 00:46] Generating page.
[19.02.2025 00:46] Renaming previous page.
[19.02.2025 00:46] Renaming previous data. index.html to ./d/2025-02-19.html
[19.02.2025 00:46] [Experimental] Generating Chinese page for reading.
[19.02.2025 00:46] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': '‰∫∫ÂΩ¢Êú∫Âô®‰∫∫', 'pinyin': 'r√©n x√≠ng jƒ´ q√¨ r√©n', 'trans': 'humanoid robot'}, {'word': 'Ëá™Âä®', 'pinyin': 'z√¨ d√≤ng', 'trans': 'automatic'}, {'word': 'Ë∑åÂÄí', 'pinyin': 'diƒì d«éo', 'trans': 'fall down'}, {'word': 'ÊÅ¢Â§ç', 'pinyin': 'huƒ´ f√π', 'trans': 'recover'}, {'word': 'ÈáçË¶ÅÊÄß', 'pinyin': 'zh√≤ng y√†o x√¨ng', 'trans': 'importance'}, {'word': 'ËÆæËÆ°', 'pinyin': 'sh√® j√¨', 'trans': 'design'}, {'word': 'ÊéßÂà∂Âô®', 'pinyin': 'k√≤ng zh√¨ q√¨', 'trans': 'controller'}, {'word': 'Á´ôËµ∑Êù•', 'pinyin': 'zh√†n q«ê l√°i', 'trans': 'stand up'}, {'word': 'ÂßøÊÄÅ', 'pinyin': 'zƒ´ t√†i', 'trans': 'posture'}, {'word': 'Âú∞ÂΩ¢', 'pinyin': 'd√¨ x√≠ng', 'trans': 'terrain'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√π z√°', 'trans': 'complex'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'Â≠¶‰π†', 'pinyin': 'xu√© x√≠', 'trans': 'learn'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ng ji√†', 'trans': 'framework'}, {'word': 'Èò∂ÊÆµ', 'pinyin': 'jiƒì du√†n', 'trans': 'stage'}, {'word': 'Ë∑ØÂæÑ', 'pinyin': 'l√π j√¨ng', 'trans': 'path'}, {'word': '‰ºòÂåñ', 'pinyin': 'y≈çu hu√†', 'trans': 'optimize'}, {'word': 'Âä®‰Ωú', 'pinyin': 'd√≤ng zu√≤', 'trans': 'action'}, {'word': 'Âπ≥Á®≥', 'pinyin': 'p√≠ng wƒõn', 'trans': 'stable'}, {'word': 'ÂèØÈù†', 'pinyin': 'kƒõ k√†o', 'trans': 'reliable'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'Âú∞Èù¢', 'pinyin': 'd√¨ mi√†n', 'trans': 'ground'}, {'word': 'ÊàêÂäü', 'pinyin': 'ch√©ng g≈çng', 'trans': 'success'}, {'word': 'È¶ñÊ¨°', 'pinyin': 'sh«íu c√¨', 'trans': 'first time'}, {'word': 'ÁúüÂÆû', 'pinyin': 'zhƒìn sh√≠', 'trans': 'real'}, {'word': 'ÁéØÂ¢É', 'pinyin': 'hu√°n j√¨ng', 'trans': 'environment'}, {'word': 'Â∫îÁî®', 'pinyin': 'y√¨ng y√≤ng', 'trans': 'apply'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}]
[19.02.2025 00:46] Renaming previous Chinese page.
[19.02.2025 00:46] Renaming previous data. zh.html to ./d/2025-02-18_zh_reading_task.html
[19.02.2025 00:46] Writing Chinese reading task.
[19.02.2025 00:46] Writing result.
[19.02.2025 00:46] Renaming log file.
[19.02.2025 00:46] Renaming previous data. log.txt to ./logs/2025-02-19_last_log.txt
