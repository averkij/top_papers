[19.02.2025 03:18] Read previous papers.
[19.02.2025 03:18] Generating top page (month).
[19.02.2025 03:18] Writing top page (month).
[19.02.2025 04:13] Read previous papers.
[19.02.2025 04:13] Get feed.
[19.02.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11079
[19.02.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13143
[19.02.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13131
[19.02.2025 04:13] Extract page data from URL. URL: https://huggingface.co/papers/2502.11564
[19.02.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13145
[19.02.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11433
[19.02.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12513
[19.02.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12501
[19.02.2025 04:13] Extract page data from URL. URL: https://huggingface.co/papers/2502.09838
[19.02.2025 04:13] Extract page data from URL. URL: https://huggingface.co/papers/2502.12170
[19.02.2025 04:13] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12574
[19.02.2025 04:13] Extract page data from URL. URL: https://huggingface.co/papers/2502.10852
[19.02.2025 04:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.02.2025 04:13] No deleted papers detected.
[19.02.2025 04:13] Downloading and parsing papers (pdf, html). Total: 12.
[19.02.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2502.11079.
[19.02.2025 04:13] Extra JSON file exists (./assets/json/2502.11079.json), skip PDF parsing.
[19.02.2025 04:13] Paper image links file exists (./assets/img_data/2502.11079.json), skip HTML parsing.
[19.02.2025 04:13] Success.
[19.02.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2502.13143.
[19.02.2025 04:13] Extra JSON file exists (./assets/json/2502.13143.json), skip PDF parsing.
[19.02.2025 04:13] Paper image links file exists (./assets/img_data/2502.13143.json), skip HTML parsing.
[19.02.2025 04:13] Success.
[19.02.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2502.13131.
[19.02.2025 04:13] Extra JSON file exists (./assets/json/2502.13131.json), skip PDF parsing.
[19.02.2025 04:13] Paper image links file exists (./assets/img_data/2502.13131.json), skip HTML parsing.
[19.02.2025 04:13] Success.
[19.02.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2502.11564.
[19.02.2025 04:13] Downloading paper 2502.11564 from http://arxiv.org/pdf/2502.11564v1...
[19.02.2025 04:13] Extracting affiliations from text.
[19.02.2025 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Jaehyeong Jo 1 Sung Ju Hwang 1 2 5 2 0 2 7 1 ] . [ 1 4 6 5 1 1 . 2 0 5 2 : r a "
[19.02.2025 04:13] Response: []
[19.02.2025 04:13] Extracting affiliations from text.
[19.02.2025 04:13] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Jaehyeong Jo 1 Sung Ju Hwang 1 2 5 2 0 2 7 1 ] . [ 1 4 6 5 1 1 . 2 0 5 2 : r aDiffusion models have emerged as promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discrete states. Existing continuous diffusion models for discrete data have limited performance compared to discrete approaches, and the unclear link between them restricts the development of diffusion models for discrete data. In this work, we propose continuous diffusion model for language modeling that incorporates the geometry of the underlying categorical distribution. We establish connection between the discrete diffusion and continuous flow on the statistical manifold, and building on the analogy, we introduce simple design for the diffusion process that generalizes previous discrete diffusion models. We further propose simulation-free training framework based on radial symmetry and simple technique to address the high dimensionality of the manifold. Comprehensive experiments on language modeling benchmarks and other modalities show that our method outperforms existing discrete diffusion models and approaches the performance of autoregressive models. Codes available at https://github.com/harryjo97/RDLM. 1. Introduction Discrete diffusion models (Austin et al., 2021; Lou et al., 2024) emerged as promising competitor to autoregressive models for the generative modeling of discrete data. These models have demonstrated competitive performance on tasks such as language modeling (Shi et al., 2024; Sahoo et al., 2024) and code generation (Gat et al., 2024). Unlike 1Korea Advanced ogy (KAIST) hyeong <sungju.hwang@kaist.ac.kr>. Institute 2DeepAuto.ai. Jo <harryjo97@kaist.ac.kr>, of Science Correspondence to: Sung and TechnolJaeJu Hwang Preprint. Under review. 1 autoregressive models that generate data sequentially, diffusion models generate the sequence in parallel, allowing for bidirectional controllable generation and faster sampling. However, discrete diffusion models do not fully leverage the power of iterative refinement which is the key to generative modeling of continuous data, for example, image synthesis (Saharia et al., 2022; Esser et al., 2024) and video generation (Polyak et al., 2024; Brooks et al., 2024). In discrete diffusion models, the progressive corruption during the forward process is modeled by stochastic jumps between states in Markov chains. Since denoising is achieved by jumping between states, discrete diffusion loses valuable signals during refinement which limits the generative performance and controllability. Several efforts were made to adapt continuous diffusion models for discrete data, but their performance falls short of discrete diffusion models, demonstrating significant gap compared to autoregressive models. Past works applied diffusion models for images to discrete data through continuous relaxation without constraint (Han et al., 2022; Li et al., 2022). Other lines of works (Avdeyev et al., 2023; Stärk et al., 2024) map discrete data to the probability simplex which exerts strong prior assumption on Dirichlet distribution, but often fails to model complex patterns. This led to recent works (Cheng et al., 2024; Davis et al., 2024) that apply flow matching to learn the categorical distributions using the structure of the statistical manifold, but these methods are limited to small sequences and categories. In particular, the link between discrete and continuous diffusion remains unclear, hindering the development of coherent diffusion framework for discrete data. In this work, we present Riemannian Diffusion Language Model (RDLM), continuous diffusion framework for language modeling that incorporates the geometry of the statistical manifold in the diffusion processes. We establish connection between continuous flow on the statistical manifold and the discrete diffusion process, showing that the trajectory of the transition distribution can be modeled to conditional flow on the manifold. Based on the analogy, we introduce simple design of the diffusion processes on the manifold that generalizes previous discrete diffusion models. We further present simulation-free training scheme using radial symmetry that integrates simple parameterization and Continuous Diffusion Model for Language Modeling maximum likelihood-based training objectives. Through experiments on language modeling tasks, image modeling, and biological sequence design, we validate that our framework outperforms existing discrete diffusion models. 2. Background 2.1. Discrete diffusion models Discrete diffusion models (Austin et al., 2021; Lou et al., 2024; Sahoo et al., 2024; Shi et al., 2024) define the diffusion process directly on discrete structures using the Markov chains. The forward process describes the transition from the current state to other states, which is formalized by multiplying the transition matrix Qt: q(xtxt1) = Cat(xt; Qtxt1), (1) where xt is the random variable for the discrete states and Cat() denotes the categorical distribution. This induces the marginal distribution that corresponds to repeatedly multiplying the transition matrices over time steps: q(xtx) = Cat(xt; Qtx) = Cat(xt; Qt Q1x). (2) Austin et al. (2021) introduced several designs of the transition matrices, including the masked (absorbing state) diffusion and the uniform diffusion, and the continuous-time Markov chains (CTMC) (Austin et al., 2021; Campbell et al., 2022) extends the framework to continuous-time. 2.2. Statistical Manifold of Categorical Distribution Let = {1, , d} denote the discrete data space and d1 = {(p1, , pd) Rd (cid:80) pi = 1, pi 0} denote the (d 1)-dimensional probability simplex. d-class categorical distribution over can be parameterized by the parameters p1, , pd such that {(cid:80) pi = 1, pi 0}. Then the statistical manifold P(X ) of the categorical distribution corresponds to d1 equipped with the FisherRao metric (Rao, 1992; Amari, 2016) (see Appendix A.1). Moreover, there exists diffeomorphism from P(X ) to the positive orthant of (d 1)-dimensional sphere Sd1 + : π : P(X ) Sd1 + ; pi (cid:55) ui = pi, (3) which induces the following geodesic distance on Sd1 + : dg(u, v) = cos1u, v, (4) where denotes the Euclidean inner product. We provide further explanation in Appendix A.1. 2.3. Riemannian Diffusion Mixt"
[19.02.2025 04:13] Mistral response. {"id": "93f2aca61f8947809fb1ce9df582eb3a", "object": "chat.completion", "created": 1739938394, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Korea Advanced Institute of Science and Technology (KAIST)\", \"DeepAuto.ai\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1631, "total_tokens": 1658, "completion_tokens": 27}}
[19.02.2025 04:13] Response: ```python
["Korea Advanced Institute of Science and Technology (KAIST)", "DeepAuto.ai"]
```
[19.02.2025 04:13] Deleting PDF ./assets/pdf/2502.11564.pdf.
[19.02.2025 04:13] Success.
[19.02.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2502.13145.
[19.02.2025 04:13] Extra JSON file exists (./assets/json/2502.13145.json), skip PDF parsing.
[19.02.2025 04:13] Paper image links file exists (./assets/img_data/2502.13145.json), skip HTML parsing.
[19.02.2025 04:13] Success.
[19.02.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2502.11433.
[19.02.2025 04:13] Extra JSON file exists (./assets/json/2502.11433.json), skip PDF parsing.
[19.02.2025 04:13] Paper image links file exists (./assets/img_data/2502.11433.json), skip HTML parsing.
[19.02.2025 04:13] Success.
[19.02.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2502.12513.
[19.02.2025 04:13] Extra JSON file exists (./assets/json/2502.12513.json), skip PDF parsing.
[19.02.2025 04:13] Paper image links file exists (./assets/img_data/2502.12513.json), skip HTML parsing.
[19.02.2025 04:13] Success.
[19.02.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2502.12501.
[19.02.2025 04:13] Extra JSON file exists (./assets/json/2502.12501.json), skip PDF parsing.
[19.02.2025 04:13] Paper image links file exists (./assets/img_data/2502.12501.json), skip HTML parsing.
[19.02.2025 04:13] Success.
[19.02.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2502.09838.
[19.02.2025 04:13] Downloading paper 2502.09838 from http://arxiv.org/pdf/2502.09838v2...
[19.02.2025 04:13] Extracting affiliations from text.
[19.02.2025 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"HealthGPT: Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation Tianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3, Mengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5 1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba, 4The Hong Kong University of Science and Technology, 5National University of Singapore Project Page Code 5 2 0 2 7 1 ] . [ 2 8 3 8 9 0 . 2 0 5 2 : r Figure 1: HealthGPT enables medical multi-modal comprehension and generation, outperforming both state-of-the-art unified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling complex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation. Abstract We present HealthGPT, powerful Medical Large VisionLanguage Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by tailored hierarchical visual perception approach and three-stage learning strategy. To effectively learn the HealthGPT, we devise comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT. Large Vision-Language Models (LVLMs) (Liu et al. 2023; OpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have demonstrated outstanding open-world visual comprehension and reasoning abilities through lan"
[19.02.2025 04:13] Response: ```python
[
    "Zhejiang University",
    "University of Electronic Science and Technology of China",
    "Alibaba",
    "The Hong Kong University of Science and Technology",
    "National University of Singapore"
]
```
[19.02.2025 04:13] Deleting PDF ./assets/pdf/2502.09838.pdf.
[19.02.2025 04:13] Success.
[19.02.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2502.12170.
[19.02.2025 04:13] Downloading paper 2502.12170 from http://arxiv.org/pdf/2502.12170v1...
[19.02.2025 04:13] Extracting affiliations from text.
[19.02.2025 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections Da Xiao 1 Qingye Meng 2 Shengping Li 2 Xingyuan Yuan 2 5 2 0 2 3 1 ] . [ 1 0 7 1 2 1 . 2 0 5 2 : r a "
[19.02.2025 04:13] Response: ```python
[]
```
[19.02.2025 04:13] Extracting affiliations from text.
[19.02.2025 04:13] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections Da Xiao 1 Qingye Meng 2 Shengping Li 2 Xingyuan Yuan 2 5 2 0 2 3 1 ] . [ 1 0 7 1 2 1 . 2 0 5 2 : r aWe propose MUltiway Dynamic Dense (MUDD) connections, simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.82.4 compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/ Caiyun-AI/MUDDFormer. 1. Introduction Residual connections (He et al., 2016), which help mitigate the vanish gradient problem, have become indispensable to training deep learning architectures from CNNs to Transformers, the latter becoming the de facto backbone for foundation models. Though being very simple and effective, residual connections still have limitations to be solved, especially with deep Transformers with dozens of layers made common by prevailing Transformer-based LLMs. On one hand, although theoretical (Merrill et al., 2022) and 1Beijing University of Posts and Telecommunications, Beijing, China 2ColorfulClouds Technology Co., Ltd., Beijing, China. Correspondence to: Da Xiao <xiaoda99@bupt.edu.cn>. Figure 1. Downstream average accuracy of Pythia and MUDDPythia with different sizes. experimental (Tay et al., 2021b) work have suggested that adding layers increases the expressive capacity and generalization performance of Transformers, it is observed that increasing depth beyond certain point yields diminishing returns (Petty et al., 2023). The common practice of using Pre-Norm to stabilize training leads to the issue of representation collapse (Liu et al., 2020b), where hidden features in deeper layers become highly similar, and for popular families of open-weight LLMs, large fraction of the layers can be removed with minimal degradation of performance (Gromov et al., 2024). On the other hand, mechanistic interpretability studies reveal that Transformers do in-context learning tasks by composing model components (attention heads and MLPs) across different layers to form circuits, (Elhage et al., 2020; Wang et al., 2023; Merullo et al., 2024; Ni et al., 2025), where layers communicate with each other by writing to and reading from different subspaces of the residual stream. The residual stream as the shared communication channel may be overloaded and become the bottleneck for very deep models, hindering the formation of sophisticated circuits spanning distant layers necessary for complex tasks. Dense connections (Huang et al., 2017) was proposed as promising solution to the above issues, by allowing subsequent layers to directly access outputs of all preceding layers (Figure 2 (a)). It has shown effectiveness for CNNs with DenseNet (Huang et al., 2017), and recently for Transformers with DenseFormer (Pagliardini et al., 2024). However, 1 Multiway Dynamic Dense Connections these dense connection approaches use static (either fixed (Huang et al., 2017) or learnable (Pagliardini et al., 2024)) dense connection weights that are shared across sequence positions and different input streams of Transformer block. As will be shown, this rigidity severely limits their expressive capacity in Transformers. In this work, we propose MUltiway Dynamic Dense (MUDD) connections, simple yet effective approach to address the shortcomings of residual connections. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input (the query, key, value or residual) of Transformer block. These weights are used by depth-wise aggregate modules to combine outputs from all preceding layers, creating multiple input streams for the current layer. MUDD connections can be seen as depth-wise multi-head attention (Vaswani et al., 2017) and the crosslayer communication bandwidth is expanded far beyond the restriction of the residual stream. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer models. We conduct extensive experiments focusing on language model pretraining to evaluate MUDDFormers effectiveness, efficiency and scalability. MUDDFormer significantly outperforms Transformer across various model architectures and scales (from 405M model on 7B tokens to 2.8B models on 300B tokens), achieving performance of Transformers trained with 1.82.4 compute. Notably, MUDDPythia2.8B matches Pythia-6.9B in pretraining perplexity and downstream tasks and even rivals Pythia-12B in five-shot in-context learning settings (Figure 1), while adding only 0.23% parameters and 0.4% computation. We also evaluate MUDD connections on vision Transformers and analyze the trained models to elucidate why MUDD connections work. 2. Method We begin with standard Transformer decoder with layers on input sequence = {x0, ..., xT }: X0 = Embedding(X) Xi = Bi(Xi1), [1, L] Transformer(X) = XL (1) where Bi() is the ith Transformer block1 composed of multi-head attention (MHA) module followed by fully connected feed-forward network (FFN), both wrapped with Pre-LayerNorm (LN) residual connections: XA = MHA(LN(X), LN(X), LN(X)) + B(X) = FFN(LN(XA)) + XA 1In this paper we use layer and block interchangeably. (2) In this architecture, the output Xi RT (D is model dim) of layer is used as input to layer + 1. With dense connections, the input to layer + 1 is an aggregation RT of outputs of all + 1 preceding layers, from the embedding till layer i: X:i := {X0, ..., Xi} (Figure 2 (a)). In this way, the cross-layer communication bandwidth is significantly increased compared to residual connections. To obt"
[19.02.2025 04:13] Mistral response. {"id": "9b781969b0cd463abf6ebb08c5912eac", "object": "chat.completion", "created": 1739938438, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Beijing University of Posts and Telecommunications, Beijing, China\", \"ColorfulClouds Technology Co., Ltd., Beijing, China\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1745, "total_tokens": 1781, "completion_tokens": 36}}
[19.02.2025 04:13] Response: ```python
["Beijing University of Posts and Telecommunications, Beijing, China", "ColorfulClouds Technology Co., Ltd., Beijing, China"]
```
[19.02.2025 04:13] Deleting PDF ./assets/pdf/2502.12170.pdf.
[19.02.2025 04:13] Success.
[19.02.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2502.12574.
[19.02.2025 04:13] Extra JSON file exists (./assets/json/2502.12574.json), skip PDF parsing.
[19.02.2025 04:13] Paper image links file exists (./assets/img_data/2502.12574.json), skip HTML parsing.
[19.02.2025 04:13] Success.
[19.02.2025 04:13] Downloading and parsing paper https://huggingface.co/papers/2502.10852.
[19.02.2025 04:13] Downloading paper 2502.10852 from http://arxiv.org/pdf/2502.10852v1...
[19.02.2025 04:14] Extracting affiliations from text.
[19.02.2025 04:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages Zeli Su1,2 Ziyin Zhang3 Guixian Xu1,2 Jianing Liu2 XU Han1,2 Ting Zhang1,2 Yushuang Dong1,2 1Key Laboratory of Ethnic Language Intelligent Analysis and Security Governance of MOE 2Minzu University of China 3Shanghai Jiao Tong University {rickamorty,guixian_xu,hanxu,jianing_liu,yushuangdong}@muc.edu.cn daenerystargaryen@sjtu.edu.cn tozhangting@126.com Corresponding author 5 2 0 2 5 1 ] . [ 1 2 5 8 0 1 . 2 0 5 2 : r a "
[19.02.2025 04:14] Response: ```python
[
    "Key Laboratory of Ethnic Language Intelligent Analysis and Security Governance of MOE",
    "Minzu University of China",
    "Shanghai Jiao Tong University"
]
```
[19.02.2025 04:14] Deleting PDF ./assets/pdf/2502.10852.pdf.
[19.02.2025 04:14] Success.
[19.02.2025 04:14] Enriching papers with extra data.
[19.02.2025 04:14] ********************************************************************************
[19.02.2025 04:14] Abstract 0. The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-con...
[19.02.2025 04:14] ********************************************************************************
[19.02.2025 04:14] Abstract 1. Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand ...
[19.02.2025 04:14] ********************************************************************************
[19.02.2025 04:14] Abstract 2. Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collec...
[19.02.2025 04:14] ********************************************************************************
[19.02.2025 04:14] Abstract 3. Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discre...
[19.02.2025 04:14] ********************************************************************************
[19.02.2025 04:14] Abstract 4. Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-c...
[19.02.2025 04:14] ********************************************************************************
[19.02.2025 04:14] Abstract 5. Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approach...
[19.02.2025 04:14] ********************************************************************************
[19.02.2025 04:14] Abstract 6. After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language r...
[19.02.2025 04:14] ********************************************************************************
[19.02.2025 04:14] Abstract 7. LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly re...
[19.02.2025 04:14] ********************************************************************************
[19.02.2025 04:14] Abstract 8. We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowled...
[19.02.2025 04:14] ********************************************************************************
[19.02.2025 04:14] Abstract 9. We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates conne...
[19.02.2025 04:14] ********************************************************************************
[19.02.2025 04:14] Abstract 10. Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloa...
[19.02.2025 04:14] ********************************************************************************
[19.02.2025 04:14] Abstract 11. While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models n...
[19.02.2025 04:14] Read previous papers.
[19.02.2025 04:14] Generating reviews via LLM API.
[19.02.2025 04:14] Using data from previous issue: {"categories": ["#architecture", "#video", "#multimodal"], "emoji": "🎥", "ru": {"title": "Phantom: баланс текста и изображения для создания согласованного видео", "desc": "Эта статья представляет Phantom - унифицированную систему генерации видео на основе текстовых и визуальных подсказок. Авторы пре
[19.02.2025 04:14] Using data from previous issue: {"categories": ["#3d", "#games", "#dataset", "#reasoning", "#robotics"], "emoji": "🤖", "ru": {"title": "Семантическая ориентация: новый подход к пространственному интеллекту роботов", "desc": "Статья представляет концепцию семантической ориентации для улучшения пространственного интеллекта роботов. 
[19.02.2025 04:14] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#training", "#dataset", "#interpretability"], "emoji": "🧩", "ru": {"title": "Разложение предпочтений для персонализированного обучения языковых моделей", "desc": "Статья представляет новый подход к извлечению разнообразных человеческих предпочтений из бинарных
[19.02.2025 04:14] Querying the API.
[19.02.2025 04:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discrete states. Existing continuous diffusion models for discrete data have limited performance compared to discrete approaches, and the unclear link between them restricts the development of diffusion models for discrete data. In this work, we propose a continuous diffusion model for language modeling that incorporates the geometry of the underlying categorical distribution. We establish a connection between the discrete diffusion and continuous flow on the statistical manifold, and building on the analogy, we introduce a simple design for the diffusion process that generalizes previous discrete diffusion models. We further propose a simulation-free training framework based on radial symmetry and a simple technique to address the high dimensionality of the manifold. Comprehensive experiments on language modeling benchmarks and other modalities show that our method outperforms existing discrete diffusion models and approaches the performance of autoregressive models. Codes available at https://github.com/harryjo97/RDLM{https://github.com/harryjo97/RDLM}.
[19.02.2025 04:14] Response: {
  "desc": "Статья представляет новый подход к моделированию естественного языка с использованием непрерывных диффузионных моделей. Авторы устанавливают связь между дискретной диффузией и непрерывным потоком на статистическом многообразии. Предложенный метод обобщает предыдущие дискретные диффузионные модели и вводит безсимуляционную схему обучения. Эксперименты показывают, что данный подход превосходит существующие дискретные диффузионные модели и приближается к производительности авторегрессионных моделей в задачах языкового моделирования.",

  "emoji": "🌊",

  "title": "Непрерывная диффузия на статистическом многообразии для языкового моделирования"
}
[19.02.2025 04:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discrete states. Existing continuous diffusion models for discrete data have limited performance compared to discrete approaches, and the unclear link between them restricts the development of diffusion models for discrete data. In this work, we propose a continuous diffusion model for language modeling that incorporates the geometry of the underlying categorical distribution. We establish a connection between the discrete diffusion and continuous flow on the statistical manifold, and building on the analogy, we introduce a simple design for the diffusion process that generalizes previous discrete diffusion models. We further propose a simulation-free training framework based on radial symmetry and a simple technique to address the high dimensionality of the manifold. Comprehensive experiments on language modeling benchmarks and other modalities show that our method outperforms existing discrete diffusion models and approaches the performance of autoregressive models. Codes available at https://github.com/harryjo97/RDLM{https://github.com/harryjo97/RDLM}."

[19.02.2025 04:14] Response: ```python
['DATASET', 'BENCHMARK', 'ARCHITECTURE', 'TRAINING']
```
[19.02.2025 04:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discrete states. Existing continuous diffusion models for discrete data have limited performance compared to discrete approaches, and the unclear link between them restricts the development of diffusion models for discrete data. In this work, we propose a continuous diffusion model for language modeling that incorporates the geometry of the underlying categorical distribution. We establish a connection between the discrete diffusion and continuous flow on the statistical manifold, and building on the analogy, we introduce a simple design for the diffusion process that generalizes previous discrete diffusion models. We further propose a simulation-free training framework based on radial symmetry and a simple technique to address the high dimensionality of the manifold. Comprehensive experiments on language modeling benchmarks and other modalities show that our method outperforms existing discrete diffusion models and approaches the performance of autoregressive models. Codes available at https://github.com/harryjo97/RDLM{https://github.com/harryjo97/RDLM}."

[19.02.2025 04:14] Response: ```python
['DIFFUSION', 'OPTIMIZATION']
```
[19.02.2025 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new continuous diffusion model designed for language modeling, which effectively handles discrete categorical data. The authors highlight the limitations of existing discrete diffusion models and propose a method that leverages the geometry of categorical distributions to enhance performance. By establishing a connection between discrete diffusion and continuous flow on a statistical manifold, they introduce a novel design that improves iterative refinement. Their experiments demonstrate that this approach not only surpasses traditional discrete models but also approaches the performance of autoregressive models.","title":"Revolutionizing Language Modeling with Continuous Diffusion"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new continuous diffusion model designed for language modeling, which effectively handles discrete categorical data. The authors highlight the limitations of existing discrete diffusion models and propose a method that leverages the geometry of categorical distributions to enhance performance. By establishing a connection between discrete diffusion and continuous flow on a statistical manifold, they introduce a novel design that improves iterative refinement. Their experiments demonstrate that this approach not only surpasses traditional discrete models but also approaches the performance of autoregressive models.', title='Revolutionizing Language Modeling with Continuous Diffusion'))
[19.02.2025 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"扩散模型作为一种新兴的替代自回归模型的方法，在建模离散分类数据方面展现了良好的前景。现有的连续扩散模型在处理离散数据时性能有限，且二者之间的联系不明确，限制了扩散模型的发展。本文提出了一种新的连续扩散模型，结合了基础分类分布的几何特性，并建立了离散扩散与连续流动之间的联系。通过全面的实验，我们的方法在语言建模基准测试中超越了现有的离散扩散模型，接近自回归模型的性能。","title":"连续扩散模型：提升离散数据建模的性能"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='扩散模型作为一种新兴的替代自回归模型的方法，在建模离散分类数据方面展现了良好的前景。现有的连续扩散模型在处理离散数据时性能有限，且二者之间的联系不明确，限制了扩散模型的发展。本文提出了一种新的连续扩散模型，结合了基础分类分布的几何特性，并建立了离散扩散与连续流动之间的联系。通过全面的实验，我们的方法在语言建模基准测试中超越了现有的离散扩散模型，接近自回归模型的性能。', title='连续扩散模型：提升离散数据建模的性能'))
[19.02.2025 04:14] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#transfer_learning", "#architecture", "#training", "#multimodal"], "emoji": "🚀", "ru": {"title": "mmMamba: эффективные мультимодальные модели с линейной сложностью", "desc": "Статья представляет mmMamba - фреймворк для разработки мультимодальных моде
[19.02.2025 04:14] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#architecture", "#training", "#reasoning", "#rl", "#multimodal"], "emoji": "📈", "ru": {"title": "Усиление торговых стратегий с помощью языковых моделей и обучения с подкреплением", "desc": "В статье представлена архитектура FLAG-Trader, объединяющая языкову
[19.02.2025 04:14] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#data", "#synthetic", "#dataset", "#multimodal"], "emoji": "🖼️", "ru": {"title": "RealSyn: Улучшение мультимодального обучения с помощью реальных и синтетических данных", "desc": "Статья представляет новый подход к обучению мультимодальных моделей, испо
[19.02.2025 04:14] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#inference", "#training", "#reasoning"], "emoji": "🧠", "ru": {"title": "Улучшение оценки ИИ через сравнение с мнением толпы", "desc": "Статья представляет новый метод оценки моделей машинного обучения под названием 'Crowd-based Comparative Evaluation'.
[19.02.2025 04:14] Querying the API.
[19.02.2025 04:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.
[19.02.2025 04:14] Response: {
  "desc": "HealthGPT - это мощная мультимодальная модель для медицинской визуальной обработки и генерации. Она использует новую технику адаптации H-LoRA и иерархический подход к визуальному восприятию. Модель обучена на специально созданном наборе данных VL-Health. HealthGPT демонстрирует исключительную производительность в медицинских визуальных задачах.",
  "emoji": "🏥",
  "title": "HealthGPT: Единая модель для понимания и генерации медицинских изображений"
}
[19.02.2025 04:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT."

[19.02.2025 04:14] Response: ```python
['DATASET', 'DATA', 'CV', 'HEALTHCARE', 'TRAINING']
```
[19.02.2025 04:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT."

[19.02.2025 04:14] Response: ```python
[]
```
[19.02.2025 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HealthGPT is a Medical Large Vision-Language Model that combines understanding and generating medical images and text in one system. It uses a unique method called heterogeneous low-rank adaptation (H-LoRA) to enhance pre-trained large language models with diverse medical knowledge. The model is trained on a specialized dataset named VL-Health, which focuses on medical comprehension and generation tasks. Results show that HealthGPT performs exceptionally well in various medical visual tasks, demonstrating its effectiveness and scalability.","title":"Revolutionizing Medical AI with HealthGPT"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HealthGPT is a Medical Large Vision-Language Model that combines understanding and generating medical images and text in one system. It uses a unique method called heterogeneous low-rank adaptation (H-LoRA) to enhance pre-trained large language models with diverse medical knowledge. The model is trained on a specialized dataset named VL-Health, which focuses on medical comprehension and generation tasks. Results show that HealthGPT performs exceptionally well in various medical visual tasks, demonstrating its effectiveness and scalability.', title='Revolutionizing Medical AI with HealthGPT'))
[19.02.2025 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"我们提出了HealthGPT，这是一种强大的医疗大型视觉语言模型（Med-LVLM），它将医疗视觉理解和生成能力整合在一个统一的自回归框架中。我们的自举理念是逐步将异构的理解和生成知识适应于预训练的大型语言模型（LLMs）。这通过一种新颖的异构低秩适应（H-LoRA）技术实现，并辅以定制的分层视觉感知方法和三阶段学习策略。实验结果表明，HealthGPT在医疗视觉统一任务中表现出色，具有良好的可扩展性。","title":"HealthGPT：医疗视觉语言模型的创新整合"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='我们提出了HealthGPT，这是一种强大的医疗大型视觉语言模型（Med-LVLM），它将医疗视觉理解和生成能力整合在一个统一的自回归框架中。我们的自举理念是逐步将异构的理解和生成知识适应于预训练的大型语言模型（LLMs）。这通过一种新颖的异构低秩适应（H-LoRA）技术实现，并辅以定制的分层视觉感知方法和三阶段学习策略。实验结果表明，HealthGPT在医疗视觉统一任务中表现出色，具有良好的可扩展性。', title='HealthGPT：医疗视觉语言模型的创新整合'))
[19.02.2025 04:14] Querying the API.
[19.02.2025 04:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer .
[19.02.2025 04:14] Response: {
  "desc": "Исследователи предлагают новый метод MUDD (MUltiway Dynamic Dense) для улучшения связей между слоями в архитектуре Transformer. MUDD генерирует веса связей динамически в зависимости от скрытых состояний на каждой позиции последовательности и для каждого входного потока блока Transformer. Эксперименты показывают, что MUDDFormer значительно превосходит стандартные Transformer-модели в задачах языкового моделирования, достигая производительности моделей, обученных с использованием в 1.8-2.4 раза больше вычислительных ресурсов. Примечательно, что MUDDPythia-2.8B сопоставима по эффективности с Pythia-6.9B и даже конкурирует с Pythia-12B в некоторых задачах, добавляя всего 0.23% параметров и 0.4% вычислений.",
  "emoji": "🔀",
  "title": "Динамические связи для эффективных трансформеров"
}
[19.02.2025 04:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer ."

[19.02.2025 04:14] Response: ```python
['ARCHITECTURE', 'TRAINING']
```
[19.02.2025 04:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer ."

[19.02.2025 04:14] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```
[19.02.2025 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces MUDD connections, which improve the flow of information between layers in Transformer models. Unlike traditional residual connections that use fixed weights, MUDD connections adaptively generate weights based on the hidden states of the input at each position. This dynamic approach allows for better integration of information from different input streams, enhancing the overall performance of the model. The proposed MUDDFormer architecture shows significant improvements in language modeling tasks, achieving results comparable to larger models while maintaining a smaller parameter count.","title":"Dynamic Connections for Enhanced Transformer Performance"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces MUDD connections, which improve the flow of information between layers in Transformer models. Unlike traditional residual connections that use fixed weights, MUDD connections adaptively generate weights based on the hidden states of the input at each position. This dynamic approach allows for better integration of information from different input streams, enhancing the overall performance of the model. The proposed MUDDFormer architecture shows significant improvements in language modeling tasks, achieving results comparable to larger models while maintaining a smaller parameter count.', title='Dynamic Connections for Enhanced Transformer Performance'))
[19.02.2025 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"我们提出了一种名为多向动态稠密连接（MUDD）的简单有效方法，旨在解决残差连接的局限性，并增强Transformer中跨层信息流动。与现有的静态共享连接权重的稠密连接方法不同，MUDD根据每个序列位置的隐藏状态动态生成连接权重，并针对Transformer块的每个解耦输入流（查询、键、值或残差）。MUDD连接可以无缝集成到任何Transformer架构中，形成MUDDFormer。大量实验表明，MUDDFormer在语言建模中显著超越了各种模型架构和规模的Transformer，表现出与训练时计算量为1.8X-2.4X的Transformer相当的性能。","title":"动态连接，提升Transformer性能！"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='我们提出了一种名为多向动态稠密连接（MUDD）的简单有效方法，旨在解决残差连接的局限性，并增强Transformer中跨层信息流动。与现有的静态共享连接权重的稠密连接方法不同，MUDD根据每个序列位置的隐藏状态动态生成连接权重，并针对Transformer块的每个解耦输入流（查询、键、值或残差）。MUDD连接可以无缝集成到任何Transformer架构中，形成MUDDFormer。大量实验表明，MUDDFormer在语言建模中显著超越了各种模型架构和规模的Transformer，表现出与训练时计算量为1.8X-2.4X的Transformer相当的性能。', title='动态连接，提升Transformer性能！'))
[19.02.2025 04:14] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#inference", "#training", "#long_context"], "emoji": "🧠", "ru": {"title": "HEADINFER: эффективное использование памяти для LLM с длинным контекстом", "desc": "HEADINFER - это новый метод для оптимизации памяти при работе с большими языковыми моделям
[19.02.2025 04:14] Querying the API.
[19.02.2025 04:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.
[19.02.2025 04:14] Response: {
  "desc": "Статья представляет новый подход к адаптации многоязычных энкодеров для генерации текста на языках с крайне ограниченными ресурсами. Авторы предлагают метод повторного использования весов между энкодером и декодером, что позволяет модели эффективно использовать семантическое пространство энкодера. Применяя этот подход к четырем китайским языкам меньшинств, исследователи разработали модель XLM-SWCM. Эксперименты показали превосходные результаты XLM-SWCM на различных задачах даже в сравнении с гораздо более крупными моделями.",
  "emoji": "🌍",
  "title": "Преодоление языкового барьера: эффективная генерация текста для малоресурсных языков"
}
[19.02.2025 04:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models."

[19.02.2025 04:14] Response: ```python
['MULTILINGUAL']
```
[19.02.2025 04:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models."

[19.02.2025 04:14] Response: ```python
["LOW_RESOURCE"]
```
[19.02.2025 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenges faced by multilingual language models in generating text for extremely low-resource languages. It introduces a new framework that adapts multilingual encoders for text generation by reusing weights between the encoder and decoder. This approach allows the model to utilize the semantic knowledge learned by the encoder, leading to better performance in low-resource settings. The authors demonstrate the effectiveness of their framework, named XLM-SWCM, on four Chinese minority languages, showing that it outperforms larger models in various tasks.","title":"Empowering Low-Resource Languages with Efficient Multilingual Models"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenges faced by multilingual language models in generating text for extremely low-resource languages. It introduces a new framework that adapts multilingual encoders for text generation by reusing weights between the encoder and decoder. This approach allows the model to utilize the semantic knowledge learned by the encoder, leading to better performance in low-resource settings. The authors demonstrate the effectiveness of their framework, named XLM-SWCM, on four Chinese minority languages, showing that it outperforms larger models in various tasks.', title='Empowering Low-Resource Languages with Efficient Multilingual Models'))
[19.02.2025 04:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"多语言模型如XLM-R在自然语言处理中的多语言能力有所提升，但在极低资源语言上表现仍然较差。现代大型语言模型如LLaMA和Qwen支持的语言数量远少于XLM-R，导致许多语言缺乏文本生成模型。为了解决这个问题，我们提出了一种新框架，将多语言编码器适应于极低资源语言的文本生成。通过重用编码器和解码器之间的权重，我们的框架能够利用编码器学习到的语义空间，从而在低资源语言中实现高效学习和有效泛化。","title":"为极低资源语言赋能的多语言文本生成框架"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='多语言模型如XLM-R在自然语言处理中的多语言能力有所提升，但在极低资源语言上表现仍然较差。现代大型语言模型如LLaMA和Qwen支持的语言数量远少于XLM-R，导致许多语言缺乏文本生成模型。为了解决这个问题，我们提出了一种新框架，将多语言编码器适应于极低资源语言的文本生成。通过重用编码器和解码器之间的权重，我们的框架能够利用编码器学习到的语义空间，从而在低资源语言中实现高效学习和有效泛化。', title='为极低资源语言赋能的多语言文本生成框架'))
[19.02.2025 04:14] Loading Chinese text from previous data.
[19.02.2025 04:14] Renaming data file.
[19.02.2025 04:14] Renaming previous data. hf_papers.json to ./d/2025-02-19.json
[19.02.2025 04:14] Saving new data file.
[19.02.2025 04:14] Generating page.
[19.02.2025 04:14] Renaming previous page.
[19.02.2025 04:14] Renaming previous data. index.html to ./d/2025-02-19.html
[19.02.2025 04:14] [Experimental] Generating Chinese page for reading.
[19.02.2025 04:14] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '人形机器人', 'pinyin': 'rén xíng jī qì rén', 'trans': 'humanoid robot'}, {'word': '自动', 'pinyin': 'zì dòng', 'trans': 'automatic'}, {'word': '跌倒', 'pinyin': 'diē dǎo', 'trans': 'fall down'}, {'word': '恢复', 'pinyin': 'huī fù', 'trans': 'recover'}, {'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'}, {'word': '设计', 'pinyin': 'shè jì', 'trans': 'design'}, {'word': '控制器', 'pinyin': 'kòng zhì qì', 'trans': 'controller'}, {'word': '站起来', 'pinyin': 'zhàn qǐ lái', 'trans': 'stand up'}, {'word': '姿态', 'pinyin': 'zī tài', 'trans': 'posture'}, {'word': '地形', 'pinyin': 'dì xíng', 'trans': 'terrain'}, {'word': '复杂', 'pinyin': 'fù zá', 'trans': 'complex'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '学习', 'pinyin': 'xué xí', 'trans': 'learn'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '阶段', 'pinyin': 'jiē duàn', 'trans': 'stage'}, {'word': '路径', 'pinyin': 'lù jìng', 'trans': 'path'}, {'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimize'}, {'word': '动作', 'pinyin': 'dòng zuò', 'trans': 'action'}, {'word': '平稳', 'pinyin': 'píng wěn', 'trans': 'stable'}, {'word': '可靠', 'pinyin': 'kě kào', 'trans': 'reliable'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '地面', 'pinyin': 'dì miàn', 'trans': 'ground'}, {'word': '成功', 'pinyin': 'chéng gōng', 'trans': 'success'}, {'word': '首次', 'pinyin': 'shǒu cì', 'trans': 'first time'}, {'word': '真实', 'pinyin': 'zhēn shí', 'trans': 'real'}, {'word': '环境', 'pinyin': 'huán jìng', 'trans': 'environment'}, {'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'apply'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}]
[19.02.2025 04:14] Renaming previous Chinese page.
[19.02.2025 04:14] Renaming previous data. zh.html to ./d/2025-02-18_zh_reading_task.html
[19.02.2025 04:14] Writing Chinese reading task.
[19.02.2025 04:14] Writing result.
[19.02.2025 04:14] Renaming log file.
[19.02.2025 04:14] Renaming previous data. log.txt to ./logs/2025-02-19_last_log.txt
