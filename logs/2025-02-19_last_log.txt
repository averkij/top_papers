[19.02.2025 08:17] Read previous papers.
[19.02.2025 08:17] Generating top page (month).
[19.02.2025 08:17] Writing top page (month).
[19.02.2025 09:11] Read previous papers.
[19.02.2025 09:11] Get feed.
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12900
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11564
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11079
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12464
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13131
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13143
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11433
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13145
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09245
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12513
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13130
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12859
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12170
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12215
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12501
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11271
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09838
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12574
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10708
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12669
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13142
[19.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10852
[19.02.2025 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.02.2025 09:11] No deleted papers detected.
[19.02.2025 09:11] Downloading and parsing papers (pdf, html). Total: 22.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.12900.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.12900.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.12900.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.11564.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.11564.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.11564.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.11079.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.11079.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.11079.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.12464.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.12464.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.12464.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.13131.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.13131.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.13131.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.13143.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.13143.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.13143.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.11433.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.11433.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.11433.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.13145.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.13145.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.13145.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.09245.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.09245.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.09245.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.12513.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.12513.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.12513.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.13130.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.13130.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.13130.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.12859.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.12859.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.12859.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.12170.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.12170.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.12170.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.12215.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.12215.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.12215.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.12501.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.12501.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.12501.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.11271.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.11271.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.11271.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.09838.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.09838.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.09838.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.12574.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.12574.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.12574.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.10708.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.10708.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.10708.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.12669.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.12669.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.12669.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.13142.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.13142.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.13142.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.10852.
[19.02.2025 09:11] Extra JSON file exists (./assets/json/2502.10852.json), skip PDF parsing.
[19.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.10852.json), skip HTML parsing.
[19.02.2025 09:11] Success.
[19.02.2025 09:11] Enriching papers with extra data.
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 0. Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 1. Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discre...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 2. The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-con...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 3. Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, bu...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 4. Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collec...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 5. Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand ...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 6. Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approach...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 7. Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-c...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 8. In contrast to RNNs, which compress previous tokens into a single hidden state, Transformers can attend to all previous tokens directly. However, standard Transformers only use representations from the immediately preceding layer. In this paper, we show that this design choice causes representation ...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 9. After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language r...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 10. We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped wit...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 11. While Large Language Models (LLMs) adapt well to downstream tasks after fine-tuning, this adaptability often compromises prompt robustness, as even minor prompt variations can significantly degrade performance. To address this, we propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective app...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 12. We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates conne...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 13. The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these mode...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 14. LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly re...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 15. Solving complex reasoning tasks may involve visual understanding, domain knowledge retrieval, numerical calculation, and multi-step reasoning. Existing methods augment large language models (LLMs) with external tools but are restricted to specialized domains, limited tool types, or require additiona...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 16. We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowled...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 17. Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloa...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 18. Large Language Models (LLMs) have demonstrated remarkable success in various tasks such as natural language understanding, text summarization, and machine translation. However, their general-purpose nature often limits their effectiveness in domain-specific applications that require specialized know...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 19. The rapid advancement of perovskite solar cells (PSCs) has led to an exponential growth in research publications, creating an urgent need for efficient knowledge management and reasoning systems in this domain. We present a comprehensive knowledge-enhanced system for PSCs that integrates three key c...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 20. Foundation models pre-trained on massive unlabeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by ei...
[19.02.2025 09:11] ********************************************************************************
[19.02.2025 09:11] Abstract 21. While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models n...
[19.02.2025 09:11] Read previous papers.
[19.02.2025 09:11] Generating reviews via LLM API.
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#training", "#audio", "#transfer_learning", "#open_source", "#optimization", "#data", "#architecture"], "emoji": "ğŸ™ï¸", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Soundwave - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€ĞµÑ‡Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°Ğµ
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#training", "#benchmark", "#optimization", "#dataset", "#diffusion", "#architecture"], "emoji": "ğŸŒŠ", "ru": {"title": "ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ñ Ğ¸ÑĞ¿
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#architecture", "#video", "#multimodal"], "emoji": "ğŸ¥", "ru": {"title": "Phantom: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Phantom - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğµ
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#security", "#benchmark", "#training", "#inference", "#optimization"], "emoji": "ğŸ›¡ï¸", "ru": {"title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SafeRoute Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#training", "#dataset", "#interpretability"], "emoji": "ğŸ§©", "ru": {"title": "Ğ Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ…
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#3d", "#games", "#dataset", "#reasoning", "#robotics"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². 
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#architecture", "#training", "#reasoning", "#rl", "#multimodal"], "emoji": "ğŸ“ˆ", "ru": {"title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° FLAG-Trader, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñƒ
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#transfer_learning", "#architecture", "#training", "#multimodal"], "emoji": "ğŸš€", "ru": {"title": "mmMamba: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ mmMamba - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğµ
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "ğŸ§ ", "ru": {"title": "LIMe: Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Layer-Integrated Memory (LIMe). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#data", "#synthetic", "#dataset", "#multimodal"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "RealSyn: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#cv", "#robotics", "#agi", "#multimodal", "#agents", "#open_source"], "emoji": "ğŸ¤–", "ru": {"title": "Magma: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Magma - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ°Ğ³
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#training", "#dataset", "#optimization", "#synthetic", "#inference"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ£ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ P
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#optimization", "#training", "#open_source", "#architecture"], "emoji": "ğŸ”€", "ru": {"title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MUDD (MUltiway Dynamic Dense) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Transformer. M
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#training", "#optimization"], "emoji": "ğŸ”", "ru": {"title": "ĞšĞ¾Ñ€Ğ¾Ñ‡Ğµ - Ğ»ÑƒÑ‡ÑˆĞµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#inference", "#training", "#reasoning"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ¼Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»Ğ¿Ñ‹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Crowd-based Comparative Evaluation'.
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#multimodal", "#open_source", "#training"], "emoji": "ğŸ™", "ru": {"title": "OctoTools: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OctoTools - ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#data", "#training", "#cv", "#dataset", "#healthcare"], "emoji": "ğŸ¥", "ru": {"title": "HealthGPT: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "desc": "HealthGPT - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#inference", "#training", "#long_context"], "emoji": "ğŸ§ ", "ru": {"title": "HEADINFER: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ LLM Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼", "desc": "HEADINFER - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#survey", "#dataset", "#healthcare", "#multilingual", "#benchmark", "#translation", "#open_source"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ LLM Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸: ĞºĞ»ÑÑ‡ Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#data", "#science", "#architecture", "#multimodal", "#training", "#graphs"], "emoji": "â˜€ï¸", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ¿ĞµÑ€Ğ¾Ğ²ÑĞºĞ¸Ñ‚Ğ½Ñ‹Ñ… ÑĞ¾Ğ»Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿ĞµÑ€Ğ¾Ğ²ÑĞºĞ¸
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#training", "#dataset", "#transfer_learning", "#3d", "#robotics"], "emoji": "ğŸ¤–", "ru": {"title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ARM4R - Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ 4D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°
[19.02.2025 09:11] Using data from previous issue: {"categories": ["#multilingual", "#low_resource"], "emoji": "ğŸŒ", "ru": {"title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ°: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ°Ñ… Ñ ĞºÑ€Ğ°Ğ¹Ğ½Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ 
[19.02.2025 09:11] Trying to get texts in Chinese.
[19.02.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency. We propose Soundwave, which utilizes an efficient training strategy and a novel architecture to address these issues. Results show that Soundwave outperforms the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks, using only one-fiftieth of the training data. Further analysis shows that Soundwave still retains its intelligence during conversation. The project is available at https://github.com/FreedomIntelligence/Soundwave.
[19.02.2025 09:11] Mistral response. {"id": "8fd259b163984102b9396b5e8d7a1240", "object": "chat.completion", "created": 1739956269, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u73b0\u6709\u7aef\u5230\u7aef\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u5e38\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u6570\u636e\u9ad8\u6548\u8bad\u7ec3\u5c1a\u672a\u6df1\u5165\u63a2\u8ba8\u3002\u6587\u7ae0\u805a\u7126\u4e8e\u8bed\u97f3\u548c\u6587\u672c\u4e4b\u95f4\u7684\u4e24\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u8868\u793a\u7a7a\u95f4\u5dee\u8ddd\u548c\u5e8f\u5217\u957f\u5ea6\u4e0d\u4e00\u81f4\u3002\u4f5c\u8005\u63d0\u51fa\u4e86Soundwave\uff0c\u5b83\u4f7f\u7528\u9ad8\u6548\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u65b0\u9896\u7684\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u7ed3\u679c\u663e\u793a\uff0cSoundwave\u5728\u4f7f\u7528\u4e94\u5341\u5206\u4e4b\u4e00\u7684\u8bad\u7ec3\u6570\u636e\u65f6\uff0c\u5728\u8bed\u97f3\u7ffb\u8bd1\u548cAIR-Bench\u8bed\u97f3\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5148\u8fdb\u7684Qwen2-Audio\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u8868\u660e\uff0cSoundwave\u5728\u5bf9\u8bdd\u4e2d\u4ecd\u4fdd\u6301\u5176\u667a\u80fd\u3002\u9879\u76ee\u7684\u4ee3\u7801\u53ef\u5728https://github.com/FreedomIntelligence/Soundwave\u627e\u5230\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 184, "total_tokens": 403, "completion_tokens": 219}}
[19.02.2025 09:11] Response: è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç°æœ‰ç«¯åˆ°ç«¯è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸ä¾èµ–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè€Œæ•°æ®é«˜æ•ˆè®­ç»ƒå°šæœªæ·±å…¥æ¢è®¨ã€‚æ–‡ç« èšç„¦äºè¯­éŸ³å’Œæ–‡æœ¬ä¹‹é—´çš„ä¸¤ä¸ªåŸºæœ¬é—®é¢˜ï¼šè¡¨ç¤ºç©ºé—´å·®è·å’Œåºåˆ—é•¿åº¦ä¸ä¸€è‡´ã€‚ä½œè€…æå‡ºäº†Soundwaveï¼Œå®ƒä½¿ç”¨é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥å’Œæ–°é¢–çš„æ¶æ„æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ç»“æœæ˜¾ç¤ºï¼ŒSoundwaveåœ¨ä½¿ç”¨äº”ååˆ†ä¹‹ä¸€çš„è®­ç»ƒæ•°æ®æ—¶ï¼Œåœ¨è¯­éŸ³ç¿»è¯‘å’ŒAIR-Benchè¯­éŸ³ä»»åŠ¡ä¸­ä¼˜äºå…ˆè¿›çš„Qwen2-Audioã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒSoundwaveåœ¨å¯¹è¯ä¸­ä»ä¿æŒå…¶æ™ºèƒ½ã€‚é¡¹ç›®çš„ä»£ç å¯åœ¨https://github.com/FreedomIntelligence/Soundwaveæ‰¾åˆ°ã€‚
[19.02.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç°æœ‰ç«¯åˆ°ç«¯è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸ä¾èµ–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè€Œæ•°æ®é«˜æ•ˆè®­ç»ƒå°šæœªæ·±å…¥æ¢è®¨ã€‚æ–‡ç« èšç„¦äºè¯­éŸ³å’Œæ–‡æœ¬ä¹‹é—´çš„ä¸¤ä¸ªåŸºæœ¬é—®é¢˜ï¼šè¡¨ç¤ºç©ºé—´å·®è·å’Œåºåˆ—é•¿åº¦ä¸ä¸€è‡´ã€‚ä½œè€…æå‡ºäº†Soundwaveï¼Œå®ƒä½¿ç”¨é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥å’Œæ–°é¢–çš„æ¶æ„æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ç»“æœæ˜¾ç¤ºï¼ŒSoundwaveåœ¨ä½¿ç”¨äº”ååˆ†ä¹‹ä¸€çš„è®­ç»ƒæ•°æ®æ—¶ï¼Œåœ¨è¯­éŸ³ç¿»è¯‘å’ŒAIR-Benchè¯­éŸ³ä»»åŠ¡ä¸­ä¼˜äºå…ˆè¿›çš„Qwen2-Audioã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒSoundwaveåœ¨å¯¹è¯ä¸­ä»ä¿æŒå…¶æ™ºèƒ½ã€‚é¡¹ç›®çš„ä»£ç å¯åœ¨https://github.com/FreedomIntelligence/Soundwaveæ‰¾åˆ°ã€‚
[19.02.2025 09:11] Mistral response. {"id": "a10de53f25bb471ea6d4cd6ca427d33c", "object": "chat.completion", "created": 1739956274, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u73b0\u6709\u7aef\u5230\u7aef\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u5e38\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u6570\u636e\u9ad8\u6548\u8bad\u7ec3\u5c1a\u672a\u6df1\u5165\u63a2\u8ba8\u3002\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng t\u01ceol\u00f9n le xi\u00e0ny\u01d2u du\u0101ndu\u0101n y\u01d4y\u012bn d\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng (LLMs) t\u014dngch\u00e1ng y\u012bl\u00e0i d\u00e0 gu\u012bm\u00f3 bi\u0101ozh\u00f9 sh\u00f9j\u00f9 j\u00ecnx\u00edng x\u00f9nli\u00e0n, \u00e9r sh\u00f9j\u00f9 g\u0101oxi\u00e0o x\u00f9nli\u00e0n sh\u00e0ngw\u00e8i sh\u0113nr\u00f9 t\u00e0nt\u00e0o.\n\n\u6587\u7ae0\u805a\u7126\u4e8e\u8bed\u97f3\u548c\u6587\u672c\u4e4b\u95f4\u7684\u4e24\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u8868\u793a\u7a7a\u95f4\u5dee\u8ddd\u548c\u5e8f\u5217\u957f\u5ea6\u4e0d\u4e00\u81f4\u3002\nW\u00e9nzh\u0101ng j\u00f9ji\u0101o y\u00fa y\u01d4y\u012bn h\u00e9 w\u00e9nb\u011bn zh\u012bji\u0101n de li\u01ceng g\u00e8 j\u012bb\u011bn w\u00e8nt\u00ed: bi\u01ceosh\u00ec k\u014dngji\u0101n ch\u0101j\u00f9 h\u00e9 x\u00f9li\u00e8 ch\u00e1ngd\u00f9 b\u00f9 y\u012bzh\u00ec.\n\n\u4f5c\u8005\u63d0\u51fa\u4e86Soundwave\uff0c\u5b83\u4f7f\u7528\u9ad8\u6548\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u65b0\u9896\u7684\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\nZu\u00f2zh\u011b t\u00edch\u016b le Soundwave, t\u0101 sh\u01d0y\u00f2ng g\u0101oxi\u00e0o de x\u00f9nli\u00e0n c\u00e8l\u00fc\u00e8 h\u00e9 x\u012bny\u01d0ng de ji\u00e0g\u00f2u l\u00e1i ji\u011bju\u00e9 zh\u00e8xi\u0113 w\u00e8nt\u00ed.\n\n\u7ed3\u679c\u663e\u793a\uff0cSoundwave\u5728\u4f7f\u7528\u4e94\u5341\u5206\u4e4b\u4e00\u7684\u8bad\u7ec3\u6570\u636e\u65f6\uff0c\u5728\u8bed\u97f3\u7ffb\u8bd1\u548cAIR-Bench\u8bed\u97f3\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5148\u8fdb\u7684Qwen2-Audio\u3002\nJi\u00e9g\u01d4o xi\u01censh\u00ec, Soundwave z\u00e0i sh\u01d0y\u00f2ng w\u01d4sh\u00ed f\u0113n zh\u012b y\u012b de x\u00f9nli\u00e0n sh\u00f9j\u00f9 sh\u00ed, z\u00e0i y\u01d4y\u012bn f\u0101ny\u00ec h\u00e9 AIR-Bench y\u01d4y\u012bn r\u00e8nw\u00f9 zh\u014dng y\u014duy\u00fa xi\u0101nj\u00ecn de Qwen2-Audio.\n\n\u8fdb\u4e00\u6b65\u5206\u6790\u8868\u660e\uff0cSoundwave\u5728\u5bf9\u8bdd\u4e2d\u4ecd\u4fdd\u6301\u5176\u667a\u80fd\u3002\nJ\u00ecn y\u012bb\u00f9 f\u0113nxi\u01cen bi\u01ceom\u00edng, Soundwave z\u00e0i du\u00echu\u00e0 zh\u014dng r\u00e9ng b\u01ceoch\u00ed q\u00ed zh\u00ecn\u00e9ng.\n\n\u9879\u76ee\u7684\u4ee3\u7801\u53ef\u5728https://github.com/FreedomIntelligence/Soundwave\u627e\u5230\u3002\nXi\u00e0ngm\u00f9 de d\u00e0im\u01ce k\u011b z\u00e0i https://github.com/FreedomIntelligence/Soundwave zh\u01ceo d\u00e0o."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 234, "total_tokens": 918, "completion_tokens": 684}}
[19.02.2025 09:11] Response: è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç°æœ‰ç«¯åˆ°ç«¯è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸ä¾èµ–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè€Œæ•°æ®é«˜æ•ˆè®­ç»ƒå°šæœªæ·±å…¥æ¢è®¨ã€‚
ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le xiÃ nyÇ’u duÄnduÄn yÇ”yÄ«n dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) tÅngchÃ¡ng yÄ«lÃ i dÃ  guÄ«mÃ³ biÄozhÃ¹ shÃ¹jÃ¹ jÃ¬nxÃ­ng xÃ¹nliÃ n, Ã©r shÃ¹jÃ¹ gÄoxiÃ o xÃ¹nliÃ n shÃ ngwÃ¨i shÄ“nrÃ¹ tÃ ntÃ o.

æ–‡ç« èšç„¦äºè¯­éŸ³å’Œæ–‡æœ¬ä¹‹é—´çš„ä¸¤ä¸ªåŸºæœ¬é—®é¢˜ï¼šè¡¨ç¤ºç©ºé—´å·®è·å’Œåºåˆ—é•¿åº¦ä¸ä¸€è‡´ã€‚
WÃ©nzhÄng jÃ¹jiÄo yÃº yÇ”yÄ«n hÃ© wÃ©nbÄ›n zhÄ«jiÄn de liÇng gÃ¨ jÄ«bÄ›n wÃ¨ntÃ­: biÇoshÃ¬ kÅngjiÄn chÄjÃ¹ hÃ© xÃ¹liÃ¨ chÃ¡ngdÃ¹ bÃ¹ yÄ«zhÃ¬.

ä½œè€…æå‡ºäº†Soundwaveï¼Œå®ƒä½¿ç”¨é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥å’Œæ–°é¢–çš„æ¶æ„æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚
ZuÃ²zhÄ› tÃ­chÅ« le Soundwave, tÄ shÇyÃ²ng gÄoxiÃ o de xÃ¹nliÃ n cÃ¨lÃ¼Ã¨ hÃ© xÄ«nyÇng de jiÃ gÃ²u lÃ¡i jiÄ›juÃ© zhÃ¨xiÄ“ wÃ¨ntÃ­.

ç»“æœæ˜¾ç¤ºï¼ŒSoundwaveåœ¨ä½¿ç”¨äº”ååˆ†ä¹‹ä¸€çš„è®­ç»ƒæ•°æ®æ—¶ï¼Œåœ¨è¯­éŸ³ç¿»è¯‘å’ŒAIR-Benchè¯­éŸ³ä»»åŠ¡ä¸­ä¼˜äºå…ˆè¿›çš„Qwen2-Audioã€‚
JiÃ©gÇ”o xiÇnshÃ¬, Soundwave zÃ i shÇyÃ²ng wÇ”shÃ­ fÄ“n zhÄ« yÄ« de xÃ¹nliÃ n shÃ¹jÃ¹ shÃ­, zÃ i yÇ”yÄ«n fÄnyÃ¬ hÃ© AIR-Bench yÇ”yÄ«n rÃ¨nwÃ¹ zhÅng yÅuyÃº xiÄnjÃ¬n de Qwen2-Audio.

è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒSoundwaveåœ¨å¯¹è¯ä¸­ä»ä¿æŒå…¶æ™ºèƒ½ã€‚
JÃ¬n yÄ«bÃ¹ fÄ“nxiÇn biÇomÃ­ng, Soundwave zÃ i duÃ¬huÃ  zhÅng rÃ©ng bÇochÃ­ qÃ­ zhÃ¬nÃ©ng.

é¡¹ç›®çš„ä»£ç å¯åœ¨https://github.com/FreedomIntelligence/Soundwaveæ‰¾åˆ°ã€‚
XiÃ ngmÃ¹ de dÃ imÇ kÄ› zÃ i https://github.com/FreedomIntelligence/Soundwave zhÇo dÃ o.
[19.02.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç°æœ‰ç«¯åˆ°ç«¯è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸ä¾èµ–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè€Œæ•°æ®é«˜æ•ˆè®­ç»ƒå°šæœªæ·±å…¥æ¢è®¨ã€‚æ–‡ç« èšç„¦äºè¯­éŸ³å’Œæ–‡æœ¬ä¹‹é—´çš„ä¸¤ä¸ªåŸºæœ¬é—®é¢˜ï¼šè¡¨ç¤ºç©ºé—´å·®è·å’Œåºåˆ—é•¿åº¦ä¸ä¸€è‡´ã€‚ä½œè€…æå‡ºäº†Soundwaveï¼Œå®ƒä½¿ç”¨é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥å’Œæ–°é¢–çš„æ¶æ„æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ç»“æœæ˜¾ç¤ºï¼ŒSoundwaveåœ¨ä½¿ç”¨äº”ååˆ†ä¹‹ä¸€çš„è®­ç»ƒæ•°æ®æ—¶ï¼Œåœ¨è¯­éŸ³ç¿»è¯‘å’ŒAIR-Benchè¯­éŸ³ä»»åŠ¡ä¸­ä¼˜äºå…ˆè¿›çš„Qwen2-Audioã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒSoundwaveåœ¨å¯¹è¯ä¸­ä»ä¿æŒå…¶æ™ºèƒ½ã€‚é¡¹ç›®çš„ä»£ç å¯åœ¨https://github.com/FreedomIntelligence/Soundwaveæ‰¾åˆ°ã€‚
[19.02.2025 09:11] Mistral response. {"id": "6c43c8474b0144cb9309fdb3e45eb1f7", "object": "chat.completion", "created": 1739956289, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u8ba8\u8bba\", \"pinyin\": \"t\u01ceo l\u00f9n\", \"trans\": \"discuss\"},\n    {\"word\": \"\u7aef\u5230\u7aef\", \"pinyin\": \"du\u0101n d\u00e0o du\u0101n\", \"trans\": \"end-to-end\"},\n    {\"word\": \"\u8bed\u97f3\", \"pinyin\": \"y\u01d4 y\u012bn\", \"trans\": \"speech\"},\n    {\"word\": \"\u5927\u8bed\u8a00\u6a21\u578b\", \"pinyin\": \"d\u00e0 y\u01d4 y\u00e1n m\u00f3 x\u00edng\", \"trans\": \"large language model\"},\n    {\"word\": \"\u4f9d\u8d56\", \"pinyin\": \"y\u012b l\u00e0i\", \"trans\": \"rely on\"},\n    {\"word\": \"\u5927\u89c4\u6a21\", \"pinyin\": \"d\u00e0 gu\u012b m\u00f3\", \"trans\": \"large-scale\"},\n    {\"word\": \"\u6807\u6ce8\", \"pinyin\": \"bi\u0101o zh\u00f9\", \"trans\": \"annotate\"},\n    {\"word\": \"\u6570\u636e\", \"pinyin\": \"sh\u00f9 j\u00f9\", \"trans\": \"data\"},\n    {\"word\": \"\u8fdb\u884c\", \"pinyin\": \"j\u00ecn x\u00edng\", \"trans\": \"carry out\"},\n    {\"word\": \"\u8bad\u7ec3\", \"pinyin\": \"x\u00f9n li\u00e0n\", \"trans\": \"train\"},\n    {\"word\": \"\u9ad8\u6548\", \"pinyin\": \"g\u0101o xi\u00e0o\", \"trans\": \"efficient\"},\n    {\"word\": \"\u6df1\u5165\", \"pinyin\": \"sh\u0113n r\u00f9\", \"trans\": \"in-depth\"},\n    {\"word\": \"\u63a2\u8ba8\", \"pinyin\": \"t\u00e0n t\u01ceo\", \"trans\": \"explore\"},\n    {\"word\": \"\u805a\u7126\", \"pinyin\": \"j\u00f9 ji\u0101o\", \"trans\": \"focus on\"},\n    {\"word\": \"\u57fa\u672c\", \"pinyin\": \"j\u012b b\u011bn\", \"trans\": \"basic\"},\n    {\"word\": \"\u95ee\u9898\", \"pinyin\": \"w\u00e8n t\u00ed\", \"trans\": \"problem\"},\n    {\"word\": \"\u8868\u793a\", \"pinyin\": \"bi\u01ceo sh\u00ec\", \"trans\": \"represent\"},\n    {\"word\": \"\u7a7a\u95f4\", \"pinyin\": \"k\u014dng ji\u0101n\", \"trans\": \"space\"},\n    {\"word\": \"\u5dee\u8ddd\", \"pinyin\": \"ch\u0101 j\u00f9\", \"trans\": \"gap\"},\n    {\"word\": \"\u5e8f\u5217\", \"pinyin\": \"x\u00f9 li\u00e8\", \"trans\": \"sequence\"},\n    {\"word\": \"\u957f\u5ea6\", \"pinyin\": \"ch\u00e1ng d\u00f9\", \"trans\": \"length\"},\n    {\"word\": \"\u4e0d\u4e00\u81f4\", \"pinyin\": \"b\u00f9 y\u012b zh\u00ec\", \"trans\": \"inconsistent\"},\n    {\"word\": \"\u63d0\u51fa\", \"pinyin\": \"t\u00ed ch\u016b\", \"trans\": \"propose\"},\n    {\"word\": \"\u7b56\u7565\", \"pinyin\": \"c\u00e8 l\u00fc\u00e8\", \"trans\": \"strategy\"},\n    {\"word\": \"\u67b6\u6784\", \"pinyin\": \"ji\u00e0 g\u00f2u\", \"trans\": \"architecture\"},\n    {\"word\": \"\u89e3\u51b3\", \"pinyin\": \"ji\u011b ju\u00e9\", \"trans\": \"solve\"},\n    {\"word\": \"\u7ed3\u679c\", \"pinyin\": \"ji\u00e9 gu\u01d2\", \"trans\": \"result\"},\n    {\"word\": \"\u663e\u793a\", \"pinyin\": \"xi\u01cen sh\u00ec\", \"trans\": \"show\"},\n    {\"word\": \"\u4f18\u4e8e\", \"pinyin\": \"y\u014du y\u00fa\", \"trans\": \"superior to\"},\n    {\"word\": \"\u5148\u8fdb\", \"pinyin\": \"xi\u0101n j\u00ecn\", \"trans\": \"advanced\"},\n    {\"word\": \"\u5206\u6790\", \"pinyin\": \"f\u0113n x\u012b\", \"trans\": \"analyze\"},\n    {\"word\": \"\u8868\u660e\", \"pinyin\": \"bi\u01ceo m\u00edng\", \"trans\": \"indicate\"},\n    {\"word\": \"\u4fdd\u6301\", \"pinyin\": \"b\u01ceo ch\u00ed\", \"trans\": \"maintain\"},\n    {\"word\": \"\u667a\u80fd\", \"pinyin\": \"zh\u00ec n\u00e9ng\", \"trans\": \"intelligence\"},\n    {\"word\": \"\u9879\u76ee\", \"pinyin\": \"xi\u00e0ng m\u00f9\", \"trans\": \"project\"},\n    {\"word\": \"\u4ee3\u7801\", \"pinyin\": \"d\u00e0i m\u01ce\", \"trans\": \"code\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 266, "total_tokens": 1302, "completion_tokens": 1036}}
[19.02.2025 09:11] Response: [
    {"word": "è®¨è®º", "pinyin": "tÇo lÃ¹n", "trans": "discuss"},
    {"word": "ç«¯åˆ°ç«¯", "pinyin": "duÄn dÃ o duÄn", "trans": "end-to-end"},
    {"word": "è¯­éŸ³", "pinyin": "yÇ” yÄ«n", "trans": "speech"},
    {"word": "å¤§è¯­è¨€æ¨¡å‹", "pinyin": "dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng", "trans": "large language model"},
    {"word": "ä¾èµ–", "pinyin": "yÄ« lÃ i", "trans": "rely on"},
    {"word": "å¤§è§„æ¨¡", "pinyin": "dÃ  guÄ« mÃ³", "trans": "large-scale"},
    {"word": "æ ‡æ³¨", "pinyin": "biÄo zhÃ¹", "trans": "annotate"},
    {"word": "æ•°æ®", "pinyin": "shÃ¹ jÃ¹", "trans": "data"},
    {"word": "è¿›è¡Œ", "pinyin": "jÃ¬n xÃ­ng", "trans": "carry out"},
    {"word": "è®­ç»ƒ", "pinyin": "xÃ¹n liÃ n", "trans": "train"},
    {"word": "é«˜æ•ˆ", "pinyin": "gÄo xiÃ o", "trans": "efficient"},
    {"word": "æ·±å…¥", "pinyin": "shÄ“n rÃ¹", "trans": "in-depth"},
    {"word": "æ¢è®¨", "pinyin": "tÃ n tÇo", "trans": "explore"},
    {"word": "èšç„¦", "pinyin": "jÃ¹ jiÄo", "trans": "focus on"},
    {"word": "åŸºæœ¬", "pinyin": "jÄ« bÄ›n", "trans": "basic"},
    {"word": "é—®é¢˜", "pinyin": "wÃ¨n tÃ­", "trans": "problem"},
    {"word": "è¡¨ç¤º", "pinyin": "biÇo shÃ¬", "trans": "represent"},
    {"word": "ç©ºé—´", "pinyin": "kÅng jiÄn", "trans": "space"},
    {"word": "å·®è·", "pinyin": "chÄ jÃ¹", "trans": "gap"},
    {"word": "åºåˆ—", "pinyin": "xÃ¹ liÃ¨", "trans": "sequence"},
    {"word": "é•¿åº¦", "pinyin": "chÃ¡ng dÃ¹", "trans": "length"},
    {"word": "ä¸ä¸€è‡´", "pinyin": "bÃ¹ yÄ« zhÃ¬", "trans": "inconsistent"},
    {"word": "æå‡º", "pinyin": "tÃ­ chÅ«", "trans": "propose"},
    {"word": "ç­–ç•¥", "pinyin": "cÃ¨ lÃ¼Ã¨", "trans": "strategy"},
    {"word": "æ¶æ„", "pinyin": "jiÃ  gÃ²u", "trans": "architecture"},
    {"word": "è§£å†³", "pinyin": "jiÄ› juÃ©", "trans": "solve"},
    {"word": "ç»“æœ", "pinyin": "jiÃ© guÇ’", "trans": "result"},
    {"word": "æ˜¾ç¤º", "pinyin": "xiÇn shÃ¬", "trans": "show"},
    {"word": "ä¼˜äº", "pinyin": "yÅu yÃº", "trans": "superior to"},
    {"word": "å…ˆè¿›", "pinyin": "xiÄn jÃ¬n", "trans": "advanced"},
    {"word": "åˆ†æ", "pinyin": "fÄ“n xÄ«", "trans": "analyze"},
    {"word": "è¡¨æ˜", "pinyin": "biÇo mÃ­ng", "trans": "indicate"},
    {"word": "ä¿æŒ", "pinyin": "bÇo chÃ­", "trans": "maintain"},
    {"word": "æ™ºèƒ½", "pinyin": "zhÃ¬ nÃ©ng", "trans": "intelligence"},
    {"word": "é¡¹ç›®", "pinyin": "xiÃ ng mÃ¹", "trans": "project"},
    {"word": "ä»£ç ", "pinyin": "dÃ i mÇ", "trans": "code"}
]
[19.02.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç°æœ‰ç«¯åˆ°ç«¯è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸ä¾èµ–å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè€Œæ•°æ®é«˜æ•ˆè®­ç»ƒå°šæœªæ·±å…¥æ¢è®¨ã€‚æ–‡ç« èšç„¦äºè¯­éŸ³å’Œæ–‡æœ¬ä¹‹é—´çš„ä¸¤ä¸ªåŸºæœ¬é—®é¢˜ï¼šè¡¨ç¤ºç©ºé—´å·®è·å’Œåºåˆ—é•¿åº¦ä¸ä¸€è‡´ã€‚ä½œè€…æå‡ºäº†Soundwaveï¼Œå®ƒä½¿ç”¨é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥å’Œæ–°é¢–çš„æ¶æ„æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ç»“æœæ˜¾ç¤ºï¼ŒSoundwaveåœ¨ä½¿ç”¨äº”ååˆ†ä¹‹ä¸€çš„è®­ç»ƒæ•°æ®æ—¶ï¼Œåœ¨è¯­éŸ³ç¿»è¯‘å’ŒAIR-Benchè¯­éŸ³ä»»åŠ¡ä¸­ä¼˜äºå…ˆè¿›çš„Qwen2-Audioã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒSoundwaveåœ¨å¯¹è¯ä¸­ä»ä¿æŒå…¶æ™ºèƒ½ã€‚é¡¹ç›®çš„ä»£ç å¯åœ¨https://github.com/FreedomIntelligence/Soundwaveæ‰¾åˆ°ã€‚
[19.02.2025 09:11] Mistral response. {"id": "7305f53a505547f7872dcf20b2f74308", "object": "chat.completion", "created": 1739956308, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses how existing end-to-end speech large language models (LLMs) typically rely on large-scale annotated data for training, while data-efficient training has not been deeply explored. The article focuses on two fundamental issues between speech and text: the representation space gap and the inconsistency in sequence length. The authors propose Soundwave, which employs efficient training strategies and novel architecture to address these problems. The results show that Soundwave outperforms the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks while using one-fiftieth of the training data. Further analysis indicates that Soundwave retains its intelligence in conversations. The project's code can be found at https://github.com/FreedomIntelligence/Soundwave."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 232, "total_tokens": 397, "completion_tokens": 165}}
[19.02.2025 09:11] Response: This article discusses how existing end-to-end speech large language models (LLMs) typically rely on large-scale annotated data for training, while data-efficient training has not been deeply explored. The article focuses on two fundamental issues between speech and text: the representation space gap and the inconsistency in sequence length. The authors propose Soundwave, which employs efficient training strategies and novel architecture to address these problems. The results show that Soundwave outperforms the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks while using one-fiftieth of the training data. Further analysis indicates that Soundwave retains its intelligence in conversations. The project's code can be found at https://github.com/FreedomIntelligence/Soundwave.
[19.02.2025 09:11] Renaming data file.
[19.02.2025 09:11] Renaming previous data. hf_papers.json to ./d/2025-02-19.json
[19.02.2025 09:11] Saving new data file.
[19.02.2025 09:11] Generating page.
[19.02.2025 09:11] Renaming previous page.
[19.02.2025 09:11] Renaming previous data. index.html to ./d/2025-02-19.html
[19.02.2025 09:11] [Experimental] Generating Chinese page for reading.
[19.02.2025 09:11] Chinese vocab [{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'ç«¯åˆ°ç«¯', 'pinyin': 'duÄn dÃ o duÄn', 'trans': 'end-to-end'}, {'word': 'è¯­éŸ³', 'pinyin': 'yÇ” yÄ«n', 'trans': 'speech'}, {'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'}, {'word': 'ä¾èµ–', 'pinyin': 'yÄ« lÃ i', 'trans': 'rely on'}, {'word': 'å¤§è§„æ¨¡', 'pinyin': 'dÃ  guÄ« mÃ³', 'trans': 'large-scale'}, {'word': 'æ ‡æ³¨', 'pinyin': 'biÄo zhÃ¹', 'trans': 'annotate'}, {'word': 'æ•°æ®', 'pinyin': 'shÃ¹ jÃ¹', 'trans': 'data'}, {'word': 'è¿›è¡Œ', 'pinyin': 'jÃ¬n xÃ­ng', 'trans': 'carry out'}, {'word': 'è®­ç»ƒ', 'pinyin': 'xÃ¹n liÃ n', 'trans': 'train'}, {'word': 'é«˜æ•ˆ', 'pinyin': 'gÄo xiÃ o', 'trans': 'efficient'}, {'word': 'æ·±å…¥', 'pinyin': 'shÄ“n rÃ¹', 'trans': 'in-depth'}, {'word': 'æ¢è®¨', 'pinyin': 'tÃ n tÇo', 'trans': 'explore'}, {'word': 'èšç„¦', 'pinyin': 'jÃ¹ jiÄo', 'trans': 'focus on'}, {'word': 'åŸºæœ¬', 'pinyin': 'jÄ« bÄ›n', 'trans': 'basic'}, {'word': 'é—®é¢˜', 'pinyin': 'wÃ¨n tÃ­', 'trans': 'problem'}, {'word': 'è¡¨ç¤º', 'pinyin': 'biÇo shÃ¬', 'trans': 'represent'}, {'word': 'ç©ºé—´', 'pinyin': 'kÅng jiÄn', 'trans': 'space'}, {'word': 'å·®è·', 'pinyin': 'chÄ jÃ¹', 'trans': 'gap'}, {'word': 'åºåˆ—', 'pinyin': 'xÃ¹ liÃ¨', 'trans': 'sequence'}, {'word': 'é•¿åº¦', 'pinyin': 'chÃ¡ng dÃ¹', 'trans': 'length'}, {'word': 'ä¸ä¸€è‡´', 'pinyin': 'bÃ¹ yÄ« zhÃ¬', 'trans': 'inconsistent'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'}, {'word': 'æ¶æ„', 'pinyin': 'jiÃ  gÃ²u', 'trans': 'architecture'}, {'word': 'è§£å†³', 'pinyin': 'jiÄ› juÃ©', 'trans': 'solve'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'}, {'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇn shÃ¬', 'trans': 'show'}, {'word': 'ä¼˜äº', 'pinyin': 'yÅu yÃº', 'trans': 'superior to'}, {'word': 'å…ˆè¿›', 'pinyin': 'xiÄn jÃ¬n', 'trans': 'advanced'}, {'word': 'åˆ†æ', 'pinyin': 'fÄ“n xÄ«', 'trans': 'analyze'}, {'word': 'è¡¨æ˜', 'pinyin': 'biÇo mÃ­ng', 'trans': 'indicate'}, {'word': 'ä¿æŒ', 'pinyin': 'bÇo chÃ­', 'trans': 'maintain'}, {'word': 'æ™ºèƒ½', 'pinyin': 'zhÃ¬ nÃ©ng', 'trans': 'intelligence'}, {'word': 'é¡¹ç›®', 'pinyin': 'xiÃ ng mÃ¹', 'trans': 'project'}, {'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'}]
[19.02.2025 09:11] Renaming previous Chinese page.
[19.02.2025 09:11] Renaming previous data. zh.html to ./d/2025-02-18_zh_reading_task.html
[19.02.2025 09:11] Writing Chinese reading task.
[19.02.2025 09:11] Writing result.
[19.02.2025 09:11] Renaming log file.
[19.02.2025 09:11] Renaming previous data. log.txt to ./logs/2025-02-19_last_log.txt
