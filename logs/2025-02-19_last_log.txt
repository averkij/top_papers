[19.02.2025 06:15] Read previous papers.
[19.02.2025 06:15] Generating top page (month).
[19.02.2025 06:15] Writing top page (month).
[19.02.2025 07:10] Read previous papers.
[19.02.2025 07:10] Get feed.
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12900
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11564
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11079
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12464
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13131
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13143
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.11433
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13145
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12513
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.13130
[19.02.2025 07:10] Extract page data from URL. URL: https://huggingface.co/papers/2502.12859
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12501
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12215
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12170
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.09838
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12574
[19.02.2025 07:10] Extract page data from URL. URL: https://huggingface.co/papers/2502.13142
[19.02.2025 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2502.10852
[19.02.2025 07:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.02.2025 07:10] No deleted papers detected.
[19.02.2025 07:10] Downloading and parsing papers (pdf, html). Total: 18.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.12900.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.12900.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.12900.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.11564.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.11564.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.11564.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.11079.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.11079.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.11079.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.12464.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.12464.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.12464.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.13131.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.13131.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.13131.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.13143.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.13143.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.13143.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.11433.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.11433.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.11433.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.13145.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.13145.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.13145.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.12513.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.12513.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.12513.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.13130.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.13130.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.13130.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.12859.
[19.02.2025 07:10] Downloading paper 2502.12859 from http://arxiv.org/pdf/2502.12859v1...
[19.02.2025 07:10] Extracting affiliations from text.
[19.02.2025 07:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"PAFT: Prompt-Agnostic Fine-Tuning Chenxing Wei, Yao Shu, Mingwen Ou, Ying Tiffany He, Fei Richard Yu# College of Computer Science and Software Engineering, Shenzhen University, China Tsinghua Shenzhen International Graduate School, Tsinghua University, China Guangdong Lab of AI and Digital Economy (SZ), China School of Information Technology, Carleton University, Canada weichenxing2023@email.szu.edu.cn, shuyao@gml.ac.cn omm23@mails.tsinghua.edu.cn, richard.yu@ieee.org 5 2 0 2 8 1 ] . [ 1 9 5 8 2 1 . 2 0 5 2 : r a "
[19.02.2025 07:10] Response: ```python
[
    "College of Computer Science and Software Engineering, Shenzhen University, China",
    "Tsinghua Shenzhen International Graduate School, Tsinghua University, China",
    "Guangdong Lab of AI and Digital Economy (SZ), China",
    "School of Information Technology, Carleton University, Canada"
]
```
[19.02.2025 07:10] Deleting PDF ./assets/pdf/2502.12859.pdf.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.12501.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.12501.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.12501.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.12215.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.12215.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.12215.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.12170.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.12170.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.12170.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.09838.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.09838.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.09838.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.12574.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.12574.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.12574.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.13142.
[19.02.2025 07:10] Downloading paper 2502.13142 from http://arxiv.org/pdf/2502.13142v1...
[19.02.2025 07:10] Extracting affiliations from text.
[19.02.2025 07:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Pre-training Auto-regressive Robotic Models with 4D Representations Dantong Niu * 1 Yuvan Sharma * 1 Haoru Xue 1 Giscard Biamby 1 Junyi Zhang 1 Ziteng Ji 1 Trevor Darrell 1 Roei Herzig 1 5 2 0 2 8 1 ] . [ 1 2 4 1 3 1 . 2 0 5 2 : r a "
[19.02.2025 07:10] Response: ```python
[]
```
[19.02.2025 07:10] Extracting affiliations from text.
[19.02.2025 07:10] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Pre-training Auto-regressive Robotic Models with 4D Representations Dantong Niu * 1 Yuvan Sharma * 1 Haoru Xue 1 Giscard Biamby 1 Junyi Zhang 1 Ziteng Ji 1 Trevor Darrell 1 Roei Herzig 1 5 2 0 2 8 1 ] . [ 1 2 4 1 3 1 . 2 0 5 2 : r aFoundation models pre-trained on massive unlabeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by either the need for costly robotic annotations or the lack of representations that effectively model the physical world. In this paper, we introduce ARM4R, an Auto-regressive Robotic Model that leverages low-level 4D Representations learned from human video data to yield better pretrained robotic model. Specifically, we focus on utilizing 3D point tracking representations from videos derived by lifting 2D representations into 3D space via monocular depth estimation across time. These 4D representations maintain shared geometric structure between the points and robot state representations up to linear transformation, enabling efficient transfer learning from human video data to low-level robotic control. Our experiments show that ARM4R can transfer efficiently from human video data to robotics and consistently improves performance on tasks across various robot environments and configurations. 1. Introduction Recently, foundation models (FMs) have shown remarkable success, particularly in the domains of language (Brown et al., 2020; Touvron et al., 2023), vision (Kirillov et al., 2023), and multi-modal models (Chen et al., 2022; Alayrac et al., 2022; Liu et al., 2023; Li et al., 2023; OpenAI, 2023) pre-trained on vast amounts of vision and text data. These models exhibit impressive zero-shot and few-shot learning capabilities (Radford et al., 2021; Ouyang et al., 2022; Wei et al., 2021; Chung et al., 2024), highlighting the power of pre-training on generic data. However, numerous attempts in robotics (Xiao et al., 2022; Kim et al., 2024; Zhen et al., 1BAIR, UC Berkeley. *Equal contribution. Equal advising. 1 2024b; Niu et al., 2024; Ye et al., 2024) have yet to achieve the same pre-training success seen in other domains. This could potentially be attributed to the scarcity of large-scale, diverse robotic data, unlike the abundance of text and image data available for vision and language FMs. The lack of robotic data poses significant bottleneck in training foundation models that can effectively generalize across diverse robotic platforms and tasks. To overcome this limitation, several recent approaches (Xiao et al., 2022; Ye et al., 2024) employ representation learning by pre-training on an abundance of human data, enabling transfer to robotic systems. These approaches aim to recognize the inherent similarities between human and robot manipulation tasks and exploit the vast repositories of human video data available on the internet. Yet, these approaches have not been able to demonstrate effective generalization to downstream tasks. In part, this is due to their representations lacking an understanding of the physical world (Zhen et al., 2024a), and therefore being less effective for robotics. In contrast with these methods, Vision-Language-Action (VLAs) models take slightly different approach, implicitly leveraging human data in robotics by incorporating pre-trained components from Vision-and-Language Models In particular, they use language decoders pre- (VLMs). trained on tasks like visual question answering (e.g., RT2 (Brohan et al., 2023a)) and image captioning (e.g., OpenVLA (Kim et al., 2024)). Despite such efforts, there is discrepancy between these models high-level pre-training objective and the goal of enabling robotic models to handle low-level action prediction. While these initial objectives are valuable for comprehending visual and linguistic content, they dont directly address the nuances of low-level robot control, which involves aspects like precise manipulation and spatial reasoning. To address this, this papers method employs lower-level pre-training objective by starting with model that utilizes next-token prediction to learn 4D representations from human video data. These representations can then be transferred to more specialized scenarios by finetuning on robotic scenes and subsequently on proprioceptive data, while maintaining the same training objective. In this paper, we introduce ARM4R (Auto-regressive Pre-training Auto-regressive Robotic Models with 4D Representations Figure 1: Overview of ARM4R. We introduce an Auto-regressive Robotic Model that leverages low-level 4D Representations (3D point tracks across time) learned from human videos to yield better pre-trained robotic model. Robotic Model with 4D Representations).1 The key insight behind ARM4R is to learn low-level representation from the abundance of human video data that can capture properties of the physical world. This involves lifting 2D representations to 3D using monocular depth estimation and subsequently tracking the 3D points. The resulting 4D representations maintain shared geometric structure up to linear transformation between the 3D points and robot state representations used downstream, enabling efficient transfer learning from human video data to robotic manipulation tasks. Surprisingly, pre-training our method solely on human data yields superior results compared to other models like VLAs (Kim et al., 2024) that are pre-trained on robotic data such as OpenX (Collaboration et al., 2023). We summarize our main contributions as follows: (i) We introduce novel robotics pre-training approach that incorporates low-level 4D representations that enhance understanding of the physical world while also learning from unlabeled videos. (ii) Our approach shows that pre-training solely on human video data can lead to better performance than other methods that are pre-trained only on robotic data; (iii) Our method on average surpasses baselines like PerAct (Shridhar et al., 2023) on 12 different tasks in RLBenchs simulated environment, and OpenVLA (Kim et al., 2024) on real tasks with 7-DoF Kinova Gen3 robot; (iv) Our model also exhibits several advantageous properties, 1ARM4R is pronounced armor. including cross-robot generalization and 3D point track prediction for out-of-domain human and robotic videos. 2. Related Work Vision-Language-Action Models. VLAs are type of robot"
[19.02.2025 07:10] Mistral response. {"id": "9a445d0ec52d4e4596166f43621020e0", "object": "chat.completion", "created": 1739949045, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"BAIR, UC Berkeley\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1657, "total_tokens": 1671, "completion_tokens": 14}}
[19.02.2025 07:10] Response: ```python
["BAIR, UC Berkeley"]
```
[19.02.2025 07:10] Deleting PDF ./assets/pdf/2502.13142.pdf.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Downloading and parsing paper https://huggingface.co/papers/2502.10852.
[19.02.2025 07:10] Extra JSON file exists (./assets/json/2502.10852.json), skip PDF parsing.
[19.02.2025 07:10] Paper image links file exists (./assets/img_data/2502.10852.json), skip HTML parsing.
[19.02.2025 07:10] Success.
[19.02.2025 07:10] Enriching papers with extra data.
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 0. Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 1. Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discre...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 2. The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-con...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 3. Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, bu...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 4. Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collec...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 5. Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand ...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 6. Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approach...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 7. Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-c...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 8. After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language r...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 9. We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped wit...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 10. While Large Language Models (LLMs) adapt well to downstream tasks after fine-tuning, this adaptability often compromises prompt robustness, as even minor prompt variations can significantly degrade performance. To address this, we propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective app...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 11. LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly re...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 12. The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these mode...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 13. We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates conne...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 14. We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowled...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 15. Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloa...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 16. Foundation models pre-trained on massive unlabeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by ei...
[19.02.2025 07:10] ********************************************************************************
[19.02.2025 07:10] Abstract 17. While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models n...
[19.02.2025 07:10] Read previous papers.
[19.02.2025 07:10] Generating reviews via LLM API.
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#training", "#audio", "#transfer_learning", "#open_source", "#optimization", "#data", "#architecture"], "emoji": "üéôÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –º–∏–Ω–∏–º—É–º–æ–º –¥–∞–Ω–Ω—ã—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Soundwave - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ—á–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#training", "#benchmark", "#optimization", "#dataset", "#diffusion", "#architecture"], "emoji": "üåä", "ru": {"title": "–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–º –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–∏ –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ —Å –∏—Å–ø
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#architecture", "#video", "#multimodal"], "emoji": "üé•", "ru": {"title": "Phantom: –±–∞–ª–∞–Ω—Å —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Phantom - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#security", "#benchmark", "#training", "#inference", "#optimization"], "emoji": "üõ°Ô∏è", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∑–∞—â–∏—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ SafeRoute –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#alignment", "#rlhf", "#training", "#dataset", "#interpretability"], "emoji": "üß©", "ru": {"title": "–†–∞–∑–ª–æ–∂–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏–∑ –±–∏–Ω–∞—Ä–Ω—ã—Ö
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#3d", "#games", "#dataset", "#reasoning", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É —Ä–æ–±–æ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —Ä–æ–±–æ—Ç–æ–≤. 
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#architecture", "#training", "#reasoning", "#rl", "#multimodal"], "emoji": "üìà", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ FLAG-Trader, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —è–∑—ã–∫–æ–≤—É
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#transfer_learning", "#architecture", "#training", "#multimodal"], "emoji": "üöÄ", "ru": {"title": "mmMamba: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –ª–∏–Ω–µ–π–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç mmMamba - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#data", "#synthetic", "#dataset", "#multimodal"], "emoji": "üñºÔ∏è", "ru": {"title": "RealSyn: –£–ª—É—á—à–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —Ä–µ–∞–ª—å–Ω—ã—Ö –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#cv", "#robotics", "#agi", "#multimodal", "#agents", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "Magma: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ò–ò-–∞–≥–µ–Ω—Ç –¥–ª—è —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –º–∏—Ä–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Magma - –Ω–æ–≤—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, —Å–ø–æ—Å–æ–±–Ω—É—é –≤—ã–ø–æ–ª–Ω—è—Ç—å –∞–≥
[19.02.2025 07:10] Querying the API.
[19.02.2025 07:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

While Large Language Models (LLMs) adapt well to downstream tasks after fine-tuning, this adaptability often compromises prompt robustness, as even minor prompt variations can significantly degrade performance. To address this, we propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach that dynamically adjusts prompts during fine-tuning. This encourages the model to learn underlying task principles rather than overfitting to specific prompt formulations. PAFT operates in two stages: First, a diverse set of meaningful, synthetic candidate prompts is constructed. Second, during fine-tuning, prompts are randomly sampled from this set to create dynamic training inputs. Extensive experiments across diverse datasets and LLMs demonstrate that models trained with PAFT exhibit strong robustness and generalization across a wide range of prompts, including unseen ones. This enhanced robustness improves both model performance and inference speed while maintaining training efficiency. Ablation studies further confirm the effectiveness of PAFT.
[19.02.2025 07:10] Response: {
  "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Prompt-Agnostic Fine-Tuning (PAFT). PAFT —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–Ω–∏–∂–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫—É –ø—Ä–æ–º–ø—Ç–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ —Å–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä –∏–∑ –Ω–∏—Ö –≤–æ –≤—Ä–µ–º—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ PAFT –ø–æ–≤—ã—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø—Ä–æ–º–ø—Ç–∞–º, —É–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –≤—ã–≤–æ–¥–∞.",
  "emoji": "üß†",
  "title": "–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –ø—Ä–æ–º–ø—Ç–∞–º: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[19.02.2025 07:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"While Large Language Models (LLMs) adapt well to downstream tasks after fine-tuning, this adaptability often compromises prompt robustness, as even minor prompt variations can significantly degrade performance. To address this, we propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach that dynamically adjusts prompts during fine-tuning. This encourages the model to learn underlying task principles rather than overfitting to specific prompt formulations. PAFT operates in two stages: First, a diverse set of meaningful, synthetic candidate prompts is constructed. Second, during fine-tuning, prompts are randomly sampled from this set to create dynamic training inputs. Extensive experiments across diverse datasets and LLMs demonstrate that models trained with PAFT exhibit strong robustness and generalization across a wide range of prompts, including unseen ones. This enhanced robustness improves both model performance and inference speed while maintaining training efficiency. Ablation studies further confirm the effectiveness of PAFT."

[19.02.2025 07:10] Response: ```python
['TRAINING', 'DATASET', 'INFERENCE']
```
[19.02.2025 07:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"While Large Language Models (LLMs) adapt well to downstream tasks after fine-tuning, this adaptability often compromises prompt robustness, as even minor prompt variations can significantly degrade performance. To address this, we propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach that dynamically adjusts prompts during fine-tuning. This encourages the model to learn underlying task principles rather than overfitting to specific prompt formulations. PAFT operates in two stages: First, a diverse set of meaningful, synthetic candidate prompts is constructed. Second, during fine-tuning, prompts are randomly sampled from this set to create dynamic training inputs. Extensive experiments across diverse datasets and LLMs demonstrate that models trained with PAFT exhibit strong robustness and generalization across a wide range of prompts, including unseen ones. This enhanced robustness improves both model performance and inference speed while maintaining training efficiency. Ablation studies further confirm the effectiveness of PAFT."

[19.02.2025 07:10] Response: ```python
["OPTIMIZATION", "SYNTHETIC"]
```
[19.02.2025 07:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Prompt-Agnostic Fine-Tuning (PAFT), a method designed to improve the robustness of Large Language Models (LLMs) when adapting to various tasks. PAFT works by dynamically adjusting prompts during the fine-tuning process, which helps the model focus on the core principles of the tasks instead of memorizing specific prompt formats. The approach involves creating a diverse set of synthetic prompts and randomly sampling from them during training, leading to better generalization and performance on unseen prompts. Experimental results show that PAFT enhances both the robustness and inference speed of LLMs while keeping training efficient.","title":"Enhancing Robustness in LLMs with Dynamic Prompting"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Prompt-Agnostic Fine-Tuning (PAFT), a method designed to improve the robustness of Large Language Models (LLMs) when adapting to various tasks. PAFT works by dynamically adjusting prompts during the fine-tuning process, which helps the model focus on the core principles of the tasks instead of memorizing specific prompt formats. The approach involves creating a diverse set of synthetic prompts and randomly sampling from them during training, leading to better generalization and performance on unseen prompts. Experimental results show that PAFT enhances both the robustness and inference speed of LLMs while keeping training efficient.', title='Enhancing Robustness in LLMs with Dynamic Prompting'))
[19.02.2025 07:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂæÆË∞ÉÂêéËÉΩÂ§üÂæàÂ•ΩÂú∞ÈÄÇÂ∫î‰∏ãÊ∏∏‰ªªÂä°Ôºå‰ΩÜËøôÁßçÈÄÇÂ∫îÊÄßÂæÄÂæÄ‰ºöÂΩ±ÂìçÊèêÁ§∫ÁöÑÈ≤ÅÊ£íÊÄßÔºåÂõ†‰∏∫Âç≥‰ΩøÊòØÂæÆÂ∞èÁöÑÊèêÁ§∫ÂèòÂåñ‰πü‰ºöÊòæËëóÈôç‰ΩéÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÊñπÊ≥ï‚Äî‚ÄîÊèêÁ§∫Êó†ÂÖ≥ÂæÆË∞ÉÔºàPAFTÔºâÔºåËØ•ÊñπÊ≥ïÂú®ÂæÆË∞ÉËøáÁ®ã‰∏≠Âä®ÊÄÅË∞ÉÊï¥ÊèêÁ§∫„ÄÇPAFTÁöÑÊìç‰ΩúÂàÜ‰∏∫‰∏§‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖàÔºåÊûÑÂª∫‰∏ÄÁªÑÂ§öÊ†∑Âåñ‰∏îÊúâÊÑè‰πâÁöÑÂêàÊàêÂÄôÈÄâÊèêÁ§∫ÔºõÂÖ∂Ê¨°ÔºåÂú®ÂæÆË∞ÉËøáÁ®ã‰∏≠Ôºå‰ªéËøôÁªÑÊèêÁ§∫‰∏≠ÈöèÊú∫ÊäΩÂèñÔºå‰ª•ÂàõÂª∫Âä®ÊÄÅËÆ≠ÁªÉËæìÂÖ•„ÄÇÈÄöËøáÂú®Â§öÁßçÊï∞ÊçÆÈõÜÂíåLLMs‰∏äÁöÑÂπøÊ≥õÂÆûÈ™åÔºåËØÅÊòé‰∫Ü‰ΩøÁî®PAFTËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®ÂêÑÁßçÊèêÁ§∫ÔºàÂåÖÊã¨Êú™ËßÅËøáÁöÑÊèêÁ§∫Ôºâ‰∏äË°®Áé∞Âá∫Âº∫Â§ßÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ","title":"ÊèêÂçáÊ®°ÂûãÈ≤ÅÊ£íÊÄß‰∏éÊ≥õÂåñËÉΩÂäõÁöÑÊèêÁ§∫Êó†ÂÖ≥ÂæÆË∞É"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂæÆË∞ÉÂêéËÉΩÂ§üÂæàÂ•ΩÂú∞ÈÄÇÂ∫î‰∏ãÊ∏∏‰ªªÂä°Ôºå‰ΩÜËøôÁßçÈÄÇÂ∫îÊÄßÂæÄÂæÄ‰ºöÂΩ±ÂìçÊèêÁ§∫ÁöÑÈ≤ÅÊ£íÊÄßÔºåÂõ†‰∏∫Âç≥‰ΩøÊòØÂæÆÂ∞èÁöÑÊèêÁ§∫ÂèòÂåñ‰πü‰ºöÊòæËëóÈôç‰ΩéÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÊñπÊ≥ï‚Äî‚ÄîÊèêÁ§∫Êó†ÂÖ≥ÂæÆË∞ÉÔºàPAFTÔºâÔºåËØ•ÊñπÊ≥ïÂú®ÂæÆË∞ÉËøáÁ®ã‰∏≠Âä®ÊÄÅË∞ÉÊï¥ÊèêÁ§∫„ÄÇPAFTÁöÑÊìç‰ΩúÂàÜ‰∏∫‰∏§‰∏™Èò∂ÊÆµÔºöÈ¶ñÂÖàÔºåÊûÑÂª∫‰∏ÄÁªÑÂ§öÊ†∑Âåñ‰∏îÊúâÊÑè‰πâÁöÑÂêàÊàêÂÄôÈÄâÊèêÁ§∫ÔºõÂÖ∂Ê¨°ÔºåÂú®ÂæÆË∞ÉËøáÁ®ã‰∏≠Ôºå‰ªéËøôÁªÑÊèêÁ§∫‰∏≠ÈöèÊú∫ÊäΩÂèñÔºå‰ª•ÂàõÂª∫Âä®ÊÄÅËÆ≠ÁªÉËæìÂÖ•„ÄÇÈÄöËøáÂú®Â§öÁßçÊï∞ÊçÆÈõÜÂíåLLMs‰∏äÁöÑÂπøÊ≥õÂÆûÈ™åÔºåËØÅÊòé‰∫Ü‰ΩøÁî®PAFTËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®ÂêÑÁßçÊèêÁ§∫ÔºàÂåÖÊã¨Êú™ËßÅËøáÁöÑÊèêÁ§∫Ôºâ‰∏äË°®Áé∞Âá∫Âº∫Â§ßÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ', title='ÊèêÂçáÊ®°ÂûãÈ≤ÅÊ£íÊÄß‰∏éÊ≥õÂåñËÉΩÂäõÁöÑÊèêÁ§∫Êó†ÂÖ≥ÂæÆË∞É'))
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#inference", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –ò–ò —á–µ—Ä–µ–∑ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –º–Ω–µ–Ω–∏–µ–º —Ç–æ–ª–ø—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'Crowd-based Comparative Evaluation'.
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#training", "#optimization"], "emoji": "üîç", "ru": {"title": "–ö–æ—Ä–æ—á–µ - –ª—É—á—à–µ: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –±–æ
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#optimization", "#training", "#open_source", "#architecture"], "emoji": "üîÄ", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ MUDD (MUltiway Dynamic Dense) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å–ª–æ—è–º–∏ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Transformer. M
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#data", "#training", "#cv", "#dataset", "#healthcare"], "emoji": "üè•", "ru": {"title": "HealthGPT: –ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "HealthGPT - —ç—Ç–æ –º–æ—â–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç
[19.02.2025 07:10] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#inference", "#training", "#long_context"], "emoji": "üß†", "ru": {"title": "HEADINFER: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è LLM —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "HEADINFER - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º
[19.02.2025 07:10] Querying the API.
[19.02.2025 07:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Foundation models pre-trained on massive unlabeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by either the need for costly robotic annotations or the lack of representations that effectively model the physical world. In this paper, we introduce ARM4R, an Auto-regressive Robotic Model that leverages low-level 4D Representations learned from human video data to yield a better pre-trained robotic model. Specifically, we focus on utilizing 3D point tracking representations from videos derived by lifting 2D representations into 3D space via monocular depth estimation across time. These 4D representations maintain a shared geometric structure between the points and robot state representations up to a linear transformation, enabling efficient transfer learning from human video data to low-level robotic control. Our experiments show that ARM4R can transfer efficiently from human video data to robotics and consistently improves performance on tasks across various robot environments and configurations.
[19.02.2025 07:10] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ARM4R - –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é 4D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–∑ –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö —Å –ª—é–¥—å–º–∏. –ú–æ–¥–µ–ª—å –ø—Ä–∏–º–µ–Ω—è–µ—Ç 3D-–æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Ç–æ—á–µ–∫ –∏–∑ –≤–∏–¥–µ–æ, –ø–æ–¥–Ω–∏–º–∞—è 2D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤ 3D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å –ø–æ–º–æ—â—å—é –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –∑–Ω–∞–Ω–∏—è —Å –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö –ª—é–¥–µ–π –Ω–∞ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ARM4R —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö.",
  "emoji": "ü§ñ",
  "title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ –Ω–∞ –≤–∏–¥–µ–æ —Å –ª—é–¥—å–º–∏: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏ –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏"
}
[19.02.2025 07:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Foundation models pre-trained on massive unlabeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by either the need for costly robotic annotations or the lack of representations that effectively model the physical world. In this paper, we introduce ARM4R, an Auto-regressive Robotic Model that leverages low-level 4D Representations learned from human video data to yield a better pre-trained robotic model. Specifically, we focus on utilizing 3D point tracking representations from videos derived by lifting 2D representations into 3D space via monocular depth estimation across time. These 4D representations maintain a shared geometric structure between the points and robot state representations up to a linear transformation, enabling efficient transfer learning from human video data to low-level robotic control. Our experiments show that ARM4R can transfer efficiently from human video data to robotics and consistently improves performance on tasks across various robot environments and configurations."

[19.02.2025 07:11] Response: ```python
['DATASET', '3D', 'ROBOTICS', 'TRAINING']
```
[19.02.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Foundation models pre-trained on massive unlabeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by either the need for costly robotic annotations or the lack of representations that effectively model the physical world. In this paper, we introduce ARM4R, an Auto-regressive Robotic Model that leverages low-level 4D Representations learned from human video data to yield a better pre-trained robotic model. Specifically, we focus on utilizing 3D point tracking representations from videos derived by lifting 2D representations into 3D space via monocular depth estimation across time. These 4D representations maintain a shared geometric structure between the points and robot state representations up to a linear transformation, enabling efficient transfer learning from human video data to low-level robotic control. Our experiments show that ARM4R can transfer efficiently from human video data to robotics and consistently improves performance on tasks across various robot environments and configurations."

[19.02.2025 07:11] Response: ```python
['TRANSFER_LEARNING']
```
[19.02.2025 07:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents ARM4R, an Auto-regressive Robotic Model designed to enhance robotic learning by utilizing low-level 4D representations derived from human video data. By converting 2D video representations into 3D point tracking through monocular depth estimation, ARM4R captures the geometric relationships necessary for effective robotic control. The model enables efficient transfer learning, allowing robots to learn from human actions without the need for extensive annotations. Experimental results demonstrate that ARM4R significantly improves performance across diverse robotic tasks and environments.","title":"Revolutionizing Robotics with Human Video Insights"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents ARM4R, an Auto-regressive Robotic Model designed to enhance robotic learning by utilizing low-level 4D representations derived from human video data. By converting 2D video representations into 3D point tracking through monocular depth estimation, ARM4R captures the geometric relationships necessary for effective robotic control. The model enables efficient transfer learning, allowing robots to learn from human actions without the need for extensive annotations. Experimental results demonstrate that ARM4R significantly improves performance across diverse robotic tasks and environments.', title='Revolutionizing Robotics with Human Video Insights'))
[19.02.2025 07:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ARM4RÁöÑËá™ÂõûÂΩíÊú∫Âô®‰∫∫Ê®°ÂûãÔºåÂÆÉÂà©Áî®‰ªé‰∫∫Á±ªËßÜÈ¢ëÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÁöÑ‰ΩéÁ∫ß4DË°®Á§∫Êù•ÊîπËøõÊú∫Âô®‰∫∫ÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°Âûã„ÄÇÊàë‰ª¨‰∏ìÊ≥®‰∫éÂà©Áî®‰ªéËßÜÈ¢ë‰∏≠ÊèêÂèñÁöÑ3DÁÇπË∑üË∏™Ë°®Á§∫ÔºåËøô‰∫õË°®Á§∫ÈÄöËøáÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Â∞Ü2DË°®Á§∫ÊèêÂçáÂà∞3DÁ©∫Èó¥„ÄÇ4DË°®Á§∫Âú®ÁÇπÂíåÊú∫Âô®‰∫∫Áä∂ÊÄÅË°®Á§∫‰πãÈó¥‰øùÊåÅÂÖ±‰∫´ÁöÑÂá†‰ΩïÁªìÊûÑÔºå‰ªéËÄåÂÆûÁé∞‰ªé‰∫∫Á±ªËßÜÈ¢ëÊï∞ÊçÆÂà∞‰ΩéÁ∫ßÊú∫Âô®‰∫∫ÊéßÂà∂ÁöÑÈ´òÊïàËøÅÁßªÂ≠¶‰π†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåARM4RËÉΩÂ§üÊúâÊïàÂú∞‰ªé‰∫∫Á±ªËßÜÈ¢ëÊï∞ÊçÆËøÅÁßªÂà∞Êú∫Âô®‰∫∫ÔºåÂπ∂Âú®ÂêÑÁßçÊú∫Âô®‰∫∫ÁéØÂ¢ÉÂíåÈÖçÁΩÆ‰∏≠ÊåÅÁª≠ÊèêÈ´ò‰ªªÂä°ÊÄßËÉΩ„ÄÇ","title":"Âà©Áî®ËßÜÈ¢ëÊï∞ÊçÆÊèêÂçáÊú∫Âô®‰∫∫ÊéßÂà∂ËÉΩÂäõ"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫ARM4RÁöÑËá™ÂõûÂΩíÊú∫Âô®‰∫∫Ê®°ÂûãÔºåÂÆÉÂà©Áî®‰ªé‰∫∫Á±ªËßÜÈ¢ëÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÁöÑ‰ΩéÁ∫ß4DË°®Á§∫Êù•ÊîπËøõÊú∫Âô®‰∫∫ÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°Âûã„ÄÇÊàë‰ª¨‰∏ìÊ≥®‰∫éÂà©Áî®‰ªéËßÜÈ¢ë‰∏≠ÊèêÂèñÁöÑ3DÁÇπË∑üË∏™Ë°®Á§∫ÔºåËøô‰∫õË°®Á§∫ÈÄöËøáÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Â∞Ü2DË°®Á§∫ÊèêÂçáÂà∞3DÁ©∫Èó¥„ÄÇ4DË°®Á§∫Âú®ÁÇπÂíåÊú∫Âô®‰∫∫Áä∂ÊÄÅË°®Á§∫‰πãÈó¥‰øùÊåÅÂÖ±‰∫´ÁöÑÂá†‰ΩïÁªìÊûÑÔºå‰ªéËÄåÂÆûÁé∞‰ªé‰∫∫Á±ªËßÜÈ¢ëÊï∞ÊçÆÂà∞‰ΩéÁ∫ßÊú∫Âô®‰∫∫ÊéßÂà∂ÁöÑÈ´òÊïàËøÅÁßªÂ≠¶‰π†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåARM4RËÉΩÂ§üÊúâÊïàÂú∞‰ªé‰∫∫Á±ªËßÜÈ¢ëÊï∞ÊçÆËøÅÁßªÂà∞Êú∫Âô®‰∫∫ÔºåÂπ∂Âú®ÂêÑÁßçÊú∫Âô®‰∫∫ÁéØÂ¢ÉÂíåÈÖçÁΩÆ‰∏≠ÊåÅÁª≠ÊèêÈ´ò‰ªªÂä°ÊÄßËÉΩ„ÄÇ', title='Âà©Áî®ËßÜÈ¢ëÊï∞ÊçÆÊèêÂçáÊú∫Âô®‰∫∫ÊéßÂà∂ËÉΩÂäõ'))
[19.02.2025 07:11] Using data from previous issue: {"categories": ["#multilingual", "#low_resource"], "emoji": "üåç", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –±–∞—Ä—å–µ—Ä–∞: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –∫—Ä–∞–π–Ω–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ 
[19.02.2025 07:11] Loading Chinese text from previous data.
[19.02.2025 07:11] Renaming data file.
[19.02.2025 07:11] Renaming previous data. hf_papers.json to ./d/2025-02-19.json
[19.02.2025 07:11] Saving new data file.
[19.02.2025 07:11] Generating page.
[19.02.2025 07:11] Renaming previous page.
[19.02.2025 07:11] Renaming previous data. index.html to ./d/2025-02-19.html
[19.02.2025 07:11] [Experimental] Generating Chinese page for reading.
[19.02.2025 07:11] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': '‰∫∫ÂΩ¢Êú∫Âô®‰∫∫', 'pinyin': 'r√©n x√≠ng jƒ´ q√¨ r√©n', 'trans': 'humanoid robot'}, {'word': 'Ëá™Âä®', 'pinyin': 'z√¨ d√≤ng', 'trans': 'automatic'}, {'word': 'Ë∑åÂÄí', 'pinyin': 'diƒì d«éo', 'trans': 'fall down'}, {'word': 'ÊÅ¢Â§ç', 'pinyin': 'huƒ´ f√π', 'trans': 'recover'}, {'word': 'ÈáçË¶ÅÊÄß', 'pinyin': 'zh√≤ng y√†o x√¨ng', 'trans': 'importance'}, {'word': 'ËÆæËÆ°', 'pinyin': 'sh√® j√¨', 'trans': 'design'}, {'word': 'ÊéßÂà∂Âô®', 'pinyin': 'k√≤ng zh√¨ q√¨', 'trans': 'controller'}, {'word': 'Á´ôËµ∑Êù•', 'pinyin': 'zh√†n q«ê l√°i', 'trans': 'stand up'}, {'word': 'ÂßøÊÄÅ', 'pinyin': 'zƒ´ t√†i', 'trans': 'posture'}, {'word': 'Âú∞ÂΩ¢', 'pinyin': 'd√¨ x√≠ng', 'trans': 'terrain'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√π z√°', 'trans': 'complex'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'Â≠¶‰π†', 'pinyin': 'xu√© x√≠', 'trans': 'learn'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ng ji√†', 'trans': 'framework'}, {'word': 'Èò∂ÊÆµ', 'pinyin': 'jiƒì du√†n', 'trans': 'stage'}, {'word': 'Ë∑ØÂæÑ', 'pinyin': 'l√π j√¨ng', 'trans': 'path'}, {'word': '‰ºòÂåñ', 'pinyin': 'y≈çu hu√†', 'trans': 'optimize'}, {'word': 'Âä®‰Ωú', 'pinyin': 'd√≤ng zu√≤', 'trans': 'action'}, {'word': 'Âπ≥Á®≥', 'pinyin': 'p√≠ng wƒõn', 'trans': 'stable'}, {'word': 'ÂèØÈù†', 'pinyin': 'kƒõ k√†o', 'trans': 'reliable'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'Âú∞Èù¢', 'pinyin': 'd√¨ mi√†n', 'trans': 'ground'}, {'word': 'ÊàêÂäü', 'pinyin': 'ch√©ng g≈çng', 'trans': 'success'}, {'word': 'È¶ñÊ¨°', 'pinyin': 'sh«íu c√¨', 'trans': 'first time'}, {'word': 'ÁúüÂÆû', 'pinyin': 'zhƒìn sh√≠', 'trans': 'real'}, {'word': 'ÁéØÂ¢É', 'pinyin': 'hu√°n j√¨ng', 'trans': 'environment'}, {'word': 'Â∫îÁî®', 'pinyin': 'y√¨ng y√≤ng', 'trans': 'apply'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}]
[19.02.2025 07:11] Renaming previous Chinese page.
[19.02.2025 07:11] Renaming previous data. zh.html to ./d/2025-02-18_zh_reading_task.html
[19.02.2025 07:11] Writing Chinese reading task.
[19.02.2025 07:11] Writing result.
[19.02.2025 07:11] Renaming log file.
[19.02.2025 07:11] Renaming previous data. log.txt to ./logs/2025-02-19_last_log.txt
