[19.02.2025 02:12] Read previous papers.
[19.02.2025 02:12] Generating top page (month).
[19.02.2025 02:12] Writing top page (month).
[19.02.2025 03:15] Read previous papers.
[19.02.2025 03:15] Get feed.
[19.02.2025 03:15] Extract page data from URL. URL: https://huggingface.co/papers/2502.12513
[19.02.2025 03:15] Extract page data from URL. URL: https://huggingface.co/papers/2502.13131
[19.02.2025 03:15] Extract page data from URL. URL: https://huggingface.co/papers/2502.12501
[19.02.2025 03:15] Extract page data from URL. URL: https://huggingface.co/papers/2502.13143
[19.02.2025 03:15] Extract page data from URL. URL: https://huggingface.co/papers/2502.13145
[19.02.2025 03:15] Extract page data from URL. URL: https://huggingface.co/papers/2502.11433
[19.02.2025 03:15] Extract page data from URL. URL: https://huggingface.co/papers/2502.11079
[19.02.2025 03:15] Extract page data from URL. URL: https://huggingface.co/papers/2502.12574
[19.02.2025 03:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.02.2025 03:15] Downloading and parsing papers (pdf, html). Total: 8.
[19.02.2025 03:15] Downloading and parsing paper https://huggingface.co/papers/2502.12513.
[19.02.2025 03:15] Downloading paper 2502.12513 from http://arxiv.org/pdf/2502.12513v1...
[19.02.2025 03:15] Extracting affiliations from text.
[19.02.2025 03:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"RealSyn: An Effective and Scalable Multimodal Interleaved Document Tiancheng Gu*, Kaicheng Yang*, Chaoyi Zhang, Yin Xie, Xiang An, Ziyong Feng Dongnan Liu, Weidong Cai, Jiankang Deng The University of Sydney DeepGlint Imperial College London tigu8498@uni.sydney.edu.au, kaichengyang@deepglint.com https://garygutc.github.io/RealSyn "
[19.02.2025 03:15] Response: ```python
["The University of Sydney", "DeepGlint", "Imperial College London"]
```
[19.02.2025 03:15] Deleting PDF ./assets/pdf/2502.12513.pdf.
[19.02.2025 03:15] Success.
[19.02.2025 03:15] Downloading and parsing paper https://huggingface.co/papers/2502.13131.
[19.02.2025 03:15] Downloading paper 2502.13131 from http://arxiv.org/pdf/2502.13131v1...
[19.02.2025 03:15] Extracting affiliations from text.
[19.02.2025 03:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 1 3 1 3 1 . 2 0 5 2 : r a Feng Luo1, Rui Yang2, Hao Sun3, Chunyuan Deng1, Jiarui Yao2, Jingyan Shen4, Huan Zhang2, Hanjie Chen1 1Rice University, 2University of Illinois at Urbana-Champaign, 3University of Cambridge, 4 Columbia University Equal contribution. Emails: fl38@rice.edu, ry21@illinois.edu "
[19.02.2025 03:15] Response: ```python
["Rice University", "University of Illinois at Urbana-Champaign", "University of Cambridge", "Columbia University"]
```
[19.02.2025 03:15] Deleting PDF ./assets/pdf/2502.13131.pdf.
[19.02.2025 03:15] Success.
[19.02.2025 03:15] Downloading and parsing paper https://huggingface.co/papers/2502.12501.
[19.02.2025 03:15] Downloading paper 2502.12501 from http://arxiv.org/pdf/2502.12501v1...
[19.02.2025 03:15] Extracting affiliations from text.
[19.02.2025 03:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge Qiyuan Zhang1, Yufei Wang2, Yuxin Jiang3, Liangyou Li2, Chuhan Wu2, Yasheng Wang2, Xin Jiang2, Lifeng Shang2, Ruiming Tang2, Fuyuan Lyu4, Chen Ma1 1City University of Hong Kong, 2Huawei Noahs Ark Lab, 3The Hong Kong University of Science and Technology (Guangzhou), 4McGill University & MILA qzhang732-c@my.cityu.edu.hk, wang.yufei1@huawei.com, 5 2 0 2 8 1 ] . [ 1 1 0 5 2 1 . 2 0 5 2 : r a "
[19.02.2025 03:15] Response: ```python
["City University of Hong Kong", "Huawei Noahs Ark Lab", "The Hong Kong University of Science and Technology (Guangzhou)", "McGill University & MILA"]
```
[19.02.2025 03:15] Deleting PDF ./assets/pdf/2502.12501.pdf.
[19.02.2025 03:15] Success.
[19.02.2025 03:15] Downloading and parsing paper https://huggingface.co/papers/2502.13143.
[19.02.2025 03:15] Downloading paper 2502.13143 from http://arxiv.org/pdf/2502.13143v1...
[19.02.2025 03:16] Extracting affiliations from text.
[19.02.2025 03:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 3 4 1 3 1 . 2 0 5 2 : r SOFAR: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation Zekun Qi13 Wenyao Zhang237 Yufei Ding34 Runpei Dong5 Xinqiang Yu3 Jingwen Li4 Lingyun Xu4 Baoyu Li5 Xialin He5 Guofan Fan1 Jiazhao Zhang3 Jiawei He3 Jiayuan Gu6 Xin Jin7 Kaisheng Ma1 Zhizheng Zhang3 He Wang34 Li Yi189 Equal Contribution Corresponding authors 1Tsinghua University 2Shanghai Jiao Tong University 3Galbot 4Peking University 6ShanghaiTech University 7Eastern Institute of Technology 8Shanghai Qi Zhi Institute https://qizekun.github.io/sofar 5UIUC 9Shanghai AI Laboratory Fig. 1: Overview. We present SOFAR, the first system to incorporate object orientation into spatial reasoning and robotic manipulation. We introduce the concept of Semantic Orientation, which refers to natural language-grounded object orientations, such as the cutting direction of knife or the handle direction of cup. To support this, we construct OrienText300K, large-scale dataset of object-text-orientation pairs. SOFAR enables accurate 6-DoF object understanding and excels in tasks such as visual question answering, object manipulation, and navigation. AbstractSpatial intelligence is critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand object orientationsa key requirement for tasks involving finegrained manipulations. Addressing this limitation not only requires geometric reasoning but also an expressive and intuitive way to represent orientation. In this context, we propose that natural language offers more flexible representation space than canonical frames, making it particularly suitable for instruction-following robotic systems. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natura"
[19.02.2025 03:16] Response: ```python
[
    "Tsinghua University",
    "Shanghai Jiao Tong University",
    "Galbot",
    "Peking University",
    "ShanghaiTech University",
    "Eastern Institute of Technology",
    "Shanghai Qi Zhi Institute",
    "UIUC",
    "Shanghai AI Laboratory"
]
```
[19.02.2025 03:16] Deleting PDF ./assets/pdf/2502.13143.pdf.
[19.02.2025 03:16] Success.
[19.02.2025 03:16] Downloading and parsing paper https://huggingface.co/papers/2502.13145.
[19.02.2025 03:16] Downloading paper 2502.13145 from http://arxiv.org/pdf/2502.13145v1...
[19.02.2025 03:16] Extracting affiliations from text.
[19.02.2025 03:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 5 4 1 3 1 . 2 0 5 2 : r Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation Bencheng Liao * 1 2 Hongyuan Tao * 2 Qian Zhang 3 Tianheng Cheng 2 Yingyue Li 2 Haoran Yin 3 Wenyu Liu 2 Xinggang Wang 2 Figure 1: Comprehensive comparison of mmMamba. (a) Our mmMamba can build linear-complexity and hybrid decoder-only VLM by distilling the knowledge in Transformer to Mamba-2. (b) By distilling from the quadratic-complexity decoder-only VLM HoVLE, our mmMamba-linear achieves competitive performance against existing linear and quadraticcomplexity VLMs with fewer parameters (e.g., 2 fewer than EVE-7B), while mmMamba-hybrid surpasses them across all benchmarks and approaches the teacher model HoVLEs performance. (c)-(d) We compare the speed and memory of mmMamba-linear and mmMamba-hybrid with the teacher model HoVLE on the same single NVIDIA 4090 GPU. mmMamba-linear maintains consistently low latency and memory usage, while mmMamba-hybrids resource consumption scales significantly better than HoVLE. At 103K tokens, mmMamba-linear demonstrates 20.6 speedup compared to HoVLE and saves 75.8% GPU memory, while mmMamba-hybrid achieves 13.5 speedup and saves 60.2% GPU memory. "
[19.02.2025 03:16] Response: ```python
[]
```
[19.02.2025 03:16] Extracting affiliations from text.
[19.02.2025 03:16] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 5 4 1 3 1 . 2 0 5 2 : r Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation Bencheng Liao * 1 2 Hongyuan Tao * 2 Qian Zhang 3 Tianheng Cheng 2 Yingyue Li 2 Haoran Yin 3 Wenyu Liu 2 Xinggang Wang 2 Figure 1: Comprehensive comparison of mmMamba. (a) Our mmMamba can build linear-complexity and hybrid decoder-only VLM by distilling the knowledge in Transformer to Mamba-2. (b) By distilling from the quadratic-complexity decoder-only VLM HoVLE, our mmMamba-linear achieves competitive performance against existing linear and quadraticcomplexity VLMs with fewer parameters (e.g., 2 fewer than EVE-7B), while mmMamba-hybrid surpasses them across all benchmarks and approaches the teacher model HoVLEs performance. (c)-(d) We compare the speed and memory of mmMamba-linear and mmMamba-hybrid with the teacher model HoVLE on the same single NVIDIA 4090 GPU. mmMamba-linear maintains consistently low latency and memory usage, while mmMamba-hybrids resource consumption scales significantly better than HoVLE. At 103K tokens, mmMamba-linear demonstrates 20.6 speedup compared to HoVLE and saves 75.8% GPU memory, while mmMamba-hybrid achieves 13.5 speedup and saves 60.2% GPU memory.Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, framework for developing linearcomplexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computa- *Equal contribution Intelligence, Huazhong University of Science & Technology 2School of EIC, Huazhong University of Science & Technology 3Horizon Robotics. Correspondence to: Xinggang Wang <xgwang@hust.edu.cn>. 1Institute of Artificial tional resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoderonly HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMambahybrid further improves performance significantly, Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation approaching HoVLEs capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6 speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5 speedup and 60.2% memory savings. Code and models are released at https: //github.com/hustvl/mmMamba 1. Introduction inadvances Language Models Recent (LLMs) (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023a;b; Dubey et al., 2024; Bai et al., 2023a; Jiang et al., 2023; Bi et al., 2024; Javaheripi et al., 2023) have catalyzed significant research interest in expanding their capabilities beyond text to encompass multimodal understanding, particularly in processing both visual and textual information simultaneously. This expansion has given rise to Multimodal Large Language Models (MLLMs), with Vision Language Models (VLMs) emerging as prominent subset. Notable examples such as LLaVA (Liu et al., 2024a), BLIP (Li et al., 2022), Qwen-VL (Bai et al., 2023b), InternVL (Chen et al., 2024b), and Monkey (Li et al., 2024b) have demonstrated remarkable success in enhancing LLMs visual comprehension capabilities through the integration of pre-trained vision encoders and specialized connectors that bridge the modality gap between vision and language. While these encoder-based compositional VLMs have achieved state-of-the-art (SOTA) performance and established themselves as the de-facto paradigm, they face two critical limitations. First, processing long contexts becomes prohibitively expensive due to the quadratic computational complexity and linearly growing Key-Value (KV) cache with respect to sequence length. This limitation becomes particularly problematic given the increasing demand for long chain-of-thought reasoning (Muennighoff et al., 2025; DeepSeek-AI et al., 2025; Team et al., 2025; Xu et al., 2024) and high-resolution image/video understanding (Chen et al., 2023; Li et al., 2024a). Second, their heterogeneous architecture heavily relies on pre-trained vision encoders, introducing significant complexity in both training procedures and deployment scenarios (Chen et al., 2024a). Current research efforts to address these limitations have followed two distinct paths. One approach focuses on developing linear-complexity VLMs by adhering to the conventional encoder-based recipe, which requires both pre-trained vision encoders and pre-trained linear-complexity language models (Hou et al., 2024; Qiao et al., 2024). The alternative approach aims to enhance decoder-only VLMs through increased model scale and expanded training datasets, achieving performance competitive with encoder-based counter2 parts (Bavishi et al., 2023; Diao et al., 2024; Tao et al., 2024; Team, 2024; Wang et al., 2024b). the development of Despite these advances, linearcomplexity decoder-only MLLMs remains an understudied yet critical challenge. Addressing this gap holds substantial value for three key reasons: (1) Unified multimodal understanding: Such models could seamlessly integrate multimodal reasoning within single architecture, eliminating the need for heterogeneous, modality-specific frameworks. (2) Practical efficiency: Linear-complexity models inherently reduce computational demands during both training and inference, lowering costs and enabling deployment on resource-constrained edge devices. (3) Untapped Potential: While recent linear-time models like Mamba-2 demonstrate high text-processing capabilities, their ability to handle multimodal tasksparticularly in cross-modal alignment and reasoningremains largely unexplored. The research of linear-complexity decoder-only MLLMs could unlock scalable, cost-effective multimodal systems without sacrificing performance. straightforward solution"
[19.02.2025 03:16] Mistral response. {"id": "7969eed31d984408a3181ee0f2d76735", "object": "chat.completion", "created": 1739934983, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['Institute of Artificial Intelligence, Huazhong University of Science & Technology', 'School of EIC, Huazhong University of Science & Technology', 'Horizon Robotics']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1806, "total_tokens": 1854, "completion_tokens": 48}}
[19.02.2025 03:16] Response: ```python
['Institute of Artificial Intelligence, Huazhong University of Science & Technology', 'School of EIC, Huazhong University of Science & Technology', 'Horizon Robotics']
```
[19.02.2025 03:16] Deleting PDF ./assets/pdf/2502.13145.pdf.
[19.02.2025 03:16] Success.
[19.02.2025 03:16] Downloading and parsing paper https://huggingface.co/papers/2502.11433.
[19.02.2025 03:16] Downloading paper 2502.11433 from http://arxiv.org/pdf/2502.11433v2...
[19.02.2025 03:16] Extracting affiliations from text.
[19.02.2025 03:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"FLAG-TRADER: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading Guojun Xiong1, Zhiyang Deng2, Keyi Wang3, Yupeng Cao2, Haohang Li2, Yangyang Yu2, Xueqing Peng7, Mingquan Lin4, Kaleb Smith5, Xiao-Yang Liu Yanglet3,6, Jimin Huang7, Sophia Ananiadou8, Qianqian Xie7,* 1Harvard University, 2Stevens Institute of Technology, 3Columbia University, 4University of Minnesota, 5NVIDIA, 6Rensselaer Polytechnic Institute, 7TheFinAI, 8University of Manchester *Correspondence: xqq.sincere@gmail.com "
[19.02.2025 03:16] Response: ```python
[
    "Harvard University",
    "Stevens Institute of Technology",
    "Columbia University",
    "University of Minnesota",
    "NVIDIA",
    "Rensselaer Polytechnic Institute",
    "TheFinAI",
    "University of Manchester"
]
```
[19.02.2025 03:16] Deleting PDF ./assets/pdf/2502.11433.pdf.
[19.02.2025 03:16] Success.
[19.02.2025 03:16] Downloading and parsing paper https://huggingface.co/papers/2502.11079.
[19.02.2025 03:16] Downloading paper 2502.11079 from http://arxiv.org/pdf/2502.11079v1...
[19.02.2025 03:16] Extracting affiliations from text.
[19.02.2025 03:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 9 7 0 1 1 . 2 0 5 2 : r PHANTOM: SUBJECT-CONSISTENT VIDEO GENERATION VIA CROSS-MODAL ALIGNMENT Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Qian He, Xinglong Wu Intelligent Creation Team, ByteDance "
[19.02.2025 03:16] Response: ```python
["Intelligent Creation Team, ByteDance"]
```
[19.02.2025 03:16] Deleting PDF ./assets/pdf/2502.11079.pdf.
[19.02.2025 03:16] Success.
[19.02.2025 03:16] Downloading and parsing paper https://huggingface.co/papers/2502.12574.
[19.02.2025 03:16] Downloading paper 2502.12574 from http://arxiv.org/pdf/2502.12574v1...
[19.02.2025 03:16] Extracting affiliations from text.
[19.02.2025 03:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading Cheng Luo 1 Zefan Cai 2 Hanshi Sun 3 Jinqi Xiao 4 Bo Yuan 4 Wen Xiao 5 Junjie Hu 2 Jiawei Zhao 1 Beidi Chen 3 Anima Anandkumar 1 5 2 0 2 8 1 ] . [ 1 4 7 5 2 1 . 2 0 5 2 : r Abstract Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs fine-grained, head-wise offloading strategy, maintaining only selective attention heads KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods. 1. Introduction Modern Large Language Models (LLMs) increasingly support extremely long inputs: Llama-3 (Dubey et al., 2024) handles up to 128K tokens, Claude (Anthropic, 2024) supports up to 1 million tokens, while Gradient AI (Pekelis et al., 2024) extends Llama-3 to 4 million tokens. These extended context lengths improve performance on tasks such as book summarization (Pal et al., 2023) and video generation (Liu et al., 2024b), requiring millions of tokens. 1California Institute of Technology 2University of Wisconsin-Madison 3Carnegie Mellon University 4Rutgers University 5Microsoft. Cor"
[19.02.2025 03:16] Response: ```python
[
    "California Institute of Technology",
    "University of Wisconsin-Madison",
    "Carnegie Mellon University",
    "Rutgers University",
    "Microsoft"
]
```
[19.02.2025 03:16] Deleting PDF ./assets/pdf/2502.12574.pdf.
[19.02.2025 03:16] Success.
[19.02.2025 03:16] Enriching papers with extra data.
[19.02.2025 03:16] ********************************************************************************
[19.02.2025 03:16] Abstract 0. After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language r...
[19.02.2025 03:16] ********************************************************************************
[19.02.2025 03:16] Abstract 1. Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collec...
[19.02.2025 03:16] ********************************************************************************
[19.02.2025 03:16] Abstract 2. LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly re...
[19.02.2025 03:16] ********************************************************************************
[19.02.2025 03:16] Abstract 3. Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand ...
[19.02.2025 03:16] ********************************************************************************
[19.02.2025 03:16] Abstract 4. Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-c...
[19.02.2025 03:16] ********************************************************************************
[19.02.2025 03:16] Abstract 5. Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approach...
[19.02.2025 03:16] ********************************************************************************
[19.02.2025 03:16] Abstract 6. The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-con...
[19.02.2025 03:16] ********************************************************************************
[19.02.2025 03:16] Abstract 7. Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloa...
[19.02.2025 03:16] Read previous papers.
[19.02.2025 03:16] Generating reviews via LLM API.
[19.02.2025 03:16] Querying the API.
[19.02.2025 03:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. Extensive experiments demonstrate that RealSyn effectively advances vision-language representation learning and exhibits strong scalability. Models pre-trained on RealSyn achieve state-of-the-art performance on multiple downstream tasks. To facilitate future research, the RealSyn dataset and pre-trained model weights are released at https://github.com/deepglint/RealSyn.
[19.02.2025 03:16] Response: {
  "desc": "Статья представляет новый подход к обучению мультимодальных моделей, использующих изображения и текст. Авторы разработали метод RealSyn, который извлекает высококачественные данные из непарных документов и создает синтетические тексты для улучшения визуальной информации. Они также применили стратегию семантически сбалансированной выборки для повышения разнообразия данных. Эксперименты показали, что модели, предобученные на датасете RealSyn, достигают наилучших результатов в различных задачах компьютерного зрения и обработки естественного языка.",
  "emoji": "🖼️",
  "title": "RealSyn: Улучшение мультимодального обучения с помощью реальных и синтетических данных"
}
[19.02.2025 03:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. Extensive experiments demonstrate that RealSyn effectively advances vision-language representation learning and exhibits strong scalability. Models pre-trained on RealSyn achieve state-of-the-art performance on multiple downstream tasks. To facilitate future research, the RealSyn dataset and pre-trained model weights are released at https://github.com/deepglint/RealSyn."

[19.02.2025 03:16] Response: ```python
['DATASET', 'DATA', 'BENCHMARK', 'MULTIMODAL']
```
[19.02.2025 03:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. Extensive experiments demonstrate that RealSyn effectively advances vision-language representation learning and exhibits strong scalability. Models pre-trained on RealSyn achieve state-of-the-art performance on multiple downstream tasks. To facilitate future research, the RealSyn dataset and pre-trained model weights are released at https://github.com/deepglint/RealSyn."

[19.02.2025 03:16] Response: ```python
['SYNTHETIC', 'OPEN_SOURCE']
```
[19.02.2025 03:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces RealSyn, a new dataset designed to improve vision-language representation learning by utilizing both realistic and synthetic texts. The authors develop a data extraction pipeline to gather high-quality images and texts from unpaired multimodal documents. They also implement a hierarchical retrieval method to link images with relevant texts and propose an image semantic augmented generation module to create synthetic text. The resulting dataset, RealSyn, shows significant improvements in model performance on various tasks, demonstrating its effectiveness and scalability in the field of machine learning.","title":"Unlocking Vision-Language Learning with RealSyn Dataset"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces RealSyn, a new dataset designed to improve vision-language representation learning by utilizing both realistic and synthetic texts. The authors develop a data extraction pipeline to gather high-quality images and texts from unpaired multimodal documents. They also implement a hierarchical retrieval method to link images with relevant texts and propose an image semantic augmented generation module to create synthetic text. The resulting dataset, RealSyn, shows significant improvements in model performance on various tasks, demonstrating its effectiveness and scalability in the field of machine learning.', title='Unlocking Vision-Language Learning with RealSyn Dataset'))
[19.02.2025 03:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"这篇论文介绍了一种新的数据集RealSyn，用于视觉-语言表示学习。研究者们通过提取高质量的图像和文本，利用未配对的数据来提升模型的性能。为了更好地关联图像和文本，设计了一种层次检索方法，并提出了图像语义增强生成模块来生成合成文本。实验结果表明，基于RealSyn预训练的模型在多个下游任务上达到了最先进的性能。","title":"利用未配对数据提升视觉-语言学习的创新方法"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='这篇论文介绍了一种新的数据集RealSyn，用于视觉-语言表示学习。研究者们通过提取高质量的图像和文本，利用未配对的数据来提升模型的性能。为了更好地关联图像和文本，设计了一种层次检索方法，并提出了图像语义增强生成模块来生成合成文本。实验结果表明，基于RealSyn预训练的模型在多个下游任务上达到了最先进的性能。', title='利用未配对数据提升视觉-语言学习的创新方法'))
[19.02.2025 03:16] Querying the API.
[19.02.2025 03:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment.
[19.02.2025 03:16] Response: {
  "desc": "Статья представляет новый подход к извлечению разнообразных человеческих предпочтений из бинарных сравнений без необходимости в детальных аннотациях - Decomposed Reward Models (DRMs). Метод использует векторное представление предпочтений и анализ методом главных компонент (PCA) для выявления ортогональных базисных векторов, отражающих различные аспекты предпочтений. DRMs позволяют гибко комбинировать разложенные награды для адаптации к потребностям разных пользователей, предлагая интерпретируемую и масштабируемую альтернативу традиционным моделям вознаграждения. Эксперименты показывают, что DRMs эффективно извлекают значимые измерения предпочтений и адаптируются к новым пользователям без дополнительного обучения.",
  "emoji": "🧩",
  "title": "Разложение предпочтений для персонализированного обучения языковых моделей"
}
[19.02.2025 03:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment."

[19.02.2025 03:17] Response: ```python
["RLHF", "DATASET", "TRAINING"]
```
[19.02.2025 03:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment."

[19.02.2025 03:17] Response: ```python
["ALIGNMENT", "INTERPRETABILITY"]
```
[19.02.2025 03:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents Decomposed Reward Models (DRMs), a new method for understanding human preferences in AI systems. Instead of relying on detailed preference data, DRMs use binary comparisons to extract diverse preferences, making the process more scalable and cost-effective. By representing preferences as vectors and applying Principal Component Analysis (PCA), the model identifies key dimensions of preference that can be combined to meet different user needs. The results show that DRMs can effectively adapt to new users and provide interpretable insights into preferences like helpfulness and safety.","title":"Decomposed Reward Models: Unlocking Diverse Human Preferences for Personalized AI"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents Decomposed Reward Models (DRMs), a new method for understanding human preferences in AI systems. Instead of relying on detailed preference data, DRMs use binary comparisons to extract diverse preferences, making the process more scalable and cost-effective. By representing preferences as vectors and applying Principal Component Analysis (PCA), the model identifies key dimensions of preference that can be combined to meet different user needs. The results show that DRMs can effectively adapt to new users and provide interpretable insights into preferences like helpfulness and safety.', title='Decomposed Reward Models: Unlocking Diverse Human Preferences for Personalized AI'))
[19.02.2025 03:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"理解人类偏好对于改善基础模型和构建个性化AI系统至关重要。传统的奖励模型难以捕捉偏好的多样性和复杂性。我们提出了一种新方法，称为分解奖励模型（DRMs），它通过二元比较提取人类偏好，而无需细粒度的注释。DRMs利用主成分分析（PCA）分析偏好向量，能够灵活组合以满足不同用户需求，提供了一种可解释且可扩展的替代方案。","title":"分解奖励模型：个性化AI的新方法"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='理解人类偏好对于改善基础模型和构建个性化AI系统至关重要。传统的奖励模型难以捕捉偏好的多样性和复杂性。我们提出了一种新方法，称为分解奖励模型（DRMs），它通过二元比较提取人类偏好，而无需细粒度的注释。DRMs利用主成分分析（PCA）分析偏好向量，能够灵活组合以满足不同用户需求，提供了一种可解释且可扩展的替代方案。', title='分解奖励模型：个性化AI的新方法'))
[19.02.2025 03:17] Querying the API.
[19.02.2025 03:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly rely on majority voting or criteria expansion, which is insufficient to address the limitation in CoT. We propose Crowd-based Comparative Evaluation, which introduces additional crowd responses to compare with the candidate responses, thereby exposing deeper and more comprehensive details within the candidate responses. This process effectively guides LLM-as-a-Judge to provide a more detailed CoT judgment. Extensive experiments demonstrate that our approach enhances evaluation reliability, achieving an average accuracy gain of 6.7% across five benchmarks. Moreover, our method produces higher-quality CoTs that facilitate judge distillation and exhibit superior performance in rejection sampling for supervised fine-tuning (SFT), referred to as crowd rejection sampling, thereby enabling more efficient SFT. Our analysis confirms that CoTs generated by ours are more comprehensive and of higher quality, and evaluation accuracy improves as inference scales.
[19.02.2025 03:17] Response: {
  "desc": "Статья представляет новый метод оценки моделей машинного обучения под названием 'Crowd-based Comparative Evaluation'. Этот подход улучшает традиционный метод LLM-as-a-Judge, добавляя сравнение с ответами толпы для более глубокого анализа. Эксперименты показали повышение точности оценки в среднем на 6.7% на пяти бенчмарках. Метод также демонстрирует улучшенное качество рассуждений (Chain-of-Thought) и эффективность в отборе данных для обучения с учителем.",

  "emoji": "🧠",

  "title": "Улучшение оценки ИИ через сравнение с мнением толпы"
}
[19.02.2025 03:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly rely on majority voting or criteria expansion, which is insufficient to address the limitation in CoT. We propose Crowd-based Comparative Evaluation, which introduces additional crowd responses to compare with the candidate responses, thereby exposing deeper and more comprehensive details within the candidate responses. This process effectively guides LLM-as-a-Judge to provide a more detailed CoT judgment. Extensive experiments demonstrate that our approach enhances evaluation reliability, achieving an average accuracy gain of 6.7% across five benchmarks. Moreover, our method produces higher-quality CoTs that facilitate judge distillation and exhibit superior performance in rejection sampling for supervised fine-tuning (SFT), referred to as crowd rejection sampling, thereby enabling more efficient SFT. Our analysis confirms that CoTs generated by ours are more comprehensive and of higher quality, and evaluation accuracy improves as inference scales."

[19.02.2025 03:17] Response: ```python
["BENCHMARK", "TRAINING", "INFERENCE"]
```
[19.02.2025 03:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly rely on majority voting or criteria expansion, which is insufficient to address the limitation in CoT. We propose Crowd-based Comparative Evaluation, which introduces additional crowd responses to compare with the candidate responses, thereby exposing deeper and more comprehensive details within the candidate responses. This process effectively guides LLM-as-a-Judge to provide a more detailed CoT judgment. Extensive experiments demonstrate that our approach enhances evaluation reliability, achieving an average accuracy gain of 6.7% across five benchmarks. Moreover, our method produces higher-quality CoTs that facilitate judge distillation and exhibit superior performance in rejection sampling for supervised fine-tuning (SFT), referred to as crowd rejection sampling, thereby enabling more efficient SFT. Our analysis confirms that CoTs generated by ours are more comprehensive and of higher quality, and evaluation accuracy improves as inference scales."

[19.02.2025 03:17] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[19.02.2025 03:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the limitations of the LLM-as-a-Judge method, which generates chain-of-thought (CoT) judgments for auto-evaluation. The authors identify that CoT reasoning often lacks depth and comprehensiveness, leading to incomplete evaluations. To overcome this, they propose a new method called Crowd-based Comparative Evaluation, which incorporates additional crowd responses to enhance the evaluation process. Their experiments show that this approach improves the reliability of evaluations, increases accuracy by 6.7%, and produces higher-quality CoTs that benefit supervised fine-tuning.","title":"Enhancing LLM Evaluations with Crowd Insights"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the limitations of the LLM-as-a-Judge method, which generates chain-of-thought (CoT) judgments for auto-evaluation. The authors identify that CoT reasoning often lacks depth and comprehensiveness, leading to incomplete evaluations. To overcome this, they propose a new method called Crowd-based Comparative Evaluation, which incorporates additional crowd responses to enhance the evaluation process. Their experiments show that this approach improves the reliability of evaluations, increases accuracy by 6.7%, and produces higher-quality CoTs that benefit supervised fine-tuning.', title='Enhancing LLM Evaluations with Crowd Insights'))
[19.02.2025 03:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新的评估方法，称为基于人群的比较评估（Crowd-based Comparative Evaluation），旨在提高大型语言模型（LLM）作为评判者的可靠性。传统的链式推理（CoT）方法常常无法捕捉到全面和深入的细节，导致评估结果不完整。我们的方法通过引入额外的人群反馈，与候选响应进行比较，从而揭示候选响应中的更深层次和更全面的细节。实验结果表明，该方法在五个基准测试中平均提高了6.7%的评估准确性，并生成了更高质量的CoT，促进了监督微调的效率。","title":"提升LLM评估可靠性的创新方法"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新的评估方法，称为基于人群的比较评估（Crowd-based Comparative Evaluation），旨在提高大型语言模型（LLM）作为评判者的可靠性。传统的链式推理（CoT）方法常常无法捕捉到全面和深入的细节，导致评估结果不完整。我们的方法通过引入额外的人群反馈，与候选响应进行比较，从而揭示候选响应中的更深层次和更全面的细节。实验结果表明，该方法在五个基准测试中平均提高了6.7%的评估准确性，并生成了更高质量的CoT，促进了监督微调的效率。', title='提升LLM评估可靠性的创新方法'))
[19.02.2025 03:17] Querying the API.
[19.02.2025 03:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand object orientations-a key requirement for tasks involving fine-grained manipulations. Addressing this limitation not only requires geometric reasoning but also an expressive and intuitive way to represent orientation. In this context, we propose that natural language offers a more flexible representation space than canonical frames, making it particularly suitable for instruction-following robotic systems. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the ''handle'' direction of a knife). To support this, we construct OrienText300K, a large-scale dataset of 3D models annotated with semantic orientations that link geometric understanding to functional semantics. By integrating semantic orientation into a VLM system, we enable robots to generate manipulation actions with both positional and orientational constraints. Extensive experiments in simulation and real world demonstrate that our approach significantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy on Open6DOR and 74.9% accuracy on SIMPLER.
[19.02.2025 03:17] Response: {
  "desc": "Статья представляет концепцию семантической ориентации для улучшения пространственного интеллекта роботов. Авторы создали датасет OrienText300K с 3D моделями, аннотированными семантическими ориентациями на естественном языке. Интеграция семантической ориентации в систему с визуально-языковой моделью (VLM) позволяет роботам генерировать действия с учетом позиционных и ориентационных ограничений. Эксперименты показали значительное улучшение возможностей роботов в манипуляциях с объектами.",
  "emoji": "🤖",
  "title": "Семантическая ориентация: новый подход к пространственному интеллекту роботов"
}
[19.02.2025 03:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand object orientations-a key requirement for tasks involving fine-grained manipulations. Addressing this limitation not only requires geometric reasoning but also an expressive and intuitive way to represent orientation. In this context, we propose that natural language offers a more flexible representation space than canonical frames, making it particularly suitable for instruction-following robotic systems. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the ''handle'' direction of a knife). To support this, we construct OrienText300K, a large-scale dataset of 3D models annotated with semantic orientations that link geometric understanding to functional semantics. By integrating semantic orientation into a VLM system, we enable robots to generate manipulation actions with both positional and orientational constraints. Extensive experiments in simulation and real world demonstrate that our approach significantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy on Open6DOR and 74.9% accuracy on SIMPLER."

[19.02.2025 03:17] Response: ```python
['DATASET', '3D', 'ROBOTICS']
```
[19.02.2025 03:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand object orientations-a key requirement for tasks involving fine-grained manipulations. Addressing this limitation not only requires geometric reasoning but also an expressive and intuitive way to represent orientation. In this context, we propose that natural language offers a more flexible representation space than canonical frames, making it particularly suitable for instruction-following robotic systems. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the ''handle'' direction of a knife). To support this, we construct OrienText300K, a large-scale dataset of 3D models annotated with semantic orientations that link geometric understanding to functional semantics. By integrating semantic orientation into a VLM system, we enable robots to generate manipulation actions with both positional and orientational constraints. Extensive experiments in simulation and real world demonstrate that our approach significantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy on Open6DOR and 74.9% accuracy on SIMPLER."

[19.02.2025 03:17] Response: ```python
["REASONING", "GAMES"]
```
[19.02.2025 03:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenge of teaching robots to understand object orientations, which is essential for precise manipulation tasks. It introduces the concept of semantic orientation, allowing robots to interpret orientations using natural language instead of traditional geometric frames. The authors present OrienText300K, a dataset of 3D models annotated with these semantic orientations, linking geometric understanding to functional semantics. By incorporating this approach into vision-language models (VLMs), the paper demonstrates improved accuracy in robotic manipulation tasks, showcasing the effectiveness of using natural language for orientation representation.","title":"Empowering Robots with Semantic Orientation for Enhanced Manipulation"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenge of teaching robots to understand object orientations, which is essential for precise manipulation tasks. It introduces the concept of semantic orientation, allowing robots to interpret orientations using natural language instead of traditional geometric frames. The authors present OrienText300K, a dataset of 3D models annotated with these semantic orientations, linking geometric understanding to functional semantics. By incorporating this approach into vision-language models (VLMs), the paper demonstrates improved accuracy in robotic manipulation tasks, showcasing the effectiveness of using natural language for orientation representation.', title='Empowering Robots with Semantic Orientation for Enhanced Manipulation'))
[19.02.2025 03:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"空间智能是具身人工智能的重要组成部分，帮助机器人理解和与环境互动。尽管最近的进展提高了视觉语言模型（VLMs）对物体位置和关系的感知能力，但它们仍然缺乏精确理解物体方向的能力，这对于细致操作任务至关重要。为了解决这个问题，我们提出了语义方向的概念，使用自然语言以无参考框架的方式定义物体方向。通过构建OrienText300K数据集，我们将几何理解与功能语义联系起来，从而提升机器人在操作中的能力。","title":"用自然语言提升机器人的空间智能"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='空间智能是具身人工智能的重要组成部分，帮助机器人理解和与环境互动。尽管最近的进展提高了视觉语言模型（VLMs）对物体位置和关系的感知能力，但它们仍然缺乏精确理解物体方向的能力，这对于细致操作任务至关重要。为了解决这个问题，我们提出了语义方向的概念，使用自然语言以无参考框架的方式定义物体方向。通过构建OrienText300K数据集，我们将几何理解与功能语义联系起来，从而提升机器人在操作中的能力。', title='用自然语言提升机器人的空间智能'))
[19.02.2025 03:17] Querying the API.
[19.02.2025 03:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6times speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5times speedup and 60.2% memory savings. Code and models are released at https://github.com/hustvl/mmMamba
[19.02.2025 03:17] Response: {
  "desc": "Статья представляет mmMamba - фреймворк для разработки мультимодальных моделей с линейной сложностью на основе state space models. Авторы предлагают метод прогрессивной дистилляции знаний от существующих мультимодальных языковых моделей (MLLM) к линейным архитектурам без использования предобученных RNN-моделей или энкодеров изображений. Предложенный подход включает стратегию инициализации Mamba из обученного Transformer и трехэтапный рецепт дистилляции. Результаты показывают, что mmMamba достигает конкурентоспособной производительности по сравнению с существующими моделями, обеспечивая значительное ускорение и экономию памяти.",

  "emoji": "🚀",

  "title": "mmMamba: эффективные мультимодальные модели с линейной сложностью"
}
[19.02.2025 03:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6times speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5times speedup and 60.2% memory savings. Code and models are released at https://github.com/hustvl/mmMamba"

[19.02.2025 03:17] Response: ```python
['MULTIMODAL', 'ARCHITECTURE', 'TRAINING']
```
[19.02.2025 03:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6times speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5times speedup and 60.2% memory savings. Code and models are released at https://github.com/hustvl/mmMamba"

[19.02.2025 03:17] Response: ```python
["OPTIMIZATION", "TRANSFER_LEARNING", "OPEN_SOURCE"]
```
[19.02.2025 03:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces mmMamba, a new framework designed to create efficient multimodal large language models (MLLMs) with linear computational complexity. It utilizes a progressive distillation process to convert existing decoder-only MLLMs into more efficient architectures without needing separate vision encoders or pre-trained RNNs. The proposed seeding strategy and three-stage distillation recipe allow for effective knowledge transfer from Transformer models while maintaining multimodal capabilities. The results show that mmMamba-linear and mmMamba-hybrid significantly outperform traditional models in terms of speed and memory usage, making them suitable for practical deployment.","title":"Efficient Multimodal Models with mmMamba"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces mmMamba, a new framework designed to create efficient multimodal large language models (MLLMs) with linear computational complexity. It utilizes a progressive distillation process to convert existing decoder-only MLLMs into more efficient architectures without needing separate vision encoders or pre-trained RNNs. The proposed seeding strategy and three-stage distillation recipe allow for effective knowledge transfer from Transformer models while maintaining multimodal capabilities. The results show that mmMamba-linear and mmMamba-hybrid significantly outperform traditional models in terms of speed and memory usage, making them suitable for practical deployment.', title='Efficient Multimodal Models with mmMamba'))
[19.02.2025 03:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"最近的多模态大型语言模型（MLLMs）在性能上取得了显著进展，但由于其计算复杂度呈平方增长、对键值缓存的需求增加以及依赖于独立的视觉编码器，面临部署挑战。我们提出了mmMamba框架，通过从现有的MLLMs进行渐进蒸馏，开发线性复杂度的本地多模态状态空间模型，使用适度的学术计算资源。该方法允许将训练好的仅解码器MLLMs直接转换为线性复杂度架构，而无需预训练的基于RNN的LLM或视觉编码器。我们的蒸馏策略有效地将知识从Transformer转移到Mamba，同时保留多模态能力，并支持灵活的混合架构，以实现可定制的效率与性能权衡。","title":"mmMamba：高效的多模态模型架构"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='最近的多模态大型语言模型（MLLMs）在性能上取得了显著进展，但由于其计算复杂度呈平方增长、对键值缓存的需求增加以及依赖于独立的视觉编码器，面临部署挑战。我们提出了mmMamba框架，通过从现有的MLLMs进行渐进蒸馏，开发线性复杂度的本地多模态状态空间模型，使用适度的学术计算资源。该方法允许将训练好的仅解码器MLLMs直接转换为线性复杂度架构，而无需预训练的基于RNN的LLM或视觉编码器。我们的蒸馏策略有效地将知识从Transformer转移到Mamba，同时保留多模态能力，并支持灵活的混合架构，以实现可定制的效率与性能权衡。', title='mmMamba：高效的多模态模型架构'))
[19.02.2025 03:17] Querying the API.
[19.02.2025 03:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose FLAG-Trader, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.
[19.02.2025 03:17] Response: {
  "desc": "В статье представлена архитектура FLAG-Trader, объединяющая языковую обработку с помощью больших языковых моделей (БЯМ) и обучение с подкреплением для оптимизации торговых стратегий. БЯМ используется в качестве политики, адаптируясь к финансовой области через эффективную дообучение. Метод улучшает принятие решений в интерактивных финансовых сценариях, таких как торговля. Эмпирические результаты подтверждают эффективность подхода не только в торговле, но и в других финансовых задачах.",
  "emoji": "📈",
  "title": "Усиление торговых стратегий с помощью языковых моделей и обучения с подкреплением"
}
[19.02.2025 03:17] Renaming some terms.
[19.02.2025 03:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose FLAG-Trader, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements."

[19.02.2025 03:17] Response: ```python
['RL', 'RLHF', 'MULTIMODAL', 'ARCHITECTURE', 'TRAINING']
```
[19.02.2025 03:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose FLAG-Trader, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements."

[19.02.2025 03:17] Response: ```python
['REASONING', 'OPTIMIZATION']
```
[19.02.2025 03:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces FLAG-Trader, a new architecture that combines large language models (LLMs) with reinforcement learning (RL) to improve decision-making in financial trading. The approach uses a partially fine-tuned LLM as a policy network, allowing it to utilize its pre-trained knowledge while adapting specifically to financial tasks. By applying policy gradient optimization based on trading rewards, FLAG-Trader enhances the performance of LLMs in trading scenarios and other financial applications. The authors provide extensive empirical evidence demonstrating the effectiveness of their proposed method.","title":"Empowering Financial Trading with FLAG-Trader: Merging Language and Reinforcement Learning"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces FLAG-Trader, a new architecture that combines large language models (LLMs) with reinforcement learning (RL) to improve decision-making in financial trading. The approach uses a partially fine-tuned LLM as a policy network, allowing it to utilize its pre-trained knowledge while adapting specifically to financial tasks. By applying policy gradient optimization based on trading rewards, FLAG-Trader enhances the performance of LLMs in trading scenarios and other financial applications. The authors provide extensive empirical evidence demonstrating the effectiveness of their proposed method.', title='Empowering Financial Trading with FLAG-Trader: Merging Language and Reinforcement Learning'))
[19.02.2025 03:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种名为FLAG-Trader的统一架构，旨在提升大型语言模型（LLMs）在金融市场中的决策能力。该架构结合了语言处理和基于梯度的强化学习（RL）策略优化，使得部分微调的LLM可以作为策略网络，利用预训练知识并适应金融领域。通过交易奖励驱动的策略梯度优化，FLAG-Trader不仅提高了LLM在交易中的表现，还改善了其他金融领域任务的结果。我们提供了大量实证证据来验证这些改进。","title":"FLAG-Trader：提升金融决策的智能交易架构"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种名为FLAG-Trader的统一架构，旨在提升大型语言模型（LLMs）在金融市场中的决策能力。该架构结合了语言处理和基于梯度的强化学习（RL）策略优化，使得部分微调的LLM可以作为策略网络，利用预训练知识并适应金融领域。通过交易奖励驱动的策略梯度优化，FLAG-Trader不仅提高了LLM在交易中的表现，还改善了其他金融领域任务的结果。我们提供了大量实证证据来验证这些改进。', title='FLAG-Trader：提升金融决策的智能交易架构'))
[19.02.2025 03:17] Querying the API.
[19.02.2025 03:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/.
[19.02.2025 03:17] Response: {
  "desc": "Эта статья представляет Phantom - унифицированную систему генерации видео на основе текстовых и визуальных подсказок. Авторы предлагают новый подход к созданию видео с сохранением характеристик субъектов из опорных изображений. Phantom переосмысливает модель совместной обработки текста и изображений, обучаясь на триплетах текст-изображение-видео. Особое внимание уделяется сохранению идентичности людей при генерации видео.",
  "emoji": "🎥",
  "title": "Phantom: баланс текста и изображения для создания согласованного видео"
}
[19.02.2025 03:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/."

[19.02.2025 03:17] Response: ```python
['VIDEO', 'MULTIMODAL', 'ARCHITECTURE']
```
[19.02.2025 03:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/."

[19.02.2025 03:17] Response: ```python
[]
```
[19.02.2025 03:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new approach called Subject-to-Video, which focuses on generating videos that maintain consistency with specific subjects extracted from reference images. The authors present Phantom, a unified framework that integrates both text and image inputs to create videos, ensuring that the generated content aligns well with the provided prompts. By utilizing a triplet data structure of text, image, and video, Phantom enhances the learning of cross-modal relationships, improving the quality of video generation. The framework particularly excels in generating videos of humans while preserving their identity across frames, marking a significant advancement in video generation technology.","title":"Phantom: Consistent Video Generation from Text and Images"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new approach called Subject-to-Video, which focuses on generating videos that maintain consistency with specific subjects extracted from reference images. The authors present Phantom, a unified framework that integrates both text and image inputs to create videos, ensuring that the generated content aligns well with the provided prompts. By utilizing a triplet data structure of text, image, and video, Phantom enhances the learning of cross-modal relationships, improving the quality of video generation. The framework particularly excels in generating videos of humans while preserving their identity across frames, marking a significant advancement in video generation technology.', title='Phantom: Consistent Video Generation from Text and Images'))
[19.02.2025 03:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"这篇论文介绍了一种新的视频生成模型，称为Phantom，旨在实现主题一致性的视频生成。该模型通过提取参考图像中的主题元素，并结合文本指令生成视频。Phantom框架能够处理单一和多个主题的参考，强调文本和图像的双模态提示的平衡。研究者们通过文本-图像-视频三元组数据来学习跨模态对齐，从而提升人类生成视频的主题一致性。","title":"实现主题一致性的视频生成"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='这篇论文介绍了一种新的视频生成模型，称为Phantom，旨在实现主题一致性的视频生成。该模型通过提取参考图像中的主题元素，并结合文本指令生成视频。Phantom框架能够处理单一和多个主题的参考，强调文本和图像的双模态提示的平衡。研究者们通过文本-图像-视频三元组数据来学习跨模态对齐，从而提升人类生成视频的主题一致性。', title='实现主题一致性的视频生成'))
[19.02.2025 03:17] Querying the API.
[19.02.2025 03:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise offloading strategy, maintaining only selective attention heads KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with a 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods.
[19.02.2025 03:17] Response: {
  "desc": "HEADINFER - это новый метод для оптимизации памяти при работе с большими языковыми моделями (LLM). Он позволяет выгружать кэш ключей-значений (KV cache) в оперативную память CPU, значительно снижая нагрузку на GPU. Используя стратегию выборочного хранения данных для отдельных голов внимания, HEADINFER сохраняет вычислительную эффективность при существенном уменьшении объема используемой памяти. Метод был успешно протестирован на модели Llama-3-8B с последовательностью в 1 миллион токенов, сократив использование памяти GPU на 92% по сравнению с базовым методом.",
  "emoji": "🧠",
  "title": "HEADINFER: эффективное использование памяти для LLM с длинным контекстом"
}
[19.02.2025 03:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise offloading strategy, maintaining only selective attention heads KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with a 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods."

[19.02.2025 03:17] Response: ```python
["INFERENCE", "TRAINING", "ARCHITECTURE"]
```
[19.02.2025 03:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise offloading strategy, maintaining only selective attention heads KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with a 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods."

[19.02.2025 03:17] Response: ```python
["LONG_CONTEXT", "OPTIMIZATION"]
```
[19.02.2025 03:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces HEADINFER, a novel approach to manage the memory usage of large language models (LLMs) during long context generation. By offloading the key-value (KV) cache to CPU RAM, HEADINFER reduces the reliance on GPU memory, which is crucial for efficient inference. The method employs a fine-grained, head-wise offloading strategy, allowing selective storage of attention heads on the GPU while dynamically computing attention outputs. Evaluation on the Llama-3-8B model shows a remarkable reduction in GPU memory usage, enabling efficient processing of extremely long sequences without compromising performance.","title":"HEADINFER: Efficient Memory Management for Long Contexts in LLMs"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces HEADINFER, a novel approach to manage the memory usage of large language models (LLMs) during long context generation. By offloading the key-value (KV) cache to CPU RAM, HEADINFER reduces the reliance on GPU memory, which is crucial for efficient inference. The method employs a fine-grained, head-wise offloading strategy, allowing selective storage of attention heads on the GPU while dynamically computing attention outputs. Evaluation on the Llama-3-8B model shows a remarkable reduction in GPU memory usage, enabling efficient processing of extremely long sequences without compromising performance.', title='HEADINFER: Efficient Memory Management for Long Contexts in LLMs'))
[19.02.2025 03:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种名为HEADINFER的方法，旨在优化大语言模型（LLM）在长上下文生成中的内存使用。通过将关键值缓存（KV缓存）转移到CPU RAM，HEADINFER避免了在GPU上完全存储KV缓存的需求。该方法采用细粒度的头部级别卸载策略，仅在GPU上保留选择性的注意力头KV缓存，同时动态计算注意力输出。实验结果表明，HEADINFER在显著减少内存占用的同时，保持了计算效率，使得在单个消费级GPU上实现了对长达400万标记的推理。","title":"HEADINFER：优化大语言模型的内存使用"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种名为HEADINFER的方法，旨在优化大语言模型（LLM）在长上下文生成中的内存使用。通过将关键值缓存（KV缓存）转移到CPU RAM，HEADINFER避免了在GPU上完全存储KV缓存的需求。该方法采用细粒度的头部级别卸载策略，仅在GPU上保留选择性的注意力头KV缓存，同时动态计算注意力输出。实验结果表明，HEADINFER在显著减少内存占用的同时，保持了计算效率，使得在单个消费级GPU上实现了对长达400万标记的推理。', title='HEADINFER：优化大语言模型的内存使用'))
[19.02.2025 03:18] Loading Chinese text from previous data.
[19.02.2025 03:18] Renaming data file.
[19.02.2025 03:18] Renaming previous data. hf_papers.json to ./d/2025-02-19.json
[19.02.2025 03:18] Saving new data file.
[19.02.2025 03:18] Generating page.
[19.02.2025 03:18] Renaming previous page.
[19.02.2025 03:18] Renaming previous data. index.html to ./d/2025-02-19.html
[19.02.2025 03:18] [Experimental] Generating Chinese page for reading.
[19.02.2025 03:18] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '人形机器人', 'pinyin': 'rén xíng jī qì rén', 'trans': 'humanoid robot'}, {'word': '自动', 'pinyin': 'zì dòng', 'trans': 'automatic'}, {'word': '跌倒', 'pinyin': 'diē dǎo', 'trans': 'fall down'}, {'word': '恢复', 'pinyin': 'huī fù', 'trans': 'recover'}, {'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'}, {'word': '设计', 'pinyin': 'shè jì', 'trans': 'design'}, {'word': '控制器', 'pinyin': 'kòng zhì qì', 'trans': 'controller'}, {'word': '站起来', 'pinyin': 'zhàn qǐ lái', 'trans': 'stand up'}, {'word': '姿态', 'pinyin': 'zī tài', 'trans': 'posture'}, {'word': '地形', 'pinyin': 'dì xíng', 'trans': 'terrain'}, {'word': '复杂', 'pinyin': 'fù zá', 'trans': 'complex'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '学习', 'pinyin': 'xué xí', 'trans': 'learn'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '阶段', 'pinyin': 'jiē duàn', 'trans': 'stage'}, {'word': '路径', 'pinyin': 'lù jìng', 'trans': 'path'}, {'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimize'}, {'word': '动作', 'pinyin': 'dòng zuò', 'trans': 'action'}, {'word': '平稳', 'pinyin': 'píng wěn', 'trans': 'stable'}, {'word': '可靠', 'pinyin': 'kě kào', 'trans': 'reliable'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '地面', 'pinyin': 'dì miàn', 'trans': 'ground'}, {'word': '成功', 'pinyin': 'chéng gōng', 'trans': 'success'}, {'word': '首次', 'pinyin': 'shǒu cì', 'trans': 'first time'}, {'word': '真实', 'pinyin': 'zhēn shí', 'trans': 'real'}, {'word': '环境', 'pinyin': 'huán jìng', 'trans': 'environment'}, {'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'apply'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}]
[19.02.2025 03:18] Renaming previous Chinese page.
[19.02.2025 03:18] Renaming previous data. zh.html to ./d/2025-02-18_zh_reading_task.html
[19.02.2025 03:18] Writing Chinese reading task.
[19.02.2025 03:18] Writing result.
[19.02.2025 03:18] Renaming log file.
[19.02.2025 03:18] Renaming previous data. log.txt to ./logs/2025-02-19_last_log.txt
