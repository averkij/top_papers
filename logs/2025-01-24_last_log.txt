[24.01.2025 00:44] Read previous papers.
[24.01.2025 00:44] Generating top page (month).
[24.01.2025 00:44] Writing top page (month).
[24.01.2025 02:09] Read previous papers.
[24.01.2025 02:09] Get feed.
[24.01.2025 02:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.12948
[24.01.2025 02:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.13106
[24.01.2025 02:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.12909
[24.01.2025 02:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.12895
[24.01.2025 02:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.12599
[24.01.2025 02:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.13074
[24.01.2025 02:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.13007
[24.01.2025 02:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.12570
[24.01.2025 02:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.11067
[24.01.2025 02:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[24.01.2025 02:09] No deleted papers detected.
[24.01.2025 02:09] Downloading and parsing papers (pdf, html). Total: 9.
[24.01.2025 02:09] Downloading and parsing paper https://huggingface.co/papers/2501.12948.
[24.01.2025 02:09] Extra JSON file exists (./assets/json/2501.12948.json), skip PDF parsing.
[24.01.2025 02:09] Paper image links file exists (./assets/img_data/2501.12948.json), skip HTML parsing.
[24.01.2025 02:09] Success.
[24.01.2025 02:09] Downloading and parsing paper https://huggingface.co/papers/2501.13106.
[24.01.2025 02:09] Extra JSON file exists (./assets/json/2501.13106.json), skip PDF parsing.
[24.01.2025 02:09] Paper image links file exists (./assets/img_data/2501.13106.json), skip HTML parsing.
[24.01.2025 02:09] Success.
[24.01.2025 02:09] Downloading and parsing paper https://huggingface.co/papers/2501.12909.
[24.01.2025 02:09] Extra JSON file exists (./assets/json/2501.12909.json), skip PDF parsing.
[24.01.2025 02:09] Paper image links file exists (./assets/img_data/2501.12909.json), skip HTML parsing.
[24.01.2025 02:09] Success.
[24.01.2025 02:09] Downloading and parsing paper https://huggingface.co/papers/2501.12895.
[24.01.2025 02:09] Extra JSON file exists (./assets/json/2501.12895.json), skip PDF parsing.
[24.01.2025 02:09] Paper image links file exists (./assets/img_data/2501.12895.json), skip HTML parsing.
[24.01.2025 02:09] Success.
[24.01.2025 02:09] Downloading and parsing paper https://huggingface.co/papers/2501.12599.
[24.01.2025 02:09] Extra JSON file exists (./assets/json/2501.12599.json), skip PDF parsing.
[24.01.2025 02:09] Paper image links file exists (./assets/img_data/2501.12599.json), skip HTML parsing.
[24.01.2025 02:09] Success.
[24.01.2025 02:09] Downloading and parsing paper https://huggingface.co/papers/2501.13074.
[24.01.2025 02:09] Extra JSON file exists (./assets/json/2501.13074.json), skip PDF parsing.
[24.01.2025 02:09] Paper image links file exists (./assets/img_data/2501.13074.json), skip HTML parsing.
[24.01.2025 02:09] Success.
[24.01.2025 02:09] Downloading and parsing paper https://huggingface.co/papers/2501.13007.
[24.01.2025 02:09] Extra JSON file exists (./assets/json/2501.13007.json), skip PDF parsing.
[24.01.2025 02:09] Paper image links file exists (./assets/img_data/2501.13007.json), skip HTML parsing.
[24.01.2025 02:09] Success.
[24.01.2025 02:09] Downloading and parsing paper https://huggingface.co/papers/2501.12570.
[24.01.2025 02:09] Extra JSON file exists (./assets/json/2501.12570.json), skip PDF parsing.
[24.01.2025 02:09] Paper image links file exists (./assets/img_data/2501.12570.json), skip HTML parsing.
[24.01.2025 02:09] Success.
[24.01.2025 02:09] Downloading and parsing paper https://huggingface.co/papers/2501.11067.
[24.01.2025 02:09] Extra JSON file exists (./assets/json/2501.11067.json), skip PDF parsing.
[24.01.2025 02:09] Paper image links file exists (./assets/img_data/2501.11067.json), skip HTML parsing.
[24.01.2025 02:09] Success.
[24.01.2025 02:09] Enriching papers with extra data.
[24.01.2025 02:09] ********************************************************************************
[24.01.2025 02:09] Abstract 0. We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero...
[24.01.2025 02:09] ********************************************************************************
[24.01.2025 02:09] Abstract 1. In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation model for image and video understanding. The core design philosophy of VideoLLaMA3 is vision-centric. The meaning of "vision-centric" is two-fold: the vision-centric training paradigm and vision-centric framework design. Th...
[24.01.2025 02:09] ********************************************************************************
[24.01.2025 02:09] Abstract 2. Virtual film production requires intricate decision-making processes, including scriptwriting, virtual cinematography, and precise actor positioning and actions. Motivated by recent advances in automated decision-making with language agent-based societies, this paper introduces FilmAgent, a novel LL...
[24.01.2025 02:09] ********************************************************************************
[24.01.2025 02:09] Abstract 3. Large language models (LLMs) demonstrate impressive performance but lack the flexibility to adapt to human preferences quickly without retraining. In this work, we introduce Test-time Preference Optimization (TPO), a framework that aligns LLM outputs with human preferences during inference, removing...
[24.01.2025 02:09] ********************************************************************************
[24.01.2025 02:09] Abstract 4. Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large languag...
[24.01.2025 02:09] ********************************************************************************
[24.01.2025 02:09] Abstract 5. Mixture-of-Experts (MoE) models mostly use a router to assign tokens to specific expert modules, activating only partial parameters and often outperforming dense models. We argue that the separation between the router's decision-making and the experts' execution is a critical yet overlooked issue, l...
[24.01.2025 02:09] ********************************************************************************
[24.01.2025 02:09] Abstract 6. Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large Language Models (LLMs), relies on reward models to select the best candidate solution from multiple generations. However, traditional reward models often assign arbitrary and inconsistent scores, limiting their effectiveness....
[24.01.2025 02:09] ********************************************************************************
[24.01.2025 02:09] Abstract 7. Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the model's problem-solving abilities and has achieved promising results. However, long-thought reasoning ...
[24.01.2025 02:09] ********************************************************************************
[24.01.2025 02:09] Abstract 8. Large Language Models (LLMs) are transforming artificial intelligence, evolving into task-oriented systems capable of autonomous planning and execution. One of the primary applications of LLMs is conversational AI systems, which must navigate multi-turn dialogues, integrate domain-specific APIs, and...
[24.01.2025 02:09] Read previous papers.
[24.01.2025 02:09] Generating reviews via LLM API.
[24.01.2025 02:09] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#open_source", "#dataset"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤–æ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –ø—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–Ω–æ–º—É –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π DeepSeek-R1-Zero –∏ DeepSeek-R1. DeepSeek
[24.01.2025 02:09] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#agi", "#games", "#video", "#benchmark"], "emoji": "üé•", "ru": {"title": "VideoLLaMA3: –ó—Ä–µ–Ω–∏–µ –∫–∞–∫ –∫–ª—é—á –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ", "desc": "VideoLLaMA3 - —ç—Ç–æ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. –ö–ª—é—á–µ–≤–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å
[24.01.2025 02:09] Using data from previous issue: {"categories": ["#multimodal", "#story_generation", "#3d", "#open_source", "#agents", "#hallucinations"], "emoji": "üé¨", "ru": {"title": "–í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –∫–∏–Ω–æ—Å—Ç—É–¥–∏—è: –ò–ò-–∞–≥–µ–Ω—Ç—ã —Å–æ–∑–¥–∞—é—Ç —Ñ–∏–ª—å–º—ã –æ—Ç –∏–¥–µ–∏ –¥–æ –≥–æ—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞", "desc": "FilmAgent - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
[24.01.2025 02:09] Using data from previous issue: {"categories": ["#rlhf", "#training", "#alignment", "#inference"], "emoji": "üéØ", "ru": {"title": "–ê–¥–∞–ø—Ç–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ª–µ—Ç—É: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Test-time Preference Optimization (TPO), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥–Ω—ã–µ
[24.01.2025 02:09] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#training", "#benchmark", "#rl", "#reasoning", "#long_context", "#math"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Kimi 
[24.01.2025 02:09] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–æ—Ç–±–æ—Ä —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª—è–º Mixture-of-Experts (MoE) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Autonomy-of-Experts (AoE). –í AoE —ç–∫—Å–ø–µ—Ä—Ç—ã —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª
[24.01.2025 02:09] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#dataset", "#math", "#rlhf"], "emoji": "üèÜ", "ru": {"title": "–ü–æ–ø–∞—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã–±–æ—Ä—É –ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è –≤ LLM", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã–±–æ—Ä—É –ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞
[24.01.2025 02:09] Using data from previous issue: {"categories": ["#reasoning", "#math", "#optimization", "#training", "#benchmark", "#inference"], "emoji": "‚ö°", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏—è –ò–ò –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏—Ç–µ–ª—å–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º, —Ç–∞–∫–∏—Ö –∫–∞–∫ OpenAI's O1. –ê–≤—Ç–æ—Ä—ã –ø—Ä
[24.01.2025 02:09] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#open_source", "#games", "#optimization", "#graphs", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "IntellAgent: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–≥–æ –ò–ò", "desc": "IntellAgent - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω
[24.01.2025 02:09] Loading Chinese text from previous data.
[24.01.2025 02:09] Renaming data file.
[24.01.2025 02:09] Renaming previous data. hf_papers.json to ./d/2025-01-24.json
[24.01.2025 02:09] Saving new data file.
[24.01.2025 02:09] Generating page.
[24.01.2025 02:09] Renaming previous page.
[24.01.2025 02:09] Renaming previous data. index.html to ./d/2025-01-24.html
[24.01.2025 02:09] [Experimental] Generating Chinese page for reading.
[24.01.2025 02:09] Chinese vocab [{'word': '‰ªãÁªç', 'pinyin': 'ji√®sh√†o', 'trans': 'introduce'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}, {'word': 'Âº∫ÂåñÂ≠¶‰π†', 'pinyin': 'qi√°ng\u200bhu√†\u200bxu√©\u200bx√≠', 'trans': 'reinforcement learning'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πnli√†n', 'trans': 'training'}, {'word': 'Â±ïÁ§∫', 'pinyin': 'zh«énsh√¨', 'trans': 'demonstrate'}, {'word': 'ÂèØËØªÊÄß', 'pinyin': 'kƒõ\u200bd√∫\u200bx√¨ng', 'trans': 'readability'}, {'word': 'ËØ≠Ë®ÄÊ∑∑Âêà', 'pinyin': 'y«î\u200by√°n\u200bh√πn\u200bh√©', 'trans': 'language mixing'}, {'word': 'ÂºÄÂèë', 'pinyin': 'kƒÅifƒÅ', 'trans': 'develop'}, {'word': 'Â§öÈò∂ÊÆµ', 'pinyin': 'du≈ç\u200bjiƒì\u200bdu√†n', 'trans': 'multi-stage'}, {'word': 'ÂÜ∑ÂêØÂä®', 'pinyin': 'lƒõng\u200bq«ê\u200bd√≤ng', 'trans': 'cold start'}, {'word': 'Êï∞ÊçÆÂ§ÑÁêÜ', 'pinyin': 'sh√π\u200bj√π\u200bch«î\u200bl«ê', 'trans': 'data processing'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo\u200bxi√†n', 'trans': 'performance'}, {'word': 'Áõ∏ÂΩì', 'pinyin': 'xiƒÅng\u200bdƒÅng', 'trans': 'equivalent'}, {'word': 'ÂºÄÊ∫ê', 'pinyin': 'kƒÅi\u200byu√°n', 'trans': 'open source'}, {'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´\u200by√∫', 'trans': 'based on'}, {'word': 'ÂéãÁº©', 'pinyin': 'yƒÅ\u200bsu≈ç', 'trans': 'compression'}]
[24.01.2025 02:09] Renaming previous Chinese page.
[24.01.2025 02:09] Renaming previous data. zh.html to ./d/2025-01-23_zh_reading_task.html
[24.01.2025 02:09] Writing Chinese reading task.
[24.01.2025 02:09] Writing result.
[24.01.2025 02:09] Renaming log file.
[24.01.2025 02:09] Renaming previous data. log.txt to ./logs/2025-01-24_last_log.txt
