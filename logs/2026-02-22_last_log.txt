[21.02.2026 18:45] Read previous papers.
[21.02.2026 18:45] Generating top page (month).
[21.02.2026 18:45] Writing top page (month).
[22.02.2026 02:44] Read previous papers.
[22.02.2026 02:44] Get feed.
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13515
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16855
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17270
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14457
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17004
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16699
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15569
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16968
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13579
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17365
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16849
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17363
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17288
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16928
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17259
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16756
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15823
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14857
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17588
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16802
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10377
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16915
[22.02.2026 02:44] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16835
[22.02.2026 02:44] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[22.02.2026 02:44] No deleted papers detected.
[22.02.2026 02:44] Downloading and parsing papers (pdf, html). Total: 23.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.13515.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.13515.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.13515.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.16855.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.16855.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.16855.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.17270.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.17270.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.17270.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.14457.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.14457.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.14457.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.17004.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.17004.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.17004.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.16699.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.16699.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.16699.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.15569.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.15569.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.15569.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.16968.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.16968.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.16968.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.13579.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.13579.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.13579.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.17365.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.17365.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.17365.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.16849.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.16849.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.16849.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.17363.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.17363.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.17363.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.17288.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.17288.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.17288.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.16928.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.16928.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.16928.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.17259.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.17259.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.17259.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.16756.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.16756.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.16756.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.15823.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.15823.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.15823.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.14857.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.14857.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.14857.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.17588.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.17588.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.17588.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.16802.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.16802.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.16802.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.10377.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.10377.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.10377.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.16915.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.16915.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.16915.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Downloading and parsing paper https://huggingface.co/papers/2602.16835.
[22.02.2026 02:44] Extra JSON file exists (./assets/json/2602.16835.json), skip PDF parsing.
[22.02.2026 02:44] Paper image links file exists (./assets/img_data/2602.16835.json), skip HTML parsing.
[22.02.2026 02:44] Success.
[22.02.2026 02:44] Enriching papers with extra data.
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 0. A trainable sparse attention method called SpargeAttention2 is proposed that achieves high sparsity in diffusion models while maintaining generation quality through hybrid masking rules and distillation-inspired fine-tuning.  					AI-generated summary 				 Many training-free sparse attention methods...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 1. GUI-Owl-1.5 is a multi-platform GUI agent model with varying sizes that achieves superior performance across GUI automation, grounding, tool-calling, and memory tasks through innovations in data pipelines, unified reasoning enhancement, and multi-platform reinforcement learning.  					AI-generated s...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 2. Unified Latents framework learns joint latent representations using diffusion prior regularization and diffusion model decoding, achieving competitive FID scores with reduced training compute.  					AI-generated summary 				 We present Unified Latents (UL), a framework for learning latent representa...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 3. Frontier AI risk analysis assesses critical dimensions including cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R&D, and self-replication, proposing mitigation strategies for secure deployment of advanced AI systems.  					AI-generated summary 				 To understand and...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 4. Arcee Trinity models are sparse Mixture-of-Experts architectures with varying parameter counts and activation patterns, utilizing advanced attention mechanisms and training optimizations.  					AI-generated summary 				 We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Exp...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 5. Large language models can be improved for complex tasks by explicitly reasoning about cost-uncertainty tradeoffs through a Calibrate-Then-Act framework that enhances decision-making in sequential environments.  					AI-generated summary 				 LLMs are increasingly being used for complex problems whic...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 6. Users prefer adaptive feedback mechanisms in in-car AI assistants, starting with high transparency to build trust and then reducing verbosity as reliability increases, particularly in attention-critical driving scenarios.  					AI-generated summary 				 Agentic AI assistants that autonomously perfor...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 7. Dynamic tokenization improves diffusion transformer efficiency by adjusting patch sizes based on content complexity and denoising timestep, achieving significant speedup without quality loss.  					AI-generated summary 				 Diffusion Transformers (DiTs) have achieved state-of-the-art performance in ...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 8. TactAlign enables transfer of human tactile demonstrations to robots with different embodiments through cross-embodiment tactile alignment without requiring paired data or manual labels.  					AI-generated summary 				 Human demonstrations collected by wearable devices (e.g., tactile gloves) provide...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 9. A world model for desktop software that predicts UI state changes through textual description followed by visual synthesis, improving decision quality and execution robustness in computer-using tasks.  					AI-generated summary 				 Agents operating in complex software environments benefit from reas...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 10. Two-layer neural networks solve modular addition by learning Fourier features through phase symmetry and frequency diversification, enabling robust computation via majority voting despite individual neuron noise.  					AI-generated summary 				 We present a comprehensive analysis of how two-layer ne...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 11. Researchers enhance linear attention by simplifying Mamba-2 and improving its architectural components to achieve near-softmax accuracy while maintaining memory efficiency for long sequences.  					AI-generated summary 				 Linear attention transformers have become a strong alternative to softmax at...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 12. Training a 1.36B-parameter scientific language model from raw arXiv LaTeX sources demonstrates the impact of preprocessing decisions, tokenization, and infrastructure constraints on model development under limited computational resources.  					AI-generated summary 				 While frontier large language...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 13. AlphaEvolve, an evolutionary coding agent using large language models, automatically discovers new multiagent learning algorithms for imperfect-information games by evolving regret minimization and population-based training variants.  					AI-generated summary 				 Much of the advancement of Multi-A...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 14. FRAPPE addresses limitations in world modeling for robotics by using parallel progressive expansion to improve representation alignment and reduce error accumulation in predictive models.  					AI-generated summary 				 Enabling VLA models to predict environmental dynamics, known as world modeling, ...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 15. NESSiE benchmark reveals safety vulnerabilities in large language models through simple security tests, demonstrating that even state-of-the-art models fail basic safety requirements without adversarial attacks.  					AI-generated summary 				 We introduce NESSiE, the NEceSsary SafEty benchmark for ...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 16. CrispEdit is a second-order editing algorithm for large language models that preserves capabilities by constraining updates to low-curvature subspaces of the capability-loss landscape using Bregman divergence and efficient Kronecker-factored approximations.  					AI-generated summary 				 A central ...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 17. StarWM, a world model for StarCraft II, predicts future observations under partial observability using a structured textual representation and achieves significant performance improvements in win rate and decision-making stability.  					AI-generated summary 				 Large Language Models (LLMs) have re...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 18. Human intervention patterns in web navigation are modeled to improve agent adaptability and collaboration, with language models achieving better intervention prediction and user satisfaction.  					AI-generated summary 				 Despite rapid progress in autonomous web agents, human involvement remains e...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 19. Reference-guided LLM-evaluators enhance LLM alignment by serving as soft verifiers, improving judge accuracy and enabling effective post-training in non-verifiable domains through self-improvement techniques.  					AI-generated summary 				 While Reinforcement Learning with Verifiable Rewards (RLVR)...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 20. A hardware-software co-design framework for on-device large language model deployment that establishes accuracy-latency relationships through training loss modeling and roofline analysis, enabling rapid architecture selection and improved performance.  					AI-generated summary 				 Vision-Language-...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 21. StereoAdapter-2 improves underwater stereo depth estimation by replacing ConvGRU with a selective state space ConvSS2D operator for efficient long-range propagation and introduces a large-scale synthetic underwater dataset.  					AI-generated summary 				 Stereo depth estimation is fundamental to un...
[22.02.2026 02:44] ********************************************************************************
[22.02.2026 02:44] Abstract 22. NeST is a lightweight safety alignment framework that selectively adapts safety-relevant neurons while keeping the rest of the model frozen, achieving significant reductions in unsafe generations with minimal trainable parameters.  					AI-generated summary 				 Safety alignment is essential for the...
[22.02.2026 02:44] Read previous papers.
[22.02.2026 02:44] Generating reviews via LLM API.
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#optimization", "#inference", "#video", "#architecture", "#diffusion", "#training"], "emoji": "‚ö°", "ru": {"title": "–û–±—É—á–∞–µ–º–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –º–æ–¥–µ–ª—è–º–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è SpargeAttention2 ‚Äî –æ–±—É—á–∞–µ–º—ã–π –º–µ—Ç–æ–¥ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è 
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#optimization", "#rl", "#data", "#open_source", "#agents", "#benchmark", "#training"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π GUI-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º", "desc": "GUI-Owl-1.5 ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–ø–ª–∞—Ç—Ñ–æ—Ä–º–Ω–∞—è –º–æ–¥–µ–ª—å-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º
[22.02.2026 02:44] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–ï–¥–∏–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Unified Latents ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–µ–≥—É–ª—è—Ä–∏–∑—É—é—Ç—Å—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º –ø—Ä–∏–æ—Ä–æ–º –∏ –¥–µ–∫–æ–¥–∏—Ä—É—é—Ç—Å—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é. 
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#agi", "#alignment", "#security"], "emoji": "‚ö†Ô∏è", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏ –≥—Ä–∞–Ω–∏—Ü AI: –æ—Ç –≤—ã—è–≤–ª–µ–Ω–∏—è —É–≥—Ä–æ–∑ –∫ –Ω–∞–¥–µ–∂–Ω—ã–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º –∑–∞—â–∏—Ç—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤ –ø–µ—Ä–µ–¥–æ–≤—ã—Ö AI —Å–∏—Å—Ç–µ–º, –≤–∫–ª—é—á–∞—é—â–∞—è –∞–Ω–∞–ª–∏–∑ –ø—è—Ç–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π: –∫–∏–±–µ—Ä–∞—Ç–∞–∫, –º
[22.02.2026 02:44] Using data from previous issue: {"categories": [], "emoji": "‚öôÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Mixture-of-Experts —Å –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ç—Ä–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-of-Experts –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Arcee Trinity —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: Trinity L
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#rl", "#optimization", "#reasoning", "#training", "#agents", "#plp"], "emoji": "‚öñÔ∏è", "ru": {"title": "–Ø–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –æ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–µ –∑–∞—Ç—Ä–∞—Ç –∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –≤ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Calibrate-Then-Act, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –±–æ–ª—å—à–∏
[22.02.2026 02:44] Using data from previous issue: {"categories": [], "emoji": "üöó", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –≤ –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞—Ö: –æ—Ç –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –∫–∞–∫ –∞–≥–µ–Ω—Ç–Ω—ã–µ LLM-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –∫–æ–º–º—É–Ω–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –∑–∞–¥–∞—á –≤ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#optimization", "#inference", "#video", "#architecture", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#multimodal", "#transfer_learning", "#training", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ç–∞–∫—Ç–∏–ª—å–Ω—ã—Ö –æ—â—É—â–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Ä–æ–±–æ—Ç–∞ –±–µ–∑ –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "TactAlign ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ç–∞–∫—Ç–∏–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ –∫ —Ä–æ–±–æ—Ç—É —Å —Ä–∞–∑–ª–∏—á–Ω–æ–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–µ–π, –∏—Å–ø–æ–ª—å–∑
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#multimodal", "#rl", "#agents", "#cv", "#training"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ: –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è —É–º–Ω–æ–≥–æ –ø–æ–º–æ—â–Ω–∏–∫–∞ –≤ –¥–µ—Å–∫—Ç–æ–ø–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –¥–µ—Å–∫—Ç–æ–ø–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è (CUWM), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∏
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#architecture", "#math", "#training"], "emoji": "üî¢", "ru": {"title": "–ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞ —á–µ—Ä–µ–∑ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –§—É—Ä—å–µ", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç, –∫–∞–∫ –¥–≤—É—Ö—Å–ª–æ–π–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Ä–µ—à–∞—é—Ç –∑–∞–¥–∞—á—É –º–æ–¥—É–ª—å–Ω–æ–≥–æ —Å–ª–æ–∂–µ–Ω–∏—è, –æ–±—É—á–∞—è—Å—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#optimization", "#inference", "#open_source", "#architecture", "#training", "#long_context"], "emoji": "‚ö°", "ru": {"title": "–õ–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —É–ª—É—á—à–∏–ª–∏ –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –ø—É—Ç—ë–º —É–ø—Ä–æ—â–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mamba-2 –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –µ—ë –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#training", "#open_source", "#data", "#plp", "#small_models", "#science", "#optimization"], "emoji": "üî¨", "ru": {"title": "–ò–Ω–∂–µ–Ω–µ—Ä–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞—É—á–Ω–æ–π LLM –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö: –æ—Ç —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#games", "#rl", "#agents", "#architecture", "#training"], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω AlphaEvolve ‚Äî —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#training", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –±—É–¥—É—â–µ–≥–æ –¥–ª—è –æ—Å–æ–∑–Ω–∞—é—â–µ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "FRAPPE ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–∏—Ä–∞ –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é —ç–∫—Å–ø–∞–Ω
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#security", "#alignment", "#open_source", "#ethics", "#dataset", "#benchmark"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏: –∫–∞–∫ –ø—Ä–æ—Å—Ç—ã–µ —Ç–µ—Å—Ç—ã –≤—ã—è–≤–ª—è—é—Ç –ø—Ä–æ–≤–∞–ª—ã LLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç NESSiE ‚Äî –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#training"], "emoji": "‚úèÔ∏è", "ru": {"title": "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ø—Ä–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–∏–∑–∫–æ–∫—Ä–∏–≤–∏–∑–Ω—ã–µ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "CrispEdit ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#games", "#reasoning", "#synthetic"], "emoji": "üéÆ", "ru": {"title": "–ú–æ–¥–µ–ª—å –º–∏—Ä–∞ –Ω–∞ —è–∑—ã–∫–µ –¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π –≤ StarCraft II", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω StarWM ‚Äî –ø–µ—Ä–≤–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –∏–≥—Ä—ã StarCraft II, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –±—É–¥—É—â–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –≤ —É—Å–ª–æ–≤–∏—è—Ö —á–∞—Å—Ç–∏—á–Ω–æ–π –≤–∏–¥–∏–º–æ
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#training", "#agents", "#dataset"], "emoji": "ü§ù", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —á–µ–ª–æ–≤–µ–∫–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∑–∞–¥–∞—á–∞ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —á–µ–ª–æ–≤–µ–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤–µ–±-–∑–∞–¥–∞—á. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç 
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#alignment", "#reasoning"], "emoji": "üéØ", "ru": {"title": "–≠—Ç–∞–ª–æ–Ω–Ω—ã–µ LLM-–æ—Ü–µ–Ω—â–∏–∫–∏ –∫–∞–∫ –º–æ—Å—Ç–∏–∫ –∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é –≤ –Ω–µ–≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º—è–≥–∫–∏—Ö –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –æ–±–ª–∞—Å—Ç—è—Ö –±–µ–∑ –æ–±—ä–µ–∫—Ç–∏–≤–Ω
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#architecture", "#robotics", "#small_models", "#inference", "#training", "#multimodal", "#benchmark"], "emoji": "‚ö°", "ru": {"title": "–°–æ–≤–º–µ—Å—Ç–Ω–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: –æ—Ç –º–µ—Å—è—Ü–µ–≤ –ø–æ–∏—Å–∫–∞ –∫ –¥–Ω—è–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#open_source", "#cv", "#synthetic", "#architecture"], "emoji": "üåä", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å—Ç–µ—Ä–µ–æ–≥–ª—É–±–∏–Ω–∞ –ø–æ–¥ –≤–æ–¥–æ–π —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–∏–≤–Ω—ã–µ state space –º–æ–¥–µ–ª–∏", "desc": "StereoAdapter-2 —É–ª—É—á—à–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –≥–ª—É–±–∏–Ω—ã –≤ –ø–æ–¥–≤–æ–¥–Ω—ã—Ö —Å—Ç–µ—Ä–µ–æ–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, –∑–∞–º–µ–Ω–∏–≤ ConvGRU –Ω
[22.02.2026 02:44] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#benchmark", "#alignment", "#security", "#training"], "emoji": "üõ°Ô∏è", "ru": {"title": "–¶–µ–ª–µ–≤–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: –º–∏–Ω–∏–º—É–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –º–∞–∫—Å–∏–º—É–º –∑–∞—â–∏—Ç—ã", "desc": "NeST ‚Äî —ç—Ç–æ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Å–µ–ª–µ–∫
[22.02.2026 02:44] Renaming data file.
[22.02.2026 02:44] Renaming previous data. hf_papers.json to ./d/2026-02-20.json
[22.02.2026 02:44] Saving new data file.
[22.02.2026 02:44] Generating page.
[22.02.2026 02:44] Renaming previous page.
[22.02.2026 02:44] Renaming previous data. index.html to ./d/2026-02-20.html
[22.02.2026 02:44] Writing result.
[22.02.2026 02:44] Renaming log file.
[22.02.2026 02:44] Renaming previous data. log.txt to ./logs/2026-02-22_last_log.txt
