[19.06.2025 22:11] Read previous papers.
[19.06.2025 22:11] Generating top page (month).
[19.06.2025 22:11] Writing top page (month).
[19.06.2025 23:11] Read previous papers.
[19.06.2025 23:11] Get feed.
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15675
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15211
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15681
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15677
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13414
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15068
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15569
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14842
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15672
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15050
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06279
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15461
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14315
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14824
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14866
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14435
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.11110
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14770
[19.06.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15682
[19.06.2025 23:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.06.2025 23:11] No deleted papers detected.
[19.06.2025 23:11] Downloading and parsing papers (pdf, html). Total: 19.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.15675.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.15675.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.15675.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.15211.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.15211.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.15211.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.15681.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.15681.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.15681.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.15677.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.15677.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.15677.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.13414.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.13414.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.13414.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.15068.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.15068.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.15068.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.15569.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.15569.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.15569.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14842.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14842.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14842.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.15672.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.15672.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.15672.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.15050.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.15050.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.15050.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.06279.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.06279.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.06279.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.15461.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.15461.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.15461.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14315.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14315.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14315.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14824.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14824.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14824.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14866.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14866.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14866.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14435.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14435.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14435.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.11110.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.11110.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.11110.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.14770.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.14770.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.14770.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2506.15682.
[19.06.2025 23:11] Extra JSON file exists (./assets/json/2506.15682.json), skip PDF parsing.
[19.06.2025 23:11] Paper image links file exists (./assets/img_data/2506.15682.json), skip HTML parsing.
[19.06.2025 23:11] Success.
[19.06.2025 23:11] Enriching papers with extra data.
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 0. Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.  					AI-generated summary 				 Video generation techniques have made remarkable progress, promising to be the foundation of interactive world ...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 1. ProtoReasoning enhances large reasoning models through prototypical representations, leading to improved cross-domain generalization in logical reasoning, planning, and other tasks.  					AI-generated summary 				 Recent advances in Large Reasoning Models (LRMs) trained with Long Chain-of-Thought (L...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 2. GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.  					AI-generated summary 				 Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve per...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 3. Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.  					AI-generated summary 				 AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge o...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 4. The combined DiCoW and DiariZen ASR system demonstrates strong performance in multilingual scenarios, with DiCoW preserving its multilingual capabilities and DiariZen improving through fine-tuning.  					AI-generated summary 				 We present a two-speaker automatic speech recognition (ASR) system tha...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 5. PrefBERT, a scoring model, improves open-ended long-form generation by providing better semantic reward feedback than traditional metrics.  					AI-generated summary 				 Evaluating open-ended long-form generation is challenging because it is hard to define what clearly separates good from bad outpu...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 6. A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  					AI-generated summary 				 We introduce SciVer, the first benchmark specifically designed to evaluate the ab...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 7. PictSure is an in-context learning framework that enhances few-shot image classification by optimizing embedding models' architecture, pretraining, and fine-tuning strategies to improve out-of-domain performance.  					AI-generated summary 				 Building image classification models remains cumbersome...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 8. SwarmAgentic is a framework for automated agentic system generation that optimize agent functionality and collaboration through language-driven exploration, outperforming existing baselines in unconstrained tasks.  					AI-generated summary 				 The rapid progress of Large Language Models has advanc...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 9. T-PPO, an extension of PPO, improves training efficiency for Large Language Models by optimizing policy updates and utilizing hardware resources more effectively.  					AI-generated summary 				 Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabili...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 10. CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.  					AI-generated summary 				 Recent advancements in Large Vision-Language Models built upon Large Language Models have establishe...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 11. A novel method, CheckFree, and its extended version CheckFree+, efficiently recover from node failures during LLM training by substituting failed stages with averaged neighboring stages or through out-of-order pipeline execution, improving convergence time over existing checkpointing methods.  					...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 12. An agent-guided framework generates photorealistic 3D scenes for VR by synthesizing textures onto lightweight geometric proxies, enabling real-time rendering and superior visual quality.  					AI-generated summary 				 Automatic creation of 3D scenes for immersive VR presence has been a significant ...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 13. FedNano is a federated learning framework that centralizes large language models on servers and uses NanoEdge modules for client-specific adaptation, addressing scalability and privacy issues.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) excel in tasks like multimodal rea...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 14. A new benchmark called OS-Harm measures the safety of computer use agents interacting with GUIs, evaluating their susceptibility to misuse, prompt injection attacks, and misbehavior across various safety violations and applications.  					AI-generated summary 				 Computer use agents are LLM-based a...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 15. MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  					AI-generated summary 				 Large multimodal Mixture-of-Experts (MoEs) effectively scale the m...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 16. AssertBench evaluates Large Language Models' ability to maintain consistent truth evaluation when faced with contradictory user assertions about factually true statements by analyzing framing-induced variability.  					AI-generated summary 				 Recent benchmarks have probed factual consistency and r...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 17. GMT, a unified motion-tracking framework, addresses challenges in tracking diverse humanoid robot motions through adaptive sampling and a motion mixture-of-experts architecture, achieving state-of-the-art performance.  					AI-generated summary 				 The ability to track general whole-body motions in...
[19.06.2025 23:11] ********************************************************************************
[19.06.2025 23:11] Abstract 18. ECAD, a genetic algorithm, optimizes caching schedules for diffusion models, enhancing inference speed while maintaining quality across various benchmarks.  					AI-generated summary 				 Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow ...
[19.06.2025 23:11] Read previous papers.
[19.06.2025 23:11] Generating reviews via LLM API.
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#synthetic", "#dataset", "#games", "#data", "#video"], "emoji": "üåé", "ru": {"title": "Sekai: –≥–ª–æ–±–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –º–∏—Ä", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Sekai –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —Ü–µ–ª—å—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –º–∏—Ä–∞. –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 5000 —á–∞—Å–æ
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#reasoning", "#transfer_learning", "#training", "#dataset", "#architecture"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Ç–æ—Ç–∏–ø—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∫–∞–∫ –∫–ª—é—á –∫ –æ–±–æ–±—â–µ–Ω–∏—é –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ProtoReasoning - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#inference", "#training", "#multimodal", "#dataset", "#architecture"], "emoji": "üî¨", "ru": {"title": "GenRecal: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "GenRecal - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è –≤–∏–∑—É–∞–ª—å
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#open_source", "#agents", "#multimodal", "#agi", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –∏ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –º–∏—Ä–æ–≤ –≤ –ò–ò-–∞–≥–µ–Ω—Ç–∞—Ö –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ - Embodied Web Agents,
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#audio", "#training", "#multilingual", "#data"], "emoji": "üó£Ô∏è", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ DiCoW –∏ DiariZen: –º–æ—â–Ω–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ ASR", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–≤—É—Ö–¥–∏–∫—Ç–æ—Ä–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ (ASR), –æ–±—ä–µ–¥–∏–Ω—è—é—â—É—é DiCoW –∏ DiariZen. DiCoW - —ç—Ç–æ –≤–∞
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#long_context", "#alignment", "#rlhf", "#benchmark", "#optimization", "#open_source", "#dataset"], "emoji": "üìù", "ru": {"title": "PrefBERT: –£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏", "desc": "PrefBERT - —ç—Ç–æ –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∫–∏, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#rag", "#science", "#benchmark", "#open_source"], "emoji": "üß™", "ru": {"title": "SciVer: –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—É—á–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –ò–ò-–º–æ–¥–µ–ª—è–º–∏", "desc": "SciVer - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–µ—Ä–∏—Ñ
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#dataset", "#cv", "#transfer_learning", "#optimization", "#training", "#benchmark"], "emoji": "üñºÔ∏è", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤–Ω–µ –¥–æ–º–µ–Ω–∞", "desc": "PictSure - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#optimization", "#agents", "#benchmark", "#games", "#agi", "#open_source", "#alignment"], "emoji": "üêù", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è —ç–≤–æ–ª—é—Ü–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "SwarmAgentic - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º. –û–Ω –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ñ—É–Ω–∫—Ü
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#optimization", "#rl", "#reasoning", "#training"], "emoji": "üöÄ", "ru": {"title": "T-PPO: –£—Å–∫–æ—Ä—è–µ–º –æ–±—É—á–µ–Ω–∏–µ LLM –≤ 2,5 —Ä–∞–∑–∞", "desc": "T-PPO - —ç—Ç–æ –Ω–æ–≤–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ PPO, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#multimodal", "#reasoning", "#architecture"], "emoji": "üß†", "ru": {"title": "CoMemo: –£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "CoMemo - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, —Ä–µ—à–∞—é—â–∞—è –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–Ω–µ–±—Ä–µ–∂–µ–Ω–∏—è –≤–∏–∑
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training"], "emoji": "üîÑ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ LLM –±–µ–∑ –ª–∏—à–Ω–∏—Ö –∑–∞—Ç—Ä–∞—Ç", "desc": "CheckFree –∏ CheckFree+ - –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ —Å–±–æ–µ–≤ —É–∑–ª–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∏ –∑–∞–º–µ–Ω—è—é—Ç –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω—ã–µ —Å—Ç–∞–¥–∏–∏ 
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#3d", "#video", "#multimodal", "#synthetic", "#games", "#agents"], "emoji": "üï∂Ô∏è", "ru": {"title": "–§–æ—Ç–æ—Ä–µ–∞–ª–∏–∑–º –≤ VR –±–µ–∑ —Å–ª–æ–∂–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏", "desc": "ImmerseGen - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D-—Å—Ü–µ–Ω –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#agents", "#scalability", "#privacy", "#training", "#federated_learning"], "emoji": "üî¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-—Å–∏—Å—Ç–µ–º", "desc": "FedNano - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö 
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#security", "#ethics", "#agents", "#benchmark"], "emoji": "üñ•Ô∏è", "ru": {"title": "OS-Harm: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞—Ö", "desc": "–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ OS-Harm –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–æ–≤, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–∞. –û–Ω –∏–∑
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#inference", "#training"], "emoji": "üß†", "ru": {"title": "MoTE: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ Mixture-of-Experts –º–æ–¥–µ–ª–∏ –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MoTE - –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π Mixture-of-Experts —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ—Ä–Ω–∞—Ä–Ω—ã
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#alignment", "#interpretability", "#benchmark", "#hallucinations"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–æ–π–∫–æ—Å—Ç–∏ –ò–ò –ø–µ—Ä–µ–¥ –ª–∏—Ü–æ–º –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π", "desc": "AssertBench - —ç—Ç–æ –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –æ—Ü–µ–Ω–∫–µ 
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#robotics", "#architecture", "#agents"], "emoji": "ü§ñ", "ru": {"title": "GMT: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏–π –¥–ª—è –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "GMT - —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π –¥–ª—è –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å–º–µ—Å–∏ —ç–∫
[19.06.2025 23:11] Using data from previous issue: {"categories": ["#optimization", "#inference", "#benchmark", "#diffusion"], "emoji": "üöÄ", "ru": {"title": "ECAD: –≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ECAD - —ç—Ç–æ –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—â–∏–π —Å—Ö–µ–º—ã –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –ø–æ–≤—ã—à–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –≤—ã
[19.06.2025 23:11] Renaming data file.
[19.06.2025 23:11] Renaming previous data. hf_papers.json to ./d/2025-06-19.json
[19.06.2025 23:11] Saving new data file.
[19.06.2025 23:11] Generating page.
[19.06.2025 23:11] Renaming previous page.
[19.06.2025 23:11] Renaming previous data. index.html to ./d/2025-06-19.html
[19.06.2025 23:11] Writing result.
[19.06.2025 23:11] Renaming log file.
[19.06.2025 23:11] Renaming previous data. log.txt to ./logs/2025-06-19_last_log.txt
