[19.06.2025 02:45] Read previous papers.
[19.06.2025 02:45] Generating top page (month).
[19.06.2025 02:45] Writing top page (month).
[19.06.2025 03:43] Read previous papers.
[19.06.2025 03:43] Get feed.
[19.06.2025 03:43] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15681
[19.06.2025 03:43] Extract page data from URL. URL: https://huggingface.co/papers/2506.15569
[19.06.2025 03:43] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15677
[19.06.2025 03:43] Extract page data from URL. URL: https://huggingface.co/papers/2506.14435
[19.06.2025 03:43] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06279
[19.06.2025 03:43] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.06.2025 03:43] No deleted papers detected.
[19.06.2025 03:43] Downloading and parsing papers (pdf, html). Total: 5.
[19.06.2025 03:43] Downloading and parsing paper https://huggingface.co/papers/2506.15681.
[19.06.2025 03:43] Extra JSON file exists (./assets/json/2506.15681.json), skip PDF parsing.
[19.06.2025 03:43] Paper image links file exists (./assets/img_data/2506.15681.json), skip HTML parsing.
[19.06.2025 03:43] Success.
[19.06.2025 03:43] Downloading and parsing paper https://huggingface.co/papers/2506.15569.
[19.06.2025 03:43] Failed to download and parse paper https://huggingface.co/papers/2506.15569: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
[19.06.2025 03:43] Downloading and parsing paper https://huggingface.co/papers/2506.15677.
[19.06.2025 03:43] Extra JSON file exists (./assets/json/2506.15677.json), skip PDF parsing.
[19.06.2025 03:43] Paper image links file exists (./assets/img_data/2506.15677.json), skip HTML parsing.
[19.06.2025 03:43] Success.
[19.06.2025 03:43] Downloading and parsing paper https://huggingface.co/papers/2506.14435.
[19.06.2025 03:43] Downloading paper 2506.14435 from http://arxiv.org/pdf/2506.14435v1...
[19.06.2025 03:44] Extracting affiliations from text.
[19.06.2025 03:44] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 1 ] . [ 1 5 3 4 4 1 . 6 0 5 2 : r MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models Hongyu Wang Jiayu Xu Ruiping Wang Yan Feng Yitao Zhai Peng Pei Xunliang Cai Xilin Chen Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences Meituan University of Chinese Academy of Sciences "
[19.06.2025 03:44] Response: ```python
["Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences", "Meituan University of Chinese Academy of Sciences"]
```
[19.06.2025 03:44] Deleting PDF ./assets/pdf/2506.14435.pdf.
[19.06.2025 03:44] Success.
[19.06.2025 03:44] Downloading and parsing paper https://huggingface.co/papers/2506.06279.
[19.06.2025 03:44] Downloading paper 2506.06279 from http://arxiv.org/pdf/2506.06279v1...
[19.06.2025 03:44] Extracting affiliations from text.
[19.06.2025 03:44] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CoMemo: LVLMs Need Image Context with Image Memory Shi Liu * 1 Weijie Su * (cid:0) 1 Xizhou Zhu 2 1 Wenhai Wang 3 1 Jifeng Dai 2 1 5 2 0 2 6 ] . [ 1 9 7 2 6 0 . 6 0 5 2 : r a "
[19.06.2025 03:44] Response: ```python
[]
```
[19.06.2025 03:44] Extracting affiliations from text.
[19.06.2025 03:44] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CoMemo: LVLMs Need Image Context with Image Memory Shi Liu * 1 Weijie Su * (cid:0) 1 Xizhou Zhu 2 1 Wenhai Wang 3 1 Jifeng Dai 2 1 5 2 0 2 6 ] . [ 1 9 7 2 6 0 . 6 0 5 2 : r aRecent advancements in Large Vision-Language Models built upon Large Language Models have established aligning visual features with LLM representations as the dominant paradigm. However, inherited LLM architectural designs introduce suboptimal characteristics for multimodal processing. First, LVLMs exhibit bimodal distribution in attention allocation, leading to the progressive neglect of middle visual content as context expands. Second, conventional positional encoding schemes fail to preserve vital 2D structural relationships when processing dynamic high-resolution images. To address these limitations, we propose CoMemo - dual-path architecture that combines Context image path with an image Memory path for visual processing, effectively alleviating visual information neglect. Additionally, we introduce RoPE-DHR, novel positional encoding mechanism that employs thumbnail-based positional aggregation to maintain 2D spatial awareness while mitigating remote decay in extended sequences. Evaluations across seven benchmarks,including long-context comprehension, multi-image reasoning, and visual question answering, demonstrate CoMemos superior performance compared to conventional LVLM architectures. Project page is available at https://lalbj.github.io/projects/CoMemo/. 1. Introduction Recent advances in large language models (LLMs) have demonstrated unprecedented generative capabilities (Ouyang et al., 2022; Touvron et al., 2023), primarily driven by the exponential scaling of training data and model *Equal contribution Project lead 1Shanghai Artificial Intelligence Laboratory 2Tsinghua University 3The Chinese University of Hong Kong. Correspondence to: Weijie Su <suweijie@pjlab.org.cn>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). parameters. Building upon this foundation, large visionlanguage models (LVLMs) have emerged as powerful multimodal systems that align visual representations with LLM embedding spaces to enable cross-modal reasoning (Liu et al., 2024b; Alayrac et al., 2022). Current methodologies for visual information injection predominantly follow two architectural paradigms. The first paradigm (referred to as LVLM-X, e.g., Flamingo (Alayrac et al., 2022)) employs cross-attention mechanisms to integrate visual features into textual representations. While this approach offers flexible modality interaction, recent studies (Laurencon et al., 2024) have revealed its suboptimal performance compared to alternative approaches when using identical LLM backbones and training data - finding corroborated by our studies in Figure 1. The second paradigm (referred to as LVLM-S, e.g., LLaVA (Liu et al., 2024b)) aligns visual tokens into text token embeddings space and then performs autoregressive processing. This paradigm is more compatible with LLM architectures; however, the preservation intrinsic mechanisms such as attention bimodality (Xiao et al., 2023; Liu et al., 2025a) and the linearly increasing position encoding leads to critical limitations: (1) the lost in the middle (Liu et al., 2024c; Song et al., 2024) phenomenon degrades performance with increasing context length, and (2) positional encoding sparsity induces remote decay and 2d-dimensional lost in high-resolution image processing. In this paper, we propose novel framework for LVLM, named CoMemo. Our key idea is to introduce an additional image-processing mechanism that is unaffected by context, without modifying the internal mechanisms of the LLM. Specifically, we concatenate image tokens with text tokens as the input sequence for fully autoregressive processing, while simultaneously feeding the image tokens into mixin layer for cross-attention computation. Crossattention retrieves image information based on text, avoiding the issue of image neglect that can occur in causal selfattention. However, simply combining these two structures, as in NVLM-H (Dai et al., 2024), does not work well. To address this, we first investigated balanced scheme for visual representation inputs. Then, we introduced threestage training technique to prevent overreliance on the crossattention path, effectively transforming it into memory CoMemo: LVLMs Need Image Context with Image Memory (a) LVLM-S. (b) LVLM-X. (c) Ours. Figure 1: Evaluation results of three architectures with same training data and model size (2B). Please refer to Tables 2 to 4 for details. Figure 2: Comparison three types of architectures for LVLMs. Method (a) use image encoder to align visual features with the LLMs continuous token representation space. Method (b) employs mixin layer with cross-attention to update LLMs hidden states based on visual features. And Method (c) contrust dual-path structure to enable the model to focus more on visual content during generation. path. Meanwhile, the fully autoregressive process serves as the primary path for introducing image context, referred to as the context path. The positional encoding scheme in LVLMs typically adopts RoPE from LLMs, treating each image patch token as an individual token for encoding. However, this approach results in highly sparse positional encodings for dynamic highresolution image patch tokens. Such sparse encoding can lead to remote decay issues in positional encoding, and the one-dimensional incremental encoding scheme also loses the two-dimensional information of the image. To address this, we propose novel positional encoding scheme based on the dynamic high-resolution method. Specifically, in the dynamic high-resolution approach, the image is divided into multiple image tiles and single image thumbnail. We treat the image thumbnail as part of the input sequence for standard sequential positional encoding, while mapping the image tiles to the image thumbnail index based on their two-dimensional positional relationships. To fully evaluate the impact of the CoMemo architecture, we collected set of multimodal benchmarks and categorized them into seven evaluation tasks: Caption, Long-generation, Multi-image, Long-context, Math, General VQA, and OCRrelated tasks. The results demonstrates CoMemos superiority over LVLM-X/S baselines under same data and model settings. Our framework achieves 17.2%, 7.0% and 5.6% relative improvement on Caption, Lo"
[19.06.2025 03:44] Mistral response. {"id": "92747031caee4e04a793159c9746c76f", "object": "chat.completion", "created": 1750304666, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Shanghai Artificial Intelligence Laboratory\", \"Tsinghua University\", \"The Chinese University of Hong Kong\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1625, "total_tokens": 1657, "completion_tokens": 32}}
[19.06.2025 03:44] Response: ```python
["Shanghai Artificial Intelligence Laboratory", "Tsinghua University", "The Chinese University of Hong Kong"]
```
[19.06.2025 03:44] Deleting PDF ./assets/pdf/2506.06279.pdf.
[19.06.2025 03:44] Success.
[19.06.2025 03:44] Enriching papers with extra data.
[19.06.2025 03:44] ********************************************************************************
[19.06.2025 03:44] Abstract 0. GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.  					AI-generated summary 				 Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve per...
[19.06.2025 03:44] ********************************************************************************
[19.06.2025 03:44] Abstract 1. A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  					AI-generated summary 				 We introduce SciVer, the first benchmark specifically designed to evaluate the ab...
[19.06.2025 03:44] ********************************************************************************
[19.06.2025 03:44] Abstract 2. Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.  					AI-generated summary 				 AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge o...
[19.06.2025 03:44] ********************************************************************************
[19.06.2025 03:44] Abstract 3. MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  					AI-generated summary 				 Large multimodal Mixture-of-Experts (MoEs) effectively scale the m...
[19.06.2025 03:44] ********************************************************************************
[19.06.2025 03:44] Abstract 4. CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.  					AI-generated summary 				 Recent advancements in Large Vision-Language Models built upon Large Language Models have establishe...
[19.06.2025 03:44] Read previous papers.
[19.06.2025 03:44] Generating reviews via LLM API.
[19.06.2025 03:44] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#inference", "#training", "#multimodal", "#dataset", "#architecture"], "emoji": "🔬", "ru": {"title": "GenRecal: Универсальная дистилляция для эффективных визуально-языковых моделей", "desc": "GenRecal - это новая система дистилляции для визуаль
[19.06.2025 03:44] Querying the API.
[19.06.2025 03:44] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  					AI-generated summary 				 We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks.
[19.06.2025 03:44] Response: {
  "desc": "SciVer - это новый бенчмарк для оценки способности мультимодальных фундаментальных моделей верифицировать научные утверждения. Он состоит из 3000 аннотированных экспертами примеров из 1113 научных статей, охватывающих четыре типа рассуждений. Авторы протестировали 21 современную мультимодальную модель, включая o4-mini, Gemini-2.5-Flash и другие. Результаты показали значительный разрыв в производительности между моделями и экспертами-людьми, выявив ключевые ограничения в понимании и рассуждениях моделей в задачах с мультимодальной научной литературой.",
  "emoji": "🧪",
  "title": "SciVer: проверка научных утверждений мультимодальными ИИ-моделями"
}
[19.06.2025 03:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  					AI-generated summary 				 We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks."

[19.06.2025 03:44] Response: ```python
['BENCHMARK', 'MULTIMODAL', 'RAG']
```
[19.06.2025 03:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  					AI-generated summary 				 We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks."

[19.06.2025 03:44] Response: ```python
['REASONING', 'SCIENCE', 'OPEN_SOURCE']
```
[19.06.2025 03:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces SciVer, a benchmark designed to assess how well multimodal foundation models can verify claims in scientific contexts. It includes 3,000 expert-annotated examples from 1,113 scientific papers, focusing on four reasoning types relevant to claim verification. The study evaluates 21 advanced multimodal models, revealing significant performance gaps compared to human experts. Additionally, it highlights limitations in current models and provides insights for improving their understanding and reasoning capabilities in scientific literature.","title":"Bridging the Gap: Evaluating Multimodal Models in Scientific Claim Verification"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces SciVer, a benchmark designed to assess how well multimodal foundation models can verify claims in scientific contexts. It includes 3,000 expert-annotated examples from 1,113 scientific papers, focusing on four reasoning types relevant to claim verification. The study evaluates 21 advanced multimodal models, revealing significant performance gaps compared to human experts. Additionally, it highlights limitations in current models and provides insights for improving their understanding and reasoning capabilities in scientific literature.', title='Bridging the Gap: Evaluating Multimodal Models in Scientific Claim Verification'))
[19.06.2025 03:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SciVer是一个新的基准，专门用于评估多模态基础模型在科学背景下验证声明的能力。它包含3000个专家注释的示例，涵盖1113篇科学论文，分为四个子集，代表多模态科学声明验证中的常见推理类型。我们评估了21个最先进的多模态基础模型的表现，发现这些模型与人类专家之间存在显著的性能差距。通过对检索增强生成（RAG）和人类错误评估的深入分析，我们识别了当前开源模型的关键局限性，为提高模型在多模态科学文献任务中的理解和推理能力提供了重要见解。","title":"提升多模态模型的科学验证能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SciVer是一个新的基准，专门用于评估多模态基础模型在科学背景下验证声明的能力。它包含3000个专家注释的示例，涵盖1113篇科学论文，分为四个子集，代表多模态科学声明验证中的常见推理类型。我们评估了21个最先进的多模态基础模型的表现，发现这些模型与人类专家之间存在显著的性能差距。通过对检索增强生成（RAG）和人类错误评估的深入分析，我们识别了当前开源模型的关键局限性，为提高模型在多模态科学文献任务中的理解和推理能力提供了重要见解。', title='提升多模态模型的科学验证能力'))
[19.06.2025 03:44] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#open_source", "#agents", "#multimodal", "#agi", "#reasoning"], "emoji": "🤖", "ru": {"title": "Объединение физического и цифрового миров в ИИ-агентах нового поколения", "desc": "Статья представляет новую парадигму искусственного интеллекта - Embodied Web Agents,
[19.06.2025 03:44] Querying the API.
[19.06.2025 03:44] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  					AI-generated summary 				 Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.
[19.06.2025 03:44] Response: {
  "desc": "Статья представляет MoTE - метод для улучшения моделей Mixture-of-Experts с использованием тернарных экспертов низкой точности. Этот подход позволяет масштабировать размер модели, повышая производительность при сохранении фиксированных активных параметров. MoTE демонстрирует многообещающую тенденцию масштабирования и достигает сопоставимой производительности с полноточными базовыми MoE-моделями при меньшем объеме памяти. Метод особенно эффективен для устройств с ограниченной памятью, превосходя MoE-LLaVA на 4.3% по средней точности при том же объеме памяти экспертов.",

  "emoji": "🧠",

  "title": "MoTE: Эффективные Mixture-of-Experts модели для устройств с ограниченной памятью"
}
[19.06.2025 03:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  					AI-generated summary 				 Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices."

[19.06.2025 03:44] Response: ```python
["INFERENCE", "ARCHITECTURE", "TRAINING"]
```
[19.06.2025 03:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  					AI-generated summary 				 Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices."

[19.06.2025 03:44] Response: ```python
["OPTIMIZATION"]
```
[19.06.2025 03:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces MoTE, a new method that enhances Mixture-of-Experts (MoE) models by using low-precision ternary experts, which consist of parameters limited to -1, 0, and 1. This approach allows for a significant reduction in memory usage while maintaining competitive performance compared to traditional full-precision experts. MoTE is designed to be scalable and efficient, making it suitable for deployment on edge devices where memory is limited. Experimental results show that MoTE not only matches the performance of existing models but also improves accuracy when combined with post-training quantization techniques.","title":"MoTE: Efficient Ternary Experts for Scalable Edge Deployment"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces MoTE, a new method that enhances Mixture-of-Experts (MoE) models by using low-precision ternary experts, which consist of parameters limited to -1, 0, and 1. This approach allows for a significant reduction in memory usage while maintaining competitive performance compared to traditional full-precision experts. MoTE is designed to be scalable and efficient, making it suitable for deployment on edge devices where memory is limited. Experimental results show that MoTE not only matches the performance of existing models but also improves accuracy when combined with post-training quantization techniques.', title='MoTE: Efficient Ternary Experts for Scalable Edge Deployment'))
[19.06.2025 03:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MoTE是一种可扩展且内存高效的方法，旨在改进混合专家模型，使用低精度的三元专家来提升性能并减少内存占用，适合在边缘设备上部署。与以往主要使用全精度专家的稀疏上升方法不同，MoTE通过训练更多低精度专家来实现更好的性能。具体来说，我们使用预训练的前馈网络作为共享专家，并训练参数为{-1, 0, 1}的三元路由专家。实验结果表明，MoTE在模型规模上具有良好的扩展趋势，并在内存受限的情况下表现出色。","title":"MoTE：内存高效的混合专家模型"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MoTE是一种可扩展且内存高效的方法，旨在改进混合专家模型，使用低精度的三元专家来提升性能并减少内存占用，适合在边缘设备上部署。与以往主要使用全精度专家的稀疏上升方法不同，MoTE通过训练更多低精度专家来实现更好的性能。具体来说，我们使用预训练的前馈网络作为共享专家，并训练参数为{-1, 0, 1}的三元路由专家。实验结果表明，MoTE在模型规模上具有良好的扩展趋势，并在内存受限的情况下表现出色。', title='MoTE：内存高效的混合专家模型'))
[19.06.2025 03:44] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#multimodal", "#reasoning", "#architecture"], "emoji": "🧠", "ru": {"title": "CoMemo: Улучшение визуального восприятия в мультимодальных моделях", "desc": "CoMemo - это новая архитектура для мультимодальной обработки, решающая проблемы пренебрежения виз
[19.06.2025 03:44] Renaming data file.
[19.06.2025 03:44] Renaming previous data. hf_papers.json to ./d/2025-06-19.json
[19.06.2025 03:44] Saving new data file.
[19.06.2025 03:44] Generating page.
[19.06.2025 03:44] Renaming previous page.
[19.06.2025 03:44] Renaming previous data. index.html to ./d/2025-06-19.html
[19.06.2025 03:44] Writing result.
[19.06.2025 03:44] Renaming log file.
[19.06.2025 03:44] Renaming previous data. log.txt to ./logs/2025-06-19_last_log.txt
