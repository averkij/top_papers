[19.06.2025 03:44] Read previous papers.
[19.06.2025 03:44] Generating top page (month).
[19.06.2025 03:44] Writing top page (month).
[19.06.2025 04:20] Read previous papers.
[19.06.2025 04:20] Get feed.
[19.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15681
[19.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15677
[19.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15569
[19.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14435
[19.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06279
[19.06.2025 04:20] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.06.2025 04:20] No deleted papers detected.
[19.06.2025 04:20] Downloading and parsing papers (pdf, html). Total: 5.
[19.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.15681.
[19.06.2025 04:20] Extra JSON file exists (./assets/json/2506.15681.json), skip PDF parsing.
[19.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.15681.json), skip HTML parsing.
[19.06.2025 04:20] Success.
[19.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.15677.
[19.06.2025 04:20] Extra JSON file exists (./assets/json/2506.15677.json), skip PDF parsing.
[19.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.15677.json), skip HTML parsing.
[19.06.2025 04:20] Success.
[19.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.15569.
[19.06.2025 04:21] Downloading paper 2506.15569 from http://arxiv.org/pdf/2506.15569v1...
[19.06.2025 04:21] Extracting affiliations from text.
[19.06.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SCIVER: Evaluating Foundation Models for Multimodal Scientific Claim Verification 5 2 0 2 8 1 ] . [ 1 9 6 5 5 1 . 6 0 5 2 : r a "
[19.06.2025 04:21] Response: []
[19.06.2025 04:21] Extracting affiliations from text.
[19.06.2025 04:21] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SCIVER: Evaluating Foundation Models for Multimodal Scientific Claim Verification5 2 0 2 8 1 ] . [ 1 9 6 5 5 1 . 6 0 5 2 : r aWe introduce SCIVER, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within multimodal scientific context. SCIVER consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals substantial performance gap between these models and human experts on SCIVER. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models comprehension and reasoning in multimodal scientific literature tasks.chengyewang/SciVer QDRhhhh/SciVerScientific claim verification has become increasingly vital as the research community grapples with an ever-expanding body of scientific literature across diverse domains (Dasigi et al., 2021; Wadden et al., 2022; Lee et al., 2023; Asai et al., 2024). The accuracy of claim verification in scientific paper is not merely matter of cross-checking numerical consistency or validating conclusionsit necessitates holistic understanding of the papers context (e.g., textual content, charts, and tables). Despite the significance of multimodal reasoning, existing benchmarks in scientific claim verification have often treated these components in isolation. Predominantly, prior works have focused either on textual content alone (Wadden et al., 2022) or on verifying claims based on single table (Lu et al., 2023). While previous multimodal questionanswering (QA) benchmarks in scientific literature comprehension incorporate scientific charts, they still remain limited to QA tasks over single chart (Li et al., 2024d; Wang et al., 2024b; Li et al., 2024c), failing to capture the broader multimodal context of scientific literature. Consequently, the lack of comprehensive multimodal benchmark restricts the systematic evaluation of foundation models ability to reason across the diverse and interconnected modalities in scientific literature. In this work, we introduce SCIVER, comprehensive and high-quality benchmark for evaluating multimodal SCIentific claim VERification. SCIVER consists of 3,000 expert-annotated examples over 1,113 scientific papers spanning diverse domains within computer science. To ensure our benchmark reflects real-world scenarios in scientific literature comprehension, we design four finegrained tasks (as illustrated in Figure 1): direct reasoning, parallel reasoning, sequential reasoning, and analytical reasoning. Each task targets common reasoning type in multimodal scientific claim verification. Moreover, each example includes expert-annotated supporting evidence, facilitating fine-grained performance evaluation. We conduct an extensive evaluation on SCIVER, covering 21 frontier open-source and proprietary multimodal foundation models. Our experimental results reveal that while state-of-the-art models achieve human-comparable performance on simpler tasks (e.g., direct reasoning), they continue to struggle with more complex challenges. For instance, GPT-4.1, achieves an accuracy of 70.8% on analytical reasoning, falling significantly short of human expert performance (i.e., 90.0%). This demonstrates the challenging nature of SCIVER. Furthermore, our analysis of retrieval-augmented generation (RAG) and human-conducted error analyses provide insights for future advancement. Figure 1: An illustration of the four subsets in the SCIVER benchmark. Our benchmark is designed to evaluate document-grounded scientific claim verification in multimodal setting. To effectively perform this task, models must go through the full context of scientific paperincluding text, charts, and tablesto locate the appropriate supporting evidence before verifying claim. The complete data examples are provided in Appendix C. We summarize our contributions as follows: We introduce new claim verification benchmark to challenge foundation models across diverse reasoning scenarios in multimodal scientific literature comprehension. Each example undergoes expert annotation and strict quality control to ensure benchmark reliability and high standards. We conduct an extensive evaluation that encompasses 21 open-source and proprietary foundation models, comprehensively assessing their capabilities and limitations in our task. We provide an in-depth analysis of Chain-ofThought reasoning, RAG settings, and model reasoning errors, offering valuable insights for future advancements and targeted improvements.2021; Wadden et al., 2022; Rangapur et al., 2024). The second is context-grounded claim verification, where claims are verified based solely on given context, without relying on external retrieval (Chen et al., 2020; Kamoi et al., 2023; Lu et al., 2023; Glockner et al., 2024; Zhao et al., 2024). This work focuses on the latter setting, as it removes variability introduced by retriever performance and enables more controlled evaluation of foundation models ability to verify claims within multimodal scientific context. As shown in Table 1, existing multimodal claim verification benchmarks primarily use either single table (Chen et al., 2020; Gupta et al., 2020; Lu et al., 2023) or single chart (Akhtar et al., 2024) as input context. In real-world scenarios, however, verifying claims in scientific literature requires reasoning across multiple modalities, including textual descriptions, tables, and figures."
[19.06.2025 04:21] Mistral response. {"id": "51ca9119a9954735b92b088110d89586", "object": "chat.completion", "created": 1750306865, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1454, "total_tokens": 1456, "completion_tokens": 2}}
[19.06.2025 04:21] Response: []
[19.06.2025 04:21] Deleting PDF ./assets/pdf/2506.15569.pdf.
[19.06.2025 04:21] Success.
[19.06.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2506.14435.
[19.06.2025 04:21] Extra JSON file exists (./assets/json/2506.14435.json), skip PDF parsing.
[19.06.2025 04:21] Paper image links file exists (./assets/img_data/2506.14435.json), skip HTML parsing.
[19.06.2025 04:21] Success.
[19.06.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2506.06279.
[19.06.2025 04:21] Extra JSON file exists (./assets/json/2506.06279.json), skip PDF parsing.
[19.06.2025 04:21] Paper image links file exists (./assets/img_data/2506.06279.json), skip HTML parsing.
[19.06.2025 04:21] Success.
[19.06.2025 04:21] Enriching papers with extra data.
[19.06.2025 04:21] ********************************************************************************
[19.06.2025 04:21] Abstract 0. GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.  					AI-generated summary 				 Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve per...
[19.06.2025 04:21] ********************************************************************************
[19.06.2025 04:21] Abstract 1. Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.  					AI-generated summary 				 AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge o...
[19.06.2025 04:21] ********************************************************************************
[19.06.2025 04:21] Abstract 2. A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  					AI-generated summary 				 We introduce SciVer, the first benchmark specifically designed to evaluate the ab...
[19.06.2025 04:21] ********************************************************************************
[19.06.2025 04:21] Abstract 3. MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  					AI-generated summary 				 Large multimodal Mixture-of-Experts (MoEs) effectively scale the m...
[19.06.2025 04:21] ********************************************************************************
[19.06.2025 04:21] Abstract 4. CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.  					AI-generated summary 				 Recent advancements in Large Vision-Language Models built upon Large Language Models have establishe...
[19.06.2025 04:21] Read previous papers.
[19.06.2025 04:21] Generating reviews via LLM API.
[19.06.2025 04:21] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#inference", "#training", "#multimodal", "#dataset", "#architecture"], "emoji": "üî¨", "ru": {"title": "GenRecal: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "GenRecal - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è –≤–∏–∑—É–∞–ª—å
[19.06.2025 04:21] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#open_source", "#agents", "#multimodal", "#agi", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –∏ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –º–∏—Ä–æ–≤ –≤ –ò–ò-–∞–≥–µ–Ω—Ç–∞—Ö –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ - Embodied Web Agents,
[19.06.2025 04:21] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#rag", "#science", "#benchmark", "#open_source"], "emoji": "üß™", "ru": {"title": "SciVer: –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—É—á–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –ò–ò-–º–æ–¥–µ–ª—è–º–∏", "desc": "SciVer - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–µ—Ä–∏—Ñ
[19.06.2025 04:21] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#inference", "#training"], "emoji": "üß†", "ru": {"title": "MoTE: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ Mixture-of-Experts –º–æ–¥–µ–ª–∏ –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MoTE - –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π Mixture-of-Experts —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ—Ä–Ω–∞—Ä–Ω—ã
[19.06.2025 04:21] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#multimodal", "#reasoning", "#architecture"], "emoji": "üß†", "ru": {"title": "CoMemo: –£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "CoMemo - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, —Ä–µ—à–∞—é—â–∞—è –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–Ω–µ–±—Ä–µ–∂–µ–Ω–∏—è –≤–∏–∑
[19.06.2025 04:21] Renaming data file.
[19.06.2025 04:21] Renaming previous data. hf_papers.json to ./d/2025-06-19.json
[19.06.2025 04:21] Saving new data file.
[19.06.2025 04:21] Generating page.
[19.06.2025 04:21] Renaming previous page.
[19.06.2025 04:21] Renaming previous data. index.html to ./d/2025-06-19.html
[19.06.2025 04:21] Writing result.
[19.06.2025 04:21] Renaming log file.
[19.06.2025 04:21] Renaming previous data. log.txt to ./logs/2025-06-19_last_log.txt
