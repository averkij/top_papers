[19.06.2025 02:45] Read previous papers.
[19.06.2025 02:45] Generating top page (month).
[19.06.2025 02:45] Writing top page (month).
[19.06.2025 03:43] Read previous papers.
[19.06.2025 03:43] Get feed.
[19.06.2025 03:43] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15681
[19.06.2025 03:43] Extract page data from URL. URL: https://huggingface.co/papers/2506.15569
[19.06.2025 03:43] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15677
[19.06.2025 03:43] Extract page data from URL. URL: https://huggingface.co/papers/2506.14435
[19.06.2025 03:43] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06279
[19.06.2025 03:43] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.06.2025 03:43] No deleted papers detected.
[19.06.2025 03:43] Downloading and parsing papers (pdf, html). Total: 5.
[19.06.2025 03:43] Downloading and parsing paper https://huggingface.co/papers/2506.15681.
[19.06.2025 03:43] Extra JSON file exists (./assets/json/2506.15681.json), skip PDF parsing.
[19.06.2025 03:43] Paper image links file exists (./assets/img_data/2506.15681.json), skip HTML parsing.
[19.06.2025 03:43] Success.
[19.06.2025 03:43] Downloading and parsing paper https://huggingface.co/papers/2506.15569.
[19.06.2025 03:43] Failed to download and parse paper https://huggingface.co/papers/2506.15569: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))
[19.06.2025 03:43] Downloading and parsing paper https://huggingface.co/papers/2506.15677.
[19.06.2025 03:43] Extra JSON file exists (./assets/json/2506.15677.json), skip PDF parsing.
[19.06.2025 03:43] Paper image links file exists (./assets/img_data/2506.15677.json), skip HTML parsing.
[19.06.2025 03:43] Success.
[19.06.2025 03:43] Downloading and parsing paper https://huggingface.co/papers/2506.14435.
[19.06.2025 03:43] Downloading paper 2506.14435 from http://arxiv.org/pdf/2506.14435v1...
[19.06.2025 03:44] Extracting affiliations from text.
[19.06.2025 03:44] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 1 ] . [ 1 5 3 4 4 1 . 6 0 5 2 : r MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models Hongyu Wang Jiayu Xu Ruiping Wang Yan Feng Yitao Zhai Peng Pei Xunliang Cai Xilin Chen Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences Meituan University of Chinese Academy of Sciences "
[19.06.2025 03:44] Response: ```python
["Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences", "Meituan University of Chinese Academy of Sciences"]
```
[19.06.2025 03:44] Deleting PDF ./assets/pdf/2506.14435.pdf.
[19.06.2025 03:44] Success.
[19.06.2025 03:44] Downloading and parsing paper https://huggingface.co/papers/2506.06279.
[19.06.2025 03:44] Downloading paper 2506.06279 from http://arxiv.org/pdf/2506.06279v1...
[19.06.2025 03:44] Extracting affiliations from text.
[19.06.2025 03:44] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CoMemo: LVLMs Need Image Context with Image Memory Shi Liu * 1 Weijie Su * (cid:0) 1 Xizhou Zhu 2 1 Wenhai Wang 3 1 Jifeng Dai 2 1 5 2 0 2 6 ] . [ 1 9 7 2 6 0 . 6 0 5 2 : r a "
[19.06.2025 03:44] Response: ```python
[]
```
[19.06.2025 03:44] Extracting affiliations from text.
[19.06.2025 03:44] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CoMemo: LVLMs Need Image Context with Image Memory Shi Liu * 1 Weijie Su * (cid:0) 1 Xizhou Zhu 2 1 Wenhai Wang 3 1 Jifeng Dai 2 1 5 2 0 2 6 ] . [ 1 9 7 2 6 0 . 6 0 5 2 : r aRecent advancements in Large Vision-Language Models built upon Large Language Models have established aligning visual features with LLM representations as the dominant paradigm. However, inherited LLM architectural designs introduce suboptimal characteristics for multimodal processing. First, LVLMs exhibit bimodal distribution in attention allocation, leading to the progressive neglect of middle visual content as context expands. Second, conventional positional encoding schemes fail to preserve vital 2D structural relationships when processing dynamic high-resolution images. To address these limitations, we propose CoMemo - dual-path architecture that combines Context image path with an image Memory path for visual processing, effectively alleviating visual information neglect. Additionally, we introduce RoPE-DHR, novel positional encoding mechanism that employs thumbnail-based positional aggregation to maintain 2D spatial awareness while mitigating remote decay in extended sequences. Evaluations across seven benchmarks,including long-context comprehension, multi-image reasoning, and visual question answering, demonstrate CoMemos superior performance compared to conventional LVLM architectures. Project page is available at https://lalbj.github.io/projects/CoMemo/. 1. Introduction Recent advances in large language models (LLMs) have demonstrated unprecedented generative capabilities (Ouyang et al., 2022; Touvron et al., 2023), primarily driven by the exponential scaling of training data and model *Equal contribution Project lead 1Shanghai Artificial Intelligence Laboratory 2Tsinghua University 3The Chinese University of Hong Kong. Correspondence to: Weijie Su <suweijie@pjlab.org.cn>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). parameters. Building upon this foundation, large visionlanguage models (LVLMs) have emerged as powerful multimodal systems that align visual representations with LLM embedding spaces to enable cross-modal reasoning (Liu et al., 2024b; Alayrac et al., 2022). Current methodologies for visual information injection predominantly follow two architectural paradigms. The first paradigm (referred to as LVLM-X, e.g., Flamingo (Alayrac et al., 2022)) employs cross-attention mechanisms to integrate visual features into textual representations. While this approach offers flexible modality interaction, recent studies (Laurencon et al., 2024) have revealed its suboptimal performance compared to alternative approaches when using identical LLM backbones and training data - finding corroborated by our studies in Figure 1. The second paradigm (referred to as LVLM-S, e.g., LLaVA (Liu et al., 2024b)) aligns visual tokens into text token embeddings space and then performs autoregressive processing. This paradigm is more compatible with LLM architectures; however, the preservation intrinsic mechanisms such as attention bimodality (Xiao et al., 2023; Liu et al., 2025a) and the linearly increasing position encoding leads to critical limitations: (1) the lost in the middle (Liu et al., 2024c; Song et al., 2024) phenomenon degrades performance with increasing context length, and (2) positional encoding sparsity induces remote decay and 2d-dimensional lost in high-resolution image processing. In this paper, we propose novel framework for LVLM, named CoMemo. Our key idea is to introduce an additional image-processing mechanism that is unaffected by context, without modifying the internal mechanisms of the LLM. Specifically, we concatenate image tokens with text tokens as the input sequence for fully autoregressive processing, while simultaneously feeding the image tokens into mixin layer for cross-attention computation. Crossattention retrieves image information based on text, avoiding the issue of image neglect that can occur in causal selfattention. However, simply combining these two structures, as in NVLM-H (Dai et al., 2024), does not work well. To address this, we first investigated balanced scheme for visual representation inputs. Then, we introduced threestage training technique to prevent overreliance on the crossattention path, effectively transforming it into memory CoMemo: LVLMs Need Image Context with Image Memory (a) LVLM-S. (b) LVLM-X. (c) Ours. Figure 1: Evaluation results of three architectures with same training data and model size (2B). Please refer to Tables 2 to 4 for details. Figure 2: Comparison three types of architectures for LVLMs. Method (a) use image encoder to align visual features with the LLMs continuous token representation space. Method (b) employs mixin layer with cross-attention to update LLMs hidden states based on visual features. And Method (c) contrust dual-path structure to enable the model to focus more on visual content during generation. path. Meanwhile, the fully autoregressive process serves as the primary path for introducing image context, referred to as the context path. The positional encoding scheme in LVLMs typically adopts RoPE from LLMs, treating each image patch token as an individual token for encoding. However, this approach results in highly sparse positional encodings for dynamic highresolution image patch tokens. Such sparse encoding can lead to remote decay issues in positional encoding, and the one-dimensional incremental encoding scheme also loses the two-dimensional information of the image. To address this, we propose novel positional encoding scheme based on the dynamic high-resolution method. Specifically, in the dynamic high-resolution approach, the image is divided into multiple image tiles and single image thumbnail. We treat the image thumbnail as part of the input sequence for standard sequential positional encoding, while mapping the image tiles to the image thumbnail index based on their two-dimensional positional relationships. To fully evaluate the impact of the CoMemo architecture, we collected set of multimodal benchmarks and categorized them into seven evaluation tasks: Caption, Long-generation, Multi-image, Long-context, Math, General VQA, and OCRrelated tasks. The results demonstrates CoMemos superiority over LVLM-X/S baselines under same data and model settings. Our framework achieves 17.2%, 7.0% and 5.6% relative improvement on Caption, Lo"
[19.06.2025 03:44] Mistral response. {"id": "92747031caee4e04a793159c9746c76f", "object": "chat.completion", "created": 1750304666, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Shanghai Artificial Intelligence Laboratory\", \"Tsinghua University\", \"The Chinese University of Hong Kong\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1625, "total_tokens": 1657, "completion_tokens": 32}}
[19.06.2025 03:44] Response: ```python
["Shanghai Artificial Intelligence Laboratory", "Tsinghua University", "The Chinese University of Hong Kong"]
```
[19.06.2025 03:44] Deleting PDF ./assets/pdf/2506.06279.pdf.
[19.06.2025 03:44] Success.
[19.06.2025 03:44] Enriching papers with extra data.
[19.06.2025 03:44] ********************************************************************************
[19.06.2025 03:44] Abstract 0. GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.  					AI-generated summary 				 Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve per...
[19.06.2025 03:44] ********************************************************************************
[19.06.2025 03:44] Abstract 1. A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  					AI-generated summary 				 We introduce SciVer, the first benchmark specifically designed to evaluate the ab...
[19.06.2025 03:44] ********************************************************************************
[19.06.2025 03:44] Abstract 2. Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.  					AI-generated summary 				 AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge o...
[19.06.2025 03:44] ********************************************************************************
[19.06.2025 03:44] Abstract 3. MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  					AI-generated summary 				 Large multimodal Mixture-of-Experts (MoEs) effectively scale the m...
[19.06.2025 03:44] ********************************************************************************
[19.06.2025 03:44] Abstract 4. CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.  					AI-generated summary 				 Recent advancements in Large Vision-Language Models built upon Large Language Models have establishe...
[19.06.2025 03:44] Read previous papers.
[19.06.2025 03:44] Generating reviews via LLM API.
[19.06.2025 03:44] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#inference", "#training", "#multimodal", "#dataset", "#architecture"], "emoji": "ğŸ”¬", "ru": {"title": "GenRecal: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "GenRecal - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒ
[19.06.2025 03:44] Querying the API.
[19.06.2025 03:44] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  					AI-generated summary 				 We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks.
[19.06.2025 03:44] Response: {
  "desc": "SciVer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 3000 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· 1113 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ñ‚Ğ¸Ğ¿Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 21 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ o4-mini, Gemini-2.5-Flash Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸-Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹.",
  "emoji": "ğŸ§ª",
  "title": "SciVer: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸"
}
[19.06.2025 03:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  					AI-generated summary 				 We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks."

[19.06.2025 03:44] Response: ```python
['BENCHMARK', 'MULTIMODAL', 'RAG']
```
[19.06.2025 03:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  					AI-generated summary 				 We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks."

[19.06.2025 03:44] Response: ```python
['REASONING', 'SCIENCE', 'OPEN_SOURCE']
```
[19.06.2025 03:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces SciVer, a benchmark designed to assess how well multimodal foundation models can verify claims in scientific contexts. It includes 3,000 expert-annotated examples from 1,113 scientific papers, focusing on four reasoning types relevant to claim verification. The study evaluates 21 advanced multimodal models, revealing significant performance gaps compared to human experts. Additionally, it highlights limitations in current models and provides insights for improving their understanding and reasoning capabilities in scientific literature.","title":"Bridging the Gap: Evaluating Multimodal Models in Scientific Claim Verification"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces SciVer, a benchmark designed to assess how well multimodal foundation models can verify claims in scientific contexts. It includes 3,000 expert-annotated examples from 1,113 scientific papers, focusing on four reasoning types relevant to claim verification. The study evaluates 21 advanced multimodal models, revealing significant performance gaps compared to human experts. Additionally, it highlights limitations in current models and provides insights for improving their understanding and reasoning capabilities in scientific literature.', title='Bridging the Gap: Evaluating Multimodal Models in Scientific Claim Verification'))
[19.06.2025 03:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SciVeræ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨ç§‘å­¦èƒŒæ™¯ä¸‹éªŒè¯å£°æ˜çš„èƒ½åŠ›ã€‚å®ƒåŒ…å«3000ä¸ªä¸“å®¶æ³¨é‡Šçš„ç¤ºä¾‹ï¼Œæ¶µç›–1113ç¯‡ç§‘å­¦è®ºæ–‡ï¼Œåˆ†ä¸ºå››ä¸ªå­é›†ï¼Œä»£è¡¨å¤šæ¨¡æ€ç§‘å­¦å£°æ˜éªŒè¯ä¸­çš„å¸¸è§æ¨ç†ç±»å‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†21ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è¡¨ç°ï¼Œå‘ç°è¿™äº›æ¨¡å‹ä¸äººç±»ä¸“å®¶ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚é€šè¿‡å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œäººç±»é”™è¯¯è¯„ä¼°çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«äº†å½“å‰å¼€æºæ¨¡å‹çš„å…³é”®å±€é™æ€§ï¼Œä¸ºæé«˜æ¨¡å‹åœ¨å¤šæ¨¡æ€ç§‘å­¦æ–‡çŒ®ä»»åŠ¡ä¸­çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦è§è§£ã€‚","title":"æå‡å¤šæ¨¡æ€æ¨¡å‹çš„ç§‘å­¦éªŒè¯èƒ½åŠ›"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SciVeræ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨ç§‘å­¦èƒŒæ™¯ä¸‹éªŒè¯å£°æ˜çš„èƒ½åŠ›ã€‚å®ƒåŒ…å«3000ä¸ªä¸“å®¶æ³¨é‡Šçš„ç¤ºä¾‹ï¼Œæ¶µç›–1113ç¯‡ç§‘å­¦è®ºæ–‡ï¼Œåˆ†ä¸ºå››ä¸ªå­é›†ï¼Œä»£è¡¨å¤šæ¨¡æ€ç§‘å­¦å£°æ˜éªŒè¯ä¸­çš„å¸¸è§æ¨ç†ç±»å‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†21ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è¡¨ç°ï¼Œå‘ç°è¿™äº›æ¨¡å‹ä¸äººç±»ä¸“å®¶ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚é€šè¿‡å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œäººç±»é”™è¯¯è¯„ä¼°çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«äº†å½“å‰å¼€æºæ¨¡å‹çš„å…³é”®å±€é™æ€§ï¼Œä¸ºæé«˜æ¨¡å‹åœ¨å¤šæ¨¡æ€ç§‘å­¦æ–‡çŒ®ä»»åŠ¡ä¸­çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦è§è§£ã€‚', title='æå‡å¤šæ¨¡æ€æ¨¡å‹çš„ç§‘å­¦éªŒè¯èƒ½åŠ›'))
[19.06.2025 03:44] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#open_source", "#agents", "#multimodal", "#agi", "#reasoning"], "emoji": "ğŸ¤–", "ru": {"title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ² Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° - Embodied Web Agents,
[19.06.2025 03:44] Querying the API.
[19.06.2025 03:44] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  					AI-generated summary 				 Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.
[19.06.2025 03:44] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MoTE - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµÑ€Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². MoTE Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰ÑƒÑ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ MoE-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ MoE-LLaVA Ğ½Ğ° 4.3% Ğ¿Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ².",

  "emoji": "ğŸ§ ",

  "title": "MoTE: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Mixture-of-Experts Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ"
}
[19.06.2025 03:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  					AI-generated summary 				 Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices."

[19.06.2025 03:44] Response: ```python
["INFERENCE", "ARCHITECTURE", "TRAINING"]
```
[19.06.2025 03:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  					AI-generated summary 				 Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices."

[19.06.2025 03:44] Response: ```python
["OPTIMIZATION"]
```
[19.06.2025 03:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces MoTE, a new method that enhances Mixture-of-Experts (MoE) models by using low-precision ternary experts, which consist of parameters limited to -1, 0, and 1. This approach allows for a significant reduction in memory usage while maintaining competitive performance compared to traditional full-precision experts. MoTE is designed to be scalable and efficient, making it suitable for deployment on edge devices where memory is limited. Experimental results show that MoTE not only matches the performance of existing models but also improves accuracy when combined with post-training quantization techniques.","title":"MoTE: Efficient Ternary Experts for Scalable Edge Deployment"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces MoTE, a new method that enhances Mixture-of-Experts (MoE) models by using low-precision ternary experts, which consist of parameters limited to -1, 0, and 1. This approach allows for a significant reduction in memory usage while maintaining competitive performance compared to traditional full-precision experts. MoTE is designed to be scalable and efficient, making it suitable for deployment on edge devices where memory is limited. Experimental results show that MoTE not only matches the performance of existing models but also improves accuracy when combined with post-training quantization techniques.', title='MoTE: Efficient Ternary Experts for Scalable Edge Deployment'))
[19.06.2025 03:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MoTEæ˜¯ä¸€ç§å¯æ‰©å±•ä¸”å†…å­˜é«˜æ•ˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹è¿›æ··åˆä¸“å®¶æ¨¡å‹ï¼Œä½¿ç”¨ä½ç²¾åº¦çš„ä¸‰å…ƒä¸“å®¶æ¥æå‡æ€§èƒ½å¹¶å‡å°‘å†…å­˜å ç”¨ï¼Œé€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚ä¸ä»¥å¾€ä¸»è¦ä½¿ç”¨å…¨ç²¾åº¦ä¸“å®¶çš„ç¨€ç–ä¸Šå‡æ–¹æ³•ä¸åŒï¼ŒMoTEé€šè¿‡è®­ç»ƒæ›´å¤šä½ç²¾åº¦ä¸“å®¶æ¥å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„å‰é¦ˆç½‘ç»œä½œä¸ºå…±äº«ä¸“å®¶ï¼Œå¹¶è®­ç»ƒå‚æ•°ä¸º{-1, 0, 1}çš„ä¸‰å…ƒè·¯ç”±ä¸“å®¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoTEåœ¨æ¨¡å‹è§„æ¨¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ‰©å±•è¶‹åŠ¿ï¼Œå¹¶åœ¨å†…å­˜å—é™çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ã€‚","title":"MoTEï¼šå†…å­˜é«˜æ•ˆçš„æ··åˆä¸“å®¶æ¨¡å‹"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MoTEæ˜¯ä¸€ç§å¯æ‰©å±•ä¸”å†…å­˜é«˜æ•ˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹è¿›æ··åˆä¸“å®¶æ¨¡å‹ï¼Œä½¿ç”¨ä½ç²¾åº¦çš„ä¸‰å…ƒä¸“å®¶æ¥æå‡æ€§èƒ½å¹¶å‡å°‘å†…å­˜å ç”¨ï¼Œé€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚ä¸ä»¥å¾€ä¸»è¦ä½¿ç”¨å…¨ç²¾åº¦ä¸“å®¶çš„ç¨€ç–ä¸Šå‡æ–¹æ³•ä¸åŒï¼ŒMoTEé€šè¿‡è®­ç»ƒæ›´å¤šä½ç²¾åº¦ä¸“å®¶æ¥å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„å‰é¦ˆç½‘ç»œä½œä¸ºå…±äº«ä¸“å®¶ï¼Œå¹¶è®­ç»ƒå‚æ•°ä¸º{-1, 0, 1}çš„ä¸‰å…ƒè·¯ç”±ä¸“å®¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoTEåœ¨æ¨¡å‹è§„æ¨¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ‰©å±•è¶‹åŠ¿ï¼Œå¹¶åœ¨å†…å­˜å—é™çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ã€‚', title='MoTEï¼šå†…å­˜é«˜æ•ˆçš„æ··åˆä¸“å®¶æ¨¡å‹'))
[19.06.2025 03:44] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#multimodal", "#reasoning", "#architecture"], "emoji": "ğŸ§ ", "ru": {"title": "CoMemo: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "CoMemo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ½ĞµĞ±Ñ€ĞµĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·
[19.06.2025 03:44] Renaming data file.
[19.06.2025 03:44] Renaming previous data. hf_papers.json to ./d/2025-06-19.json
[19.06.2025 03:44] Saving new data file.
[19.06.2025 03:44] Generating page.
[19.06.2025 03:44] Renaming previous page.
[19.06.2025 03:44] Renaming previous data. index.html to ./d/2025-06-19.html
[19.06.2025 03:44] Writing result.
[19.06.2025 03:44] Renaming log file.
[19.06.2025 03:44] Renaming previous data. log.txt to ./logs/2025-06-19_last_log.txt
