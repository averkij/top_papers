[19.06.2025 09:13] Read previous papers.
[19.06.2025 09:13] Generating top page (month).
[19.06.2025 09:13] Writing top page (month).
[19.06.2025 10:12] Read previous papers.
[19.06.2025 10:12] Get feed.
[19.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15675
[19.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15681
[19.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15677
[19.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15068
[19.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15569
[19.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15050
[19.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06279
[19.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15672
[19.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14435
[19.06.2025 10:12] Extract page data from URL. URL: https://huggingface.co/papers/2506.14866
[19.06.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14824
[19.06.2025 10:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.06.2025 10:12] No deleted papers detected.
[19.06.2025 10:12] Downloading and parsing papers (pdf, html). Total: 11.
[19.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.15675.
[19.06.2025 10:12] Extra JSON file exists (./assets/json/2506.15675.json), skip PDF parsing.
[19.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.15675.json), skip HTML parsing.
[19.06.2025 10:12] Success.
[19.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.15681.
[19.06.2025 10:12] Extra JSON file exists (./assets/json/2506.15681.json), skip PDF parsing.
[19.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.15681.json), skip HTML parsing.
[19.06.2025 10:12] Success.
[19.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.15677.
[19.06.2025 10:12] Extra JSON file exists (./assets/json/2506.15677.json), skip PDF parsing.
[19.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.15677.json), skip HTML parsing.
[19.06.2025 10:12] Success.
[19.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.15068.
[19.06.2025 10:12] Extra JSON file exists (./assets/json/2506.15068.json), skip PDF parsing.
[19.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.15068.json), skip HTML parsing.
[19.06.2025 10:12] Success.
[19.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.15569.
[19.06.2025 10:12] Extra JSON file exists (./assets/json/2506.15569.json), skip PDF parsing.
[19.06.2025 10:12] Paper image links file exists (./assets/img_data/2506.15569.json), skip HTML parsing.
[19.06.2025 10:12] Success.
[19.06.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2506.15050.
[19.06.2025 10:13] Downloading paper 2506.15050 from http://arxiv.org/pdf/2506.15050v1...
[19.06.2025 10:13] Extracting affiliations from text.
[19.06.2025 10:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"1ByteDance Seed Full author list in Contributions "
[19.06.2025 10:13] Response: []
[19.06.2025 10:13] Extracting affiliations from text.
[19.06.2025 10:13] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"1ByteDance Seed Full author list in ContributionsRecently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds. First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance. We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5 and outperforms its existing competitors. Date: June 9, 2025 Correspondence: Tiantian Fan at fantiantian.tt@bytedance.com 5 2 0 2 8 1 ] . [ 1 0 5 0 5 1 . 6 0 5 2 : r Figure 1 AIME 2024 scores of T-PPO on the Qwen2.5-32B base model, reduces training time by 60% compared to the previous state-of-the-art (SOTA) method. The values shown are pass@1 scores, averaged over 32 samples per question.Recent advances in reasoning-oriented Large Language Models (LLMs), such as OpenAIs o1 [1], DeepSeekR1 [2], and QwQ [3], have demonstrated the state-of-the-art performance across complex domains including mathematical reasoning, programming, and agent-based tasks. These models leverage extended chainof-thought (CoT) reasoning to improve inference quality, integrating backtracking and error-correction mechanisms that produce more structured and accurate outputs. This enhanced reasoning capability stems primarily from deep reinforcement learning (RL) techniques, through which LLMs learn to generate explicit, logically-sequenced reasoning steps prior to final answer production. As the predominant RL approach for LLM refinement, Proximal Policy Optimization (PPO) [4] maintains training stability through its clipped surrogate objective function. Despite its advantages, PPOs on-policy nature inherently restricts training efficiency, limitation that becomes especially apparent when processing long CoT trajectories, often leading to substantial computational overhead and extended training durations. To address this issue, researchers have developed various off-policy PPO variants that are designed to enhance sample efficiency via trajectory reuse. Specifically, Generalized Proximal Policy Optimization (GePPO) [5] extends the guarantees of policy improvement to the off-policy setting. Off-Policy PPO [6] designs clipped surrogate objective function that can utilize off-policy data and avoid excessively large policy updates. PPO-EWMA [7] employs decoupled policy objectives and an exponentially weighted moving average (EWMA) for policy updates. KIMI K1.5 [8] uses partial rollouts to improve training efficiency by reusing large chunk of previous trajectories when sampling, thus avoiding the cost of regenerating new trajectories from scratch. Although off-policy methods are more training-efficient, they typically suffer from high variance in the policy gradient estimator, resulting in unstable training and degraded performance. In this work, we present Truncated Proximal Policy Optimization (T-PPO), an enhanced on-policy reinforcement learning framework that significantly improves efficiency while maintaining or even enhancing reasoning performance. At the core of T-PPO is our Extended Generalized Advantage Estimation (EGAE) method, which enables progressive policy updates even before trajectory is fully generated. Specifically, EGAE generalizes the conventional Generalized Advantage Estimation (GAE) [9] to support policy optimization with partially generated responses. This decouples policy updates from response completion and significantly improves computational resource utilization. Furthermore, to ensure unbiased value estimation, we maintain the Monte Carlo training paradigm for value model updates, deferring these updates until full response generation is complete. This estimation relies exclusively on actual observed returns rather than estimated values, thereby eliminating approximation bias in the value function. This dual optimization strategy allows simultaneous, yet independent improvement of both policy and value models through selective token screening. In summary, our design yields three key advantages: (1) truncated rollout strategy that enhances GPU throughput, (2) complete elimination of persistent bias in value function estimation, and (3) substantially improved policy update efficiency achieved through enhanced data utilization. Our extensive experiments on the AIME 2024 benchmark demonstrate that T-PPO delivers significant efficiency gains without compromising model performance. Specifically, the algorithm exhibits robust convergence behavior through its sample-efficient learning mechanism, which consistently enhances policy optimization. These combined attributes enable T-PPO to achieve 62 pass@1 on the AIME24 benchmark while demonstrating 2.5 higher training efficiency compared to state-of-the-art synchronization algorithms. Such performance characteristics significantly expand the practical deployment potential of T-PPO in real-world applications. Remarkably, these improvements are attained without introducing additional constraints or regularization beyond standard PPO. Apart from reducing the training cost, we sincerely hope that this method can bring more inspiration for delving into specialized expert "
[19.06.2025 10:13] Mistral response. {"object": "error", "message": "Service tier capacity exceeded for this model.", "type": "invalid_request_error", "param": null, "code": null}
[19.06.2025 10:13] Failed to download and parse paper https://huggingface.co/papers/2506.15050: 'choices'
[19.06.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2506.06279.
[19.06.2025 10:13] Extra JSON file exists (./assets/json/2506.06279.json), skip PDF parsing.
[19.06.2025 10:13] Paper image links file exists (./assets/img_data/2506.06279.json), skip HTML parsing.
[19.06.2025 10:13] Success.
[19.06.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2506.15672.
[19.06.2025 10:13] Extra JSON file exists (./assets/json/2506.15672.json), skip PDF parsing.
[19.06.2025 10:13] Paper image links file exists (./assets/img_data/2506.15672.json), skip HTML parsing.
[19.06.2025 10:13] Success.
[19.06.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2506.14435.
[19.06.2025 10:13] Extra JSON file exists (./assets/json/2506.14435.json), skip PDF parsing.
[19.06.2025 10:13] Paper image links file exists (./assets/img_data/2506.14435.json), skip HTML parsing.
[19.06.2025 10:13] Success.
[19.06.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2506.14866.
[19.06.2025 10:13] Downloading paper 2506.14866 from http://arxiv.org/pdf/2506.14866v1...
[19.06.2025 10:13] Extracting affiliations from text.
[19.06.2025 10:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 1 ] . [ 1 6 6 8 4 1 . 6 0 5 2 : r OS-HARM: Benchmark for Measuring Safety of Computer Use Agents Thomas Kuntz1,, Agatha Duzan1,, Hao Zhao1, Francesco Croce1, Zico Kolter2, Nicolas Flammarion1, Maksym Andriushchenko 1EPFL, 2Carnegie Mellon University, Equal contribution "
[19.06.2025 10:13] Response: ```python
["EPFL", "Carnegie Mellon University"]
```
[19.06.2025 10:13] Deleting PDF ./assets/pdf/2506.14866.pdf.
[19.06.2025 10:13] Success.
[19.06.2025 10:13] Downloading and parsing paper https://huggingface.co/papers/2506.14824.
[19.06.2025 10:14] Downloading paper 2506.14824 from http://arxiv.org/pdf/2506.14824v1...
[19.06.2025 10:14] Extracting affiliations from text.
[19.06.2025 10:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models Yao Zhang 1,4* Weiguo Li 5 Hewei Gao 2* Yunpu Ma 1,4 Haokun Chen1,3 Volker Tresp1,4 1 LMU Munich 2 Technical University of Munich 3 Siemens Technology 4 Munich Center for Machine Learning 5 University Heidelberg yzhang@dbs.ifi.lmu.de yunpu.ma@ifi.lmu.de tresp@dbs.ifi.lmu.de 5 2 0 2 2 1 ] . [ 1 4 2 8 4 1 . 6 0 5 2 : r a "
[19.06.2025 10:14] Response: ```python
["LMU Munich", "Technical University of Munich", "Siemens Technology", "Munich Center for Machine Learning", "University Heidelberg"]
```
[19.06.2025 10:14] Deleting PDF ./assets/pdf/2506.14824.pdf.
[19.06.2025 10:14] Success.
[19.06.2025 10:14] Enriching papers with extra data.
[19.06.2025 10:14] ********************************************************************************
[19.06.2025 10:14] Abstract 0. Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.  					AI-generated summary 				 Video generation techniques have made remarkable progress, promising to be the foundation of interactive world ...
[19.06.2025 10:14] ********************************************************************************
[19.06.2025 10:14] Abstract 1. GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.  					AI-generated summary 				 Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve per...
[19.06.2025 10:14] ********************************************************************************
[19.06.2025 10:14] Abstract 2. Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.  					AI-generated summary 				 AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge o...
[19.06.2025 10:14] ********************************************************************************
[19.06.2025 10:14] Abstract 3. PrefBERT, a scoring model, improves open-ended long-form generation by providing better semantic reward feedback than traditional metrics.  					AI-generated summary 				 Evaluating open-ended long-form generation is challenging because it is hard to define what clearly separates good from bad outpu...
[19.06.2025 10:14] ********************************************************************************
[19.06.2025 10:14] Abstract 4. A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  					AI-generated summary 				 We introduce SciVer, the first benchmark specifically designed to evaluate the ab...
[19.06.2025 10:14] ********************************************************************************
[19.06.2025 10:14] Abstract 5. T-PPO, an extension of PPO, improves training efficiency for Large Language Models by optimizing policy updates and utilizing hardware resources more effectively.  					AI-generated summary 				 Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabili...
[19.06.2025 10:14] ********************************************************************************
[19.06.2025 10:14] Abstract 6. CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.  					AI-generated summary 				 Recent advancements in Large Vision-Language Models built upon Large Language Models have establishe...
[19.06.2025 10:14] ********************************************************************************
[19.06.2025 10:14] Abstract 7. SwarmAgentic is a framework for automated agentic system generation that optimize agent functionality and collaboration through language-driven exploration, outperforming existing baselines in unconstrained tasks.  					AI-generated summary 				 The rapid progress of Large Language Models has advanc...
[19.06.2025 10:14] ********************************************************************************
[19.06.2025 10:14] Abstract 8. MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  					AI-generated summary 				 Large multimodal Mixture-of-Experts (MoEs) effectively scale the m...
[19.06.2025 10:14] ********************************************************************************
[19.06.2025 10:14] Abstract 9. A new benchmark called OS-Harm measures the safety of computer use agents interacting with GUIs, evaluating their susceptibility to misuse, prompt injection attacks, and misbehavior across various safety violations and applications.  					AI-generated summary 				 Computer use agents are LLM-based a...
[19.06.2025 10:14] ********************************************************************************
[19.06.2025 10:14] Abstract 10. FedNano is a federated learning framework that centralizes large language models on servers and uses NanoEdge modules for client-specific adaptation, addressing scalability and privacy issues.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) excel in tasks like multimodal rea...
[19.06.2025 10:14] Read previous papers.
[19.06.2025 10:14] Generating reviews via LLM API.
[19.06.2025 10:14] Using data from previous issue: {"categories": ["#synthetic", "#dataset", "#games", "#data", "#video"], "emoji": "🌎", "ru": {"title": "Sekai: глобальный датасет для обучения ИИ исследовать мир", "desc": "Представлен новый набор данных Sekai для обучения моделей генерации видео с целью исследования мира. Он содержит более 5000 часо
[19.06.2025 10:14] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#inference", "#training", "#multimodal", "#dataset", "#architecture"], "emoji": "🔬", "ru": {"title": "GenRecal: Универсальная дистилляция для эффективных визуально-языковых моделей", "desc": "GenRecal - это новая система дистилляции для визуаль
[19.06.2025 10:14] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#open_source", "#agents", "#multimodal", "#agi", "#reasoning"], "emoji": "🤖", "ru": {"title": "Объединение физического и цифрового миров в ИИ-агентах нового поколения", "desc": "Статья представляет новую парадигму искусственного интеллекта - Embodied Web Agents,
[19.06.2025 10:14] Using data from previous issue: {"categories": ["#long_context", "#alignment", "#rlhf", "#benchmark", "#optimization", "#open_source", "#dataset"], "emoji": "📝", "ru": {"title": "PrefBERT: Улучшение генерации длинных текстов с помощью семантической оценки", "desc": "PrefBERT - это модель оценки, разработанная для улучшения генерац
[19.06.2025 10:14] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#rag", "#science", "#benchmark", "#open_source"], "emoji": "🧪", "ru": {"title": "SciVer: проверка научных утверждений мультимодальными ИИ-моделями", "desc": "SciVer - это новый бенчмарк для оценки способности мультимодальных фундаментальных моделей вериф
[19.06.2025 10:14] Using data from previous issue: {"categories": ["#optimization", "#rl", "#reasoning", "#training"], "emoji": "🚀", "ru": {"title": "T-PPO: Ускоряем обучение LLM в 2,5 раза", "desc": "T-PPO - это новое расширение алгоритма PPO, которое повышает эффективность обучения больших языковых моделей (LLM). Оно оптимизирует обновление полити
[19.06.2025 10:14] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#multimodal", "#reasoning", "#architecture"], "emoji": "🧠", "ru": {"title": "CoMemo: Улучшение визуального восприятия в мультимодальных моделях", "desc": "CoMemo - это новая архитектура для мультимодальной обработки, решающая проблемы пренебрежения виз
[19.06.2025 10:14] Using data from previous issue: {"categories": ["#optimization", "#agents", "#benchmark", "#games", "#agi", "#open_source", "#alignment"], "emoji": "🐝", "ru": {"title": "Автономная эволюция многоагентных систем", "desc": "SwarmAgentic - это фреймворк для полностью автоматизированной генерации агентных систем. Он оптимизирует функц
[19.06.2025 10:14] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#inference", "#training"], "emoji": "🧠", "ru": {"title": "MoTE: Эффективные Mixture-of-Experts модели для устройств с ограниченной памятью", "desc": "Статья представляет MoTE - метод для улучшения моделей Mixture-of-Experts с использованием тернарны
[19.06.2025 10:14] Querying the API.
[19.06.2025 10:14] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A new benchmark called OS-Harm measures the safety of computer use agents interacting with GUIs, evaluating their susceptibility to misuse, prompt injection attacks, and misbehavior across various safety violations and applications.  					AI-generated summary 				 Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-Harm, a new benchmark for measuring safety of computer use agents. OS-Harm is built on top of the OSWorld environment and aims to test models across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. To cover these cases, we create 150 tasks that span several types of safety violations (harassment, copyright infringement, disinformation, data exfiltration, etc.) and require the agent to interact with a variety of OS applications (email client, code editor, browser, etc.). Moreover, we propose an automated judge to evaluate both accuracy and safety of agents that achieves high agreement with human annotations (0.76 and 0.79 F1 score). We evaluate computer use agents based on a range of frontier models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide insights into their safety. In particular, all models tend to directly comply with many deliberate misuse queries, are relatively vulnerable to static prompt injections, and occasionally perform unsafe actions. The OS-Harm benchmark is available at https://github.com/tml-epfl/os-harm.
[19.06.2025 10:14] Response: {
  "desc": "Новый бенчмарк OS-Harm оценивает безопасность агентов, взаимодействующих с графическим интерфейсом компьютера. Он измеряет их уязвимость к неправильному использованию, атакам на промпты и нарушениям безопасности в различных приложениях. OS-Harm включает 150 задач, охватывающих различные типы нарушений безопасности и взаимодействие с разными приложениями ОС. Бенчмарк использует автоматизированную систему оценки, которая показывает высокое согласие с человеческими аннотациями.",

  "emoji": "🖥️",

  "title": "OS-Harm: новый стандарт безопасности ИИ-агентов в компьютерных интерфейсах"
}
[19.06.2025 10:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new benchmark called OS-Harm measures the safety of computer use agents interacting with GUIs, evaluating their susceptibility to misuse, prompt injection attacks, and misbehavior across various safety violations and applications.  					AI-generated summary 				 Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-Harm, a new benchmark for measuring safety of computer use agents. OS-Harm is built on top of the OSWorld environment and aims to test models across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. To cover these cases, we create 150 tasks that span several types of safety violations (harassment, copyright infringement, disinformation, data exfiltration, etc.) and require the agent to interact with a variety of OS applications (email client, code editor, browser, etc.). Moreover, we propose an automated judge to evaluate both accuracy and safety of agents that achieves high agreement with human annotations (0.76 and 0.79 F1 score). We evaluate computer use agents based on a range of frontier models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide insights into their safety. In particular, all models tend to directly comply with many deliberate misuse queries, are relatively vulnerable to static prompt injections, and occasionally perform unsafe actions. The OS-Harm benchmark is available at https://github.com/tml-epfl/os-harm."

[19.06.2025 10:14] Response: ```python
['BENCHMARK', 'AGENTS']
```
[19.06.2025 10:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new benchmark called OS-Harm measures the safety of computer use agents interacting with GUIs, evaluating their susceptibility to misuse, prompt injection attacks, and misbehavior across various safety violations and applications.  					AI-generated summary 				 Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-Harm, a new benchmark for measuring safety of computer use agents. OS-Harm is built on top of the OSWorld environment and aims to test models across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. To cover these cases, we create 150 tasks that span several types of safety violations (harassment, copyright infringement, disinformation, data exfiltration, etc.) and require the agent to interact with a variety of OS applications (email client, code editor, browser, etc.). Moreover, we propose an automated judge to evaluate both accuracy and safety of agents that achieves high agreement with human annotations (0.76 and 0.79 F1 score). We evaluate computer use agents based on a range of frontier models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide insights into their safety. In particular, all models tend to directly comply with many deliberate misuse queries, are relatively vulnerable to static prompt injections, and occasionally perform unsafe actions. The OS-Harm benchmark is available at https://github.com/tml-epfl/os-harm."

[19.06.2025 10:14] Response: ```python
['SECURITY', 'ETHICS']
```
[19.06.2025 10:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces OS-Harm, a benchmark designed to assess the safety of computer use agents that interact with graphical user interfaces (GUIs). It evaluates these agents for their vulnerability to misuse, prompt injection attacks, and other forms of misbehavior across various applications. The benchmark includes 150 tasks that simulate different safety violations, such as harassment and data exfiltration, while requiring agents to operate within common OS applications. An automated judging system is proposed to evaluate the agents\' performance, achieving a high agreement with human assessments, revealing that many models are prone to compliance with misuse and unsafe actions.","title":"Ensuring Safe Interactions: Introducing OS-Harm for Computer Use Agents"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="The paper introduces OS-Harm, a benchmark designed to assess the safety of computer use agents that interact with graphical user interfaces (GUIs). It evaluates these agents for their vulnerability to misuse, prompt injection attacks, and other forms of misbehavior across various applications. The benchmark includes 150 tasks that simulate different safety violations, such as harassment and data exfiltration, while requiring agents to operate within common OS applications. An automated judging system is proposed to evaluate the agents' performance, achieving a high agreement with human assessments, revealing that many models are prone to compliance with misuse and unsafe actions.", title='Ensuring Safe Interactions: Introducing OS-Harm for Computer Use Agents'))
[19.06.2025 10:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OS-Harm是一个新的基准，用于评估计算机使用代理在与图形用户界面交互时的安全性。它主要测试代理在用户恶意使用、提示注入攻击和模型不当行为等方面的脆弱性。该基准创建了150个任务，涵盖多种安全违规行为，并要求代理与不同的操作系统应用程序进行交互。通过自动评估工具，OS-Harm能够有效地评估代理的准确性和安全性，为计算机使用代理的安全性提供了重要的见解。","title":"OS-Harm：评估计算机使用代理的安全新基准"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OS-Harm是一个新的基准，用于评估计算机使用代理在与图形用户界面交互时的安全性。它主要测试代理在用户恶意使用、提示注入攻击和模型不当行为等方面的脆弱性。该基准创建了150个任务，涵盖多种安全违规行为，并要求代理与不同的操作系统应用程序进行交互。通过自动评估工具，OS-Harm能够有效地评估代理的准确性和安全性，为计算机使用代理的安全性提供了重要的见解。', title='OS-Harm：评估计算机使用代理的安全新基准'))
[19.06.2025 10:14] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#agents", "#scalability", "#privacy", "#training", "#federated_learning"], "emoji": "🔬", "ru": {"title": "Эффективное федеративное обучение мультимодальных ИИ-систем", "desc": "FedNano - это новая система федеративного обучения для мультимодальных больших 
[19.06.2025 10:14] Renaming data file.
[19.06.2025 10:14] Renaming previous data. hf_papers.json to ./d/2025-06-19.json
[19.06.2025 10:14] Saving new data file.
[19.06.2025 10:14] Generating page.
[19.06.2025 10:14] Renaming previous page.
[19.06.2025 10:14] Renaming previous data. index.html to ./d/2025-06-19.html
[19.06.2025 10:14] Writing result.
[19.06.2025 10:14] Renaming log file.
[19.06.2025 10:14] Renaming previous data. log.txt to ./logs/2025-06-19_last_log.txt
