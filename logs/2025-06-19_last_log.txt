[19.06.2025 15:12] Read previous papers.
[19.06.2025 15:12] Generating top page (month).
[19.06.2025 15:12] Writing top page (month).
[19.06.2025 16:14] Read previous papers.
[19.06.2025 16:14] Get feed.
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15675
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15211
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15681
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.13414
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15677
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15068
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15569
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14842
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15050
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06279
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15672
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14315
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14435
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14824
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14866
[19.06.2025 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14770
[19.06.2025 16:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.06.2025 16:14] No deleted papers detected.
[19.06.2025 16:14] Downloading and parsing papers (pdf, html). Total: 16.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.15675.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.15675.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.15675.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.15211.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.15211.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.15211.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.15681.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.15681.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.15681.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.13414.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.13414.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.13414.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.15677.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.15677.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.15677.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.15068.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.15068.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.15068.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.15569.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.15569.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.15569.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.14842.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.14842.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.14842.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.15050.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.15050.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.15050.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.06279.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.06279.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.06279.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.15672.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.15672.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.15672.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.14315.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.14315.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.14315.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.14435.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.14435.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.14435.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.14824.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.14824.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.14824.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.14866.
[19.06.2025 16:14] Extra JSON file exists (./assets/json/2506.14866.json), skip PDF parsing.
[19.06.2025 16:14] Paper image links file exists (./assets/img_data/2506.14866.json), skip HTML parsing.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Downloading and parsing paper https://huggingface.co/papers/2506.14770.
[19.06.2025 16:14] Downloading paper 2506.14770 from http://arxiv.org/pdf/2506.14770v1...
[19.06.2025 16:14] Extracting affiliations from text.
[19.06.2025 16:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 1 ] . [ 1 0 7 7 4 1 . 6 0 5 2 : r GMT: General Motion Tracking for Humanoid Whole-Body Control Zixuan Chen1,2 Mazeyu Ji1 Xuxin Cheng1 Xuanbin Peng1 Xue Bin Peng Xiaolong Wang1 1UC San Diego 2Simon Fraser University Equal Contribution Equal Advising gmt-humanoid.github.io Figure 1: We deploy the general unified motion tracking policy on medium-sized humanoid robot. GMT can perform wide range of motion skills with good stability and generalizability, including (a) stretching, (b) kicking-ball, (c) dancing, (d) high kicking, (e) kungfu, and (f) other dynamic skills such as boxing, running, side stepping, and squatting. Abstract: The ability to track general whole-body motions in the real world is useful way to build general-purpose humanoid robots. However, achieving this can be challenging due to the temporal and kinematic diversity of the motions, the policys capability, and the difficulty of coordination of the upper and lower bodies. To address these issues, we propose GMT, general and scalable motiontracking framework that trains single unified policy to enable humanoid robots to track diverse motions in the real world. GMT is built upon two core components: an Adaptive Sampling strategy and Motion Mixture-of-Experts (MoE) architecture. The Adaptive Sampling automatically balances easy and difficult motions during training. The MoE ensures better specialization of different regions of the motion manifold. We show through extensive experiments in both simulation and the real world the effectiveness of GMT, achieving state-of-the-art performance across broad spectrum of motions using unified general policy. Videos and additional information can be found at gmt-humanoid.github.io. Keywords: Humanoid, Locomotion, Learning-based Control, Motion Imitation One of the primary goals for humanoid robots is to perform wide range of tasks in everyday environments. Enabling humanoid robots to produce broad repertoire of human-like movements is promising approach towa"
[19.06.2025 16:14] Response: ```python
["UC San Diego", "Simon Fraser University"]
```
[19.06.2025 16:14] Deleting PDF ./assets/pdf/2506.14770.pdf.
[19.06.2025 16:14] Success.
[19.06.2025 16:14] Enriching papers with extra data.
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 0. Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.  					AI-generated summary 				 Video generation techniques have made remarkable progress, promising to be the foundation of interactive world ...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 1. ProtoReasoning enhances large reasoning models through prototypical representations, leading to improved cross-domain generalization in logical reasoning, planning, and other tasks.  					AI-generated summary 				 Recent advances in Large Reasoning Models (LRMs) trained with Long Chain-of-Thought (L...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 2. GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.  					AI-generated summary 				 Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve per...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 3. The combined DiCoW and DiariZen ASR system demonstrates strong performance in multilingual scenarios, with DiCoW preserving its multilingual capabilities and DiariZen improving through fine-tuning.  					AI-generated summary 				 We present a two-speaker automatic speech recognition (ASR) system tha...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 4. Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.  					AI-generated summary 				 AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge o...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 5. PrefBERT, a scoring model, improves open-ended long-form generation by providing better semantic reward feedback than traditional metrics.  					AI-generated summary 				 Evaluating open-ended long-form generation is challenging because it is hard to define what clearly separates good from bad outpu...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 6. A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  					AI-generated summary 				 We introduce SciVer, the first benchmark specifically designed to evaluate the ab...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 7. PictSure is an in-context learning framework that enhances few-shot image classification by optimizing embedding models' architecture, pretraining, and fine-tuning strategies to improve out-of-domain performance.  					AI-generated summary 				 Building image classification models remains cumbersome...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 8. T-PPO, an extension of PPO, improves training efficiency for Large Language Models by optimizing policy updates and utilizing hardware resources more effectively.  					AI-generated summary 				 Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabili...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 9. CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.  					AI-generated summary 				 Recent advancements in Large Vision-Language Models built upon Large Language Models have establishe...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 10. SwarmAgentic is a framework for automated agentic system generation that optimize agent functionality and collaboration through language-driven exploration, outperforming existing baselines in unconstrained tasks.  					AI-generated summary 				 The rapid progress of Large Language Models has advanc...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 11. An agent-guided framework generates photorealistic 3D scenes for VR by synthesizing textures onto lightweight geometric proxies, enabling real-time rendering and superior visual quality.  					AI-generated summary 				 Automatic creation of 3D scenes for immersive VR presence has been a significant ...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 12. MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  					AI-generated summary 				 Large multimodal Mixture-of-Experts (MoEs) effectively scale the m...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 13. FedNano is a federated learning framework that centralizes large language models on servers and uses NanoEdge modules for client-specific adaptation, addressing scalability and privacy issues.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) excel in tasks like multimodal rea...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 14. A new benchmark called OS-Harm measures the safety of computer use agents interacting with GUIs, evaluating their susceptibility to misuse, prompt injection attacks, and misbehavior across various safety violations and applications.  					AI-generated summary 				 Computer use agents are LLM-based a...
[19.06.2025 16:14] ********************************************************************************
[19.06.2025 16:14] Abstract 15. GMT, a unified motion-tracking framework, addresses challenges in tracking diverse humanoid robot motions through adaptive sampling and a motion mixture-of-experts architecture, achieving state-of-the-art performance.  					AI-generated summary 				 The ability to track general whole-body motions in...
[19.06.2025 16:14] Read previous papers.
[19.06.2025 16:14] Generating reviews via LLM API.
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#synthetic", "#dataset", "#games", "#data", "#video"], "emoji": "üåé", "ru": {"title": "Sekai: –≥–ª–æ–±–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –º–∏—Ä", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Sekai –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —Ü–µ–ª—å—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –º–∏—Ä–∞. –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 5000 —á–∞—Å–æ
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#reasoning", "#transfer_learning", "#training", "#dataset", "#architecture"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ—Ç–æ—Ç–∏–ø—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∫–∞–∫ –∫–ª—é—á –∫ –æ–±–æ–±—â–µ–Ω–∏—é –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ProtoReasoning - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#inference", "#training", "#multimodal", "#dataset", "#architecture"], "emoji": "üî¨", "ru": {"title": "GenRecal: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "GenRecal - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–ª—è –≤–∏–∑—É–∞–ª—å
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#audio", "#training", "#multilingual", "#data"], "emoji": "üó£Ô∏è", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ DiCoW –∏ DiariZen: –º–æ—â–Ω–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ ASR", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–≤—É—Ö–¥–∏–∫—Ç–æ—Ä–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ (ASR), –æ–±—ä–µ–¥–∏–Ω—è—é—â—É—é DiCoW –∏ DiariZen. DiCoW - —ç—Ç–æ –≤–∞
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#open_source", "#agents", "#multimodal", "#agi", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –∏ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –º–∏—Ä–æ–≤ –≤ –ò–ò-–∞–≥–µ–Ω—Ç–∞—Ö –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ - Embodied Web Agents,
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#long_context", "#alignment", "#rlhf", "#benchmark", "#optimization", "#open_source", "#dataset"], "emoji": "üìù", "ru": {"title": "PrefBERT: –£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏", "desc": "PrefBERT - —ç—Ç–æ –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∫–∏, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#rag", "#science", "#benchmark", "#open_source"], "emoji": "üß™", "ru": {"title": "SciVer: –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—É—á–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –ò–ò-–º–æ–¥–µ–ª—è–º–∏", "desc": "SciVer - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–µ—Ä–∏—Ñ
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#dataset", "#cv", "#transfer_learning", "#optimization", "#training", "#benchmark"], "emoji": "üñºÔ∏è", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤–Ω–µ –¥–æ–º–µ–Ω–∞", "desc": "PictSure - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#optimization", "#rl", "#reasoning", "#training"], "emoji": "üöÄ", "ru": {"title": "T-PPO: –£—Å–∫–æ—Ä—è–µ–º –æ–±—É—á–µ–Ω–∏–µ LLM –≤ 2,5 —Ä–∞–∑–∞", "desc": "T-PPO - —ç—Ç–æ –Ω–æ–≤–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ PPO, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#multimodal", "#reasoning", "#architecture"], "emoji": "üß†", "ru": {"title": "CoMemo: –£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "CoMemo - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, —Ä–µ—à–∞—é—â–∞—è –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–Ω–µ–±—Ä–µ–∂–µ–Ω–∏—è –≤–∏–∑
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#optimization", "#agents", "#benchmark", "#games", "#agi", "#open_source", "#alignment"], "emoji": "üêù", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è —ç–≤–æ–ª—é—Ü–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "SwarmAgentic - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º. –û–Ω –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ñ—É–Ω–∫—Ü
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#3d", "#video", "#multimodal", "#synthetic", "#games", "#agents"], "emoji": "üï∂Ô∏è", "ru": {"title": "–§–æ—Ç–æ—Ä–µ–∞–ª–∏–∑–º –≤ VR –±–µ–∑ —Å–ª–æ–∂–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏", "desc": "ImmerseGen - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D-—Å—Ü–µ–Ω –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#inference", "#training"], "emoji": "üß†", "ru": {"title": "MoTE: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ Mixture-of-Experts –º–æ–¥–µ–ª–∏ –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MoTE - –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π Mixture-of-Experts —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ—Ä–Ω–∞—Ä–Ω—ã
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#agents", "#scalability", "#privacy", "#training", "#federated_learning"], "emoji": "üî¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-—Å–∏—Å—Ç–µ–º", "desc": "FedNano - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ñ–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö 
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#security", "#ethics", "#agents", "#benchmark"], "emoji": "üñ•Ô∏è", "ru": {"title": "OS-Harm: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞—Ö", "desc": "–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ OS-Harm –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–æ–≤, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–∞. –û–Ω –∏–∑
[19.06.2025 16:14] Using data from previous issue: {"categories": ["#robotics", "#architecture", "#agents"], "emoji": "ü§ñ", "ru": {"title": "GMT: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏–π –¥–ª—è –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "GMT - —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π –¥–ª—è –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å–º–µ—Å–∏ —ç–∫
[19.06.2025 16:14] Renaming data file.
[19.06.2025 16:14] Renaming previous data. hf_papers.json to ./d/2025-06-19.json
[19.06.2025 16:14] Saving new data file.
[19.06.2025 16:14] Generating page.
[19.06.2025 16:14] Renaming previous page.
[19.06.2025 16:14] Renaming previous data. index.html to ./d/2025-06-19.html
[19.06.2025 16:14] Writing result.
[19.06.2025 16:14] Renaming log file.
[19.06.2025 16:14] Renaming previous data. log.txt to ./logs/2025-06-19_last_log.txt
