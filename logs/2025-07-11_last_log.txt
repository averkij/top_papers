[11.07.2025 17:12] Read previous papers.
[11.07.2025 17:12] Generating top page (month).
[11.07.2025 17:12] Writing top page (month).
[11.07.2025 18:16] Read previous papers.
[11.07.2025 18:16] Get feed.
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07966
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05964
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07999
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07984
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07990
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07998
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07982
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07136
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07202
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07996
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06543
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07484
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07867
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07574
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07129
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05241
[11.07.2025 18:16] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04886
[11.07.2025 18:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.07.2025 18:16] No deleted papers detected.
[11.07.2025 18:16] Downloading and parsing papers (pdf, html). Total: 17.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.07966.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.07966.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.07966.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.05964.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.05964.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.05964.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.07999.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.07999.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.07999.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.07984.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.07984.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.07984.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.07990.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.07990.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.07990.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.07998.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.07998.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.07998.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.07982.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.07982.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.07982.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.07136.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.07136.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.07136.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.07202.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.07202.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.07202.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.07996.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.07996.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.07996.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.06543.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.06543.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.06543.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.07484.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.07484.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.07484.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.07867.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.07867.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.07867.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.07574.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.07574.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.07574.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.07129.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.07129.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.07129.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.05241.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.05241.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.05241.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Downloading and parsing paper https://huggingface.co/papers/2507.04886.
[11.07.2025 18:16] Extra JSON file exists (./assets/json/2507.04886.json), skip PDF parsing.
[11.07.2025 18:16] Paper image links file exists (./assets/img_data/2507.04886.json), skip HTML parsing.
[11.07.2025 18:16] Success.
[11.07.2025 18:16] Enriching papers with extra data.
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 0. A framework for scaling vision-language models to long videos using reinforcement learning, achieving strong performance on various reasoning tasks with a specialized training infrastructure.  					AI-generated summary 				 We introduce a full-stack framework that scales up reasoning in vision-langu...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 1. T-LoRA, a timestep-dependent low-rank adaptation framework, enhances diffusion model personalization with a dynamic fine-tuning strategy and orthogonal initialization, achieving better concept fidelity and text alignment in data-limited settings.  					AI-generated summary 				 While diffusion model...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 2. TreeBench evaluates visual grounded reasoning through subtle target detection, traceable evidence, and second-order reasoning, while TreeVGR enhances this with joint localization and reasoning using reinforcement learning.  					AI-generated summary 				 Models like OpenAI-o3 pioneer visual grounded...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 3. OST-Bench evaluates multimodal large language models in online spatio-temporal reasoning tasks, revealing challenges in handling complex spatial cues and long-term memory in real-world scenarios.  					AI-generated summary 				 Recent advances in multimodal large language models (MLLMs) have shown r...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 4. A spatio-temporal token merging method improves video LLM efficiency by exploiting redundancy, achieving significant speed-ups with minimal accuracy loss.  					AI-generated summary 				 Video large language models (LLMs) achieve strong video understanding by leveraging a large number of spatio-temp...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 5. PyVision, an interactive framework, enables LLMs to autonomously create and refine Python-based tools for visual reasoning, achieving significant performance improvements across benchmarks.  					AI-generated summary 				 LLMs are increasingly deployed as agents, systems capable of planning, reasoni...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 6. Videos inherently represent 2D projections of a dynamic 3D world. However, our analysis suggests that video diffusion models trained solely on raw video data often fail to capture meaningful geometric-aware structure in their learned representations. To bridge this gap between video diffusion models...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 7. LangSplatV2 enhances 3D text querying speed and accuracy by replacing the heavyweight decoder with a sparse coefficient field and efficient CUDA optimization.  					AI-generated summary 				 In this paper, we introduce LangSplatV2, which achieves high-dimensional feature splatting at 476.2 FPS and 3...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 8. Despite the significant progress that has been made in video generative models, existing state-of-the-art methods can only produce videos lasting 5-16 seconds, often labeled "long-form videos". Furthermore, videos exceeding 16 seconds struggle to maintain consistent character appearances and scene l...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 9. A method using chain-of-layers (CoLa) derived from a pretrained large language model allows for dynamic architecture adaptation, improving efficiency and accuracy across diverse tasks through selective layer manipulation and Monte Carlo Tree Search optimization.  					AI-generated summary 				 Can a...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 10. ToBo is a self-supervised learning method that creates compact, temporally aware visual representations for sequential scene understanding tasks, outperforming baselines in both simulated and real-world environments.  					AI-generated summary 				 Deriving compact and temporally aware visual repres...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 11. Machine bullshit, characterized by LLMs' indifference to truth, is quantified and analyzed through a new framework, revealing that RLHF and CoT prompting exacerbate certain bullshit forms.  					AI-generated summary 				 Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to statement...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 12. A Re-Bottleneck framework modifies pre-trained autoencoders to enforce specific latent structures, improving performance in diverse downstream applications.  					AI-generated summary 				 Neural audio codecs and autoencoders have emerged as versatile models for audio compression, transmission, feat...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 13. The study identifies a linear reasoning bottleneck in Visual-Language Models and proposes the Linear Separability Ceiling as a metric to evaluate it, suggesting targeted alignment rather than improved representation learning as a solution.  					AI-generated summary 				 Most state-of-the-art Visual...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 14. A novel approach to scaling large language models through modular composition and layer-wise growth using fixed embeddings enhances performance and flexibility.  					AI-generated summary 				 The prevailing paradigm for scaling large language models (LLMs) involves monolithic, end-to-end training, ...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 15. The rapid advancements of AI agents have ignited the long-held ambition of leveraging them to accelerate scientific discovery. Achieving this goal requires a deep understanding of the frontiers of human knowledge. As such, Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone f...
[11.07.2025 18:16] ********************************************************************************
[11.07.2025 18:16] Abstract 16. Transformer models equipped with fixed, visually derived embeddings outperform those with trainable embeddings on a reasoning benchmark, challenging the traditional role of embeddings in LLMs.  					AI-generated summary 				 Understanding the locus of semantic representation in large language models...
[11.07.2025 18:16] Read previous papers.
[11.07.2025 18:16] Generating reviews via LLM API.
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#long_context", "#reasoning", "#multimodal", "#rl", "#open_source", "#training", "#dataset", "#video"], "emoji": "üé¨", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#optimization", "#training", "#low_resource"], "emoji": "üé®", "ru": {"title": "T-LoRA: –¢–æ—á–Ω–∞—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏", "desc": "T-LoRA - —ç—Ç–æ –Ω–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—é –Ω–∏–∑–∫–æ–≥–æ —Ä–∞
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#training", "#interpretability", "#cv", "#benchmark"], "emoji": "üîç", "ru": {"title": "–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ - –∫–ª—é—á –∫ —É–ª—É—á—à–µ–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "TreeBench - —ç—Ç–æ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π 
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#multimodal", "#dataset", "#reasoning", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "–ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –ò–ò: —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏ –≤—Ä–µ–º–µ–Ω–∏", "desc": "OST-Bench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –æ–Ω–ª
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#video", "#optimization", "#training", "#benchmark"], "emoji": "üéûÔ∏è", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤–∏–¥–µ–æ-LLM –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–ú–µ—Ç–æ–¥ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ (STTM) –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤–∏–¥–µ–æ-LLM, –∏—Å–ø–æ–ª—å–∑—É—è –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö. STTM –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∫–∞–∂–¥
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#interpretability", "#cv", "#benchmark"], "emoji": "üîß", "ru": {"title": "PyVision: LLM —Å–æ–∑–¥–∞—é—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "PyVision - —ç—Ç–æ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM) –∞–≤—Ç–æ–Ω–æ–º–Ω–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å –∏ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#alignment", "#video", "#diffusion", "#3d"], "emoji": "üßä", "ru": {"title": "Geometry Forcing: –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ 3D-–≥–µ–æ–º–µ—Ç—Ä–∏–∏ –≤ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Geometry Forcing –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≤—ã—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#3d", "#optimization", "#inference", "#data"], "emoji": "üöÄ", "ru": {"title": "LangSplatV2: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ 3D —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤", "desc": "LangSplatV2 - —ç—Ç–æ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–∏—Å—Ç–µ–º—ã –¥–ª—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –û–Ω–∞ –∑–∞–º–µ–Ω—è–µ—Ç —Ç—è–∂–µ–ª–æ–≤–µ—Å–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä –Ω–∞ —Ä–∞
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#survey", "#architecture", "#video", "#training"], "emoji": "üé¨", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ: –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–Ω–∞–ª–∏–∑—É —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∏–ª–∏ 32 –Ω–∞—É
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization", "#reasoning", "#architecture"], "emoji": "üß†", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏", "desc": "–ú–µ—Ç–æ–¥ —Ü–µ–ø–æ—á–∫–∏ —Å–ª–æ–µ–≤ (CoLa), –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ 
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#training", "#optimization", "#transfer_learning", "#cv", "#robotics", "#video"], "emoji": "ü§ñ", "ru": {"title": "ToBo: –ö–æ–º–ø–∞–∫—Ç–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "ToBo - —ç—Ç–æ –º–µ—Ç–æ–¥ —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#alignment", "#ethics", "#benchmark", "#hallucinations", "#rlhf", "#training"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–æ–±–ª–µ–º–∞ –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç–∏ –≤ LLM: –∞–Ω–∞–ª–∏–∑ –º–∞—à–∏–Ω–Ω–æ–≥–æ –±—É–ª–ª—à–∏—Ç–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è \"–º–∞—à–∏–Ω–Ω–æ–≥–æ –±—É–ª–ª—à–∏—Ç–∞\", –∫–æ–≥–¥–∞ LLMs –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –±–µ–∑ —É—á–µ—Ç–∞ –∏—Ö –∏
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#audio", "#training", "#optimization"], "emoji": "üéõÔ∏è", "ru": {"title": "–ì–∏–±–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∞—É–¥–∏–æ–º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ 'Re-Bottleneck' –¥–ª—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ —Å —Ü–µ–ª—å—é —É–ª—É—á—à–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞. –ú–µ—Ç–æ–¥ –≤–≤–æ–¥–∏—Ç
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#reasoning", "#alignment", "#training", "#cv", "#multimodal"], "emoji": "üß†", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –±–∞—Ä—å–µ—Ä–∞ –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –ª–∏–Ω–µ–π–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#optimization", "#training", "#architecture"], "emoji": "üß©", "ru": {"title": "–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –ò–ò: —Å–±–æ—Ä–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ –∫–∏—Ä–ø–∏—á–∏–∫–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥—É–ª—å–Ω–æ–π –∫–æ–º
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#agents", "#agi", "#training", "#science", "#reasoning", "#open_source", "#architecture"], "emoji": "üß†", "ru": {"title": "X-Master: –ò–ò-–∞–≥–µ–Ω—Ç –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç X-Master - –∞–≥–µ–Ω—Ç–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, —Å–ø–æ—Å–æ–±–Ω–æ–≥–æ —ç–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç
[11.07.2025 18:16] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#open_source", "#architecture", "#interpretability"], "emoji": "üß†", "ru": {"title": "–ü–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ —Ä–æ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–æ–¥–µ
[11.07.2025 18:16] Renaming data file.
[11.07.2025 18:16] Renaming previous data. hf_papers.json to ./d/2025-07-11.json
[11.07.2025 18:16] Saving new data file.
[11.07.2025 18:16] Generating page.
[11.07.2025 18:16] Renaming previous page.
[11.07.2025 18:16] Renaming previous data. index.html to ./d/2025-07-11.html
[11.07.2025 18:16] Writing result.
[11.07.2025 18:16] Renaming log file.
[11.07.2025 18:16] Renaming previous data. log.txt to ./logs/2025-07-11_last_log.txt
