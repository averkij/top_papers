[10.07.2025 23:12] Read previous papers.
[10.07.2025 23:12] Generating top page (month).
[10.07.2025 23:12] Writing top page (month).
[11.07.2025 00:58] Read previous papers.
[11.07.2025 00:58] Get feed.
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07105
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07095
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06448
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06920
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06457
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07017
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05687
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06804
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2506.24044
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06853
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05455
[11.07.2025 00:58] Extract page data from URL. URL: https://huggingface.co/papers/2507.07024
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06607
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06485
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06415
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06260
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10251
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07106
[11.07.2025 00:58] Extract page data from URL. URL: https://huggingface.co/papers/2507.05980
[11.07.2025 00:58] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01702
[11.07.2025 00:58] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[11.07.2025 00:58] No deleted papers detected.
[11.07.2025 00:58] Downloading and parsing papers (pdf, html). Total: 20.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.07105.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.07105.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.07105.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.07095.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.07095.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.07095.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.06448.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.06448.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.06448.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.06920.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.06920.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.06920.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.06457.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.06457.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.06457.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.07017.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.07017.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.07017.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.05687.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.05687.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.05687.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.06804.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.06804.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.06804.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2506.24044.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2506.24044.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2506.24044.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.06853.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.06853.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.06853.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.05455.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.05455.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.05455.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.07024.
[11.07.2025 00:58] Downloading paper 2507.07024 from http://arxiv.org/pdf/2507.07024v1...
[11.07.2025 00:58] Extracting affiliations from text.
[11.07.2025 00:58] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 ] . [ 1 4 2 0 7 0 . 7 0 5 2 : r FLEXOLMO: Open Language Models for Flexible Data Use Weijia Shi aw Akshita Bhagia Kevin Farhat Niklas Muennighoff as Pete Walsh Jacob Morrison aw Dustin Schwenk Shayne Longpre Jake Poznanski Allyson Ettinger Daogao Liu Margaret Li Dirk Groeneveld Mike Lewis Wen-tau Yih Luca Soldaini Kyle Lo Noah A. Smith Luke Zettlemoyer Pang Wei Koh aw Hannaneh Hajishirzi aw Ali Farhadi aw Sewon Min ab aAllen Institute for AI wUniversity of Washington bUniversity of California, Berkeley sStanford University mMIT swj0419@uw.edu akshitab@allenai.org sewonm@berkeley.edu Model Code Blog hf.co/allenai/FlexOlmo-7x7B-1T github.com/allenai/FlexOlmo allenai.org/blog/flexolmo "
[11.07.2025 00:58] Response: ```python
["Allen Institute for AI", "University of Washington", "University of California, Berkeley", "Stanford University", "MIT"]
```
[11.07.2025 00:58] Deleting PDF ./assets/pdf/2507.07024.pdf.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.06607.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.06607.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.06607.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.06485.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.06485.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.06485.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.06415.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.06415.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.06415.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.06260.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.06260.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.06260.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2505.10251.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2505.10251.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2505.10251.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.07106.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.07106.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.07106.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.05980.
[11.07.2025 00:58] Downloading paper 2507.05980 from http://arxiv.org/pdf/2507.05980v1...
[11.07.2025 00:58] Extracting affiliations from text.
[11.07.2025 00:58] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 ] . [ 1 0 8 9 5 0 . 7 0 5 2 : r RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages Gabriel Chua1 Leanne Tan1 Ziyu Ge2 Roy Ka-Wei Lee1, 2 1GovTech, Singapore 2Singapore University of Technology and Design { gabriel_chualeanne_tan }@tech.gov.sg Warning: this paper contains references and data that may be offensive. "
[11.07.2025 00:58] Response: ```python
["GovTech, Singapore", "Singapore University of Technology and Design"]
```
[11.07.2025 00:58] Deleting PDF ./assets/pdf/2507.05980.pdf.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Downloading and parsing paper https://huggingface.co/papers/2507.01702.
[11.07.2025 00:58] Extra JSON file exists (./assets/json/2507.01702.json), skip PDF parsing.
[11.07.2025 00:58] Paper image links file exists (./assets/img_data/2507.01702.json), skip HTML parsing.
[11.07.2025 00:58] Success.
[11.07.2025 00:58] Enriching papers with extra data.
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 0. 4KAgent, a unified agentic super-resolution system, enhances low-resolution images to 4K using profiling, perception, and restoration agents, achieving state-of-the-art performance across various imaging domains.  					AI-generated summary 				 We present 4KAgent, a unified agentic super-resolution ...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 1. A new dataset and evaluation framework improve zero-shot text-to-motion generation through a large-scale, high-quality dataset and a scalable model architecture.  					AI-generated summary 				 Generating diverse and natural human motion sequences based on textual descriptions constitutes a fundamen...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 2. Perception-Aware Policy Optimization (PAPO) enhances reinforcement learning with verifiable rewards for multimodal reasoning by integrating implicit perception loss, improving visual perception and reasoning.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has p...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 3. A human-LLM collaborative method enhances code generation test case generation, improving reliability and detection rates in code evaluation benchmarks.  					AI-generated summary 				 Large language models (LLMs) have recently achieved notable success in code-generation benchmarks such as HumanEval...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 4. Research evaluates various linear attention models and their integration with full attention in Transformers, identifying key mechanisms like selective gating and hierarchical recurrence for enhanced recall performance.  					AI-generated summary 				 Transformers face quadratic complexity and memor...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 5. FR3E enhances LLM reasoning by providing structured exploration through targeted rollouts at high-uncertainty points, leading to more stable training and accurate responses.  					AI-generated summary 				 Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning abilities of Larg...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 6. Kernel development in deep learning requires optimizing computational units across hardware while balancing memory management, parallelism, and hardware-specific optimizations through extensive empirical tuning. Although domain-specific languages like Triton simplify GPU programming by abstracting l...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 7. A novel framework decouples reasoning and proving in ATP to improve formal proving performance, achieving success on challenging IMO problems.  					AI-generated summary 				 Automated Theorem Proving (ATP) in formal languages is a foundational challenge for AI. While Large Language Models (LLMs) ha...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 8. This survey provides a comprehensive overview of Vision-Language-Action (VLA) paradigms and their adaptation for autonomous driving, detailing architectural components, evolution of models, datasets, and future challenges.  					AI-generated summary 				 The rapid progress of multimodal large langua...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 9. DiffSpectra uses diffusion models with SE(3)-equivariant architecture and SpecFormer spectral encoder to accurately infer both 2D and 3D molecular structures from multi-modal spectral data.  					AI-generated summary 				 Molecular structure elucidation from spectra is a foundational problem in chem...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 10. A new dataset and models for toxic language detection incorporate diverse community perspectives and conversational context, improving accuracy over existing tools.  					AI-generated summary 				 Automatic toxic language detection is critical for creating safe, inclusive online spaces. However, it ...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 11. FlexOlmo, a distributed and data-flexible language model using a mixture-of-experts architecture, achieves significant improvements in performance across diverse tasks while respecting data privacy and ownership.  					AI-generated summary 				 We introduce FlexOlmo, a new class of language models (...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 12. Gated Memory Units improve memory sharing in hybrid decoder architectures, enhancing efficiency and performance in language modeling tasks.  					AI-generated summary 				 Recent advances in language modeling have demonstrated the effectiveness of State Space Models (SSMs) for efficient sequence mod...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 13. Video-RTS enhances video reasoning efficiency and accuracy through pure RL training and adaptive test-time scaling, reducing data and computational costs.  					AI-generated summary 				 Despite advances in reinforcement learning (RL)-based video reasoning with large language models (LLMs), data col...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 14. PERK, a scalable approach using parameter-efficient adapters, enhances long-context reasoning by encoding contexts into a lightweight model at test time, achieving significant performance improvements over prompt-based methods.  					AI-generated summary 				 Long-context reasoning requires accurate...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 15. Nova Premier is Amazon's most capable multimodal foundation model and teacher for model distillation. It processes text, images, and video with a one-million-token context window, enabling analysis of large codebases, 400-page documents, and 90-minute videos in a single prompt. We present the first ...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 16. A hierarchical framework combining high-level task planning and low-level trajectory generation enables autonomous surgical procedures with high success rates in ex vivo experiments.  					AI-generated summary 				 Research on autonomous surgery has largely focused on simple task automation in contr...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 17. Text-to-image diffusion models enhance image-based question-answering by providing semantically rich and instruction-aware visual encodings, complementing CLIP and improving spatial and compositional reasoning.  					AI-generated summary 				 Recent advances in multimodal large language models (MLLM...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 18. Large language models (LLMs) and their safety classifiers often perform poorly on low-resource languages due to limited training data and evaluation benchmarks. This paper introduces RabakBench, a new multilingual safety benchmark localized to Singapore's unique linguistic context, covering Singlish...
[11.07.2025 00:58] ********************************************************************************
[11.07.2025 00:58] Abstract 19. AdamMeme, an adaptive agent-based framework, evaluates multimodal Large Language Models' understanding of harmful memes through iterative updates and multi-agent collaboration, revealing model-specific weaknesses.  					AI-generated summary 				 The proliferation of multimodal memes in the social me...
[11.07.2025 00:58] Read previous papers.
[11.07.2025 00:58] Generating reviews via LLM API.
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#low_resource", "#cv", "#open_source", "#benchmark", "#agents", "#healthcare"], "emoji": "🔬", "ru": {"title": "4KAgent: Революция в улучшении изображений с помощью ИИ", "desc": "4KAgent - это унифицированная агентная система сверхвысокого разрешения, которая улучшает изображения низ
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#transfer_learning", "#cv", "#benchmark", "#dataset", "#robotics", "#games"], "emoji": "🤖", "ru": {"title": "Революция в генерации движений: от текста к реальности", "desc": "Исследователи представили новый набор данных MotionMillion и систему оценки MotionMillion-Eval для улучшения
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#multimodal", "#training", "#reasoning", "#rl", "#rlhf", "#optimization"], "emoji": "👁️", "ru": {"title": "Улучшение восприятия в мультимодальном обучении с подкреплением", "desc": "Статья представляет метод Perception-Aware Policy Optimization (PAPO) для улучшения обучения с подкре
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#optimization", "#rl", "#benchmark", "#dataset", "#training", "#games"], "emoji": "🧪", "ru": {"title": "Человек и ИИ объединяются для создания надежных тестов кода", "desc": "Статья представляет новый метод SAGA для генерации тестовых случаев, объединяющий человеческий опыт и возмож
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#dataset", "#training", "#open_source", "#benchmark", "#architecture", "#optimization"], "emoji": "🧠", "ru": {"title": "Гибридные архитектуры внимания: оптимизация производительности и эффективности", "desc": "Исследование оценивает различные модели линейного внимания и их интеграци
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#reasoning", "#training"], "emoji": "🧠", "ru": {"title": "FR3E: Структурированное исследование для улучшения рассуждений ИИ", "desc": "FR3E - это новый метод для улучшения способностей рассуждения больших языковых моделей (LLM). Он использует структурированное и
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization", "#rl"], "emoji": "🚀", "ru": {"title": "AutoTriton: Революция в оптимизации ядер глубокого обучения с помощью RL", "desc": "AutoTriton - это первая модель, основанная на обучении с подкреплением (RL), для программирования на Triton. Она и
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#dataset", "#training", "#math"], "emoji": "🧠", "ru": {"title": "Разделяй и властвуй: новый подход к автоматическому доказательству теорем", "desc": "Статья представляет новый подход к автоматическому доказательству теорем, разделяющий процессы рассужде
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#architecture", "#benchmark", "#survey", "#dataset", "#multimodal", "#alignment", "#interpretability"], "emoji": "🚗", "ru": {"title": "VLA: Новый горизонт для интерпретируемых и социально-ориентированных беспилотных автомобилей", "desc": "Это обзор парадигм 
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#architecture", "#science", "#multimodal", "#data", "#diffusion", "#3d"], "emoji": "🧪", "ru": {"title": "Революция в определении структуры молекул с помощью ИИ", "desc": "DiffSpectra - это генеративная модель для определения 2D и 3D структур молекул на основе мультимодальных спектра
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#data", "#open_source", "#ethics", "#dataset", "#training"], "emoji": "🗨️", "ru": {"title": "Инклюзивная модерация контента: учет разнообразия и контекста", "desc": "Статья представляет новый датасет MODELCITIZENS для обнаружения токсичного языка, учитывающий разнообразные перспекти
[11.07.2025 00:58] Querying the API.
[11.07.2025 00:58] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FlexOlmo, a distributed and data-flexible language model using a mixture-of-experts architecture, achieves significant improvements in performance across diverse tasks while respecting data privacy and ownership.  					AI-generated summary 				 We introduce FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fine-grained control of data access during inference.
[11.07.2025 00:58] Response: {
  "desc": "FlexOlmo - это новый класс языковых моделей, использующий архитектуру смеси экспертов (MoE). Модель позволяет проводить распределенное обучение без обмена данными и гибкое вывод с возможностью включения или исключения параметров и связанных с ними данных. FlexOlmo демонстрирует значительное улучшение производительности на различных задачах, превосходя стандартные MoE и методы объединения моделей. Этот подход обеспечивает использование закрытых данных с соблюдением конфиденциальности и прав собственности.",
  "emoji": "🔐",
  "title": "Гибкое обучение языковых моделей с защитой данных"
}
[11.07.2025 00:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FlexOlmo, a distributed and data-flexible language model using a mixture-of-experts architecture, achieves significant improvements in performance across diverse tasks while respecting data privacy and ownership.  					AI-generated summary 				 We introduce FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fine-grained control of data access during inference."

[11.07.2025 00:58] Response: ```python
["DATASET", "DATA", "ARCHITECTURE", "TRAINING"]
```
[11.07.2025 00:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FlexOlmo, a distributed and data-flexible language model using a mixture-of-experts architecture, achieves significant improvements in performance across diverse tasks while respecting data privacy and ownership.  					AI-generated summary 				 We introduce FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fine-grained control of data access during inference."

[11.07.2025 00:58] Response: ```python
['ETHICS', 'OPEN_SOURCE']
```
[11.07.2025 00:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FlexOlmo is a novel language model that utilizes a mixture-of-experts (MoE) architecture to enhance performance on various tasks while ensuring data privacy. It allows for distributed training without sharing data, meaning different parts of the model can learn from separate datasets without compromising data ownership. During inference, users can flexibly include or exclude data associated with model parameters, providing control over data usage without needing additional training. This approach not only improves model performance significantly but also respects the preferences of data owners, making it suitable for sensitive applications.","title":"Empowering Language Models with Privacy and Flexibility"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FlexOlmo is a novel language model that utilizes a mixture-of-experts (MoE) architecture to enhance performance on various tasks while ensuring data privacy. It allows for distributed training without sharing data, meaning different parts of the model can learn from separate datasets without compromising data ownership. During inference, users can flexibly include or exclude data associated with model parameters, providing control over data usage without needing additional training. This approach not only improves model performance significantly but also respects the preferences of data owners, making it suitable for sensitive applications.', title='Empowering Language Models with Privacy and Flexibility'))
[11.07.2025 00:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FlexOlmo是一种新型的语言模型，采用混合专家架构，能够在不共享数据的情况下进行分布式训练。它支持数据灵活推理，允许根据需要灵活地包含或排除模型推理中的参数和数据。通过独立训练的专家与公共数据训练的通用专家相结合，FlexOlmo在多项任务上实现了41%的相对性能提升，同时尊重数据所有者的隐私和许可要求。该研究为在受监管行业中处理敏感数据的研究人员和数据所有者提供了解决方案。","title":"FlexOlmo：尊重数据隐私的语言模型"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FlexOlmo是一种新型的语言模型，采用混合专家架构，能够在不共享数据的情况下进行分布式训练。它支持数据灵活推理，允许根据需要灵活地包含或排除模型推理中的参数和数据。通过独立训练的专家与公共数据训练的通用专家相结合，FlexOlmo在多项任务上实现了41%的相对性能提升，同时尊重数据所有者的隐私和许可要求。该研究为在受监管行业中处理敏感数据的研究人员和数据所有者提供了解决方案。', title='FlexOlmo：尊重数据隐私的语言模型'))
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization", "#long_context", "#reasoning", "#open_source"], "emoji": "🧠", "ru": {"title": "GMU: Революция в обмене памятью для эффективных языковых моделей", "desc": "Статья представляет Gated Memory Unit (GMU) - механизм для эффективного обмена пам
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#training", "#optimization", "#video", "#rl", "#reasoning", "#benchmark"], "emoji": "🎬", "ru": {"title": "Эффективное рассуждение о видео с помощью RL и адаптивного масштабирования", "desc": "Video-RTS - это новый подход к улучшению способности рассуждать о видео с drastically повыш
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#long_context", "#training", "#small_models", "#reasoning", "#architecture", "#optimization"], "emoji": "🧠", "ru": {"title": "PERK: Эффективное обучение рассуждениям на длинном контексте", "desc": "PERK - это масштабируемый подход, использующий параметрически-эффективные адаптеры дл
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#healthcare", "#benchmark", "#multimodal", "#alignment", "#security"], "emoji": "🔬", "ru": {"title": "Nova Premier: мощная и безопасная мультимодальная модель ИИ от Amazon", "desc": "Amazon представила Nova Premier - мультимодальную фундаментальную модель, способную обрабатывать тек
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#robotics", "#agents", "#optimization", "#science", "#agi"], "emoji": "🤖", "ru": {"title": "Автономная хирургия: от планирования к точным движениям", "desc": "Предложена иерархическая система для выполнения сложных хирургических операций роботом. Система сочетает высокоуровневое пла
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#cv", "#benchmark", "#leakage", "#reasoning"], "emoji": "🖼️", "ru": {"title": "Диффузионные модели как ключ к улучшению визуального ИИ", "desc": "Это исследование показывает, как модели диффузии текста в изображение могут улучшить возможности вопросно-от
[11.07.2025 00:58] Querying the API.
[11.07.2025 00:58] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large language models (LLMs) and their safety classifiers often perform poorly on low-resource languages due to limited training data and evaluation benchmarks. This paper introduces RabakBench, a new multilingual safety benchmark localized to Singapore's unique linguistic context, covering Singlish, Chinese, Malay, and Tamil. RabakBench is constructed through a scalable three-stage pipeline: (i) Generate - adversarial example generation by augmenting real Singlish web content with LLM-driven red teaming; (ii) Label - semi-automated multi-label safety annotation using majority-voted LLM labelers aligned with human judgments; and (iii) Translate - high-fidelity translation preserving linguistic nuance and toxicity across languages. The final dataset comprises over 5,000 safety-labeled examples across four languages and six fine-grained safety categories with severity levels. Evaluations of 11 popular open-source and closed-source guardrail classifiers reveal significant performance degradation. RabakBench not only enables robust safety evaluation in Southeast Asian multilingual settings but also offers a reproducible framework for building localized safety datasets in low-resource environments. The benchmark dataset, including the human-verified translations, and evaluation code are publicly available.
[11.07.2025 00:58] Response: {
  "desc": "Статья представляет RabakBench - новый многоязычный бенчмарк для оценки безопасности больших языковых моделей (LLM) в контексте Сингапура. Бенчмарк создан с помощью трехэтапного процесса: генерация примеров, разметка данных и перевод на четыре языка. RabakBench включает более 5000 размеченных примеров по шести категориям безопасности. Оценка 11 популярных классификаторов показала значительное снижение производительности на этом наборе данных.",
  "emoji": "🌏",
  "title": "Многоязычная оценка безопасности LLM для Юго-Восточной Азии"
}
[11.07.2025 00:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models (LLMs) and their safety classifiers often perform poorly on low-resource languages due to limited training data and evaluation benchmarks. This paper introduces RabakBench, a new multilingual safety benchmark localized to Singapore's unique linguistic context, covering Singlish, Chinese, Malay, and Tamil. RabakBench is constructed through a scalable three-stage pipeline: (i) Generate - adversarial example generation by augmenting real Singlish web content with LLM-driven red teaming; (ii) Label - semi-automated multi-label safety annotation using majority-voted LLM labelers aligned with human judgments; and (iii) Translate - high-fidelity translation preserving linguistic nuance and toxicity across languages. The final dataset comprises over 5,000 safety-labeled examples across four languages and six fine-grained safety categories with severity levels. Evaluations of 11 popular open-source and closed-source guardrail classifiers reveal significant performance degradation. RabakBench not only enables robust safety evaluation in Southeast Asian multilingual settings but also offers a reproducible framework for building localized safety datasets in low-resource environments. The benchmark dataset, including the human-verified translations, and evaluation code are publicly available."

[11.07.2025 00:58] Response: ```python
['DATASET', 'BENCHMARK', 'MULTILINGUAL']
```
[11.07.2025 00:58] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models (LLMs) and their safety classifiers often perform poorly on low-resource languages due to limited training data and evaluation benchmarks. This paper introduces RabakBench, a new multilingual safety benchmark localized to Singapore's unique linguistic context, covering Singlish, Chinese, Malay, and Tamil. RabakBench is constructed through a scalable three-stage pipeline: (i) Generate - adversarial example generation by augmenting real Singlish web content with LLM-driven red teaming; (ii) Label - semi-automated multi-label safety annotation using majority-voted LLM labelers aligned with human judgments; and (iii) Translate - high-fidelity translation preserving linguistic nuance and toxicity across languages. The final dataset comprises over 5,000 safety-labeled examples across four languages and six fine-grained safety categories with severity levels. Evaluations of 11 popular open-source and closed-source guardrail classifiers reveal significant performance degradation. RabakBench not only enables robust safety evaluation in Southeast Asian multilingual settings but also offers a reproducible framework for building localized safety datasets in low-resource environments. The benchmark dataset, including the human-verified translations, and evaluation code are publicly available."

[11.07.2025 00:58] Response: ```python
["LOW_RESOURCE", "OPEN_SOURCE"]
```
[11.07.2025 00:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenges faced by large language models (LLMs) in ensuring safety for low-resource languages. It introduces RabakBench, a multilingual safety benchmark tailored to Singapore\'s linguistic diversity, including Singlish, Chinese, Malay, and Tamil. The benchmark is created through a three-stage process: generating adversarial examples, semi-automated safety labeling, and translating content while maintaining linguistic nuances. The resulting dataset, with over 5,000 examples across multiple languages and safety categories, highlights the performance issues of existing classifiers and provides a framework for future localized safety evaluations.","title":"RabakBench: Enhancing Safety for Multilingual LLMs in Low-Resource Settings"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper addresses the challenges faced by large language models (LLMs) in ensuring safety for low-resource languages. It introduces RabakBench, a multilingual safety benchmark tailored to Singapore's linguistic diversity, including Singlish, Chinese, Malay, and Tamil. The benchmark is created through a three-stage process: generating adversarial examples, semi-automated safety labeling, and translating content while maintaining linguistic nuances. The resulting dataset, with over 5,000 examples across multiple languages and safety categories, highlights the performance issues of existing classifiers and provides a framework for future localized safety evaluations.", title='RabakBench: Enhancing Safety for Multilingual LLMs in Low-Resource Settings'))
[11.07.2025 00:58] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文介绍了RabakBench，这是一个针对新加坡独特语言环境的多语言安全基准，旨在解决大型语言模型在低资源语言上的表现不佳问题。RabakBench通过一个可扩展的三阶段流程构建，包括生成对抗样本、半自动化多标签安全注释和高保真翻译。最终数据集包含超过5000个安全标记的示例，涵盖四种语言和六个细分的安全类别。该基准不仅支持东南亚多语言环境中的安全评估，还提供了在低资源环境中构建本地化安全数据集的可重复框架。","title":"提升低资源语言的安全性评估"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文介绍了RabakBench，这是一个针对新加坡独特语言环境的多语言安全基准，旨在解决大型语言模型在低资源语言上的表现不佳问题。RabakBench通过一个可扩展的三阶段流程构建，包括生成对抗样本、半自动化多标签安全注释和高保真翻译。最终数据集包含超过5000个安全标记的示例，涵盖四种语言和六个细分的安全类别。该基准不仅支持东南亚多语言环境中的安全评估，还提供了在低资源环境中构建本地化安全数据集的可重复框架。', title='提升低资源语言的安全性评估'))
[11.07.2025 00:58] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#benchmark", "#reasoning", "#interpretability", "#alignment"], "emoji": "🕵️", "ru": {"title": "AdamMeme: Умный детектив в мире вредоносных мемов", "desc": "AdamMeme - это адаптивная система оценки мультимодальных языковых моделей в понимании вредоносных мем
[11.07.2025 00:58] Renaming data file.
[11.07.2025 00:58] Renaming previous data. hf_papers.json to ./d/2025-07-11.json
[11.07.2025 00:58] Saving new data file.
[11.07.2025 00:58] Generating page.
[11.07.2025 00:58] Renaming previous page.
[11.07.2025 00:58] Renaming previous data. index.html to ./d/2025-07-11.html
[11.07.2025 00:58] Writing result.
[11.07.2025 00:58] Renaming log file.
[11.07.2025 00:58] Renaming previous data. log.txt to ./logs/2025-07-11_last_log.txt
