[17.10.2024 04:15] Read previous papers.
[17.10.2024 04:15] Get feed.
[17.10.2024 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12613
[17.10.2024 04:15] Extract page data from URL. URL: https://huggingface.co/papers/2410.12381
[17.10.2024 04:15] Extract page data from URL. URL: https://huggingface.co/papers/2410.12787
[17.10.2024 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12628
[17.10.2024 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12405
[17.10.2024 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11817
[17.10.2024 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12409
[17.10.2024 04:15] Extract page data from URL. URL: https://huggingface.co/papers/2410.12109
[17.10.2024 04:15] ********************************************************************************
[17.10.2024 04:15] Abstract 0. Model merging has become one of the key technologies for enhancing the capabilities and efficiency of Large Language Models (LLMs). However, our understanding of the expected performance gains and principles when merging any two models remains limited. In this work, we introduce model kinship, the d...
[17.10.2024 04:15] ********************************************************************************
[17.10.2024 04:15] Abstract 1. Coding tasks have been valuable for evaluating Large Language Models (LLMs), as they demand the comprehension of high-level instructions, complex reasoning, and the implementation of functional programs -- core capabilities for advancing Artificial General Intelligence. Despite the progress in Large...
[17.10.2024 04:15] ********************************************************************************
[17.10.2024 04:15] Abstract 2. Recent advancements in large multimodal models (LMMs) have significantly enhanced performance across diverse tasks, with ongoing efforts to further integrate additional modalities such as video and audio. However, most existing LMMs remain vulnerable to hallucinations, the discrepancy between the fa...
[17.10.2024 04:15] ********************************************************************************
[17.10.2024 04:15] Abstract 3. Document Layout Analysis is crucial for real-world document understanding systems, but it encounters a challenging trade-off between speed and accuracy: multimodal methods leveraging both text and visual features achieve higher accuracy but suffer from significant latency, whereas unimodal methods r...
[17.10.2024 04:15] ********************************************************************************
[17.10.2024 04:15] Abstract 4. Large language models (LLMs) have demonstrated impressive capabilities across various tasks, but their performance is highly sensitive to the prompts utilized. This variability poses challenges for accurate assessment and user satisfaction. Current research frequently overlooks instance-level prompt...
[17.10.2024 04:15] ********************************************************************************
[17.10.2024 04:15] Abstract 5. The rapid advancement of text-to-image (T2I) diffusion models has enabled them to generate unprecedented results from given texts. However, as text inputs become longer, existing encoding methods like CLIP face limitations, and aligning the generated images with long texts becomes challenging. To ta...
[17.10.2024 04:15] ********************************************************************************
[17.10.2024 04:15] Abstract 6. Autonomous planning has been an ongoing pursuit since the inception of artificial intelligence. Based on curated problem solvers, early planning agents could deliver precise solutions for specific tasks but lacked generalization. The emergence of large language models (LLMs) and their powerful reaso...
[17.10.2024 04:15] ********************************************************************************
[17.10.2024 04:15] Abstract 7. Large Language Models (LLMs) have made significant strides in text generation and comprehension, with recent advancements extending into multimodal LLMs that integrate visual and audio inputs. However, these models continue to struggle with fine-grained, cross-modal temporal understanding, particula...
[17.10.2024 04:15] Read previous papers.
[17.10.2024 04:15] Generating reviews via LLM API.
[17.10.2024 04:15] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –ø–æ–Ω—è—Ç–∏–µ '—Ä–æ–¥—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π', –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–µ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —ç–≤–æ–ª—é—Ü–∏–∏, –∏ –∏—Å—Å–ª–µ–¥—É—é—Ç –µ–≥–æ —Å–≤—è–∑—å —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –ø–æ—Å–ª–µ —Å–ª–∏—è–Ω–∏—è. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è 'Top-k –ñ–∞–¥–Ω–æ–≥–æ –°
[17.10.2024 04:15] Querying the API.
[17.10.2024 04:15] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ HumanEval-V –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç 108 –∑–∞–¥–∞—á –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–∞ Python, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –æ—Ü–µ–Ω–∫—É 19 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LMM, –≤—ã—è–≤–∏–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –¥–∞–∂–µ –¥–ª—è –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç–µ–∫—É—â–∏—Ö LMM –≤ –æ–±–ª–∞—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –æ–±–æ–∑–Ω–∞—á–∞—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.",
  "categories": ["#multimodal", "#benchmark", "#code", "#cv"],
  "emoji": "üëÅÔ∏è‚Äçüó®Ô∏è",
  "title": "HumanEval-V: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –≤–∏–∑—É–∞–ª—å–Ω–æ-–∫–æ–¥–æ–≤—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π AI"
}
[17.10.2024 04:15] Get embedding for a paper via LLM API.
[17.10.2024 04:15] Querying the API.
[17.10.2024 04:15] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤–æ–µ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö (LMM), –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–µ–µ —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏: —è–∑—ã–∫, –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏ –∞—É–¥–∏–æ. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏—á–∏–Ω—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π: —á—Ä–µ–∑–º–µ—Ä–Ω–∞—è –æ–ø–æ—Ä–∞ –Ω–∞ –æ–¥–Ω–æ–º–æ–¥–∞–ª—å–Ω—ã–µ –ø—Ä–∏–æ—Ä—ã –∏ –ª–æ–∂–Ω—ã–µ –º–µ–∂–º–æ–¥–∞–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏. –î–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ LMM –±—ã–ª —Å–æ–∑–¥–∞–Ω –±–µ–Ω—á–º–∞—Ä–∫ The Curse of Multi-Modalities (CMM). –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø–æ —Å–Ω–∏–∂–µ–Ω–∏—é –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.",
  "categories": [
    "#multimodal",
    "#benchmark",
    "#nlp",
    "#cv"
  ],
  "emoji": "üåà",
  "title": "–ë–æ—Ä—å–±–∞ —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö"
}
[17.10.2024 04:15] Get embedding for a paper via LLM API.
[17.10.2024 04:15] Using data from previous issue: {"desc": "DocLayout-YOLO - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–Ω–∞–ª–∏–∑—É –º–∞–∫–µ—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—â–∏–π —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º Mesh-candidate BestFit –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö DocSynth-300K, —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏. –í –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –≤–Ω–µ–¥—Ä–µ–Ω –º–æ–¥—É–ª—å Global-to-
[17.10.2024 04:15] Using data from previous issue: {"desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç ProSA - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –ø—Ä–æ–º–ø—Ç–∞–º. –û–Ω–∏ –≤–≤–æ–¥—è—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É PromptSensiScore –∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ —Ä–∞–±–æ—Ç—ã LLM. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –ø—Ä
[17.10.2024 04:15] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ LongAlign –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º –æ–ø–∏—Å–∞–Ω–∏—è–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏
[17.10.2024 04:15] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–∞, –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É—é—â–∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –∞–≥–µ–Ω—Ç–æ–≤: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é —Ä–æ–ª—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ —É–º–µ–Ω—å—à–∞—é—â–µ–µ—Å—è –≤–ª–∏—è–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–∞–∂–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫
[17.10.2024 04:15] Querying the API.
[17.10.2024 04:15] Got response. {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç OCTAV –∏ –º–æ–¥–µ–ª—å OMCAT –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. OCTAV —Å–æ–∑–¥–∞–Ω –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ —Å–æ–±—ã—Ç–∏–π –º–µ–∂–¥—É –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ –ø–æ—Ç–æ–∫–∞–º–∏. OMCAT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ RoTE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –∞—É–¥–∏–æ-–≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ OCTAV.",
  "categories": ["#multimodal", "#dataset", "#benchmark", "#video"],
  "emoji": "üé¨",
  "title": "OMCAT: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º—É –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é"
}
[17.10.2024 04:15] Get embedding for a paper via LLM API.
[17.10.2024 04:15] Loading Chinese text from previous data.
[17.10.2024 04:15] Renaming data file.
[17.10.2024 04:15] Renaming previous data. hf_papers.json to 2024-10-16_hf_papers.json
[17.10.2024 04:15] Saving new data file.
[17.10.2024 04:15] Generating page.
[17.10.2024 04:15] Generating Chinese page for reading.
[17.10.2024 04:15] Chinese vocab [{'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ shu√†i', 'trans': 'multimodal'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'ÂπªËßâ', 'pinyin': 'hu√†n ju√©', 'trans': 'hallucination'}, {'word': 'Áé∞Ë±°', 'pinyin': 'xi√†n xi√†ng', 'trans': 'phenomenon'}, {'word': 'ÂéüÂõ†', 'pinyin': 'yu√°n yƒ´n', 'trans': 'reason'}, {'word': 'ÈîôËØØ', 'pinyin': 'cu√≤ w√π', 'trans': 'error'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generate'}, {'word': 'ÂØπË±°', 'pinyin': 'du√¨ xi√†ng', 'trans': 'object'}, {'word': 'ËØÜÂà´', 'pinyin': 'sh√≠ bi√©', 'trans': 'recognize'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'visual'}, {'word': 'Âº∫', 'pinyin': 'qi√°ng', 'trans': 'strong'}, {'word': 'Áü•ËØÜ', 'pinyin': 'zhƒ´ shi', 'trans': 'knowledge'}, {'word': 'ÂÖàÈ™å', 'pinyin': 'xiƒÅn y√†n', 'trans': 'prior'}, {'word': 'ÊäëÂà∂', 'pinyin': 'y√¨ zh√¨', 'trans': 'suppress'}, {'word': '‰ø°ÊÅØ', 'pinyin': 'x√¨n xƒ´', 'trans': 'information'}, {'word': 'Âä®ÊÄÅ', 'pinyin': 'd√≤ng t√†i', 'trans': 'dynamic'}, {'word': 'Á∫†Ê≠£', 'pinyin': 'ji≈´ zh√®ng', 'trans': 'correct'}, {'word': 'Ëß£Á†Å', 'pinyin': 'jiƒõ m«é', 'trans': 'decode'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'ÈÄÇÂ∫îÊÄß', 'pinyin': 'sh√¨ y√¨ng x√¨ng', 'trans': 'adaptive'}, {'word': 'ÈÄâÊã©', 'pinyin': 'xu«én z√©', 'trans': 'select'}, {'word': 'ÂêàÈÄÇ', 'pinyin': 'h√© sh√¨', 'trans': 'suitable'}, {'word': 'Êï¥Âêà', 'pinyin': 'zhƒõng h√©', 'trans': 'integrate'}, {'word': 'Ë∞ÉÊï¥', 'pinyin': 'ti√°o zhƒõng', 'trans': 'adjust'}, {'word': 'ÁªèÂÖ∏', 'pinyin': 'jƒ´ng di«én', 'trans': 'classical'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'Êó†Áºù', 'pinyin': 'w√∫ fƒìng', 'trans': 'seamless'}, {'word': 'ÁªìÂêà', 'pinyin': 'ji√© h√©', 'trans': 'combine'}, {'word': 'Â∫îÁî®', 'pinyin': 'y√¨ng y√≤ng', 'trans': 'apply'}, {'word': '‰∏çÂêå', 'pinyin': 'b√π t√≥ng', 'trans': 'different'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'significant'}, {'word': 'Èôç‰Ωé', 'pinyin': 'ji√†ng dƒ´', 'trans': 'reduce'}, {'word': 'Áéá', 'pinyin': 'l«ú', 'trans': 'rate'}]
[17.10.2024 04:15] Renaming previous page.
[17.10.2024 04:15] Renaming previous data. index.html to 2024-10-16_hf_papers.html
[17.10.2024 04:15] Renaming previous Chinese page.
[17.10.2024 04:15] Renaming previous data. zh.html to 2024-10-16_zh_reading_task.html
[17.10.2024 04:15] Writing result.
[17.10.2024 04:15] Writing Chinese reading task.
[17.10.2024 04:15] Renaming log file.
[17.10.2024 04:15] Renaming previous data. log.txt to 2024-10-16_last_log.txt
