[15.08.2025 02:49] Read previous papers.
[15.08.2025 02:49] Generating top page (month).
[15.08.2025 02:49] Writing top page (month).
[15.08.2025 03:48] Read previous papers.
[15.08.2025 03:48] Get feed.
[15.08.2025 03:48] Extract page data from URL. URL: https://huggingface.co/papers/2508.10711
[15.08.2025 03:48] Get page data from previous paper. URL: https://huggingface.co/papers/2508.10433
[15.08.2025 03:48] Get page data from previous paper. URL: https://huggingface.co/papers/2508.09848
[15.08.2025 03:48] Extract page data from URL. URL: https://huggingface.co/papers/2508.10833
[15.08.2025 03:48] Get page data from previous paper. URL: https://huggingface.co/papers/2508.10576
[15.08.2025 03:48] Get page data from previous paper. URL: https://huggingface.co/papers/2508.10860
[15.08.2025 03:48] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.08.2025 03:48] No deleted papers detected.
[15.08.2025 03:48] Downloading and parsing papers (pdf, html). Total: 6.
[15.08.2025 03:48] Downloading and parsing paper https://huggingface.co/papers/2508.10711.
[15.08.2025 03:48] Downloading paper 2508.10711 from http://arxiv.org/pdf/2508.10711v1...
[15.08.2025 03:48] Extracting affiliations from text.
[15.08.2025 03:48] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 1 ] . [ 1 1 1 7 0 1 . 8 0 5 2 : r NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale NextStep-Team, StepFun Homepage: https://stepfun.ai/research/en/nextstep1 Github: https://github.com/stepfun-ai/NextStep-1 Huggingface: NextStep-1 Collections "
[15.08.2025 03:48] Response: []
[15.08.2025 03:48] Extracting affiliations from text.
[15.08.2025 03:48] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 1 ] . [ 1 1 1 7 0 1 . 8 0 5 2 : r NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale NextStep-Team, StepFun Homepage: https://stepfun.ai/research/en/nextstep1 Github: https://github.com/stepfun-ai/NextStep-1 Huggingface: NextStep-1 CollectionsPrevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, 14B autoregressive model paired with 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in highfidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community. 1. Introduction The remarkable success of autoregressive models in large language models (Brown et al., 2020; OpenAI, 2025a; Radford et al., 2018, 2019) has motivated their extension to text-to-image generation. By unifying multimodal inputs into single sequence, autoregressive image generation models (Chen et al., 2025b; Fan et al., 2024; Sun et al., 2023, 2024b,c; Wang et al., 2024b; Yu et al., 2022) offer scalable and flexible approach to text-to-image generation that naturally accommodates diverse conditioning signals. However, most existing autoregressive approaches for text-to-image generation (Chen et al., 2025b; Dong et al., 2024; Sun et al., 2024a,b; Tong et al., 2024; Wang et al., 2024b) either rely on heavy diffusion models or adopt vector quantization (VQ) (Eslami et al., 2021; Yu et al., 2023; Zheng et al., 2022) to tokenize images into discrete visual tokens, which encounter limitations including exposure bias (Han et al., 2025) and suboptimal image tokenization (Li et al., 2024c). While recent efforts with continuous latent representations (Fan et al., 2024; Li et al., 2024c) have shown promise, significant performance gap persists between autoregressive models and state-of-the-art diffusion methods (Esser et al., 2024; Labs, 2024; Podell et al., 2024), particularly in image quality and consistency. In this paper, we introduce NextStep-1, simple yet effective autoregressive model built on the next-token prediction paradigm that achieves state-of-the-art performance in text-toimage generation tasks. Comprehensive evaluations confirm its competitive performance across suite of challenging benchmarks. Specifically, NextStep-1 demonstrates exceptional compositional and linguistic understanding, achieving 0.54 on WISE (Niu et al., 2025), 0.67 on the advanced prompts of GenAI-Bench (Lin et al., 2024), 85.28 on DPG-Bench (Hu et al., 2024), and 0.417 on the English prompts of OneIG-Bench (Chang et al., 2025). These results demonstrate its capabilities across diverse scenarios, from short and long prompts to tasks Figure 1 Overview of NextStep-1 in high-fidelity image generation, diverse image editing, and complex free-form manipulation. 2 Figure 2 Overview of NextStep-1 Framework. NextStep-1 employs causal transformer to process tokenized text and image tokens. During training, Flow Matching Head predicts the continuous flow from noise sample to the next target image patch, conditioned on the output hidden state. At inference, this allows for generating images by iteratively guiding noise to create the next patch. requiring world knowledge. Beyond generation, the versatility of NextStep-1 is validated by its strong performance on instruction-based image editing, NextStep-1-Edit, achieving competitive scores of 6.58 for English prompts on GEdit-Bench (Liu et al., 2025) and 3.71 on ImgEditBench (Ye et al., 2025). We showcase the qualitatve performance in Fig. 1. NextStep-1 is 14-billion-parameter autoregressive model composed of Transformer backbone, standard language modeling head for discrete text tokens, lightweight flow matching head for continuous image tokens, and an image tokenizer. The flow matching head is 157million-parameter, MLP-based model trained with flow matching objective, following the approach of (Li et al., 2024c). In autoregressive modeling, high-dimensional latent spaces are critical for achieving high image quality but often induce training instability and divergence. Our image tokenizer addresses this trade-off by enhancing the robustness of continuous image tokens and promoting well-dispersed, normalized latent space, thereby ensuring stable convergence even at higher dimensionalities (e.g., 16 channels). Empirical results confirm that this design is essential for stable and effective training with 16-channel latents. 2. Framework 2.1. Unified Multi-model Generation with Continuous Visual Tokens NextStep-1 extends the well-established autoregressive language modeling paradigm to image generation through simple and intuitive architecture, as illustrated in Fig. 2. To unify multimodal inputs into single sequence, the images will be tokenized to continuous image tokens by the image tokenizer and combined with discrete text tokens. Supposing 𝑥 = {𝑥0, 𝑥1, ..., 𝑥𝑛} is the multimodel token sequence, where 𝑥𝑖 is either discrete text token or continuous visual token, the autoregressive objective under the unified sequence is formalized as: 𝑝(𝑥) = 𝑛 (cid:214) 𝑖=1 𝑝(𝑥𝑖 𝑥<𝑖). (1) The unified multi-modal generation task proceeds by sampling the next token 𝑥𝑖 from the conditional distribution 𝑝(𝑥𝑖 𝑥<𝑖) modeled by network. Discrete text tokens are sampled via language modeling head, while continuous image tokens are sampled by flow-matching head. 3 Our training objective consists of two distinct losses: standard cross-entropy loss for discrete text tokens, and flow matching loss (Lipman et al., 2023b) for continuous image tokens. Specifically, the flow matching loss is the mean squared error between the predicted and target velocity vectors that map noised patch to its corresponding clean patch. The model is trained end-to-end by optimizing weighted sum of these two losses: Ltotal = 𝜆textLtext + 𝜆 visualLvisual (2) where Ltext and Lvisual denote the loss for text and image tokens respectively, which are balanced by the hy"
[15.08.2025 03:48] Mistral response. {"id": "8b76694fd7444addac1839c6ff4ced8f", "created": 1755229738, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1686, "total_tokens": 1695, "completion_tokens": 9}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"StepFun\"]\n```"}}]}
[15.08.2025 03:48] Response: ```python
["StepFun"]
```
[15.08.2025 03:48] Deleting PDF ./assets/pdf/2508.10711.pdf.
[15.08.2025 03:48] Success.
[15.08.2025 03:48] Downloading and parsing paper https://huggingface.co/papers/2508.10433.
[15.08.2025 03:48] Extra JSON file exists (./assets/json/2508.10433.json), skip PDF parsing.
[15.08.2025 03:48] Paper image links file exists (./assets/img_data/2508.10433.json), skip HTML parsing.
[15.08.2025 03:48] Success.
[15.08.2025 03:48] Downloading and parsing paper https://huggingface.co/papers/2508.09848.
[15.08.2025 03:48] Extra JSON file exists (./assets/json/2508.09848.json), skip PDF parsing.
[15.08.2025 03:48] Paper image links file exists (./assets/img_data/2508.09848.json), skip HTML parsing.
[15.08.2025 03:48] Success.
[15.08.2025 03:48] Downloading and parsing paper https://huggingface.co/papers/2508.10833.
[15.08.2025 03:48] Downloading paper 2508.10833 from http://arxiv.org/pdf/2508.10833v1...
[15.08.2025 03:49] Extracting affiliations from text.
[15.08.2025 03:49] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"UI-Venus Technical Report: Building High-performance UI Agents with RFT Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen,, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, Weiqiang Wang Ant Group We present UI-Venus, native UI agent that takes only screenshots as input based on multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5. To show UI-Venuss summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models. To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies. To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment & Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA opensource UI agents, comprehensive data cleaning protocols and novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code: https://github.com/antgroup/UI-Venus 5 2 0 A 4 1 ] . [ 1 3 3 8 0 1 . 8 0 5 2 : r Figure 1 UI-Venus achieves SO"
[15.08.2025 03:49] Response: ```python
["Ant Group"]
```
[15.08.2025 03:49] Deleting PDF ./assets/pdf/2508.10833.pdf.
[15.08.2025 03:49] Success.
[15.08.2025 03:49] Downloading and parsing paper https://huggingface.co/papers/2508.10576.
[15.08.2025 03:49] Extra JSON file exists (./assets/json/2508.10576.json), skip PDF parsing.
[15.08.2025 03:49] Paper image links file exists (./assets/img_data/2508.10576.json), skip HTML parsing.
[15.08.2025 03:49] Success.
[15.08.2025 03:49] Downloading and parsing paper https://huggingface.co/papers/2508.10860.
[15.08.2025 03:49] Extra JSON file exists (./assets/json/2508.10860.json), skip PDF parsing.
[15.08.2025 03:49] Paper image links file exists (./assets/img_data/2508.10860.json), skip HTML parsing.
[15.08.2025 03:49] Success.
[15.08.2025 03:49] Enriching papers with extra data.
[15.08.2025 03:49] ********************************************************************************
[15.08.2025 03:49] Abstract 0. NextStep-1, a 14B autoregressive model with a 157M flow matching head, achieves state-of-the-art performance in text-to-image generation and image editing by processing discrete text tokens and continuous image tokens.  					AI-generated summary 				 Prevailing autoregressive (AR) models for text-to...
[15.08.2025 03:49] ********************************************************************************
[15.08.2025 03:49] Abstract 1. We-Math 2.0 enhances MLLMs' mathematical reasoning through a structured knowledge system, model-centric data space modeling, and reinforcement learning, demonstrating competitive performance on benchmarks.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have demonstrated imp...
[15.08.2025 03:49] ********************************************************************************
[15.08.2025 03:49] Abstract 2. A benchmark called PRELUDE evaluates long-context understanding by assessing the consistency of prequel stories with original books, revealing significant challenges for models compared to humans.  					AI-generated summary 				 We introduce PRELUDE, a benchmark for evaluating long-context understan...
[15.08.2025 03:49] ********************************************************************************
[15.08.2025 03:49] Abstract 3. UI-Venus, a multimodal large language model-based UI agent, achieves state-of-the-art performance in UI grounding and navigation tasks using reinforcement fine-tuning and novel self-evolving frameworks.  					AI-generated summary 				 We present UI-Venus, a native UI agent that takes only screenshot...
[15.08.2025 03:49] ********************************************************************************
[15.08.2025 03:49] Abstract 4. HumanSense is a benchmark for evaluating human-centered perception and interaction in Multimodal Large Language Models, focusing on multimodal context understanding and rational feedback through reinforcement learning.  					AI-generated summary 				 While Multimodal Large Language Models (MLLMs) sh...
[15.08.2025 03:49] ********************************************************************************
[15.08.2025 03:49] Abstract 5. A multi-dimensional modeling framework enhances automated interpreting quality assessment by integrating feature engineering, data augmentation, and explainable machine learning, focusing on transparency and detailed diagnostic feedback.  					AI-generated summary 				 Recent advancements in machine...
[15.08.2025 03:49] Read previous papers.
[15.08.2025 03:49] Generating reviews via LLM API.
[15.08.2025 03:49] Querying the API.
[15.08.2025 03:49] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

NextStep-1, a 14B autoregressive model with a 157M flow matching head, achieves state-of-the-art performance in text-to-image generation and image editing by processing discrete text tokens and continuous image tokens.  					AI-generated summary 				 Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.
[15.08.2025 03:49] Response: {
  "desc": "NextStep-1 - это новая модель для генерации изображений по тексту, состоящая из авторегрессионной части (14 млрд параметров) и головы для сопоставления потоков (157 млн параметров). Модель обрабатывает дискретные текстовые токены и непрерывные токены изображений, достигая высоких результатов в генерации и редактировании изображений. В отличие от предыдущих подходов, NextStep-1 не использует тяжелые диффузионные модели или векторное квантование. Авторы планируют открыть исходный код и модели для исследовательского сообщества.",
  "emoji": "🎨",
  "title": "Революция в генерации изображений: NextStep-1 объединяет текст и визуализацию"
}
[15.08.2025 03:49] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"NextStep-1, a 14B autoregressive model with a 157M flow matching head, achieves state-of-the-art performance in text-to-image generation and image editing by processing discrete text tokens and continuous image tokens.  					AI-generated summary 				 Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community."

[15.08.2025 03:49] Response: ```python
['CV', 'MULTIMODAL', 'ARCHITECTURE', 'TRAINING']
```
[15.08.2025 03:49] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"NextStep-1, a 14B autoregressive model with a 157M flow matching head, achieves state-of-the-art performance in text-to-image generation and image editing by processing discrete text tokens and continuous image tokens.  					AI-generated summary 				 Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community."

[15.08.2025 03:49] Response: ```python
['DIFFUSION', 'OPEN_SOURCE']
```
[15.08.2025 03:49] Response: ParsedChatCompletionMessage[Article](content='{"desc":"NextStep-1 is a 14 billion parameter autoregressive model designed for text-to-image generation and image editing. It uniquely combines discrete text tokens with continuous image tokens, using a 157 million parameter flow matching head to enhance performance. This model outperforms existing methods by avoiding the computational heaviness of diffusion models and the quantization loss associated with vector quantization. The results demonstrate its ability to generate high-fidelity images and effectively edit them, showcasing the model\'s versatility and potential for future research.","title":"NextStep-1: Revolutionizing Text-to-Image Generation with Autoregressive Power"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="NextStep-1 is a 14 billion parameter autoregressive model designed for text-to-image generation and image editing. It uniquely combines discrete text tokens with continuous image tokens, using a 157 million parameter flow matching head to enhance performance. This model outperforms existing methods by avoiding the computational heaviness of diffusion models and the quantization loss associated with vector quantization. The results demonstrate its ability to generate high-fidelity images and effectively edit them, showcasing the model's versatility and potential for future research.", title='NextStep-1: Revolutionizing Text-to-Image Generation with Autoregressive Power'))
[15.08.2025 03:49] Response: ParsedChatCompletionMessage[Article](content='{"desc":"NextStep-1是一种14B的自回归模型，配备157M的流匹配头，专注于文本到图像生成和图像编辑。与传统的自回归模型不同，它通过处理离散的文本标记和连续的图像标记，避免了量化损失。该模型在文本到图像生成任务中表现出色，能够生成高保真的图像。此外，NextStep-1在图像编辑方面也展现了强大的能力，证明了其方法的强大和多样性。","title":"NextStep-1：文本到图像生成的新突破"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='NextStep-1是一种14B的自回归模型，配备157M的流匹配头，专注于文本到图像生成和图像编辑。与传统的自回归模型不同，它通过处理离散的文本标记和连续的图像标记，避免了量化损失。该模型在文本到图像生成任务中表现出色，能够生成高保真的图像。此外，NextStep-1在图像编辑方面也展现了强大的能力，证明了其方法的强大和多样性。', title='NextStep-1：文本到图像生成的新突破'))
[15.08.2025 03:49] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#benchmark", "#reasoning", "#training", "#data", "#rl"], "emoji": "🧮", "ru": {"title": "Усиление математических способностей ИИ через структурированные знания и обучение с подкреплением", "desc": "We-Math 2.0 - это унифицированная система для улучшения м
[15.08.2025 03:49] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#multimodal", "#long_context"], "emoji": "📚", "ru": {"title": "PRELUDE: новый рубеж в оценке глубины понимания текста искусственным интеллектом", "desc": "Представлен новый бенчмарк PRELUDE для оценки понимания длинного контекста моделями машинного обучен
[15.08.2025 03:49] Querying the API.
[15.08.2025 03:49] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

UI-Venus, a multimodal large language model-based UI agent, achieves state-of-the-art performance in UI grounding and navigation tasks using reinforcement fine-tuning and novel self-evolving frameworks.  					AI-generated summary 				 We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models.To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies.To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment \& Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/antgroup/UI-Venus.
[15.08.2025 03:49] Response: {
  "desc": "UI-Venus - это мультимодальная модель на основе большой языковой модели для работы с пользовательскими интерфейсами. Она достигает наилучших результатов в задачах навигации и привязки к элементам интерфейса, используя только скриншоты в качестве входных данных. Модель обучается с помощью подкрепляющего обучения на основе Qwen2.5-VL. Авторы представляют новые методы самоэволюции для улучшения производительности в сложных задачах навигации.",
  "emoji": "🖥️",
  "title": "UI-Venus: ИИ-агент нового поколения для работы с пользовательскими интерфейсами"
}
[15.08.2025 03:49] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"UI-Venus, a multimodal large language model-based UI agent, achieves state-of-the-art performance in UI grounding and navigation tasks using reinforcement fine-tuning and novel self-evolving frameworks.  					AI-generated summary 				 We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models.To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies.To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment \& Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/antgroup/UI-Venus."

[15.08.2025 03:49] Response: ```python
['AGENTS', 'RL', 'DATA', 'TRAINING', 'MULTIMODAL']
```
[15.08.2025 03:49] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"UI-Venus, a multimodal large language model-based UI agent, achieves state-of-the-art performance in UI grounding and navigation tasks using reinforcement fine-tuning and novel self-evolving frameworks.  					AI-generated summary 				 We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models.To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies.To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment \& Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/antgroup/UI-Venus."

[15.08.2025 03:49] Response: ```python
["GAMES", "OPEN_SOURCE", "OPTIMIZATION"]
```
[15.08.2025 03:49] Response: ParsedChatCompletionMessage[Article](content='{"desc":"UI-Venus is a multimodal large language model-based UI agent that excels in UI grounding and navigation tasks. It utilizes reinforcement fine-tuning and innovative self-evolving frameworks to achieve state-of-the-art performance with only a few hundred thousand high-quality training samples. The model\'s variants, 7B and 72B, have surpassed previous benchmarks, demonstrating impressive success rates in both grounding and navigation tasks. Key contributions include the introduction of specialized reward functions and data cleaning strategies, along with a novel framework that enhances navigation performance through improved planning and reasoning.","title":"UI-Venus: Redefining UI Navigation with Multimodal Intelligence"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="UI-Venus is a multimodal large language model-based UI agent that excels in UI grounding and navigation tasks. It utilizes reinforcement fine-tuning and innovative self-evolving frameworks to achieve state-of-the-art performance with only a few hundred thousand high-quality training samples. The model's variants, 7B and 72B, have surpassed previous benchmarks, demonstrating impressive success rates in both grounding and navigation tasks. Key contributions include the introduction of specialized reward functions and data cleaning strategies, along with a novel framework that enhances navigation performance through improved planning and reasoning.", title='UI-Venus: Redefining UI Navigation with Multimodal Intelligence'))
[15.08.2025 03:49] Response: ParsedChatCompletionMessage[Article](content='{"desc":"UI-Venus是一种基于多模态大语言模型的用户界面代理，能够仅通过截图输入来完成任务。它在用户界面定位和导航任务中表现出色，采用了强化微调和自我演化框架。UI-Venus的7B和72B版本在标准基准测试中超越了之前的最佳模型，显示出其强大的能力。该模型还引入了精心设计的奖励函数和高效的数据清理策略，以提升导航性能和规划能力。","title":"UI-Venus：用户界面任务的新标杆"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='UI-Venus是一种基于多模态大语言模型的用户界面代理，能够仅通过截图输入来完成任务。它在用户界面定位和导航任务中表现出色，采用了强化微调和自我演化框架。UI-Venus的7B和72B版本在标准基准测试中超越了之前的最佳模型，显示出其强大的能力。该模型还引入了精心设计的奖励函数和高效的数据清理策略，以提升导航性能和规划能力。', title='UI-Venus：用户界面任务的新标杆'))
[15.08.2025 03:49] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#interpretability", "#reasoning", "#rl", "#multimodal"], "emoji": "🧠", "ru": {"title": "Оценка человекоориентированного восприятия в мультимодальных ИИ-моделях", "desc": "HumanSense - это комплексный бенчмарк для оценки восприятия и взаимодействия, ориентирова
[15.08.2025 03:49] Using data from previous issue: {"categories": ["#multilingual", "#science", "#dataset", "#optimization", "#interpretability", "#training", "#data"], "emoji": "🗣️", "ru": {"title": "Прозрачная оценка качества устного перевода с помощью ИИ", "desc": "Статья представляет многомерную модель для автоматической оценки качества устного 
[15.08.2025 03:49] Renaming data file.
[15.08.2025 03:49] Renaming previous data. hf_papers.json to ./d/2025-08-15.json
[15.08.2025 03:49] Saving new data file.
[15.08.2025 03:49] Generating page.
[15.08.2025 03:49] Renaming previous page.
[15.08.2025 03:49] Renaming previous data. index.html to ./d/2025-08-15.html
[15.08.2025 03:49] Writing result.
[15.08.2025 03:49] Renaming log file.
[15.08.2025 03:49] Renaming previous data. log.txt to ./logs/2025-08-15_last_log.txt
