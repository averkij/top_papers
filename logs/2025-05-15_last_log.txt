[15.05.2025 08:16] Read previous papers.
[15.05.2025 08:16] Generating top page (month).
[15.05.2025 08:16] Writing top page (month).
[15.05.2025 09:12] Read previous papers.
[15.05.2025 09:12] Get feed.
[15.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04410
[15.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09568
[15.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09343
[15.05.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08455
[15.05.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.05.2025 09:12] No deleted papers detected.
[15.05.2025 09:12] Downloading and parsing papers (pdf, html). Total: 4.
[15.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.04410.
[15.05.2025 09:12] Extra JSON file exists (./assets/json/2505.04410.json), skip PDF parsing.
[15.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.04410.json), skip HTML parsing.
[15.05.2025 09:12] Success.
[15.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.09568.
[15.05.2025 09:12] Extra JSON file exists (./assets/json/2505.09568.json), skip PDF parsing.
[15.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.09568.json), skip HTML parsing.
[15.05.2025 09:12] Success.
[15.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.09343.
[15.05.2025 09:12] Extra JSON file exists (./assets/json/2505.09343.json), skip PDF parsing.
[15.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.09343.json), skip HTML parsing.
[15.05.2025 09:12] Success.
[15.05.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2505.08455.
[15.05.2025 09:12] Extra JSON file exists (./assets/json/2505.08455.json), skip PDF parsing.
[15.05.2025 09:12] Paper image links file exists (./assets/img_data/2505.08455.json), skip HTML parsing.
[15.05.2025 09:12] Success.
[15.05.2025 09:12] Enriching papers with extra data.
[15.05.2025 09:12] ********************************************************************************
[15.05.2025 09:12] Abstract 0. Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct applicatio...
[15.05.2025 09:12] ********************************************************************************
[15.05.2025 09:12] Abstract 1. Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain und...
[15.05.2025 09:12] ********************************************************************************
[15.05.2025 09:12] Abstract 2. The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware mo...
[15.05.2025 09:12] ********************************************************************************
[15.05.2025 09:12] Abstract 3. Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven ...
[15.05.2025 09:12] Read previous papers.
[15.05.2025 09:12] Generating reviews via LLM API.
[15.05.2025 09:12] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#cv", "#optimization", "#open_source"], "emoji": "🔍", "ru": {"title": "DeCLIP: Новый шаг к универсальному компьютерному зрению", "desc": "Статья представляет новый подход DeCLIP для улучшения возможностей моделей компьютерного зрения в задачах плотног
[15.05.2025 09:12] Using data from previous issue: {"categories": ["#dataset", "#diffusion", "#open_source", "#multimodal", "#training", "#architecture"], "emoji": "🖼️", "ru": {"title": "Объединение понимания и генерации изображений с помощью диффузионных трансформеров", "desc": "Статья представляет новый подход к объединению понимания и генерации и
[15.05.2025 09:12] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization"], "emoji": "🧠", "ru": {"title": "Совместное проектирование моделей и оборудования для масштабирования ИИ", "desc": "Статья описывает архитектуру модели DeepSeek-V3/R1 и инфраструктуру ИИ, разработанные для преодоления ограни
[15.05.2025 09:12] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#long_context", "#video"], "emoji": "🎬", "ru": {"title": "Новый бенчмарк для оценки причинно-следственного рассуждения в видео-языковых моделях", "desc": "Исследователи представили новый бенчмарк VCRBench для оценки способностей Больших Видео-Языковых Мод
[15.05.2025 09:12] Trying to get texts in Chinese.
[15.05.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. The ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. Code is available at magenta{https://github.com/xiaomoguhz/DeCLIP}.
[15.05.2025 09:12] Mistral response. {"id": "0a48fc6cef634e389d67495d620348b3", "object": "chat.completion", "created": 1747300369, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u4f20\u7edf\u7684\u89c6\u89c9\u9884\u6d4b\u4efb\u52a1\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u800c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5982CLIP\u5728\u5f00\u653e\u8bcd\u6c47\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\u3002\u7136\u800c\uff0cCLIP\u5728\u5bc6\u96c6\u9884\u6d4b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5176\u5c40\u90e8\u7279\u5f81\u8868\u793a\u6709\u9650\u3002\u7814\u7a76\u53d1\u73b0\uff0cCLIP\u7684\u56fe\u50cf\u6807\u8bb0\u96be\u4ee5\u6709\u6548\u805a\u5408\u7a7a\u95f4\u6216\u8bed\u4e49\u76f8\u5173\u533a\u57df\u7684\u4fe1\u606f\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86DeCLIP\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u81ea\u6ce8\u610f\u6a21\u5757\u83b7\u5f97\u201c\u5185\u5bb9\u201d\u548c\u201c\u4e0a\u4e0b\u6587\u201d\u7279\u5f81\uff0c\u5206\u522b\u63d0\u9ad8\u5c40\u90e8\u8fa8\u522b\u6027\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cDeCLIP\u5728\u591a\u4e2a\u5f00\u653e\u8bcd\u6c47\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 294, "total_tokens": 505, "completion_tokens": 211}}
[15.05.2025 09:12] Response: 这篇文章讨论了传统的视觉预测任务受限于预定义类别，而视觉-语言模型（VLMs）如CLIP在开放词汇任务中表现出潜力。然而，CLIP在密集预测中表现不佳，因为其局部特征表示有限。研究发现，CLIP的图像标记难以有效聚合空间或语义相关区域的信息。为解决这一问题，作者提出了DeCLIP框架，通过解耦自注意模块获得“内容”和“上下文”特征，分别提高局部辨别性和空间一致性。实验证明，DeCLIP在多个开放词汇密集预测任务中表现优异。
[15.05.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

这篇文章讨论了传统的视觉预测任务受限于预定义类别，而视觉-语言模型（VLMs）如CLIP在开放词汇任务中表现出潜力。然而，CLIP在密集预测中表现不佳，因为其局部特征表示有限。研究发现，CLIP的图像标记难以有效聚合空间或语义相关区域的信息。为解决这一问题，作者提出了DeCLIP框架，通过解耦自注意模块获得“内容”和“上下文”特征，分别提高局部辨别性和空间一致性。实验证明，DeCLIP在多个开放词汇密集预测任务中表现优异。
[15.05.2025 09:13] Mistral response. {"id": "47d017864f20418982c02b7d65b5fa32", "object": "chat.completion", "created": 1747300372, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u4f20\u7edf\u7684\u89c6\u89c9\u9884\u6d4b\u4efb\u52a1\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u800c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5982CLIP\u5728\u5f00\u653e\u8bcd\u6c47\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\u3002\u7136\u800c\uff0cCLIP\u5728\u5bc6\u96c6\u9884\u6d4b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5176\u5c40\u90e8\u7279\u5f81\u8868\u793a\u6709\u9650\u3002\u7814\u7a76\u53d1\u73b0\uff0cCLIP\u7684\u56fe\u50cf\u6807\u8bb0\u96be\u4ee5\u6709\u6548\u805a\u5408\u7a7a\u95f4\u6216\u8bed\u4e49\u76f8\u5173\u533a\u57df\u7684\u4fe1\u606f\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86DeCLIP\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u81ea\u6ce8\u610f\u6a21\u5757\u83b7\u5f97\u201c\u5185\u5bb9\u201d\u548c\u201c\u4e0a\u4e0b\u6587\u201d\u7279\u5f81\uff0c\u5206\u522b\u63d0\u9ad8\u5c40\u90e8\u8fa8\u522b\u6027\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cDeCLIP\u5728\u591a\u4e2a\u5f00\u653e\u8bcd\u6c47\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\n\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng t\u01ceol\u00f9n le chu\u00e1nt\u01d2ng de sh\u00ecju\u00e9 y\u00f9c\u00e8 r\u00e8nw\u00f9 sh\u00f2uxi\u00e0n z\u00e0i y\u00f9d\u00ecngy\u00ec l\u00e8ibi\u00e9, \u00e9r sh\u00ecju\u00e9-y\u01d4y\u00e1n m\u00f3x\u00edng (VLMs) r\u00fa CLIP z\u00e0i k\u0101if\u00e0ng c\u00edhu\u00ec r\u00e8nw\u00f9 zh\u014dng bi\u01ceoxi\u00e0n ch\u016b qi\u00e1nl\u00ec. R\u00e1n'\u00e9r, CLIP z\u00e0i m\u00ecj\u012b y\u00f9c\u00e8 zh\u014dng bi\u01ceoxi\u00e0n b\u00f9ji\u0101, y\u012bnw\u00e8i q\u00ed j\u00fab\u00f9 t\u00e8zh\u0113ng bi\u01ceosh\u00ec y\u01d2uxi\u00e0n. Y\u00e1nji\u016b f\u0101xi\u00e0n, CLIP de t\u00faxi\u00e0ng bi\u0101oji\u00e0n n\u00e1n y\u01d0 y\u01d2uxi\u00e0o j\u00f9h\u00e9 k\u014dngji\u0101n hu\u00f2 y\u00f9y\u00ec xi\u0101nggu\u0101n q\u016by\u00f9 de x\u00ecnx\u012b. W\u00e8i ji\u011bju\u00e9 zh\u00e8 y\u012b w\u00e8nt\u00ed, zu\u00f2zh\u011b t\u00edch\u016b le DeCLIP ku\u00e0ngji\u00e0, t\u014dnggu\u00f2 ji\u011bk\u01d2u z\u00eczh\u00f9y\u00ec m\u00f3ku\u00e0i hu\u00f2d\u00e9 \u201cn\u00e8ir\u00f3ng\u201d h\u00e9 \u201csh\u00e0ngx\u00ecat\u00fa\u201d t\u00e8zh\u0113ng, f\u0113nbi\u00e9 t\u012bg\u0101o j\u00fab\u00f9 bi\u00e0nbi\u00e9x\u00ecng h\u00e9 k\u014dngji\u0101n y\u012bzh\u00ecx\u00ecng. Sh\u00edy\u00e0n zh\u00e8ngm\u00edng, DeCLIP z\u00e0i du\u014dg\u00e8 k\u0101if\u00e0ng c\u00edhu\u00ec m\u00ecj\u012b y\u00f9c\u00e8 r\u00e8nw\u00f9 zh\u014dng bi\u01ceoxi\u00e0n y\u014duy\u00f9n."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 225, "total_tokens": 868, "completion_tokens": 643}}
[15.05.2025 09:13] Response: 这篇文章讨论了传统的视觉预测任务受限于预定义类别，而视觉-语言模型（VLMs）如CLIP在开放词汇任务中表现出潜力。然而，CLIP在密集预测中表现不佳，因为其局部特征表示有限。研究发现，CLIP的图像标记难以有效聚合空间或语义相关区域的信息。为解决这一问题，作者提出了DeCLIP框架，通过解耦自注意模块获得“内容”和“上下文”特征，分别提高局部辨别性和空间一致性。实验证明，DeCLIP在多个开放词汇密集预测任务中表现优异。

Zhè piān wénzhāng tǎolùn le chuántǒng de shìjué yùcè rènwù shòuxiàn zài yùdìngyì lèibié, ér shìjué-yǔyán móxíng (VLMs) rú CLIP zài kāifàng cíhuì rènwù zhōng biǎoxiàn chū qiánlì. Rán'ér, CLIP zài mìjī yùcè zhōng biǎoxiàn bùjiā, yīnwèi qí júbù tèzhēng biǎoshì yǒuxiàn. Yánjiū fāxiàn, CLIP de túxiàng biāojiàn nán yǐ yǒuxiào jùhé kōngjiān huò yùyì xiāngguān qūyù de xìnxī. Wèi jiějué zhè yī wèntí, zuòzhě tíchū le DeCLIP kuàngjià, tōngguò jiěkǒu zìzhùyì mókuài huòdé “nèiróng” hé “shàngxìatú” tèzhēng, fēnbié tīgāo júbù biànbiéxìng hé kōngjiān yīzhìxìng. Shíyàn zhèngmíng, DeCLIP zài duōgè kāifàng cíhuì mìjī yùcè rènwù zhōng biǎoxiàn yōuyùn.
[15.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

这篇文章讨论了传统的视觉预测任务受限于预定义类别，而视觉-语言模型（VLMs）如CLIP在开放词汇任务中表现出潜力。然而，CLIP在密集预测中表现不佳，因为其局部特征表示有限。研究发现，CLIP的图像标记难以有效聚合空间或语义相关区域的信息。为解决这一问题，作者提出了DeCLIP框架，通过解耦自注意模块获得“内容”和“上下文”特征，分别提高局部辨别性和空间一致性。实验证明，DeCLIP在多个开放词汇密集预测任务中表现优异。
[15.05.2025 09:13] Mistral response. {"id": "91b6bfccab1d4aa2a5c2078da68c7384", "object": "chat.completion", "created": 1747300382, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u8ba8\u8bba', 'pinyin': 't\u01ceo l\u00f9n', 'trans': 'discuss'}, {'word': '\u9884\u6d4b', 'pinyin': 'y\u00f9 c\u00e8', 'trans': 'predict'}, {'word': '\u53d7\u9650', 'pinyin': 'sh\u00f2u xi\u00e0n', 'trans': 'be limited'}, {'word': '\u9884\u5b9a\u4e49', 'pinyin': 'y\u00f9 d\u00ecng y\u00ec', 'trans': 'predefined'}, {'word': '\u7c7b\u522b', 'pinyin': 'l\u00e8i bi\u00e9', 'trans': 'category'}, {'word': '\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b', 'pinyin': 'sh\u00ec ju\u00e9 y\u01d4 y\u00e1n m\u00f3 x\u00edng', 'trans': 'vision-language model'}, {'word': '\u8868\u73b0', 'pinyin': 'bi\u01ceo xi\u00e0n', 'trans': 'perform'}, {'word': '\u6f5c\u529b', 'pinyin': 'qi\u00e1n l\u00ec', 'trans': 'potential'}, {'word': '\u5bc6\u96c6', 'pinyin': 'm\u00ec j\u00ed', 'trans': 'dense'}, {'word': '\u5c40\u90e8', 'pinyin': 'j\u00fa b\u00f9', 'trans': 'local'}, {'word': '\u7279\u5f81', 'pinyin': 't\u00e8 zh\u0113ng', 'trans': 'feature'}, {'word': '\u8868\u793a', 'pinyin': 'bi\u01ceo sh\u00ec', 'trans': 'represent'}, {'word': '\u6709\u9650', 'pinyin': 'y\u01d2u xi\u00e0n', 'trans': 'limited'}, {'word': '\u6807\u8bb0', 'pinyin': 'bi\u0101o j\u00ec', 'trans': 'mark'}, {'word': '\u805a\u5408', 'pinyin': 'j\u00f9 h\u00e9', 'trans': 'aggregate'}, {'word': '\u7a7a\u95f4', 'pinyin': 'k\u014dng ji\u0101n', 'trans': 'spatial'}, {'word': '\u8bed\u4e49', 'pinyin': 'y\u01d4 y\u00ec', 'trans': 'semantic'}, {'word': '\u76f8\u5173', 'pinyin': 'xi\u0101ng gu\u0101n', 'trans': 'related'}, {'word': '\u533a\u57df', 'pinyin': 'q\u016b y\u00f9', 'trans': 'region'}, {'word': '\u4fe1\u606f', 'pinyin': 'x\u00ecn x\u012b', 'trans': 'information'}, {'word': '\u89e3\u51b3', 'pinyin': 'ji\u011b ju\u00e9', 'trans': 'solve'}, {'word': '\u63d0\u51fa', 'pinyin': 't\u00ed ch\u016b', 'trans': 'propose'}, {'word': '\u6846\u67b6', 'pinyin': 'ku\u00e0ng ji\u00e0', 'trans': 'framework'}, {'word': '\u89e3\u8026', 'pinyin': 'ji\u011b \u01d2u', 'trans': 'decouple'}, {'word': '\u81ea\u6ce8\u610f', 'pinyin': 'z\u00ec zh\u00f9 y\u00ec', 'trans': 'self-attention'}, {'word': '\u6a21\u5757', 'pinyin': 'm\u00f3 ku\u00e0i', 'trans': 'module'}, {'word': '\u83b7\u5f97', 'pinyin': 'hu\u00f2 d\u00e9', 'trans': 'obtain'}, {'word': '\u5185\u5bb9', 'pinyin': 'n\u00e8i r\u00f3ng', 'trans': 'content'}, {'word': '\u4e0a\u4e0b\u6587', 'pinyin': 'sh\u00e0ng xi\u00e0 w\u00e9n', 'trans': 'context'}, {'word': '\u8fa8\u522b\u6027', 'pinyin': 'bi\u00e0n bi\u00e9 x\u00ecng', 'trans': 'discriminability'}, {'word': '\u4e00\u81f4\u6027', 'pinyin': 'y\u012b zh\u00ec x\u00ecng', 'trans': 'consistency'}, {'word': '\u5b9e\u9a8c', 'pinyin': 'sh\u00ed y\u00e0n', 'trans': 'experiment'}, {'word': '\u8bc1\u660e', 'pinyin': 'zh\u00e8ng m\u00edng', 'trans': 'prove'}, {'word': '\u4f18\u5f02', 'pinyin': 'y\u014du y\u00ec', 'trans': 'excellent'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 257, "total_tokens": 1183, "completion_tokens": 926}}
[15.05.2025 09:13] Response: [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '预测', 'pinyin': 'yù cè', 'trans': 'predict'}, {'word': '受限', 'pinyin': 'shòu xiàn', 'trans': 'be limited'}, {'word': '预定义', 'pinyin': 'yù dìng yì', 'trans': 'predefined'}, {'word': '类别', 'pinyin': 'lèi bié', 'trans': 'category'}, {'word': '视觉-语言模型', 'pinyin': 'shì jué yǔ yán mó xíng', 'trans': 'vision-language model'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'}, {'word': '潜力', 'pinyin': 'qián lì', 'trans': 'potential'}, {'word': '密集', 'pinyin': 'mì jí', 'trans': 'dense'}, {'word': '局部', 'pinyin': 'jú bù', 'trans': 'local'}, {'word': '特征', 'pinyin': 'tè zhēng', 'trans': 'feature'}, {'word': '表示', 'pinyin': 'biǎo shì', 'trans': 'represent'}, {'word': '有限', 'pinyin': 'yǒu xiàn', 'trans': 'limited'}, {'word': '标记', 'pinyin': 'biāo jì', 'trans': 'mark'}, {'word': '聚合', 'pinyin': 'jù hé', 'trans': 'aggregate'}, {'word': '空间', 'pinyin': 'kōng jiān', 'trans': 'spatial'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'}, {'word': '相关', 'pinyin': 'xiāng guān', 'trans': 'related'}, {'word': '区域', 'pinyin': 'qū yù', 'trans': 'region'}, {'word': '信息', 'pinyin': 'xìn xī', 'trans': 'information'}, {'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '解耦', 'pinyin': 'jiě ǒu', 'trans': 'decouple'}, {'word': '自注意', 'pinyin': 'zì zhù yì', 'trans': 'self-attention'}, {'word': '模块', 'pinyin': 'mó kuài', 'trans': 'module'}, {'word': '获得', 'pinyin': 'huò dé', 'trans': 'obtain'}, {'word': '内容', 'pinyin': 'nèi róng', 'trans': 'content'}, {'word': '上下文', 'pinyin': 'shàng xià wén', 'trans': 'context'}, {'word': '辨别性', 'pinyin': 'biàn bié xìng', 'trans': 'discriminability'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '证明', 'pinyin': 'zhèng míng', 'trans': 'prove'}, {'word': '优异', 'pinyin': 'yōu yì', 'trans': 'excellent'}]
[15.05.2025 09:13] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

这篇文章讨论了传统的视觉预测任务受限于预定义类别，而视觉-语言模型（VLMs）如CLIP在开放词汇任务中表现出潜力。然而，CLIP在密集预测中表现不佳，因为其局部特征表示有限。研究发现，CLIP的图像标记难以有效聚合空间或语义相关区域的信息。为解决这一问题，作者提出了DeCLIP框架，通过解耦自注意模块获得“内容”和“上下文”特征，分别提高局部辨别性和空间一致性。实验证明，DeCLIP在多个开放词汇密集预测任务中表现优异。
[15.05.2025 09:13] Mistral response. {"id": "df371438cdf044cfae96bb0d87ba0c15", "object": "chat.completion", "created": 1747300390, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses how traditional visual prediction tasks are constrained by predefined categories, while vision-language models (VLMs) such as CLIP show potential in open-vocabulary tasks. However, CLIP performs poorly in dense prediction because its local feature representations are limited. Research has found that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions. To address this issue, the authors propose the DeCLIP framework, which obtains \"content\" and \"context\" features by decoupling the self-attention module, thereby enhancing local discriminability and spatial consistency, respectively. Experiments demonstrate that DeCLIP performs excellently in multiple open-vocabulary dense prediction tasks."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 223, "total_tokens": 376, "completion_tokens": 153}}
[15.05.2025 09:13] Response: This article discusses how traditional visual prediction tasks are constrained by predefined categories, while vision-language models (VLMs) such as CLIP show potential in open-vocabulary tasks. However, CLIP performs poorly in dense prediction because its local feature representations are limited. Research has found that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions. To address this issue, the authors propose the DeCLIP framework, which obtains "content" and "context" features by decoupling the self-attention module, thereby enhancing local discriminability and spatial consistency, respectively. Experiments demonstrate that DeCLIP performs excellently in multiple open-vocabulary dense prediction tasks.
[15.05.2025 09:13] Renaming data file.
[15.05.2025 09:13] Renaming previous data. hf_papers.json to ./d/2025-05-15.json
[15.05.2025 09:13] Saving new data file.
[15.05.2025 09:13] Generating page.
[15.05.2025 09:13] Renaming previous page.
[15.05.2025 09:13] Renaming previous data. index.html to ./d/2025-05-15.html
[15.05.2025 09:13] [Experimental] Generating Chinese page for reading.
[15.05.2025 09:13] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '预测', 'pinyin': 'yù cè', 'trans': 'predict'}, {'word': '受限', 'pinyin': 'shòu xiàn', 'trans': 'be limited'}, {'word': '预定义', 'pinyin': 'yù dìng yì', 'trans': 'predefined'}, {'word': '类别', 'pinyin': 'lèi bié', 'trans': 'category'}, {'word': '视觉-语言模型', 'pinyin': 'shì jué yǔ yán mó xíng', 'trans': 'vision-language model'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'}, {'word': '潜力', 'pinyin': 'qián lì', 'trans': 'potential'}, {'word': '密集', 'pinyin': 'mì jí', 'trans': 'dense'}, {'word': '局部', 'pinyin': 'jú bù', 'trans': 'local'}, {'word': '特征', 'pinyin': 'tè zhēng', 'trans': 'feature'}, {'word': '表示', 'pinyin': 'biǎo shì', 'trans': 'represent'}, {'word': '有限', 'pinyin': 'yǒu xiàn', 'trans': 'limited'}, {'word': '标记', 'pinyin': 'biāo jì', 'trans': 'mark'}, {'word': '聚合', 'pinyin': 'jù hé', 'trans': 'aggregate'}, {'word': '空间', 'pinyin': 'kōng jiān', 'trans': 'spatial'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'}, {'word': '相关', 'pinyin': 'xiāng guān', 'trans': 'related'}, {'word': '区域', 'pinyin': 'qū yù', 'trans': 'region'}, {'word': '信息', 'pinyin': 'xìn xī', 'trans': 'information'}, {'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '解耦', 'pinyin': 'jiě ǒu', 'trans': 'decouple'}, {'word': '自注意', 'pinyin': 'zì zhù yì', 'trans': 'self-attention'}, {'word': '模块', 'pinyin': 'mó kuài', 'trans': 'module'}, {'word': '获得', 'pinyin': 'huò dé', 'trans': 'obtain'}, {'word': '内容', 'pinyin': 'nèi róng', 'trans': 'content'}, {'word': '上下文', 'pinyin': 'shàng xià wén', 'trans': 'context'}, {'word': '辨别性', 'pinyin': 'biàn bié xìng', 'trans': 'discriminability'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '证明', 'pinyin': 'zhèng míng', 'trans': 'prove'}, {'word': '优异', 'pinyin': 'yōu yì', 'trans': 'excellent'}]
[15.05.2025 09:13] Renaming previous Chinese page.
[15.05.2025 09:13] Renaming previous data. zh.html to ./d/2025-05-14_zh_reading_task.html
[15.05.2025 09:13] Writing Chinese reading task.
[15.05.2025 09:13] Writing result.
[15.05.2025 09:13] Renaming log file.
[15.05.2025 09:13] Renaming previous data. log.txt to ./logs/2025-05-15_last_log.txt
