[15.05.2025 22:10] Read previous papers.
[15.05.2025 22:10] Generating top page (month).
[15.05.2025 22:10] Writing top page (month).
[15.05.2025 23:11] Read previous papers.
[15.05.2025 23:11] Get feed.
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09568
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04410
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09343
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09358
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08787
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07849
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09558
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12894
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09439
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08455
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09608
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04793
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08084
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06356
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.05587
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08910
[15.05.2025 23:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.05.2025 23:11] No deleted papers detected.
[15.05.2025 23:11] Downloading and parsing papers (pdf, html). Total: 16.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.09568.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.09568.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.09568.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.04410.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.04410.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.04410.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.09343.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.09343.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.09343.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.09358.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.09358.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.09358.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.08787.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.08787.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.08787.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.07849.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.07849.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.07849.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.09558.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.09558.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.09558.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2502.12894.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2502.12894.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2502.12894.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.09439.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.09439.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.09439.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.08455.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.08455.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.08455.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.09608.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.09608.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.09608.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.04793.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.04793.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.04793.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.08084.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.08084.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.08084.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.06356.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.06356.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.06356.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.05587.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.05587.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.05587.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.08910.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.08910.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.08910.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Enriching papers with extra data.
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 0. Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain und...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 1. Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct applicatio...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 2. The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware mo...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 3. The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning h...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 4. Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appeara...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 5. Software issue localization, the task of identifying the precise code locations (files, classes, or functions) relevant to a natural language issue description (e.g., bug report, feature request), is a critical yet time-consuming aspect of software development. While recent LLM-based agentic approac...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 6. End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models' conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of no...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 7. Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 8. We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, s...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 9. Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven ...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 10. We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our m...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 11. Person reidentification (ReID) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, s...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 12. Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing task...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 13. Pretraining datasets are foundational to the development of multimodal models, yet they often have inherent biases and toxic content from the web-scale corpora they are sourced from. In this paper, we investigate the prevalence of toxicity in LLaVA image-text pretraining dataset, examining how harmf...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 14. 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capt...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 15. In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we intr...
[15.05.2025 23:11] Read previous papers.
[15.05.2025 23:11] Generating reviews via LLM API.
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#dataset", "#diffusion", "#open_source", "#multimodal", "#training", "#architecture"], "emoji": "üñºÔ∏è", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—é –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#cv", "#optimization", "#open_source"], "emoji": "üîç", "ru": {"title": "DeCLIP: –ù–æ–≤—ã–π —à–∞–≥ –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º—É –∑—Ä–µ–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ DeCLIP –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –ø–ª–æ—Ç–Ω–æ–≥
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization"], "emoji": "üß†", "ru": {"title": "–°–æ–≤–º–µ—Å—Ç–Ω–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ DeepSeek-V3/R1 –∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –ò–ò, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#cv", "#dataset", "#diffusion", "#synthetic", "#transfer_learning", "#training"], "emoji": "üåº", "ru": {"title": "Marigold: –†–∞—Å–∫—Ä—ã—Ç–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–ª–æ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Marigold - —Å–µ–º–µ–π—Å—Ç–≤–æ —É—Å–ª–æ–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –ø
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#video", "#transfer_learning", "#dataset", "#robotics", "#agents"], "emoji": "ü§ñ", "ru": {"title": "UniSkill: –û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –Ω–∞–≤—ã–∫–∞–º –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω UniSkill - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –Ω–∞–≤—ã–∫–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ —Å –ª—é–¥—å–º–∏. –û–Ω —Å–æ–∑–¥–∞–µ
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#data", "#benchmark", "#optimization", "#dataset", "#survey", "#agents"], "emoji": "üîç", "ru": {"title": "SweRank: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–ª–µ–º –≤ –∫–æ–¥–µ —Å –ø–æ–º–æ—â—å—é –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "SweRank - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–±–ª–µ–º –≤ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–º –æ–±–µ—Å–ø–µ—á
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#audio", "#reasoning", "#open_source", "#rlhf", "#dataset", "#benchmark"], "emoji": "üéôÔ∏è", "ru": {"title": "WavReward: –ø—Ä–æ—Ä—ã–≤ –≤ –æ—Ü–µ–Ω–∫–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –ò–ò-—Å–∏—Å—Ç–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç WavReward - –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ. –≠—Ç–∞ –º–æ–¥–µ–ª—å —Å–ø–æ—Å–æ–±–Ω–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å 
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#3d", "#graphs", "#robotics", "#optimization"], "emoji": "üèôÔ∏è", "ru": {"title": "–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è 3D-—Å—Ü–µ–Ω —Å —É—á–µ—Ç–æ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏ —Ñ–∏–∑–∏–∫–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "desc": "CAST - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 3D-—Å—Ü–µ–Ω –∏–∑ –æ–¥–Ω–æ–≥–æ RGB-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –æ–±—ä–µ–∫—Ç–æ–≤, –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#training", "#multimodal", "#rl", "#reasoning", "#optimization", "#benchmark", "#rag"], "emoji": "üéß", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞—É–¥–∏–æ-–ò–ò: Omni-R1 –ø–æ–∫–æ—Ä—è–µ—Ç –Ω–æ–≤—ã–µ –≤–µ—Ä—à–∏–Ω—ã –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Omni-R1 - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å, —É–ª—É—á—à–µ
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#long_context", "#video"], "emoji": "üé¨", "ru": {"title": "–ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ VCRBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ë–æ–ª—å—à–∏—Ö –í–∏–¥–µ–æ-–Ø–∑—ã–∫–æ–≤—ã—Ö –ú–æ–¥
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#synthetic", "#data", "#dataset", "#training"], "emoji": "üí°", "ru": {"title": "–ü—Ä–µ—Ü–∏–∑–∏–æ–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Å–≤–µ—â–µ–Ω–∏–µ–º —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å–≤–µ—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –ú
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#cv", "#dataset", "#synthetic"], "emoji": "üéØ", "ru": {"title": "DetReIDX: –°—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç –¥–ª—è —Ä–µ–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª—é–¥–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DetReIDX - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞—á–∏ —Ä–µ–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª—é–¥–µ–π –≤ –≤–æ–∑–¥—É—à–Ω–æ-–Ω–∞–∑–µ–º–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞–µ
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#training", "#benchmark", "#dataset", "#reasoning", "#multimodal", "#interpretability", "#cv"], "emoji": "üß†", "ru": {"title": "VISTAR: –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VISTAR - –º–æ–¥–µ–ª—å –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–¥
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#ethics", "#open_source", "#data"], "emoji": "üßπ", "ru": {"title": "–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ç–∏—á–Ω–æ–≥–æ –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –≤ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö LLaVA –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∏ –ø—Ä–æ–≤–µ–ª–∏ –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#inference", "#3d", "#optimization"], "emoji": "üîç", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è 3D Gaussian Splatting: –º–µ–Ω—å—à–µ —Ç–æ—á–µ–∫, –≤—ã—à–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫—É—é –æ—Å–Ω–æ–≤—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—è –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –≤ –º–µ—Ç–æ–¥–µ 3D Gaussian Splatting (3DGS). –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#multilingual", "#low_resource", "#dataset", "#cv", "#open_source"], "emoji": "üåç", "ru": {"title": "Maya: –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è VLM –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –±–∞—Ä—å–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Maya - –æ—Ç–∫—Ä—ã—Ç—É—é –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ
[15.05.2025 23:11] Loading Chinese text from previous data.
[15.05.2025 23:11] Renaming data file.
[15.05.2025 23:11] Renaming previous data. hf_papers.json to ./d/2025-05-15.json
[15.05.2025 23:11] Saving new data file.
[15.05.2025 23:11] Generating page.
[15.05.2025 23:11] Renaming previous page.
[15.05.2025 23:11] Renaming previous data. index.html to ./d/2025-05-15.html
[15.05.2025 23:11] [Experimental] Generating Chinese page for reading.
[15.05.2025 23:11] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'È¢ÑÊµã', 'pinyin': 'y√π c√®', 'trans': 'predict'}, {'word': 'ÂèóÈôê', 'pinyin': 'sh√≤u xi√†n', 'trans': 'be limited'}, {'word': 'È¢ÑÂÆö‰πâ', 'pinyin': 'y√π d√¨ng y√¨', 'trans': 'predefined'}, {'word': 'Á±ªÂà´', 'pinyin': 'l√®i bi√©', 'trans': 'category'}, {'word': 'ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'sh√¨ ju√© y«î y√°n m√≥ x√≠ng', 'trans': 'vision-language model'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'perform'}, {'word': 'ÊΩúÂäõ', 'pinyin': 'qi√°n l√¨', 'trans': 'potential'}, {'word': 'ÂØÜÈõÜ', 'pinyin': 'm√¨ j√≠', 'trans': 'dense'}, {'word': 'Â±ÄÈÉ®', 'pinyin': 'j√∫ b√π', 'trans': 'local'}, {'word': 'ÁâπÂæÅ', 'pinyin': 't√® zhƒìng', 'trans': 'feature'}, {'word': 'Ë°®Á§∫', 'pinyin': 'bi«éo sh√¨', 'trans': 'represent'}, {'word': 'ÊúâÈôê', 'pinyin': 'y«íu xi√†n', 'trans': 'limited'}, {'word': 'Ê†áËÆ∞', 'pinyin': 'biƒÅo j√¨', 'trans': 'mark'}, {'word': 'ËÅöÂêà', 'pinyin': 'j√π h√©', 'trans': 'aggregate'}, {'word': 'Á©∫Èó¥', 'pinyin': 'k≈çng jiƒÅn', 'trans': 'spatial'}, {'word': 'ËØ≠‰πâ', 'pinyin': 'y«î y√¨', 'trans': 'semantic'}, {'word': 'Áõ∏ÂÖ≥', 'pinyin': 'xiƒÅng guƒÅn', 'trans': 'related'}, {'word': 'Âå∫Âüü', 'pinyin': 'q≈´ y√π', 'trans': 'region'}, {'word': '‰ø°ÊÅØ', 'pinyin': 'x√¨n xƒ´', 'trans': 'information'}, {'word': 'Ëß£ÂÜ≥', 'pinyin': 'jiƒõ ju√©', 'trans': 'solve'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ng ji√†', 'trans': 'framework'}, {'word': 'Ëß£ËÄ¶', 'pinyin': 'jiƒõ «íu', 'trans': 'decouple'}, {'word': 'Ëá™Ê≥®ÊÑè', 'pinyin': 'z√¨ zh√π y√¨', 'trans': 'self-attention'}, {'word': 'Ê®°Âùó', 'pinyin': 'm√≥ ku√†i', 'trans': 'module'}, {'word': 'Ëé∑Âæó', 'pinyin': 'hu√≤ d√©', 'trans': 'obtain'}, {'word': 'ÂÜÖÂÆπ', 'pinyin': 'n√®i r√≥ng', 'trans': 'content'}, {'word': '‰∏ä‰∏ãÊñá', 'pinyin': 'sh√†ng xi√† w√©n', 'trans': 'context'}, {'word': 'Ëæ®Âà´ÊÄß', 'pinyin': 'bi√†n bi√© x√¨ng', 'trans': 'discriminability'}, {'word': '‰∏ÄËá¥ÊÄß', 'pinyin': 'yƒ´ zh√¨ x√¨ng', 'trans': 'consistency'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ËØÅÊòé', 'pinyin': 'zh√®ng m√≠ng', 'trans': 'prove'}, {'word': '‰ºòÂºÇ', 'pinyin': 'y≈çu y√¨', 'trans': 'excellent'}]
[15.05.2025 23:11] Renaming previous Chinese page.
[15.05.2025 23:11] Renaming previous data. zh.html to ./d/2025-05-14_zh_reading_task.html
[15.05.2025 23:11] Writing Chinese reading task.
[15.05.2025 23:11] Writing result.
[15.05.2025 23:11] Renaming log file.
[15.05.2025 23:11] Renaming previous data. log.txt to ./logs/2025-05-15_last_log.txt
