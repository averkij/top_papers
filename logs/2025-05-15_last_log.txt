[15.05.2025 18:16] Read previous papers.
[15.05.2025 18:16] Generating top page (month).
[15.05.2025 18:16] Writing top page (month).
[15.05.2025 19:09] Read previous papers.
[15.05.2025 19:09] Get feed.
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09568
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04410
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09343
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09358
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08787
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07849
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12894
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09558
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09439
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08455
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04793
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08084
[15.05.2025 19:09] Extract page data from URL. URL: https://huggingface.co/papers/2505.09608
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08910
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06356
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.05587
[15.05.2025 19:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.05.2025 19:09] No deleted papers detected.
[15.05.2025 19:09] Downloading and parsing papers (pdf, html). Total: 16.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.09568.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.09568.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.09568.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.04410.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.04410.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.04410.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.09343.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.09343.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.09343.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.09358.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.09358.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.09358.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.08787.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.08787.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.08787.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.07849.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.07849.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.07849.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2502.12894.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2502.12894.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2502.12894.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.09558.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.09558.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.09558.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.09439.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.09439.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.09439.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.08455.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.08455.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.08455.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.04793.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.04793.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.04793.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.08084.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.08084.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.08084.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.09608.
[15.05.2025 19:09] Downloading paper 2505.09608 from http://arxiv.org/pdf/2505.09608v1...
[15.05.2025 19:09] Extracting affiliations from text.
[15.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 1 ] . [ 1 8 0 6 9 0 . 5 0 5 2 : r LightLab: Controlling Light Sources in Images with Diffusion Models NADAV MAGAR, Tel Aviv University and Google, Israel AMIR HERTZ, Google, United States of America ERIC TABELLION, Google, United States of America YAEL PRITCH, Google, Israel ALEX RAV-ACHA, Google, Israel ARIEL SHAMIR, Reichman University and Google, Israel YEDID HOSHEN, Hebrew University of Jerusalem and Google, Israel Fig. 1. Light editing results of LightLab. Our method enables explicit parametric control over light sources in an image, while producing physically plausible shadows and environmental effects (top). The method can manipulate the intensity of light sources, change their color, and adjust ambient illumination (middle). LightLab can be used for intricate sequential editing of lighting in images (bottom from left to right): starting with an input image, turning off even outside light coming from window, turning off an inside light source, turning on another light source, and changing lights colors. Note how reflections and shadows are plausibly handled in all these cases (please zoom in for better view and examine the tabletop). We present simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes diffusion model on small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for the relighting task. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method ca"
[15.05.2025 19:09] Response: ```python
[
    "Tel Aviv University and Google, Israel",
    "Google, United States of America",
    "Google, United States of America",
    "Google, Israel",
    "Google, Israel",
    "Reichman University and Google, Israel",
    "Hebrew University of Jerusalem and Google, Israel"
]
```
[15.05.2025 19:09] Deleting PDF ./assets/pdf/2505.09608.pdf.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.08910.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.08910.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.08910.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.06356.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.06356.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.06356.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.05587.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.05587.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.05587.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Enriching papers with extra data.
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 0. Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain und...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 1. Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct applicatio...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 2. The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware mo...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 3. The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning h...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 4. Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appeara...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 5. Software issue localization, the task of identifying the precise code locations (files, classes, or functions) relevant to a natural language issue description (e.g., bug report, feature request), is a critical yet time-consuming aspect of software development. While recent LLM-based agentic approac...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 6. Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 7. End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models' conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of no...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 8. We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, s...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 9. Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven ...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 10. Person reidentification (ReID) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, s...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 11. Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing task...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 12. We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our m...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 13. In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we intr...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 14. Pretraining datasets are foundational to the development of multimodal models, yet they often have inherent biases and toxic content from the web-scale corpora they are sourced from. In this paper, we investigate the prevalence of toxicity in LLaVA image-text pretraining dataset, examining how harmf...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 15. 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capt...
[15.05.2025 19:09] Read previous papers.
[15.05.2025 19:09] Generating reviews via LLM API.
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#diffusion", "#open_source", "#multimodal", "#training", "#architecture"], "emoji": "🖼️", "ru": {"title": "Объединение понимания и генерации изображений с помощью диффузионных трансформеров", "desc": "Статья представляет новый подход к объединению понимания и генерации и
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#cv", "#optimization", "#open_source"], "emoji": "🔍", "ru": {"title": "DeCLIP: Новый шаг к универсальному компьютерному зрению", "desc": "Статья представляет новый подход DeCLIP для улучшения возможностей моделей компьютерного зрения в задачах плотног
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization"], "emoji": "🧠", "ru": {"title": "Совместное проектирование моделей и оборудования для масштабирования ИИ", "desc": "Статья описывает архитектуру модели DeepSeek-V3/R1 и инфраструктуру ИИ, разработанные для преодоления ограни
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#cv", "#dataset", "#diffusion", "#synthetic", "#transfer_learning", "#training"], "emoji": "🌼", "ru": {"title": "Marigold: Раскрытие потенциала генеративных моделей для плотного анализа изображений", "desc": "Статья представляет Marigold - семейство условных генеративных моделей и п
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#video", "#transfer_learning", "#dataset", "#robotics", "#agents"], "emoji": "🤖", "ru": {"title": "UniSkill: Обучение роботов человеческим навыкам без разметки", "desc": "В статье представлен UniSkill - новый фреймворк для обучения роботов навыкам на основе видео с людьми. Он создае
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#data", "#benchmark", "#optimization", "#dataset", "#survey", "#agents"], "emoji": "🔍", "ru": {"title": "SweRank: эффективная локализация проблем в коде с помощью извлечения и переранжирования", "desc": "SweRank - это эффективная система для локализации проблем в программном обеспеч
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#3d", "#graphs", "#robotics", "#optimization"], "emoji": "🏙️", "ru": {"title": "Реконструкция 3D-сцен с учетом компонентов и физики из одного изображения", "desc": "CAST - это новый метод реконструкции 3D-сцен из одного RGB-изображения. Он использует сегментацию объектов, анализ про
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#audio", "#reasoning", "#open_source", "#rlhf", "#dataset", "#benchmark"], "emoji": "🎙️", "ru": {"title": "WavReward: прорыв в оценке разговорных ИИ-систем", "desc": "Статья представляет WavReward - модель оценки разговорных систем на основе аудио. Эта модель способна анализировать 
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#multimodal", "#rl", "#reasoning", "#optimization", "#benchmark", "#rag"], "emoji": "🎧", "ru": {"title": "Революция в аудио-ИИ: Omni-R1 покоряет новые вершины мультимодального анализа", "desc": "Исследователи представили Omni-R1 - мультимодальную языковую модель, улучше
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#long_context", "#video"], "emoji": "🎬", "ru": {"title": "Новый бенчмарк для оценки причинно-следственного рассуждения в видео-языковых моделях", "desc": "Исследователи представили новый бенчмарк VCRBench для оценки способностей Больших Видео-Языковых Мод
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#cv", "#dataset", "#synthetic"], "emoji": "🎯", "ru": {"title": "DetReIDX: Стресс-тест для реидентификации людей в реальном мире", "desc": "Статья представляет DetReIDX - крупномасштабный набор данных для задачи реидентификации людей в воздушно-наземных условиях. Набор данных включае
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#benchmark", "#dataset", "#reasoning", "#multimodal", "#interpretability", "#cv"], "emoji": "🧠", "ru": {"title": "VISTAR: Интерпретируемые рассуждения в мультимодальных моделях", "desc": "Статья представляет VISTAR - модель для интерпретируемых рассуждений на основе под
[15.05.2025 19:09] Querying the API.
[15.05.2025 19:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference.
[15.05.2025 19:09] Response: {
  "desc": "Авторы представляют новый метод на основе диффузии для точного контроля источников света на изображении. Модель дообучается на парах реальных фотографий и синтетических изображениях, используя фотореалистичные возможности диффузионных моделей. Метод позволяет точно контролировать интенсивность и цвет освещения. По результатам пользовательских оценок, он превосходит существующие подходы к редактированию освещения.",
  "emoji": "💡",
  "title": "Прецизионное управление освещением с помощью диффузионных моделей"
}
[15.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference."

[15.05.2025 19:09] Response: ```python
['DATASET', 'DATA', 'TRAINING', 'CV']
```
[15.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference."

[15.05.2025 19:09] Response: ```python
['DIFFUSION', 'SYNTHETIC']
```
[15.05.2025 19:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a diffusion-based technique for precise control of lighting in images. Unlike traditional methods that require multiple views or lack control over lighting adjustments, this approach fine-tunes a diffusion model using a small set of real photographs and additional synthetic images. By exploiting the linear properties of light, the method generates image pairs that reflect specific changes in light sources or ambient illumination. The results demonstrate superior performance in light editing, as preferred by users compared to existing techniques.","title":"Mastering Light: Fine-Grained Control with Diffusion Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a diffusion-based technique for precise control of lighting in images. Unlike traditional methods that require multiple views or lack control over lighting adjustments, this approach fine-tunes a diffusion model using a small set of real photographs and additional synthetic images. By exploiting the linear properties of light, the method generates image pairs that reflect specific changes in light sources or ambient illumination. The results demonstrate superior performance in light editing, as preferred by users compared to existing techniques.', title='Mastering Light: Fine-Grained Control with Diffusion Models'))
[15.05.2025 19:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"我们提出了一种简单而有效的基于扩散的方法，用于对图像中的光源进行细粒度的参数控制。现有的重光照方法要么依赖多个输入视图进行逆向渲染，要么无法提供对光变化的明确控制。我们通过在一小组真实原始照片对上微调扩散模型，并结合大规模合成渲染图像，来引导其在重光照方面的真实感先验。最终，我们展示了该方法在光编辑结果上的优越性，超越了基于用户偏好的现有方法。","title":"精确控制光源的扩散重光照方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='我们提出了一种简单而有效的基于扩散的方法，用于对图像中的光源进行细粒度的参数控制。现有的重光照方法要么依赖多个输入视图进行逆向渲染，要么无法提供对光变化的明确控制。我们通过在一小组真实原始照片对上微调扩散模型，并结合大规模合成渲染图像，来引导其在重光照方面的真实感先验。最终，我们展示了该方法在光编辑结果上的优越性，超越了基于用户偏好的现有方法。', title='精确控制光源的扩散重光照方法'))
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#multilingual", "#low_resource", "#dataset", "#cv", "#open_source"], "emoji": "🌍", "ru": {"title": "Maya: мультиязычная VLM для преодоления языковых и культурных барьеров", "desc": "Статья представляет Maya - открытую мультиязычную модель компьютерного зрения и обработки естественно
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#ethics", "#open_source", "#data"], "emoji": "🧹", "ru": {"title": "Очистка данных для этичного ИИ", "desc": "Исследователи изучили проблему токсичности в наборе данных LLaVA для предобучения мультимодальных моделей. Они провели анализ различных категорий т
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#inference", "#3d", "#optimization"], "emoji": "🔍", "ru": {"title": "Оптимизация 3D Gaussian Splatting: меньше точек, выше эффективность", "desc": "Статья представляет теоретическую основу для улучшения контроля плотности в методе 3D Gaussian Splatting (3DGS). Авторы анализируют про
[15.05.2025 19:09] Loading Chinese text from previous data.
[15.05.2025 19:09] Renaming data file.
[15.05.2025 19:09] Renaming previous data. hf_papers.json to ./d/2025-05-15.json
[15.05.2025 19:09] Saving new data file.
[15.05.2025 19:09] Generating page.
[15.05.2025 19:09] Renaming previous page.
[15.05.2025 19:09] Renaming previous data. index.html to ./d/2025-05-15.html
[15.05.2025 19:09] [Experimental] Generating Chinese page for reading.
[15.05.2025 19:09] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '预测', 'pinyin': 'yù cè', 'trans': 'predict'}, {'word': '受限', 'pinyin': 'shòu xiàn', 'trans': 'be limited'}, {'word': '预定义', 'pinyin': 'yù dìng yì', 'trans': 'predefined'}, {'word': '类别', 'pinyin': 'lèi bié', 'trans': 'category'}, {'word': '视觉-语言模型', 'pinyin': 'shì jué yǔ yán mó xíng', 'trans': 'vision-language model'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'}, {'word': '潜力', 'pinyin': 'qián lì', 'trans': 'potential'}, {'word': '密集', 'pinyin': 'mì jí', 'trans': 'dense'}, {'word': '局部', 'pinyin': 'jú bù', 'trans': 'local'}, {'word': '特征', 'pinyin': 'tè zhēng', 'trans': 'feature'}, {'word': '表示', 'pinyin': 'biǎo shì', 'trans': 'represent'}, {'word': '有限', 'pinyin': 'yǒu xiàn', 'trans': 'limited'}, {'word': '标记', 'pinyin': 'biāo jì', 'trans': 'mark'}, {'word': '聚合', 'pinyin': 'jù hé', 'trans': 'aggregate'}, {'word': '空间', 'pinyin': 'kōng jiān', 'trans': 'spatial'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'}, {'word': '相关', 'pinyin': 'xiāng guān', 'trans': 'related'}, {'word': '区域', 'pinyin': 'qū yù', 'trans': 'region'}, {'word': '信息', 'pinyin': 'xìn xī', 'trans': 'information'}, {'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '解耦', 'pinyin': 'jiě ǒu', 'trans': 'decouple'}, {'word': '自注意', 'pinyin': 'zì zhù yì', 'trans': 'self-attention'}, {'word': '模块', 'pinyin': 'mó kuài', 'trans': 'module'}, {'word': '获得', 'pinyin': 'huò dé', 'trans': 'obtain'}, {'word': '内容', 'pinyin': 'nèi róng', 'trans': 'content'}, {'word': '上下文', 'pinyin': 'shàng xià wén', 'trans': 'context'}, {'word': '辨别性', 'pinyin': 'biàn bié xìng', 'trans': 'discriminability'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '证明', 'pinyin': 'zhèng míng', 'trans': 'prove'}, {'word': '优异', 'pinyin': 'yōu yì', 'trans': 'excellent'}]
[15.05.2025 19:09] Renaming previous Chinese page.
[15.05.2025 19:09] Renaming previous data. zh.html to ./d/2025-05-14_zh_reading_task.html
[15.05.2025 19:09] Writing Chinese reading task.
[15.05.2025 19:09] Writing result.
[15.05.2025 19:09] Renaming log file.
[15.05.2025 19:09] Renaming previous data. log.txt to ./logs/2025-05-15_last_log.txt
