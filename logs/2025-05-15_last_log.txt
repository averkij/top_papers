[15.05.2025 13:24] Read previous papers.
[15.05.2025 13:24] Generating top page (month).
[15.05.2025 13:24] Writing top page (month).
[15.05.2025 14:12] Read previous papers.
[15.05.2025 14:12] Get feed.
[15.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04410
[15.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09568
[15.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09343
[15.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09358
[15.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08787
[15.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07849
[15.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08455
[15.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12894
[15.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09439
[15.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08084
[15.05.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04793
[15.05.2025 14:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.05.2025 14:12] No deleted papers detected.
[15.05.2025 14:12] Downloading and parsing papers (pdf, html). Total: 11.
[15.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.04410.
[15.05.2025 14:12] Extra JSON file exists (./assets/json/2505.04410.json), skip PDF parsing.
[15.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.04410.json), skip HTML parsing.
[15.05.2025 14:12] Success.
[15.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.09568.
[15.05.2025 14:12] Extra JSON file exists (./assets/json/2505.09568.json), skip PDF parsing.
[15.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.09568.json), skip HTML parsing.
[15.05.2025 14:12] Success.
[15.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.09343.
[15.05.2025 14:12] Extra JSON file exists (./assets/json/2505.09343.json), skip PDF parsing.
[15.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.09343.json), skip HTML parsing.
[15.05.2025 14:12] Success.
[15.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.09358.
[15.05.2025 14:12] Extra JSON file exists (./assets/json/2505.09358.json), skip PDF parsing.
[15.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.09358.json), skip HTML parsing.
[15.05.2025 14:12] Success.
[15.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.08787.
[15.05.2025 14:12] Downloading paper 2505.08787 from http://arxiv.org/pdf/2505.08787v1...
[15.05.2025 14:12] Extracting affiliations from text.
[15.05.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 1 ] . [ 1 7 8 7 8 0 . 5 0 5 2 : r UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations Hanjung Kim Jaehyun Kang Seon Joo Kim Youngwoon Lee Hyolim Kang Meedeum Cho https://kimhanjung.github.io/UniSkill Abstract: Mimicry is fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts. The project website can be found at: https://kimhanjung.github.io/UniSkill. Keywords: Learning from Videos, Skill Representations Learning from human videos has emerged as central paradigm in robot learning, offering scalable approach to the scarcity of robot-specific data by leveraging large, diverse video sources. Human videos contain everyday behaviors such as human-object interactions, which could provide rich source of skills for robot learning. Here, central question arises: Can robots acquire crossembodiment skill representations by watching large-scale human demonstrations? Translating human videos into robot-executable skill representations has traditionally relied on paired human-robot datasets [1, 2, 3] or predefined semantic skill labels ["
[15.05.2025 14:12] Response: ```python
[]
```
[15.05.2025 14:12] Extracting affiliations from text.
[15.05.2025 14:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 1 ] . [ 1 7 8 7 8 0 . 5 0 5 2 : r UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations Hanjung Kim Jaehyun Kang Seon Joo Kim Youngwoon Lee Hyolim Kang Meedeum Chohttps://kimhanjung.github.io/UniSkill Abstract: Mimicry is fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts. The project website can be found at: https://kimhanjung.github.io/UniSkill. Keywords: Learning from Videos, Skill RepresentationsLearning from human videos has emerged as central paradigm in robot learning, offering scalable approach to the scarcity of robot-specific data by leveraging large, diverse video sources. Human videos contain everyday behaviors such as human-object interactions, which could provide rich source of skills for robot learning. Here, central question arises: Can robots acquire crossembodiment skill representations by watching large-scale human demonstrations? Translating human videos into robot-executable skill representations has traditionally relied on paired human-robot datasets [1, 2, 3] or predefined semantic skill labels [4, 5], both of which are difficult to scale. Recent approaches aim to bypass these requirements by learning cross-embodiment skill representations without explicit pairing or labeling [6, 7, 8, 9, 10]. However, these methods still impose constraints on data collection, such as multi-view camera setups, and task and scene alignment between human and robot demonstrations, which limit their scalability and applicability to real-world, in-the-wild human videos. To this end, we propose Universal Skill representations (UniSkill), scalable approach for learning cross-embodiment skill representations from large-scale in-the-wild video data so that robot can translate an unseen human demonstration into sequence of robot-executable skill representations, as illustrated in Figure 1. To extract reusable, embodiment-agnostic motion patterns from videos, UniSkill focuses on capturing dynamics changes between temporally distant video frames, which denotes equal contributions. Figure 1: Universal Skill representations (UniSkill) are cross-embodiment skill representations shared across various embodiments (e.g., humans, Franka, WidowX) trained from both human and robot videos via skill dynamics modeling. Unlike prior works that require additional supervision (e.g., trajectory labels) or alignment between human and robot videos, UniSkill removes these constraints by learning solely from off-the-shelf video datasetssuch as Something-Something V2 [11] and H2O [12] for human videos, and DROID [13], Bridge V2 [14], and LIBERO [15] for robot videos. UniSkill, trained on large-scale cross-embodiment videos, learns an embodiment-agnostic skill representation that enables interpreting human videos as skill sequences executable directly through skill-conditioned policy. can be agnostic to embodiments and shared across diverse videos. UniSkill leverages an imageediting pipeline, which naturally emphasizes dynamic regions over static content, and encodes the resulting motion patterns into skill representations. The design choice enables the use of arbitrary, embodiment-agnostic video datasets for training, making it possible to scale cross-embodiment skill representation learning to large, in-the-wild datasets. As result of its embodiment-agnostic skill representation, UniSkill can imitate given prompt video by capturing sequence of motion patterns within it, even when demonstration is performed by human. Our experiments demonstrate that UniSkill effectively learn cross-embodiment skill representations by training on large-scale video datasets. Its embodiment-agnostic design allows it to generalize to unseen human prompts at test time, without any kind of additional guidance such as language instructions. Notably, UniSkills skill-centric architecture enhances robustness to novel objects and supports compositional task solving. In addition, its versatile training pipeline benefits from incorporating diverse video datasets, with performance improving as more data sources are added. Finally, qualitative results from the Forward Skill Dynamics (FSD) model predictions and skill representation visualizations highlight the interpretability of the learned representations. In summary, our contributions are twofold: We introduce UniSkill, universal skill representation learning approach that enables the use of large-scale video data by removing the need for labels or any form of alignment constraints. UniSkill shows effective human-to-robot and robot-to-robot imitation in both simulation and real-world experiments through its embodiment-agnostic skill representation.Learning action (or skill) representations for robot learning from in-the-wild video dataset is challenging due to the absence of action labels. Recent work on latent action models addresses this by deriving action-relevant information through inverse or forward dynamics models. LAPO [16] and Genie [17] propose to learn generative interactive environments from gameplay videos with latent actions, but they are primarily tailored to game settings with discrete actions. LAPA [18] extends this line of research to real-world robotic manipulation by incorporating diverse videos, including 2 Figure 2: The overview of UniSkill. (a) Inverse Skill Dynamics (ISD) and Forward Skill Dynamics (FSD) are jointly trained on diverse video datasets to encode dynamics information into universal skill representations by predicting skills and future frames, respectively. (b) universal skillconditioned policy is train"
[15.05.2025 14:12] Mistral response. {"id": "62070a1024cc47bcad20345e1deeb058", "object": "chat.completion", "created": 1747318353, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1486, "total_tokens": 1488, "completion_tokens": 2}}
[15.05.2025 14:12] Response: []
[15.05.2025 14:12] Deleting PDF ./assets/pdf/2505.08787.pdf.
[15.05.2025 14:12] Success.
[15.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.07849.
[15.05.2025 14:12] Extra JSON file exists (./assets/json/2505.07849.json), skip PDF parsing.
[15.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.07849.json), skip HTML parsing.
[15.05.2025 14:12] Success.
[15.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.08455.
[15.05.2025 14:12] Extra JSON file exists (./assets/json/2505.08455.json), skip PDF parsing.
[15.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.08455.json), skip HTML parsing.
[15.05.2025 14:12] Success.
[15.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2502.12894.
[15.05.2025 14:12] Extra JSON file exists (./assets/json/2502.12894.json), skip PDF parsing.
[15.05.2025 14:12] Paper image links file exists (./assets/img_data/2502.12894.json), skip HTML parsing.
[15.05.2025 14:12] Success.
[15.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.09439.
[15.05.2025 14:12] Extra JSON file exists (./assets/json/2505.09439.json), skip PDF parsing.
[15.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.09439.json), skip HTML parsing.
[15.05.2025 14:12] Success.
[15.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.08084.
[15.05.2025 14:12] Extra JSON file exists (./assets/json/2505.08084.json), skip PDF parsing.
[15.05.2025 14:12] Paper image links file exists (./assets/img_data/2505.08084.json), skip HTML parsing.
[15.05.2025 14:12] Success.
[15.05.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2505.04793.
[15.05.2025 14:12] Downloading paper 2505.04793 from http://arxiv.org/pdf/2505.04793v1...
[15.05.2025 14:12] Extracting affiliations from text.
[15.05.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SUBMITTED TO IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE 1 DetReIDX: Stress-Test Dataset for Real-World UAV-Based Person Kailash A. Hambarde, Nzakiese Mbongo, Pavan Kumar MP, Satish Mekewad, Carolina Fernandes, Gokhan Silahtaroglu, Alice Nithya, Pawan Wasnik, MD. Rashidunnabi, Pranita Samale, Hugo Proenca Senior Member, IEEE 5 2 0 2 7 ] . [ 1 3 9 7 4 0 . 5 0 5 2 : r AbstractPerson reidentification (ReID) technology has been considered to perform relatively well under controlled, groundlevel conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, scale variations, occlusions, and appearance shifts from clothing or session drifts. Moreover, the publicly available data sets do not realistically incorporate such kinds and magnitudes of variability, which limits the progress of this technology. This paper introduces DetReIDX, large-scale aerial-ground person dataset, that was explicitly designed as stress test to ReID under real-world conditions. DetReIDX is multi-session set that includes over 13 million bounding boxes from 509 identities, collected in seven university campuses from three continents, with drone altitudes between 5.8 and 120 meters. More important, as key novelty, DetReIDX subjects were recorded in (at least) two sessions on different days, with changes in clothing, daylight and location, making it suitable to actually evaluate long-term person ReID. Plus, data were annotated from 16 soft biometric attributes and multitask labels for detection, tracking, ReID, and action recognition. In order to provide empirical evidence of DetReIDX usefulness, we considered the specific tasks of human detection and ReID, where SOTA methods catastrophically degrade performance (up to 80% in detection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs conditions. The dataset, annotations, and official evaluation proto"
[15.05.2025 14:12] Response: ```python
[]
```
[15.05.2025 14:12] Extracting affiliations from text.
[15.05.2025 14:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SUBMITTED TO IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE 1 DetReIDX: Stress-Test Dataset for Real-World UAV-Based PersonKailash A. Hambarde, Nzakiese Mbongo, Pavan Kumar MP, Satish Mekewad, Carolina Fernandes, Gokhan Silahtaroglu, Alice Nithya, Pawan Wasnik, MD. Rashidunnabi, Pranita Samale, Hugo Proenca Senior Member, IEEE 5 2 0 2 7 ] . [ 1 3 9 7 4 0 . 5 0 5 2 : r AbstractPerson reidentification (ReID) technology has been considered to perform relatively well under controlled, groundlevel conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, scale variations, occlusions, and appearance shifts from clothing or session drifts. Moreover, the publicly available data sets do not realistically incorporate such kinds and magnitudes of variability, which limits the progress of this technology. This paper introduces DetReIDX, large-scale aerial-ground person dataset, that was explicitly designed as stress test to ReID under real-world conditions. DetReIDX is multi-session set that includes over 13 million bounding boxes from 509 identities, collected in seven university campuses from three continents, with drone altitudes between 5.8 and 120 meters. More important, as key novelty, DetReIDX subjects were recorded in (at least) two sessions on different days, with changes in clothing, daylight and location, making it suitable to actually evaluate long-term person ReID. Plus, data were annotated from 16 soft biometric attributes and multitask labels for detection, tracking, ReID, and action recognition. In order to provide empirical evidence of DetReIDX usefulness, we considered the specific tasks of human detection and ReID, where SOTA methods catastrophically degrade performance (up to 80% in detection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs conditions. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/. Index TermsPerson Re-Identification, UAV Surveillance, Cross-View Recognition, Aerial-Ground Dataset, Soft Biometrics. I. Introduction identification, ERSON centric visual understanding including detection, tracking, and re-identification (ReID) is foundational to wide range of critical applications such as surveillance, public safety, autonomous UAV patrolling, and the desearch-and-rescue operations [19][21][?]. However, ployment of such systems in unconstrained aerial-ground environments remains extremely limited. The core bottleneck is not Manuscript received February XX, 2025; revised XX XX, 2025. This work was supported. Kailash A. Hambarde, Nzakiese Mbongo, Carolina Fernandes, MD. Rashidunnabi, Pranita Samale, and Hugo Proenca are with the Instituto de Telecomunicac oes and the University of Beira Interior, Covilha, Portugal (corresponding author e-mail: kailas.srt@gmail.com). Pavan Kumar MP is with J.N.N. College of Engineering, Shivamogga, Karnataka, India. Satish Mekewad and Pawan Wasnik are with the School of Computational Sciences, SRTM University, Nanded, India. Gokhan Silahtaroglu is with Istanbul Medipol University, Istanbul, Turkey. Alice Nithya is with SRM Institute of Science and Technology, Kattankulathur, India. Fig. 1. Comparison between the most important features of the publicly available datasets (ground-ground, aerial-aerial, and aerial-ground) and the DetReIDX dataset. Unlike its counterparts, DetReIDX includes clothing variations within subjects, with detection and tracking annotations, action labels, at wide altitude ranges (5.8m120m). model capacity but rather the lack of datasets that reflect the true operational complexity of drone-based surveillance: low resolution, cross-viewpoint domain gaps, long-range degradation, and appearance shifts due to clothing or occlusion. Despite impressive progress in ground-level person ReID using datasets like Market-1501 [1], CUHK03 [2], MARS [3], DukeMTMC-ReID [4], and LTCC [5], these benchmarks are largely constrained to fixed-camera, close-range, lateral-view scenarios. While they have catalyzed algorithmic advances, they fail to capture the severe viewpoint and scale variations encountered in aerial settings. On the other hand, aerial-only datasets such as PDESTRE [6], UAV-Human [7], PRID-2011 [8], MRP [9], PRAI-1581 [10], Mini-drone [11], AVI [12], and DRoneHIT [13] offer aerial captures but are limited to relatively low altitudes (<10m), lack multi-session diversity, or exclude ground-view perspectives, thus limiting their value for crossview understanding and realistic tracking tasks. Bridging the aerial-ground domain remains vastly underexplored. Notable attempts include AG-ReID.v2 [14], G2APS [15], CSM [16], and iQIYI-VID [17], which introduce hybrid viewpoints. Yet, these datasets suffer from narrow altitude ranges (typically <45m), limited clothing variation, and lack fine-grained annotations necessary for robust multi-task learning. The gap: Existing datasets either (i) operate in narrow altitude domains, (ii) fail to support cross-view matching, (iii) lack 00000000/00$00.00 2021 IEEE SUBMITTED TO IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE 2 TABLE Comparison between DetReIDX and the publicly available datasets for person detection, ReID, tracking, and action recognition. (: Available, : Not available, : No information available.) Category Dataset Camera Format o - o a A - r d r - r CUHK03 [2] iLIDS-VID [23] Market-1501 [1] MARS [3] DukeMTMC-ReID [4] LTCC [5] PRID-2011 [8] MRP [9] PRAI-1581 [10] Mini-drone [11] AVI [12] DRone-HIT [13] P-DESTRE [6] UAV-Human [7] CSM [16] iQIYI-VID [17] AG-ReID.v2 [14] G2APS-ReID [7] DetReIDX (Ours) CCTV CCTV CCTV CCTV CCTV CCTV UAV UAV UAV UAV UAV UAV UAV UAV Various Various UAV+CCTV UAV+CCTV DSLR+UAV Still Video Still Video Video Still Still Video Still Video Still Still Video Still Video Video Still Still Video+Still Detection Tracking Task ReID Search Action Rec. #Identities #BBox Height (m) Distance (m) 1467 300 1501 1261 1812 152 1581 28 1581 5124 101 269 1144 1218 5000 1615 2788 509 13K 42K 32.6K 20K 815K 17K 40K 4K 39K >27K 10K 40K >14.8M 41K 11M 600K 100.6K 200.8K 12.6M <10 2060 <10 2060 <10 28 5.86.7 28 1545 2060 5120 10 annotation density and appearance variation to evaluate longterm recognition, or (iv) omit long-term identity retention under clothing changes across sessions. Most benchmarks assume fixed attire and "
[15.05.2025 14:12] Mistral response. {"id": "b4c6b729c33d4b1baea1078a828f99bf", "object": "chat.completion", "created": 1747318357, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['Instituto de Telecomunicacoes and the University of Beira Interior, Covilha, Portugal', 'J.N.N. College of Engineering, Shivamogga, Karnataka, India', 'School of Computational Sciences, SRTM University, Nanded, India', 'Istanbul Medipol University, Istanbul, Turkey', 'SRM Institute of Science and Technology, Kattankulathur, India']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 2099, "total_tokens": 2205, "completion_tokens": 106}}
[15.05.2025 14:12] Response: ```python
['Instituto de Telecomunicacoes and the University of Beira Interior, Covilha, Portugal', 'J.N.N. College of Engineering, Shivamogga, Karnataka, India', 'School of Computational Sciences, SRTM University, Nanded, India', 'Istanbul Medipol University, Istanbul, Turkey', 'SRM Institute of Science and Technology, Kattankulathur, India']
```
[15.05.2025 14:12] Deleting PDF ./assets/pdf/2505.04793.pdf.
[15.05.2025 14:12] Success.
[15.05.2025 14:12] Enriching papers with extra data.
[15.05.2025 14:12] ********************************************************************************
[15.05.2025 14:12] Abstract 0. Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct applicatio...
[15.05.2025 14:12] ********************************************************************************
[15.05.2025 14:12] Abstract 1. Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain und...
[15.05.2025 14:12] ********************************************************************************
[15.05.2025 14:12] Abstract 2. The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware mo...
[15.05.2025 14:12] ********************************************************************************
[15.05.2025 14:12] Abstract 3. The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning h...
[15.05.2025 14:12] ********************************************************************************
[15.05.2025 14:12] Abstract 4. Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appeara...
[15.05.2025 14:12] ********************************************************************************
[15.05.2025 14:12] Abstract 5. Software issue localization, the task of identifying the precise code locations (files, classes, or functions) relevant to a natural language issue description (e.g., bug report, feature request), is a critical yet time-consuming aspect of software development. While recent LLM-based agentic approac...
[15.05.2025 14:12] ********************************************************************************
[15.05.2025 14:12] Abstract 6. Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven ...
[15.05.2025 14:12] ********************************************************************************
[15.05.2025 14:12] Abstract 7. Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image...
[15.05.2025 14:12] ********************************************************************************
[15.05.2025 14:12] Abstract 8. We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, s...
[15.05.2025 14:12] ********************************************************************************
[15.05.2025 14:12] Abstract 9. Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing task...
[15.05.2025 14:12] ********************************************************************************
[15.05.2025 14:12] Abstract 10. Person reidentification (ReID) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, s...
[15.05.2025 14:12] Read previous papers.
[15.05.2025 14:12] Generating reviews via LLM API.
[15.05.2025 14:12] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#cv", "#optimization", "#open_source"], "emoji": "🔍", "ru": {"title": "DeCLIP: Новый шаг к универсальному компьютерному зрению", "desc": "Статья представляет новый подход DeCLIP для улучшения возможностей моделей компьютерного зрения в задачах плотног
[15.05.2025 14:12] Using data from previous issue: {"categories": ["#dataset", "#diffusion", "#open_source", "#multimodal", "#training", "#architecture"], "emoji": "🖼️", "ru": {"title": "Объединение понимания и генерации изображений с помощью диффузионных трансформеров", "desc": "Статья представляет новый подход к объединению понимания и генерации и
[15.05.2025 14:12] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization"], "emoji": "🧠", "ru": {"title": "Совместное проектирование моделей и оборудования для масштабирования ИИ", "desc": "Статья описывает архитектуру модели DeepSeek-V3/R1 и инфраструктуру ИИ, разработанные для преодоления ограни
[15.05.2025 14:12] Using data from previous issue: {"categories": ["#cv", "#dataset", "#diffusion", "#synthetic", "#transfer_learning", "#training"], "emoji": "🌼", "ru": {"title": "Marigold: Раскрытие потенциала генеративных моделей для плотного анализа изображений", "desc": "Статья представляет Marigold - семейство условных генеративных моделей и п
[15.05.2025 14:12] Using data from previous issue: {"categories": ["#video", "#transfer_learning", "#dataset", "#robotics", "#agents"], "emoji": "🤖", "ru": {"title": "UniSkill: Обучение роботов человеческим навыкам без разметки", "desc": "В статье представлен UniSkill - новый фреймворк для обучения роботов навыкам на основе видео с людьми. Он создае
[15.05.2025 14:12] Using data from previous issue: {"categories": ["#data", "#benchmark", "#optimization", "#dataset", "#survey", "#agents"], "emoji": "🔍", "ru": {"title": "SweRank: эффективная локализация проблем в коде с помощью извлечения и переранжирования", "desc": "SweRank - это эффективная система для локализации проблем в программном обеспеч
[15.05.2025 14:12] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#long_context", "#video"], "emoji": "🎬", "ru": {"title": "Новый бенчмарк для оценки причинно-следственного рассуждения в видео-языковых моделях", "desc": "Исследователи представили новый бенчмарк VCRBench для оценки способностей Больших Видео-Языковых Мод
[15.05.2025 14:12] Using data from previous issue: {"categories": ["#3d", "#graphs", "#robotics", "#optimization"], "emoji": "🏙️", "ru": {"title": "Реконструкция 3D-сцен с учетом компонентов и физики из одного изображения", "desc": "CAST - это новый метод реконструкции 3D-сцен из одного RGB-изображения. Он использует сегментацию объектов, анализ про
[15.05.2025 14:12] Using data from previous issue: {"categories": ["#training", "#multimodal", "#rl", "#reasoning", "#optimization", "#benchmark", "#rag"], "emoji": "🎧", "ru": {"title": "Революция в аудио-ИИ: Omni-R1 покоряет новые вершины мультимодального анализа", "desc": "Исследователи представили Omni-R1 - мультимодальную языковую модель, улучше
[15.05.2025 14:12] Using data from previous issue: {"categories": ["#training", "#benchmark", "#dataset", "#reasoning", "#multimodal", "#interpretability", "#cv"], "emoji": "🧠", "ru": {"title": "VISTAR: Интерпретируемые рассуждения в мультимодальных моделях", "desc": "Статья представляет VISTAR - модель для интерпретируемых рассуждений на основе под
[15.05.2025 14:12] Using data from previous issue: {"categories": ["#cv", "#dataset", "#synthetic"], "emoji": "🎯", "ru": {"title": "DetReIDX: Стресс-тест для реидентификации людей в реальном мире", "desc": "Статья представляет DetReIDX - крупномасштабный набор данных для задачи реидентификации людей в воздушно-наземных условиях. Набор данных включае
[15.05.2025 14:12] Loading Chinese text from previous data.
[15.05.2025 14:12] Renaming data file.
[15.05.2025 14:12] Renaming previous data. hf_papers.json to ./d/2025-05-15.json
[15.05.2025 14:12] Saving new data file.
[15.05.2025 14:12] Generating page.
[15.05.2025 14:12] Renaming previous page.
[15.05.2025 14:12] Renaming previous data. index.html to ./d/2025-05-15.html
[15.05.2025 14:12] [Experimental] Generating Chinese page for reading.
[15.05.2025 14:12] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '预测', 'pinyin': 'yù cè', 'trans': 'predict'}, {'word': '受限', 'pinyin': 'shòu xiàn', 'trans': 'be limited'}, {'word': '预定义', 'pinyin': 'yù dìng yì', 'trans': 'predefined'}, {'word': '类别', 'pinyin': 'lèi bié', 'trans': 'category'}, {'word': '视觉-语言模型', 'pinyin': 'shì jué yǔ yán mó xíng', 'trans': 'vision-language model'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'}, {'word': '潜力', 'pinyin': 'qián lì', 'trans': 'potential'}, {'word': '密集', 'pinyin': 'mì jí', 'trans': 'dense'}, {'word': '局部', 'pinyin': 'jú bù', 'trans': 'local'}, {'word': '特征', 'pinyin': 'tè zhēng', 'trans': 'feature'}, {'word': '表示', 'pinyin': 'biǎo shì', 'trans': 'represent'}, {'word': '有限', 'pinyin': 'yǒu xiàn', 'trans': 'limited'}, {'word': '标记', 'pinyin': 'biāo jì', 'trans': 'mark'}, {'word': '聚合', 'pinyin': 'jù hé', 'trans': 'aggregate'}, {'word': '空间', 'pinyin': 'kōng jiān', 'trans': 'spatial'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'}, {'word': '相关', 'pinyin': 'xiāng guān', 'trans': 'related'}, {'word': '区域', 'pinyin': 'qū yù', 'trans': 'region'}, {'word': '信息', 'pinyin': 'xìn xī', 'trans': 'information'}, {'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '解耦', 'pinyin': 'jiě ǒu', 'trans': 'decouple'}, {'word': '自注意', 'pinyin': 'zì zhù yì', 'trans': 'self-attention'}, {'word': '模块', 'pinyin': 'mó kuài', 'trans': 'module'}, {'word': '获得', 'pinyin': 'huò dé', 'trans': 'obtain'}, {'word': '内容', 'pinyin': 'nèi róng', 'trans': 'content'}, {'word': '上下文', 'pinyin': 'shàng xià wén', 'trans': 'context'}, {'word': '辨别性', 'pinyin': 'biàn bié xìng', 'trans': 'discriminability'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '证明', 'pinyin': 'zhèng míng', 'trans': 'prove'}, {'word': '优异', 'pinyin': 'yōu yì', 'trans': 'excellent'}]
[15.05.2025 14:12] Renaming previous Chinese page.
[15.05.2025 14:12] Renaming previous data. zh.html to ./d/2025-05-14_zh_reading_task.html
[15.05.2025 14:12] Writing Chinese reading task.
[15.05.2025 14:12] Writing result.
[15.05.2025 14:12] Renaming log file.
[15.05.2025 14:12] Renaming previous data. log.txt to ./logs/2025-05-15_last_log.txt
