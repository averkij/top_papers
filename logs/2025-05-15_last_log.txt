[15.05.2025 18:16] Read previous papers.
[15.05.2025 18:16] Generating top page (month).
[15.05.2025 18:16] Writing top page (month).
[15.05.2025 19:09] Read previous papers.
[15.05.2025 19:09] Get feed.
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09568
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04410
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09343
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09358
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08787
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07849
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12894
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09558
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09439
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08455
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04793
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08084
[15.05.2025 19:09] Extract page data from URL. URL: https://huggingface.co/papers/2505.09608
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08910
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06356
[15.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.05587
[15.05.2025 19:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.05.2025 19:09] No deleted papers detected.
[15.05.2025 19:09] Downloading and parsing papers (pdf, html). Total: 16.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.09568.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.09568.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.09568.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.04410.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.04410.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.04410.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.09343.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.09343.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.09343.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.09358.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.09358.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.09358.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.08787.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.08787.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.08787.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.07849.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.07849.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.07849.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2502.12894.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2502.12894.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2502.12894.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.09558.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.09558.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.09558.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.09439.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.09439.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.09439.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.08455.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.08455.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.08455.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.04793.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.04793.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.04793.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.08084.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.08084.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.08084.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.09608.
[15.05.2025 19:09] Downloading paper 2505.09608 from http://arxiv.org/pdf/2505.09608v1...
[15.05.2025 19:09] Extracting affiliations from text.
[15.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 1 ] . [ 1 8 0 6 9 0 . 5 0 5 2 : r LightLab: Controlling Light Sources in Images with Diffusion Models NADAV MAGAR, Tel Aviv University and Google, Israel AMIR HERTZ, Google, United States of America ERIC TABELLION, Google, United States of America YAEL PRITCH, Google, Israel ALEX RAV-ACHA, Google, Israel ARIEL SHAMIR, Reichman University and Google, Israel YEDID HOSHEN, Hebrew University of Jerusalem and Google, Israel Fig. 1. Light editing results of LightLab. Our method enables explicit parametric control over light sources in an image, while producing physically plausible shadows and environmental effects (top). The method can manipulate the intensity of light sources, change their color, and adjust ambient illumination (middle). LightLab can be used for intricate sequential editing of lighting in images (bottom from left to right): starting with an input image, turning off even outside light coming from window, turning off an inside light source, turning on another light source, and changing lights colors. Note how reflections and shadows are plausibly handled in all these cases (please zoom in for better view and examine the tabletop). We present simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes diffusion model on small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for the relighting task. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method ca"
[15.05.2025 19:09] Response: ```python
[
    "Tel Aviv University and Google, Israel",
    "Google, United States of America",
    "Google, United States of America",
    "Google, Israel",
    "Google, Israel",
    "Reichman University and Google, Israel",
    "Hebrew University of Jerusalem and Google, Israel"
]
```
[15.05.2025 19:09] Deleting PDF ./assets/pdf/2505.09608.pdf.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.08910.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.08910.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.08910.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.06356.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.06356.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.06356.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.05587.
[15.05.2025 19:09] Extra JSON file exists (./assets/json/2505.05587.json), skip PDF parsing.
[15.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.05587.json), skip HTML parsing.
[15.05.2025 19:09] Success.
[15.05.2025 19:09] Enriching papers with extra data.
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 0. Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain und...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 1. Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct applicatio...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 2. The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware mo...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 3. The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning h...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 4. Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appeara...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 5. Software issue localization, the task of identifying the precise code locations (files, classes, or functions) relevant to a natural language issue description (e.g., bug report, feature request), is a critical yet time-consuming aspect of software development. While recent LLM-based agentic approac...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 6. Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 7. End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models' conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of no...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 8. We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, s...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 9. Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven ...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 10. Person reidentification (ReID) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, s...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 11. Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing task...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 12. We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our m...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 13. In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we intr...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 14. Pretraining datasets are foundational to the development of multimodal models, yet they often have inherent biases and toxic content from the web-scale corpora they are sourced from. In this paper, we investigate the prevalence of toxicity in LLaVA image-text pretraining dataset, examining how harmf...
[15.05.2025 19:09] ********************************************************************************
[15.05.2025 19:09] Abstract 15. 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capt...
[15.05.2025 19:09] Read previous papers.
[15.05.2025 19:09] Generating reviews via LLM API.
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#diffusion", "#open_source", "#multimodal", "#training", "#architecture"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#cv", "#optimization", "#open_source"], "emoji": "ğŸ”", "ru": {"title": "DeCLIP: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ DeCLIP Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DeepSeek-V3/R1 Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ˜Ğ˜, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#cv", "#dataset", "#diffusion", "#synthetic", "#transfer_learning", "#training"], "emoji": "ğŸŒ¼", "ru": {"title": "Marigold: Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Marigold - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#video", "#transfer_learning", "#dataset", "#robotics", "#agents"], "emoji": "ğŸ¤–", "ru": {"title": "UniSkill: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ UniSkill - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸. ĞĞ½ ÑĞ¾Ğ·Ğ´Ğ°Ğµ
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#data", "#benchmark", "#optimization", "#dataset", "#survey", "#agents"], "emoji": "ğŸ”", "ru": {"title": "SweRank: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² ĞºĞ¾Ğ´Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ", "desc": "SweRank - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#3d", "#graphs", "#robotics", "#optimization"], "emoji": "ğŸ™ï¸", "ru": {"title": "Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ", "desc": "CAST - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#audio", "#reasoning", "#open_source", "#rlhf", "#dataset", "#benchmark"], "emoji": "ğŸ™ï¸", "ru": {"title": "WavReward: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WavReward - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#multimodal", "#rl", "#reasoning", "#optimization", "#benchmark", "#rag"], "emoji": "ğŸ§", "ru": {"title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ˜Ğ˜: Omni-R1 Ğ¿Ğ¾ĞºĞ¾Ñ€ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑ€ÑˆĞ¸Ğ½Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Omni-R1 - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑƒĞ»ÑƒÑ‡ÑˆĞµ
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#long_context", "#video"], "emoji": "ğŸ¬", "ru": {"title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VCRBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ’Ğ¸Ğ´ĞµĞ¾-Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ĞœĞ¾Ğ´
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#cv", "#dataset", "#synthetic"], "emoji": "ğŸ¯", "ru": {"title": "DetReIDX: Ğ¡Ñ‚Ñ€ĞµÑÑ-Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ñ€ĞµĞ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DetReIDX - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€ĞµĞ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ğ²Ğ¾Ğ·Ğ´ÑƒÑˆĞ½Ğ¾-Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞºĞ»ÑÑ‡Ğ°Ğµ
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#benchmark", "#dataset", "#reasoning", "#multimodal", "#interpretability", "#cv"], "emoji": "ğŸ§ ", "ru": {"title": "VISTAR: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VISTAR - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´
[15.05.2025 19:09] Querying the API.
[15.05.2025 19:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference.
[15.05.2025 19:09] Response: {
  "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² ÑĞ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ†Ğ²ĞµÑ‚ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞŸĞ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº, Ğ¾Ğ½ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ.",
  "emoji": "ğŸ’¡",
  "title": "ĞŸÑ€ĞµÑ†Ğ¸Ğ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"
}
[15.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference."

[15.05.2025 19:09] Response: ```python
['DATASET', 'DATA', 'TRAINING', 'CV']
```
[15.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference."

[15.05.2025 19:09] Response: ```python
['DIFFUSION', 'SYNTHETIC']
```
[15.05.2025 19:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a diffusion-based technique for precise control of lighting in images. Unlike traditional methods that require multiple views or lack control over lighting adjustments, this approach fine-tunes a diffusion model using a small set of real photographs and additional synthetic images. By exploiting the linear properties of light, the method generates image pairs that reflect specific changes in light sources or ambient illumination. The results demonstrate superior performance in light editing, as preferred by users compared to existing techniques.","title":"Mastering Light: Fine-Grained Control with Diffusion Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a diffusion-based technique for precise control of lighting in images. Unlike traditional methods that require multiple views or lack control over lighting adjustments, this approach fine-tunes a diffusion model using a small set of real photographs and additional synthetic images. By exploiting the linear properties of light, the method generates image pairs that reflect specific changes in light sources or ambient illumination. The results demonstrate superior performance in light editing, as preferred by users compared to existing techniques.', title='Mastering Light: Fine-Grained Control with Diffusion Models'))
[15.05.2025 19:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œç”¨äºå¯¹å›¾åƒä¸­çš„å…‰æºè¿›è¡Œç»†ç²’åº¦çš„å‚æ•°æ§åˆ¶ã€‚ç°æœ‰çš„é‡å…‰ç…§æ–¹æ³•è¦ä¹ˆä¾èµ–å¤šä¸ªè¾“å…¥è§†å›¾è¿›è¡Œé€†å‘æ¸²æŸ“ï¼Œè¦ä¹ˆæ— æ³•æä¾›å¯¹å…‰å˜åŒ–çš„æ˜ç¡®æ§åˆ¶ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä¸€å°ç»„çœŸå®åŸå§‹ç…§ç‰‡å¯¹ä¸Šå¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶ç»“åˆå¤§è§„æ¨¡åˆæˆæ¸²æŸ“å›¾åƒï¼Œæ¥å¼•å¯¼å…¶åœ¨é‡å…‰ç…§æ–¹é¢çš„çœŸå®æ„Ÿå…ˆéªŒã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å…‰ç¼–è¾‘ç»“æœä¸Šçš„ä¼˜è¶Šæ€§ï¼Œè¶…è¶Šäº†åŸºäºç”¨æˆ·åå¥½çš„ç°æœ‰æ–¹æ³•ã€‚","title":"ç²¾ç¡®æ§åˆ¶å…‰æºçš„æ‰©æ•£é‡å…‰ç…§æ–¹æ³•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œç”¨äºå¯¹å›¾åƒä¸­çš„å…‰æºè¿›è¡Œç»†ç²’åº¦çš„å‚æ•°æ§åˆ¶ã€‚ç°æœ‰çš„é‡å…‰ç…§æ–¹æ³•è¦ä¹ˆä¾èµ–å¤šä¸ªè¾“å…¥è§†å›¾è¿›è¡Œé€†å‘æ¸²æŸ“ï¼Œè¦ä¹ˆæ— æ³•æä¾›å¯¹å…‰å˜åŒ–çš„æ˜ç¡®æ§åˆ¶ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä¸€å°ç»„çœŸå®åŸå§‹ç…§ç‰‡å¯¹ä¸Šå¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶ç»“åˆå¤§è§„æ¨¡åˆæˆæ¸²æŸ“å›¾åƒï¼Œæ¥å¼•å¯¼å…¶åœ¨é‡å…‰ç…§æ–¹é¢çš„çœŸå®æ„Ÿå…ˆéªŒã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å…‰ç¼–è¾‘ç»“æœä¸Šçš„ä¼˜è¶Šæ€§ï¼Œè¶…è¶Šäº†åŸºäºç”¨æˆ·åå¥½çš„ç°æœ‰æ–¹æ³•ã€‚', title='ç²¾ç¡®æ§åˆ¶å…‰æºçš„æ‰©æ•£é‡å…‰ç…§æ–¹æ³•'))
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#multilingual", "#low_resource", "#dataset", "#cv", "#open_source"], "emoji": "ğŸŒ", "ru": {"title": "Maya: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ VLM Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Maya - Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#ethics", "#open_source", "#data"], "emoji": "ğŸ§¹", "ru": {"title": "ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LLaVA Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ñ‚
[15.05.2025 19:09] Using data from previous issue: {"categories": ["#inference", "#3d", "#optimization"], "emoji": "ğŸ”", "ru": {"title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 3D Gaussian Splatting: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ²Ñ‹ÑˆĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ 3D Gaussian Splatting (3DGS). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾
[15.05.2025 19:09] Loading Chinese text from previous data.
[15.05.2025 19:09] Renaming data file.
[15.05.2025 19:09] Renaming previous data. hf_papers.json to ./d/2025-05-15.json
[15.05.2025 19:09] Saving new data file.
[15.05.2025 19:09] Generating page.
[15.05.2025 19:09] Renaming previous page.
[15.05.2025 19:09] Renaming previous data. index.html to ./d/2025-05-15.html
[15.05.2025 19:09] [Experimental] Generating Chinese page for reading.
[15.05.2025 19:09] Chinese vocab [{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'é¢„æµ‹', 'pinyin': 'yÃ¹ cÃ¨', 'trans': 'predict'}, {'word': 'å—é™', 'pinyin': 'shÃ²u xiÃ n', 'trans': 'be limited'}, {'word': 'é¢„å®šä¹‰', 'pinyin': 'yÃ¹ dÃ¬ng yÃ¬', 'trans': 'predefined'}, {'word': 'ç±»åˆ«', 'pinyin': 'lÃ¨i biÃ©', 'trans': 'category'}, {'word': 'è§†è§‰-è¯­è¨€æ¨¡å‹', 'pinyin': 'shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'vision-language model'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'perform'}, {'word': 'æ½œåŠ›', 'pinyin': 'qiÃ¡n lÃ¬', 'trans': 'potential'}, {'word': 'å¯†é›†', 'pinyin': 'mÃ¬ jÃ­', 'trans': 'dense'}, {'word': 'å±€éƒ¨', 'pinyin': 'jÃº bÃ¹', 'trans': 'local'}, {'word': 'ç‰¹å¾', 'pinyin': 'tÃ¨ zhÄ“ng', 'trans': 'feature'}, {'word': 'è¡¨ç¤º', 'pinyin': 'biÇo shÃ¬', 'trans': 'represent'}, {'word': 'æœ‰é™', 'pinyin': 'yÇ’u xiÃ n', 'trans': 'limited'}, {'word': 'æ ‡è®°', 'pinyin': 'biÄo jÃ¬', 'trans': 'mark'}, {'word': 'èšåˆ', 'pinyin': 'jÃ¹ hÃ©', 'trans': 'aggregate'}, {'word': 'ç©ºé—´', 'pinyin': 'kÅng jiÄn', 'trans': 'spatial'}, {'word': 'è¯­ä¹‰', 'pinyin': 'yÇ” yÃ¬', 'trans': 'semantic'}, {'word': 'ç›¸å…³', 'pinyin': 'xiÄng guÄn', 'trans': 'related'}, {'word': 'åŒºåŸŸ', 'pinyin': 'qÅ« yÃ¹', 'trans': 'region'}, {'word': 'ä¿¡æ¯', 'pinyin': 'xÃ¬n xÄ«', 'trans': 'information'}, {'word': 'è§£å†³', 'pinyin': 'jiÄ› juÃ©', 'trans': 'solve'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'}, {'word': 'è§£è€¦', 'pinyin': 'jiÄ› Ç’u', 'trans': 'decouple'}, {'word': 'è‡ªæ³¨æ„', 'pinyin': 'zÃ¬ zhÃ¹ yÃ¬', 'trans': 'self-attention'}, {'word': 'æ¨¡å—', 'pinyin': 'mÃ³ kuÃ i', 'trans': 'module'}, {'word': 'è·å¾—', 'pinyin': 'huÃ² dÃ©', 'trans': 'obtain'}, {'word': 'å†…å®¹', 'pinyin': 'nÃ¨i rÃ³ng', 'trans': 'content'}, {'word': 'ä¸Šä¸‹æ–‡', 'pinyin': 'shÃ ng xiÃ  wÃ©n', 'trans': 'context'}, {'word': 'è¾¨åˆ«æ€§', 'pinyin': 'biÃ n biÃ© xÃ¬ng', 'trans': 'discriminability'}, {'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ« zhÃ¬ xÃ¬ng', 'trans': 'consistency'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'è¯æ˜', 'pinyin': 'zhÃ¨ng mÃ­ng', 'trans': 'prove'}, {'word': 'ä¼˜å¼‚', 'pinyin': 'yÅu yÃ¬', 'trans': 'excellent'}]
[15.05.2025 19:09] Renaming previous Chinese page.
[15.05.2025 19:09] Renaming previous data. zh.html to ./d/2025-05-14_zh_reading_task.html
[15.05.2025 19:09] Writing Chinese reading task.
[15.05.2025 19:09] Writing result.
[15.05.2025 19:09] Renaming log file.
[15.05.2025 19:09] Renaming previous data. log.txt to ./logs/2025-05-15_last_log.txt
