[15.05.2025 22:10] Read previous papers.
[15.05.2025 22:10] Generating top page (month).
[15.05.2025 22:10] Writing top page (month).
[15.05.2025 23:11] Read previous papers.
[15.05.2025 23:11] Get feed.
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09568
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04410
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09343
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09358
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08787
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.07849
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09558
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.12894
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09439
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08455
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.09608
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04793
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08084
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.06356
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.05587
[15.05.2025 23:11] Get page data from previous paper. URL: https://huggingface.co/papers/2505.08910
[15.05.2025 23:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.05.2025 23:11] No deleted papers detected.
[15.05.2025 23:11] Downloading and parsing papers (pdf, html). Total: 16.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.09568.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.09568.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.09568.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.04410.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.04410.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.04410.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.09343.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.09343.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.09343.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.09358.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.09358.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.09358.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.08787.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.08787.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.08787.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.07849.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.07849.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.07849.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.09558.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.09558.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.09558.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2502.12894.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2502.12894.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2502.12894.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.09439.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.09439.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.09439.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.08455.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.08455.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.08455.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.09608.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.09608.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.09608.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.04793.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.04793.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.04793.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.08084.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.08084.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.08084.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.06356.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.06356.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.06356.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.05587.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.05587.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.05587.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2505.08910.
[15.05.2025 23:11] Extra JSON file exists (./assets/json/2505.08910.json), skip PDF parsing.
[15.05.2025 23:11] Paper image links file exists (./assets/img_data/2505.08910.json), skip HTML parsing.
[15.05.2025 23:11] Success.
[15.05.2025 23:11] Enriching papers with extra data.
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 0. Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain und...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 1. Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct applicatio...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 2. The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware mo...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 3. The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning h...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 4. Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appeara...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 5. Software issue localization, the task of identifying the precise code locations (files, classes, or functions) relevant to a natural language issue description (e.g., bug report, feature request), is a critical yet time-consuming aspect of software development. While recent LLM-based agentic approac...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 6. End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models' conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of no...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 7. Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 8. We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, s...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 9. Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven ...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 10. We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our m...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 11. Person reidentification (ReID) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, s...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 12. Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing task...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 13. Pretraining datasets are foundational to the development of multimodal models, yet they often have inherent biases and toxic content from the web-scale corpora they are sourced from. In this paper, we investigate the prevalence of toxicity in LLaVA image-text pretraining dataset, examining how harmf...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 14. 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capt...
[15.05.2025 23:11] ********************************************************************************
[15.05.2025 23:11] Abstract 15. In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we intr...
[15.05.2025 23:11] Read previous papers.
[15.05.2025 23:11] Generating reviews via LLM API.
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#dataset", "#diffusion", "#open_source", "#multimodal", "#training", "#architecture"], "emoji": "🖼️", "ru": {"title": "Объединение понимания и генерации изображений с помощью диффузионных трансформеров", "desc": "Статья представляет новый подход к объединению понимания и генерации и
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#cv", "#optimization", "#open_source"], "emoji": "🔍", "ru": {"title": "DeCLIP: Новый шаг к универсальному компьютерному зрению", "desc": "Статья представляет новый подход DeCLIP для улучшения возможностей моделей компьютерного зрения в задачах плотног
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#training", "#inference", "#architecture", "#optimization"], "emoji": "🧠", "ru": {"title": "Совместное проектирование моделей и оборудования для масштабирования ИИ", "desc": "Статья описывает архитектуру модели DeepSeek-V3/R1 и инфраструктуру ИИ, разработанные для преодоления ограни
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#cv", "#dataset", "#diffusion", "#synthetic", "#transfer_learning", "#training"], "emoji": "🌼", "ru": {"title": "Marigold: Раскрытие потенциала генеративных моделей для плотного анализа изображений", "desc": "Статья представляет Marigold - семейство условных генеративных моделей и п
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#video", "#transfer_learning", "#dataset", "#robotics", "#agents"], "emoji": "🤖", "ru": {"title": "UniSkill: Обучение роботов человеческим навыкам без разметки", "desc": "В статье представлен UniSkill - новый фреймворк для обучения роботов навыкам на основе видео с людьми. Он создае
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#data", "#benchmark", "#optimization", "#dataset", "#survey", "#agents"], "emoji": "🔍", "ru": {"title": "SweRank: эффективная локализация проблем в коде с помощью извлечения и переранжирования", "desc": "SweRank - это эффективная система для локализации проблем в программном обеспеч
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#audio", "#reasoning", "#open_source", "#rlhf", "#dataset", "#benchmark"], "emoji": "🎙️", "ru": {"title": "WavReward: прорыв в оценке разговорных ИИ-систем", "desc": "Статья представляет WavReward - модель оценки разговорных систем на основе аудио. Эта модель способна анализировать 
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#3d", "#graphs", "#robotics", "#optimization"], "emoji": "🏙️", "ru": {"title": "Реконструкция 3D-сцен с учетом компонентов и физики из одного изображения", "desc": "CAST - это новый метод реконструкции 3D-сцен из одного RGB-изображения. Он использует сегментацию объектов, анализ про
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#training", "#multimodal", "#rl", "#reasoning", "#optimization", "#benchmark", "#rag"], "emoji": "🎧", "ru": {"title": "Революция в аудио-ИИ: Omni-R1 покоряет новые вершины мультимодального анализа", "desc": "Исследователи представили Omni-R1 - мультимодальную языковую модель, улучше
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#long_context", "#video"], "emoji": "🎬", "ru": {"title": "Новый бенчмарк для оценки причинно-следственного рассуждения в видео-языковых моделях", "desc": "Исследователи представили новый бенчмарк VCRBench для оценки способностей Больших Видео-Языковых Мод
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#synthetic", "#data", "#dataset", "#training"], "emoji": "💡", "ru": {"title": "Прецизионное управление освещением с помощью диффузионных моделей", "desc": "Авторы представляют новый метод на основе диффузии для точного контроля источников света на изображении. М
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#cv", "#dataset", "#synthetic"], "emoji": "🎯", "ru": {"title": "DetReIDX: Стресс-тест для реидентификации людей в реальном мире", "desc": "Статья представляет DetReIDX - крупномасштабный набор данных для задачи реидентификации людей в воздушно-наземных условиях. Набор данных включае
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#training", "#benchmark", "#dataset", "#reasoning", "#multimodal", "#interpretability", "#cv"], "emoji": "🧠", "ru": {"title": "VISTAR: Интерпретируемые рассуждения в мультимодальных моделях", "desc": "Статья представляет VISTAR - модель для интерпретируемых рассуждений на основе под
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#ethics", "#open_source", "#data"], "emoji": "🧹", "ru": {"title": "Очистка данных для этичного ИИ", "desc": "Исследователи изучили проблему токсичности в наборе данных LLaVA для предобучения мультимодальных моделей. Они провели анализ различных категорий т
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#inference", "#3d", "#optimization"], "emoji": "🔍", "ru": {"title": "Оптимизация 3D Gaussian Splatting: меньше точек, выше эффективность", "desc": "Статья представляет теоретическую основу для улучшения контроля плотности в методе 3D Gaussian Splatting (3DGS). Авторы анализируют про
[15.05.2025 23:11] Using data from previous issue: {"categories": ["#multilingual", "#low_resource", "#dataset", "#cv", "#open_source"], "emoji": "🌍", "ru": {"title": "Maya: мультиязычная VLM для преодоления языковых и культурных барьеров", "desc": "Статья представляет Maya - открытую мультиязычную модель компьютерного зрения и обработки естественно
[15.05.2025 23:11] Loading Chinese text from previous data.
[15.05.2025 23:11] Renaming data file.
[15.05.2025 23:11] Renaming previous data. hf_papers.json to ./d/2025-05-15.json
[15.05.2025 23:11] Saving new data file.
[15.05.2025 23:11] Generating page.
[15.05.2025 23:11] Renaming previous page.
[15.05.2025 23:11] Renaming previous data. index.html to ./d/2025-05-15.html
[15.05.2025 23:11] [Experimental] Generating Chinese page for reading.
[15.05.2025 23:11] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '预测', 'pinyin': 'yù cè', 'trans': 'predict'}, {'word': '受限', 'pinyin': 'shòu xiàn', 'trans': 'be limited'}, {'word': '预定义', 'pinyin': 'yù dìng yì', 'trans': 'predefined'}, {'word': '类别', 'pinyin': 'lèi bié', 'trans': 'category'}, {'word': '视觉-语言模型', 'pinyin': 'shì jué yǔ yán mó xíng', 'trans': 'vision-language model'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'perform'}, {'word': '潜力', 'pinyin': 'qián lì', 'trans': 'potential'}, {'word': '密集', 'pinyin': 'mì jí', 'trans': 'dense'}, {'word': '局部', 'pinyin': 'jú bù', 'trans': 'local'}, {'word': '特征', 'pinyin': 'tè zhēng', 'trans': 'feature'}, {'word': '表示', 'pinyin': 'biǎo shì', 'trans': 'represent'}, {'word': '有限', 'pinyin': 'yǒu xiàn', 'trans': 'limited'}, {'word': '标记', 'pinyin': 'biāo jì', 'trans': 'mark'}, {'word': '聚合', 'pinyin': 'jù hé', 'trans': 'aggregate'}, {'word': '空间', 'pinyin': 'kōng jiān', 'trans': 'spatial'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantic'}, {'word': '相关', 'pinyin': 'xiāng guān', 'trans': 'related'}, {'word': '区域', 'pinyin': 'qū yù', 'trans': 'region'}, {'word': '信息', 'pinyin': 'xìn xī', 'trans': 'information'}, {'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '解耦', 'pinyin': 'jiě ǒu', 'trans': 'decouple'}, {'word': '自注意', 'pinyin': 'zì zhù yì', 'trans': 'self-attention'}, {'word': '模块', 'pinyin': 'mó kuài', 'trans': 'module'}, {'word': '获得', 'pinyin': 'huò dé', 'trans': 'obtain'}, {'word': '内容', 'pinyin': 'nèi róng', 'trans': 'content'}, {'word': '上下文', 'pinyin': 'shàng xià wén', 'trans': 'context'}, {'word': '辨别性', 'pinyin': 'biàn bié xìng', 'trans': 'discriminability'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '证明', 'pinyin': 'zhèng míng', 'trans': 'prove'}, {'word': '优异', 'pinyin': 'yōu yì', 'trans': 'excellent'}]
[15.05.2025 23:11] Renaming previous Chinese page.
[15.05.2025 23:11] Renaming previous data. zh.html to ./d/2025-05-14_zh_reading_task.html
[15.05.2025 23:11] Writing Chinese reading task.
[15.05.2025 23:11] Writing result.
[15.05.2025 23:11] Renaming log file.
[15.05.2025 23:11] Renaming previous data. log.txt to ./logs/2025-05-15_last_log.txt
