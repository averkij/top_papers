[02.04.2025 19:09] Read previous papers.
[02.04.2025 19:09] Generating top page (month).
[02.04.2025 19:09] Writing top page (month).
[02.04.2025 20:12] Read previous papers.
[02.04.2025 20:12] Get feed.
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24379
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00050
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23145
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24376
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00698
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01016
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00595
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00810
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01019
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00906
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00509
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24377
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.22952
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01017
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01005
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00927
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23434
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23733
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00557
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.22165
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00869
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00294
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23361
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00072
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24210
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23157
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.21860
[02.04.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24219
[02.04.2025 20:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[02.04.2025 20:12] No deleted papers detected.
[02.04.2025 20:12] Downloading and parsing papers (pdf, html). Total: 28.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2503.24379.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2503.24379.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2503.24379.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.00050.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.00050.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.00050.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2503.23145.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2503.23145.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2503.23145.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2503.24376.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2503.24376.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2503.24376.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.00698.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.00698.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.00698.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.01016.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.01016.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.01016.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.00595.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.00595.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.00595.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.00810.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.00810.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.00810.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.01019.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.01019.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.01019.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.00906.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.00906.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.00906.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.00509.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.00509.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.00509.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2503.24377.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2503.24377.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2503.24377.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2503.22952.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2503.22952.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2503.22952.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.01017.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.01017.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.01017.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.01005.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.01005.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.01005.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.00927.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.00927.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.00927.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2503.23434.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2503.23434.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2503.23434.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2503.23733.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2503.23733.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2503.23733.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.00557.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.00557.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.00557.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2503.22165.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2503.22165.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2503.22165.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.00869.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.00869.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.00869.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.00294.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.00294.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.00294.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2503.23361.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2503.23361.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2503.23361.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2504.00072.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2504.00072.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2504.00072.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2503.24210.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2503.24210.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2503.24210.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2503.23157.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2503.23157.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2503.23157.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2503.21860.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2503.21860.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2503.21860.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2503.24219.
[02.04.2025 20:12] Extra JSON file exists (./assets/json/2503.24219.json), skip PDF parsing.
[02.04.2025 20:12] Paper image links file exists (./assets/img_data/2503.24219.json), skip HTML parsing.
[02.04.2025 20:12] Success.
[02.04.2025 20:12] Enriching papers with extra data.
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 0. To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 1. The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanc...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 2. Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthe...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 3. Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 4. In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balanc...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 5. Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 6. The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 7. Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning traject...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 8. Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 9. Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenge...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 10. The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 11. Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substant...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 12. The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in stre...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 13. Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP mod...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 14. Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most com...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 15. Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This "single token attention" bottlenecks the amount of information used in distin...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 16. GUI agents, powered by large foundation models, can interact with digital interfaces, enabling various applications in web automation, mobile navigation, and software testing. However, their increasing autonomy has raised critical concerns about their security, privacy, and safety. This survey exami...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 17. Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 18. Visual token reduction lowers inference costs caused by extensive image features in large vision-language models (LVLMs). Unlike relevant studies that prune tokens in self-attention-only LVLMs, our work uniquely addresses cross-attention-based models, which achieve superior performance. We identify ...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 19. Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts-the first ...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 20. Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and dec...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 21. Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 22. Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 23. We address the task of video chaptering, i.e., partitioning a long video timeline into semantic units and generating corresponding chapter titles. While relatively underexplored, automatic chaptering has the potential to enable efficient navigation and content retrieval in long-form videos. In this ...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 24. Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal re...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 25. Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their over...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 26. Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world ...
[02.04.2025 20:12] ********************************************************************************
[02.04.2025 20:12] Abstract 27. We propose a unified framework that integrates object detection (OD) and visual grounding (VG) for remote sensing (RS) imagery. To support conventional OD and establish an intuitive prior for VG task, we fine-tune an open-set object detector using referring expression data, framing it as a partially...
[02.04.2025 20:12] Read previous papers.
[02.04.2025 20:12] Generating reviews via LLM API.
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#multimodal", "#video", "#dataset"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –ª—é–±—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "Any2Caption - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è 
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#training", "#rl"], "emoji": "‚öñÔ∏è", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò-—Å—É–¥–µ–π: –æ—Ç –ø—Ä–æ—Å—Ç–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫ –≥–ª—É–±–æ–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–±—ã—á–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á –æ—Ü–µ–Ω–∫–∏, —Ç—Ä–µ–±—É—é—â–∏—Ö —Å–ª–æ–∂–Ω—ã—Ö
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#benchmark", "#plp", "#agents", "#training"], "emoji": "üß†", "ru": {"title": "CodeARC: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–º —Å–∏–Ω—Ç–µ–∑–µ –ø—Ä–æ–≥—Ä–∞–º–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CodeARC - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –ø—Ä–æ–≥—Ä–∞–º–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#reasoning", "#training", "#multimodal", "#rl", "#benchmark", "#optimization", "#video"], "emoji": "üé•", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤–∏–¥–µ–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –ò–ò-–º–æ–¥–µ–ª—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ SEED-Bench-R1 –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å—Ç
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#training", "#low_resource", "#agents", "#open_source", "#rag", "#multilingual", "#architecture", "#benchmark"], "emoji": "üöÄ", "ru": {"title": "Command A: –ú–æ—â–Ω–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –ò–ò-–º–æ–¥–µ–ª—å –¥–ª—è –±–∏–∑–Ω–µ—Å–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ Command A - –º–æ—â–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –æ–ø—Ç–∏–º–∏–∑
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#video", "#3d", "#architecture", "#diffusion", "#optimization"], "emoji": "üé•", "ru": {"title": "GeometryCrafter: –í—ã—Å–æ–∫–æ—Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≥–ª—É–±–∏–Ω—ã –≤–∏–¥–µ–æ –¥–ª—è 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "desc": "GeometryCrafter - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã –≤–∏–¥–µ–æ. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset", "#training", "#data", "#open_source"], "emoji": "üß†", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –ò–ò-–º–æ–¥–µ–ª—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Open-Qwen2VL - –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å —Å 2 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#dataset", "#training", "#long_context"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ —Ç—Ä
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#diffusion", "#benchmark", "#dataset"], "emoji": "üï∫", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MixerMDM - –ø–µ—Ä–≤—É—é –æ–±—É—á–∞–µ–º—É—é —Ç–µ—Ö–Ω–∏–∫—É –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è 
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#agents"], "emoji": "ü§ñ", "ru": {"title": "Agent S2: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∑–∞–¥–∞—á —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Agent S2 - –Ω–æ–≤—É—é –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∑–∞–¥–∞—á —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å. –°–∏—Å
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#multimodal", "#hallucinations", "#benchmark", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: —É–º–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ?", "desc": "–ê–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ RoR-Bench –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#inference", "#survey", "#reasoning", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–≠–∫–æ–Ω–æ–º–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –±–∞–ª–∞–Ω—Å–∏—Ä—É—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –∑–∞—Ç—Ä–∞—Ç—ã", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –≤ –±–æ–ª—å—à
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#video", "#games", "#inference", "#multimodal", "#reasoning", "#benchmark"], "emoji": "üé•", "ru": {"title": "OmniMMI: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OmniMMI - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#cv", "#training", "#multimodal"], "emoji": "üîç", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–æ–µ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–µ —É—Å—Ç—É–ø–∞–µ—Ç —è–∑—ã–∫–æ–≤–æ–º—É –∫–æ–Ω—Ç—Ä–æ–ª—é –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –º–µ—Ç–æ–¥—ã –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (SSL) –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —è–∑—ã–∫–µ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö (CL
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#training", "#math", "#optimization", "#reasoning", "#inference"], "emoji": "üß†", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ä–µ—à–µ–Ω–∏–π –∏ –∏—Ö –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º, –æ—Å–æ–±–µ–Ω–Ω–æ
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#long_context", "#architecture", "#benchmark", "#optimization"], "emoji": "üîç", "ru": {"title": "–ú–Ω–æ–≥–æ—Ç–æ–∫–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: –Ω–æ–≤—ã–π —à–∞–≥ –≤ –ø–æ–≤—ã—à–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Multi-Token Attention (MTA). –í –æ—Ç–ª–∏—á–∏–µ –æ
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#ethics", "#security", "#agents", "#survey", "#training", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–î–æ–≤–µ—Ä–∏–µ –∫ –ò–ò: –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–≠—Ç–æ –æ–±–∑–æ—Ä –∏—Å—Å–ª–µ–¥—É–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö —Å —Ü–∏—Ñ—Ä–æ–≤—ã–º–∏ –∏–Ω—Ç
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#training", "#architecture", "#transfer_learning", "#optimization", "#multimodal", "#benchmark"], "emoji": "üîÄ", "ru": {"title": "AdaMMS: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AdaMMS - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#inference", "#optimization", "#benchmark", "#cv"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —Å–Ω–∏–∂–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –≤ –∫—Ä—É–ø–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö 
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#benchmark", "#interpretability", "#reasoning", "#multimodal", "#dataset"], "emoji": "üß†", "ru": {"title": "–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'landscape of thoughts' –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#reasoning", "#healthcare", "#science", "#training", "#inference"], "emoji": "ü©∫", "ru": {"title": "–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∑–Ω–∞–Ω–∏—è –≤–∞–∂–Ω–µ–µ –≥–ª—É–±–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#training", "#math", "#optimization"], "emoji": "üß†", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ LLM: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–µ—à–∞—Ç—å —Å
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#training", "#hallucinations", "#optimization", "#graphs", "#benchmark", "#data"], "emoji": "üîç", "ru": {"title": "SEA: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –∑–Ω–∞–Ω–∏—è—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º SEA (—Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—ä–µ–º –æ—à–∏–±–æ–∫) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#open_source", "#video", "#multimodal"], "emoji": "üìΩÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤–∏–¥–µ–æ –Ω–∞ –≥–ª–∞–≤—ã —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é –≤–∏–¥–µ–æ –Ω–∞ –≥–ª–∞–≤—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#synthetic", "#3d", "#cv", "#diffusion"], "emoji": "üîç", "ru": {"title": "–ß–µ—Ç–∫–æ–µ 3D –∏–∑ —Ä–∞–∑–º—ã—Ç–æ–≥–æ 2D: DiET-GS —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DiET-GS - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —á–µ—Ç–∫–∏—Ö 3D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏–∑ —Ä–∞–∑–º—ã—Ç—ã—Ö –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å–æ–±—ã
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#optimization", "#training", "#benchmark", "#reasoning"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ Text-to-SQL —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ —á–∞—Å—Ç–∏—á–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ Text-to-SQL, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#training", "#dataset", "#robotics", "#transfer_learning", "#optimization", "#agents"], "emoji": "ü§ñ", "ru": {"title": "ManipTrans: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –Ω–∞–≤—ã–∫–æ–≤ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–∞–º", "desc": "ManipTrans - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–µ—Ä–µ–¥–∞—á–∏ –Ω–∞–≤—ã–∫–æ–≤ –±–∏–º–∞–Ω—É–∞–ª—å–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –æ—Ç —á–µ–ª–æ–≤
[02.04.2025 20:12] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#cv", "#architecture", "#graphs", "#dataset"], "emoji": "üõ∞Ô∏è", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é –∏ –ø—Ä–∏–≤—è–∑–∫–µ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö —Å–Ω–∏–º–∫–∞—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –≤–∏
[02.04.2025 20:12] Loading Chinese text from previous data.
[02.04.2025 20:12] Renaming data file.
[02.04.2025 20:12] Renaming previous data. hf_papers.json to ./d/2025-04-02.json
[02.04.2025 20:12] Saving new data file.
[02.04.2025 20:12] Generating page.
[02.04.2025 20:12] Renaming previous page.
[02.04.2025 20:12] Renaming previous data. index.html to ./d/2025-04-02.html
[02.04.2025 20:12] [Experimental] Generating Chinese page for reading.
[02.04.2025 20:12] Chinese vocab [{'word': 'Êó®Âú®', 'pinyin': 'zh«ê z√†i', 'trans': 'aim to'}, {'word': 'Áì∂È¢à', 'pinyin': 'p√≠ng j«êng', 'trans': 'bottleneck'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ng ji√†', 'trans': 'framework'}, {'word': 'Ê†∏ÂøÉ', 'pinyin': 'h√© xƒ´n', 'trans': 'core'}, {'word': 'ÊÄùÊÉ≥', 'pinyin': 'sƒ´ xi«éng', 'trans': 'idea'}, {'word': 'Ëß£Èáä', 'pinyin': 'jiƒõ sh√¨', 'trans': 'interpretation'}, {'word': 'Ê≠•È™§', 'pinyin': 'b√π zh√≤u', 'trans': 'step'}, {'word': 'ÂêàÊàê', 'pinyin': 'h√© ch√©ng', 'trans': 'synthesis'}, {'word': 'ÂàÜÁ¶ª', 'pinyin': 'fƒìn l√≠', 'trans': 'separation'}, {'word': 'Âà©Áî®', 'pinyin': 'l√¨ y√≤ng', 'trans': 'utilize'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ shu√†i', 'trans': 'multimodal'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'ËΩ¨Êç¢', 'pinyin': 'zhu«én hu√†n', 'trans': 'convert'}, {'word': 'ÂØÜÈõÜ', 'pinyin': 'm√¨ j√≠', 'trans': 'dense'}, {'word': 'ÁªìÊûÑÂåñ', 'pinyin': 'ji√© g√≤u hu√†', 'trans': 'structured'}, {'word': 'Â≠óÂπï', 'pinyin': 'z√¨ m√π', 'trans': 'subtitle'}, {'word': 'ÊåáÂØº', 'pinyin': 'zh«ê d«éo', 'trans': 'guidance'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√π j√π j√≠', 'trans': 'dataset'}, {'word': 'ÂÆû‰æã', 'pinyin': 'sh√≠ l√¨', 'trans': 'instance'}, {'word': 'Ë∞É‰ºò', 'pinyin': 'ti√°o y≈çu', 'trans': 'tuning'}, {'word': 'ÁªºÂêà', 'pinyin': 'z√≤ng h√©', 'trans': 'comprehensive'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluation'}, {'word': 'ÂèØÊéßÊÄß', 'pinyin': 'kƒõ k√≤ng x√¨ng', 'trans': 'controllability'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'significant'}, {'word': '‰ºò‰∫é', 'pinyin': 'y≈çu y√∫', 'trans': 'superior to'}]
[02.04.2025 20:12] Renaming previous Chinese page.
[02.04.2025 20:12] Renaming previous data. zh.html to ./d/2025-04-01_zh_reading_task.html
[02.04.2025 20:12] Writing Chinese reading task.
[02.04.2025 20:12] Writing result.
[02.04.2025 20:12] Renaming log file.
[02.04.2025 20:12] Renaming previous data. log.txt to ./logs/2025-04-02_last_log.txt
