[02.04.2025 06:16] Read previous papers.
[02.04.2025 06:16] Generating top page (month).
[02.04.2025 06:16] Writing top page (month).
[02.04.2025 07:11] Read previous papers.
[02.04.2025 07:11] Get feed.
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24379
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23145
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24376
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00906
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00810
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.24377
[02.04.2025 07:11] Extract page data from URL. URL: https://huggingface.co/papers/2504.00050
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01016
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00595
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00509
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.01005
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00557
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00294
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23361
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00927
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00869
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.23733
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.21860
[02.04.2025 07:11] Extract page data from URL. URL: https://huggingface.co/papers/2504.01019
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00698
[02.04.2025 07:11] Get page data from previous paper. URL: https://huggingface.co/papers/2504.00072
[02.04.2025 07:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[02.04.2025 07:11] No deleted papers detected.
[02.04.2025 07:11] Downloading and parsing papers (pdf, html). Total: 21.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.24379.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2503.24379.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2503.24379.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.23145.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2503.23145.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2503.23145.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.24376.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2503.24376.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2503.24376.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2504.00906.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2504.00906.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2504.00906.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2504.00810.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2504.00810.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2504.00810.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.24377.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2503.24377.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2503.24377.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2504.00050.
[02.04.2025 07:11] Downloading paper 2504.00050 from http://arxiv.org/pdf/2504.00050v1...
[02.04.2025 07:11] Extracting affiliations from text.
[02.04.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 3 ] . [ 1 0 5 0 0 0 . 4 0 5 2 : r JudgeLRM: Large Reasoning Models as Judge Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, Bingsheng He National University of Singapore https://github.com/NuoJohnChen/JudgeLRM https://huggingface.co/nuojohnchen/JudgeLRM-7B https://huggingface.co/spaces/nuojohnchen/JudgeLRMDemo "
[02.04.2025 07:11] Response: ```python
["National University of Singapore"]
```
[02.04.2025 07:11] Deleting PDF ./assets/pdf/2504.00050.pdf.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2504.01016.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2504.01016.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2504.01016.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2504.00595.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2504.00595.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2504.00595.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2504.00509.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2504.00509.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2504.00509.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2504.01005.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2504.01005.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2504.01005.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2504.00557.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2504.00557.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2504.00557.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2504.00294.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2504.00294.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2504.00294.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.23361.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2503.23361.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2503.23361.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2504.00927.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2504.00927.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2504.00927.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2504.00869.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2504.00869.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2504.00869.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.23733.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2503.23733.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2503.23733.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2503.21860.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2503.21860.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2503.21860.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2504.01019.
[02.04.2025 07:11] Downloading paper 2504.01019 from http://arxiv.org/pdf/2504.01019v1...
[02.04.2025 07:11] Extracting affiliations from text.
[02.04.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 ] . [ 1 9 1 0 1 0 . 4 0 5 2 : r MixerMDM: Learnable Composition of Human Motion Diffusion Models Pablo Ruiz-Ponce1, German Barquero2, Cristina Palmero3, Sergio Escalera2, Jose Garcƒ±a-Rodrƒ±guez1 1Universidad de Alicante, Spain 2Universitat de Barcelona and Computer Vision Center, Spain 3Kings College London, UK pruiz@dtic.ua.es https://pabloruizponce.com/papers/MixerMDM Figure 1. We introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. MixerMDM has demonstrated consistent ability to generate highly controllable human interactions by combining model that generates individual motions from textual descriptions with model that creates human-human interactions. "
[02.04.2025 07:11] Response: ```python
[
    "Universidad de Alicante, Spain",
    "Universitat de Barcelona and Computer Vision Center, Spain",
    "Kings College London, UK"
]
```
[02.04.2025 07:11] Deleting PDF ./assets/pdf/2504.01019.pdf.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2504.00698.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2504.00698.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2504.00698.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Downloading and parsing paper https://huggingface.co/papers/2504.00072.
[02.04.2025 07:11] Extra JSON file exists (./assets/json/2504.00072.json), skip PDF parsing.
[02.04.2025 07:11] Paper image links file exists (./assets/img_data/2504.00072.json), skip HTML parsing.
[02.04.2025 07:11] Success.
[02.04.2025 07:11] Enriching papers with extra data.
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 0. To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 1. Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthe...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 2. Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 3. Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenge...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 4. Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning traject...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 5. Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substant...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 6. The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanc...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 7. Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 8. The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 9. The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 10. Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most com...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 11. Visual token reduction lowers inference costs caused by extensive image features in large vision-language models (LVLMs). Unlike relevant studies that prune tokens in self-attention-only LVLMs, our work uniquely addresses cross-attention-based models, which achieve superior performance. We identify ...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 12. Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 13. Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 14. Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This "single token attention" bottlenecks the amount of information used in distin...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 15. Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and dec...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 16. Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 17. Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world ...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 18. Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 19. In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balanc...
[02.04.2025 07:11] ********************************************************************************
[02.04.2025 07:11] Abstract 20. We address the task of video chaptering, i.e., partitioning a long video timeline into semantic units and generating corresponding chapter titles. While relatively underexplored, automatic chaptering has the potential to enable efficient navigation and content retrieval in long-form videos. In this ...
[02.04.2025 07:11] Read previous papers.
[02.04.2025 07:11] Generating reviews via LLM API.
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#multimodal", "#video", "#dataset"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –ª—é–±—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "Any2Caption - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è 
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#benchmark", "#plp", "#agents", "#training"], "emoji": "üß†", "ru": {"title": "CodeARC: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–º —Å–∏–Ω—Ç–µ–∑–µ –ø—Ä–æ–≥—Ä–∞–º–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CodeARC - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –ø—Ä–æ–≥—Ä–∞–º–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#reasoning", "#training", "#multimodal", "#rl", "#benchmark", "#optimization", "#video"], "emoji": "üé•", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤–∏–¥–µ–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –ò–ò-–º–æ–¥–µ–ª—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ SEED-Bench-R1 –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å—Ç
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#agents"], "emoji": "ü§ñ", "ru": {"title": "Agent S2: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∑–∞–¥–∞—á —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Agent S2 - –Ω–æ–≤—É—é –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∑–∞–¥–∞—á —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å. –°–∏—Å
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#dataset", "#training", "#long_context"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ —Ç—Ä
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#inference", "#survey", "#reasoning", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–≠–∫–æ–Ω–æ–º–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –±–∞–ª–∞–Ω—Å–∏—Ä—É—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –∑–∞—Ç—Ä–∞—Ç—ã", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –≤ –±–æ–ª—å—à
[02.04.2025 07:11] Querying the API.
[02.04.2025 07:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.
[02.04.2025 07:11] Response: {
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–±—ã—á–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á –æ—Ü–µ–Ω–∫–∏, —Ç—Ä–µ–±—É—é—â–∏—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ JudgeLRM, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π-—Å—É–¥–µ–π. JudgeLRM –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã, –≤–∫–ª—é—á–∞—è GPT-4, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –≥–ª—É–±–æ–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å JudgeLRM-7B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 2.79% –ø–æ F1-–º–µ—Ä–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å DeepSeek-R1.",
  "emoji": "‚öñÔ∏è",
  "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò-—Å—É–¥–µ–π: –æ—Ç –ø—Ä–æ—Å—Ç–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫ –≥–ª—É–±–æ–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º"
}
[02.04.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning."

[02.04.2025 07:11] Response: ```python
['RL', 'TRAINING', 'ARCHITECTURE']
```
[02.04.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning."

[02.04.2025 07:11] Response: ```python
["REASONING"]
```
[02.04.2025 07:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores the effectiveness of Large Language Models (LLMs) as judges in evaluation tasks, particularly in scenarios that require complex reasoning. It highlights that traditional Supervised Fine-Tuning (SFT) methods do not perform well when faced with tasks that demand higher reasoning skills. The authors introduce JudgeLRM, a new approach that utilizes reinforcement learning (RL) to enhance the reasoning capabilities of LLMs by providing judge-specific rewards. The results show that JudgeLRM models significantly outperform both SFT-tuned models and other leading reasoning models, demonstrating superior performance in tasks that require deep reasoning.","title":"Reinforcement Learning Boosts LLMs for Complex Judging Tasks"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores the effectiveness of Large Language Models (LLMs) as judges in evaluation tasks, particularly in scenarios that require complex reasoning. It highlights that traditional Supervised Fine-Tuning (SFT) methods do not perform well when faced with tasks that demand higher reasoning skills. The authors introduce JudgeLRM, a new approach that utilizes reinforcement learning (RL) to enhance the reasoning capabilities of LLMs by providing judge-specific rewards. The results show that JudgeLRM models significantly outperform both SFT-tuned models and other leading reasoning models, demonstrating superior performance in tasks that require deep reasoning.', title='Reinforcement Learning Boosts LLMs for Complex Judging Tasks'))
[02.04.2025 07:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰Ωú‰∏∫ËØÑ‰º∞ËÄÖÁöÑÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÁé∞ÊúâÁöÑÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÊñπÊ≥ïÂú®Â§ÑÁêÜÈúÄË¶ÅÊ∑±Â∫¶Êé®ÁêÜÁöÑÊ†∑Êú¨Êó∂ÊïàÊûú‰∏ç‰Ω≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜJudgeLRMÔºåËøôÊòØ‰∏ÄÁßçÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËÆ≠ÁªÉÁöÑËØÑÂà§ÂØºÂêëLLMÔºåËÉΩÂ§üÊèê‰æõÊõ¥ÊúâÊïàÁöÑËØÑ‰º∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåJudgeLRMÊ®°ÂûãÂú®ËØÑÂà§‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑSFTÊ®°ÂûãÂíåÊúÄÂÖàËøõÁöÑÊé®ÁêÜÊ®°ÂûãÔºåÂ∞§ÂÖ∂Âú®ÈúÄË¶ÅÊ∑±Â∫¶Êé®ÁêÜÁöÑ‰ªªÂä°‰∏≠Ë°®Áé∞Á™ÅÂá∫„ÄÇ","title":"JudgeLRMÔºöÊ∑±Â∫¶Êé®ÁêÜÁöÑËØÑÂà§ËÄÖ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰Ωú‰∏∫ËØÑ‰º∞ËÄÖÁöÑÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÁé∞ÊúâÁöÑÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÊñπÊ≥ïÂú®Â§ÑÁêÜÈúÄË¶ÅÊ∑±Â∫¶Êé®ÁêÜÁöÑÊ†∑Êú¨Êó∂ÊïàÊûú‰∏ç‰Ω≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜJudgeLRMÔºåËøôÊòØ‰∏ÄÁßçÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËÆ≠ÁªÉÁöÑËØÑÂà§ÂØºÂêëLLMÔºåËÉΩÂ§üÊèê‰æõÊõ¥ÊúâÊïàÁöÑËØÑ‰º∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåJudgeLRMÊ®°ÂûãÂú®ËØÑÂà§‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑSFTÊ®°ÂûãÂíåÊúÄÂÖàËøõÁöÑÊé®ÁêÜÊ®°ÂûãÔºåÂ∞§ÂÖ∂Âú®ÈúÄË¶ÅÊ∑±Â∫¶Êé®ÁêÜÁöÑ‰ªªÂä°‰∏≠Ë°®Áé∞Á™ÅÂá∫„ÄÇ', title='JudgeLRMÔºöÊ∑±Â∫¶Êé®ÁêÜÁöÑËØÑÂà§ËÄÖ'))
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#video", "#3d", "#architecture", "#diffusion", "#optimization"], "emoji": "üé•", "ru": {"title": "GeometryCrafter: –í—ã—Å–æ–∫–æ—Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≥–ª—É–±–∏–Ω—ã –≤–∏–¥–µ–æ –¥–ª—è 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "desc": "GeometryCrafter - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã –≤–∏–¥–µ–æ. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset", "#training", "#data", "#open_source"], "emoji": "üß†", "ru": {"title": "–û—Ç–∫—Ä—ã—Ç–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –ò–ò-–º–æ–¥–µ–ª—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Open-Qwen2VL - –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å —Å 2 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#multimodal", "#hallucinations", "#benchmark", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: —É–º–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ?", "desc": "–ê–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ RoR-Bench –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#training", "#math", "#optimization", "#reasoning", "#inference"], "emoji": "üß†", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ä–µ—à–µ–Ω–∏–π –∏ –∏—Ö –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º, –æ—Å–æ–±–µ–Ω–Ω–æ
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#inference", "#optimization", "#benchmark", "#cv"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —Å–Ω–∏–∂–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –≤ –∫—Ä—É–ø–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö 
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#training", "#math", "#optimization"], "emoji": "üß†", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ LLM: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–µ—à–∞—Ç—å —Å
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#training", "#hallucinations", "#optimization", "#graphs", "#benchmark", "#data"], "emoji": "üîç", "ru": {"title": "SEA: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –∑–Ω–∞–Ω–∏—è—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º SEA (—Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—ä–µ–º –æ—à–∏–±–æ–∫) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#long_context", "#architecture", "#benchmark", "#optimization"], "emoji": "üîç", "ru": {"title": "–ú–Ω–æ–≥–æ—Ç–æ–∫–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: –Ω–æ–≤—ã–π —à–∞–≥ –≤ –ø–æ–≤—ã—à–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Multi-Token Attention (MTA). –í –æ—Ç–ª–∏—á–∏–µ –æ
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#reasoning", "#healthcare", "#science", "#training", "#inference"], "emoji": "ü©∫", "ru": {"title": "–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∑–Ω–∞–Ω–∏—è –≤–∞–∂–Ω–µ–µ –≥–ª—É–±–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#training", "#architecture", "#transfer_learning", "#optimization", "#multimodal", "#benchmark"], "emoji": "üîÄ", "ru": {"title": "AdaMMS: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AdaMMS - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è
[02.04.2025 07:11] Using data from previous issue: {"categories": ["#training", "#dataset", "#robotics", "#transfer_learning", "#optimization", "#agents"], "emoji": "ü§ñ", "ru": {"title": "ManipTrans: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –Ω–∞–≤—ã–∫–æ–≤ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–∞–º", "desc": "ManipTrans - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–µ—Ä–µ–¥–∞—á–∏ –Ω–∞–≤—ã–∫–æ–≤ –±–∏–º–∞–Ω—É–∞–ª—å–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –æ—Ç —á–µ–ª–æ–≤
[02.04.2025 07:11] Querying the API.
[02.04.2025 07:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix.
[02.04.2025 07:11] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MixerMDM - –ø–µ—Ä–≤—É—é –æ–±—É—á–∞–µ–º—É—é —Ç–µ—Ö–Ω–∏–∫—É –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, MixerMDM –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å–º–µ—à–∏–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç—Å—è —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –¥–ª—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π. –ò—Å–ø–æ–ª—å–∑—É—è MixerMDM –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–≤–∏–∂–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ –∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —á–µ–ª–æ–≤–µ–∫, –∞–≤—Ç–æ—Ä—ã –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Ç–æ–Ω–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –¥–∏–Ω–∞–º–∏–∫–æ–π –∫–∞–∂–¥–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ, –∞ —Ç–∞–∫–∂–µ –Ω–∞–¥ –æ–±—â–∏–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä–∞—è –∏–∑–º–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π.",
  "emoji": "üï∫",
  "title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞"
}
[02.04.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix."

[02.04.2025 07:11] Response: ```python
['DATASET', 'CV', 'BENCHMARK', 'MULTIMODAL']
```
[02.04.2025 07:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix."

[02.04.2025 07:11] Response: ```python
["DIFFUSION"]
```
[02.04.2025 07:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenge of generating human motion based on textual descriptions by introducing MixerMDM, a novel learnable model composition technique. Unlike previous methods, MixerMDM dynamically combines pre-trained motion diffusion models in an adversarial manner, allowing for better control over the generated motions based on specific conditions. The approach enables fine-grained control over individual and overall interactions in multi-person scenarios. Additionally, the authors propose a new evaluation method to assess the quality of generated motions in relation to their conditions, highlighting the adaptability of MixerMDM during the denoising process.","title":"Dynamic Control of Human Motion Generation with MixerMDM"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenge of generating human motion based on textual descriptions by introducing MixerMDM, a novel learnable model composition technique. Unlike previous methods, MixerMDM dynamically combines pre-trained motion diffusion models in an adversarial manner, allowing for better control over the generated motions based on specific conditions. The approach enables fine-grained control over individual and overall interactions in multi-person scenarios. Additionally, the authors propose a new evaluation method to assess the quality of generated motions in relation to their conditions, highlighting the adaptability of MixerMDM during the denoising process.', title='Dynamic Control of Human Motion Generation with MixerMDM'))
[02.04.2025 07:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ®°ÂûãÁªÑÂêàÊäÄÊúØÔºåÁß∞‰∏∫MixerMDMÔºåÁî®‰∫éÁªìÂêàÈ¢ÑËÆ≠ÁªÉÁöÑÊñáÊú¨Êù°‰ª∂‰∫∫Á±ªËøêÂä®Êâ©Êï£Ê®°Âûã„ÄÇ‰∏é‰ª•ÂæÄÁöÑÊñπÊ≥ï‰∏çÂêåÔºåMixerMDMÈááÁî®Âä®ÊÄÅÊ∑∑ÂêàÁ≠ñÁï•ÔºåÈÄöËøáÂØπÊäóËÆ≠ÁªÉÂ≠¶‰π†Â¶Ç‰ΩïÊ†πÊçÆÁîüÊàêÊù°‰ª∂ÁªÑÂêàÂéªÂô™ËøáÁ®ã„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÂÆûÁé∞ÂØπÂçï‰∫∫ÂíåÂ§ö‰∫∫ËøêÂä®ÁöÑÁ≤æÁªÜÊéßÂà∂ÔºåÊèêÂçá‰∫ÜÊØè‰∏™‰∫∫ÁöÑÂä®ÊÄÅË°®Áé∞ÂèäÊï¥‰Ωì‰∫íÂä®ÊïàÊûú„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØÑ‰º∞ÊäÄÊúØÔºåÈ¶ñÊ¨°ÈáèÂåñ‰∫ÜÁîüÊàêËøêÂä®‰∏éÊù°‰ª∂‰πãÈó¥ÁöÑÂØπÈΩêÁ®ãÂ∫¶Ôºå‰ª•ÂèäMixerMDMÂú®ÂéªÂô™ËøáÁ®ã‰∏≠ÈÄÇÂ∫îÊ∑∑ÂêàÁöÑËÉΩÂäõ„ÄÇ","title":"Âä®ÊÄÅÊ∑∑ÂêàÔºåÁ≤æÁªÜÊéßÂà∂‰∫∫Á±ªËøêÂä®ÁîüÊàê"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ®°ÂûãÁªÑÂêàÊäÄÊúØÔºåÁß∞‰∏∫MixerMDMÔºåÁî®‰∫éÁªìÂêàÈ¢ÑËÆ≠ÁªÉÁöÑÊñáÊú¨Êù°‰ª∂‰∫∫Á±ªËøêÂä®Êâ©Êï£Ê®°Âûã„ÄÇ‰∏é‰ª•ÂæÄÁöÑÊñπÊ≥ï‰∏çÂêåÔºåMixerMDMÈááÁî®Âä®ÊÄÅÊ∑∑ÂêàÁ≠ñÁï•ÔºåÈÄöËøáÂØπÊäóËÆ≠ÁªÉÂ≠¶‰π†Â¶Ç‰ΩïÊ†πÊçÆÁîüÊàêÊù°‰ª∂ÁªÑÂêàÂéªÂô™ËøáÁ®ã„ÄÇËØ•ÊñπÊ≥ïËÉΩÂ§üÂÆûÁé∞ÂØπÂçï‰∫∫ÂíåÂ§ö‰∫∫ËøêÂä®ÁöÑÁ≤æÁªÜÊéßÂà∂ÔºåÊèêÂçá‰∫ÜÊØè‰∏™‰∫∫ÁöÑÂä®ÊÄÅË°®Áé∞ÂèäÊï¥‰Ωì‰∫íÂä®ÊïàÊûú„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØÑ‰º∞ÊäÄÊúØÔºåÈ¶ñÊ¨°ÈáèÂåñ‰∫ÜÁîüÊàêËøêÂä®‰∏éÊù°‰ª∂‰πãÈó¥ÁöÑÂØπÈΩêÁ®ãÂ∫¶Ôºå‰ª•ÂèäMixerMDMÂú®ÂéªÂô™ËøáÁ®ã‰∏≠ÈÄÇÂ∫îÊ∑∑ÂêàÁöÑËÉΩÂäõ„ÄÇ', title='Âä®ÊÄÅÊ∑∑ÂêàÔºåÁ≤æÁªÜÊéßÂà∂‰∫∫Á±ªËøêÂä®ÁîüÊàê'))
[02.04.2025 07:12] Using data from previous issue: {"categories": ["#training", "#low_resource", "#agents", "#open_source", "#rag", "#multilingual", "#architecture", "#benchmark"], "emoji": "üöÄ", "ru": {"title": "Command A: –ú–æ—â–Ω–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –ò–ò-–º–æ–¥–µ–ª—å –¥–ª—è –±–∏–∑–Ω–µ—Å–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ Command A - –º–æ—â–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –æ–ø—Ç–∏–º–∏–∑
[02.04.2025 07:12] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#open_source", "#video", "#multimodal"], "emoji": "üìΩÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤–∏–¥–µ–æ –Ω–∞ –≥–ª–∞–≤—ã —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é –≤–∏–¥–µ–æ –Ω–∞ –≥–ª–∞–≤—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏
[02.04.2025 07:12] Loading Chinese text from previous data.
[02.04.2025 07:12] Renaming data file.
[02.04.2025 07:12] Renaming previous data. hf_papers.json to ./d/2025-04-02.json
[02.04.2025 07:12] Saving new data file.
[02.04.2025 07:12] Generating page.
[02.04.2025 07:12] Renaming previous page.
[02.04.2025 07:12] Renaming previous data. index.html to ./d/2025-04-02.html
[02.04.2025 07:12] [Experimental] Generating Chinese page for reading.
[02.04.2025 07:12] Chinese vocab [{'word': 'ËßÜÈ¢ëÁîüÊàê', 'pinyin': 'sh√¨p√≠n shƒìngch√©ng', 'trans': 'video generation'}, {'word': 'Áß∞‰∏∫', 'pinyin': 'chƒìngw√©i', 'trans': 'called'}, {'word': 'Talking Characters', 'pinyin': 'Talking Characters', 'trans': 'Talking Characters'}, {'word': 'ËØ≠Èü≥', 'pinyin': 'y«îyƒ´n', 'trans': 'audio'}, {'word': 'ÊñáÊú¨', 'pinyin': 'w√©nbƒõn', 'trans': 'text'}, {'word': 'Âä®ÁîªËßíËâ≤', 'pinyin': 'd√≤nghu√† ju√©s√®', 'trans': 'animated character'}, {'word': 'talking head', 'pinyin': 'talking head', 'trans': 'talking head'}, {'word': 'ÂÖ®Ë∫´ÂÉè', 'pinyin': 'qu√°nshƒìn xi√†ng', 'trans': 'full-body image'}, {'word': 'Èù¢ÈÉ®', 'pinyin': 'mi√†nb√π', 'trans': 'face'}, {'word': 'MoCha', 'pinyin': 'MoCha', 'trans': 'MoCha'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}, {'word': 'ËØ≠Èü≥-ËßÜÈ¢ëÁ™óÂè£Ê≥®ÊÑèÂäõÊú∫Âà∂', 'pinyin': 'y«îyƒ´n sh√¨p√≠n chuƒÅngk«íu zh√πy√¨l√¨ jƒ´zh√¨', 'trans': 'audio-video window attention mechanism'}, {'word': 'Á≤æÁ°ÆÂêåÊ≠•', 'pinyin': 'jƒ´ngqu√® t√≥ngb√π', 'trans': 'precise synchronization'}, {'word': 'Â§ßËßÑÊ®°', 'pinyin': 'd√†guƒ´m√≥', 'trans': 'large-scale'}, {'word': 'ËØ≠Èü≥Ê†áÊ≥®', 'pinyin': 'y«îyƒ´n biƒÅozh√π', 'trans': 'audio annotation'}, {'word': 'ËßÜÈ¢ëÊï∞ÊçÆÈõÜ', 'pinyin': 'sh√¨p√≠n sh√πj√πj√≠', 'trans': 'video dataset'}, {'word': 'Á®ÄÁº∫', 'pinyin': 'xƒ´quƒì', 'trans': 'scarce'}, {'word': 'ËÅîÂêàËÆ≠ÁªÉÁ≠ñÁï•', 'pinyin': 'li√°nh√© x√πnli√†n c√®l√º√®', 'trans': 'joint training strategy'}, {'word': 'ÊèêÂçá', 'pinyin': 't√≠shƒìng', 'trans': 'improve'}, {'word': 'Ê≥õÂåñËÉΩÂäõ', 'pinyin': 'f√†nhu√† n√©ngl√¨', 'trans': 'generalization capability'}]
[02.04.2025 07:12] Renaming previous Chinese page.
[02.04.2025 07:12] Renaming previous data. zh.html to ./d/2025-04-01_zh_reading_task.html
[02.04.2025 07:12] Writing Chinese reading task.
[02.04.2025 07:12] Writing result.
[02.04.2025 07:12] Renaming log file.
[02.04.2025 07:12] Renaming previous data. log.txt to ./logs/2025-04-02_last_log.txt
