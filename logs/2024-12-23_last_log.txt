[23.12.2024 03:17] Read previous papers.
[23.12.2024 03:17] Generating top page (month).
[23.12.2024 03:17] Writing top page (month).
[23.12.2024 04:12] Read previous papers.
[23.12.2024 04:12] Get feed.
[23.12.2024 04:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.15119
[23.12.2024 04:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.13649
[23.12.2024 04:12] Get page data from previous paper. URL: https://huggingface.co/papers/2412.15322
[23.12.2024 04:12] Extract page data from URL. URL: https://huggingface.co/papers/2412.16112
[23.12.2024 04:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.12.2024 04:12] No deleted papers detected.
[23.12.2024 04:12] Downloading and parsing papers (pdf, html). Total: 4.
[23.12.2024 04:12] Downloading and parsing paper https://huggingface.co/papers/2412.15119.
[23.12.2024 04:12] Extra JSON file exists (./assets/json/2412.15119.json), skip PDF parsing.
[23.12.2024 04:12] Paper image links file exists (./assets/img_data/2412.15119.json), skip HTML parsing.
[23.12.2024 04:12] Success.
[23.12.2024 04:12] Downloading and parsing paper https://huggingface.co/papers/2412.13649.
[23.12.2024 04:12] Extra JSON file exists (./assets/json/2412.13649.json), skip PDF parsing.
[23.12.2024 04:12] Paper image links file exists (./assets/img_data/2412.13649.json), skip HTML parsing.
[23.12.2024 04:12] Success.
[23.12.2024 04:12] Downloading and parsing paper https://huggingface.co/papers/2412.15322.
[23.12.2024 04:12] Extra JSON file exists (./assets/json/2412.15322.json), skip PDF parsing.
[23.12.2024 04:12] Paper image links file exists (./assets/img_data/2412.15322.json), skip HTML parsing.
[23.12.2024 04:12] Success.
[23.12.2024 04:12] Downloading and parsing paper https://huggingface.co/papers/2412.16112.
[23.12.2024 04:12] Downloading paper 2412.16112 from http://arxiv.org/pdf/2412.16112v1...
[23.12.2024 04:13] Extracting affiliations from text.
[23.12.2024 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"4 2 0 2 0 2 ] . [ 1 2 1 1 6 1 . 2 1 4 2 : r CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up Zhenxiong Tan Xinchao Wang National University of Singapore {songhua.liu, zhenxiong}@u.nus.edu, xinchao@nus.edu.sg Figure 1. Ultra-resolution results generated by the linearized FLUX.1-dev model with our approach CLEAR. Resolution is marked on the top-right corner of each result in the format of widthheight. Corresponding prompts can be found in the appendix. "
[23.12.2024 04:13] Response: ```python
["National University of Singapore"]
```
[23.12.2024 04:13] Deleting PDF ./assets/pdf/2412.16112.pdf.
[23.12.2024 04:13] Success.
[23.12.2024 04:13] Enriching papers with extra data.
[23.12.2024 04:13] ********************************************************************************
[23.12.2024 04:13] Abstract 0. Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves ge...
[23.12.2024 04:13] ********************************************************************************
[23.12.2024 04:13] Abstract 1. Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the follow...
[23.12.2024 04:13] ********************************************************************************
[23.12.2024 04:13] Abstract 2. We propose to synthesize high-quality and synchronized audio, given video and optional text conditions, using a novel multimodal joint training framework MMAudio. In contrast to single-modality training conditioned on (limited) video data only, MMAudio is jointly trained with larger-scale, readily a...
[23.12.2024 04:13] ********************************************************************************
[23.12.2024 04:13] Abstract 3. Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, w...
[23.12.2024 04:13] Read previous papers.
[23.12.2024 04:13] Generating reviews via LLM API.
[23.12.2024 04:13] Using data from previous issue: {"categories": ["#training", "#inference", "#video", "#cv", "#architecture", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ
[23.12.2024 04:13] Using data from previous issue: {"categories": ["#training", "#long_context", "#inference", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è KV-–∫—ç—à–∞ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SCOPE - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ KV-–∫—ç—à–∞ –≤ –º–æ–¥–µ–ª—è—Ö LLM –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª
[23.12.2024 04:13] Using data from previous issue: {"categories": ["#small_models", "#audio", "#inference", "#video", "#multimodal", "#synthetic"], "emoji": "üéµ", "ru": {"title": "MMAudio: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∏–Ω—Ç–µ–∑–µ –∞—É–¥–∏–æ –ø–æ –≤–∏–¥–µ–æ –∏ —Ç–µ–∫—Å—Ç—É", "desc": "MMAudio - —ç—Ç–æ –Ω–æ–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ –Ω–∞ –æ
[23.12.2024 04:13] Querying the API.
[23.12.2024 04:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, we aim at a linear attention mechanism in this paper that reduces the complexity of pre-trained DiTs to linear. We begin our exploration with a comprehensive summary of existing efficient attention mechanisms and identify four key factors crucial for successful linearization of pre-trained DiTs: locality, formulation consistency, high-rank attention maps, and feature integrity. Based on these insights, we introduce a convolution-like local attention strategy termed CLEAR, which limits feature interactions to a local window around each query token, and thus achieves linear complexity. Our experiments indicate that, by fine-tuning the attention layer on merely 10K self-generated samples for 10K iterations, we can effectively transfer knowledge from a pre-trained DiT to a student model with linear complexity, yielding results comparable to the teacher model. Simultaneously, it reduces attention computations by 99.5% and accelerates generation by 6.3 times for generating 8K-resolution images. Furthermore, we investigate favorable properties in the distilled attention layers, such as zero-shot generalization cross various models and plugins, and improved support for multi-GPU parallel inference. Models and codes are available here: https://github.com/Huage001/CLEAR.
[23.12.2024 04:13] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è Diffusion Transformers (DiT), –Ω–∞–∑—ã–≤–∞–µ–º—ã–π CLEAR. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Å–Ω–∏–∂–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö DiT —Å –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π –¥–æ –ª–∏–Ω–µ–π–Ω–æ–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. CLEAR –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –ø–æ–¥–æ–±–Ω–æ–µ —Å–≤—ë—Ä—Ç–æ—á–Ω—ã–º –æ–ø–µ—Ä–∞—Ü–∏—è–º, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ª–æ–∫–∞–ª—å–Ω—ã–º –æ–∫–Ω–æ–º –≤–æ–∫—Ä—É–≥ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∑–∞–ø—Ä–æ—Å–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ fine-tuning —Å–ª–æ—è –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –≤—Å–µ–≥–æ 10 —Ç—ã—Å—è—á–∞—Ö —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∑–Ω–∞–Ω–∏—è –æ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∫ –º–æ–¥–µ–ª–∏ —Å –ª–∏–Ω–µ–π–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é.",
  "emoji": "üöÄ",
  "title": "CLEAR: –£—Å–∫–æ—Ä–µ–Ω–∏–µ DiT –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞"
}
[23.12.2024 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, we aim at a linear attention mechanism in this paper that reduces the complexity of pre-trained DiTs to linear. We begin our exploration with a comprehensive summary of existing efficient attention mechanisms and identify four key factors crucial for successful linearization of pre-trained DiTs: locality, formulation consistency, high-rank attention maps, and feature integrity. Based on these insights, we introduce a convolution-like local attention strategy termed CLEAR, which limits feature interactions to a local window around each query token, and thus achieves linear complexity. Our experiments indicate that, by fine-tuning the attention layer on merely 10K self-generated samples for 10K iterations, we can effectively transfer knowledge from a pre-trained DiT to a student model with linear complexity, yielding results comparable to the teacher model. Simultaneously, it reduces attention computations by 99.5% and accelerates generation by 6.3 times for generating 8K-resolution images. Furthermore, we investigate favorable properties in the distilled attention layers, such as zero-shot generalization cross various models and plugins, and improved support for multi-GPU parallel inference. Models and codes are available here: https://github.com/Huage001/CLEAR."

[23.12.2024 04:13] Response: ```python
['ARCHITECTURE', 'TRAINING', 'INFERENCE']
```
[23.12.2024 04:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, we aim at a linear attention mechanism in this paper that reduces the complexity of pre-trained DiTs to linear. We begin our exploration with a comprehensive summary of existing efficient attention mechanisms and identify four key factors crucial for successful linearization of pre-trained DiTs: locality, formulation consistency, high-rank attention maps, and feature integrity. Based on these insights, we introduce a convolution-like local attention strategy termed CLEAR, which limits feature interactions to a local window around each query token, and thus achieves linear complexity. Our experiments indicate that, by fine-tuning the attention layer on merely 10K self-generated samples for 10K iterations, we can effectively transfer knowledge from a pre-trained DiT to a student model with linear complexity, yielding results comparable to the teacher model. Simultaneously, it reduces attention computations by 99.5% and accelerates generation by 6.3 times for generating 8K-resolution images. Furthermore, we investigate favorable properties in the distilled attention layers, such as zero-shot generalization cross various models and plugins, and improved support for multi-GPU parallel inference. Models and codes are available here: https://github.com/Huage001/CLEAR."

[23.12.2024 04:13] Response: ```python
['OPTIMIZATION', 'TRANSFER_LEARNING', 'DIFFUSION']
```
[23.12.2024 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new approach to improve the efficiency of Diffusion Transformers (DiT) in image generation by introducing a linear attention mechanism. The authors identify key factors necessary for linearizing DiTs, such as locality and feature integrity, and propose a local attention strategy called CLEAR. This method significantly reduces the computational complexity of attention mechanisms, achieving a 99.5% reduction in attention computations and a 6.3 times speedup in generating high-resolution images. Additionally, the study shows that the distilled model retains performance comparable to the original DiT while enabling better generalization and multi-GPU support.","title":"Linearizing Attention for Faster Image Generation with CLEAR"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper presents a new approach to improve the efficiency of Diffusion Transformers (DiT) in image generation by introducing a linear attention mechanism. The authors identify key factors necessary for linearizing DiTs, such as locality and feature integrity, and propose a local attention strategy called CLEAR. This method significantly reduces the computational complexity of attention mechanisms, achieving a 99.5% reduction in attention computations and a 6.3 times speedup in generating high-resolution images. Additionally, the study shows that the distilled model retains performance comparable to the original DiT while enabling better generalization and multi-GPU support.', title='Linearizing Attention for Faster Image Generation with CLEAR'))
[23.12.2024 04:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ∫øÊÄßÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊó®Âú®Ëß£ÂÜ≥Êâ©Êï£ÂèòÊç¢Âô®ÔºàDiTÔºâÂú®ÁîüÊàêÈ´òÂàÜËæ®ÁéáÂõæÂÉèÊó∂ÁöÑÂª∂ËøüÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊÄªÁªì‰∫ÜÁé∞ÊúâÁöÑÈ´òÊïàÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂπ∂Á°ÆÂÆö‰∫ÜÊàêÂäüÁ∫øÊÄßÂåñÈ¢ÑËÆ≠ÁªÉDiTÁöÑÂõõ‰∏™ÂÖ≥ÈîÆÂõ†Á¥†„ÄÇÂü∫‰∫éËøô‰∫õËßÅËß£ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂêç‰∏∫CLEARÁöÑÂ±ÄÈÉ®Ê≥®ÊÑèÂäõÁ≠ñÁï•ÔºåÈôêÂà∂ÁâπÂæÅ‰∫§‰∫íÂú®ÊØè‰∏™Êü•ËØ¢Ê†áËÆ∞Âë®Âõ¥ÁöÑÂ±ÄÈÉ®Á™óÂè£ÂÜÖÔºå‰ªéËÄåÂÆûÁé∞Á∫øÊÄßÂ§çÊùÇÂ∫¶„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈÄöËøáÂØπÊ≥®ÊÑèÂäõÂ±ÇËøõË°åÂæÆË∞ÉÔºåÊàë‰ª¨ÂèØ‰ª•ÊúâÊïàÂú∞Â∞ÜÁü•ËØÜ‰ªéÈ¢ÑËÆ≠ÁªÉÁöÑDiTËΩ¨ÁßªÂà∞Â≠¶ÁîüÊ®°ÂûãÔºåÂêåÊó∂ÊòæËëóÂáèÂ∞ëËÆ°ÁÆóÈáèÂπ∂Âä†ÈÄüÁîüÊàêËøáÁ®ã„ÄÇ","title":"Á∫øÊÄßÊ≥®ÊÑèÂäõÔºåÂø´ÈÄüÁîüÊàêÈ´òÂàÜËæ®ÁéáÂõæÂÉèÔºÅ"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ∫øÊÄßÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊó®Âú®Ëß£ÂÜ≥Êâ©Êï£ÂèòÊç¢Âô®ÔºàDiTÔºâÂú®ÁîüÊàêÈ´òÂàÜËæ®ÁéáÂõæÂÉèÊó∂ÁöÑÂª∂ËøüÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊÄªÁªì‰∫ÜÁé∞ÊúâÁöÑÈ´òÊïàÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂπ∂Á°ÆÂÆö‰∫ÜÊàêÂäüÁ∫øÊÄßÂåñÈ¢ÑËÆ≠ÁªÉDiTÁöÑÂõõ‰∏™ÂÖ≥ÈîÆÂõ†Á¥†„ÄÇÂü∫‰∫éËøô‰∫õËßÅËß£ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂêç‰∏∫CLEARÁöÑÂ±ÄÈÉ®Ê≥®ÊÑèÂäõÁ≠ñÁï•ÔºåÈôêÂà∂ÁâπÂæÅ‰∫§‰∫íÂú®ÊØè‰∏™Êü•ËØ¢Ê†áËÆ∞Âë®Âõ¥ÁöÑÂ±ÄÈÉ®Á™óÂè£ÂÜÖÔºå‰ªéËÄåÂÆûÁé∞Á∫øÊÄßÂ§çÊùÇÂ∫¶„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈÄöËøáÂØπÊ≥®ÊÑèÂäõÂ±ÇËøõË°åÂæÆË∞ÉÔºåÊàë‰ª¨ÂèØ‰ª•ÊúâÊïàÂú∞Â∞ÜÁü•ËØÜ‰ªéÈ¢ÑËÆ≠ÁªÉÁöÑDiTËΩ¨ÁßªÂà∞Â≠¶ÁîüÊ®°ÂûãÔºåÂêåÊó∂ÊòæËëóÂáèÂ∞ëËÆ°ÁÆóÈáèÂπ∂Âä†ÈÄüÁîüÊàêËøáÁ®ã„ÄÇ', title='Á∫øÊÄßÊ≥®ÊÑèÂäõÔºåÂø´ÈÄüÁîüÊàêÈ´òÂàÜËæ®ÁéáÂõæÂÉèÔºÅ'))
[23.12.2024 04:13] Loading Chinese text from previous data.
[23.12.2024 04:13] Renaming data file.
[23.12.2024 04:13] Renaming previous data. hf_papers.json to ./d/2024-12-23.json
[23.12.2024 04:13] Saving new data file.
[23.12.2024 04:13] Generating page.
[23.12.2024 04:13] Renaming previous page.
[23.12.2024 04:13] Renaming previous data. index.html to ./d/2024-12-23.html
[23.12.2024 04:13] [Experimental] Generating Chinese page for reading.
[23.12.2024 04:13] Chinese vocab [{'word': 'ÁØá', 'pinyin': 'piƒÅn', 'trans': 'piece of writing'}, {'word': '‰ªãÁªç', 'pinyin': 'ji√®sh√†o', 'trans': 'introduce'}, {'word': 'ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'y«îy√°n m√≥x√≠ng', 'trans': 'language model'}, {'word': 'Ââç‰∏ÄÁâàÊú¨', 'pinyin': 'qi√°n yƒ´ b«énbƒõn', 'trans': 'previous version'}, {'word': 'Áõ∏ÊØî', 'pinyin': 'xiƒÅngb«ê', 'trans': 'compare'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πnli√†n', 'trans': 'pre-training'}, {'word': 'ÂêéËÆ≠ÁªÉ', 'pinyin': 'h√≤u x√πnli√†n', 'trans': 'post-training'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'}, {'word': 'ÊîπËøõ', 'pinyin': 'g«éij√¨n', 'trans': 'improvement'}, {'word': 'Ê†áËÆ∞', 'pinyin': 'biƒÅoj√¨', 'trans': 'token'}, {'word': 'Êâ©Â±ï', 'pinyin': 'ku√≤zh«én', 'trans': 'expand'}, {'word': 'Â∏∏ËØÜ', 'pinyin': 'ch√°ngsh√≠', 'trans': 'common sense'}, {'word': '‰∏ì‰∏öÁü•ËØÜ', 'pinyin': 'zhuƒÅny√® zhƒ´shi', 'trans': 'professional knowledge'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability'}, {'word': 'ÁõëÁù£', 'pinyin': 'ji√†nd≈´', 'trans': 'supervised'}, {'word': 'ÂæÆË∞É', 'pinyin': 'wƒìiti√°o', 'trans': 'fine-tuning'}, {'word': 'Â§öÈò∂ÊÆµ', 'pinyin': 'du≈ç jiƒìdu√†n', 'trans': 'multi-stage'}, {'word': 'Âº∫ÂåñÂ≠¶‰π†', 'pinyin': 'qi√°ngji√† xu√©x√≠', 'trans': 'reinforcement learning'}, {'word': 'ÊèêÂçá', 'pinyin': 't√≠shƒìng', 'trans': 'enhance'}, {'word': '‰∫∫Á±ªÂÅèÂ•Ω', 'pinyin': 'r√©nl√®i piƒÅnh√†o', 'trans': 'human preference'}, {'word': 'ÈïøÊñáÊú¨', 'pinyin': 'ch√°ng w√©nbƒõn', 'trans': 'long text'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generation'}, {'word': 'Â∞∫ÂØ∏', 'pinyin': 'ch«êc√πn', 'trans': 'size'}, {'word': 'Âü∫Á°ÄÊ®°Âûã', 'pinyin': 'jƒ´ch«î m√≥x√≠ng', 'trans': 'base model'}, {'word': 'Êåá‰ª§', 'pinyin': 'zh«êl√¨ng', 'trans': 'instruction'}, {'word': 'ÈáèÂåñ', 'pinyin': 'li√†nghu√†', 'trans': 'quantization'}]
[23.12.2024 04:13] Renaming previous Chinese page.
[23.12.2024 04:13] Renaming previous data. zh.html to ./d/2024-12-22_zh_reading_task.html
[23.12.2024 04:13] Writing Chinese reading task.
[23.12.2024 04:13] Writing result.
[23.12.2024 04:13] Renaming log file.
[23.12.2024 04:13] Renaming previous data. log.txt to ./logs/2024-12-23_last_log.txt
