[23.12.2024 08:14] Read previous papers.
[23.12.2024 08:14] Generating top page (month).
[23.12.2024 08:14] Writing top page (month).
[23.12.2024 09:11] Read previous papers.
[23.12.2024 09:11] Get feed.
[23.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.15119
[23.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.13649
[23.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.16145
[23.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.16112
[23.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.15322
[23.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.15450
[23.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.14590
[23.12.2024 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.12.2024 09:11] No deleted papers detected.
[23.12.2024 09:11] Downloading and parsing papers (pdf, html). Total: 7.
[23.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.15119.
[23.12.2024 09:11] Extra JSON file exists (./assets/json/2412.15119.json), skip PDF parsing.
[23.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.15119.json), skip HTML parsing.
[23.12.2024 09:11] Success.
[23.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.13649.
[23.12.2024 09:11] Extra JSON file exists (./assets/json/2412.13649.json), skip PDF parsing.
[23.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.13649.json), skip HTML parsing.
[23.12.2024 09:11] Success.
[23.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.16145.
[23.12.2024 09:11] Extra JSON file exists (./assets/json/2412.16145.json), skip PDF parsing.
[23.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.16145.json), skip HTML parsing.
[23.12.2024 09:11] Success.
[23.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.16112.
[23.12.2024 09:11] Extra JSON file exists (./assets/json/2412.16112.json), skip PDF parsing.
[23.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.16112.json), skip HTML parsing.
[23.12.2024 09:11] Success.
[23.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.15322.
[23.12.2024 09:11] Extra JSON file exists (./assets/json/2412.15322.json), skip PDF parsing.
[23.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.15322.json), skip HTML parsing.
[23.12.2024 09:11] Success.
[23.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.15450.
[23.12.2024 09:11] Extra JSON file exists (./assets/json/2412.15450.json), skip PDF parsing.
[23.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.15450.json), skip HTML parsing.
[23.12.2024 09:11] Success.
[23.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.14590.
[23.12.2024 09:11] Extra JSON file exists (./assets/json/2412.14590.json), skip PDF parsing.
[23.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.14590.json), skip HTML parsing.
[23.12.2024 09:11] Success.
[23.12.2024 09:11] Enriching papers with extra data.
[23.12.2024 09:11] ********************************************************************************
[23.12.2024 09:11] Abstract 0. Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves ge...
[23.12.2024 09:11] ********************************************************************************
[23.12.2024 09:11] Abstract 1. Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the follow...
[23.12.2024 09:11] ********************************************************************************
[23.12.2024 09:11] Abstract 2. Improving the multi-step reasoning ability of large language models (LLMs) with offline reinforcement learning (RL) is essential for quickly adapting them to complex tasks. While Direct Preference Optimization (DPO) has shown promise in aligning LLMs with human preferences, it is less suitable for m...
[23.12.2024 09:11] ********************************************************************************
[23.12.2024 09:11] Abstract 3. Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, w...
[23.12.2024 09:11] ********************************************************************************
[23.12.2024 09:11] Abstract 4. We propose to synthesize high-quality and synchronized audio, given video and optional text conditions, using a novel multimodal joint training framework MMAudio. In contrast to single-modality training conditioned on (limited) video data only, MMAudio is jointly trained with larger-scale, readily a...
[23.12.2024 09:11] ********************************************************************************
[23.12.2024 09:11] Abstract 5. This paper introduces Fietje, a family of small language models (SLMs) specifically designed for the Dutch language. The model is based on Phi 2, an English-centric model of 2.7 billion parameters. Fietje demonstrated competitive results with larger language models upon its release. A core emphasis ...
[23.12.2024 09:11] ********************************************************************************
[23.12.2024 09:11] Abstract 6. Quantization has become one of the most effective methodologies to compress LLMs into smaller size. However, the existing quantization solutions still show limitations of either non-negligible accuracy drop or system inefficiency. In this paper, we make a comprehensive analysis of the general quanti...
[23.12.2024 09:11] Read previous papers.
[23.12.2024 09:11] Generating reviews via LLM API.
[23.12.2024 09:11] Using data from previous issue: {"categories": ["#training", "#inference", "#video", "#cv", "#architecture", "#optimization"], "emoji": "ğŸš€", "ru": {"title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾
[23.12.2024 09:11] Using data from previous issue: {"categories": ["#training", "#long_context", "#inference", "#optimization"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ KV-ĞºÑÑˆĞ° Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² LLM", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SCOPE - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ KV-ĞºÑÑˆĞ° Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… LLM Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»
[23.12.2024 09:11] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#optimization", "#math", "#rlhf", "#agents", "#reasoning"], "emoji": "ğŸ§ ", "ru": {"title": "OREO: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ OREO (Offline Reasoning Optimization) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹
[23.12.2024 09:11] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#training", "#inference", "#architecture", "#diffusion"], "emoji": "ğŸš€", "ru": {"title": "CLEAR: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ DiT Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Diffusion Transformers (DiT), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ CLEAR. 
[23.12.2024 09:11] Using data from previous issue: {"categories": ["#small_models", "#audio", "#inference", "#video", "#multimodal", "#synthetic"], "emoji": "ğŸµ", "ru": {"title": "MMAudio: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ñƒ", "desc": "MMAudio - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ½Ğ° Ğ¾
[23.12.2024 09:11] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#reasoning", "#open_source", "#small_models", "#benchmark"], "emoji": "ğŸ‡³ğŸ‡±", "ru": {"title": "Fietje: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ°Ğ»Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ´ĞµÑ€Ğ»Ğ°Ğ½Ğ´ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Fietje - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (SLM), ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±
[23.12.2024 09:11] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "MixLLM: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MixLLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½ÑƒÑ 
[23.12.2024 09:11] Trying to get texts in Chinese.
[23.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves generation efficiency while preserving the advantages of autoregressive modeling. Our key insight is that parallel generation depends on visual token dependencies-tokens with weak dependencies can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together, as their independent sampling may lead to inconsistencies. Based on this observation, we develop a parallel generation strategy that generates distant tokens with weak dependencies in parallel while maintaining sequential generation for strongly dependent local tokens. Our approach can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that our method achieves a 3.6x speedup with comparable quality and up to 9.5x speedup with minimal quality degradation across both image and video generation tasks. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.
[23.12.2024 09:11] Mistral response. {"id": "b7b55f3db91a45ca99bf39e8b52f9be0", "object": "chat.completion", "created": 1734945069, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u89c9\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u5176\u987a\u5e8f\u9010\u6807\u8bb0\u9884\u6d4b\u7684\u8fc7\u7a0b\uff0c\u63a8\u7406\u901f\u5ea6\u8f83\u6162\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u5e76\u884c\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u65b9\u6cd5\uff0c\u63d0\u9ad8\u751f\u6210\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u7559\u81ea\u56de\u5f52\u5efa\u6a21\u7684\u4f18\u52bf\u3002\u6211\u4eec\u7684\u5173\u952e\u6d1e\u89c1\u662f\u5e76\u884c\u751f\u6210\u4f9d\u8d56\u4e8e\u89c6\u89c9\u6807\u8bb0\u7684\u4f9d\u8d56\u5173\u7cfb\uff1a\u5f31\u4f9d\u8d56\u7684\u6807\u8bb0\u53ef\u4ee5\u5e76\u884c\u751f\u6210\uff0c\u800c\u5f3a\u4f9d\u8d56\u7684\u76f8\u90bb\u6807\u8bb0\u96be\u4ee5\u4e00\u8d77\u751f\u6210\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u72ec\u7acb\u91c7\u6837\u53ef\u80fd\u5bfc\u81f4\u4e0d\u4e00\u81f4\u3002\u57fa\u4e8e\u6b64\u89c2\u5bdf\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u5e76\u884c\u751f\u6210\u7b56\u7565\uff0c\u5e76\u884c\u751f\u6210\u5f31\u4f9d\u8d56\u7684\u8fdc\u8ddd\u79bb\u6807\u8bb0\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u4f9d\u8d56\u7684\u5c40\u90e8\u6807\u8bb0\u7684\u987a\u5e8f\u751f\u6210\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u6807\u51c6\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\uff0c\u65e0\u9700\u4fee\u6539\u67b6\u6784\u6216\u6807\u8bb0\u5668\u3002\u5728ImageNet\u548cUCF-101\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad89.5\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u8d28\u91cf\u635f\u5931\u6700\u5c0f\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u6fc0\u53d1\u672a\u6765\u5728\u9ad8\u6548\u89c6\u89c9\u751f\u6210\u548c\u7edf\u4e00\u81ea\u56de\u5f52\u5efa\u6a21\u65b9\u9762\u7684\u7814\u7a76\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://epiphqny.github.io/PAR-project\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 290, "total_tokens": 651, "completion_tokens": 361}}
[23.12.2024 09:11] Response: è‡ªå›å½’æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ç”±äºå…¶é¡ºåºé€æ ‡è®°é¢„æµ‹çš„è¿‡ç¨‹ï¼Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¹¶è¡Œè‡ªå›å½’è§†è§‰ç”Ÿæˆæ–¹æ³•ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡ï¼ŒåŒæ—¶ä¿ç•™è‡ªå›å½’å»ºæ¨¡çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„å…³é”®æ´è§æ˜¯å¹¶è¡Œç”Ÿæˆä¾èµ–äºè§†è§‰æ ‡è®°çš„ä¾èµ–å…³ç³»ï¼šå¼±ä¾èµ–çš„æ ‡è®°å¯ä»¥å¹¶è¡Œç”Ÿæˆï¼Œè€Œå¼ºä¾èµ–çš„ç›¸é‚»æ ‡è®°éš¾ä»¥ä¸€èµ·ç”Ÿæˆï¼Œå› ä¸ºå®ƒä»¬çš„ç‹¬ç«‹é‡‡æ ·å¯èƒ½å¯¼è‡´ä¸ä¸€è‡´ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¹¶è¡Œç”Ÿæˆç­–ç•¥ï¼Œå¹¶è¡Œç”Ÿæˆå¼±ä¾èµ–çš„è¿œè·ç¦»æ ‡è®°ï¼ŒåŒæ—¶ä¿æŒå¼ºä¾èµ–çš„å±€éƒ¨æ ‡è®°çš„é¡ºåºç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°æ ‡å‡†è‡ªå›å½’æ¨¡å‹ä¸­ï¼Œæ— éœ€ä¿®æ”¹æ¶æ„æˆ–æ ‡è®°å™¨ã€‚åœ¨ImageNetå’ŒUCF-101ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æœ€é«˜9.5å€çš„åŠ é€Ÿï¼ŒåŒæ—¶è´¨é‡æŸå¤±æœ€å°ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½æ¿€å‘æœªæ¥åœ¨é«˜æ•ˆè§†è§‰ç”Ÿæˆå’Œç»Ÿä¸€è‡ªå›å½’å»ºæ¨¡æ–¹é¢çš„ç ”ç©¶ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://epiphqny.github.io/PAR-projectã€‚
[23.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è‡ªå›å½’æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ç”±äºå…¶é¡ºåºé€æ ‡è®°é¢„æµ‹çš„è¿‡ç¨‹ï¼Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¹¶è¡Œè‡ªå›å½’è§†è§‰ç”Ÿæˆæ–¹æ³•ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡ï¼ŒåŒæ—¶ä¿ç•™è‡ªå›å½’å»ºæ¨¡çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„å…³é”®æ´è§æ˜¯å¹¶è¡Œç”Ÿæˆä¾èµ–äºè§†è§‰æ ‡è®°çš„ä¾èµ–å…³ç³»ï¼šå¼±ä¾èµ–çš„æ ‡è®°å¯ä»¥å¹¶è¡Œç”Ÿæˆï¼Œè€Œå¼ºä¾èµ–çš„ç›¸é‚»æ ‡è®°éš¾ä»¥ä¸€èµ·ç”Ÿæˆï¼Œå› ä¸ºå®ƒä»¬çš„ç‹¬ç«‹é‡‡æ ·å¯èƒ½å¯¼è‡´ä¸ä¸€è‡´ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¹¶è¡Œç”Ÿæˆç­–ç•¥ï¼Œå¹¶è¡Œç”Ÿæˆå¼±ä¾èµ–çš„è¿œè·ç¦»æ ‡è®°ï¼ŒåŒæ—¶ä¿æŒå¼ºä¾èµ–çš„å±€éƒ¨æ ‡è®°çš„é¡ºåºç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°æ ‡å‡†è‡ªå›å½’æ¨¡å‹ä¸­ï¼Œæ— éœ€ä¿®æ”¹æ¶æ„æˆ–æ ‡è®°å™¨ã€‚åœ¨ImageNetå’ŒUCF-101ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æœ€é«˜9.5å€çš„åŠ é€Ÿï¼ŒåŒæ—¶è´¨é‡æŸå¤±æœ€å°ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½æ¿€å‘æœªæ¥åœ¨é«˜æ•ˆè§†è§‰ç”Ÿæˆå’Œç»Ÿä¸€è‡ªå›å½’å»ºæ¨¡æ–¹é¢çš„ç ”ç©¶ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://epiphqny.github.io/PAR-projectã€‚
[23.12.2024 09:11] Mistral response. {"id": "46a6c95f1ac449a8b0a90333bed78ce9", "object": "chat.completion", "created": 1734945076, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Z\u00ec hu\u00edgu\u012b m\u00f3x\u00edng z\u00e0i sh\u00ecju\u00e9 sh\u0113ngch\u00e9ng zh\u014dng bi\u01ceoxi\u00e0n ch\u016b qi\u00e1ngd\u00e0 de x\u00ecngn\u00e9ng, d\u00e0n y\u00f3uy\u00fa q\u00ed sh\u00f9nx\u00f9 zh\u00fa bi\u0101oj\u00ec y\u00f9c\u00e8 de gu\u00f2ch\u00e9ng, tu\u012bl\u01d0 s\u00f9d\u00f9 ji\u00e0o m\u00e0n. B\u011bnw\u00e9n t\u00edch\u016b le y\u012bzh\u01d2ng ji\u01cend\u0101n y\u01d2uxi\u00e0o de b\u00ecngx\u00edng z\u00echu\u00edgu\u012b sh\u00ecju\u00e9 sh\u0113ngch\u00e9ng f\u0101ngf\u01ce, t\u00edg\u0101o sh\u0113ngch\u00e9ng xi\u00e0ol\u01dc, t\u00f3ngsh\u00ed b\u01ceoli\u00fa z\u00echu\u00edgu\u012b ji\u00e0nm\u00f3 de y\u014dush\u00ec. W\u01d2men de gu\u01cenji\u00e0n d\u00f2ngji\u00e0n sh\u00ec b\u00ecngx\u00edng sh\u0113ngch\u00e9ng y\u012bl\u00e0i y\u00fa sh\u00ecju\u00e9 bi\u0101oj\u00ec de y\u012bl\u00e0i gu\u0101nx\u00ec: ru\u00f2 y\u012bl\u00e0i de bi\u0101oj\u00ec k\u011by\u01d0 b\u00ecngx\u00edng sh\u0113ngch\u00e9ng, \u00e9r qi\u00e1ng y\u012bl\u00e0i de xi\u0101ngl\u00edn bi\u0101oj\u00ec n\u00e1n y\u01d0 y\u012bq\u01d0 sh\u0113ngch\u00e9ng, y\u012bnw\u00e8i t\u0101men de d\u00fal\u00ec c\u01ceiy\u01ceng k\u011bn\u00e9ng d\u01ceozh\u00ec b\u00f9 y\u012bzh\u00ec. J\u012by\u00fa c\u01d0 gu\u0101nch\u00e1, w\u01d2men k\u0101if\u0101 le y\u012bzh\u01d2ng b\u00ecngx\u00edng sh\u0113ngch\u00e9ng c\u00e8l\u00fc\u00e8, b\u00ecngx\u00edng sh\u0113ngch\u00e9ng ru\u00f2 y\u012bl\u00e0i de yu\u01cen j\u00f9l\u00ed bi\u0101oj\u00ec, t\u00f3ngsh\u00ed b\u01ceoch\u00ed qi\u00e1ng y\u012bl\u00e0i de j\u00fab\u00f9 bi\u0101oj\u00ec de sh\u00f9nx\u00f9 sh\u0113ngch\u00e9ng. W\u01d2men de f\u0101ngf\u01ce k\u011by\u01d0 w\u00faf\u00e8ng j\u00edch\u00e9ng d\u00e0o bi\u0101ozh\u01d4n z\u00echu\u00edgu\u012b m\u00f3x\u00edng zh\u014dng, w\u00fax\u016b xi\u016bg\u01cei ji\u00e0g\u00f2u hu\u00f2 bi\u0101oj\u00ecq\u00ec. Z\u00e0i ImageNet h\u00e9 UCF-101 sh\u00e0ng de sh\u00edy\u00e0n bi\u01ceom\u00edng, w\u01d2men de f\u0101ngf\u01ce z\u00e0i t\u00faxi\u00e0ng h\u00e9 sh\u00ecp\u01d0n sh\u0113ngch\u00e9ng r\u00e8nw\u00f9 zh\u014dng sh\u00edxi\u00e0n le zu\u00ecg\u0101o 9.5 b\u00e8i de ji\u0101s\u00f9, t\u00f3ngsh\u00ed zh\u00ecli\u00e0ng s\u01d4nsh\u012b zu\u00ecsh\u01ceo. W\u01d2men x\u012bw\u00e0ng zh\u00e8 ji\u00e0n g\u014dngzu\u00f2 n\u00e9ng j\u012bf\u0101 w\u00e8il\u00e1i z\u00e0i g\u0101oxi\u00e0o sh\u00ecju\u00e9 sh\u0113ngch\u00e9ng h\u00e9 t\u01d2ngy\u012b z\u00echu\u00edgu\u012b ji\u00e0nm\u00f3 f\u0101ngmi\u00e0n de y\u00e1nji\u016b. Xi\u00e0ngm\u00f9 y\u00e8mi\u00e0n: https://epiphqny.github.io/PAR-project."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 376, "total_tokens": 1120, "completion_tokens": 744}}
[23.12.2024 09:11] Response: ZÃ¬ huÃ­guÄ« mÃ³xÃ­ng zÃ i shÃ¬juÃ© shÄ“ngchÃ©ng zhÅng biÇoxiÃ n chÅ« qiÃ¡ngdÃ  de xÃ¬ngnÃ©ng, dÃ n yÃ³uyÃº qÃ­ shÃ¹nxÃ¹ zhÃº biÄojÃ¬ yÃ¹cÃ¨ de guÃ²chÃ©ng, tuÄ«lÇ sÃ¹dÃ¹ jiÃ o mÃ n. BÄ›nwÃ©n tÃ­chÅ« le yÄ«zhÇ’ng jiÇndÄn yÇ’uxiÃ o de bÃ¬ngxÃ­ng zÃ¬huÃ­guÄ« shÃ¬juÃ© shÄ“ngchÃ©ng fÄngfÇ, tÃ­gÄo shÄ“ngchÃ©ng xiÃ olÇœ, tÃ³ngshÃ­ bÇoliÃº zÃ¬huÃ­guÄ« jiÃ nmÃ³ de yÅushÃ¬. WÇ’men de guÇnjiÃ n dÃ²ngjiÃ n shÃ¬ bÃ¬ngxÃ­ng shÄ“ngchÃ©ng yÄ«lÃ i yÃº shÃ¬juÃ© biÄojÃ¬ de yÄ«lÃ i guÄnxÃ¬: ruÃ² yÄ«lÃ i de biÄojÃ¬ kÄ›yÇ bÃ¬ngxÃ­ng shÄ“ngchÃ©ng, Ã©r qiÃ¡ng yÄ«lÃ i de xiÄnglÃ­n biÄojÃ¬ nÃ¡n yÇ yÄ«qÇ shÄ“ngchÃ©ng, yÄ«nwÃ¨i tÄmen de dÃºlÃ¬ cÇiyÇng kÄ›nÃ©ng dÇozhÃ¬ bÃ¹ yÄ«zhÃ¬. JÄ«yÃº cÇ guÄnchÃ¡, wÇ’men kÄifÄ le yÄ«zhÇ’ng bÃ¬ngxÃ­ng shÄ“ngchÃ©ng cÃ¨lÃ¼Ã¨, bÃ¬ngxÃ­ng shÄ“ngchÃ©ng ruÃ² yÄ«lÃ i de yuÇn jÃ¹lÃ­ biÄojÃ¬, tÃ³ngshÃ­ bÇochÃ­ qiÃ¡ng yÄ«lÃ i de jÃºbÃ¹ biÄojÃ¬ de shÃ¹nxÃ¹ shÄ“ngchÃ©ng. WÇ’men de fÄngfÇ kÄ›yÇ wÃºfÃ¨ng jÃ­chÃ©ng dÃ o biÄozhÇ”n zÃ¬huÃ­guÄ« mÃ³xÃ­ng zhÅng, wÃºxÅ« xiÅ«gÇi jiÃ gÃ²u huÃ² biÄojÃ¬qÃ¬. ZÃ i ImageNet hÃ© UCF-101 shÃ ng de shÃ­yÃ n biÇomÃ­ng, wÇ’men de fÄngfÇ zÃ i tÃºxiÃ ng hÃ© shÃ¬pÇn shÄ“ngchÃ©ng rÃ¨nwÃ¹ zhÅng shÃ­xiÃ n le zuÃ¬gÄo 9.5 bÃ¨i de jiÄsÃ¹, tÃ³ngshÃ­ zhÃ¬liÃ ng sÇ”nshÄ« zuÃ¬shÇo. WÇ’men xÄ«wÃ ng zhÃ¨ jiÃ n gÅngzuÃ² nÃ©ng jÄ«fÄ wÃ¨ilÃ¡i zÃ i gÄoxiÃ o shÃ¬juÃ© shÄ“ngchÃ©ng hÃ© tÇ’ngyÄ« zÃ¬huÃ­guÄ« jiÃ nmÃ³ fÄngmiÃ n de yÃ¡njiÅ«. XiÃ ngmÃ¹ yÃ¨miÃ n: https://epiphqny.github.io/PAR-project.
[23.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è‡ªå›å½’æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ç”±äºå…¶é¡ºåºé€æ ‡è®°é¢„æµ‹çš„è¿‡ç¨‹ï¼Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¹¶è¡Œè‡ªå›å½’è§†è§‰ç”Ÿæˆæ–¹æ³•ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡ï¼ŒåŒæ—¶ä¿ç•™è‡ªå›å½’å»ºæ¨¡çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„å…³é”®æ´è§æ˜¯å¹¶è¡Œç”Ÿæˆä¾èµ–äºè§†è§‰æ ‡è®°çš„ä¾èµ–å…³ç³»ï¼šå¼±ä¾èµ–çš„æ ‡è®°å¯ä»¥å¹¶è¡Œç”Ÿæˆï¼Œè€Œå¼ºä¾èµ–çš„ç›¸é‚»æ ‡è®°éš¾ä»¥ä¸€èµ·ç”Ÿæˆï¼Œå› ä¸ºå®ƒä»¬çš„ç‹¬ç«‹é‡‡æ ·å¯èƒ½å¯¼è‡´ä¸ä¸€è‡´ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¹¶è¡Œç”Ÿæˆç­–ç•¥ï¼Œå¹¶è¡Œç”Ÿæˆå¼±ä¾èµ–çš„è¿œè·ç¦»æ ‡è®°ï¼ŒåŒæ—¶ä¿æŒå¼ºä¾èµ–çš„å±€éƒ¨æ ‡è®°çš„é¡ºåºç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°æ ‡å‡†è‡ªå›å½’æ¨¡å‹ä¸­ï¼Œæ— éœ€ä¿®æ”¹æ¶æ„æˆ–æ ‡è®°å™¨ã€‚åœ¨ImageNetå’ŒUCF-101ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æœ€é«˜9.5å€çš„åŠ é€Ÿï¼ŒåŒæ—¶è´¨é‡æŸå¤±æœ€å°ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½æ¿€å‘æœªæ¥åœ¨é«˜æ•ˆè§†è§‰ç”Ÿæˆå’Œç»Ÿä¸€è‡ªå›å½’å»ºæ¨¡æ–¹é¢çš„ç ”ç©¶ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://epiphqny.github.io/PAR-projectã€‚
[23.12.2024 09:12] Mistral response. {"id": "b5e0525432794fed9c71f4e51cf20d37", "object": "chat.completion", "created": 1734945091, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u81ea\u56de\u5f52', 'pinyin': 'z\u00ec hu\u00ed gu\u012b', 'trans': 'autoregressive'},\n{'word': '\u89c6\u89c9', 'pinyin': 'sh\u00ec ju\u00e9', 'trans': 'visual'},\n{'word': '\u751f\u6210', 'pinyin': 'sh\u0113ng ch\u00e9ng', 'trans': 'generation'},\n{'word': '\u8868\u73b0', 'pinyin': 'bi\u01ceo xi\u00e0n', 'trans': 'performance'},\n{'word': '\u5f3a\u5927', 'pinyin': 'qi\u00e1ng d\u00e0', 'trans': 'powerful'},\n{'word': '\u6027\u80fd', 'pinyin': 'x\u00ecng n\u00e9ng', 'trans': 'performance'},\n{'word': '\u987a\u5e8f', 'pinyin': 'sh\u00f9n x\u00f9', 'trans': 'sequential'},\n{'word': '\u9010', 'pinyin': 'zh\u00fa', 'trans': 'gradual'},\n{'word': '\u6807\u8bb0', 'pinyin': 'bi\u0101o j\u00ec', 'trans': 'token'},\n{'word': '\u9884\u6d4b', 'pinyin': 'y\u00f9 c\u00e8', 'trans': 'prediction'},\n{'word': '\u63a8\u7406', 'pinyin': 'tu\u012b l\u01d0', 'trans': 'inference'},\n{'word': '\u901f\u5ea6', 'pinyin': 's\u00f9 d\u00f9', 'trans': 'speed'},\n{'word': '\u63d0\u51fa', 'pinyin': 't\u00ed ch\u016b', 'trans': 'propose'},\n{'word': '\u5e76\u884c', 'pinyin': 'b\u00ecng x\u00edng', 'trans': 'parallel'},\n{'word': '\u6709\u6548', 'pinyin': 'y\u01d2u xi\u00e0o', 'trans': 'effective'},\n{'word': '\u65b9\u6cd5', 'pinyin': 'f\u0101ng f\u01ce', 'trans': 'method'},\n{'word': '\u63d0\u9ad8', 'pinyin': 't\u00ed g\u0101o', 'trans': 'improve'},\n{'word': '\u6548\u7387', 'pinyin': 'xi\u00e0o l\u01dc', 'trans': 'efficiency'},\n{'word': '\u4fdd\u7559', 'pinyin': 'b\u01ceo li\u00fa', 'trans': 'retain'},\n{'word': '\u4f18\u52bf', 'pinyin': 'y\u014du sh\u00ec', 'trans': 'advantage'},\n{'word': '\u5173\u952e', 'pinyin': 'gu\u01cen ji\u00e0n', 'trans': 'key'},\n{'word': '\u6d1e\u89c1', 'pinyin': 'd\u00f2ng ji\u00e0n', 'trans': 'insight'},\n{'word': '\u4f9d\u8d56', 'pinyin': 'y\u012b l\u00e0i', 'trans': 'dependency'},\n{'word': '\u5173\u7cfb', 'pinyin': 'gu\u0101n x\u00ec', 'trans': 'relationship'},\n{'word': '\u5f31', 'pinyin': 'ru\u00f2', 'trans': 'weak'},\n{'word': '\u5f3a', 'pinyin': 'qi\u00e1ng', 'trans': 'strong'},\n{'word': '\u76f8\u90bb', 'pinyin': 'xi\u0101ng l\u00edn', 'trans': 'adjacent'},\n{'word': '\u96be\u4ee5', 'pinyin': 'n\u00e1n y\u01d0', 'trans': 'difficult'},\n{'word': '\u72ec\u7acb', 'pinyin': 'd\u00fa l\u00ec', 'trans': 'independent'},\n{'word': '\u91c7\u6837', 'pinyin': 'c\u01cei y\u00e0ng', 'trans': 'sampling'},\n{'word': '\u4e0d\u4e00\u81f4', 'pinyin': 'b\u00f9 y\u012b zh\u00ec', 'trans': 'inconsistency'},\n{'word': '\u7b56\u7565', 'pinyin': 'c\u00e8 l\u00fc\u00e8', 'trans': 'strategy'},\n{'word': '\u8fdc\u8ddd\u79bb', 'pinyin': 'yu\u01cen j\u00f9 l\u00ed', 'trans': 'long-distance'},\n{'word': '\u5c40\u90e8', 'pinyin': 'j\u00fa b\u00f9', 'trans': 'local'},\n{'word': '\u65e0\u7f1d', 'pinyin': 'w\u00fa f\u00e8ng', 'trans': 'seamless'},\n{'word': '\u96c6\u6210', 'pinyin': 'j\u00ed ch\u00e9ng', 'trans': 'integrate'},\n{'word': '\u6807\u51c6', 'pinyin': 'bi\u0101o zh\u01d4n', 'trans': 'standard'},\n{'word': '\u67b6\u6784', 'pinyin': 'ji\u00e0 g\u00f2u', 'trans': 'architecture'},\n{'word': '\u4fee\u6539', 'pinyin': 'xi\u016b g\u01cei', 'trans': 'modify'},\n{'word': '\u5b9e\u9a8c', 'pinyin': 'sh\u00ed y\u00e0n', 'trans': 'experiment'},\n{'word': '\u56fe\u50cf', 'pinyin': 't\u00fa xi\u00e0ng', 'trans': 'image'},\n{'word': '\u89c6\u9891', 'pinyin': 'sh\u00ec p\u00edn', 'trans': 'video'},\n{'word': '\u4efb\u52a1', 'pinyin': 'r\u00e8n w\u00f9', 'trans': 'task'},\n{'word': '\u5b9e\u73b0', 'pinyin': 'sh\u00ed xi\u00e0n', 'trans': 'achieve'},\n{'word': '\u52a0\u901f', 'pinyin': 'ji\u0101 s\u00f9', 'trans': 'acceleration'},\n{'word': '\u8d28\u91cf', 'pinyin': 'zh\u00ec li\u00e0ng', 'trans': 'quality'},\n{'word': '\u635f\u5931', 'pinyin': 's\u01d4n sh\u012b', 'trans': 'loss'},\n{'word': '\u6fc0\u53d1', 'pinyin': 'j\u012b f\u0101', 'trans': 'inspire'},\n{'word': '\u672a\u6765', 'pinyin': 'w\u00e8i l\u00e1i', 'trans': 'future'},\n{'word': '\u9ad8\u6548', 'pinyin': 'g\u0101o xi\u00e0o', 'trans': 'efficient'},\n{'word': '\u7edf\u4e00', 'pinyin': 't\u01d2ng y\u012b', 'trans': 'unified'},\n{'word': '\u9879\u76ee', 'pinyin': 'xi\u00e0ng m\u00f9', 'trans': 'project'},\n{'word': '\u9875\u9762', 'pinyin': 'y\u00e8 mi\u00e0n', 'trans': 'page'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 408, "total_tokens": 1845, "completion_tokens": 1437}}
[23.12.2024 09:12] Response: [{'word': 'è‡ªå›å½’', 'pinyin': 'zÃ¬ huÃ­ guÄ«', 'trans': 'autoregressive'},
{'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'},
{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generation'},
{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'},
{'word': 'å¼ºå¤§', 'pinyin': 'qiÃ¡ng dÃ ', 'trans': 'powerful'},
{'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'},
{'word': 'é¡ºåº', 'pinyin': 'shÃ¹n xÃ¹', 'trans': 'sequential'},
{'word': 'é€', 'pinyin': 'zhÃº', 'trans': 'gradual'},
{'word': 'æ ‡è®°', 'pinyin': 'biÄo jÃ¬', 'trans': 'token'},
{'word': 'é¢„æµ‹', 'pinyin': 'yÃ¹ cÃ¨', 'trans': 'prediction'},
{'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'inference'},
{'word': 'é€Ÿåº¦', 'pinyin': 'sÃ¹ dÃ¹', 'trans': 'speed'},
{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},
{'word': 'å¹¶è¡Œ', 'pinyin': 'bÃ¬ng xÃ­ng', 'trans': 'parallel'},
{'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’u xiÃ o', 'trans': 'effective'},
{'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'},
{'word': 'æé«˜', 'pinyin': 'tÃ­ gÄo', 'trans': 'improve'},
{'word': 'æ•ˆç‡', 'pinyin': 'xiÃ o lÇœ', 'trans': 'efficiency'},
{'word': 'ä¿ç•™', 'pinyin': 'bÇo liÃº', 'trans': 'retain'},
{'word': 'ä¼˜åŠ¿', 'pinyin': 'yÅu shÃ¬', 'trans': 'advantage'},
{'word': 'å…³é”®', 'pinyin': 'guÇn jiÃ n', 'trans': 'key'},
{'word': 'æ´è§', 'pinyin': 'dÃ²ng jiÃ n', 'trans': 'insight'},
{'word': 'ä¾èµ–', 'pinyin': 'yÄ« lÃ i', 'trans': 'dependency'},
{'word': 'å…³ç³»', 'pinyin': 'guÄn xÃ¬', 'trans': 'relationship'},
{'word': 'å¼±', 'pinyin': 'ruÃ²', 'trans': 'weak'},
{'word': 'å¼º', 'pinyin': 'qiÃ¡ng', 'trans': 'strong'},
{'word': 'ç›¸é‚»', 'pinyin': 'xiÄng lÃ­n', 'trans': 'adjacent'},
{'word': 'éš¾ä»¥', 'pinyin': 'nÃ¡n yÇ', 'trans': 'difficult'},
{'word': 'ç‹¬ç«‹', 'pinyin': 'dÃº lÃ¬', 'trans': 'independent'},
{'word': 'é‡‡æ ·', 'pinyin': 'cÇi yÃ ng', 'trans': 'sampling'},
{'word': 'ä¸ä¸€è‡´', 'pinyin': 'bÃ¹ yÄ« zhÃ¬', 'trans': 'inconsistency'},
{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'},
{'word': 'è¿œè·ç¦»', 'pinyin': 'yuÇn jÃ¹ lÃ­', 'trans': 'long-distance'},
{'word': 'å±€éƒ¨', 'pinyin': 'jÃº bÃ¹', 'trans': 'local'},
{'word': 'æ— ç¼', 'pinyin': 'wÃº fÃ¨ng', 'trans': 'seamless'},
{'word': 'é›†æˆ', 'pinyin': 'jÃ­ chÃ©ng', 'trans': 'integrate'},
{'word': 'æ ‡å‡†', 'pinyin': 'biÄo zhÇ”n', 'trans': 'standard'},
{'word': 'æ¶æ„', 'pinyin': 'jiÃ  gÃ²u', 'trans': 'architecture'},
{'word': 'ä¿®æ”¹', 'pinyin': 'xiÅ« gÇi', 'trans': 'modify'},
{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},
{'word': 'å›¾åƒ', 'pinyin': 'tÃº xiÃ ng', 'trans': 'image'},
{'word': 'è§†é¢‘', 'pinyin': 'shÃ¬ pÃ­n', 'trans': 'video'},
{'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wÃ¹', 'trans': 'task'},
{'word': 'å®ç°', 'pinyin': 'shÃ­ xiÃ n', 'trans': 'achieve'},
{'word': 'åŠ é€Ÿ', 'pinyin': 'jiÄ sÃ¹', 'trans': 'acceleration'},
{'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'},
{'word': 'æŸå¤±', 'pinyin': 'sÇ”n shÄ«', 'trans': 'loss'},
{'word': 'æ¿€å‘', 'pinyin': 'jÄ« fÄ', 'trans': 'inspire'},
{'word': 'æœªæ¥', 'pinyin': 'wÃ¨i lÃ¡i', 'trans': 'future'},
{'word': 'é«˜æ•ˆ', 'pinyin': 'gÄo xiÃ o', 'trans': 'efficient'},
{'word': 'ç»Ÿä¸€', 'pinyin': 'tÇ’ng yÄ«', 'trans': 'unified'},
{'word': 'é¡¹ç›®', 'pinyin': 'xiÃ ng mÃ¹', 'trans': 'project'},
{'word': 'é¡µé¢', 'pinyin': 'yÃ¨ miÃ n', 'trans': 'page'}]
[23.12.2024 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è‡ªå›å½’æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ç”±äºå…¶é¡ºåºé€æ ‡è®°é¢„æµ‹çš„è¿‡ç¨‹ï¼Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¹¶è¡Œè‡ªå›å½’è§†è§‰ç”Ÿæˆæ–¹æ³•ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡ï¼ŒåŒæ—¶ä¿ç•™è‡ªå›å½’å»ºæ¨¡çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„å…³é”®æ´è§æ˜¯å¹¶è¡Œç”Ÿæˆä¾èµ–äºè§†è§‰æ ‡è®°çš„ä¾èµ–å…³ç³»ï¼šå¼±ä¾èµ–çš„æ ‡è®°å¯ä»¥å¹¶è¡Œç”Ÿæˆï¼Œè€Œå¼ºä¾èµ–çš„ç›¸é‚»æ ‡è®°éš¾ä»¥ä¸€èµ·ç”Ÿæˆï¼Œå› ä¸ºå®ƒä»¬çš„ç‹¬ç«‹é‡‡æ ·å¯èƒ½å¯¼è‡´ä¸ä¸€è‡´ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¹¶è¡Œç”Ÿæˆç­–ç•¥ï¼Œå¹¶è¡Œç”Ÿæˆå¼±ä¾èµ–çš„è¿œè·ç¦»æ ‡è®°ï¼ŒåŒæ—¶ä¿æŒå¼ºä¾èµ–çš„å±€éƒ¨æ ‡è®°çš„é¡ºåºç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°æ ‡å‡†è‡ªå›å½’æ¨¡å‹ä¸­ï¼Œæ— éœ€ä¿®æ”¹æ¶æ„æˆ–æ ‡è®°å™¨ã€‚åœ¨ImageNetå’ŒUCF-101ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æœ€é«˜9.5å€çš„åŠ é€Ÿï¼ŒåŒæ—¶è´¨é‡æŸå¤±æœ€å°ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½æ¿€å‘æœªæ¥åœ¨é«˜æ•ˆè§†è§‰ç”Ÿæˆå’Œç»Ÿä¸€è‡ªå›å½’å»ºæ¨¡æ–¹é¢çš„ç ”ç©¶ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://epiphqny.github.io/PAR-projectã€‚
[23.12.2024 09:12] Mistral response. {"id": "e1f5b1cca0e54cb7ad1ff79b19f5fb82", "object": "chat.completion", "created": 1734945121, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Autoregressive models have demonstrated strong performance in visual generation, but their sequential, token-by-token prediction process results in slow inference speeds. This paper proposes a simple and effective parallel autoregressive visual generation method that improves generation efficiency while retaining the advantages of autoregressive modeling. Our key insight is that parallel generation depends on the dependency relationships among visual tokens: weakly dependent tokens can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together because their independent sampling may lead to inconsistencies. Based on this observation, we developed a parallel generation strategy that generates weakly dependent distant tokens in parallel while maintaining the sequential generation of strongly dependent local tokens. Our method can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 show that our method achieves up to a 9.5x speedup in image and video generation tasks with minimal quality loss. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 374, "total_tokens": 619, "completion_tokens": 245}}
[23.12.2024 09:12] Response: Autoregressive models have demonstrated strong performance in visual generation, but their sequential, token-by-token prediction process results in slow inference speeds. This paper proposes a simple and effective parallel autoregressive visual generation method that improves generation efficiency while retaining the advantages of autoregressive modeling. Our key insight is that parallel generation depends on the dependency relationships among visual tokens: weakly dependent tokens can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together because their independent sampling may lead to inconsistencies. Based on this observation, we developed a parallel generation strategy that generates weakly dependent distant tokens in parallel while maintaining the sequential generation of strongly dependent local tokens. Our method can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 show that our method achieves up to a 9.5x speedup in image and video generation tasks with minimal quality loss. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.
[23.12.2024 09:12] Renaming data file.
[23.12.2024 09:12] Renaming previous data. hf_papers.json to ./d/2024-12-23.json
[23.12.2024 09:12] Saving new data file.
[23.12.2024 09:12] Generating page.
[23.12.2024 09:12] Renaming previous page.
[23.12.2024 09:12] Renaming previous data. index.html to ./d/2024-12-23.html
[23.12.2024 09:12] [Experimental] Generating Chinese page for reading.
[23.12.2024 09:12] Chinese vocab [{'word': 'è‡ªå›å½’', 'pinyin': 'zÃ¬ huÃ­ guÄ«', 'trans': 'autoregressive'}, {'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generation'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'}, {'word': 'å¼ºå¤§', 'pinyin': 'qiÃ¡ng dÃ ', 'trans': 'powerful'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'}, {'word': 'é¡ºåº', 'pinyin': 'shÃ¹n xÃ¹', 'trans': 'sequential'}, {'word': 'é€', 'pinyin': 'zhÃº', 'trans': 'gradual'}, {'word': 'æ ‡è®°', 'pinyin': 'biÄo jÃ¬', 'trans': 'token'}, {'word': 'é¢„æµ‹', 'pinyin': 'yÃ¹ cÃ¨', 'trans': 'prediction'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'inference'}, {'word': 'é€Ÿåº¦', 'pinyin': 'sÃ¹ dÃ¹', 'trans': 'speed'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'å¹¶è¡Œ', 'pinyin': 'bÃ¬ng xÃ­ng', 'trans': 'parallel'}, {'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’u xiÃ o', 'trans': 'effective'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}, {'word': 'æé«˜', 'pinyin': 'tÃ­ gÄo', 'trans': 'improve'}, {'word': 'æ•ˆç‡', 'pinyin': 'xiÃ o lÇœ', 'trans': 'efficiency'}, {'word': 'ä¿ç•™', 'pinyin': 'bÇo liÃº', 'trans': 'retain'}, {'word': 'ä¼˜åŠ¿', 'pinyin': 'yÅu shÃ¬', 'trans': 'advantage'}, {'word': 'å…³é”®', 'pinyin': 'guÇn jiÃ n', 'trans': 'key'}, {'word': 'æ´è§', 'pinyin': 'dÃ²ng jiÃ n', 'trans': 'insight'}, {'word': 'ä¾èµ–', 'pinyin': 'yÄ« lÃ i', 'trans': 'dependency'}, {'word': 'å…³ç³»', 'pinyin': 'guÄn xÃ¬', 'trans': 'relationship'}, {'word': 'å¼±', 'pinyin': 'ruÃ²', 'trans': 'weak'}, {'word': 'å¼º', 'pinyin': 'qiÃ¡ng', 'trans': 'strong'}, {'word': 'ç›¸é‚»', 'pinyin': 'xiÄng lÃ­n', 'trans': 'adjacent'}, {'word': 'éš¾ä»¥', 'pinyin': 'nÃ¡n yÇ', 'trans': 'difficult'}, {'word': 'ç‹¬ç«‹', 'pinyin': 'dÃº lÃ¬', 'trans': 'independent'}, {'word': 'é‡‡æ ·', 'pinyin': 'cÇi yÃ ng', 'trans': 'sampling'}, {'word': 'ä¸ä¸€è‡´', 'pinyin': 'bÃ¹ yÄ« zhÃ¬', 'trans': 'inconsistency'}, {'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨ lÃ¼Ã¨', 'trans': 'strategy'}, {'word': 'è¿œè·ç¦»', 'pinyin': 'yuÇn jÃ¹ lÃ­', 'trans': 'long-distance'}, {'word': 'å±€éƒ¨', 'pinyin': 'jÃº bÃ¹', 'trans': 'local'}, {'word': 'æ— ç¼', 'pinyin': 'wÃº fÃ¨ng', 'trans': 'seamless'}, {'word': 'é›†æˆ', 'pinyin': 'jÃ­ chÃ©ng', 'trans': 'integrate'}, {'word': 'æ ‡å‡†', 'pinyin': 'biÄo zhÇ”n', 'trans': 'standard'}, {'word': 'æ¶æ„', 'pinyin': 'jiÃ  gÃ²u', 'trans': 'architecture'}, {'word': 'ä¿®æ”¹', 'pinyin': 'xiÅ« gÇi', 'trans': 'modify'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'å›¾åƒ', 'pinyin': 'tÃº xiÃ ng', 'trans': 'image'}, {'word': 'è§†é¢‘', 'pinyin': 'shÃ¬ pÃ­n', 'trans': 'video'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wÃ¹', 'trans': 'task'}, {'word': 'å®ç°', 'pinyin': 'shÃ­ xiÃ n', 'trans': 'achieve'}, {'word': 'åŠ é€Ÿ', 'pinyin': 'jiÄ sÃ¹', 'trans': 'acceleration'}, {'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'}, {'word': 'æŸå¤±', 'pinyin': 'sÇ”n shÄ«', 'trans': 'loss'}, {'word': 'æ¿€å‘', 'pinyin': 'jÄ« fÄ', 'trans': 'inspire'}, {'word': 'æœªæ¥', 'pinyin': 'wÃ¨i lÃ¡i', 'trans': 'future'}, {'word': 'é«˜æ•ˆ', 'pinyin': 'gÄo xiÃ o', 'trans': 'efficient'}, {'word': 'ç»Ÿä¸€', 'pinyin': 'tÇ’ng yÄ«', 'trans': 'unified'}, {'word': 'é¡¹ç›®', 'pinyin': 'xiÃ ng mÃ¹', 'trans': 'project'}, {'word': 'é¡µé¢', 'pinyin': 'yÃ¨ miÃ n', 'trans': 'page'}]
[23.12.2024 09:12] Renaming previous Chinese page.
[23.12.2024 09:12] Renaming previous data. zh.html to ./d/2024-12-22_zh_reading_task.html
[23.12.2024 09:12] Writing Chinese reading task.
[23.12.2024 09:12] Writing result.
[23.12.2024 09:12] Renaming log file.
[23.12.2024 09:12] Renaming previous data. log.txt to ./logs/2024-12-23_last_log.txt
