[23.12.2024 08:14] Read previous papers.
[23.12.2024 08:14] Generating top page (month).
[23.12.2024 08:14] Writing top page (month).
[23.12.2024 09:11] Read previous papers.
[23.12.2024 09:11] Get feed.
[23.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.15119
[23.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.13649
[23.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.16145
[23.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.16112
[23.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.15322
[23.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.15450
[23.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.14590
[23.12.2024 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.12.2024 09:11] No deleted papers detected.
[23.12.2024 09:11] Downloading and parsing papers (pdf, html). Total: 7.
[23.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.15119.
[23.12.2024 09:11] Extra JSON file exists (./assets/json/2412.15119.json), skip PDF parsing.
[23.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.15119.json), skip HTML parsing.
[23.12.2024 09:11] Success.
[23.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.13649.
[23.12.2024 09:11] Extra JSON file exists (./assets/json/2412.13649.json), skip PDF parsing.
[23.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.13649.json), skip HTML parsing.
[23.12.2024 09:11] Success.
[23.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.16145.
[23.12.2024 09:11] Extra JSON file exists (./assets/json/2412.16145.json), skip PDF parsing.
[23.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.16145.json), skip HTML parsing.
[23.12.2024 09:11] Success.
[23.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.16112.
[23.12.2024 09:11] Extra JSON file exists (./assets/json/2412.16112.json), skip PDF parsing.
[23.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.16112.json), skip HTML parsing.
[23.12.2024 09:11] Success.
[23.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.15322.
[23.12.2024 09:11] Extra JSON file exists (./assets/json/2412.15322.json), skip PDF parsing.
[23.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.15322.json), skip HTML parsing.
[23.12.2024 09:11] Success.
[23.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.15450.
[23.12.2024 09:11] Extra JSON file exists (./assets/json/2412.15450.json), skip PDF parsing.
[23.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.15450.json), skip HTML parsing.
[23.12.2024 09:11] Success.
[23.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.14590.
[23.12.2024 09:11] Extra JSON file exists (./assets/json/2412.14590.json), skip PDF parsing.
[23.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.14590.json), skip HTML parsing.
[23.12.2024 09:11] Success.
[23.12.2024 09:11] Enriching papers with extra data.
[23.12.2024 09:11] ********************************************************************************
[23.12.2024 09:11] Abstract 0. Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves ge...
[23.12.2024 09:11] ********************************************************************************
[23.12.2024 09:11] Abstract 1. Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the follow...
[23.12.2024 09:11] ********************************************************************************
[23.12.2024 09:11] Abstract 2. Improving the multi-step reasoning ability of large language models (LLMs) with offline reinforcement learning (RL) is essential for quickly adapting them to complex tasks. While Direct Preference Optimization (DPO) has shown promise in aligning LLMs with human preferences, it is less suitable for m...
[23.12.2024 09:11] ********************************************************************************
[23.12.2024 09:11] Abstract 3. Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, w...
[23.12.2024 09:11] ********************************************************************************
[23.12.2024 09:11] Abstract 4. We propose to synthesize high-quality and synchronized audio, given video and optional text conditions, using a novel multimodal joint training framework MMAudio. In contrast to single-modality training conditioned on (limited) video data only, MMAudio is jointly trained with larger-scale, readily a...
[23.12.2024 09:11] ********************************************************************************
[23.12.2024 09:11] Abstract 5. This paper introduces Fietje, a family of small language models (SLMs) specifically designed for the Dutch language. The model is based on Phi 2, an English-centric model of 2.7 billion parameters. Fietje demonstrated competitive results with larger language models upon its release. A core emphasis ...
[23.12.2024 09:11] ********************************************************************************
[23.12.2024 09:11] Abstract 6. Quantization has become one of the most effective methodologies to compress LLMs into smaller size. However, the existing quantization solutions still show limitations of either non-negligible accuracy drop or system inefficiency. In this paper, we make a comprehensive analysis of the general quanti...
[23.12.2024 09:11] Read previous papers.
[23.12.2024 09:11] Generating reviews via LLM API.
[23.12.2024 09:11] Using data from previous issue: {"categories": ["#training", "#inference", "#video", "#cv", "#architecture", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ
[23.12.2024 09:11] Using data from previous issue: {"categories": ["#training", "#long_context", "#inference", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è KV-–∫—ç—à–∞ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SCOPE - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ KV-–∫—ç—à–∞ –≤ –º–æ–¥–µ–ª—è—Ö LLM –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª
[23.12.2024 09:11] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#optimization", "#math", "#rlhf", "#agents", "#reasoning"], "emoji": "üß†", "ru": {"title": "OREO: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ OREO (Offline Reasoning Optimization) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π
[23.12.2024 09:11] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#training", "#inference", "#architecture", "#diffusion"], "emoji": "üöÄ", "ru": {"title": "CLEAR: –£—Å–∫–æ—Ä–µ–Ω–∏–µ DiT –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è Diffusion Transformers (DiT), –Ω–∞–∑—ã–≤–∞–µ–º—ã–π CLEAR. 
[23.12.2024 09:11] Using data from previous issue: {"categories": ["#small_models", "#audio", "#inference", "#video", "#multimodal", "#synthetic"], "emoji": "üéµ", "ru": {"title": "MMAudio: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∏–Ω—Ç–µ–∑–µ –∞—É–¥–∏–æ –ø–æ –≤–∏–¥–µ–æ –∏ —Ç–µ–∫—Å—Ç—É", "desc": "MMAudio - —ç—Ç–æ –Ω–æ–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ –Ω–∞ –æ
[23.12.2024 09:11] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#reasoning", "#open_source", "#small_models", "#benchmark"], "emoji": "üá≥üá±", "ru": {"title": "Fietje: –û—Ç–∫—Ä—ã—Ç–∞—è –º–∞–ª–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∏–¥–µ—Ä–ª–∞–Ω–¥—Å–∫–æ–≥–æ —è–∑—ã–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Fietje - —Å–µ–º–µ–π—Å—Ç–≤–æ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (SLM), —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±
[23.12.2024 09:11] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training"], "emoji": "üß†", "ru": {"title": "MixLLM: –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º MixLLM. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–º–µ—à–∞–Ω–Ω—É—é 
[23.12.2024 09:11] Trying to get texts in Chinese.
[23.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves generation efficiency while preserving the advantages of autoregressive modeling. Our key insight is that parallel generation depends on visual token dependencies-tokens with weak dependencies can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together, as their independent sampling may lead to inconsistencies. Based on this observation, we develop a parallel generation strategy that generates distant tokens with weak dependencies in parallel while maintaining sequential generation for strongly dependent local tokens. Our approach can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that our method achieves a 3.6x speedup with comparable quality and up to 9.5x speedup with minimal quality degradation across both image and video generation tasks. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.
[23.12.2024 09:11] Mistral response. {"id": "b7b55f3db91a45ca99bf39e8b52f9be0", "object": "chat.completion", "created": 1734945069, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u89c9\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u5176\u987a\u5e8f\u9010\u6807\u8bb0\u9884\u6d4b\u7684\u8fc7\u7a0b\uff0c\u63a8\u7406\u901f\u5ea6\u8f83\u6162\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u5e76\u884c\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u65b9\u6cd5\uff0c\u63d0\u9ad8\u751f\u6210\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u7559\u81ea\u56de\u5f52\u5efa\u6a21\u7684\u4f18\u52bf\u3002\u6211\u4eec\u7684\u5173\u952e\u6d1e\u89c1\u662f\u5e76\u884c\u751f\u6210\u4f9d\u8d56\u4e8e\u89c6\u89c9\u6807\u8bb0\u7684\u4f9d\u8d56\u5173\u7cfb\uff1a\u5f31\u4f9d\u8d56\u7684\u6807\u8bb0\u53ef\u4ee5\u5e76\u884c\u751f\u6210\uff0c\u800c\u5f3a\u4f9d\u8d56\u7684\u76f8\u90bb\u6807\u8bb0\u96be\u4ee5\u4e00\u8d77\u751f\u6210\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u72ec\u7acb\u91c7\u6837\u53ef\u80fd\u5bfc\u81f4\u4e0d\u4e00\u81f4\u3002\u57fa\u4e8e\u6b64\u89c2\u5bdf\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u5e76\u884c\u751f\u6210\u7b56\u7565\uff0c\u5e76\u884c\u751f\u6210\u5f31\u4f9d\u8d56\u7684\u8fdc\u8ddd\u79bb\u6807\u8bb0\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u4f9d\u8d56\u7684\u5c40\u90e8\u6807\u8bb0\u7684\u987a\u5e8f\u751f\u6210\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u6807\u51c6\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\uff0c\u65e0\u9700\u4fee\u6539\u67b6\u6784\u6216\u6807\u8bb0\u5668\u3002\u5728ImageNet\u548cUCF-101\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad89.5\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u8d28\u91cf\u635f\u5931\u6700\u5c0f\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u6fc0\u53d1\u672a\u6765\u5728\u9ad8\u6548\u89c6\u89c9\u751f\u6210\u548c\u7edf\u4e00\u81ea\u56de\u5f52\u5efa\u6a21\u65b9\u9762\u7684\u7814\u7a76\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://epiphqny.github.io/PAR-project\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 290, "total_tokens": 651, "completion_tokens": 361}}
[23.12.2024 09:11] Response: Ëá™ÂõûÂΩíÊ®°ÂûãÂú®ËßÜËßâÁîüÊàê‰∏≠Ë°®Áé∞Âá∫Âº∫Â§ßÁöÑÊÄßËÉΩÔºå‰ΩÜÁî±‰∫éÂÖ∂È°∫Â∫èÈÄêÊ†áËÆ∞È¢ÑÊµãÁöÑËøáÁ®ãÔºåÊé®ÁêÜÈÄüÂ∫¶ËæÉÊÖ¢„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÂπ∂Ë°åËá™ÂõûÂΩíËßÜËßâÁîüÊàêÊñπÊ≥ïÔºåÊèêÈ´òÁîüÊàêÊïàÁéáÔºåÂêåÊó∂‰øùÁïôËá™ÂõûÂΩíÂª∫Ê®°ÁöÑ‰ºòÂäø„ÄÇÊàë‰ª¨ÁöÑÂÖ≥ÈîÆÊ¥ûËßÅÊòØÂπ∂Ë°åÁîüÊàê‰æùËµñ‰∫éËßÜËßâÊ†áËÆ∞ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºöÂº±‰æùËµñÁöÑÊ†áËÆ∞ÂèØ‰ª•Âπ∂Ë°åÁîüÊàêÔºåËÄåÂº∫‰æùËµñÁöÑÁõ∏ÈÇªÊ†áËÆ∞Èöæ‰ª•‰∏ÄËµ∑ÁîüÊàêÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÁöÑÁã¨Á´ãÈááÊ†∑ÂèØËÉΩÂØºËá¥‰∏ç‰∏ÄËá¥„ÄÇÂü∫‰∫éÊ≠§ËßÇÂØüÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçÂπ∂Ë°åÁîüÊàêÁ≠ñÁï•ÔºåÂπ∂Ë°åÁîüÊàêÂº±‰æùËµñÁöÑËøúË∑ùÁ¶ªÊ†áËÆ∞ÔºåÂêåÊó∂‰øùÊåÅÂº∫‰æùËµñÁöÑÂ±ÄÈÉ®Ê†áËÆ∞ÁöÑÈ°∫Â∫èÁîüÊàê„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞Ê†áÂáÜËá™ÂõûÂΩíÊ®°Âûã‰∏≠ÔºåÊó†ÈúÄ‰øÆÊîπÊû∂ÊûÑÊàñÊ†áËÆ∞Âô®„ÄÇÂú®ImageNetÂíåUCF-101‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàê‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÈ´ò9.5ÂÄçÁöÑÂä†ÈÄüÔºåÂêåÊó∂Ë¥®ÈáèÊçüÂ§±ÊúÄÂ∞è„ÄÇÊàë‰ª¨Â∏åÊúõËøôÈ°πÂ∑•‰ΩúËÉΩÊøÄÂèëÊú™Êù•Âú®È´òÊïàËßÜËßâÁîüÊàêÂíåÁªü‰∏ÄËá™ÂõûÂΩíÂª∫Ê®°ÊñπÈù¢ÁöÑÁ†îÁ©∂„ÄÇÈ°πÁõÆÈ°µÈù¢Ôºöhttps://epiphqny.github.io/PAR-project„ÄÇ
[23.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

Ëá™ÂõûÂΩíÊ®°ÂûãÂú®ËßÜËßâÁîüÊàê‰∏≠Ë°®Áé∞Âá∫Âº∫Â§ßÁöÑÊÄßËÉΩÔºå‰ΩÜÁî±‰∫éÂÖ∂È°∫Â∫èÈÄêÊ†áËÆ∞È¢ÑÊµãÁöÑËøáÁ®ãÔºåÊé®ÁêÜÈÄüÂ∫¶ËæÉÊÖ¢„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÂπ∂Ë°åËá™ÂõûÂΩíËßÜËßâÁîüÊàêÊñπÊ≥ïÔºåÊèêÈ´òÁîüÊàêÊïàÁéáÔºåÂêåÊó∂‰øùÁïôËá™ÂõûÂΩíÂª∫Ê®°ÁöÑ‰ºòÂäø„ÄÇÊàë‰ª¨ÁöÑÂÖ≥ÈîÆÊ¥ûËßÅÊòØÂπ∂Ë°åÁîüÊàê‰æùËµñ‰∫éËßÜËßâÊ†áËÆ∞ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºöÂº±‰æùËµñÁöÑÊ†áËÆ∞ÂèØ‰ª•Âπ∂Ë°åÁîüÊàêÔºåËÄåÂº∫‰æùËµñÁöÑÁõ∏ÈÇªÊ†áËÆ∞Èöæ‰ª•‰∏ÄËµ∑ÁîüÊàêÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÁöÑÁã¨Á´ãÈááÊ†∑ÂèØËÉΩÂØºËá¥‰∏ç‰∏ÄËá¥„ÄÇÂü∫‰∫éÊ≠§ËßÇÂØüÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçÂπ∂Ë°åÁîüÊàêÁ≠ñÁï•ÔºåÂπ∂Ë°åÁîüÊàêÂº±‰æùËµñÁöÑËøúË∑ùÁ¶ªÊ†áËÆ∞ÔºåÂêåÊó∂‰øùÊåÅÂº∫‰æùËµñÁöÑÂ±ÄÈÉ®Ê†áËÆ∞ÁöÑÈ°∫Â∫èÁîüÊàê„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞Ê†áÂáÜËá™ÂõûÂΩíÊ®°Âûã‰∏≠ÔºåÊó†ÈúÄ‰øÆÊîπÊû∂ÊûÑÊàñÊ†áËÆ∞Âô®„ÄÇÂú®ImageNetÂíåUCF-101‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàê‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÈ´ò9.5ÂÄçÁöÑÂä†ÈÄüÔºåÂêåÊó∂Ë¥®ÈáèÊçüÂ§±ÊúÄÂ∞è„ÄÇÊàë‰ª¨Â∏åÊúõËøôÈ°πÂ∑•‰ΩúËÉΩÊøÄÂèëÊú™Êù•Âú®È´òÊïàËßÜËßâÁîüÊàêÂíåÁªü‰∏ÄËá™ÂõûÂΩíÂª∫Ê®°ÊñπÈù¢ÁöÑÁ†îÁ©∂„ÄÇÈ°πÁõÆÈ°µÈù¢Ôºöhttps://epiphqny.github.io/PAR-project„ÄÇ
[23.12.2024 09:11] Mistral response. {"id": "46a6c95f1ac449a8b0a90333bed78ce9", "object": "chat.completion", "created": 1734945076, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Z\u00ec hu\u00edgu\u012b m\u00f3x\u00edng z\u00e0i sh\u00ecju\u00e9 sh\u0113ngch\u00e9ng zh\u014dng bi\u01ceoxi\u00e0n ch\u016b qi\u00e1ngd\u00e0 de x\u00ecngn\u00e9ng, d\u00e0n y\u00f3uy\u00fa q\u00ed sh\u00f9nx\u00f9 zh\u00fa bi\u0101oj\u00ec y\u00f9c\u00e8 de gu\u00f2ch\u00e9ng, tu\u012bl\u01d0 s\u00f9d\u00f9 ji\u00e0o m\u00e0n. B\u011bnw\u00e9n t\u00edch\u016b le y\u012bzh\u01d2ng ji\u01cend\u0101n y\u01d2uxi\u00e0o de b\u00ecngx\u00edng z\u00echu\u00edgu\u012b sh\u00ecju\u00e9 sh\u0113ngch\u00e9ng f\u0101ngf\u01ce, t\u00edg\u0101o sh\u0113ngch\u00e9ng xi\u00e0ol\u01dc, t\u00f3ngsh\u00ed b\u01ceoli\u00fa z\u00echu\u00edgu\u012b ji\u00e0nm\u00f3 de y\u014dush\u00ec. W\u01d2men de gu\u01cenji\u00e0n d\u00f2ngji\u00e0n sh\u00ec b\u00ecngx\u00edng sh\u0113ngch\u00e9ng y\u012bl\u00e0i y\u00fa sh\u00ecju\u00e9 bi\u0101oj\u00ec de y\u012bl\u00e0i gu\u0101nx\u00ec: ru\u00f2 y\u012bl\u00e0i de bi\u0101oj\u00ec k\u011by\u01d0 b\u00ecngx\u00edng sh\u0113ngch\u00e9ng, \u00e9r qi\u00e1ng y\u012bl\u00e0i de xi\u0101ngl\u00edn bi\u0101oj\u00ec n\u00e1n y\u01d0 y\u012bq\u01d0 sh\u0113ngch\u00e9ng, y\u012bnw\u00e8i t\u0101men de d\u00fal\u00ec c\u01ceiy\u01ceng k\u011bn\u00e9ng d\u01ceozh\u00ec b\u00f9 y\u012bzh\u00ec. J\u012by\u00fa c\u01d0 gu\u0101nch\u00e1, w\u01d2men k\u0101if\u0101 le y\u012bzh\u01d2ng b\u00ecngx\u00edng sh\u0113ngch\u00e9ng c\u00e8l\u00fc\u00e8, b\u00ecngx\u00edng sh\u0113ngch\u00e9ng ru\u00f2 y\u012bl\u00e0i de yu\u01cen j\u00f9l\u00ed bi\u0101oj\u00ec, t\u00f3ngsh\u00ed b\u01ceoch\u00ed qi\u00e1ng y\u012bl\u00e0i de j\u00fab\u00f9 bi\u0101oj\u00ec de sh\u00f9nx\u00f9 sh\u0113ngch\u00e9ng. W\u01d2men de f\u0101ngf\u01ce k\u011by\u01d0 w\u00faf\u00e8ng j\u00edch\u00e9ng d\u00e0o bi\u0101ozh\u01d4n z\u00echu\u00edgu\u012b m\u00f3x\u00edng zh\u014dng, w\u00fax\u016b xi\u016bg\u01cei ji\u00e0g\u00f2u hu\u00f2 bi\u0101oj\u00ecq\u00ec. Z\u00e0i ImageNet h\u00e9 UCF-101 sh\u00e0ng de sh\u00edy\u00e0n bi\u01ceom\u00edng, w\u01d2men de f\u0101ngf\u01ce z\u00e0i t\u00faxi\u00e0ng h\u00e9 sh\u00ecp\u01d0n sh\u0113ngch\u00e9ng r\u00e8nw\u00f9 zh\u014dng sh\u00edxi\u00e0n le zu\u00ecg\u0101o 9.5 b\u00e8i de ji\u0101s\u00f9, t\u00f3ngsh\u00ed zh\u00ecli\u00e0ng s\u01d4nsh\u012b zu\u00ecsh\u01ceo. W\u01d2men x\u012bw\u00e0ng zh\u00e8 ji\u00e0n g\u014dngzu\u00f2 n\u00e9ng j\u012bf\u0101 w\u00e8il\u00e1i z\u00e0i g\u0101oxi\u00e0o sh\u00ecju\u00e9 sh\u0113ngch\u00e9ng h\u00e9 t\u01d2ngy\u012b z\u00echu\u00edgu\u012b ji\u00e0nm\u00f3 f\u0101ngmi\u00e0n de y\u00e1nji\u016b. Xi\u00e0ngm\u00f9 y\u00e8mi\u00e0n: https://epiphqny.github.io/PAR-project."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 376, "total_tokens": 1120, "completion_tokens": 744}}
[23.12.2024 09:11] Response: Z√¨ hu√≠guƒ´ m√≥x√≠ng z√†i sh√¨ju√© shƒìngch√©ng zh≈çng bi«éoxi√†n ch≈´ qi√°ngd√† de x√¨ngn√©ng, d√†n y√≥uy√∫ q√≠ sh√πnx√π zh√∫ biƒÅoj√¨ y√πc√® de gu√≤ch√©ng, tuƒ´l«ê s√πd√π ji√†o m√†n. Bƒõnw√©n t√≠ch≈´ le yƒ´zh«íng ji«éndƒÅn y«íuxi√†o de b√¨ngx√≠ng z√¨hu√≠guƒ´ sh√¨ju√© shƒìngch√©ng fƒÅngf«é, t√≠gƒÅo shƒìngch√©ng xi√†ol«ú, t√≥ngsh√≠ b«éoli√∫ z√¨hu√≠guƒ´ ji√†nm√≥ de y≈çush√¨. W«ímen de gu«énji√†n d√≤ngji√†n sh√¨ b√¨ngx√≠ng shƒìngch√©ng yƒ´l√†i y√∫ sh√¨ju√© biƒÅoj√¨ de yƒ´l√†i guƒÅnx√¨: ru√≤ yƒ´l√†i de biƒÅoj√¨ kƒõy«ê b√¨ngx√≠ng shƒìngch√©ng, √©r qi√°ng yƒ´l√†i de xiƒÅngl√≠n biƒÅoj√¨ n√°n y«ê yƒ´q«ê shƒìngch√©ng, yƒ´nw√®i tƒÅmen de d√∫l√¨ c«éiy«éng kƒõn√©ng d«éozh√¨ b√π yƒ´zh√¨. Jƒ´y√∫ c«ê guƒÅnch√°, w«ímen kƒÅifƒÅ le yƒ´zh«íng b√¨ngx√≠ng shƒìngch√©ng c√®l√º√®, b√¨ngx√≠ng shƒìngch√©ng ru√≤ yƒ´l√†i de yu«én j√πl√≠ biƒÅoj√¨, t√≥ngsh√≠ b«éoch√≠ qi√°ng yƒ´l√†i de j√∫b√π biƒÅoj√¨ de sh√πnx√π shƒìngch√©ng. W«ímen de fƒÅngf«é kƒõy«ê w√∫f√®ng j√≠ch√©ng d√†o biƒÅozh«în z√¨hu√≠guƒ´ m√≥x√≠ng zh≈çng, w√∫x≈´ xi≈´g«éi ji√†g√≤u hu√≤ biƒÅoj√¨q√¨. Z√†i ImageNet h√© UCF-101 sh√†ng de sh√≠y√†n bi«éom√≠ng, w«ímen de fƒÅngf«é z√†i t√∫xi√†ng h√© sh√¨p«ên shƒìngch√©ng r√®nw√π zh≈çng sh√≠xi√†n le zu√¨gƒÅo 9.5 b√®i de jiƒÅs√π, t√≥ngsh√≠ zh√¨li√†ng s«înshƒ´ zu√¨sh«éo. W«ímen xƒ´w√†ng zh√® ji√†n g≈çngzu√≤ n√©ng jƒ´fƒÅ w√®il√°i z√†i gƒÅoxi√†o sh√¨ju√© shƒìngch√©ng h√© t«íngyƒ´ z√¨hu√≠guƒ´ ji√†nm√≥ fƒÅngmi√†n de y√°nji≈´. Xi√†ngm√π y√®mi√†n: https://epiphqny.github.io/PAR-project.
[23.12.2024 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

Ëá™ÂõûÂΩíÊ®°ÂûãÂú®ËßÜËßâÁîüÊàê‰∏≠Ë°®Áé∞Âá∫Âº∫Â§ßÁöÑÊÄßËÉΩÔºå‰ΩÜÁî±‰∫éÂÖ∂È°∫Â∫èÈÄêÊ†áËÆ∞È¢ÑÊµãÁöÑËøáÁ®ãÔºåÊé®ÁêÜÈÄüÂ∫¶ËæÉÊÖ¢„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÂπ∂Ë°åËá™ÂõûÂΩíËßÜËßâÁîüÊàêÊñπÊ≥ïÔºåÊèêÈ´òÁîüÊàêÊïàÁéáÔºåÂêåÊó∂‰øùÁïôËá™ÂõûÂΩíÂª∫Ê®°ÁöÑ‰ºòÂäø„ÄÇÊàë‰ª¨ÁöÑÂÖ≥ÈîÆÊ¥ûËßÅÊòØÂπ∂Ë°åÁîüÊàê‰æùËµñ‰∫éËßÜËßâÊ†áËÆ∞ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºöÂº±‰æùËµñÁöÑÊ†áËÆ∞ÂèØ‰ª•Âπ∂Ë°åÁîüÊàêÔºåËÄåÂº∫‰æùËµñÁöÑÁõ∏ÈÇªÊ†áËÆ∞Èöæ‰ª•‰∏ÄËµ∑ÁîüÊàêÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÁöÑÁã¨Á´ãÈááÊ†∑ÂèØËÉΩÂØºËá¥‰∏ç‰∏ÄËá¥„ÄÇÂü∫‰∫éÊ≠§ËßÇÂØüÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçÂπ∂Ë°åÁîüÊàêÁ≠ñÁï•ÔºåÂπ∂Ë°åÁîüÊàêÂº±‰æùËµñÁöÑËøúË∑ùÁ¶ªÊ†áËÆ∞ÔºåÂêåÊó∂‰øùÊåÅÂº∫‰æùËµñÁöÑÂ±ÄÈÉ®Ê†áËÆ∞ÁöÑÈ°∫Â∫èÁîüÊàê„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞Ê†áÂáÜËá™ÂõûÂΩíÊ®°Âûã‰∏≠ÔºåÊó†ÈúÄ‰øÆÊîπÊû∂ÊûÑÊàñÊ†áËÆ∞Âô®„ÄÇÂú®ImageNetÂíåUCF-101‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàê‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÈ´ò9.5ÂÄçÁöÑÂä†ÈÄüÔºåÂêåÊó∂Ë¥®ÈáèÊçüÂ§±ÊúÄÂ∞è„ÄÇÊàë‰ª¨Â∏åÊúõËøôÈ°πÂ∑•‰ΩúËÉΩÊøÄÂèëÊú™Êù•Âú®È´òÊïàËßÜËßâÁîüÊàêÂíåÁªü‰∏ÄËá™ÂõûÂΩíÂª∫Ê®°ÊñπÈù¢ÁöÑÁ†îÁ©∂„ÄÇÈ°πÁõÆÈ°µÈù¢Ôºöhttps://epiphqny.github.io/PAR-project„ÄÇ
[23.12.2024 09:12] Mistral response. {"id": "b5e0525432794fed9c71f4e51cf20d37", "object": "chat.completion", "created": 1734945091, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u81ea\u56de\u5f52', 'pinyin': 'z\u00ec hu\u00ed gu\u012b', 'trans': 'autoregressive'},\n{'word': '\u89c6\u89c9', 'pinyin': 'sh\u00ec ju\u00e9', 'trans': 'visual'},\n{'word': '\u751f\u6210', 'pinyin': 'sh\u0113ng ch\u00e9ng', 'trans': 'generation'},\n{'word': '\u8868\u73b0', 'pinyin': 'bi\u01ceo xi\u00e0n', 'trans': 'performance'},\n{'word': '\u5f3a\u5927', 'pinyin': 'qi\u00e1ng d\u00e0', 'trans': 'powerful'},\n{'word': '\u6027\u80fd', 'pinyin': 'x\u00ecng n\u00e9ng', 'trans': 'performance'},\n{'word': '\u987a\u5e8f', 'pinyin': 'sh\u00f9n x\u00f9', 'trans': 'sequential'},\n{'word': '\u9010', 'pinyin': 'zh\u00fa', 'trans': 'gradual'},\n{'word': '\u6807\u8bb0', 'pinyin': 'bi\u0101o j\u00ec', 'trans': 'token'},\n{'word': '\u9884\u6d4b', 'pinyin': 'y\u00f9 c\u00e8', 'trans': 'prediction'},\n{'word': '\u63a8\u7406', 'pinyin': 'tu\u012b l\u01d0', 'trans': 'inference'},\n{'word': '\u901f\u5ea6', 'pinyin': 's\u00f9 d\u00f9', 'trans': 'speed'},\n{'word': '\u63d0\u51fa', 'pinyin': 't\u00ed ch\u016b', 'trans': 'propose'},\n{'word': '\u5e76\u884c', 'pinyin': 'b\u00ecng x\u00edng', 'trans': 'parallel'},\n{'word': '\u6709\u6548', 'pinyin': 'y\u01d2u xi\u00e0o', 'trans': 'effective'},\n{'word': '\u65b9\u6cd5', 'pinyin': 'f\u0101ng f\u01ce', 'trans': 'method'},\n{'word': '\u63d0\u9ad8', 'pinyin': 't\u00ed g\u0101o', 'trans': 'improve'},\n{'word': '\u6548\u7387', 'pinyin': 'xi\u00e0o l\u01dc', 'trans': 'efficiency'},\n{'word': '\u4fdd\u7559', 'pinyin': 'b\u01ceo li\u00fa', 'trans': 'retain'},\n{'word': '\u4f18\u52bf', 'pinyin': 'y\u014du sh\u00ec', 'trans': 'advantage'},\n{'word': '\u5173\u952e', 'pinyin': 'gu\u01cen ji\u00e0n', 'trans': 'key'},\n{'word': '\u6d1e\u89c1', 'pinyin': 'd\u00f2ng ji\u00e0n', 'trans': 'insight'},\n{'word': '\u4f9d\u8d56', 'pinyin': 'y\u012b l\u00e0i', 'trans': 'dependency'},\n{'word': '\u5173\u7cfb', 'pinyin': 'gu\u0101n x\u00ec', 'trans': 'relationship'},\n{'word': '\u5f31', 'pinyin': 'ru\u00f2', 'trans': 'weak'},\n{'word': '\u5f3a', 'pinyin': 'qi\u00e1ng', 'trans': 'strong'},\n{'word': '\u76f8\u90bb', 'pinyin': 'xi\u0101ng l\u00edn', 'trans': 'adjacent'},\n{'word': '\u96be\u4ee5', 'pinyin': 'n\u00e1n y\u01d0', 'trans': 'difficult'},\n{'word': '\u72ec\u7acb', 'pinyin': 'd\u00fa l\u00ec', 'trans': 'independent'},\n{'word': '\u91c7\u6837', 'pinyin': 'c\u01cei y\u00e0ng', 'trans': 'sampling'},\n{'word': '\u4e0d\u4e00\u81f4', 'pinyin': 'b\u00f9 y\u012b zh\u00ec', 'trans': 'inconsistency'},\n{'word': '\u7b56\u7565', 'pinyin': 'c\u00e8 l\u00fc\u00e8', 'trans': 'strategy'},\n{'word': '\u8fdc\u8ddd\u79bb', 'pinyin': 'yu\u01cen j\u00f9 l\u00ed', 'trans': 'long-distance'},\n{'word': '\u5c40\u90e8', 'pinyin': 'j\u00fa b\u00f9', 'trans': 'local'},\n{'word': '\u65e0\u7f1d', 'pinyin': 'w\u00fa f\u00e8ng', 'trans': 'seamless'},\n{'word': '\u96c6\u6210', 'pinyin': 'j\u00ed ch\u00e9ng', 'trans': 'integrate'},\n{'word': '\u6807\u51c6', 'pinyin': 'bi\u0101o zh\u01d4n', 'trans': 'standard'},\n{'word': '\u67b6\u6784', 'pinyin': 'ji\u00e0 g\u00f2u', 'trans': 'architecture'},\n{'word': '\u4fee\u6539', 'pinyin': 'xi\u016b g\u01cei', 'trans': 'modify'},\n{'word': '\u5b9e\u9a8c', 'pinyin': 'sh\u00ed y\u00e0n', 'trans': 'experiment'},\n{'word': '\u56fe\u50cf', 'pinyin': 't\u00fa xi\u00e0ng', 'trans': 'image'},\n{'word': '\u89c6\u9891', 'pinyin': 'sh\u00ec p\u00edn', 'trans': 'video'},\n{'word': '\u4efb\u52a1', 'pinyin': 'r\u00e8n w\u00f9', 'trans': 'task'},\n{'word': '\u5b9e\u73b0', 'pinyin': 'sh\u00ed xi\u00e0n', 'trans': 'achieve'},\n{'word': '\u52a0\u901f', 'pinyin': 'ji\u0101 s\u00f9', 'trans': 'acceleration'},\n{'word': '\u8d28\u91cf', 'pinyin': 'zh\u00ec li\u00e0ng', 'trans': 'quality'},\n{'word': '\u635f\u5931', 'pinyin': 's\u01d4n sh\u012b', 'trans': 'loss'},\n{'word': '\u6fc0\u53d1', 'pinyin': 'j\u012b f\u0101', 'trans': 'inspire'},\n{'word': '\u672a\u6765', 'pinyin': 'w\u00e8i l\u00e1i', 'trans': 'future'},\n{'word': '\u9ad8\u6548', 'pinyin': 'g\u0101o xi\u00e0o', 'trans': 'efficient'},\n{'word': '\u7edf\u4e00', 'pinyin': 't\u01d2ng y\u012b', 'trans': 'unified'},\n{'word': '\u9879\u76ee', 'pinyin': 'xi\u00e0ng m\u00f9', 'trans': 'project'},\n{'word': '\u9875\u9762', 'pinyin': 'y\u00e8 mi\u00e0n', 'trans': 'page'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 408, "total_tokens": 1845, "completion_tokens": 1437}}
[23.12.2024 09:12] Response: [{'word': 'Ëá™ÂõûÂΩí', 'pinyin': 'z√¨ hu√≠ guƒ´', 'trans': 'autoregressive'},
{'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'visual'},
{'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generation'},
{'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'},
{'word': 'Âº∫Â§ß', 'pinyin': 'qi√°ng d√†', 'trans': 'powerful'},
{'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'},
{'word': 'È°∫Â∫è', 'pinyin': 'sh√πn x√π', 'trans': 'sequential'},
{'word': 'ÈÄê', 'pinyin': 'zh√∫', 'trans': 'gradual'},
{'word': 'Ê†áËÆ∞', 'pinyin': 'biƒÅo j√¨', 'trans': 'token'},
{'word': 'È¢ÑÊµã', 'pinyin': 'y√π c√®', 'trans': 'prediction'},
{'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'inference'},
{'word': 'ÈÄüÂ∫¶', 'pinyin': 's√π d√π', 'trans': 'speed'},
{'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'},
{'word': 'Âπ∂Ë°å', 'pinyin': 'b√¨ng x√≠ng', 'trans': 'parallel'},
{'word': 'ÊúâÊïà', 'pinyin': 'y«íu xi√†o', 'trans': 'effective'},
{'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'},
{'word': 'ÊèêÈ´ò', 'pinyin': 't√≠ gƒÅo', 'trans': 'improve'},
{'word': 'ÊïàÁéá', 'pinyin': 'xi√†o l«ú', 'trans': 'efficiency'},
{'word': '‰øùÁïô', 'pinyin': 'b«éo li√∫', 'trans': 'retain'},
{'word': '‰ºòÂäø', 'pinyin': 'y≈çu sh√¨', 'trans': 'advantage'},
{'word': 'ÂÖ≥ÈîÆ', 'pinyin': 'gu«én ji√†n', 'trans': 'key'},
{'word': 'Ê¥ûËßÅ', 'pinyin': 'd√≤ng ji√†n', 'trans': 'insight'},
{'word': '‰æùËµñ', 'pinyin': 'yƒ´ l√†i', 'trans': 'dependency'},
{'word': 'ÂÖ≥Á≥ª', 'pinyin': 'guƒÅn x√¨', 'trans': 'relationship'},
{'word': 'Âº±', 'pinyin': 'ru√≤', 'trans': 'weak'},
{'word': 'Âº∫', 'pinyin': 'qi√°ng', 'trans': 'strong'},
{'word': 'Áõ∏ÈÇª', 'pinyin': 'xiƒÅng l√≠n', 'trans': 'adjacent'},
{'word': 'Èöæ‰ª•', 'pinyin': 'n√°n y«ê', 'trans': 'difficult'},
{'word': 'Áã¨Á´ã', 'pinyin': 'd√∫ l√¨', 'trans': 'independent'},
{'word': 'ÈááÊ†∑', 'pinyin': 'c«éi y√†ng', 'trans': 'sampling'},
{'word': '‰∏ç‰∏ÄËá¥', 'pinyin': 'b√π yƒ´ zh√¨', 'trans': 'inconsistency'},
{'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'},
{'word': 'ËøúË∑ùÁ¶ª', 'pinyin': 'yu«én j√π l√≠', 'trans': 'long-distance'},
{'word': 'Â±ÄÈÉ®', 'pinyin': 'j√∫ b√π', 'trans': 'local'},
{'word': 'Êó†Áºù', 'pinyin': 'w√∫ f√®ng', 'trans': 'seamless'},
{'word': 'ÈõÜÊàê', 'pinyin': 'j√≠ ch√©ng', 'trans': 'integrate'},
{'word': 'Ê†áÂáÜ', 'pinyin': 'biƒÅo zh«în', 'trans': 'standard'},
{'word': 'Êû∂ÊûÑ', 'pinyin': 'ji√† g√≤u', 'trans': 'architecture'},
{'word': '‰øÆÊîπ', 'pinyin': 'xi≈´ g«éi', 'trans': 'modify'},
{'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'},
{'word': 'ÂõæÂÉè', 'pinyin': 't√∫ xi√†ng', 'trans': 'image'},
{'word': 'ËßÜÈ¢ë', 'pinyin': 'sh√¨ p√≠n', 'trans': 'video'},
{'word': '‰ªªÂä°', 'pinyin': 'r√®n w√π', 'trans': 'task'},
{'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'},
{'word': 'Âä†ÈÄü', 'pinyin': 'jiƒÅ s√π', 'trans': 'acceleration'},
{'word': 'Ë¥®Èáè', 'pinyin': 'zh√¨ li√†ng', 'trans': 'quality'},
{'word': 'ÊçüÂ§±', 'pinyin': 's«în shƒ´', 'trans': 'loss'},
{'word': 'ÊøÄÂèë', 'pinyin': 'jƒ´ fƒÅ', 'trans': 'inspire'},
{'word': 'Êú™Êù•', 'pinyin': 'w√®i l√°i', 'trans': 'future'},
{'word': 'È´òÊïà', 'pinyin': 'gƒÅo xi√†o', 'trans': 'efficient'},
{'word': 'Áªü‰∏Ä', 'pinyin': 't«íng yƒ´', 'trans': 'unified'},
{'word': 'È°πÁõÆ', 'pinyin': 'xi√†ng m√π', 'trans': 'project'},
{'word': 'È°µÈù¢', 'pinyin': 'y√® mi√†n', 'trans': 'page'}]
[23.12.2024 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

Ëá™ÂõûÂΩíÊ®°ÂûãÂú®ËßÜËßâÁîüÊàê‰∏≠Ë°®Áé∞Âá∫Âº∫Â§ßÁöÑÊÄßËÉΩÔºå‰ΩÜÁî±‰∫éÂÖ∂È°∫Â∫èÈÄêÊ†áËÆ∞È¢ÑÊµãÁöÑËøáÁ®ãÔºåÊé®ÁêÜÈÄüÂ∫¶ËæÉÊÖ¢„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑÂπ∂Ë°åËá™ÂõûÂΩíËßÜËßâÁîüÊàêÊñπÊ≥ïÔºåÊèêÈ´òÁîüÊàêÊïàÁéáÔºåÂêåÊó∂‰øùÁïôËá™ÂõûÂΩíÂª∫Ê®°ÁöÑ‰ºòÂäø„ÄÇÊàë‰ª¨ÁöÑÂÖ≥ÈîÆÊ¥ûËßÅÊòØÂπ∂Ë°åÁîüÊàê‰æùËµñ‰∫éËßÜËßâÊ†áËÆ∞ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºöÂº±‰æùËµñÁöÑÊ†áËÆ∞ÂèØ‰ª•Âπ∂Ë°åÁîüÊàêÔºåËÄåÂº∫‰æùËµñÁöÑÁõ∏ÈÇªÊ†áËÆ∞Èöæ‰ª•‰∏ÄËµ∑ÁîüÊàêÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÁöÑÁã¨Á´ãÈááÊ†∑ÂèØËÉΩÂØºËá¥‰∏ç‰∏ÄËá¥„ÄÇÂü∫‰∫éÊ≠§ËßÇÂØüÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçÂπ∂Ë°åÁîüÊàêÁ≠ñÁï•ÔºåÂπ∂Ë°åÁîüÊàêÂº±‰æùËµñÁöÑËøúË∑ùÁ¶ªÊ†áËÆ∞ÔºåÂêåÊó∂‰øùÊåÅÂº∫‰æùËµñÁöÑÂ±ÄÈÉ®Ê†áËÆ∞ÁöÑÈ°∫Â∫èÁîüÊàê„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂèØ‰ª•Êó†ÁºùÈõÜÊàêÂà∞Ê†áÂáÜËá™ÂõûÂΩíÊ®°Âûã‰∏≠ÔºåÊó†ÈúÄ‰øÆÊîπÊû∂ÊûÑÊàñÊ†áËÆ∞Âô®„ÄÇÂú®ImageNetÂíåUCF-101‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàê‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÈ´ò9.5ÂÄçÁöÑÂä†ÈÄüÔºåÂêåÊó∂Ë¥®ÈáèÊçüÂ§±ÊúÄÂ∞è„ÄÇÊàë‰ª¨Â∏åÊúõËøôÈ°πÂ∑•‰ΩúËÉΩÊøÄÂèëÊú™Êù•Âú®È´òÊïàËßÜËßâÁîüÊàêÂíåÁªü‰∏ÄËá™ÂõûÂΩíÂª∫Ê®°ÊñπÈù¢ÁöÑÁ†îÁ©∂„ÄÇÈ°πÁõÆÈ°µÈù¢Ôºöhttps://epiphqny.github.io/PAR-project„ÄÇ
[23.12.2024 09:12] Mistral response. {"id": "e1f5b1cca0e54cb7ad1ff79b19f5fb82", "object": "chat.completion", "created": 1734945121, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Autoregressive models have demonstrated strong performance in visual generation, but their sequential, token-by-token prediction process results in slow inference speeds. This paper proposes a simple and effective parallel autoregressive visual generation method that improves generation efficiency while retaining the advantages of autoregressive modeling. Our key insight is that parallel generation depends on the dependency relationships among visual tokens: weakly dependent tokens can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together because their independent sampling may lead to inconsistencies. Based on this observation, we developed a parallel generation strategy that generates weakly dependent distant tokens in parallel while maintaining the sequential generation of strongly dependent local tokens. Our method can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 show that our method achieves up to a 9.5x speedup in image and video generation tasks with minimal quality loss. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 374, "total_tokens": 619, "completion_tokens": 245}}
[23.12.2024 09:12] Response: Autoregressive models have demonstrated strong performance in visual generation, but their sequential, token-by-token prediction process results in slow inference speeds. This paper proposes a simple and effective parallel autoregressive visual generation method that improves generation efficiency while retaining the advantages of autoregressive modeling. Our key insight is that parallel generation depends on the dependency relationships among visual tokens: weakly dependent tokens can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together because their independent sampling may lead to inconsistencies. Based on this observation, we developed a parallel generation strategy that generates weakly dependent distant tokens in parallel while maintaining the sequential generation of strongly dependent local tokens. Our method can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 show that our method achieves up to a 9.5x speedup in image and video generation tasks with minimal quality loss. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.
[23.12.2024 09:12] Renaming data file.
[23.12.2024 09:12] Renaming previous data. hf_papers.json to ./d/2024-12-23.json
[23.12.2024 09:12] Saving new data file.
[23.12.2024 09:12] Generating page.
[23.12.2024 09:12] Renaming previous page.
[23.12.2024 09:12] Renaming previous data. index.html to ./d/2024-12-23.html
[23.12.2024 09:12] [Experimental] Generating Chinese page for reading.
[23.12.2024 09:12] Chinese vocab [{'word': 'Ëá™ÂõûÂΩí', 'pinyin': 'z√¨ hu√≠ guƒ´', 'trans': 'autoregressive'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'visual'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generation'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': 'Âº∫Â§ß', 'pinyin': 'qi√°ng d√†', 'trans': 'powerful'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'È°∫Â∫è', 'pinyin': 'sh√πn x√π', 'trans': 'sequential'}, {'word': 'ÈÄê', 'pinyin': 'zh√∫', 'trans': 'gradual'}, {'word': 'Ê†áËÆ∞', 'pinyin': 'biƒÅo j√¨', 'trans': 'token'}, {'word': 'È¢ÑÊµã', 'pinyin': 'y√π c√®', 'trans': 'prediction'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'inference'}, {'word': 'ÈÄüÂ∫¶', 'pinyin': 's√π d√π', 'trans': 'speed'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'Âπ∂Ë°å', 'pinyin': 'b√¨ng x√≠ng', 'trans': 'parallel'}, {'word': 'ÊúâÊïà', 'pinyin': 'y«íu xi√†o', 'trans': 'effective'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠ gƒÅo', 'trans': 'improve'}, {'word': 'ÊïàÁéá', 'pinyin': 'xi√†o l«ú', 'trans': 'efficiency'}, {'word': '‰øùÁïô', 'pinyin': 'b«éo li√∫', 'trans': 'retain'}, {'word': '‰ºòÂäø', 'pinyin': 'y≈çu sh√¨', 'trans': 'advantage'}, {'word': 'ÂÖ≥ÈîÆ', 'pinyin': 'gu«én ji√†n', 'trans': 'key'}, {'word': 'Ê¥ûËßÅ', 'pinyin': 'd√≤ng ji√†n', 'trans': 'insight'}, {'word': '‰æùËµñ', 'pinyin': 'yƒ´ l√†i', 'trans': 'dependency'}, {'word': 'ÂÖ≥Á≥ª', 'pinyin': 'guƒÅn x√¨', 'trans': 'relationship'}, {'word': 'Âº±', 'pinyin': 'ru√≤', 'trans': 'weak'}, {'word': 'Âº∫', 'pinyin': 'qi√°ng', 'trans': 'strong'}, {'word': 'Áõ∏ÈÇª', 'pinyin': 'xiƒÅng l√≠n', 'trans': 'adjacent'}, {'word': 'Èöæ‰ª•', 'pinyin': 'n√°n y«ê', 'trans': 'difficult'}, {'word': 'Áã¨Á´ã', 'pinyin': 'd√∫ l√¨', 'trans': 'independent'}, {'word': 'ÈááÊ†∑', 'pinyin': 'c«éi y√†ng', 'trans': 'sampling'}, {'word': '‰∏ç‰∏ÄËá¥', 'pinyin': 'b√π yƒ´ zh√¨', 'trans': 'inconsistency'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'ËøúË∑ùÁ¶ª', 'pinyin': 'yu«én j√π l√≠', 'trans': 'long-distance'}, {'word': 'Â±ÄÈÉ®', 'pinyin': 'j√∫ b√π', 'trans': 'local'}, {'word': 'Êó†Áºù', 'pinyin': 'w√∫ f√®ng', 'trans': 'seamless'}, {'word': 'ÈõÜÊàê', 'pinyin': 'j√≠ ch√©ng', 'trans': 'integrate'}, {'word': 'Ê†áÂáÜ', 'pinyin': 'biƒÅo zh«în', 'trans': 'standard'}, {'word': 'Êû∂ÊûÑ', 'pinyin': 'ji√† g√≤u', 'trans': 'architecture'}, {'word': '‰øÆÊîπ', 'pinyin': 'xi≈´ g«éi', 'trans': 'modify'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ÂõæÂÉè', 'pinyin': 't√∫ xi√†ng', 'trans': 'image'}, {'word': 'ËßÜÈ¢ë', 'pinyin': 'sh√¨ p√≠n', 'trans': 'video'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®n w√π', 'trans': 'task'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}, {'word': 'Âä†ÈÄü', 'pinyin': 'jiƒÅ s√π', 'trans': 'acceleration'}, {'word': 'Ë¥®Èáè', 'pinyin': 'zh√¨ li√†ng', 'trans': 'quality'}, {'word': 'ÊçüÂ§±', 'pinyin': 's«în shƒ´', 'trans': 'loss'}, {'word': 'ÊøÄÂèë', 'pinyin': 'jƒ´ fƒÅ', 'trans': 'inspire'}, {'word': 'Êú™Êù•', 'pinyin': 'w√®i l√°i', 'trans': 'future'}, {'word': 'È´òÊïà', 'pinyin': 'gƒÅo xi√†o', 'trans': 'efficient'}, {'word': 'Áªü‰∏Ä', 'pinyin': 't«íng yƒ´', 'trans': 'unified'}, {'word': 'È°πÁõÆ', 'pinyin': 'xi√†ng m√π', 'trans': 'project'}, {'word': 'È°µÈù¢', 'pinyin': 'y√® mi√†n', 'trans': 'page'}]
[23.12.2024 09:12] Renaming previous Chinese page.
[23.12.2024 09:12] Renaming previous data. zh.html to ./d/2024-12-22_zh_reading_task.html
[23.12.2024 09:12] Writing Chinese reading task.
[23.12.2024 09:12] Writing result.
[23.12.2024 09:12] Renaming log file.
[23.12.2024 09:12] Renaming previous data. log.txt to ./logs/2024-12-23_last_log.txt
