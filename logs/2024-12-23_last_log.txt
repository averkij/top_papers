[23.12.2024 06:14] Read previous papers.
[23.12.2024 06:14] Generating top page (month).
[23.12.2024 06:14] Writing top page (month).
[23.12.2024 07:10] Read previous papers.
[23.12.2024 07:10] Get feed.
[23.12.2024 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.15119
[23.12.2024 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.13649
[23.12.2024 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.16145
[23.12.2024 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.15322
[23.12.2024 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.16112
[23.12.2024 07:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.14590
[23.12.2024 07:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.12.2024 07:10] No deleted papers detected.
[23.12.2024 07:10] Downloading and parsing papers (pdf, html). Total: 6.
[23.12.2024 07:10] Downloading and parsing paper https://huggingface.co/papers/2412.15119.
[23.12.2024 07:10] Extra JSON file exists (./assets/json/2412.15119.json), skip PDF parsing.
[23.12.2024 07:10] Paper image links file exists (./assets/img_data/2412.15119.json), skip HTML parsing.
[23.12.2024 07:10] Success.
[23.12.2024 07:10] Downloading and parsing paper https://huggingface.co/papers/2412.13649.
[23.12.2024 07:10] Extra JSON file exists (./assets/json/2412.13649.json), skip PDF parsing.
[23.12.2024 07:10] Paper image links file exists (./assets/img_data/2412.13649.json), skip HTML parsing.
[23.12.2024 07:10] Success.
[23.12.2024 07:10] Downloading and parsing paper https://huggingface.co/papers/2412.16145.
[23.12.2024 07:10] Extra JSON file exists (./assets/json/2412.16145.json), skip PDF parsing.
[23.12.2024 07:10] Paper image links file exists (./assets/img_data/2412.16145.json), skip HTML parsing.
[23.12.2024 07:10] Success.
[23.12.2024 07:10] Downloading and parsing paper https://huggingface.co/papers/2412.15322.
[23.12.2024 07:10] Extra JSON file exists (./assets/json/2412.15322.json), skip PDF parsing.
[23.12.2024 07:10] Paper image links file exists (./assets/img_data/2412.15322.json), skip HTML parsing.
[23.12.2024 07:10] Success.
[23.12.2024 07:10] Downloading and parsing paper https://huggingface.co/papers/2412.16112.
[23.12.2024 07:10] Extra JSON file exists (./assets/json/2412.16112.json), skip PDF parsing.
[23.12.2024 07:10] Paper image links file exists (./assets/img_data/2412.16112.json), skip HTML parsing.
[23.12.2024 07:10] Success.
[23.12.2024 07:10] Downloading and parsing paper https://huggingface.co/papers/2412.14590.
[23.12.2024 07:10] Extra JSON file exists (./assets/json/2412.14590.json), skip PDF parsing.
[23.12.2024 07:10] Paper image links file exists (./assets/img_data/2412.14590.json), skip HTML parsing.
[23.12.2024 07:10] Success.
[23.12.2024 07:10] Enriching papers with extra data.
[23.12.2024 07:10] ********************************************************************************
[23.12.2024 07:10] Abstract 0. Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves ge...
[23.12.2024 07:10] ********************************************************************************
[23.12.2024 07:10] Abstract 1. Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the follow...
[23.12.2024 07:10] ********************************************************************************
[23.12.2024 07:10] Abstract 2. Improving the multi-step reasoning ability of large language models (LLMs) with offline reinforcement learning (RL) is essential for quickly adapting them to complex tasks. While Direct Preference Optimization (DPO) has shown promise in aligning LLMs with human preferences, it is less suitable for m...
[23.12.2024 07:10] ********************************************************************************
[23.12.2024 07:10] Abstract 3. We propose to synthesize high-quality and synchronized audio, given video and optional text conditions, using a novel multimodal joint training framework MMAudio. In contrast to single-modality training conditioned on (limited) video data only, MMAudio is jointly trained with larger-scale, readily a...
[23.12.2024 07:10] ********************************************************************************
[23.12.2024 07:10] Abstract 4. Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, w...
[23.12.2024 07:10] ********************************************************************************
[23.12.2024 07:10] Abstract 5. Quantization has become one of the most effective methodologies to compress LLMs into smaller size. However, the existing quantization solutions still show limitations of either non-negligible accuracy drop or system inefficiency. In this paper, we make a comprehensive analysis of the general quanti...
[23.12.2024 07:10] Read previous papers.
[23.12.2024 07:10] Generating reviews via LLM API.
[23.12.2024 07:10] Using data from previous issue: {"categories": ["#training", "#inference", "#video", "#cv", "#architecture", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ
[23.12.2024 07:10] Using data from previous issue: {"categories": ["#training", "#long_context", "#inference", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è KV-–∫—ç—à–∞ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SCOPE - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ KV-–∫—ç—à–∞ –≤ –º–æ–¥–µ–ª—è—Ö LLM –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª
[23.12.2024 07:10] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#optimization", "#math", "#rlhf", "#agents", "#reasoning"], "emoji": "üß†", "ru": {"title": "OREO: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ OREO (Offline Reasoning Optimization) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π
[23.12.2024 07:10] Using data from previous issue: {"categories": ["#small_models", "#audio", "#inference", "#video", "#multimodal", "#synthetic"], "emoji": "üéµ", "ru": {"title": "MMAudio: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∏–Ω—Ç–µ–∑–µ –∞—É–¥–∏–æ –ø–æ –≤–∏–¥–µ–æ –∏ —Ç–µ–∫—Å—Ç—É", "desc": "MMAudio - —ç—Ç–æ –Ω–æ–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ –Ω–∞ –æ
[23.12.2024 07:10] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#training", "#inference", "#architecture", "#diffusion"], "emoji": "üöÄ", "ru": {"title": "CLEAR: –£—Å–∫–æ—Ä–µ–Ω–∏–µ DiT –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è Diffusion Transformers (DiT), –Ω–∞–∑—ã–≤–∞–µ–º—ã–π CLEAR. 
[23.12.2024 07:10] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training"], "emoji": "üß†", "ru": {"title": "MixLLM: –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º MixLLM. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–º–µ—à–∞–Ω–Ω—É—é 
[23.12.2024 07:10] Loading Chinese text from previous data.
[23.12.2024 07:10] Renaming data file.
[23.12.2024 07:10] Renaming previous data. hf_papers.json to ./d/2024-12-23.json
[23.12.2024 07:10] Saving new data file.
[23.12.2024 07:10] Generating page.
[23.12.2024 07:10] Renaming previous page.
[23.12.2024 07:10] Renaming previous data. index.html to ./d/2024-12-23.html
[23.12.2024 07:10] [Experimental] Generating Chinese page for reading.
[23.12.2024 07:10] Chinese vocab [{'word': 'ÁØá', 'pinyin': 'piƒÅn', 'trans': 'piece of writing'}, {'word': '‰ªãÁªç', 'pinyin': 'ji√®sh√†o', 'trans': 'introduce'}, {'word': 'ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'y«îy√°n m√≥x√≠ng', 'trans': 'language model'}, {'word': 'Ââç‰∏ÄÁâàÊú¨', 'pinyin': 'qi√°n yƒ´ b«énbƒõn', 'trans': 'previous version'}, {'word': 'Áõ∏ÊØî', 'pinyin': 'xiƒÅngb«ê', 'trans': 'compare'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πnli√†n', 'trans': 'pre-training'}, {'word': 'ÂêéËÆ≠ÁªÉ', 'pinyin': 'h√≤u x√πnli√†n', 'trans': 'post-training'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'}, {'word': 'ÊîπËøõ', 'pinyin': 'g«éij√¨n', 'trans': 'improvement'}, {'word': 'Ê†áËÆ∞', 'pinyin': 'biƒÅoj√¨', 'trans': 'token'}, {'word': 'Êâ©Â±ï', 'pinyin': 'ku√≤zh«én', 'trans': 'expand'}, {'word': 'Â∏∏ËØÜ', 'pinyin': 'ch√°ngsh√≠', 'trans': 'common sense'}, {'word': '‰∏ì‰∏öÁü•ËØÜ', 'pinyin': 'zhuƒÅny√® zhƒ´shi', 'trans': 'professional knowledge'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability'}, {'word': 'ÁõëÁù£', 'pinyin': 'ji√†nd≈´', 'trans': 'supervised'}, {'word': 'ÂæÆË∞É', 'pinyin': 'wƒìiti√°o', 'trans': 'fine-tuning'}, {'word': 'Â§öÈò∂ÊÆµ', 'pinyin': 'du≈ç jiƒìdu√†n', 'trans': 'multi-stage'}, {'word': 'Âº∫ÂåñÂ≠¶‰π†', 'pinyin': 'qi√°ngji√† xu√©x√≠', 'trans': 'reinforcement learning'}, {'word': 'ÊèêÂçá', 'pinyin': 't√≠shƒìng', 'trans': 'enhance'}, {'word': '‰∫∫Á±ªÂÅèÂ•Ω', 'pinyin': 'r√©nl√®i piƒÅnh√†o', 'trans': 'human preference'}, {'word': 'ÈïøÊñáÊú¨', 'pinyin': 'ch√°ng w√©nbƒõn', 'trans': 'long text'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generation'}, {'word': 'Â∞∫ÂØ∏', 'pinyin': 'ch«êc√πn', 'trans': 'size'}, {'word': 'Âü∫Á°ÄÊ®°Âûã', 'pinyin': 'jƒ´ch«î m√≥x√≠ng', 'trans': 'base model'}, {'word': 'Êåá‰ª§', 'pinyin': 'zh«êl√¨ng', 'trans': 'instruction'}, {'word': 'ÈáèÂåñ', 'pinyin': 'li√†nghu√†', 'trans': 'quantization'}]
[23.12.2024 07:10] Renaming previous Chinese page.
[23.12.2024 07:10] Renaming previous data. zh.html to ./d/2024-12-22_zh_reading_task.html
[23.12.2024 07:10] Writing Chinese reading task.
[23.12.2024 07:10] Writing result.
[23.12.2024 07:10] Renaming log file.
[23.12.2024 07:10] Renaming previous data. log.txt to ./logs/2024-12-23_last_log.txt
