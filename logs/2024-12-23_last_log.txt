[23.12.2024 14:09] Read previous papers.
[23.12.2024 14:09] Generating top page (month).
[23.12.2024 14:09] Writing top page (month).
[23.12.2024 15:10] Read previous papers.
[23.12.2024 15:10] Get feed.
[23.12.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.15119
[23.12.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.16145
[23.12.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.13649
[23.12.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.16112
[23.12.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.15322
[23.12.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.14590
[23.12.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.15035
[23.12.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.11525
[23.12.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.15450
[23.12.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.14963
[23.12.2024 15:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.12.2024 15:10] No deleted papers detected.
[23.12.2024 15:10] Downloading and parsing papers (pdf, html). Total: 10.
[23.12.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2412.15119.
[23.12.2024 15:10] Extra JSON file exists (./assets/json/2412.15119.json), skip PDF parsing.
[23.12.2024 15:10] Paper image links file exists (./assets/img_data/2412.15119.json), skip HTML parsing.
[23.12.2024 15:10] Success.
[23.12.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2412.16145.
[23.12.2024 15:10] Extra JSON file exists (./assets/json/2412.16145.json), skip PDF parsing.
[23.12.2024 15:10] Paper image links file exists (./assets/img_data/2412.16145.json), skip HTML parsing.
[23.12.2024 15:10] Success.
[23.12.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2412.13649.
[23.12.2024 15:10] Extra JSON file exists (./assets/json/2412.13649.json), skip PDF parsing.
[23.12.2024 15:10] Paper image links file exists (./assets/img_data/2412.13649.json), skip HTML parsing.
[23.12.2024 15:10] Success.
[23.12.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2412.16112.
[23.12.2024 15:10] Extra JSON file exists (./assets/json/2412.16112.json), skip PDF parsing.
[23.12.2024 15:10] Paper image links file exists (./assets/img_data/2412.16112.json), skip HTML parsing.
[23.12.2024 15:10] Success.
[23.12.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2412.15322.
[23.12.2024 15:10] Extra JSON file exists (./assets/json/2412.15322.json), skip PDF parsing.
[23.12.2024 15:10] Paper image links file exists (./assets/img_data/2412.15322.json), skip HTML parsing.
[23.12.2024 15:10] Success.
[23.12.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2412.14590.
[23.12.2024 15:10] Extra JSON file exists (./assets/json/2412.14590.json), skip PDF parsing.
[23.12.2024 15:10] Paper image links file exists (./assets/img_data/2412.14590.json), skip HTML parsing.
[23.12.2024 15:10] Success.
[23.12.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2412.15035.
[23.12.2024 15:10] Extra JSON file exists (./assets/json/2412.15035.json), skip PDF parsing.
[23.12.2024 15:10] Paper image links file exists (./assets/img_data/2412.15035.json), skip HTML parsing.
[23.12.2024 15:10] Success.
[23.12.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2412.11525.
[23.12.2024 15:10] Extra JSON file exists (./assets/json/2412.11525.json), skip PDF parsing.
[23.12.2024 15:10] Paper image links file exists (./assets/img_data/2412.11525.json), skip HTML parsing.
[23.12.2024 15:10] Success.
[23.12.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2412.15450.
[23.12.2024 15:10] Extra JSON file exists (./assets/json/2412.15450.json), skip PDF parsing.
[23.12.2024 15:10] Paper image links file exists (./assets/img_data/2412.15450.json), skip HTML parsing.
[23.12.2024 15:10] Success.
[23.12.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2412.14963.
[23.12.2024 15:10] Extra JSON file exists (./assets/json/2412.14963.json), skip PDF parsing.
[23.12.2024 15:10] Paper image links file exists (./assets/img_data/2412.14963.json), skip HTML parsing.
[23.12.2024 15:10] Success.
[23.12.2024 15:10] Enriching papers with extra data.
[23.12.2024 15:10] ********************************************************************************
[23.12.2024 15:10] Abstract 0. Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves ge...
[23.12.2024 15:10] ********************************************************************************
[23.12.2024 15:10] Abstract 1. Improving the multi-step reasoning ability of large language models (LLMs) with offline reinforcement learning (RL) is essential for quickly adapting them to complex tasks. While Direct Preference Optimization (DPO) has shown promise in aligning LLMs with human preferences, it is less suitable for m...
[23.12.2024 15:10] ********************************************************************************
[23.12.2024 15:10] Abstract 2. Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the follow...
[23.12.2024 15:10] ********************************************************************************
[23.12.2024 15:10] Abstract 3. Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, w...
[23.12.2024 15:10] ********************************************************************************
[23.12.2024 15:10] Abstract 4. We propose to synthesize high-quality and synchronized audio, given video and optional text conditions, using a novel multimodal joint training framework MMAudio. In contrast to single-modality training conditioned on (limited) video data only, MMAudio is jointly trained with larger-scale, readily a...
[23.12.2024 15:10] ********************************************************************************
[23.12.2024 15:10] Abstract 5. Quantization has become one of the most effective methodologies to compress LLMs into smaller size. However, the existing quantization solutions still show limitations of either non-negligible accuracy drop or system inefficiency. In this paper, we make a comprehensive analysis of the general quanti...
[23.12.2024 15:10] ********************************************************************************
[23.12.2024 15:10] Abstract 6. Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-A...
[23.12.2024 15:10] ********************************************************************************
[23.12.2024 15:10] Abstract 7. 3D super-resolution aims to reconstruct high-fidelity 3D models from low-resolution (LR) multi-view images. Early studies primarily focused on single-image super-resolution (SISR) models to upsample LR images into high-resolution images. However, these methods often lack view consistency because the...
[23.12.2024 15:10] ********************************************************************************
[23.12.2024 15:10] Abstract 8. This paper introduces Fietje, a family of small language models (SLMs) specifically designed for the Dutch language. The model is based on Phi 2, an English-centric model of 2.7 billion parameters. Fietje demonstrated competitive results with larger language models upon its release. A core emphasis ...
[23.12.2024 15:10] ********************************************************************************
[23.12.2024 15:10] Abstract 9. Creating a high-fidelity, animatable 3D full-body avatar from a single image is a challenging task due to the diverse appearance and poses of humans and the limited availability of high-quality training data. To achieve fast and high-quality human reconstruction, this work rethinks the task from the...
[23.12.2024 15:10] Read previous papers.
[23.12.2024 15:10] Generating reviews via LLM API.
[23.12.2024 15:10] Using data from previous issue: {"categories": ["#training", "#inference", "#video", "#cv", "#architecture", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ
[23.12.2024 15:10] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#optimization", "#math", "#rlhf", "#agents", "#reasoning"], "emoji": "üß†", "ru": {"title": "OREO: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ OREO (Offline Reasoning Optimization) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π
[23.12.2024 15:10] Using data from previous issue: {"categories": ["#training", "#long_context", "#inference", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è KV-–∫—ç—à–∞ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SCOPE - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ KV-–∫—ç—à–∞ –≤ –º–æ–¥–µ–ª—è—Ö LLM –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª
[23.12.2024 15:10] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#training", "#inference", "#architecture", "#diffusion"], "emoji": "üöÄ", "ru": {"title": "CLEAR: –£—Å–∫–æ—Ä–µ–Ω–∏–µ DiT –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è Diffusion Transformers (DiT), –Ω–∞–∑—ã–≤–∞–µ–º—ã–π CLEAR. 
[23.12.2024 15:10] Using data from previous issue: {"categories": ["#small_models", "#audio", "#inference", "#video", "#multimodal", "#synthetic"], "emoji": "üéµ", "ru": {"title": "MMAudio: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∏–Ω—Ç–µ–∑–µ –∞—É–¥–∏–æ –ø–æ –≤–∏–¥–µ–æ –∏ —Ç–µ–∫—Å—Ç—É", "desc": "MMAudio - —ç—Ç–æ –Ω–æ–≤–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ –Ω–∞ –æ
[23.12.2024 15:10] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training"], "emoji": "üß†", "ru": {"title": "MixLLM: –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º MixLLM. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–º–µ—à–∞–Ω–Ω—É—é 
[23.12.2024 15:10] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#ethics", "#benchmark"], "emoji": "üåê", "ru": {"title": "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ LLM: –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç M-ALERT - –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞
[23.12.2024 15:10] Using data from previous issue: {"categories": ["#video", "#3d", "#benchmark"], "emoji": "üîç", "ru": {"title": "–í–∏–¥–µ–æ-—Å—É–ø–µ—Ä—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ 3D —Å—É–ø–µ—Ä—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—é - –º–µ—Ç–æ–¥—É —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö 3D –º–æ–¥–µ–ª–µ–π –∏–∑ –Ω–∏–∑–∫–æ—Ä–∞–∑—Ä–µ—à–∞—é—â–∏—Ö –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤
[23.12.2024 15:10] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#reasoning", "#open_source", "#small_models", "#benchmark"], "emoji": "üá≥üá±", "ru": {"title": "Fietje: –û—Ç–∫—Ä—ã—Ç–∞—è –º–∞–ª–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∏–¥–µ—Ä–ª–∞–Ω–¥—Å–∫–æ–≥–æ —è–∑—ã–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Fietje - —Å–µ–º–µ–π—Å—Ç–≤–æ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (SLM), —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±
[23.12.2024 15:10] Using data from previous issue: {"categories": ["#3d", "#dataset", "#architecture"], "emoji": "üï∫", "ru": {"title": "–ú–≥–Ω–æ–≤–µ–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∏–º–∏—Ä—É–µ–º—ã—Ö 3D-–∞–≤–∞—Ç–∞—Ä–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É —Ñ–æ—Ç–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∞–Ω–∏–º–∏—Ä—É–µ–º—ã—Ö 3D-–∞–≤–∞—Ç–∞—Ä–æ–≤ —á–µ–ª–æ–≤–µ–∫–∞ –ø–æ –æ–¥–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç Hu
[23.12.2024 15:10] Loading Chinese text from previous data.
[23.12.2024 15:10] Renaming data file.
[23.12.2024 15:10] Renaming previous data. hf_papers.json to ./d/2024-12-23.json
[23.12.2024 15:10] Saving new data file.
[23.12.2024 15:10] Generating page.
[23.12.2024 15:10] Renaming previous page.
[23.12.2024 15:10] Renaming previous data. index.html to ./d/2024-12-23.html
[23.12.2024 15:10] [Experimental] Generating Chinese page for reading.
[23.12.2024 15:10] Chinese vocab [{'word': 'Ëá™ÂõûÂΩí', 'pinyin': 'z√¨ hu√≠ guƒ´', 'trans': 'autoregressive'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'visual'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generation'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éo xi√†n', 'trans': 'performance'}, {'word': 'Âº∫Â§ß', 'pinyin': 'qi√°ng d√†', 'trans': 'powerful'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'È°∫Â∫è', 'pinyin': 'sh√πn x√π', 'trans': 'sequential'}, {'word': 'ÈÄê', 'pinyin': 'zh√∫', 'trans': 'gradual'}, {'word': 'Ê†áËÆ∞', 'pinyin': 'biƒÅo j√¨', 'trans': 'token'}, {'word': 'È¢ÑÊµã', 'pinyin': 'y√π c√®', 'trans': 'prediction'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'inference'}, {'word': 'ÈÄüÂ∫¶', 'pinyin': 's√π d√π', 'trans': 'speed'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'Âπ∂Ë°å', 'pinyin': 'b√¨ng x√≠ng', 'trans': 'parallel'}, {'word': 'ÊúâÊïà', 'pinyin': 'y«íu xi√†o', 'trans': 'effective'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠ gƒÅo', 'trans': 'improve'}, {'word': 'ÊïàÁéá', 'pinyin': 'xi√†o l«ú', 'trans': 'efficiency'}, {'word': '‰øùÁïô', 'pinyin': 'b«éo li√∫', 'trans': 'retain'}, {'word': '‰ºòÂäø', 'pinyin': 'y≈çu sh√¨', 'trans': 'advantage'}, {'word': 'ÂÖ≥ÈîÆ', 'pinyin': 'gu«én ji√†n', 'trans': 'key'}, {'word': 'Ê¥ûËßÅ', 'pinyin': 'd√≤ng ji√†n', 'trans': 'insight'}, {'word': '‰æùËµñ', 'pinyin': 'yƒ´ l√†i', 'trans': 'dependency'}, {'word': 'ÂÖ≥Á≥ª', 'pinyin': 'guƒÅn x√¨', 'trans': 'relationship'}, {'word': 'Âº±', 'pinyin': 'ru√≤', 'trans': 'weak'}, {'word': 'Âº∫', 'pinyin': 'qi√°ng', 'trans': 'strong'}, {'word': 'Áõ∏ÈÇª', 'pinyin': 'xiƒÅng l√≠n', 'trans': 'adjacent'}, {'word': 'Èöæ‰ª•', 'pinyin': 'n√°n y«ê', 'trans': 'difficult'}, {'word': 'Áã¨Á´ã', 'pinyin': 'd√∫ l√¨', 'trans': 'independent'}, {'word': 'ÈááÊ†∑', 'pinyin': 'c«éi y√†ng', 'trans': 'sampling'}, {'word': '‰∏ç‰∏ÄËá¥', 'pinyin': 'b√π yƒ´ zh√¨', 'trans': 'inconsistency'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'ËøúË∑ùÁ¶ª', 'pinyin': 'yu«én j√π l√≠', 'trans': 'long-distance'}, {'word': 'Â±ÄÈÉ®', 'pinyin': 'j√∫ b√π', 'trans': 'local'}, {'word': 'Êó†Áºù', 'pinyin': 'w√∫ f√®ng', 'trans': 'seamless'}, {'word': 'ÈõÜÊàê', 'pinyin': 'j√≠ ch√©ng', 'trans': 'integrate'}, {'word': 'Ê†áÂáÜ', 'pinyin': 'biƒÅo zh«în', 'trans': 'standard'}, {'word': 'Êû∂ÊûÑ', 'pinyin': 'ji√† g√≤u', 'trans': 'architecture'}, {'word': '‰øÆÊîπ', 'pinyin': 'xi≈´ g«éi', 'trans': 'modify'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ÂõæÂÉè', 'pinyin': 't√∫ xi√†ng', 'trans': 'image'}, {'word': 'ËßÜÈ¢ë', 'pinyin': 'sh√¨ p√≠n', 'trans': 'video'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®n w√π', 'trans': 'task'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}, {'word': 'Âä†ÈÄü', 'pinyin': 'jiƒÅ s√π', 'trans': 'acceleration'}, {'word': 'Ë¥®Èáè', 'pinyin': 'zh√¨ li√†ng', 'trans': 'quality'}, {'word': 'ÊçüÂ§±', 'pinyin': 's«în shƒ´', 'trans': 'loss'}, {'word': 'ÊøÄÂèë', 'pinyin': 'jƒ´ fƒÅ', 'trans': 'inspire'}, {'word': 'Êú™Êù•', 'pinyin': 'w√®i l√°i', 'trans': 'future'}, {'word': 'È´òÊïà', 'pinyin': 'gƒÅo xi√†o', 'trans': 'efficient'}, {'word': 'Áªü‰∏Ä', 'pinyin': 't«íng yƒ´', 'trans': 'unified'}, {'word': 'È°πÁõÆ', 'pinyin': 'xi√†ng m√π', 'trans': 'project'}, {'word': 'È°µÈù¢', 'pinyin': 'y√® mi√†n', 'trans': 'page'}]
[23.12.2024 15:10] Renaming previous Chinese page.
[23.12.2024 15:10] Renaming previous data. zh.html to ./d/2024-12-22_zh_reading_task.html
[23.12.2024 15:10] Writing Chinese reading task.
[23.12.2024 15:10] Writing result.
[23.12.2024 15:10] Renaming log file.
[23.12.2024 15:10] Renaming previous data. log.txt to ./logs/2024-12-23_last_log.txt
