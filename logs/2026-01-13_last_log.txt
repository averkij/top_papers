[13.01.2026 07:27] Read previous papers.
[13.01.2026 07:27] Generating top page (month).
[13.01.2026 07:27] Writing top page (month).
[13.01.2026 08:33] Read previous papers.
[13.01.2026 08:33] Get feed.
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06943
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06521
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05593
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06953
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05110
[13.01.2026 08:33] Extract page data from URL. URL: https://huggingface.co/papers/2601.07779
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07226
[13.01.2026 08:33] Extract page data from URL. URL: https://huggingface.co/papers/2601.07832
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07526
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05107
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06860
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.05823
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06165
[13.01.2026 08:33] Extract page data from URL. URL: https://huggingface.co/papers/2601.01528
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.04698
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07055
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07376
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07767
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06411
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07033
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03666
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07786
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07181
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06944
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06496
[13.01.2026 08:33] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06329
[13.01.2026 08:33] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.01.2026 08:33] No deleted papers detected.
[13.01.2026 08:33] Downloading and parsing papers (pdf, html). Total: 26.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.06943.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.06943.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.06943.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.06521.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.06521.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.06521.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.05593.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.05593.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.05593.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.06953.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.06953.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.06953.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.05110.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.05110.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.05110.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.07779.
[13.01.2026 08:33] Downloading paper 2601.07779 from https://arxiv.org/pdf/2601.07779v1...
[13.01.2026 08:33] Extracting affiliations from text.
[13.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"OS-SYMPHONY: Holistic Framework for Robust and Generalist Computer-Using Agent Bowen Yang1, 2*, Kaiming Jin3 , Zhenyu Wu2, Zhaoyang Liu4, Qiushi Sun5, Zehao Li2, Jingjing Xie6, Zhoumianze Liu2, Fangzhi Xu7, Kanzhi Cheng8, Qingyun Li9, Yian Wang3, Yu Qiao2, Zun Wang2, Zichen Ding2 1University of Science and Technology of China 2Shanghai AI Laboratory 3National University of Singapore 4The Hong Kong University of Science and Technology 5The University of Hong Kong 6CUHK MMLab 7Xian Jiaotong University 8Nanjing University 9Harbin Institute of Technology 6 2 0 2 2 1 ] . [ 1 9 7 7 7 0 . 1 0 6 2 : r a "
[13.01.2026 08:33] Response: ```python
[
    "University of Science and Technology of China",
    "Shanghai AI Laboratory",
    "National University of Singapore",
    "The Hong Kong University of Science and Technology",
    "The University of Hong Kong",
    "CUHK MMLab",
    "Xian Jiaotong University",
    "Nanjing University",
    "Harbin Institute of Technology"
]
```
[13.01.2026 08:33] Deleting PDF ./assets/pdf/2601.07779.pdf.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.07226.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.07226.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.07226.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.07832.
[13.01.2026 08:33] Downloading paper 2601.07832 from https://arxiv.org/pdf/2601.07832v1...
[13.01.2026 08:33] Extracting affiliations from text.
[13.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 1 ] . [ 1 2 3 8 7 0 . 1 0 6 2 : r MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head Kewei Zhang1 Ye Huang1 Yufan Deng1 Jincheng Yu2 Junsong Chen2 Huan Ling2 Enze Xie2 Daquan Zhou1 1Peking University, 2NVIDIA Equal contribution "
[13.01.2026 08:33] Response: ```python
["Peking University", "NVIDIA"]
```
[13.01.2026 08:33] Deleting PDF ./assets/pdf/2601.07832.pdf.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.07526.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.07526.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.07526.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.05107.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.05107.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.05107.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.06860.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.06860.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.06860.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.05823.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.05823.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.05823.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.06165.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.06165.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.06165.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.01528.
[13.01.2026 08:33] Downloading paper 2601.01528 from https://arxiv.org/pdf/2601.01528v1...
[13.01.2026 08:33] Extracting affiliations from text.
[13.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 4 ] . [ 1 8 2 5 1 0 . 1 0 6 2 : r DRIVINGGEN: COMPREHENSIVE BENCHMARK FOR GENERATIVE VIDEO WORLD MODELS IN AUTONOMOUS DRIVING Yang Zhou1 Hao Shao2 Letian Wang1 Zhuofan Zong2 Hongsheng Li2 Steven L. Waslander1 1 University of Toronto 2 CUHK MMLab Project Website: https://drivinggen-bench.github.io/ "
[13.01.2026 08:33] Response: ```python
[
    "University of Toronto",
    "CUHK MMLab"
]
```
[13.01.2026 08:33] Deleting PDF ./assets/pdf/2601.01528.pdf.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.04698.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.04698.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.04698.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.07055.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.07055.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.07055.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.07376.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.07376.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.07376.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.07767.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.07767.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.07767.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.06411.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.06411.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.06411.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.07033.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.07033.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.07033.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.03666.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.03666.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.03666.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.07786.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.07786.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.07786.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.07181.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.07181.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.07181.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.06944.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.06944.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.06944.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.06496.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.06496.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.06496.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Downloading and parsing paper https://huggingface.co/papers/2601.06329.
[13.01.2026 08:33] Extra JSON file exists (./assets/json/2601.06329.json), skip PDF parsing.
[13.01.2026 08:33] Paper image links file exists (./assets/img_data/2601.06329.json), skip HTML parsing.
[13.01.2026 08:33] Success.
[13.01.2026 08:33] Enriching papers with extra data.
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 0. VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.  					AI-generated summary 				 In real-world video question answering scenarios, videos often provide only localized visual cues, while veri...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 1. Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.  					AI-generated summary 				 While humans develop core visual skills long before acquiring language, contemporary Multimod...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 2. Parallel Coordinated Reasoning enables large-scale test-time compute scaling beyond sequential reasoning limitations through parallel exploration and message-passing architecture.  					AI-generated summary 				 We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 3. Code LLMs trained on fully synthetic data using a feature-based synthesis pipeline achieve superior performance on competitive programming benchmarks while reducing dependence on real-world coding datasets.  					AI-generated summary 				 Competitive programming presents great challenges for Code LL...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 4. Large reasoning models' inference latency can be reduced by routing reasoning steps to larger models based on the entropy of their first token, enabling efficient collaborative inference without additional training.  					AI-generated summary 				 Large Reasoning Models (LRMs) achieve remarkable per...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 5. OS-Symphony presents a comprehensive framework for computer-using agents that enhances robustness in long-horizon tasks through reflection-memory and multimodal search capabilities.  					AI-generated summary 				 While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents ...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 6. NoisyBench benchmark reveals significant performance degradation in state-of-the-art models when exposed to noisy contextual information, with agentic workflows amplifying errors and attention mechanisms disproportionately focusing on distractor tokens.  					AI-generated summary 				 Recent advance...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 7. Multi-Head Linear Attention addresses the performance degradation in linear attention by preserving representational diversity through head-wise token dimension computation, maintaining linear complexity while recovering softmax attention's expressive power across multiple domains.  					AI-generate...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 8. MegaFlow is a distributed orchestration system that enables large-scale training and evaluation of agents on complex tasks by providing efficient scheduling, resource allocation, and task management through modular services.  					AI-generated summary 				 The rapid development of interactive and au...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 9. A framework is presented that enables dynamic regulation of memory reliance in LLM-based agents, allowing users to control the balance between innovation and historical fidelity in long-term interactions.  					AI-generated summary 				 As LLM-based agents are increasingly used in long-term interact...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 10. ET-Agent is a training framework that calibrates tool-use behavior in large language models through self-evolving data flywheels and behavior calibration training to improve task execution effectiveness.  					AI-generated summary 				 Large Language Models (LLMs) can extend their parameter knowledg...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 11. Latent Diffusion Models generate high-quality images by operating in compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as repre...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 12. Real-world vision-language benchmarks reveal that under-specified user queries pose significant challenges for current models, with explicit query rewriting leading to substantial performance improvements.  					AI-generated summary 				 Current vision-language benchmarks predominantly feature well-...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 13. DrivingGen presents the first comprehensive benchmark for generative driving world models, addressing limitations in existing evaluations through diverse datasets and metrics that assess visual realism, trajectory plausibility, temporal coherence, and controllability.  					AI-generated summary 				...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 14. TourPlanner addresses travel planning challenges through multi-path reasoning and constraint-gated reinforcement learning to optimize both hard and soft constraints effectively.  					AI-generated summary 				 Travel planning is a sophisticated decision-making process that requires synthesizing mult...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 15. A data-free self-evolution framework enables large language models to autonomously improve reasoning capabilities through iterative question generation and solving, achieving performance comparable to supervised methods.  					AI-generated summary 				 As high-quality data becomes increasingly diffi...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 16. OpenTinker provides a modular infrastructure for reinforcement learning of large language model agents with separated components and managed execution runtime.  					AI-generated summary 				 We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) age...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 17. Large language models exhibit a disconnect between their expressed uncertainty and strategic decision-making under varying penalty conditions, failing to adjust abstention policies even when optimal.  					AI-generated summary 				 Large Language Models (LLMs) can produce surprisingly sophisticated ...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 18. Structured Episodic Event Memory (SEEM) enhances LLMs with hierarchical memory architecture combining graph and episodic layers for improved narrative coherence and reasoning.  					AI-generated summary 				 Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Re...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 19. Large language models struggle with maintaining long-range narrative dependencies, but a new framework called CFPG addresses this by structuring narrative continuity through executable causal predicates to ensure proper fulfillment of foreshadowed events.  					AI-generated summary 				 Foreshadowin...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 20. Omni-modal embedding models face challenges with modality-dependent similarity scaling, ineffective in-batch negatives, and mismatched statistics across modalities, which are addressed through explicit alignment techniques including temperature calibration, controlled negative curriculum, and batch ...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 21. Analysis of AI-referencing code comments reveals that developers explicitly acknowledge technical debt in AI-assisted code, identifying patterns of postponed testing, incomplete adaptation, and limited understanding as key factors in AI-induced technical debt emergence.  					AI-generated summary 		...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 22. ShowUI-Aloha presents a pipeline that converts unstructured human screen recordings into structured GUI tasks through recording, semantic interpretation, planning, and execution components.  					AI-generated summary 				 Graphical User Interfaces (GUIs) are central to human-computer interaction, ye...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 23. SketchJudge benchmark evaluates multimodal large language models' ability to grade hand-drawn STEM diagrams, revealing significant limitations in visual understanding compared to human performance.  					AI-generated summary 				 While Multimodal Large Language Models (MLLMs) have achieved remarkabl...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 24. 3D CoCa v2 enhances 3D captioning by combining contrastive vision-language learning with spatially-aware 3D scene encoding and test-time search for improved generalization across diverse environments.  					AI-generated summary 				 Spatial intelligence refers to the ability to perceive, reason abou...
[13.01.2026 08:33] ********************************************************************************
[13.01.2026 08:33] Abstract 25. Speech models trained on raw audio can generate appropriate content while maintaining speaker and emotion attributes, but traditional text-based evaluation methods underestimate speech characteristics; new evaluation approaches better correlate with human perception.  					AI-generated summary 				 ...
[13.01.2026 08:33] Read previous papers.
[13.01.2026 08:33] Generating reviews via LLM API.
[13.01.2026 08:33] Using data from previous issue: {"categories": ["#rag", "#video", "#agents", "#benchmark", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–û—Ç –≤–∏–¥–µ–æ –∫ –∑–Ω–∞–Ω–∏—è–º: –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–π –ø–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–µ–±", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ VideoDR –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –≤–∏–¥–µ–æ –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –≤–µ–±-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –ú–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã –≤—ã–ø–æ–ª–Ω—è—Ç—å
[13.01.2026 08:33] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#cv", "#dataset"], "emoji": "üë∂", "ru": {"title": "–î–µ—Ç–∏ –≤–∏–¥—è—Ç –ª—É—á—à–µ: –ø–æ—á–µ–º—É —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å–ª–µ–ø—ã –∫ –±–∞–∑–æ–≤–æ–º—É –∑—Ä–µ–Ω–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ BabyVision –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–∞–∑–æ–≤—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–µ
[13.01.2026 08:33] Using data from previous issue: {"categories": ["#training", "#long_context", "#open_source", "#benchmark", "#math", "#reasoning", "#rl", "#inference"], "emoji": "üîÄ", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Parallel Coordinated Reasonin
[13.01.2026 08:33] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#optimization", "#dataset", "#training", "#synthetic", "#small_models", "#benchmark", "#plp", "#data"], "emoji": "ü§ñ", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –≤–º–µ—Å—Ç–æ —Ä–µ–∞–ª—å–Ω—ã—Ö: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å LLM –ø–∏—Å–∞—Ç—å –∫–æ–¥ –±–µ–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥
[13.01.2026 08:33] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#reasoning", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ –ø–µ—Ä–≤–æ–º—É —Ç–æ–∫–µ–Ω—É –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ GlimpRouter –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –∑–∞–¥–µ—Ä–∂–∫–∏ –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ
[13.01.2026 08:33] Querying the API.
[13.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

OS-Symphony presents a comprehensive framework for computer-using agents that enhances robustness in long-horizon tasks through reflection-memory and multimodal search capabilities.  					AI-generated summary 				 While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.
[13.01.2026 08:33] Response: ```json
{
  "desc": "OS-Symphony ‚Äî —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≤–∏–¥–µ–Ω–∏–µ –∏ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ö—Ä—É–ø–∫–æ—Å—Ç–∏ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –¥–æ–ª–≥–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∑–∞–¥–∞—á –±–ª–∞–≥–æ–¥–∞—Ä—è –¥–≤—É–º –∫–ª—é—á–µ–≤—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º: –∞–≥–µ–Ω—Ç—É —Å —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π –∏ –ø–∞–º—è—Ç—å—é, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –¥–ª—è —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º –ø–æ–∏—Å–∫–∞, –∫–æ—Ç–æ—Ä—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç–µ—Ä–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ–≤—ã—à–∞–µ—Ç –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ –Ω–æ–≤—ã–µ –¥–æ–º–µ–Ω—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –¥–æ—Å—Ç–∏–≥–∞—è 65.84% –Ω–∞ OSWorld.",
  "emoji": "üéº",
  "title": "–û—Ä–∫–µ—Å—Ç—Ä–æ–≤–∫–∞ –ø–∞–º—è—Ç–∏ –∏ –ø–æ–∏—Å–∫–∞ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º"
}
```
[13.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"OS-Symphony presents a comprehensive framework for computer-using agents that enhances robustness in long-horizon tasks through reflection-memory and multimodal search capabilities.  					AI-generated summary 				 While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld."

[13.01.2026 08:33] Response: ```python
["AGENTS", "MULTIMODAL", "BENCHMARK", "CV"]
```
[13.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"OS-Symphony presents a comprehensive framework for computer-using agents that enhances robustness in long-horizon tasks through reflection-memory and multimodal search capabilities.  					AI-generated summary 				 While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld."

[13.01.2026 08:33] Response: ```python
['LONG_CONTEXT', 'REASONING']
```
[13.01.2026 08:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OS-Symphony is a new framework designed to improve the performance of computer-using agents in complex, long-term tasks. It introduces a Reflection-Memory Agent that helps these agents remember important information over time, allowing them to correct mistakes and maintain context. Additionally, it features Versatile Tool Agents that use a multimodal search approach to find and utilize relevant tutorials in real-time, enhancing their ability to adapt to new situations. The framework has shown significant improvements in performance on various benchmarks, setting new records in the field.","title":"Enhancing Agent Robustness with OS-Symphony"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OS-Symphony is a new framework designed to improve the performance of computer-using agents in complex, long-term tasks. It introduces a Reflection-Memory Agent that helps these agents remember important information over time, allowing them to correct mistakes and maintain context. Additionally, it features Versatile Tool Agents that use a multimodal search approach to find and utilize relevant tutorials in real-time, enhancing their ability to adapt to new situations. The framework has shown significant improvements in performance on various benchmarks, setting new records in the field.', title='Enhancing Agent Robustness with OS-Symphony'))
[13.01.2026 08:33] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OS-SymphonyÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∫ËÆ°ÁÆóÊú∫‰ΩøÁî®‰ª£ÁêÜÂú®ÈïøÊó∂Èó¥‰ªªÂä°‰∏≠ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂÆÉÈÄöËøáÂèçÊÄùËÆ∞ÂøÜÂíåÂ§öÊ®°ÊÄÅÊêúÁ¥¢ËÉΩÂäõÊù•Ëß£ÂÜ≥ÂΩìÂâçÊ°ÜÊû∂Âú®Êñ∞È¢ÜÂüü‰∏≠ÁöÑÊ≥õÂåñÈóÆÈ¢ò„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨‰∏Ä‰∏™ÂçèË∞ÉÂô®ÔºåÁªìÂêà‰∫ÜÂèçÊÄùËÆ∞ÂøÜ‰ª£ÁêÜÂíåÂ§öÂäüËÉΩÂ∑•ÂÖ∑‰ª£ÁêÜÔºå‰ª•ÂÆûÁé∞Êõ¥Âº∫ÁöÑËá™Âä®Âåñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOS-SymphonyÂú®Â§ö‰∏™Âú®Á∫øÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ∞§ÂÖ∂ÊòØÂú®OSWorld‰∏äËææÂà∞‰∫Ü65.84%ÁöÑÊñ∞È´ò„ÄÇ","title":"OS-SymphonyÔºöÊèêÂçáÈïøÊó∂Èó¥‰ªªÂä°È≤ÅÊ£íÊÄßÁöÑÂÖ®Êñ∞Ê°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OS-SymphonyÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∫ËÆ°ÁÆóÊú∫‰ΩøÁî®‰ª£ÁêÜÂú®ÈïøÊó∂Èó¥‰ªªÂä°‰∏≠ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂÆÉÈÄöËøáÂèçÊÄùËÆ∞ÂøÜÂíåÂ§öÊ®°ÊÄÅÊêúÁ¥¢ËÉΩÂäõÊù•Ëß£ÂÜ≥ÂΩìÂâçÊ°ÜÊû∂Âú®Êñ∞È¢ÜÂüü‰∏≠ÁöÑÊ≥õÂåñÈóÆÈ¢ò„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨‰∏Ä‰∏™ÂçèË∞ÉÂô®ÔºåÁªìÂêà‰∫ÜÂèçÊÄùËÆ∞ÂøÜ‰ª£ÁêÜÂíåÂ§öÂäüËÉΩÂ∑•ÂÖ∑‰ª£ÁêÜÔºå‰ª•ÂÆûÁé∞Êõ¥Âº∫ÁöÑËá™Âä®Âåñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOS-SymphonyÂú®Â§ö‰∏™Âú®Á∫øÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ∞§ÂÖ∂ÊòØÂú®OSWorld‰∏äËææÂà∞‰∫Ü65.84%ÁöÑÊñ∞È´ò„ÄÇ', title='OS-SymphonyÔºöÊèêÂçáÈïøÊó∂Èó¥‰ªªÂä°È≤ÅÊ£íÊÄßÁöÑÂÖ®Êñ∞Ê°ÜÊû∂'))
[13.01.2026 08:33] Using data from previous issue: {"categories": ["#reasoning", "#rlhf", "#security", "#rag", "#alignment", "#agents", "#benchmark"], "emoji": "üîß", "ru": {"title": "–ö–∞–∫ –Ω–∞—É—á–∏—Ç—å AI-–º–æ–¥–µ–ª–∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —à—É–º –≤ –¥–∞–Ω–Ω—ã—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ NoisyBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —à—É–º–Ω—ã–º –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –≤
[13.01.2026 08:33] Querying the API.
[13.01.2026 08:33] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Multi-Head Linear Attention addresses the performance degradation in linear attention by preserving representational diversity through head-wise token dimension computation, maintaining linear complexity while recovering softmax attention's expressive power across multiple domains.  					AI-generated summary 				 While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\% improvement on ImageNet classification, a 6.3\% gain on NLP, a 12.6\% improvement on image generation, and a 41\% enhancement on video generation under the same time complexity.
[13.01.2026 08:34] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Multi-Head Linear Attention (MHLA) ‚Äî –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –ø—É—Ç—ë–º –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤–∞—Ö –ø–æ –∏–∑–º–µ—Ä–µ–Ω–∏—é —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–ª–ª–∞–ø—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã –¥–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MHLA –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏–Ω–µ–π–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å O(n) –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ softmax attention. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (+3.6%), –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (+6.3%), –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (+12.6%) –∏ –≤–∏–¥–µ–æ (+41%).",
  "emoji": "‚ö°",
  "title": "–õ–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É"
}
```
[13.01.2026 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multi-Head Linear Attention addresses the performance degradation in linear attention by preserving representational diversity through head-wise token dimension computation, maintaining linear complexity while recovering softmax attention's expressive power across multiple domains.  					AI-generated summary 				 While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\% improvement on ImageNet classification, a 6.3\% gain on NLP, a 12.6\% improvement on image generation, and a 41\% enhancement on video generation under the same time complexity."

[13.01.2026 08:34] Response: ```python
["ARCHITECTURE", "TRAINING"]
```
[13.01.2026 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multi-Head Linear Attention addresses the performance degradation in linear attention by preserving representational diversity through head-wise token dimension computation, maintaining linear complexity while recovering softmax attention's expressive power across multiple domains.  					AI-generated summary 				 While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\% improvement on ImageNet classification, a 6.3\% gain on NLP, a 12.6\% improvement on image generation, and a 41\% enhancement on video generation under the same time complexity."

[13.01.2026 08:34] Response: ```python
["OPTIMIZATION"]
```
[13.01.2026 08:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Multi-Head Linear Attention (MHLA) improves linear attention mechanisms by maintaining representational diversity through head-wise computations. This approach addresses the issue of global context collapse, which can lead to performance degradation in traditional linear attention models. By dividing attention calculations across multiple heads, MHLA retains the efficiency of linear complexity while enhancing the expressive capabilities similar to softmax attention. The effectiveness of MHLA is demonstrated through significant performance gains across various tasks, including image classification, natural language processing, image generation, and video generation.","title":"Enhancing Linear Attention with Multi-Head Diversity"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Multi-Head Linear Attention (MHLA) improves linear attention mechanisms by maintaining representational diversity through head-wise computations. This approach addresses the issue of global context collapse, which can lead to performance degradation in traditional linear attention models. By dividing attention calculations across multiple heads, MHLA retains the efficiency of linear complexity while enhancing the expressive capabilities similar to softmax attention. The effectiveness of MHLA is demonstrated through significant performance gains across various tasks, including image classification, natural language processing, image generation, and video generation.', title='Enhancing Linear Attention with Multi-Head Diversity'))
[13.01.2026 08:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Â§öÂ§¥Á∫øÊÄßÊ≥®ÊÑèÂäõÔºàMHLAÔºâÁöÑÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥Á∫øÊÄßÊ≥®ÊÑèÂäõÂú®ÊÄßËÉΩ‰∏äÁöÑ‰∏ãÈôçÈóÆÈ¢ò„ÄÇÈÄöËøáÂú®Â§¥ÈÉ®Áª¥Â∫¶‰∏äËøõË°åÂàÜÂâ≤ËÆ°ÁÆóÔºåMHLAËÉΩÂ§ü‰øùÊåÅË°®Á§∫ÁöÑÂ§öÊ†∑ÊÄßÔºåÂêåÊó∂‰øùÊåÅÁ∫øÊÄßÂ§çÊùÇÂ∫¶„ÄÇÊàë‰ª¨ËØÅÊòé‰∫ÜMHLAÂú®ÊÅ¢Â§çsoftmaxÊ≥®ÊÑèÂäõÁöÑË°®ËææËÉΩÂäõÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåÂπ∂Âú®Â§ö‰∏™È¢ÜÂüü‰∏≠È™åËØÅ‰∫ÜÂÖ∂ÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂú®ImageNetÂàÜÁ±ª„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÂõæÂÉèÁîüÊàêÂíåËßÜÈ¢ëÁîüÊàêÁ≠â‰ªªÂä°‰∏≠ÔºåMHLAÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ","title":"Â§öÂ§¥Á∫øÊÄßÊ≥®ÊÑèÂäõÔºö‰øùÊåÅÂ§öÊ†∑ÊÄß‰∏éÈ´òÊïàÊÄßÂπ∂Â≠ò"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Â§öÂ§¥Á∫øÊÄßÊ≥®ÊÑèÂäõÔºàMHLAÔºâÁöÑÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥Á∫øÊÄßÊ≥®ÊÑèÂäõÂú®ÊÄßËÉΩ‰∏äÁöÑ‰∏ãÈôçÈóÆÈ¢ò„ÄÇÈÄöËøáÂú®Â§¥ÈÉ®Áª¥Â∫¶‰∏äËøõË°åÂàÜÂâ≤ËÆ°ÁÆóÔºåMHLAËÉΩÂ§ü‰øùÊåÅË°®Á§∫ÁöÑÂ§öÊ†∑ÊÄßÔºåÂêåÊó∂‰øùÊåÅÁ∫øÊÄßÂ§çÊùÇÂ∫¶„ÄÇÊàë‰ª¨ËØÅÊòé‰∫ÜMHLAÂú®ÊÅ¢Â§çsoftmaxÊ≥®ÊÑèÂäõÁöÑË°®ËææËÉΩÂäõÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåÂπ∂Âú®Â§ö‰∏™È¢ÜÂüü‰∏≠È™åËØÅ‰∫ÜÂÖ∂ÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂú®ImageNetÂàÜÁ±ª„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÂõæÂÉèÁîüÊàêÂíåËßÜÈ¢ëÁîüÊàêÁ≠â‰ªªÂä°‰∏≠ÔºåMHLAÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ', title='Â§öÂ§¥Á∫øÊÄßÊ≥®ÊÑèÂäõÔºö‰øùÊåÅÂ§öÊ†∑ÊÄß‰∏éÈ´òÊïàÊÄßÂπ∂Â≠ò'))
[13.01.2026 08:34] Using data from previous issue: {"categories": ["#open_source", "#training", "#agents"], "emoji": "üîÑ", "ru": {"title": "–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "MegaFlow ‚Äî —ç—Ç–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –°–∏—Å—Ç–µ
[13.01.2026 08:34] Using data from previous issue: {"categories": [], "emoji": "üéöÔ∏è", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–µ–º–∞—è –ø–∞–º—è—Ç—å: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∏–Ω–Ω–æ–≤–∞—Ü–∏–µ–π –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SteeM –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏ –≤ LLM-–∞–≥–µ–Ω—Ç–∞—Ö, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –±–∞–ª–∞–Ω
[13.01.2026 08:34] Using data from previous issue: {"categories": [], "emoji": "üîß", "ru": {"title": "–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ø–æ–≤–µ–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "ET-Agent ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∫–∞–ª–∏–±—Ä—É–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ —á–µ—Ä–µ–∑ —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â
[13.01.2026 08:34] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization", "#diffusion", "#cv"], "emoji": "üé®", "ru": {"title": "–î–∏sentangled –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: VAE –ø–µ—Ä–µ—É—á–∏–≤–∞–µ—Ç—Å—è –ø–æ-–Ω–æ–≤–æ–º—É", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Send-VAE ‚Äî –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è –¥–∏sent
[13.01.2026 08:34] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#dataset", "#low_resource", "#cv"], "emoji": "üîç", "ru": {"title": "–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –Ω–µ—è—Å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ ‚Äî –∫–ª—é—á –∫ —É–ª—É—á—à–µ–Ω–∏—é –∑—Ä–∏—Ç–µ–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç HAERAE-Vision, –±–µ–Ω—á–º–∞—Ä–∫ –∏–∑ 653 —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –∫–æ—Ä–µ–π—Å–∫–∏—Ö –æ–Ω–ª
[13.01.2026 08:34] Querying the API.
[13.01.2026 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DrivingGen presents the first comprehensive benchmark for generative driving world models, addressing limitations in existing evaluations through diverse datasets and metrics that assess visual realism, trajectory plausibility, temporal coherence, and controllability.  					AI-generated summary 				 Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.
[13.01.2026 08:34] Response: ```json
{
  "desc": "DrivingGen –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞ –≤–æ–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ –±—É–¥—É—â–∏—Ö —Å—Ü–µ–Ω –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º –≤–æ–∂–¥–µ–Ω–∏–∏. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø–æ–≥–æ–¥–Ω—ã–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏, –≤—Ä–µ–º–µ–Ω–µ–º —Å—É—Ç–æ–∫ –∏ —Å–ª–æ–∂–Ω—ã–º–∏ –º–∞–Ω—ë–≤—Ä–∞–º–∏, –∞ —Ç–∞–∫–∂–µ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∞–ª–∏–∑–º–∞, –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏ —É–ø—Ä–∞–≤–ª—è–µ–º–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º: –æ–±—â–∏–µ –º–æ–¥–µ–ª–∏ –≤—ã–≥–ª—è–¥—è—Ç –ª—É—á—à–µ –≤–∏–∑—É–∞–ª—å–Ω–æ, –Ω–æ –Ω–∞—Ä—É—à–∞—é—Ç —Ñ–∏–∑–∏–∫—É, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤–æ–∂–¥–µ–Ω–∏—è —Ç–æ—á–Ω–µ–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥—è—Ç –¥–≤–∏–∂–µ–Ω–∏—è, –Ω–æ —É—Å—Ç—É–ø–∞—é—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –≠—Ç–∞ —Ä–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞–¥—ë–∂–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∂–¥–µ–Ω–∏—è –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–π —Å–∏–º—É–ª—è—Ü–∏–∏ –∏ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤.",
  "emoji": "üöó",
  "title": "–ï–¥–∏–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–∏–º—É–ª—è—Ü–∏–∏ –≤–æ–∂–¥–µ–Ω–∏—è"
}
```
[13.01.2026 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DrivingGen presents the first comprehensive benchmark for generative driving world models, addressing limitations in existing evaluations through diverse datasets and metrics that assess visual realism, trajectory plausibility, temporal coherence, and controllability.  					AI-generated summary 				 Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making."

[13.01.2026 08:34] Response: ```python
["BENCHMARK", "VIDEO", "DATASET", "AUTONOMOUS_DRIVING"]
```

Wait, let me reconsider - "AUTONOMOUS_DRIVING" is not in the provided topics list. Let me correct this:

```python
["BENCHMARK", "VIDEO", "DATASET"]
```
[13.01.2026 08:34] Error. Failed to parse JSON from LLM. ["BENCHMARK", "VIDEO", "DATASET", "AUTONOMOUS_DRIVING"]


Wait, let me reconsider - "AUTONOMOUS_DRIVING" is not in the provided topics list. Let me correct this:


["BENCHMARK", "VIDEO", "DATASET"]
[13.01.2026 08:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DrivingGen presents the first comprehensive benchmark for generative driving world models, addressing limitations in existing evaluations through diverse datasets and metrics that assess visual realism, trajectory plausibility, temporal coherence, and controllability.  					AI-generated summary 				 Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making."

[13.01.2026 08:34] Response: ```python
['DIFFUSION', 'SYNTHETIC', 'SURVEY']
```
[13.01.2026 08:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DrivingGen introduces a new benchmark for evaluating generative driving world models, which are essential for simulating autonomous driving scenarios. It addresses the shortcomings of existing evaluations by providing diverse datasets and metrics that measure visual realism, trajectory plausibility, temporal coherence, and controllability. The benchmark reveals trade-offs between general models that excel in visual quality but fail in physical accuracy, and driving-specific models that maintain realistic motion but lack visual appeal. By offering a comprehensive evaluation framework, DrivingGen aims to enhance the development of reliable and deployable driving simulations for safe autonomous driving.","title":"DrivingGen: A New Standard for Evaluating Generative Driving Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DrivingGen introduces a new benchmark for evaluating generative driving world models, which are essential for simulating autonomous driving scenarios. It addresses the shortcomings of existing evaluations by providing diverse datasets and metrics that measure visual realism, trajectory plausibility, temporal coherence, and controllability. The benchmark reveals trade-offs between general models that excel in visual quality but fail in physical accuracy, and driving-specific models that maintain realistic motion but lack visual appeal. By offering a comprehensive evaluation framework, DrivingGen aims to enhance the development of reliable and deployable driving simulations for safe autonomous driving.', title='DrivingGen: A New Standard for Evaluating Generative Driving Models'))
[13.01.2026 08:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DrivingGenÊòØÁ¨¨‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÁîüÊàêÈ©æÈ©∂‰∏ñÁïåÊ®°ÂûãÂü∫ÂáÜÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËØÑ‰º∞ÁöÑÂ±ÄÈôêÊÄß„ÄÇÂÆÉÈÄöËøáÂ§öÊ†∑ÂåñÁöÑÊï∞ÊçÆÈõÜÂíåÊåáÊ†áÊù•ËØÑ‰º∞ËßÜËßâÁúüÂÆûÊÑü„ÄÅËΩ®ËøπÂêàÁêÜÊÄß„ÄÅÊó∂Èó¥‰∏ÄËá¥ÊÄßÂíåÂèØÊéßÊÄß„ÄÇËØ•Âü∫ÂáÜÁªìÂêà‰∫ÜÊù•Ëá™È©æÈ©∂Êï∞ÊçÆÈõÜÂíå‰∫íËÅîÁΩëËßÜÈ¢ëÊ∫êÁöÑÂ§öÊ†∑ÂåñËØÑ‰º∞Êï∞ÊçÆÈõÜÔºåÊ∂µÁõñ‰∏çÂêåÁöÑÂ§©Ê∞î„ÄÅÊó∂Èó¥ÂíåÂú∞ÁêÜÂå∫Âüü„ÄÇDrivingGen‰∏∫ÁîüÊàêÈ©æÈ©∂‰∏ñÁïåÊ®°ÂûãÊèê‰æõ‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑËØÑ‰º∞Ê°ÜÊû∂Ôºå‰øÉËøõÂèØÈù†„ÄÅÂèØÊéßÂíåÂèØÈÉ®ÁΩ≤ÁöÑÊ®°ÂûãÂèëÂ±ï„ÄÇ","title":"DrivingGenÔºöÁîüÊàêÈ©æÈ©∂Ê®°ÂûãÁöÑÂÖ®Èù¢Âü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DrivingGenÊòØÁ¨¨‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÁîüÊàêÈ©æÈ©∂‰∏ñÁïåÊ®°ÂûãÂü∫ÂáÜÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâËØÑ‰º∞ÁöÑÂ±ÄÈôêÊÄß„ÄÇÂÆÉÈÄöËøáÂ§öÊ†∑ÂåñÁöÑÊï∞ÊçÆÈõÜÂíåÊåáÊ†áÊù•ËØÑ‰º∞ËßÜËßâÁúüÂÆûÊÑü„ÄÅËΩ®ËøπÂêàÁêÜÊÄß„ÄÅÊó∂Èó¥‰∏ÄËá¥ÊÄßÂíåÂèØÊéßÊÄß„ÄÇËØ•Âü∫ÂáÜÁªìÂêà‰∫ÜÊù•Ëá™È©æÈ©∂Êï∞ÊçÆÈõÜÂíå‰∫íËÅîÁΩëËßÜÈ¢ëÊ∫êÁöÑÂ§öÊ†∑ÂåñËØÑ‰º∞Êï∞ÊçÆÈõÜÔºåÊ∂µÁõñ‰∏çÂêåÁöÑÂ§©Ê∞î„ÄÅÊó∂Èó¥ÂíåÂú∞ÁêÜÂå∫Âüü„ÄÇDrivingGen‰∏∫ÁîüÊàêÈ©æÈ©∂‰∏ñÁïåÊ®°ÂûãÊèê‰æõ‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑËØÑ‰º∞Ê°ÜÊû∂Ôºå‰øÉËøõÂèØÈù†„ÄÅÂèØÊéßÂíåÂèØÈÉ®ÁΩ≤ÁöÑÊ®°ÂûãÂèëÂ±ï„ÄÇ', title='DrivingGenÔºöÁîüÊàêÈ©æÈ©∂Ê®°ÂûãÁöÑÂÖ®Èù¢Âü∫ÂáÜ'))
[13.01.2026 08:34] Using data from previous issue: {"categories": ["#rl", "#benchmark"], "emoji": "‚úàÔ∏è", "ru": {"title": "–ú–Ω–æ–≥–æ–ø—É—Ç–µ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–∞—Ä—à—Ä—É—Ç–æ–≤ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–π", "desc": "TourPlanner —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è –º–Ω–æ–≥–æ–ø—É—Ç–µ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –≥–µ–π—Ç–∏–Ω–≥–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π. –°–∏—Å—Ç–µ–º–∞ —Å–Ω–∞—á–∞–ª–∞ –ø
[13.01.2026 08:34] Using data from previous issue: {"categories": ["#reasoning", "#rlhf", "#optimization", "#training", "#agents", "#synthetic"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Dr. Zero, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º 
[13.01.2026 08:34] Using data from previous issue: {"categories": ["#training", "#agents", "#rl", "#architecture"], "emoji": "üîß", "ru": {"title": "–ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –≥–∏–±–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "OpenTinker ‚Äî —ç—Ç–æ –º–æ–¥—É–ª—å–Ω–∞—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º LLM-–∞–≥–µ–Ω—Ç–æ–≤, —Ä–∞–∑–¥–µ–ª—è—é—â–∞—è –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –¥–∏–∑–∞–π–Ω, –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ –∏ –≤–∑–∞–∏–º
[13.01.2026 08:34] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#reasoning"], "emoji": "‚öñÔ∏è", "ru": {"title": "–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –±–µ–∑ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: –ø–æ—á–µ–º—É LLM –Ω–µ —É—á–∏—Ç—ã–≤–∞—é—Ç —Ä–∏—Å–∫ –≤ —Ä–µ—à–µ–Ω–∏—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –Ω–µ –º–æ–≥—É—Ç –∞–¥–µ–∫–≤–∞—Ç–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤–æ–∏ –æ—Ü–µ–Ω–∫–∏ –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø—Ä–∏
[13.01.2026 08:34] Using data from previous issue: {"categories": ["#reasoning", "#rag", "#long_context", "#agents", "#graphs", "#benchmark"], "emoji": "üß†", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –ª–æ–≥–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SEEM (Structured Episodic Event Memory), –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö
[13.01.2026 08:34] Using data from previous issue: {"categories": ["#long_context", "#story_generation", "#reasoning"], "emoji": "üî´", "ru": {"title": "–û—Ç –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–π –±–µ–≥–ª–æ—Å—Ç–∏ –∫ –ø–æ–¥–ª–∏–Ω–Ω–æ–π –Ω–∞—Ä—Ä–∞—Ç–∏–≤–Ω–æ–π –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –∫–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ CFPG –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑
[13.01.2026 08:34] Using data from previous issue: {"categories": ["#training", "#open_source", "#multimodal", "#optimization", "#benchmark"], "emoji": "üåà", "ru": {"title": "–Ø–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–ª—è –Ω–∞–¥—ë–∂–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤–ª–æ–∂–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ e5-omni –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–ª–æ–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã
[13.01.2026 08:34] Using data from previous issue: {"categories": [], "emoji": "‚öôÔ∏è", "ru": {"title": "–ö–æ–≥–¥–∞ AI –ø–æ–º–æ–≥–∞–µ—Ç, –Ω–æ —Å–æ–∑–¥–∞—ë—Ç –¥–æ–ª–≥–∏: –ø—Ä–∏–∑–Ω–∞–Ω–Ω–∞—è –Ω–µ–ø–æ–ª–Ω–æ—Ç–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –∫–æ–¥–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ –∫–æ–¥–µ, –≥–¥–µ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –ø—Ä–∏–∑–Ω–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–ª–≥–∞. –£—á—ë–Ω—ã–µ –∏–∑—É—á–∏–ª–∏ 6540 –∫–æ–º–º–µ
[13.01.2026 08:34] Using data from previous issue: {"categories": [], "emoji": "üñ•Ô∏è", "ru": {"title": "–û—Ç –∑–∞–ø–∏—Å–µ–π –∫ –¥–µ–π—Å—Ç–≤–∏—è–º: –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è GUI-–∑–∞–¥–∞—á —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é", "desc": "ShowUI-Aloha –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π pipeline –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å–µ–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞ —Å —ç–∫—Ä–∞–Ω–æ–º –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–¥
[13.01.2026 08:34] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#cv", "#dataset"], "emoji": "‚úèÔ∏è", "ru": {"title": "–ö–æ–≥–¥–∞ –¥–∞–∂–µ —É–º–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –º–æ–≥—É—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —à–∫–æ–ª—å–Ω—ã–π —Ä–∏—Å—É–Ω–æ–∫", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ SketchJudge –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≤–µ—Ä—è—Ç—å –∏ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Ä—É–∫–æ
[13.01.2026 08:34] Using data from previous issue: {"categories": ["#open_source"], "emoji": "üé¨", "ru": {"title": "–¢—Ä—ë—Ö–º–µ—Ä–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –±–µ–∑ –≥—Ä–∞–Ω–∏—Ü: –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "3D CoCa v2 ‚Äî —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–Ω–∏—è –∏ —è–∑—ã–∫
[13.01.2026 08:34] Using data from previous issue: {"categories": ["#audio", "#benchmark"], "emoji": "üé§", "ru": {"title": "–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫, –∞ –Ω–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Å—ã—Ä–æ–º –∞—É–¥–∏–æ, –∫–æ—Ç–æ—Ä—ã–µ —Å–ø–æ—Å–æ–±–Ω—ã –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ä–µ—á–µ–≤—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, 
[13.01.2026 08:34] Renaming data file.
[13.01.2026 08:34] Renaming previous data. hf_papers.json to ./d/2026-01-13.json
[13.01.2026 08:34] Saving new data file.
[13.01.2026 08:34] Generating page.
[13.01.2026 08:34] Renaming previous page.
[13.01.2026 08:34] Renaming previous data. index.html to ./d/2026-01-13.html
[13.01.2026 08:34] Writing result.
[13.01.2026 08:34] Renaming log file.
[13.01.2026 08:34] Renaming previous data. log.txt to ./logs/2026-01-13_last_log.txt
