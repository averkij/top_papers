[27.02.2025 08:15] Read previous papers.
[27.02.2025 08:15] Generating top page (month).
[27.02.2025 08:15] Writing top page (month).
[27.02.2025 09:11] Read previous papers.
[27.02.2025 09:11] Get feed.
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18934
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.19400
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.19361
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.19328
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17955
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.19414
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18864
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16776
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18906
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18772
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.19204
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.19279
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17540
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16284
[27.02.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2502.19187
[27.02.2025 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.02.2025 09:11] No deleted papers detected.
[27.02.2025 09:11] Downloading and parsing papers (pdf, html). Total: 15.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18934.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18934.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18934.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.19400.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.19400.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.19400.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.19361.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.19361.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.19361.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.19328.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.19328.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.19328.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.17955.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.17955.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.17955.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.19414.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.19414.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.19414.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18864.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18864.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18864.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.16776.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.16776.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.16776.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18906.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18906.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18906.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18772.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.18772.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.18772.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.19204.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.19204.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.19204.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.19279.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.19279.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.19279.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.17540.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.17540.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.17540.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.16284.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.16284.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.16284.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.19187.
[27.02.2025 09:11] Extra JSON file exists (./assets/json/2502.19187.json), skip PDF parsing.
[27.02.2025 09:11] Paper image links file exists (./assets/img_data/2502.19187.json), skip HTML parsing.
[27.02.2025 09:11] Success.
[27.02.2025 09:11] Enriching papers with extra data.
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 0. We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English. The computational cost of Kanana is significantly lower than that of state-of-the-art models of similar size. The report details the techniques employed...
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 1. Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their abil...
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 2. Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique ab...
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 3. Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propos...
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 4. Multilingual language models (LMs) are expected to recall factual knowledge consistently across languages, yet they often fail to transfer knowledge between languages even when they possess the correct information in one of the languages. For example, we find that an LM may correctly identify Rashed...
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 5. There is growing excitement about the potential of Language Models (LMs) to accelerate scientific discovery. Falsifying hypotheses is key to scientific progress, as it allows claims to be iteratively refined over time. This process requires significant researcher effort, reasoning, and ingenuity. Ye...
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 6. Scientific discovery relies on scientists generating novel hypotheses that undergo rigorous experimental validation. To augment this process, we introduce an AI co-scientist, a multi-agent system built on Gemini 2.0. The AI co-scientist is intended to help uncover new, original knowledge and to form...
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 7. As AI models are increasingly deployed across diverse real-world scenarios, ensuring their safety remains a critical yet underexplored challenge. While substantial efforts have been made to evaluate and enhance AI safety, the lack of a standardized framework and comprehensive toolkit poses significa...
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 8. Training Vision-Language Models (VLMs) for Graphical User Interfaces (GUI) agents via Reinforcement Learning (RL) faces critical challenges: environment-based RL requires costly interactions, while environment-free methods struggle with distribution shift and reward generalization. We propose an env...
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 9. Despite Greece's pivotal role in the global economy, large language models (LLMs) remain underexplored for Greek financial context due to the linguistic complexity of Greek and the scarcity of domain-specific datasets. Previous efforts in multilingual financial natural language processing (NLP) have...
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 10. Monocular depth estimation (MDE) aims to predict scene depth from a single RGB image and plays a crucial role in 3D scene understanding. Recent advances in zero-shot MDE leverage normalized depth representations and distillation-based learning to improve generalization across diverse scenes. However...
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 11. Language model heavily depends on high-quality data for optimal performance. Existing approaches rely on manually designed heuristics, the perplexity of existing models, training classifiers, or careful prompt engineering, which require significant expert experience and human annotation effort while...
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 12. Generating accurate and concise textual summaries from multimodal documents is challenging, especially when dealing with visually complex content like scientific posters. We introduce PosterSum, a novel benchmark to advance the development of vision-language models that can understand and summarize ...
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 13. Establishing the relationship between 3D structures and the energy states of molecular systems has proven to be a promising approach for learning 3D molecular representations. However, existing methods are limited to modeling the molecular energy states from classical mechanics. This limitation resu...
[27.02.2025 09:11] ********************************************************************************
[27.02.2025 09:11] Abstract 14. Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning benchmarks predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader rea...
[27.02.2025 09:11] Read previous papers.
[27.02.2025 09:11] Generating reviews via LLM API.
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#rag", "#open_source", "#machine_translation", "#low_resource", "#dataset", "#multilingual", "#data", "#small_models", "#training"], "emoji": "🇰🇷", "ru": {"title": "Kanana: эффективные двуязычные языковые модели для корейского и английского", "desc": "Представлена серия двуязычных я
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#reasoning", "#video", "#agents", "#interpretability", "#multimodal", "#benchmark"], "emoji": "🧮", "ru": {"title": "Агентный подход к визуальному объяснению теорем", "desc": "Данная работа представляет TheoremExplainAgent - агентный подход для генерации длинных видео-объяснений теор
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#long_context", "#dataset", "#benchmark", "#training", "#reasoning"], "emoji": "🧠", "ru": {"title": "DeltaBench: новый инструмент для анализа длинных цепочек рассуждений в ИИ", "desc": "Статья представляет DeltaBench - набор данных для оценки способности обнаружения ошибок в длинных
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#alignment", "#training", "#open_source", "#rlhf", "#benchmark"], "emoji": "🤖", "ru": {"title": "Агентное моделирование наград: новый шаг в улучшении крупных языковых моделей", "desc": "Статья представляет новый подход к моделированию наград для крупных языковых моделей, названный а
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#multilingual", "#benchmark", "#transfer_learning", "#low_resource"], "emoji": "🌐", "ru": {"title": "Преодоление языковых барьеров в фактических знаниях нейросетей", "desc": "Исследователи обнаружили, что многоязычные языковые модели часто не могут переносить фактические знания межд
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#reasoning", "#science", "#plp", "#benchmark"], "emoji": "🔍", "ru": {"title": "Новый взгляд на оценку ЛМ: от решения задач к опровержению гипотез", "desc": "Статья представляет новый подход к оценке языковых моделей (ЛМ) в контексте научного прогресса. Авторы предлагают бенчмарк REF
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#agents", "#science", "#healthcare", "#architecture"], "emoji": "🧬", "ru": {"title": "ИИ-соученый: революция в научном открытии", "desc": "Статья представляет систему ИИ-соученого, многоагентную систему на базе Gemini 2.0, предназначенную для помощи ученым в генерации новых гипотез 
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#dataset", "#security"], "emoji": "🛡️", "ru": {"title": "Единая лаборатория для обеспечения безопасности ИИ", "desc": "AISafetyLab - это унифицированная платформа и набор инструментов для исследования безопасности искусственного интеллекта. Она интегрир
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#reasoning", "#training", "#agents", "#rl", "#optimization"], "emoji": "🤖", "ru": {"title": "Семантическое понимание для эффективной автоматизации GUI без прямого взаимодействия", "desc": "Статья представляет новый подход к обучению моделей компьютерного зрения и языка для автоматиз
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#dataset", "#low_resource", "#multilingual"], "emoji": "💶", "ru": {"title": "Прорыв в греческом финансовом NLP: новые инструменты и вызовы", "desc": "Статья представляет первый греческий финансовый эталонный тест Plutus-ben и первую греческую финансовую
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#3d", "#cv", "#training", "#benchmark"], "emoji": "🔍", "ru": {"title": "Улучшение монокулярной оценки глубины через контекстную дистилляцию", "desc": "Статья посвящена монокулярной оценке глубины сцены по одному RGB-изображению. Авторы предлагают новый метод Cross-Context Distillati
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#data", "#optimization", "#interpretability", "#dataset", "#agents", "#training"], "emoji": "🔍", "ru": {"title": "Умный отбор данных для лучших языковых моделей", "desc": "CritiQ - это новый метод отбора данных для обучения языковых моделей. Он автоматически выводит критерии качеств
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#benchmark", "#science", "#dataset", "#multimodal"], "emoji": "📊", "ru": {"title": "PosterSum: новый бенчмарк для суммаризации научных постеров", "desc": "Статья представляет новый бенчмарк PosterSum для развития мультимодальных языковых моделей, способных понимать и суммировать нау
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#science", "#dataset", "#3d", "#benchmark", "#architecture"], "emoji": "🔬", "ru": {"title": "Квантовый скачок в машинном обучении молекулярных структур", "desc": "Статья предлагает новый подход к обучению 3D-представлений молекул с использованием энергетических спектров. Авторы разр
[27.02.2025 09:11] Using data from previous issue: {"categories": ["#reasoning", "#benchmark"], "emoji": "🧠", "ru": {"title": "Новый вызов для искусственного интеллекта: усложненный тест на рассуждение", "desc": "Статья представляет новый набор данных BIG-Bench Extra Hard (BBEH) для оценки способностей больших языковых моделей (LLM) к рассуждению. B
[27.02.2025 09:11] Trying to get texts in Chinese.
[27.02.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English. The computational cost of Kanana is significantly lower than that of state-of-the-art models of similar size. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation. Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users. Lastly, the report elaborates on plausible approaches used for language model adaptation to specific scenarios, such as embedding, retrieval augmented generation, and function calling. The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models.
[27.02.2025 09:11] Mistral response. {"id": "9812def5ae9345298f85fa69043a9f60", "object": "chat.completion", "created": 1740647479, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u6211\u4eec\u4ecb\u7ecd\u4e86Kanana\uff0c\u4e00\u7cfb\u5217\u5728\u97e9\u8bed\u548c\u82f1\u8bed\u4e2d\u8868\u73b0\u51fa\u8272\u7684\u53cc\u8bed\u6a21\u578b\u3002Kanana\u7684\u8ba1\u7b97\u6210\u672c\u663e\u8457\u4f4e\u4e8e\u7c7b\u4f3c\u89c4\u6a21\u7684\u9876\u7ea7\u6a21\u578b\u3002\u62a5\u544a\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u9884\u8bad\u7ec3\u4e2d\u4f7f\u7528\u7684\u6280\u672f\uff0c\u5305\u62ec\u9ad8\u8d28\u91cf\u6570\u636e\u8fc7\u6ee4\u3001\u5206\u9636\u6bb5\u9884\u8bad\u7ec3\u3001\u6df1\u5ea6\u6269\u5c55\u3001\u526a\u679d\u548c\u84b8\u998f\u3002\u6b64\u5916\uff0c\u62a5\u544a\u8fd8\u6982\u8ff0\u4e86Kanana\u6a21\u578b\u5728\u8bad\u7ec3\u540e\u4f7f\u7528\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u76d1\u7763\u5fae\u8c03\u548c\u504f\u597d\u4f18\u5316\uff0c\u4ee5\u63d0\u9ad8\u5176\u4e0e\u7528\u6237\u4e92\u52a8\u7684\u80fd\u529b\u3002\u6700\u540e\uff0c\u62a5\u544a\u8fd8\u8ba8\u8bba\u4e86\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u7279\u5b9a\u573a\u666f\u7684\u53ef\u80fd\u65b9\u6cd5\uff0c\u5982\u5d4c\u5165\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u51fd\u6570\u8c03\u7528\u3002Kanana\u6a21\u578b\u7cfb\u5217\u4ece2.1B\u523032.5B\u53c2\u6570\u4e0d\u7b49\uff0c2.1B\u6a21\u578b\uff08\u57fa\u7840\u3001\u6307\u4ee4\u3001\u5d4c\u5165\uff09\u5df2\u516c\u5f00\u53d1\u5e03\uff0c\u4ee5\u4fc3\u8fdb\u97e9\u8bed\u6a21\u578b\u7814\u7a76\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 252, "total_tokens": 509, "completion_tokens": 257}}
[27.02.2025 09:11] Response: 我们介绍了Kanana，一系列在韩语和英语中表现出色的双语模型。Kanana的计算成本显著低于类似规模的顶级模型。报告详细介绍了预训练中使用的技术，包括高质量数据过滤、分阶段预训练、深度扩展、剪枝和蒸馏。此外，报告还概述了Kanana模型在训练后使用的方法，包括监督微调和偏好优化，以提高其与用户互动的能力。最后，报告还讨论了语言模型适应特定场景的可能方法，如嵌入、检索增强生成和函数调用。Kanana模型系列从2.1B到32.5B参数不等，2.1B模型（基础、指令、嵌入）已公开发布，以促进韩语模型研究。
[27.02.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

我们介绍了Kanana，一系列在韩语和英语中表现出色的双语模型。Kanana的计算成本显著低于类似规模的顶级模型。报告详细介绍了预训练中使用的技术，包括高质量数据过滤、分阶段预训练、深度扩展、剪枝和蒸馏。此外，报告还概述了Kanana模型在训练后使用的方法，包括监督微调和偏好优化，以提高其与用户互动的能力。最后，报告还讨论了语言模型适应特定场景的可能方法，如嵌入、检索增强生成和函数调用。Kanana模型系列从2.1B到32.5B参数不等，2.1B模型（基础、指令、嵌入）已公开发布，以促进韩语模型研究。
[27.02.2025 09:11] Mistral response. {"id": "9072c37095a44eb2a0852e06fd61cd6e", "object": "chat.completion", "created": 1740647485, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "W\u01d2men ji\u00e8sh\u00e0o le Kanana, y\u012b x\u00ecli\u00e8 z\u00e0i h\u00e1ny\u01d4 h\u00e9 y\u012bngy\u01d4 zh\u014dng bi\u01ceoxi\u00e0n ch\u016bs\u00e8 de shu\u0101ngy\u01d4 m\u00f3x\u00edng. Kanana de j\u00ecsu\u00e0n ch\u00e9ngb\u011bn xi\u01cenzh\u00f9 d\u012by\u00fa l\u00e8is\u00ec gu\u012bm\u00f3 de d\u01d0ngj\u00ed m\u00f3x\u00edng. B\u00e0og\u00e0o xi\u00e1ngx\u00ec ji\u00e8sh\u00e0o le y\u00f9x\u00f9nli\u00e0n zh\u014dng sh\u01d0y\u00f2ng de j\u00ecsh\u00f9, b\u0101oku\u00f2 g\u0101o zh\u00ecli\u00e0ng sh\u00f9j\u00f9 gu\u00f2l\u01dc, f\u0113n ji\u0113du\u00e0n y\u00f9x\u00f9nli\u00e0n, sh\u0113nd\u00f9 ku\u00f2zh\u01cen, ji\u01cenzh\u012b h\u00e9 zh\u0113ngli\u00fa. C\u01d0w\u00e0i, b\u00e0og\u00e0o h\u00e1i g\u00e0iku\u00e0ng le Kanana m\u00f3x\u00edng z\u00e0i x\u00f9nli\u00e0n h\u00f2u sh\u01d0y\u00f2ng de f\u0101ngf\u01ce, b\u0101oku\u00f2 ji\u00e0nd\u016b w\u0113iti\u00e1o h\u00e9 pi\u0101nh\u01ceo y\u014duhu\u00e0, y\u01d0 t\u00edg\u0101o q\u00ed y\u01d4 y\u00f2ngh\u00f9 h\u00f9d\u00f2ng de n\u00e9ngl\u00ec. Z\u00f9ih\u00f2u, b\u00e0og\u00e0o h\u00e1i t\u01ceol\u00f9n le y\u01d4y\u00e1n m\u00f3x\u00edng sh\u00ecy\u00ecng t\u00e8d\u00ecng ch\u01cengj\u012bng de k\u011bn\u00e9ng f\u0101ngf\u01ce, r\u00fa qi\u00e0nr\u00f9, ji\u01censu\u01d2 z\u0113ngqi\u00e1ng sh\u0113ngch\u00e9ng h\u00e9 h\u00e1nsh\u00f9 di\u00e0oy\u00f2ng. Kanana m\u00f3x\u00edng x\u00ecli\u00e8 c\u00f3ng 2.1B d\u00e0o 32.5B c\u0101nsh\u00f9 b\u00f9d\u011bng, 2.1B m\u00f3x\u00edng (j\u012bch\u01d4, zh\u01d0l\u00ecng, qi\u00e0nr\u00f9) y\u01d0 g\u014dngk\u0101i f\u0101b\u00f9, y\u01d0 c\u00f9j\u00ecn h\u00e1ny\u01d4 m\u00f3x\u00edng y\u00e1nji\u016b."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 272, "total_tokens": 774, "completion_tokens": 502}}
[27.02.2025 09:11] Response: Wǒmen jièshào le Kanana, yī xìliè zài hányǔ hé yīngyǔ zhōng biǎoxiàn chūsè de shuāngyǔ móxíng. Kanana de jìsuàn chéngběn xiǎnzhù dīyú lèisì guīmó de dǐngjí móxíng. Bàogào xiángxì jièshào le yùxùnliàn zhōng shǐyòng de jìshù, bāokuò gāo zhìliàng shùjù guòlǜ, fēn jiēduàn yùxùnliàn, shēndù kuòzhǎn, jiǎnzhī hé zhēngliú. Cǐwài, bàogào hái gàikuàng le Kanana móxíng zài xùnliàn hòu shǐyòng de fāngfǎ, bāokuò jiàndū wēitiáo hé piānhǎo yōuhuà, yǐ tígāo qí yǔ yònghù hùdòng de nénglì. Zùihòu, bàogào hái tǎolùn le yǔyán móxíng shìyìng tèdìng chǎngjīng de kěnéng fāngfǎ, rú qiànrù, jiǎnsuǒ zēngqiáng shēngchéng hé hánshù diàoyòng. Kanana móxíng xìliè cóng 2.1B dào 32.5B cānshù bùděng, 2.1B móxíng (jīchǔ, zhǐlìng, qiànrù) yǐ gōngkāi fābù, yǐ cùjìn hányǔ móxíng yánjiū.
[27.02.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

我们介绍了Kanana，一系列在韩语和英语中表现出色的双语模型。Kanana的计算成本显著低于类似规模的顶级模型。报告详细介绍了预训练中使用的技术，包括高质量数据过滤、分阶段预训练、深度扩展、剪枝和蒸馏。此外，报告还概述了Kanana模型在训练后使用的方法，包括监督微调和偏好优化，以提高其与用户互动的能力。最后，报告还讨论了语言模型适应特定场景的可能方法，如嵌入、检索增强生成和函数调用。Kanana模型系列从2.1B到32.5B参数不等，2.1B模型（基础、指令、嵌入）已公开发布，以促进韩语模型研究。
[27.02.2025 09:11] Mistral response. {"id": "9b0807c3265d46c69faab7e6771b8d1c", "object": "chat.completion", "created": 1740647495, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u4ecb\u7ecd', 'pinyin': 'ji\u00e8sh\u00e0o', 'trans': 'introduce'},\n{'word': '\u53cc\u8bed', 'pinyin': 'shu\u0101ngy\u01d4', 'trans': 'bilingual'},\n{'word': '\u8ba1\u7b97\u6210\u672c', 'pinyin': 'j\u00ecsu\u00e0n ch\u00e9ngb\u011bn', 'trans': 'computational cost'},\n{'word': '\u663e\u8457', 'pinyin': 'xi\u01cenzh\u00f9', 'trans': 'significant'},\n{'word': '\u9884\u8bad\u7ec3', 'pinyin': 'y\u00f9 x\u00f9nli\u00e0n', 'trans': 'pre-training'},\n{'word': '\u9ad8\u8d28\u91cf', 'pinyin': 'g\u0101o zh\u00ecli\u00e0ng', 'trans': 'high-quality'},\n{'word': '\u8fc7\u6ee4', 'pinyin': 'gu\u00f2l\u01dc', 'trans': 'filter'},\n{'word': '\u5206\u9636\u6bb5', 'pinyin': 'f\u0113n ji\u0113du\u00e0n', 'trans': 'phased'},\n{'word': '\u6df1\u5ea6\u6269\u5c55', 'pinyin': 'sh\u0113nd\u00f9 ku\u00f2zh\u01cen', 'trans': 'deep expansion'},\n{'word': '\u526a\u679d', 'pinyin': 'ji\u01cenzh\u012b', 'trans': 'pruning'},\n{'word': '\u84b8\u998f', 'pinyin': 'zh\u0113ngli\u00fa', 'trans': 'distillation'},\n{'word': '\u76d1\u7763\u5fae\u8c03', 'pinyin': 'ji\u00e0nd\u016b w\u0113iti\u00e1o', 'trans': 'supervised fine-tuning'},\n{'word': '\u504f\u597d\u4f18\u5316', 'pinyin': 'pi\u0101nh\u00e0o y\u014duhu\u00e0', 'trans': 'preference optimization'},\n{'word': '\u4e92\u52a8', 'pinyin': 'h\u00f9d\u00f2ng', 'trans': 'interaction'},\n{'word': '\u5d4c\u5165', 'pinyin': 'qi\u00e0nr\u00f9', 'trans': 'embedding'},\n{'word': '\u68c0\u7d22\u589e\u5f3a\u751f\u6210', 'pinyin': 'ji\u01censu\u01d2 z\u0113ngqi\u00e1ng sh\u0113ngch\u00e9ng', 'trans': 'retrieval-augmented generation'},\n{'word': '\u51fd\u6570\u8c03\u7528', 'pinyin': 'h\u00e1nsh\u00f9 di\u00e0oy\u00f2ng', 'trans': 'function call'},\n{'word': '\u53c2\u6570', 'pinyin': 'c\u0101nsh\u00f9', 'trans': 'parameters'},\n{'word': '\u516c\u5f00\u53d1\u5e03', 'pinyin': 'g\u014dngk\u0101i f\u0101b\u00f9', 'trans': 'publicly released'},\n{'word': '\u4fc3\u8fdb', 'pinyin': 'c\u00f9j\u00ecn', 'trans': 'promote'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 304, "total_tokens": 943, "completion_tokens": 639}}
[27.02.2025 09:11] Response: [{'word': '介绍', 'pinyin': 'jièshào', 'trans': 'introduce'},
{'word': '双语', 'pinyin': 'shuāngyǔ', 'trans': 'bilingual'},
{'word': '计算成本', 'pinyin': 'jìsuàn chéngběn', 'trans': 'computational cost'},
{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},
{'word': '预训练', 'pinyin': 'yù xùnliàn', 'trans': 'pre-training'},
{'word': '高质量', 'pinyin': 'gāo zhìliàng', 'trans': 'high-quality'},
{'word': '过滤', 'pinyin': 'guòlǜ', 'trans': 'filter'},
{'word': '分阶段', 'pinyin': 'fēn jiēduàn', 'trans': 'phased'},
{'word': '深度扩展', 'pinyin': 'shēndù kuòzhǎn', 'trans': 'deep expansion'},
{'word': '剪枝', 'pinyin': 'jiǎnzhī', 'trans': 'pruning'},
{'word': '蒸馏', 'pinyin': 'zhēngliú', 'trans': 'distillation'},
{'word': '监督微调', 'pinyin': 'jiàndū wēitiáo', 'trans': 'supervised fine-tuning'},
{'word': '偏好优化', 'pinyin': 'piānhào yōuhuà', 'trans': 'preference optimization'},
{'word': '互动', 'pinyin': 'hùdòng', 'trans': 'interaction'},
{'word': '嵌入', 'pinyin': 'qiànrù', 'trans': 'embedding'},
{'word': '检索增强生成', 'pinyin': 'jiǎnsuǒ zēngqiáng shēngchéng', 'trans': 'retrieval-augmented generation'},
{'word': '函数调用', 'pinyin': 'hánshù diàoyòng', 'trans': 'function call'},
{'word': '参数', 'pinyin': 'cānshù', 'trans': 'parameters'},
{'word': '公开发布', 'pinyin': 'gōngkāi fābù', 'trans': 'publicly released'},
{'word': '促进', 'pinyin': 'cùjìn', 'trans': 'promote'}]
[27.02.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

我们介绍了Kanana，一系列在韩语和英语中表现出色的双语模型。Kanana的计算成本显著低于类似规模的顶级模型。报告详细介绍了预训练中使用的技术，包括高质量数据过滤、分阶段预训练、深度扩展、剪枝和蒸馏。此外，报告还概述了Kanana模型在训练后使用的方法，包括监督微调和偏好优化，以提高其与用户互动的能力。最后，报告还讨论了语言模型适应特定场景的可能方法，如嵌入、检索增强生成和函数调用。Kanana模型系列从2.1B到32.5B参数不等，2.1B模型（基础、指令、嵌入）已公开发布，以促进韩语模型研究。
[27.02.2025 09:12] Mistral response. {"id": "ce639dc20eaf4aadab5d7a4e996139db", "object": "chat.completion", "created": 1740647518, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "We introduced Kanana, a series of bilingual models that perform excellently in Korean and English. Kanana's computational cost is significantly lower than that of top models of similar scale. The report details the techniques used during pre-training, including high-quality data filtering, staged pre-training, deep scaling, pruning, and distillation. Additionally, the report outlines the methods used with the Kanana models post-training, such as supervised fine-tuning and preference optimization, to enhance their ability to interact with users. Finally, the report discusses potential methods for adapting language models to specific scenarios, such as embedding, retrieval-augmented generation, and function calling. The Kanana model series ranges from 2.1B to 32.5B parameters, with the 2.1B model (base, instruction, embedding) already publicly released to promote research on Korean language models."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 270, "total_tokens": 466, "completion_tokens": 196}}
[27.02.2025 09:12] Response: We introduced Kanana, a series of bilingual models that perform excellently in Korean and English. Kanana's computational cost is significantly lower than that of top models of similar scale. The report details the techniques used during pre-training, including high-quality data filtering, staged pre-training, deep scaling, pruning, and distillation. Additionally, the report outlines the methods used with the Kanana models post-training, such as supervised fine-tuning and preference optimization, to enhance their ability to interact with users. Finally, the report discusses potential methods for adapting language models to specific scenarios, such as embedding, retrieval-augmented generation, and function calling. The Kanana model series ranges from 2.1B to 32.5B parameters, with the 2.1B model (base, instruction, embedding) already publicly released to promote research on Korean language models.
[27.02.2025 09:12] Renaming data file.
[27.02.2025 09:12] Renaming previous data. hf_papers.json to ./d/2025-02-27.json
[27.02.2025 09:12] Saving new data file.
[27.02.2025 09:12] Generating page.
[27.02.2025 09:12] Renaming previous page.
[27.02.2025 09:12] Renaming previous data. index.html to ./d/2025-02-27.html
[27.02.2025 09:12] [Experimental] Generating Chinese page for reading.
[27.02.2025 09:12] Chinese vocab [{'word': '介绍', 'pinyin': 'jièshào', 'trans': 'introduce'}, {'word': '双语', 'pinyin': 'shuāngyǔ', 'trans': 'bilingual'}, {'word': '计算成本', 'pinyin': 'jìsuàn chéngběn', 'trans': 'computational cost'}, {'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'}, {'word': '预训练', 'pinyin': 'yù xùnliàn', 'trans': 'pre-training'}, {'word': '高质量', 'pinyin': 'gāo zhìliàng', 'trans': 'high-quality'}, {'word': '过滤', 'pinyin': 'guòlǜ', 'trans': 'filter'}, {'word': '分阶段', 'pinyin': 'fēn jiēduàn', 'trans': 'phased'}, {'word': '深度扩展', 'pinyin': 'shēndù kuòzhǎn', 'trans': 'deep expansion'}, {'word': '剪枝', 'pinyin': 'jiǎnzhī', 'trans': 'pruning'}, {'word': '蒸馏', 'pinyin': 'zhēngliú', 'trans': 'distillation'}, {'word': '监督微调', 'pinyin': 'jiàndū wēitiáo', 'trans': 'supervised fine-tuning'}, {'word': '偏好优化', 'pinyin': 'piānhào yōuhuà', 'trans': 'preference optimization'}, {'word': '互动', 'pinyin': 'hùdòng', 'trans': 'interaction'}, {'word': '嵌入', 'pinyin': 'qiànrù', 'trans': 'embedding'}, {'word': '检索增强生成', 'pinyin': 'jiǎnsuǒ zēngqiáng shēngchéng', 'trans': 'retrieval-augmented generation'}, {'word': '函数调用', 'pinyin': 'hánshù diàoyòng', 'trans': 'function call'}, {'word': '参数', 'pinyin': 'cānshù', 'trans': 'parameters'}, {'word': '公开发布', 'pinyin': 'gōngkāi fābù', 'trans': 'publicly released'}, {'word': '促进', 'pinyin': 'cùjìn', 'trans': 'promote'}]
[27.02.2025 09:12] Renaming previous Chinese page.
[27.02.2025 09:12] Renaming previous data. zh.html to ./d/2025-02-26_zh_reading_task.html
[27.02.2025 09:12] Writing Chinese reading task.
[27.02.2025 09:12] Writing result.
[27.02.2025 09:12] Renaming log file.
[27.02.2025 09:12] Renaming previous data. log.txt to ./logs/2025-02-27_last_log.txt
