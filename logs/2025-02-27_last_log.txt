[27.02.2025 00:47] Read previous papers.
[27.02.2025 00:47] Generating top page (month).
[27.02.2025 00:47] Writing top page (month).
[27.02.2025 02:15] Read previous papers.
[27.02.2025 02:15] Get feed.
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18411
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18137
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18449
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17363
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18364
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17262
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15499
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16069
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18461
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18356
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17425
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14855
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16825
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17535
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16794
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17422
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17814
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15612
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17092
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17910
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18302
[27.02.2025 02:15] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18316
[27.02.2025 02:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.02.2025 02:15] No deleted papers detected.
[27.02.2025 02:15] Downloading and parsing papers (pdf, html). Total: 22.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.18411.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.18411.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.18411.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.18137.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.18137.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.18137.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.18449.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.18449.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.18449.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.17363.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.17363.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.17363.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.18364.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.18364.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.18364.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.17262.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.17262.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.17262.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.15499.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.15499.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.15499.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.16069.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.16069.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.16069.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.18461.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.18461.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.18461.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.18356.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.18356.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.18356.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.17425.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.17425.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.17425.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.14855.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.14855.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.14855.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.16825.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.16825.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.16825.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.17535.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.17535.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.17535.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.16794.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.16794.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.16794.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.17422.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.17422.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.17422.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.17814.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.17814.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.17814.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.15612.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.15612.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.15612.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.17092.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.17092.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.17092.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.17910.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.17910.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.17910.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.18302.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.18302.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.18302.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Downloading and parsing paper https://huggingface.co/papers/2502.18316.
[27.02.2025 02:15] Extra JSON file exists (./assets/json/2502.18316.json), skip PDF parsing.
[27.02.2025 02:15] Paper image links file exists (./assets/img_data/2502.18316.json), skip HTML parsing.
[27.02.2025 02:15] Success.
[27.02.2025 02:15] Enriching papers with extra data.
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 0. Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featur...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 1. An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the s...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 2. The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, thi...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 3. Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free appr...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 4. Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variab...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 5. The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) t...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 6. Training stability is a persistent challenge in the pre-training of large language models (LLMs), particularly for architectures such as Post-Norm Transformers, which are prone to gradient explosion and dissipation. In this paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 7. Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results. Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigo...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 8. Recent studies have explored combining different LoRAs to jointly generate learned style and content. However, existing methods either fail to effectively preserve both the original subject and style simultaneously or require additional training. In this paper, we argue that the intrinsic properties...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 9. We introduce WebGames, a comprehensive benchmark suite designed to evaluate general-purpose web-browsing AI agents through a collection of 50+ interactive challenges. These challenges are specifically crafted to be straightforward for humans while systematically testing the limitations of current AI...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 10. To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM stil...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 11. Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts. This averaging obscures user- and prompt-specific variations in model performance. To address this, we propose Prompt-to-Leaderboard (P2L), a method that...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 12. Iterative data generation and model retraining are widely used to align large language models (LLMs). It typically involves a policy model to generate on-policy responses and a reward model to guide training data selection. Direct Preference Optimization (DPO) further enhances this process by constr...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 13. Motivated by reducing the computational and storage costs of LLMs, model compression and KV cache compression have attracted much attention from researchers. However, current methods predominantly emphasize maintaining the performance of compressed LLMs, as measured by perplexity or simple accuracy ...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 14. Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existi...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 15. Multimodal Large Language Models (MLLMs) have experienced rapid progress in visual recognition tasks in recent years. Given their potential integration into many critical applications, it is important to understand the limitations of their visual perception. In this work, we study whether MLLMs can ...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 16. Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks such as text generation, reasoning, and decision-making. While their success has primarily been driven by advances in computational power and dee...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 17. State space models (SSMs), such as Mamba, have emerged as an efficient alternative to transformers for long-context sequence modeling. However, despite their growing adoption, SSMs lack the interpretability tools that have been crucial for understanding and improving attention-based architectures. W...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 18. We introduce Shakti VLM, a family of vision-language models in the capacity of 1B and 4B parameters designed to address data efficiency challenges in multimodal learning. While recent VLMs achieve strong performance through extensive training data, Shakti models leverage architectural innovations to...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 19. Modern language models rely on static vocabularies, fixed before pretraining, in contrast to the adaptive vocabulary acquisition observed in human language learning. To bridge this gap, we introduce vocabulary curriculum learning, an approach that improves pretraining efficiency with log-linear scal...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 20. In this paper, we introduce LDGen, a novel method for integrating large language models (LLMs) into existing text-to-image diffusion models while minimizing computational demands. Traditional text encoders, such as CLIP and T5, exhibit limitations in multilingual processing, hindering image generati...
[27.02.2025 02:15] ********************************************************************************
[27.02.2025 02:15] Abstract 21. We introduce WiCkeD, a simple method to increase the complexity of existing multiple-choice benchmarks by randomly replacing a choice with "None of the above", a method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challen...
[27.02.2025 02:15] Read previous papers.
[27.02.2025 02:15] Generating reviews via LLM API.
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#alignment", "#training", "#benchmark", "#multimodal", "#dataset", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Ü–µ–Ω–Ω–æ—Å—Ç—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OmniAlign-V - –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö 
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#architecture", "#inference", "#video", "#optimization", "#cv"], "emoji": "üöÄ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ SpargeAttn –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –æ–Ω
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#dataset", "#math", "#open_source"], "emoji": "üß†", "ru": {"title": "SWE-RL: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ü–û", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SWE-RL - –ø–µ—Ä–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –¥–ª
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#training", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "KV-Edit: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ñ–æ–Ω–∞", "desc": "KV-Edit - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ KV-–∫—ç—à–∞ –≤ DiT-–º–æ–¥–µ–ª—è—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#multimodal", "#cv"], "emoji": "üé≠", "ru": {"title": "ART: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Anonymous Region Transformer (ART) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –ø—Ä–æ–∑—Ä–∞—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏ –∞–Ω–æ–Ω–∏–º–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –æ–±–ª–∞—Å
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#training", "#benchmark"], "emoji": "üîÆ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "üöÄ", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Scale-Distribution Decoupling (SDD) –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#agents", "#training", "#science", "#interpretability", "#benchmark", "#open_source"], "emoji": "üß™", "ru": {"title": "Curie: –ò–ò-–∞–≥–µ–Ω—Ç –¥–ª—è —Å—Ç—Ä–æ–≥–∏—Ö –Ω–∞—É—á–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Curie - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ò–ò-–∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –Ω–∞—É—á–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. Curie —Å–æ—Å—Ç–æ–∏—Ç –∏
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#training"], "emoji": "üé®", "ru": {"title": "K-LoRA: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —Å—Ç–∏–ª—è –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ K-LoRA –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LoRA (Low-Rank Adaptation) –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#games", "#agents"], "emoji": "üåê", "ru": {"title": "WebGames: —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–µ–±-—Å—Ü–µ–Ω–∞—Ä–∏—è—Ö", "desc": "WebGames - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤ –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥–ª—è –≤–µ–±-–±—Ä–∞—É–∑–∏–Ω–≥–∞. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –±–æ–ª
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#architecture", "#cv"], "emoji": "üëÅÔ∏è", "ru": {"title": "–¢–æ–∫–µ–Ω—ã –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è: –Ω–æ–≤—ã–π —à–∞–≥ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è –¢–æ–∫–µ–Ω–æ–≤ –í–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –í–æ—Å–ø—Ä–∏—è—Ç–∏—è –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (ML
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#open_source", "#interpretability", "#training"], "emoji": "üèÜ", "ru": {"title": "–¢–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á–µ—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Prompt-to-Leaderboard (P2L) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á–µ—Ç–æ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#training", "#alignment", "#rlhf"], "emoji": "üìä", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ—Ü–µ—Å—Å–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –ø–æ–º–æ—â—å—é –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ê–≤—Ç
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#inference", "#optimization", "#reasoning", "#rag", "#training"], "emoji": "üéüÔ∏è", "ru": {"title": "–õ–æ—Ç–µ—Ä–µ–π–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É —Å–∂–∞—Ç–∏—é –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≥–∏–ø–æ—Ç–µ–∑—É –ª–æ—Ç–µ—Ä–µ–π–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (LLM) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å–∂–∞—Ç–∏—è –º–æ–¥–µ–ª–µ–π –∏
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#alignment", "#audio"], "emoji": "üëÇ", "ru": {"title": "–°–ª—É—à–∞—Ç—å –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫: –ò–ò —Å –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —É—á–∏—Ç—ã–≤–∞—é—â–∏–π –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#optimization", "#interpretability", "#training", "#cv"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –º–µ–ª–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM) –º–µ–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#training", "#math", "#ethics", "#interpretability", "#architecture", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–∂–µ –¥–æ–≤–µ—Ä–∏—è –∫ —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) —Å—Ç–∞–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è 
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#interpretability", "#training", "#architecture", "#long_context", "#multimodal", "#machine_translation"], "emoji": "üîç", "ru": {"title": "–ó–∞–≥–ª—è–Ω—É—Ç—å –≤–Ω—É—Ç—Ä—å Mamba: –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ SSM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π —Å–æ—Å—Ç–æ—è–Ω–∏—è (SSM), —Ç–∞–∫–∏—Ö –∫–∞–∫
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#reasoning", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ª–∏–Ω–µ–π–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Shakti VLM —Å 1B –∏ 4B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–∞—Ü–µ–ª–µ–Ω–Ω—ã—Ö –Ω–∞ –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#training", "#open_source", "#transfer_learning", "#architecture"], "emoji": "üìö", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - curric
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#long_context", "#multimodal", "#low_resource", "#multilingual", "#training", "#diffusion"], "emoji": "üåê", "ru": {"title": "LDGen: –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "LDGen - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —Å—É—â
[27.02.2025 02:15] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#benchmark"], "emoji": "üß†", "ru": {"title": "WiCkeD: –ü–æ–≤—ã—à–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ WiCkeD, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤—ã—à–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ—Å—Ç–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –ø—É—Ç–µ–º —Å–ª—É—á–∞–π–Ω–æ–π –∑–∞–º–µ–Ω
[27.02.2025 02:15] Loading Chinese text from previous data.
[27.02.2025 02:15] Renaming data file.
[27.02.2025 02:15] Renaming previous data. hf_papers.json to ./d/2025-02-27.json
[27.02.2025 02:15] Saving new data file.
[27.02.2025 02:15] Generating page.
[27.02.2025 02:15] Renaming previous page.
[27.02.2025 02:15] Renaming previous data. index.html to ./d/2025-02-27.html
[27.02.2025 02:15] [Experimental] Generating Chinese page for reading.
[27.02.2025 02:15] Chinese vocab [{'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'Âü∫Á°ÄËÉΩÂäõ', 'pinyin': 'jƒ´ ch«î n√©ng l√¨', 'trans': 'basic capabilities'}, {'word': 'ÂØπÈΩê', 'pinyin': 'du√¨ q√≠', 'trans': 'alignment'}, {'word': 'È´òË¥®Èáè', 'pinyin': 'gƒÅo zh√¨ li√†ng', 'trans': 'high quality'}, {'word': 'Ê†∑Êú¨', 'pinyin': 'y√†ng bƒõn', 'trans': 'sample'}, {'word': 'Ê∂µÁõñ', 'pinyin': 'h√°n g√†i', 'trans': 'cover'}, {'word': 'Â§öÊ†∑Âåñ', 'pinyin': 'du≈ç y√†ng hu√†', 'trans': 'diversified'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√π z√°', 'trans': 'complex'}, {'word': 'Ê†ºÂºè', 'pinyin': 'g√© sh√¨', 'trans': 'format'}, {'word': 'ÊèêÂçá', 'pinyin': 't√≠ shƒìng', 'trans': 'improve'}, {'word': '‰∫∫Â∑•Ê†áÊ≥®', 'pinyin': 'r√©n g≈çng biƒÅo zh√π', 'trans': 'manual annotation'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ÁõëÁù£ÂæÆË∞É', 'pinyin': 'ji√†n d≈´ wƒìi ti√°o', 'trans': 'supervised fine-tuning'}, {'word': 'Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ', 'pinyin': 'zh√≠ jiƒì piƒÅn h√†o y≈çu hu√†', 'trans': 'direct preference optimization'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'significant'}, {'word': '‰øùÊåÅ', 'pinyin': 'b«éo ch√≠', 'trans': 'maintain'}, {'word': 'Ê†áÂáÜ', 'pinyin': 'biƒÅo zh«în', 'trans': 'standard'}, {'word': 'VQA', 'pinyin': 'VQA', 'trans': 'Visual Question Answering'}, {'word': 'Ê£ÄÊü•ÁÇπ', 'pinyin': 'ji«én ch√° di«én', 'trans': 'checkpoint'}]
[27.02.2025 02:15] Renaming previous Chinese page.
[27.02.2025 02:15] Renaming previous data. zh.html to ./d/2025-02-26_zh_reading_task.html
[27.02.2025 02:15] Writing Chinese reading task.
[27.02.2025 02:15] Writing result.
[27.02.2025 02:15] Renaming log file.
[27.02.2025 02:15] Renaming previous data. log.txt to ./logs/2025-02-27_last_log.txt
