[26.02.2025 23:10] Read previous papers.
[26.02.2025 23:10] Generating top page (month).
[26.02.2025 23:10] Writing top page (month).
[27.02.2025 00:47] Read previous papers.
[27.02.2025 00:47] Get feed.
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18411
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18137
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18449
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17363
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18364
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17262
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15499
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16069
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18461
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18356
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17425
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17535
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16794
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.16825
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.14855
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.15612
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17092
[27.02.2025 00:47] Extract page data from URL. URL: https://huggingface.co/papers/2502.17910
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17422
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.17814
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18302
[27.02.2025 00:47] Get page data from previous paper. URL: https://huggingface.co/papers/2502.18316
[27.02.2025 00:47] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.02.2025 00:47] No deleted papers detected.
[27.02.2025 00:47] Downloading and parsing papers (pdf, html). Total: 22.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.18411.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.18411.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.18411.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.18137.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.18137.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.18137.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.18449.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.18449.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.18449.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.17363.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.17363.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.17363.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.18364.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.18364.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.18364.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.17262.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.17262.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.17262.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.15499.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.15499.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.15499.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.16069.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.16069.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.16069.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.18461.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.18461.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.18461.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.18356.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.18356.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.18356.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.17425.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.17425.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.17425.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.17535.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.17535.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.17535.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.16794.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.16794.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.16794.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.16825.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.16825.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.16825.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.14855.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.14855.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.14855.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.15612.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.15612.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.15612.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.17092.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.17092.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.17092.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.17910.
[27.02.2025 00:47] Downloading paper 2502.17910 from http://arxiv.org/pdf/2502.17910v1...
[27.02.2025 00:47] Extracting affiliations from text.
[27.02.2025 00:47] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Scaling LLM Pre-training with Vocabulary Curriculum Fangyuan Yu Temus "
[27.02.2025 00:47] Response: []
[27.02.2025 00:47] Extracting affiliations from text.
[27.02.2025 00:47] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Scaling LLM Pre-training with Vocabulary Curriculum Fangyuan Yu TemusModern language models rely on static vocabularies, fixed before pretraining, in contrast to the adaptive vocabulary acquisition observed in human language learning. To bridge this gap, we introduce vocabulary curriculum learning, an approach that improves pretraining efficiency with log-linear scaling gains relative to vocabulary size. Our method alternates between entropy-guided vocabulary expansion and model optimization, enabling models to learn transferable representations across diverse tokenization granularities. This approach naturally gives rise to an optimal computation allocation pattern: longer tokens capture predictable content, while shorter tokens focus on more complex, harder-to-predict contexts. Experiments on small-scale GPT models demonstrate improved scaling efficiency, reinforcing the effectiveness of dynamic tokenization. We release our code to support further research and plan to extend our experiments to larger models and diverse domains.5 2 0 2 5 2 ] . [ 1 0 1 9 7 1 . 2 0 5 2 : r Figure 1: Scaling better with vocabulary curriculum Modern language model pre-training relies on static vocabularies, fixed before training and detached from the models learning dynamicsunlike human language acquisition. This fixed approach limits Preprint. Under review. models ability to adapt to different levels of linguistic granularity, potentially hindering efficiency and performance. While humans acquire language hierarchically, starting with basic units before building more complex representations, language models typically operate with predetermined tokenization schemes. Our approach dynamically merges predictable tokens, enabling the model to allocate computational resources more efficiently and shift focus toward harder-to-predict patterns. This results in an adaptive curriculum that evolves alongside the models capabilities. The vocabulary curriculum learning strategy begins with basic units (characters) and progressively expands to more complex representations, allocating more capacity to regions of high modeling entropy and refining the models understanding of difficult linguistic structures. digram of our approach is provided in 2 Empirical results from pre-training GPT models [10] on the enwiki8 dataset [11] highlight two key advantages of our vocabulary curriculum learning approach: 1. It improves model performance across various vocabulary sizes, consistently achieving lower bits-per-character (BPC) compared to traditional fixed-vocabulary training. 2. It enhances scaling efficiencymodels trained with vocabulary curriculum exhibit shallower slope (0.109 vs. 0.147) in log-scale vocabulary size vs. BPC plots, indicating more effective utilization of larger vocabularies. As shown in Figure 1, models trained with incremental vocabulary curriculum learning (red) exhibit steeper improvement curve compared to compute-matching baselines (blue). The log-scale vocabulary size vs. bits-per-character (BPC) plot reveals that vocabulary curriculum learning achieves slope of -0.147, meaning it leverages larger vocabularies more effectively than compute-matching learning, which only reaches -0.109. Additionally, we observe that the curated vocabulary naturally forms hierarchical structure, where longer tokens become increasingly predictable (lower BPC), while shorter tokens remain harder to predict (higher BPC). This structural organization emerges organically from our training process, reinforcing the effectiveness of our dynamic tokenization strategy. Our key contributions are: dynamic vocabulary creation system that adapts based on model entropy curriculum learning approach for tokenization that improves scaling efficiency Evidence that hierarchical token organization emerges naturally from our approach While our focus is on language modeling, we believe this scaling effect can generalize to other modalities and domains, as byte sequences serve as the fundamental building blocks of digital data.2.1 Tokenization Methods and Limitations Standard tokenization approaches like Byte Pair Encoding (BPE) [8], [9] rely on static co-occurrence statistics detached from model learning. This creates representational limitations, particularly evident in early language models struggles with mathematical operations [10]. Naive BPE tokenization produces inconsistent representations of numbersfor example, "711" might be encoded as single token while "703" requires multiple tokens. This inconsistency makes it harder for models to learn arithmetic operations compared to specialized approaches that assign unique tokens to all 1-3 digit integers [7]. Even with fixed vocabulary, different encoding strategies can produce varying segmentations of the same text. BPE-dropout [2] leverages this property by introducing stochasticity during training, showing improvements in neural machine translation. More recent work exploits segmentation equivariance during inference to enhance reasoning through self-consistency [3]. Additionally, research [5] has established the existence of optimal vocabulary sizes for BPE-style tokenization, which correlate with model size, log-linear relationship is observed between perplexity and vocabulary size. 2 Figure 2: Scaling better with vocabulary curriculum 2.2 Curriculum Learning Curriculum learning [13] progressively increases task difficulty during training to improve model performance. While successful in LLM post-training [15, 21], effective curriculum strategies for pretraining remain challenging [16]. Previous attempts at vocabulary-based curricula for decoder-only models found no improvements [19], highlighting the difficulty of designing effective curricula for language model pre-training. Our work addresses these limitations with novel adaptive approach to vocabulary curriculum. 2.3 Entropy Aware Tokenization Recent work has begun exploring entropy-aware tokenization. The Byte Latent Transformer [4] builds tokenization vocabularies using separately trained small language models. However, this approach creates vocabularies that are detached from the actual models entropy patterns and cannot be dynamically updated during training. Our work differs by integrating vocabulary building directly into the training process, allowing the tokenization scheme to evolve with the models developing understanding of the text. This creates true curriculum that adapts to the specific learning trajectory of each"
[27.02.2025 00:47] Mistral response. {"id": "21adc041429741f7be6effa7e86180da", "object": "chat.completion", "created": 1740617235, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1387, "total_tokens": 1394, "completion_tokens": 7}}
[27.02.2025 00:47] Response: ```python
[]
```
[27.02.2025 00:47] Deleting PDF ./assets/pdf/2502.17910.pdf.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.17422.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.17422.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.17422.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.17814.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.17814.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.17814.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.18302.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.18302.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.18302.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Downloading and parsing paper https://huggingface.co/papers/2502.18316.
[27.02.2025 00:47] Extra JSON file exists (./assets/json/2502.18316.json), skip PDF parsing.
[27.02.2025 00:47] Paper image links file exists (./assets/img_data/2502.18316.json), skip HTML parsing.
[27.02.2025 00:47] Success.
[27.02.2025 00:47] Enriching papers with extra data.
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 0. Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featur...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 1. An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the s...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 2. The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, thi...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 3. Background consistency remains a significant challenge in image editing tasks. Despite extensive developments, existing works still face a trade-off between maintaining similarity to the original image and generating content that aligns with the target. Here, we propose KV-Edit, a training-free appr...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 4. Multi-layer image generation is a fundamental task that enables users to isolate, select, and edit specific image layers, thereby revolutionizing interactions with generative models. In this paper, we introduce the Anonymous Region Transformer (ART), which facilitates the direct generation of variab...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 5. The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: (1) t...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 6. Training stability is a persistent challenge in the pre-training of large language models (LLMs), particularly for architectures such as Post-Norm Transformers, which are prone to gradient explosion and dissipation. In this paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 7. Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results. Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigo...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 8. Recent studies have explored combining different LoRAs to jointly generate learned style and content. However, existing methods either fail to effectively preserve both the original subject and style simultaneously or require additional training. In this paper, we argue that the intrinsic properties...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 9. We introduce WebGames, a comprehensive benchmark suite designed to evaluate general-purpose web-browsing AI agents through a collection of 50+ interactive challenges. These challenges are specifically crafted to be straightforward for humans while systematically testing the limitations of current AI...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 10. To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM stil...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 11. Motivated by reducing the computational and storage costs of LLMs, model compression and KV cache compression have attracted much attention from researchers. However, current methods predominantly emphasize maintaining the performance of compressed LLMs, as measured by perplexity or simple accuracy ...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 12. Auditory foundation models, including auditory large language models (LLMs), process all sound inputs equally, independent of listener perception. However, human auditory perception is inherently selective: listeners focus on specific speakers while ignoring others in complex auditory scenes. Existi...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 13. Iterative data generation and model retraining are widely used to align large language models (LLMs). It typically involves a policy model to generate on-policy responses and a reward model to guide training data selection. Direct Preference Optimization (DPO) further enhances this process by constr...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 14. Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts. This averaging obscures user- and prompt-specific variations in model performance. To address this, we propose Prompt-to-Leaderboard (P2L), a method that...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 15. State space models (SSMs), such as Mamba, have emerged as an efficient alternative to transformers for long-context sequence modeling. However, despite their growing adoption, SSMs lack the interpretability tools that have been crucial for understanding and improving attention-based architectures. W...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 16. We introduce Shakti VLM, a family of vision-language models in the capacity of 1B and 4B parameters designed to address data efficiency challenges in multimodal learning. While recent VLMs achieve strong performance through extensive training data, Shakti models leverage architectural innovations to...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 17. Modern language models rely on static vocabularies, fixed before pretraining, in contrast to the adaptive vocabulary acquisition observed in human language learning. To bridge this gap, we introduce vocabulary curriculum learning, an approach that improves pretraining efficiency with log-linear scal...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 18. Multimodal Large Language Models (MLLMs) have experienced rapid progress in visual recognition tasks in recent years. Given their potential integration into many critical applications, it is important to understand the limitations of their visual perception. In this work, we study whether MLLMs can ...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 19. Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks such as text generation, reasoning, and decision-making. While their success has primarily been driven by advances in computational power and dee...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 20. In this paper, we introduce LDGen, a novel method for integrating large language models (LLMs) into existing text-to-image diffusion models while minimizing computational demands. Traditional text encoders, such as CLIP and T5, exhibit limitations in multilingual processing, hindering image generati...
[27.02.2025 00:47] ********************************************************************************
[27.02.2025 00:47] Abstract 21. We introduce WiCkeD, a simple method to increase the complexity of existing multiple-choice benchmarks by randomly replacing a choice with "None of the above", a method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challen...
[27.02.2025 00:47] Read previous papers.
[27.02.2025 00:47] Generating reviews via LLM API.
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#alignment", "#training", "#benchmark", "#multimodal", "#dataset", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Ü–µ–Ω–Ω–æ—Å—Ç—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OmniAlign-V - –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö 
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#architecture", "#inference", "#video", "#optimization", "#cv"], "emoji": "üöÄ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ SpargeAttn –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –æ–Ω
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#training", "#rl", "#reasoning", "#dataset", "#math", "#open_source"], "emoji": "üß†", "ru": {"title": "SWE-RL: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ü–û", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SWE-RL - –ø–µ—Ä–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –¥–ª
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#training", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "KV-Edit: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ñ–æ–Ω–∞", "desc": "KV-Edit - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ KV-–∫—ç—à–∞ –≤ DiT-–º–æ–¥–µ–ª—è—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#multimodal", "#cv"], "emoji": "üé≠", "ru": {"title": "ART: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Anonymous Region Transformer (ART) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –ø—Ä–æ–∑—Ä–∞—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∏ –∞–Ω–æ–Ω–∏–º–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –æ–±–ª–∞—Å
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#training", "#benchmark"], "emoji": "üîÆ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "üöÄ", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Scale-Distribution Decoupling (SDD) –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#agents", "#training", "#science", "#interpretability", "#benchmark", "#open_source"], "emoji": "üß™", "ru": {"title": "Curie: –ò–ò-–∞–≥–µ–Ω—Ç –¥–ª—è —Å—Ç—Ä–æ–≥–∏—Ö –Ω–∞—É—á–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Curie - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ò–ò-–∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –Ω–∞—É—á–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. Curie —Å–æ—Å—Ç–æ–∏—Ç –∏
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#architecture", "#training"], "emoji": "üé®", "ru": {"title": "K-LoRA: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —Å—Ç–∏–ª—è –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ K-LoRA –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LoRA (Low-Rank Adaptation) –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#games", "#agents"], "emoji": "üåê", "ru": {"title": "WebGames: —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–µ–±-—Å—Ü–µ–Ω–∞—Ä–∏—è—Ö", "desc": "WebGames - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤ –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥–ª—è –≤–µ–±-–±—Ä–∞—É–∑–∏–Ω–≥–∞. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –±–æ–ª
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#architecture", "#cv"], "emoji": "üëÅÔ∏è", "ru": {"title": "–¢–æ–∫–µ–Ω—ã –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è: –Ω–æ–≤—ã–π —à–∞–≥ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è –¢–æ–∫–µ–Ω–æ–≤ –í–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –í–æ—Å–ø—Ä–∏—è—Ç–∏—è –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (ML
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#inference", "#optimization", "#reasoning", "#rag", "#training"], "emoji": "üéüÔ∏è", "ru": {"title": "–õ–æ—Ç–µ—Ä–µ–π–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É —Å–∂–∞—Ç–∏—é –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≥–∏–ø–æ—Ç–µ–∑—É –ª–æ—Ç–µ—Ä–µ–π–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (LLM) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å–∂–∞—Ç–∏—è –º–æ–¥–µ–ª–µ–π –∏
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#alignment", "#audio"], "emoji": "üëÇ", "ru": {"title": "–°–ª—É—à–∞—Ç—å –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫: –ò–ò —Å –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —É—á–∏—Ç—ã–≤–∞—é—â–∏–π –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#training", "#alignment", "#rlhf"], "emoji": "üìä", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ—Ü–µ—Å—Å–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –ø–æ–º–æ—â—å—é –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ê–≤—Ç
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#open_source", "#interpretability", "#training"], "emoji": "üèÜ", "ru": {"title": "–¢–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á–µ—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Prompt-to-Leaderboard (P2L) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á–µ—Ç–æ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#interpretability", "#training", "#architecture", "#long_context", "#multimodal", "#machine_translation"], "emoji": "üîç", "ru": {"title": "–ó–∞–≥–ª—è–Ω—É—Ç—å –≤–Ω—É—Ç—Ä—å Mamba: –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ SSM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π —Å–æ—Å—Ç–æ—è–Ω–∏—è (SSM), —Ç–∞–∫–∏—Ö –∫–∞–∫
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#reasoning", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ª–∏–Ω–µ–π–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Shakti VLM —Å 1B –∏ 4B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–∞—Ü–µ–ª–µ–Ω–Ω—ã—Ö –Ω–∞ –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç
[27.02.2025 00:47] Querying the API.
[27.02.2025 00:47] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Modern language models rely on static vocabularies, fixed before pretraining, in contrast to the adaptive vocabulary acquisition observed in human language learning. To bridge this gap, we introduce vocabulary curriculum learning, an approach that improves pretraining efficiency with log-linear scaling gains relative to vocabulary size. Our method alternates between entropy-guided vocabulary expansion and model optimization, enabling models to learn transferable representations across diverse tokenization granularities. This approach naturally gives rise to an optimal computation allocation pattern: longer tokens capture predictable content, while shorter tokens focus on more complex, harder-to-predict contexts. Experiments on small-scale GPT models demonstrate improved scaling efficiency, reinforcing the effectiveness of dynamic tokenization. We release our code to support further research and plan to extend our experiments to larger models and diverse domains.
[27.02.2025 00:47] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - curriculum learning –¥–ª—è —Å–ª–æ–≤–∞—Ä—è. –ú–µ—Ç–æ–¥ —á–µ—Ä–µ–¥—É–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–∏, –ø–æ–∑–≤–æ–ª—è—è –∏–∑—É—á–∞—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: –¥–ª–∏–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∫–æ—Ä–æ—Ç–∫–∏–µ - –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö GPT –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è.",
  "emoji": "üìö",
  "title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[27.02.2025 00:47] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Modern language models rely on static vocabularies, fixed before pretraining, in contrast to the adaptive vocabulary acquisition observed in human language learning. To bridge this gap, we introduce vocabulary curriculum learning, an approach that improves pretraining efficiency with log-linear scaling gains relative to vocabulary size. Our method alternates between entropy-guided vocabulary expansion and model optimization, enabling models to learn transferable representations across diverse tokenization granularities. This approach naturally gives rise to an optimal computation allocation pattern: longer tokens capture predictable content, while shorter tokens focus on more complex, harder-to-predict contexts. Experiments on small-scale GPT models demonstrate improved scaling efficiency, reinforcing the effectiveness of dynamic tokenization. We release our code to support further research and plan to extend our experiments to larger models and diverse domains."

[27.02.2025 00:47] Response: ```python
["DATASET", "TRAINING", "ARCHITECTURE"]
```
[27.02.2025 00:47] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Modern language models rely on static vocabularies, fixed before pretraining, in contrast to the adaptive vocabulary acquisition observed in human language learning. To bridge this gap, we introduce vocabulary curriculum learning, an approach that improves pretraining efficiency with log-linear scaling gains relative to vocabulary size. Our method alternates between entropy-guided vocabulary expansion and model optimization, enabling models to learn transferable representations across diverse tokenization granularities. This approach naturally gives rise to an optimal computation allocation pattern: longer tokens capture predictable content, while shorter tokens focus on more complex, harder-to-predict contexts. Experiments on small-scale GPT models demonstrate improved scaling efficiency, reinforcing the effectiveness of dynamic tokenization. We release our code to support further research and plan to extend our experiments to larger models and diverse domains."

[27.02.2025 00:47] Response: ```python
['TRANSFER_LEARNING', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[27.02.2025 00:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new method called vocabulary curriculum learning, which allows language models to adapt their vocabularies dynamically during training, similar to how humans learn language. The approach involves alternating between expanding the vocabulary based on entropy and optimizing the model, which helps in learning better representations of language. By using longer tokens for predictable content and shorter tokens for complex contexts, the model can allocate computational resources more effectively. Experiments show that this method improves the efficiency of smaller GPT models, and the authors plan to explore its application in larger models and various domains.","title":"Dynamic Vocabulary Learning for Efficient Language Models"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new method called vocabulary curriculum learning, which allows language models to adapt their vocabularies dynamically during training, similar to how humans learn language. The approach involves alternating between expanding the vocabulary based on entropy and optimizing the model, which helps in learning better representations of language. By using longer tokens for predictable content and shorter tokens for complex contexts, the model can allocate computational resources more effectively. Experiments show that this method improves the efficiency of smaller GPT models, and the authors plan to explore its application in larger models and various domains.', title='Dynamic Vocabulary Learning for Efficient Language Models'))
[27.02.2025 00:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Áé∞‰ª£ËØ≠Ë®ÄÊ®°Âûã‰æùËµñ‰∫éÂú®È¢ÑËÆ≠ÁªÉ‰πãÂâçÂõ∫ÂÆöÁöÑÈùôÊÄÅËØçÊ±áË°®ÔºåËÄå‰∫∫Á±ªËØ≠Ë®ÄÂ≠¶‰π†ÂàôË°®Áé∞Âá∫Ëá™ÈÄÇÂ∫îÁöÑËØçÊ±áËé∑ÂèñËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜËØçÊ±áËØæÁ®ãÂ≠¶‰π†ÁöÑÊñπÊ≥ïÔºåÈÄöËøáÁõ∏ÂØπ‰∫éËØçÊ±áÂ§ßÂ∞èÁöÑÂØπÊï∞Á∫øÊÄßÁº©ÊîæÂ¢ûÁõäÊù•ÊèêÈ´òÈ¢ÑËÆ≠ÁªÉÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïÂú®ÁÜµÂºïÂØºÁöÑËØçÊ±áÊâ©Â±ïÂíåÊ®°Âûã‰ºòÂåñ‰πãÈó¥‰∫§ÊõøËøõË°åÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂú®‰∏çÂêåÁöÑÂàÜËØçÁ≤íÂ∫¶‰∏äÂ≠¶‰π†ÂèØËøÅÁßªÁöÑË°®Á§∫„ÄÇÂÆûÈ™åË°®ÊòéÔºåËøôÁßçÂä®ÊÄÅÂàÜËØçÊñπÊ≥ïÂú®Â∞èËßÑÊ®°GPTÊ®°Âûã‰∏äÊèêÈ´ò‰∫ÜÁº©ÊîæÊïàÁéáÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ","title":"Âä®ÊÄÅËØçÊ±áÂ≠¶‰π†ÔºåÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊïàÁéá"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Áé∞‰ª£ËØ≠Ë®ÄÊ®°Âûã‰æùËµñ‰∫éÂú®È¢ÑËÆ≠ÁªÉ‰πãÂâçÂõ∫ÂÆöÁöÑÈùôÊÄÅËØçÊ±áË°®ÔºåËÄå‰∫∫Á±ªËØ≠Ë®ÄÂ≠¶‰π†ÂàôË°®Áé∞Âá∫Ëá™ÈÄÇÂ∫îÁöÑËØçÊ±áËé∑ÂèñËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜËØçÊ±áËØæÁ®ãÂ≠¶‰π†ÁöÑÊñπÊ≥ïÔºåÈÄöËøáÁõ∏ÂØπ‰∫éËØçÊ±áÂ§ßÂ∞èÁöÑÂØπÊï∞Á∫øÊÄßÁº©ÊîæÂ¢ûÁõäÊù•ÊèêÈ´òÈ¢ÑËÆ≠ÁªÉÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïÂú®ÁÜµÂºïÂØºÁöÑËØçÊ±áÊâ©Â±ïÂíåÊ®°Âûã‰ºòÂåñ‰πãÈó¥‰∫§ÊõøËøõË°åÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂú®‰∏çÂêåÁöÑÂàÜËØçÁ≤íÂ∫¶‰∏äÂ≠¶‰π†ÂèØËøÅÁßªÁöÑË°®Á§∫„ÄÇÂÆûÈ™åË°®ÊòéÔºåËøôÁßçÂä®ÊÄÅÂàÜËØçÊñπÊ≥ïÂú®Â∞èËßÑÊ®°GPTÊ®°Âûã‰∏äÊèêÈ´ò‰∫ÜÁº©ÊîæÊïàÁéáÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ', title='Âä®ÊÄÅËØçÊ±áÂ≠¶‰π†ÔºåÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊïàÁéá'))
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#optimization", "#interpretability", "#training", "#cv"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –º–µ–ª–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM) –º–µ–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#training", "#math", "#ethics", "#interpretability", "#architecture", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–∂–µ –¥–æ–≤–µ—Ä–∏—è –∫ —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) —Å—Ç–∞–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –≤ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–µ, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è 
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#long_context", "#multimodal", "#low_resource", "#multilingual", "#training", "#diffusion"], "emoji": "üåê", "ru": {"title": "LDGen: –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "LDGen - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —Å—É—â
[27.02.2025 00:47] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#benchmark"], "emoji": "üß†", "ru": {"title": "WiCkeD: –ü–æ–≤—ã—à–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ WiCkeD, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤—ã—à–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ—Å—Ç–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –ø—É—Ç–µ–º —Å–ª—É—á–∞–π–Ω–æ–π –∑–∞–º–µ–Ω
[27.02.2025 00:47] Loading Chinese text from previous data.
[27.02.2025 00:47] Renaming data file.
[27.02.2025 00:47] Renaming previous data. hf_papers.json to ./d/2025-02-27.json
[27.02.2025 00:47] Saving new data file.
[27.02.2025 00:47] Generating page.
[27.02.2025 00:47] Renaming previous page.
[27.02.2025 00:47] Renaming previous data. index.html to ./d/2025-02-27.html
[27.02.2025 00:47] [Experimental] Generating Chinese page for reading.
[27.02.2025 00:47] Chinese vocab [{'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'Âü∫Á°ÄËÉΩÂäõ', 'pinyin': 'jƒ´ ch«î n√©ng l√¨', 'trans': 'basic capabilities'}, {'word': 'ÂØπÈΩê', 'pinyin': 'du√¨ q√≠', 'trans': 'alignment'}, {'word': 'È´òË¥®Èáè', 'pinyin': 'gƒÅo zh√¨ li√†ng', 'trans': 'high quality'}, {'word': 'Ê†∑Êú¨', 'pinyin': 'y√†ng bƒõn', 'trans': 'sample'}, {'word': 'Ê∂µÁõñ', 'pinyin': 'h√°n g√†i', 'trans': 'cover'}, {'word': 'Â§öÊ†∑Âåñ', 'pinyin': 'du≈ç y√†ng hu√†', 'trans': 'diversified'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√π z√°', 'trans': 'complex'}, {'word': 'Ê†ºÂºè', 'pinyin': 'g√© sh√¨', 'trans': 'format'}, {'word': 'ÊèêÂçá', 'pinyin': 't√≠ shƒìng', 'trans': 'improve'}, {'word': '‰∫∫Â∑•Ê†áÊ≥®', 'pinyin': 'r√©n g≈çng biƒÅo zh√π', 'trans': 'manual annotation'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ÁõëÁù£ÂæÆË∞É', 'pinyin': 'ji√†n d≈´ wƒìi ti√°o', 'trans': 'supervised fine-tuning'}, {'word': 'Áõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ', 'pinyin': 'zh√≠ jiƒì piƒÅn h√†o y≈çu hu√†', 'trans': 'direct preference optimization'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'significant'}, {'word': '‰øùÊåÅ', 'pinyin': 'b«éo ch√≠', 'trans': 'maintain'}, {'word': 'Ê†áÂáÜ', 'pinyin': 'biƒÅo zh«în', 'trans': 'standard'}, {'word': 'VQA', 'pinyin': 'VQA', 'trans': 'Visual Question Answering'}, {'word': 'Ê£ÄÊü•ÁÇπ', 'pinyin': 'ji«én ch√° di«én', 'trans': 'checkpoint'}]
[27.02.2025 00:47] Renaming previous Chinese page.
[27.02.2025 00:47] Renaming previous data. zh.html to ./d/2025-02-26_zh_reading_task.html
[27.02.2025 00:47] Writing Chinese reading task.
[27.02.2025 00:47] Writing result.
[27.02.2025 00:47] Renaming log file.
[27.02.2025 00:47] Renaming previous data. log.txt to ./logs/2025-02-27_last_log.txt
