[14.06.2025 06:33] Read previous papers.
[14.06.2025 06:33] Generating top page (month).
[14.06.2025 06:33] Writing top page (month).
[14.06.2025 18:31] Read previous papers.
[14.06.2025 18:31] Get feed.
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09513
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10954
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09993
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10910
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10540
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10857
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10274
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10952
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10978
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10357
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09344
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10741
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09967
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10974
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10953
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10821
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09250
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10890
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10960
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06952
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10178
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09942
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08234
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08060
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06950
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10568
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09952
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10920
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10911
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10674
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10036
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08373
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.07795
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10737
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10728
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06694
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10600
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10378
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06561
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05982
[14.06.2025 18:31] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08862
[14.06.2025 18:31] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.06.2025 18:31] No deleted papers detected.
[14.06.2025 18:31] Downloading and parsing papers (pdf, html). Total: 41.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.09513.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.09513.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.09513.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10954.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10954.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10954.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.09993.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.09993.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.09993.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10910.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10910.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10910.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10540.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10540.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10540.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10857.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10857.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10857.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10274.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10274.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10274.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10952.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10952.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10952.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10978.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10978.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10978.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10357.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10357.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10357.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.09344.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.09344.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.09344.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10741.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10741.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10741.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.09967.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.09967.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.09967.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10974.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10974.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10974.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10953.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10953.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10953.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10821.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10821.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10821.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.09250.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.09250.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.09250.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10890.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10890.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10890.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10960.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10960.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10960.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.06952.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.06952.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.06952.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10178.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10178.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10178.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.09942.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.09942.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.09942.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.08234.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.08234.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.08234.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.08060.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.08060.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.08060.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.06950.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.06950.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.06950.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10568.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10568.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10568.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.09952.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.09952.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.09952.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10920.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10920.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10920.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10911.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10911.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10911.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10674.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10674.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10674.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10036.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10036.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10036.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.08373.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.08373.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.08373.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.07795.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.07795.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.07795.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10737.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10737.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10737.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10728.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10728.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10728.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.06694.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.06694.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.06694.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10600.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10600.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10600.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.10378.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.10378.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.10378.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.06561.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.06561.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.06561.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.05982.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.05982.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.05982.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Downloading and parsing paper https://huggingface.co/papers/2506.08862.
[14.06.2025 18:31] Extra JSON file exists (./assets/json/2506.08862.json), skip PDF parsing.
[14.06.2025 18:31] Paper image links file exists (./assets/img_data/2506.08862.json), skip HTML parsing.
[14.06.2025 18:31] Success.
[14.06.2025 18:31] Enriching papers with extra data.
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 0. ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.  					AI-generated summary 				 Though reasoning-based large language models (LLMs) hav...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 1. An automated pipeline, SWE-Factory, is introduced to facilitate the creation of large-scale datasets for evaluating and training Large Language Models in GitHub issue resolution tasks, offering efficient environment building, standardized grading, and automated validation.  					AI-generated summary...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 2. The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.  					AI-generated summary 				 Image restoration aims to recover degra...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 3. A scalable reinforcement learning pipeline for training reasoning models demonstrates improvements in multimodal understanding, instruction following, and function calling without relying on existing implementations.  					AI-generated summary 				 We introduce Magistral, Mistral's first reasoning m...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 4. AniMaker, a multi-agent framework using MCTS-Gen and AniEval, generates coherent storytelling videos from text input, outperforming existing models with better quality and efficiency.  					AI-generated summary 				 Despite rapid advancements in video generation models, generating coherent storytell...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 5. VRBench evaluates long video understanding by assessing multi-step reasoning capabilities across temporal and procedural validity using human-labeled question-answering pairs and reasoning chains.  					AI-generated summary 				 We present VRBench, the first long narrative video benchmark crafted fo...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 6. A systematic review and benchmark of discrete audio tokenizers across speech, music, and general audio domains is presented, covering their taxonomy, evaluation metrics, and limitations.  					AI-generated summary 				 Discrete audio tokens are compact representations that aim to preserve perceptual...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 7. Domain2Vec decomposes datasets into meta-domains to optimize language model pretraining and downstream performance with reduced computational cost.  					AI-generated summary 				 We introduce~Domain2Vec, a novel approach that decomposes any dataset into a linear combination of several meta-domains,...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 8. The paper proposes HeadHunter, a systematic framework for selecting attention heads in Diffusion Transformer architectures to enable precise control over image generation quality and style, outperforming existing methods.  					AI-generated summary 				 Recent guidance methods in diffusion models st...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 9. Optimus-3, an agent using knowledge-enhanced data generation, Mixture-of-Experts routing, and multimodal reasoning-augmented reinforcement learning, achieves superior performance across various tasks in Minecraft.  					AI-generated summary 				 Recently, agents based on multimodal large language mo...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 10. Ming-Omni is a unified multimodal model with dedicated encoders and modality-specific routers that can process images, text, audio, and video, and performs tasks like speech and image generation, context-aware chatting, and versatile image editing.  					AI-generated summary 				 We propose Ming-Omn...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 11. PosterCraft improves aesthetic poster generation through a unified, modular pipeline with enhanced text rendering, region-aware fine-tuning, aesthetic reinforcement learning, and joint vision-language refinement.  					AI-generated summary 				 Generating aesthetic posters is more challenging than s...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 12. SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.  					AI-generated summary 				 How cost-effectively can we elicit strong reasoning in language models by leveraging their und...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 13. AutoMind, a flexible and knowledgeable LLM-agent framework, improves automated data science through expert knowledge integration, strategic solution exploration, and adaptive coding, outperforming existing systems.  					AI-generated summary 				 Large Language Model (LLM) agents have shown great po...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 14. A paradigm shift in web agent research is proposed, advocating for the development of Agentic Web Interfaces (AWIs) to optimize interaction for AI agents within web environments.  					AI-generated summary 				 Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spur...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 15. VideoDeepResearch, a text-only reasoning model with modular tools, surpasses existing baselines in long video understanding tasks without extending context windows or enhancing visual perception capabilities.  					AI-generated summary 				 Long video understanding (LVU) presents a significant chall...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 16. Evaluation artifacts, particularly token limits and impractical instances in benchmarks, lead to misreported failures in Large Reasoning Models on planning puzzles.  					AI-generated summary 				 Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit "accuracy collapse" on planning...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 17. CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.  					AI-generated summary 				 Graphic design plays a crucial role in both commercial and personal contexts, yet creating high-quality, editable, and...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 18. A benchmark for Chinese harmful content detection, coupled with a knowledge-augmented baseline, improves the performance of smaller models without extensive resources.  					AI-generated summary 				 Large language models (LLMs) have been increasingly applied to automated harmful content detection t...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 19. LaTtE-Flow, a new architecture, unifies image understanding and generation with high performance and faster inference by using a Layerwise Timestep Experts flow-based Transformer and Timestep-Conditioned Residual Attention mechanism.  					AI-generated summary 				 Recent advances in multimodal foun...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 20. Efficient probing, a simplified multi-query cross-attention mechanism, enhances evaluation of self-supervised learning models by improving speed, performance, and interpretability.  					AI-generated summary 				 As fine-tuning (FT) becomes increasingly impractical at scale, probing is emerging as t...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 21. VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enha...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 22. Recent advancements in optimizing compound AI systems highlight challenges in integrating various components, with an emphasis on natural language feedback methods for non-differentiable systems.  					AI-generated summary 				 Recent advancements in large language models (LLMs) and AI systems have ...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 23. Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.  					AI-generated summary 				 Large language models have transformed natural language processing, yet supervised...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 24. A framework for evaluating and optimizing natural language prompts in large language models is proposed, revealing correlations between prompt properties and their impact on reasoning tasks.  					AI-generated summary 				 As large language models (LLMs) have progressed towards more human-like and h...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 25. A Diffusion Transformer-based framework generates high-fidelity human-product demonstration videos by preserving identities and spatial relationships, using masked cross-attention and structured text encoding.  					AI-generated summary 				 In e-commerce and digital marketing, generating high-fidel...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 26. UniPre3D is a unified pre-training method for 3D point clouds and models of any scale, using Gaussian primitives and 2D feature integration for effective performance across object and scene tasks.  					AI-generated summary 				 The scale diversity of point cloud data presents significant challenges...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 27. SNMF is used to identify interpretable features in LLMs by directly decomposing MLP activations, outperforming SAEs and supervised methods in causal evaluations and aligning with human-interpretable concepts.  					AI-generated summary 				 A central goal for mechanistic interpretability has been to...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 28. NoLoCo is a novel optimization method that eliminates explicit parameter synchronization and reduces communication overhead during the training of large language models, achieving faster convergence rates and reduced idling time compared to existing methods.  					AI-generated summary 				 Training ...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 29. A benchmark dataset called TeleMath evaluates Large Language Models in domain-specific mathematical problems within telecommunications, showing that models designed for mathematical reasoning perform better than general-purpose models.  					AI-generated summary 				 The increasing adoption of artif...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 30. Token Perturbation Guidance (TPG) enhances diffusion model generation quality without training, by perturbing intermediate token representations, achieving CFG-like performance and improving unconditional generation.  					AI-generated summary 				 Classifier-free guidance (CFG) has become an essent...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 31. A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.  					AI-generated summary 				 Optimizing inference for long-context Large Language M...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 32. Form-Dependent Bias limits the effectiveness of LLM unlearning across different knowledge expressions, and Rank-one Concept Redirection (ROCR) is proposed as a form-independent solution that enhances unlearning efficacy.  					AI-generated summary 				 Large Language Model (LLM) unlearning aims to e...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 33. TaxoAdapt dynamically adapts an LLM-generated taxonomy for scientific literature across multiple dimensions, improving granularity and coherence compared to existing methods.  					AI-generated summary 				 The rapid evolution of scientific fields introduces challenges in organizing and retrieving s...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 34. ClaimSpect is a retrieval-augmented generation-based framework that constructs a hierarchical structure of aspects for claims, enriching them with diverse perspectives from a corpus.  					AI-generated summary 				 Claims made by individuals or entities are oftentimes nuanced and cannot be clearly l...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 35. MoveGCL is a privacy-preserving framework using generative continual learning and a Mixture-of-Experts Transformer for training mobility foundation models without sharing raw data.  					AI-generated summary 				 Foundation models have revolutionized fields such as natural language processing and co...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 36. EmbodiedGen is a platform that generates high-quality, photorealistic 3D assets at low cost, enabling scalable and realistic embodied AI research through generative AI techniques.  					AI-generated summary 				 Constructing a physically realistic and accurately scaled simulated 3D world is crucial ...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 37. The study proposes a causal representation learning framework to evaluate language model capabilities through latent factors, emphasizing the importance of controlling for base model variations to uncover underlying causal relationships.  					AI-generated summary 				 Faithful evaluation of languag...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 38. LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.  					AI-generated summary 				 Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been de...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 39. MCA-Bench is a multimodal benchmark suite for CAPTCHA security evaluation which fine-tunes specialized cracking agents using a shared vision-language model.  					AI-generated summary 				 As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious ...
[14.06.2025 18:31] ********************************************************************************
[14.06.2025 18:31] Abstract 40. StreamSplat, a fully feed-forward framework, addresses real-time 3D scene reconstruction from uncalibrated video with accurate dynamics and long-term stability.  					AI-generated summary 				 Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-...
[14.06.2025 18:31] Read previous papers.
[14.06.2025 18:31] Generating reviews via LLM API.
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#benchmark", "#healthcare", "#reasoning", "#training"], "emoji": "ü©∫", "ru": {"title": "ReasonMed: –ü—Ä–æ—Ä—ã–≤ –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –ò–ò", "desc": "ReasonMed - —ç—Ç–æ –∫—Ä—É–ø–Ω–µ–π—à–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ 37
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#data", "#science", "#dataset", "#agents", "#benchmark", "#open_source"], "emoji": "üè≠", "ru": {"title": "SWE-Factory: –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è LLM –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ü–û", "desc": "SWE-Factory - —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –æ–±—É—á
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#dataset", "#cv", "#benchmark", "#diffusion", "#hallucinations"], "emoji": "üìù", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —É—á–µ—Ç–æ–º —Ç–µ–∫—Å—Ç–∞ (TAIR) –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#optimization", "#training", "#rl", "#reasoning", "#open_source", "#multimodal"], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Magistral - –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –æ–±—É—á–µ–Ω–Ω—É—é —Å –ø–æ–º–æ—â—å—é –º–∞—Å—à—Ç–∞
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#optimization", "#video", "#multimodal", "#story_generation", "#agents"], "emoji": "üé¨", "ru": {"title": "AniMaker: —É–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∏–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "AniMaker - —ç—Ç–æ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∏–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é.
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#reasoning", "#video", "#multimodal"], "emoji": "üé¨", "ru": {"title": "VRBench: –û—Ü–µ–Ω–∫–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "VRBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#survey", "#benchmark", "#audio"], "emoji": "üéµ", "ru": {"title": "–ê—É–¥–∏–æ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è: –æ—Ç —Ä–µ—á–∏ –¥–æ –º—É–∑—ã–∫–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –∞—É–¥–∏–æ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ –æ–±–ª–∞—Å—Ç—è—Ö —Ä–µ—á–∏, –º—É–∑—ã–∫–∏ –∏ –æ–±—â–µ–≥–æ –∞—É–¥–∏–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é –ø–æ–¥—Ö–æ
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#transfer_learning", "#dataset", "#training", "#optimization", "#data"], "emoji": "üß©", "ru": {"title": "–£–º–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Domain2Vec - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –Ω–∞ –º–µ—Ç–∞-–¥–æ–º–µ–Ω—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#interpretability", "#cv", "#training", "#diffusion", "#optimization", "#architecture"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –≤—ã–±–æ—Ä –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç HeadHunter - —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≤—ã–±–æ—Ä–∞ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö Di
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#agents", "#rag", "#rl", "#reasoning", "#multimodal", "#games"], "emoji": "ü§ñ", "ru": {"title": "Optimus-3: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ò–ò-–∞–≥–µ–Ω—Ç –ø–æ–∫–æ—Ä—è–µ—Ç Minecraft", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Optimus-3 - –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∏–≥—Ä—ã Minecraft, –∏—Å–ø–æ–ª—å–∑—É—é—â–µ–≥
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#audio", "#open_source", "#video", "#cv", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π: –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ –æ–¥–Ω–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Ming-Omni - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω—É—é –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–µ–∫—Å—Ç, 
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#optimization", "#training", "#rl", "#architecture", "#open_source", "#dataset", "#data", "#multimodal"], "emoji": "üé®", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Å–æ–∑–¥–∞–µ—Ç —ç—Å—Ç–µ—Ç–∏—á–Ω—ã–µ –ø–æ—Å—Ç–µ—Ä—ã –Ω–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è", "desc": "PosterCraft - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç—Å—Ç–µ—Ç–∏—á–Ω—ã—Ö –ø–æ—Å—Ç
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#rl", "#training", "#open_source", "#optimization", "#small_models", "#reasoning"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ SAE-Tuning –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#science", "#training", "#dataset", "#benchmark", "#agents"], "emoji": "ü§ñ", "ru": {"title": "AutoMind: –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –Ω–∞—É–∫–∏ –æ –¥–∞–Ω–Ω—ã—Ö", "desc": "AutoMind - —ç—Ç–æ –Ω–æ–≤–∞—è –≥–∏–±–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–¥–∞—á –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#agents", "#agi", "#optimization", "#multimodal"], "emoji": "üåê", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω—ã–µ –í–µ–±-–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ –ò–ò —Å –≤–µ–±-—Å—Ä–µ–¥–æ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –¥–ª—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤, –≤–≤–æ–¥—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ê–≥–µ–Ω—Ç–Ω—ã—Ö –í–µ–±-–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ (AWI). AWI –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#long_context", "#video", "#multimodal", "#agents"], "emoji": "üé•", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "VideoDeepResearch - —ç—Ç–æ –Ω–æ–≤–∞—è –∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#interpretability", "#training", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "–ü–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –ò–ò: –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤ –æ—Ü–µ–Ω–∫–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (LRM) –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –≥–æ–ª–æ–≤–æ–ª–æ–º–æ–∫ –ø–ª–∞–Ω
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#dataset", "#cv", "#benchmark", "#open_source", "#multimodal"], "emoji": "üé®", "ru": {"title": "CreatiPoster: –ò–ò-—Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–º –¥–∏–∑–∞–π–Ω–µ", "desc": "CreatiPoster - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∫–æ–º–ø–æ–∑–∏—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#multilingual", "#small_models", "#low_resource", "#ethics", "#dataset", "#benchmark"], "emoji": "üá®üá≥", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ —Å –ø–æ–º–æ—â—å—é –∑–Ω–∞–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –∫–∏—Ç–∞
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#games", "#cv", "#multimodal", "#architecture", "#optimization"], "emoji": "üñºÔ∏è", "ru": {"title": "–ï–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "LaTtE-Flow - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#interpretability", "#training", "#optimization", "#benchmark", "#architecture"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: –±—ã—Å—Ç—Ä—ã–π –∏ —Ç–æ—á–Ω—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —Å–∞–º–æ–æ–±—É—á–∞—é—â–∏—Ö—Å—è –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏—è (efficient probing) –¥–ª—è –æ—Ü–µ–Ω
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#rl", "#benchmark", "#open_source", "#reasoning", "#training", "#rlhf"], "emoji": "ü§ñ", "ru": {"title": "VerIF: –ì–∏–±—Ä–∏–¥–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è RL –≤ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VerIF - –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏, —Å–æ—á–µ—Ç–∞—é—â–∏–π 
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#survey", "#rlhf", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –ò–ò", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#rag", "#inference", "#training"], "emoji": "üß†", "ru": {"title": "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã: –æ–±—É—á–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –±–µ–∑ –∏–∑
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#survey", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#dataset", "#diffusion", "#video", "#3d"], "emoji": "üé•", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –≤–∏–¥–µ–æ-–¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ Diffusion Transformer –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–µ–π –ø—Ä–æ
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training", "#3d"], "emoji": "üåê", "ru": {"title": "–ï–¥–∏–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –¥–ª—è 3D-–¥–∞–Ω–Ω—ã—Ö –ª—é–±–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞", "desc": "UniPre3D - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ –∏ 3D-–º–æ–¥–µ–ª–µ–π –ª—é–±–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–∞—É—Å—Å–æ–≤—ã –ø—Ä
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#architecture", "#data", "#training", "#interpretability"], "emoji": "üß†", "ru": {"title": "SNMF: –∫–ª—é—á –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –ø–æ–ª—É–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π –º–∞—Ç—Ä–∏—á–Ω–æ–π —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (SNMF) –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#benchmark", "#training", "#optimization"], "emoji": "üöÄ", "ru": {"title": "NoLoCo: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —è–≤–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏", "desc": "NoLoCo - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —è–≤–Ω–æ–π —Å–∏–Ω—Ö—Ä
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#open_source", "#math", "#benchmark"], "emoji": "üì°", "ru": {"title": "TeleMath: –ò–∑–º–µ—Ä—è–µ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ò–ò –≤ —Ç–µ–ª–µ–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö TeleMath –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–µ—à–∞—Ç—å –º–∞—Ç–µ–º–∞
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#optimization", "#training", "#diffusion", "#cv"], "emoji": "üîÄ", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Token Perturbation Guidance (TPG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#optimization", "#long_context", "#benchmark", "#architecture", "#training", "#inference"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤—ã–≤–æ–¥–∞ –ò–ò —Å –ø–æ–º–æ—â—å—é —É–º–Ω—ã—Ö —á–µ—Ä–Ω–æ–≤–∏–∫–æ–≤", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –∏—Å–ø
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#security", "#rlhf", "#ethics", "#benchmark", "#hallucinations", "#training"], "emoji": "üß†", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º—ã –≤ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ –ø—Ä–æ–±–ª–µ–º—É –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#science", "#multimodal", "#dataset"], "emoji": "üå≥", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Ç–∞–∫—Å–æ–Ω–æ–º–∏–π –¥–ª—è —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–µ–π –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã", "desc": "TaxoAdapt - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é (LLM), –∫ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∫–æ—Ä
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#science", "#reasoning", "#rag", "#dataset"], "emoji": "üîç", "ru": {"title": "–†–∞–∑–ª–æ–∂–µ–Ω–∏–µ —Å–ª–æ–∂–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã", "desc": "ClaimSpect - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å —É—Å–∏–ª–µ–Ω–∏–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞—Å–ø–µ–∫—Ç–æ–≤ –¥–ª—è —É—Ç–≤–µ—Ä–∂
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#dataset", "#synthetic", "#data", "#training", "#open_source", "#architecture"], "emoji": "üö∂", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –º–æ–±–∏–ª—å–Ω–æ—Å—Ç–∏", "desc": "MoveGCL - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–æ–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–∞
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#games", "#3d", "#agents", "#synthetic"], "emoji": "ü§ñ", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ò–ò –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D-–º–∏—Ä–æ–≤", "desc": "EmbodiedGen - —ç—Ç–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D-–∞–∫—Ç–∏–≤–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å –Ω–∏–∑–∫–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–∫
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#science", "#dataset", "#interpretability", "#benchmark", "#reasoning", "#math"], "emoji": "üß†", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π –≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∫–∞—É–∑–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#multimodal", "#interpretability", "#games"], "emoji": "üñºÔ∏è", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥", "desc": "LaMP-Cap –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å –∏—Å–ø–æ–ª—å–∑–æ
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#security", "#open_source", "#multimodal"], "emoji": "üîê", "ru": {"title": "–ï–¥–∏–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ CAPTCHA", "desc": "MCA-Bench - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ CAPTCHA, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π
[14.06.2025 18:31] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#video", "#3d"], "emoji": "üé•", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏: –æ—Ç –≤–∏–¥–µ–æ –∫ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º —Å—Ü–µ–Ω–∞–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "StreamSplat - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D-—Å—Ü–µ–Ω –∏–∑ –Ω–µ–æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ –≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å
[14.06.2025 18:31] Trying to get texts in Chinese.
[14.06.2025 18:31] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.  					AI-generated summary 				 Though reasoning-based large language models (LLMs) have excelled in mathematics and programming, their capabilities in knowledge-intensive medical question answering remain underexplored. To address this, we introduce ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality examples distilled from 1.7 million initial reasoning paths generated by various LLMs. ReasonMed is constructed through a multi-agent verification and refinement process, where we design an Error Refiner to enhance the reasoning paths by identifying and correcting error-prone steps flagged by a verifier. Leveraging ReasonMed, we systematically investigate best practices for training medical reasoning models and find that combining detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields the most effective fine-tuning strategy. Based on this strategy, we train ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.
[14.06.2025 18:31] Mistral response. {"id": "ba536b6155784f84b16a539a6a6f45c8", "object": "chat.completion", "created": 1749925901, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5927\u578b\u533b\u5b66\u63a8\u7406\u6570\u636e\u96c6ReasonMed\u3002\u5b83\u7ed3\u5408\u8be6\u7ec6\u7684\u63a8\u7406\u8def\u5f84\u548c\u7b80\u660e\u7684\u6458\u8981\uff0c\u63d0\u9ad8\u4e86\u533b\u5b66\u95ee\u7b54\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b370k\u4e2a\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u9a8c\u8bc1\u548c\u6539\u8fdb\u8fc7\u7a0b\u751f\u6210\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u7ed3\u5408\u8be6\u7ec6\u7684\u63a8\u7406\u94fe\u548c\u7b80\u660e\u7b54\u6848\u6458\u8981\u7684\u5fae\u8c03\u7b56\u7565\u6700\u6709\u6548\u3002\u57fa\u4e8e\u6b64\u7b56\u7565\u8bad\u7ec3\u7684ReasonMed-7B\u6a21\u578b\uff0c\u6027\u80fd\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u6700\u4f73\u6a21\u578b\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 324, "total_tokens": 471, "completion_tokens": 147}}
[14.06.2025 18:31] Response: ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏Ä‰∏™Â§ßÂûãÂåªÂ≠¶Êé®ÁêÜÊï∞ÊçÆÈõÜReasonMed„ÄÇÂÆÉÁªìÂêàËØ¶ÁªÜÁöÑÊé®ÁêÜË∑ØÂæÑÂíåÁÆÄÊòéÁöÑÊëòË¶ÅÔºåÊèêÈ´ò‰∫ÜÂåªÂ≠¶ÈóÆÁ≠îÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´370k‰∏™È´òË¥®ÈáèÊ†∑Êú¨ÔºåÈÄöËøáÂ§ö‰ª£ÁêÜÈ™åËØÅÂíåÊîπËøõËøáÁ®ãÁîüÊàê„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁªìÂêàËØ¶ÁªÜÁöÑÊé®ÁêÜÈìæÂíåÁÆÄÊòéÁ≠îÊ°àÊëòË¶ÅÁöÑÂæÆË∞ÉÁ≠ñÁï•ÊúÄÊúâÊïà„ÄÇÂü∫‰∫éÊ≠§Á≠ñÁï•ËÆ≠ÁªÉÁöÑReasonMed-7BÊ®°ÂûãÔºåÊÄßËÉΩË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊúÄ‰Ω≥Ê®°Âûã„ÄÇ
[14.06.2025 18:31] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏Ä‰∏™Â§ßÂûãÂåªÂ≠¶Êé®ÁêÜÊï∞ÊçÆÈõÜReasonMed„ÄÇÂÆÉÁªìÂêàËØ¶ÁªÜÁöÑÊé®ÁêÜË∑ØÂæÑÂíåÁÆÄÊòéÁöÑÊëòË¶ÅÔºåÊèêÈ´ò‰∫ÜÂåªÂ≠¶ÈóÆÁ≠îÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´370k‰∏™È´òË¥®ÈáèÊ†∑Êú¨ÔºåÈÄöËøáÂ§ö‰ª£ÁêÜÈ™åËØÅÂíåÊîπËøõËøáÁ®ãÁîüÊàê„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁªìÂêàËØ¶ÁªÜÁöÑÊé®ÁêÜÈìæÂíåÁÆÄÊòéÁ≠îÊ°àÊëòË¶ÅÁöÑÂæÆË∞ÉÁ≠ñÁï•ÊúÄÊúâÊïà„ÄÇÂü∫‰∫éÊ≠§Á≠ñÁï•ËÆ≠ÁªÉÁöÑReasonMed-7BÊ®°ÂûãÔºåÊÄßËÉΩË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊúÄ‰Ω≥Ê®°Âûã„ÄÇ
[14.06.2025 18:31] Mistral response. {"id": "0a980b81ea9b4f48b9cd9ec223024f6c", "object": "chat.completion", "created": 1749925903, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Zh\u00e8 pi\u0101n w\u00e9nzh\u0101ng ji\u00e8sh\u00e0o le y\u012bg\u00e8 d\u00e0x\u00edng y\u012bxu\u00e9 tu\u00edl\u01d0 sh\u00f9j\u00f9j\u00ed ReasonMed. T\u0101 ji\u0113h\u00e9 xi\u00e1ngx\u00ec de tu\u00edl\u01d0 l\u00f9j\u00ecng h\u00e9 ji\u01cenm\u00edng de zh\u0101iy\u00e0o, t\u012bg\u0101o le y\u012bxu\u00e9 w\u00e8nd\u00e1 m\u00f3x\u00edng de zh\u01d4nqu\u00e8x\u00ecng. G\u01cei sh\u00f9j\u00f9j\u00ed b\u0101oh\u00e1n 370k g\u00e8 g\u0101o zh\u00ecli\u00e0ng y\u00e0ngb\u011bn, t\u014dnggu\u00f2 du\u014d d\u00e0il\u01d0 y\u00e0nzh\u00e8ng h\u00e9 g\u01ceij\u00ecn gu\u00f2ch\u00e9ng sh\u0113ngch\u00e9ng. Y\u00e1nji\u016b f\u0101xi\u00e0n, ji\u0113h\u00e9 xi\u00e1ngx\u00ec de tu\u00edl\u01d0 li\u00e0n h\u00e9 ji\u01cenm\u00edng d\u00e1'\u00e0n zh\u0101iy\u00e0o de w\u0113iti\u00e1o c\u00e8l\u00fc\u00e8 zu\u00ec y\u01d2uxi\u00e0o. J\u012by\u00fa c\u01d0 c\u00e8l\u00fc\u00e8 x\u00f9nli\u00e0n de ReasonMed-7B m\u00f3x\u00edng, x\u00ecngn\u00e9ng ch\u0101oyu\u00e8 le zh\u012bqi\u00e1n de zu\u00ecji\u0101 m\u00f3x\u00edng."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 161, "total_tokens": 466, "completion_tokens": 305}}
[14.06.2025 18:31] Response: Zh√® piƒÅn w√©nzhƒÅng ji√®sh√†o le yƒ´g√® d√†x√≠ng yƒ´xu√© tu√≠l«ê sh√πj√πj√≠ ReasonMed. TƒÅ jiƒìh√© xi√°ngx√¨ de tu√≠l«ê l√πj√¨ng h√© ji«énm√≠ng de zhƒÅiy√†o, tƒ´gƒÅo le yƒ´xu√© w√®nd√° m√≥x√≠ng de zh«înqu√®x√¨ng. G«éi sh√πj√πj√≠ bƒÅoh√°n 370k g√® gƒÅo zh√¨li√†ng y√†ngbƒõn, t≈çnggu√≤ du≈ç d√†il«ê y√†nzh√®ng h√© g«éij√¨n gu√≤ch√©ng shƒìngch√©ng. Y√°nji≈´ fƒÅxi√†n, jiƒìh√© xi√°ngx√¨ de tu√≠l«ê li√†n h√© ji«énm√≠ng d√°'√†n zhƒÅiy√†o de wƒìiti√°o c√®l√º√® zu√¨ y«íuxi√†o. Jƒ´y√∫ c«ê c√®l√º√® x√πnli√†n de ReasonMed-7B m√≥x√≠ng, x√¨ngn√©ng chƒÅoyu√® le zhƒ´qi√°n de zu√¨jiƒÅ m√≥x√≠ng.
[14.06.2025 18:31] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏Ä‰∏™Â§ßÂûãÂåªÂ≠¶Êé®ÁêÜÊï∞ÊçÆÈõÜReasonMed„ÄÇÂÆÉÁªìÂêàËØ¶ÁªÜÁöÑÊé®ÁêÜË∑ØÂæÑÂíåÁÆÄÊòéÁöÑÊëòË¶ÅÔºåÊèêÈ´ò‰∫ÜÂåªÂ≠¶ÈóÆÁ≠îÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´370k‰∏™È´òË¥®ÈáèÊ†∑Êú¨ÔºåÈÄöËøáÂ§ö‰ª£ÁêÜÈ™åËØÅÂíåÊîπËøõËøáÁ®ãÁîüÊàê„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁªìÂêàËØ¶ÁªÜÁöÑÊé®ÁêÜÈìæÂíåÁÆÄÊòéÁ≠îÊ°àÊëòË¶ÅÁöÑÂæÆË∞ÉÁ≠ñÁï•ÊúÄÊúâÊïà„ÄÇÂü∫‰∫éÊ≠§Á≠ñÁï•ËÆ≠ÁªÉÁöÑReasonMed-7BÊ®°ÂûãÔºåÊÄßËÉΩË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊúÄ‰Ω≥Ê®°Âûã„ÄÇ
[14.06.2025 18:31] Mistral response. {"id": "e146cfcf3a224b0a92d578fb65633b2e", "object": "chat.completion", "created": 1749925910, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u63a8\u7406\", \"pinyin\": \"tu\u012bl\u01d0\", \"trans\": \"reasoning\"},\n    {\"word\": \"\u6570\u636e\u96c6\", \"pinyin\": \"sh\u00f9j\u00f9j\u00ed\", \"trans\": \"dataset\"},\n    {\"word\": \"\u7ed3\u5408\", \"pinyin\": \"ji\u00e9h\u00e9\", \"trans\": \"combine\"},\n    {\"word\": \"\u8be6\u7ec6\", \"pinyin\": \"xi\u00e1ngx\u00ec\", \"trans\": \"detailed\"},\n    {\"word\": \"\u8def\u5f84\", \"pinyin\": \"l\u00f9j\u00ecng\", \"trans\": \"path\"},\n    {\"word\": \"\u7b80\u660e\", \"pinyin\": \"ji\u01cenm\u00edng\", \"trans\": \"concise\"},\n    {\"word\": \"\u6458\u8981\", \"pinyin\": \"zh\u0101iy\u00e0o\", \"trans\": \"summary\"},\n    {\"word\": \"\u63d0\u9ad8\", \"pinyin\": \"t\u00edg\u0101o\", \"trans\": \"improve\"},\n    {\"word\": \"\u51c6\u786e\u6027\", \"pinyin\": \"zh\u01d4nqu\u00e8x\u00ecng\", \"trans\": \"accuracy\"},\n    {\"word\": \"\u9ad8\u8d28\u91cf\", \"pinyin\": \"g\u0101ozh\u00ecli\u00e0ng\", \"trans\": \"high-quality\"},\n    {\"word\": \"\u6837\u672c\", \"pinyin\": \"y\u00e0ngb\u011bn\", \"trans\": \"sample\"},\n    {\"word\": \"\u591a\u4ee3\u7406\", \"pinyin\": \"du\u014dd\u00e0il\u01d0\", \"trans\": \"multi-agent\"},\n    {\"word\": \"\u9a8c\u8bc1\", \"pinyin\": \"y\u00e0nzh\u00e8ng\", \"trans\": \"verification\"},\n    {\"word\": \"\u6539\u8fdb\", \"pinyin\": \"g\u01ceij\u00ecn\", \"trans\": \"improvement\"},\n    {\"word\": \"\u8fc7\u7a0b\", \"pinyin\": \"gu\u00f2ch\u00e9ng\", \"trans\": \"process\"},\n    {\"word\": \"\u751f\u6210\", \"pinyin\": \"sh\u0113ngch\u00e9ng\", \"trans\": \"generate\"},\n    {\"word\": \"\u53d1\u73b0\", \"pinyin\": \"f\u0101xi\u00e0n\", \"trans\": \"discover\"},\n    {\"word\": \"\u5fae\u8c03\", \"pinyin\": \"w\u0113iti\u00e1o\", \"trans\": \"fine-tune\"},\n    {\"word\": \"\u7b56\u7565\", \"pinyin\": \"c\u00e8l\u00fc\u00e8\", \"trans\": \"strategy\"},\n    {\"word\": \"\u6700\u6709\u6548\", \"pinyin\": \"zu\u00ec y\u01d2uxi\u00e0o\", \"trans\": \"most effective\"},\n    {\"word\": \"\u57fa\u4e8e\", \"pinyin\": \"j\u012by\u00fa\", \"trans\": \"based on\"},\n    {\"word\": \"\u4e4b\u524d\", \"pinyin\": \"zh\u012bqi\u00e1n\", \"trans\": \"previous\"},\n    {\"word\": \"\u6700\u4f73\", \"pinyin\": \"zu\u00ecji\u0101\", \"trans\": \"best\"},\n    {\"word\": \"\u8d85\u8d8a\", \"pinyin\": \"ch\u0101oyu\u00e8\", \"trans\": \"surpass\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 193, "total_tokens": 882, "completion_tokens": 689}}
[14.06.2025 18:31] Response: [
    {"word": "Êé®ÁêÜ", "pinyin": "tuƒ´l«ê", "trans": "reasoning"},
    {"word": "Êï∞ÊçÆÈõÜ", "pinyin": "sh√πj√πj√≠", "trans": "dataset"},
    {"word": "ÁªìÂêà", "pinyin": "ji√©h√©", "trans": "combine"},
    {"word": "ËØ¶ÁªÜ", "pinyin": "xi√°ngx√¨", "trans": "detailed"},
    {"word": "Ë∑ØÂæÑ", "pinyin": "l√πj√¨ng", "trans": "path"},
    {"word": "ÁÆÄÊòé", "pinyin": "ji«énm√≠ng", "trans": "concise"},
    {"word": "ÊëòË¶Å", "pinyin": "zhƒÅiy√†o", "trans": "summary"},
    {"word": "ÊèêÈ´ò", "pinyin": "t√≠gƒÅo", "trans": "improve"},
    {"word": "ÂáÜÁ°ÆÊÄß", "pinyin": "zh«înqu√®x√¨ng", "trans": "accuracy"},
    {"word": "È´òË¥®Èáè", "pinyin": "gƒÅozh√¨li√†ng", "trans": "high-quality"},
    {"word": "Ê†∑Êú¨", "pinyin": "y√†ngbƒõn", "trans": "sample"},
    {"word": "Â§ö‰ª£ÁêÜ", "pinyin": "du≈çd√†il«ê", "trans": "multi-agent"},
    {"word": "È™åËØÅ", "pinyin": "y√†nzh√®ng", "trans": "verification"},
    {"word": "ÊîπËøõ", "pinyin": "g«éij√¨n", "trans": "improvement"},
    {"word": "ËøáÁ®ã", "pinyin": "gu√≤ch√©ng", "trans": "process"},
    {"word": "ÁîüÊàê", "pinyin": "shƒìngch√©ng", "trans": "generate"},
    {"word": "ÂèëÁé∞", "pinyin": "fƒÅxi√†n", "trans": "discover"},
    {"word": "ÂæÆË∞É", "pinyin": "wƒìiti√°o", "trans": "fine-tune"},
    {"word": "Á≠ñÁï•", "pinyin": "c√®l√º√®", "trans": "strategy"},
    {"word": "ÊúÄÊúâÊïà", "pinyin": "zu√¨ y«íuxi√†o", "trans": "most effective"},
    {"word": "Âü∫‰∫é", "pinyin": "jƒ´y√∫", "trans": "based on"},
    {"word": "‰πãÂâç", "pinyin": "zhƒ´qi√°n", "trans": "previous"},
    {"word": "ÊúÄ‰Ω≥", "pinyin": "zu√¨jiƒÅ", "trans": "best"},
    {"word": "Ë∂ÖË∂ä", "pinyin": "chƒÅoyu√®", "trans": "surpass"}
]
[14.06.2025 18:31] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏Ä‰∏™Â§ßÂûãÂåªÂ≠¶Êé®ÁêÜÊï∞ÊçÆÈõÜReasonMed„ÄÇÂÆÉÁªìÂêàËØ¶ÁªÜÁöÑÊé®ÁêÜË∑ØÂæÑÂíåÁÆÄÊòéÁöÑÊëòË¶ÅÔºåÊèêÈ´ò‰∫ÜÂåªÂ≠¶ÈóÆÁ≠îÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´370k‰∏™È´òË¥®ÈáèÊ†∑Êú¨ÔºåÈÄöËøáÂ§ö‰ª£ÁêÜÈ™åËØÅÂíåÊîπËøõËøáÁ®ãÁîüÊàê„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁªìÂêàËØ¶ÁªÜÁöÑÊé®ÁêÜÈìæÂíåÁÆÄÊòéÁ≠îÊ°àÊëòË¶ÅÁöÑÂæÆË∞ÉÁ≠ñÁï•ÊúÄÊúâÊïà„ÄÇÂü∫‰∫éÊ≠§Á≠ñÁï•ËÆ≠ÁªÉÁöÑReasonMed-7BÊ®°ÂûãÔºåÊÄßËÉΩË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊúÄ‰Ω≥Ê®°Âûã„ÄÇ
[14.06.2025 18:31] Mistral response. {"id": "7b3cbffc6ad0448d840ca4cd659374e9", "object": "chat.completion", "created": 1749925917, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article introduces a large medical reasoning dataset called ReasonMed. It combines detailed reasoning paths with concise summaries, enhancing the accuracy of medical question-answering models. The dataset contains 370k high-quality samples, generated through a multi-agent validation and improvement process. Research has found that the fine-tuning strategy combining detailed reasoning chains and concise answer summaries is the most effective. The ReasonMed-7B model, trained using this strategy, outperforms previous best models."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 159, "total_tokens": 269, "completion_tokens": 110}}
[14.06.2025 18:31] Response: This article introduces a large medical reasoning dataset called ReasonMed. It combines detailed reasoning paths with concise summaries, enhancing the accuracy of medical question-answering models. The dataset contains 370k high-quality samples, generated through a multi-agent validation and improvement process. Research has found that the fine-tuning strategy combining detailed reasoning chains and concise answer summaries is the most effective. The ReasonMed-7B model, trained using this strategy, outperforms previous best models.
[14.06.2025 18:31] Renaming data file.
[14.06.2025 18:31] Renaming previous data. hf_papers.json to ./d/2025-06-13.json
[14.06.2025 18:31] Saving new data file.
[14.06.2025 18:31] Generating page.
[14.06.2025 18:31] Renaming previous page.
[14.06.2025 18:31] Renaming previous data. index.html to ./d/2025-06-13.html
[14.06.2025 18:31] [Experimental] Generating Chinese page for reading.
[14.06.2025 18:31] Chinese vocab [{'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√πj√πj√≠', 'trans': 'dataset'}, {'word': 'ÁªìÂêà', 'pinyin': 'ji√©h√©', 'trans': 'combine'}, {'word': 'ËØ¶ÁªÜ', 'pinyin': 'xi√°ngx√¨', 'trans': 'detailed'}, {'word': 'Ë∑ØÂæÑ', 'pinyin': 'l√πj√¨ng', 'trans': 'path'}, {'word': 'ÁÆÄÊòé', 'pinyin': 'ji«énm√≠ng', 'trans': 'concise'}, {'word': 'ÊëòË¶Å', 'pinyin': 'zhƒÅiy√†o', 'trans': 'summary'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'}, {'word': 'ÂáÜÁ°ÆÊÄß', 'pinyin': 'zh«înqu√®x√¨ng', 'trans': 'accuracy'}, {'word': 'È´òË¥®Èáè', 'pinyin': 'gƒÅozh√¨li√†ng', 'trans': 'high-quality'}, {'word': 'Ê†∑Êú¨', 'pinyin': 'y√†ngbƒõn', 'trans': 'sample'}, {'word': 'Â§ö‰ª£ÁêÜ', 'pinyin': 'du≈çd√†il«ê', 'trans': 'multi-agent'}, {'word': 'È™åËØÅ', 'pinyin': 'y√†nzh√®ng', 'trans': 'verification'}, {'word': 'ÊîπËøõ', 'pinyin': 'g«éij√¨n', 'trans': 'improvement'}, {'word': 'ËøáÁ®ã', 'pinyin': 'gu√≤ch√©ng', 'trans': 'process'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'}, {'word': 'ÂèëÁé∞', 'pinyin': 'fƒÅxi√†n', 'trans': 'discover'}, {'word': 'ÂæÆË∞É', 'pinyin': 'wƒìiti√°o', 'trans': 'fine-tune'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√®l√º√®', 'trans': 'strategy'}, {'word': 'ÊúÄÊúâÊïà', 'pinyin': 'zu√¨ y«íuxi√†o', 'trans': 'most effective'}, {'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´y√∫', 'trans': 'based on'}, {'word': '‰πãÂâç', 'pinyin': 'zhƒ´qi√°n', 'trans': 'previous'}, {'word': 'ÊúÄ‰Ω≥', 'pinyin': 'zu√¨jiƒÅ', 'trans': 'best'}, {'word': 'Ë∂ÖË∂ä', 'pinyin': 'chƒÅoyu√®', 'trans': 'surpass'}]
[14.06.2025 18:31] Renaming previous Chinese page.
[14.06.2025 18:31] Renaming previous data. zh.html to ./d/2025-06-13_zh_reading_task.html
[14.06.2025 18:31] Writing Chinese reading task.
[14.06.2025 18:31] Writing result.
[14.06.2025 18:31] Renaming log file.
[14.06.2025 18:31] Renaming previous data. log.txt to ./logs/2025-06-14_last_log.txt
