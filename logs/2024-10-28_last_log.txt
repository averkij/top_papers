[28.10.2024 09:28] [Experimental] Generating an image for paper ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting.
[28.10.2024 09:28] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting' Text: 'Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. A key issue is the difficulty in smoothly connecting individual entities in low-level observations with abstract concepts required for planning. A common approach to address this problem is through the use of hierarchical agents, where VLMs serve as high-level reasoners that break down tasks into executable sub-tasks, typically specified using language and imagined observations. However, language often fails to effectively convey spatial information, while generating future images with sufficient accuracy remains challenging. To address these limitations, we propose visual-temporal context prompting, a novel communication protocol between VLMs and policy models. This protocol leverages object segmentation from both past and present observations to guide policy-environment interactions. Using this approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, with real-time object tracking provided by SAM-2. Our method unlocks the full potential of VLMs visual-language reasoning abilities, enabling them to solve complex creative tasks, especially those heavily reliant on spatial understanding. Experiments in Minecraft demonstrate that our approach allows agents to accomplish previously unattainable tasks, highlighting the effectiveness of visual-temporal context prompting in embodied decision-making. Codes and demos will be available on the project page: https://craftjarvis.github.io/ROCKET-1.'
[28.10.2024 09:28] Response: **Image Prompt:** Create a linear art illustration on a white background depicting an abstract representation of a rocket launching into a surreal open-world environment. The rocket, labeled "ROCKET-1," is visually fragmented, with each part symbolizing elements of vision-language models and spatial reasoning. Surrounding the rocket, integrate abstract shapes and symbols representing low-level observations, high-level reasoning, and hierarchical agents, intertwined with flowing lines that suggest temporal movement and connection. Include visual elements that evoke the complexity of decision-making in dynamic environments, such as fragmented landscapes, geometric patterns, and ethereal forms. 

**Label for Object:** "Master Open-World Interaction with Visual-Temporal Context Prompting"
[28.10.2024 09:28] Generating image by prompt: **Image Prompt:** Create a linear art illustration on a white background depicting an abstract representation of a rocket launching into a surreal open-world environment. The rocket, labeled "ROCKET-1," is visually fragmented, with each part symbolizing elements of vision-language models and spatial reasoning. Surrounding the rocket, integrate abstract shapes and symbols representing low-level observations, high-level reasoning, and hierarchical agents, intertwined with flowing lines that suggest temporal movement and connection. Include visual elements that evoke the complexity of decision-making in dynamic environments, such as fragmented landscapes, geometric patterns, and ethereal forms. 

**Label for Object:** "Master Open-World Interaction with Visual-Temporal Context Prompting".
[28.10.2024 09:28] Saving generated image from https://fal.media/files/penguin/SBQSExRYIW6DBH4lmPswk.png to 0a477d46d035f1d4.jpg.
[28.10.2024 10:14] Read previous papers.
[28.10.2024 10:14] Get feed.
[28.10.2024 10:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.17856
[28.10.2024 10:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16048
[28.10.2024 10:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18558
[28.10.2024 10:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19008
[28.10.2024 10:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19168
[28.10.2024 10:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19355
[28.10.2024 10:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19730
[28.10.2024 10:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19290
[28.10.2024 10:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19133
[28.10.2024 10:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18076
[28.10.2024 10:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16270
[28.10.2024 10:14] ********************************************************************************
[28.10.2024 10:14] Abstract 0. Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. A key issue is the difficulty in smoothly connecting individual entities in low-level observations with abstract concepts required for planni...
[28.10.2024 10:14] ********************************************************************************
[28.10.2024 10:14] Abstract 1. The success of autoregressive transformer models with discrete tokens has inspired quantization-based approaches for continuous modalities, though these often limit reconstruction quality. We therefore introduce SALAD, a per-token latent diffusion model for zero-shot text-to-speech, that operates on...
[28.10.2024 10:14] ********************************************************************************
[28.10.2024 10:14] Abstract 2. Vision-Language Models (VLMs) have recently made significant progress, but the limited scale and quality of open-source instruction data hinder their performance compared to closed-source models. In this work, we address this limitation by introducing Infinity-MM, a large-scale multimodal instructio...
[28.10.2024 10:14] ********************************************************************************
[28.10.2024 10:14] Abstract 3. The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for assessing cardiac conditions. Existing automatic interpretation methods suffer from limited generalizability, focusing on a narrow range of cardiac conditions, and typically depend on raw physiological signals, which may no...
[28.10.2024 10:14] ********************************************************************************
[28.10.2024 10:14] Abstract 4. The ability to comprehend audio--which includes speech, non-speech sounds, and music--is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex r...
[28.10.2024 10:14] ********************************************************************************
[28.10.2024 10:14] Abstract 5. In this paper, we present \textit{FasterCache}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that directly reusing adjacent-step features degrades video quality due to...
[28.10.2024 10:14] ********************************************************************************
[28.10.2024 10:14] Abstract 6. Transformers, the backbone of modern large language models (LLMs), face inherent architectural limitations that impede their reasoning capabilities. Unlike recurrent networks, Transformers lack recurrent connections, confining them to constant-depth computation. This restriction places them in the c...
[28.10.2024 10:14] ********************************************************************************
[28.10.2024 10:14] Abstract 7. Recent studies have identified one aggravating factor of LLM hallucinations as the knowledge inconsistency between pre-training and fine-tuning, where unfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong outputs. In this paper, we propose a novel fine-tuning strategy called P...
[28.10.2024 10:14] ********************************************************************************
[28.10.2024 10:14] Abstract 8. Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, directly collecting human preferences can be expensive, time-consuming, and can have high variance. An appealing alternative is to distill preferences from LMs as a source of synthetic an...
[28.10.2024 10:14] ********************************************************************************
[28.10.2024 10:14] Abstract 9. Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative sel...
[28.10.2024 10:14] ********************************************************************************
[28.10.2024 10:14] Abstract 10. The ability to adapt beliefs or behaviors in response to unexpected outcomes, reflection, is fundamental to intelligent systems' interaction with the world. From a cognitive science perspective, this serves as a core principle of intelligence applicable to both human and AI systems. To address the d...
[28.10.2024 10:14] Read previous papers.
[28.10.2024 10:14] Generating reviews via LLM API.
[28.10.2024 10:14] Using data from previous issue: {"categories": ["#agents", "#cv", "#multimodal", "#rl"], "emoji": "üöÄ", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–ª—è VLM –≤ –≤–æ–ø–ª–æ—â–µ–Ω–Ω–æ–º –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Å
[28.10.2024 10:14] Using data from previous issue: {"categories": ["#audio", "#diffusion", "#benchmark"], "emoji": "üó£Ô∏è", "ru": {"title": "SALAD: –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏", "desc": "SALAD - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –ª–∞—Ç–µ–Ω—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏, —á—Ç–æ –ø–æ
[28.10.2024 10:14] Using data from previous issue: {"categories": ["#dataset", "#data", "#multimodal"], "emoji": "üîç", "ru": {"title": "–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ - –∫–ª—é—á –∫ —É–ª—É—á—à–µ–Ω–∏—é –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Infinity-MM - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 40 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –æ–±—Ä–∞–∑—Ü–æ–≤.
[28.10.2024 10:14] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#medicine"], "emoji": "‚ù§Ô∏è", "ru": {"title": "PULSE: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ –≠–ö–ì —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –≠–ö–ì —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[28.10.2024 10:14] Using data from previous issue: {"categories": ["#benchmark", "#audio", "#multimodal"], "emoji": "üéµ", "ru": {"title": "MMAU: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∞—É–¥–∏–æ –¥–ª—è –ò–ò", "desc": "MMAU - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—É–¥–∏–æ. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 10 —Ç—ã—Å—è—á –∞—É–¥–∏–æ–∫–ª–∏–ø–æ–≤ —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –∏ –æ—Ç–≤–µ—Ç–∞–º–∏, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–º–∏ —Ä–µ—á
[28.10.2024 10:14] Using data from previous issue: {"categories": ["#video", "#diffusion", "#inference"], "emoji": "üöÄ", "ru": {"title": "FasterCache: –£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "FasterCache - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫—ç—à-–ø–æ
[28.10.2024 10:14] Using data from previous issue: {"categories": ["#reasoning", "#architecture"], "emoji": "üßÆ", "ru": {"title": "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–∞–∫ –∫–ª—é—á –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –≤–ª–∏—è–Ω–∏—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –ø–æ–¥—Å—á–µ—Ç—É. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
[28.10.2024 10:14] Using data from previous issue: {"categories": ["#hallucinations", "#alignment"], "emoji": "üß†", "ru": {"title": "Prereq-Tune: –£–º–µ–Ω—å—à–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç–µ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞–≤—ã–∫–∞–º –∏ –∑–Ω–∞–Ω–∏—è–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–æ–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Prereq-Tune, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –Ω–∞
[28.10.2024 10:14] Using data from previous issue: {"categories": ["#rlhf", "#dataset", "#data", "#optimization"], "emoji": "üîÄ", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥,
[28.10.2024 10:14] Using data from previous issue: {"categories": ["#rl", "#rlhf"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤ RL —Å –ø–æ–º–æ—â—å—é –Ω–µ–ø–æ–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ SUPE –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–µ–ø–æ–º–µ—á–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. SUPE —Å–Ω–∞—á–∞–ª–∞ –∏
[28.10.2024 10:14] Using data from previous issue: {"categories": ["#benchmark", "#interpretability"], "emoji": "ü§î", "ru": {"title": "Reflection-Bench: –∏–∑–º–µ—Ä—è—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ò–ò –∫ —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Reflection-Bench - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á
[28.10.2024 10:14] Loading Chinese text from previous data.
[28.10.2024 10:14] Renaming data file.
[28.10.2024 10:14] Renaming previous data. hf_papers.json to ./d/2024-10-28.json
[28.10.2024 10:14] Saving new data file.
[28.10.2024 10:14] Generating page.
[28.10.2024 10:14] Renaming previous page.
[28.10.2024 10:14] Renaming previous data. index.html to ./d/2024-10-28.html
[28.10.2024 10:14] [Experimental] Generating Chinese page for reading.
[28.10.2024 10:14] Chinese vocab [{'word': 'ËßÜËßâËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'sh√¨ju√© y«îy√°n m√≥x√≠ng', 'trans': 'visual language models'}, {'word': 'ÂºÄÊîæÁéØÂ¢É', 'pinyin': 'kƒÅif√†ng hu√°nj√¨ng', 'trans': 'open environments'}, {'word': 'ÂÜ≥Á≠ñËÉΩÂäõ', 'pinyin': 'ju√©c√® n√©ngl√¨', 'trans': 'decision-making ability'}, {'word': 'Â§öÊ®°Âºè‰ªªÂä°', 'pinyin': 'du≈ç m√≥sh√¨ r√®nw√π', 'trans': 'multimodal tasks'}, {'word': 'Ë°®Áé∞Âá∫Ëâ≤', 'pinyin': 'bi«éoxi√†n ch≈´s√®', 'trans': 'perform well'}, {'word': 'Èù¢‰∏¥ÊåëÊàò', 'pinyin': 'mi√†nl√≠n ti«éozh√†n', 'trans': 'face challenges'}, {'word': '‰ΩéÂ±ÇÊ¨°ËßÇÂØü', 'pinyin': 'dƒ´ c√©ngc√¨ guƒÅnch√°', 'trans': 'low-level observations'}, {'word': 'Âçï‰∏™ÂÆû‰Ωì', 'pinyin': 'dƒÅn g√® sh√≠t«ê', 'trans': 'individual entities'}, {'word': 'ËßÑÂàíÊâÄÈúÄÁöÑÊäΩË±°Ê¶ÇÂøµ', 'pinyin': 'guƒ´hu√† su«íx≈´ de ch≈çuxi√†ng g√†ini√†n', 'trans': 'abstract concepts required for planning'}, {'word': 'ËßÜËßâÊó∂Èó¥‰∏ä‰∏ãÊñáÊèêÁ§∫', 'pinyin': 'sh√¨ju√© sh√≠jiƒÅn sh√†ngxi√†w√©n t√≠sh√¨', 'trans': 'visual temporal context prompts'}, {'word': 'Êñ∞ÈÄö‰ø°ÂçèËÆÆ', 'pinyin': 'xƒ´n t≈çngx√¨n xi√©y√¨', 'trans': 'new communication protocol'}, {'word': 'ËøáÂéªÂíåÁé∞Âú®ÁöÑËßÇÂØü', 'pinyin': 'gu√≤q√π h√© xi√†nz√†i de guƒÅnch√°', 'trans': 'past and present observations'}, {'word': 'ÂØπË±°ÂàÜÂâ≤', 'pinyin': 'du√¨xi√†ng fƒìng√©', 'trans': 'object segmentation'}, {'word': 'ÊåáÂØºÁ≠ñÁï•‰∏éÁéØÂ¢ÉÁöÑ‰∫§‰∫í', 'pinyin': 'zh«êd«éo c√®l√º√® y«î hu√°nj√¨ng de jiƒÅoh√π', 'trans': 'guide the interaction of strategies with the environment'}, {'word': '‰ª£ÁêÜ', 'pinyin': 'd√†il«ê', 'trans': 'agent'}, {'word': 'ÂÆåÊàê‰ª•ÂâçÊó†Ê≥ïÂÆûÁé∞ÁöÑ‰ªªÂä°', 'pinyin': 'w√°nch√©ng y«êqi√°n w√∫f«é sh√≠xi√†n de r√®nw√π', 'trans': 'complete tasks that were previously impossible'}, {'word': '‰æùËµñÁ©∫Èó¥ÁêÜËß£ÁöÑÂ§çÊùÇ‰ªªÂä°', 'pinyin': 'yƒ´l√†i k≈çngjiƒÅn l«êjiƒõ de f√πz√° r√®nw√π', 'trans': 'complex tasks that rely on spatial understanding'}]
[28.10.2024 10:14] Renaming previous Chinese page.
[28.10.2024 10:14] Renaming previous data. zh.html to ./d/2024-10-27_zh_reading_task.html
[28.10.2024 10:14] Writing result.
[28.10.2024 10:14] Writing Chinese reading task.
[28.10.2024 10:14] Renaming log file.
[28.10.2024 10:14] Renaming previous data. log.txt to ./logs/2024-10-28_last_log.txt
