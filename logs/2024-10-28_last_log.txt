[28.10.2024 08:17] Read previous papers.
[28.10.2024 08:17] Get feed.
[28.10.2024 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2410.17856
[28.10.2024 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18558
[28.10.2024 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19008
[28.10.2024 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16048
[28.10.2024 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19168
[28.10.2024 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19355
[28.10.2024 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19730
[28.10.2024 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19290
[28.10.2024 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19133
[28.10.2024 08:17] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18076
[28.10.2024 08:17] ********************************************************************************
[28.10.2024 08:17] Abstract 0. Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. A key issue is the difficulty in smoothly connecting individual entities in low-level observations with abstract concepts required for planni...
[28.10.2024 08:17] ********************************************************************************
[28.10.2024 08:17] Abstract 1. Vision-Language Models (VLMs) have recently made significant progress, but the limited scale and quality of open-source instruction data hinder their performance compared to closed-source models. In this work, we address this limitation by introducing Infinity-MM, a large-scale multimodal instructio...
[28.10.2024 08:17] ********************************************************************************
[28.10.2024 08:17] Abstract 2. The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for assessing cardiac conditions. Existing automatic interpretation methods suffer from limited generalizability, focusing on a narrow range of cardiac conditions, and typically depend on raw physiological signals, which may no...
[28.10.2024 08:17] ********************************************************************************
[28.10.2024 08:17] Abstract 3. The success of autoregressive transformer models with discrete tokens has inspired quantization-based approaches for continuous modalities, though these often limit reconstruction quality. We therefore introduce SALAD, a per-token latent diffusion model for zero-shot text-to-speech, that operates on...
[28.10.2024 08:17] ********************************************************************************
[28.10.2024 08:17] Abstract 4. The ability to comprehend audio--which includes speech, non-speech sounds, and music--is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex r...
[28.10.2024 08:17] ********************************************************************************
[28.10.2024 08:17] Abstract 5. In this paper, we present \textit{FasterCache}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that directly reusing adjacent-step features degrades video quality due to...
[28.10.2024 08:17] ********************************************************************************
[28.10.2024 08:17] Abstract 6. Transformers, the backbone of modern large language models (LLMs), face inherent architectural limitations that impede their reasoning capabilities. Unlike recurrent networks, Transformers lack recurrent connections, confining them to constant-depth computation. This restriction places them in the c...
[28.10.2024 08:17] ********************************************************************************
[28.10.2024 08:17] Abstract 7. Recent studies have identified one aggravating factor of LLM hallucinations as the knowledge inconsistency between pre-training and fine-tuning, where unfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong outputs. In this paper, we propose a novel fine-tuning strategy called P...
[28.10.2024 08:17] ********************************************************************************
[28.10.2024 08:17] Abstract 8. Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, directly collecting human preferences can be expensive, time-consuming, and can have high variance. An appealing alternative is to distill preferences from LMs as a source of synthetic an...
[28.10.2024 08:17] ********************************************************************************
[28.10.2024 08:17] Abstract 9. Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative sel...
[28.10.2024 08:17] Read previous papers.
[28.10.2024 08:17] Generating reviews via LLM API.
[28.10.2024 08:17] Using data from previous issue: {"categories": ["#agents", "#cv", "#multimodal", "#rl"], "emoji": "üöÄ", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–ª—è VLM –≤ –≤–æ–ø–ª–æ—â–µ–Ω–Ω–æ–º –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Å
[28.10.2024 08:17] Using data from previous issue: {"categories": ["#dataset", "#data", "#multimodal"], "emoji": "üîç", "ru": {"title": "–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ - –∫–ª—é—á –∫ —É–ª—É—á—à–µ–Ω–∏—é –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Infinity-MM - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 40 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –æ–±—Ä–∞–∑—Ü–æ–≤.
[28.10.2024 08:17] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#medicine"], "emoji": "‚ù§Ô∏è", "ru": {"title": "PULSE: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ –≠–ö–ì —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –≠–ö–ì —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[28.10.2024 08:17] Using data from previous issue: {"categories": ["#audio", "#diffusion", "#benchmark"], "emoji": "üó£Ô∏è", "ru": {"title": "SALAD: –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏", "desc": "SALAD - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –ª–∞—Ç–µ–Ω—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏, —á—Ç–æ –ø–æ
[28.10.2024 08:17] Using data from previous issue: {"categories": ["#benchmark", "#audio", "#multimodal"], "emoji": "üéµ", "ru": {"title": "MMAU: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∞—É–¥–∏–æ –¥–ª—è –ò–ò", "desc": "MMAU - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—É–¥–∏–æ. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 10 —Ç—ã—Å—è—á –∞—É–¥–∏–æ–∫–ª–∏–ø–æ–≤ —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –∏ –æ—Ç–≤–µ—Ç–∞–º–∏, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–º–∏ —Ä–µ—á
[28.10.2024 08:17] Using data from previous issue: {"categories": ["#video", "#diffusion", "#inference"], "emoji": "üöÄ", "ru": {"title": "FasterCache: –£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "FasterCache - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫—ç—à-–ø–æ
[28.10.2024 08:17] Using data from previous issue: {"categories": ["#reasoning", "#architecture", "#training"], "emoji": "üßÆ", "ru": {"title": "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–∞–∫ –∫–ª—é—á –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –≤–ª–∏—è–Ω–∏—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –ø–æ–¥—Å—á–µ—Ç—É. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á
[28.10.2024 08:17] Using data from previous issue: {"categories": ["#hallucinations", "#alignment"], "emoji": "üß†", "ru": {"title": "Prereq-Tune: –£–º–µ–Ω—å—à–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—É—Ç–µ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞–≤—ã–∫–∞–º –∏ –∑–Ω–∞–Ω–∏—è–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–æ–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Prereq-Tune, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –Ω–∞
[28.10.2024 08:17] Using data from previous issue: {"categories": ["#rlhf", "#dataset", "#data", "#optimization"], "emoji": "üîÄ", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥,
[28.10.2024 08:17] Using data from previous issue: {"categories": ["#rl", "#rlhf"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤ RL —Å –ø–æ–º–æ—â—å—é –Ω–µ–ø–æ–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ SUPE –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–µ–ø–æ–º–µ—á–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. SUPE —Å–Ω–∞—á–∞–ª–∞ –∏
[28.10.2024 08:17] Loading Chinese text from previous data.
[28.10.2024 08:17] Renaming data file.
[28.10.2024 08:17] Renaming previous data. hf_papers.json to ./d/2024-10-28.json
[28.10.2024 08:17] Saving new data file.
[28.10.2024 08:17] Generating page.
[28.10.2024 08:17] Renaming previous page.
[28.10.2024 08:17] Renaming previous data. index.html to ./d/2024-10-28.html
[28.10.2024 08:17] [Experimental] Generating Chinese page for reading.
[28.10.2024 08:17] Chinese vocab [{'word': 'ÂØπÊØîÊçüÂ§±', 'pinyin': 'du√¨b«ê s«înshƒ´', 'trans': 'contrastive loss'}, {'word': 'ËÆ°ÁÆóÁ≠ñÁï•', 'pinyin': 'j√¨su√†n c√®l√º√®', 'trans': 'computational strategy'}, {'word': '‰º†ÁªüÊñπÊ≥ï', 'pinyin': 'chu√°nt«íng fƒÅngf«é', 'trans': 'traditional method'}, {'word': 'ÂèóÈôê‰∫é', 'pinyin': 'sh√≤u xi√†n y√∫', 'trans': 'limited by'}, {'word': 'GPUÂÜÖÂ≠òÊ∂àËÄó', 'pinyin': 'GPU n√®ic√∫n xiƒÅoh√†o', 'trans': 'GPU memory consumption'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ch≈´', 'trans': 'propose'}, {'word': 'Âü∫‰∫éÂùóÁöÑËÆ°ÁÆóÁ≠ñÁï•', 'pinyin': 'jƒ´y√∫ ku√†i de j√¨su√†n c√®l√º√®', 'trans': 'block-based computational strategy'}, {'word': 'ÈÅøÂÖç', 'pinyin': 'b√¨mi«én', 'trans': 'avoid'}, {'word': 'Áõ∏‰ººÁü©Èòµ', 'pinyin': 'xiƒÅngs√¨ j«îzh√®n', 'trans': 'similarity matrix'}, {'word': 'ÂÆåÂÖ®ÂÆû‰æãÂåñ', 'pinyin': 'w√°nqu√°n sh√≠l√¨hu√†', 'trans': 'fully instantiate'}, {'word': 'Â§öÁ∫ßÂùóÁ≠ñÁï•', 'pinyin': 'du≈çj√≠ ku√†i c√®l√º√®', 'trans': 'multi-level block strategy'}, {'word': 'Âà©Áî®', 'pinyin': 'l√¨y√≤ng', 'trans': 'utilize'}, {'word': 'ÂàÜÂ∏ÉÂºèÁ≥ªÁªü', 'pinyin': 'fƒìnb√πsh√¨ x√¨t«íng', 'trans': 'distributed system'}, {'word': 'Â±ÇÊ¨°ÁªìÊûÑ', 'pinyin': 'c√©ngc√¨ ji√©g√≤u', 'trans': 'hierarchical structure'}, {'word': 'ÂÆûÈ™åÁªìÊûú', 'pinyin': 'sh√≠y√†n ji√©gu«í', 'trans': 'experimental results'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'}, {'word': 'Êâ©Â§ß', 'pinyin': 'ku√≤d√†', 'trans': 'expand'}, {'word': 'ÊâπÈáèÂ§ßÂ∞è', 'pinyin': 'pƒ´li√†ng d√†xia«í', 'trans': 'batch size'}, {'word': 'ÂáÜÁ°ÆÊÄß', 'pinyin': 'zh«înqu√®x√¨ng', 'trans': 'accuracy'}]
[28.10.2024 08:17] Renaming previous Chinese page.
[28.10.2024 08:17] Renaming previous data. zh.html to ./d/2024-10-27_zh_reading_task.html
[28.10.2024 08:17] Writing result.
[28.10.2024 08:17] Writing Chinese reading task.
[28.10.2024 08:17] Renaming log file.
[28.10.2024 08:17] Renaming previous data. log.txt to ./logs/2024-10-28_last_log.txt
