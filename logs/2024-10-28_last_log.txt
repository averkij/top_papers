[28.10.2024 20:13] [Experimental] Generating an image for paper ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting.
[28.10.2024 20:13] [Experimental] Image for paper ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting already exists.
[28.10.2024 20:13] [Experimental] Generating an image for paper Continuous Speech Synthesis using per-token Latent Diffusion.
[28.10.2024 20:13] [Experimental] Image for paper Continuous Speech Synthesis using per-token Latent Diffusion already exists.
[28.10.2024 20:13] [Experimental] Generating an image for paper Teach Multimodal LLMs to Comprehend Electrocardiographic Images.
[28.10.2024 20:13] [Experimental] Image for paper Teach Multimodal LLMs to Comprehend Electrocardiographic Images already exists.
[28.10.2024 20:13] [Experimental] Generating an image for paper FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality.
[28.10.2024 20:13] [Experimental] Image for paper FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality already exists.
[28.10.2024 20:13] [Experimental] Generating an image for paper Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data.
[28.10.2024 20:13] [Experimental] Image for paper Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data already exists.
[28.10.2024 20:13] [Experimental] Generating an image for paper Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance.
[28.10.2024 20:13] [Experimental] Image for paper Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance already exists.
[28.10.2024 20:13] [Experimental] Generating an image for paper MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark.
[28.10.2024 20:13] [Experimental] Image for paper MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark already exists.
[28.10.2024 22:11] Read previous papers.
[28.10.2024 22:11] Get feed.
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.17856
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16048
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19008
[28.10.2024 22:11] Extract page data from URL. URL: https://huggingface.co/papers/2410.19123
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19355
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18558
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18889
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19168
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19133
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16090
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19730
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19290
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.17655
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18076
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18912
[28.10.2024 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16270
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 0. Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. A key issue is the difficulty in smoothly connecting individual entities in low-level observations with abstract concepts required for planni...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 1. The success of autoregressive transformer models with discrete tokens has inspired quantization-based approaches for continuous modalities, though these often limit reconstruction quality. We therefore introduce SALAD, a per-token latent diffusion model for zero-shot text-to-speech, that operates on...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 2. The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for assessing cardiac conditions. Existing automatic interpretation methods suffer from limited generalizability, focusing on a narrow range of cardiac conditions, and typically depend on raw physiological signals, which may no...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 3. The proliferation of large language models (LLMs) has led to the adoption of Mixture-of-Experts (MoE) architectures that dynamically leverage specialized subnetworks for improved efficiency and performance. Despite their benefits, MoE models face significant challenges during inference, including in...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 4. In this paper, we present \textit{FasterCache}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that directly reusing adjacent-step features degrades video quality due to...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 5. Vision-Language Models (VLMs) have recently made significant progress, but the limited scale and quality of open-source instruction data hinder their performance compared to closed-source models. In this work, we address this limitation by introducing Infinity-MM, a large-scale multimodal instructio...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 6. NLP benchmarks rely on standardized datasets for training and evaluating models and are crucial for advancing the field. Traditionally, expert annotations ensure high-quality labels; however, the cost of expert annotation does not scale well with the growing demand for larger datasets required by mo...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 7. The ability to comprehend audio--which includes speech, non-speech sounds, and music--is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex r...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 8. Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, directly collecting human preferences can be expensive, time-consuming, and can have high variance. An appealing alternative is to distill preferences from LMs as a source of synthetic an...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 9. Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model behaviour, such as reliance on outdated or incorrect infor...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 10. Transformers, the backbone of modern large language models (LLMs), face inherent architectural limitations that impede their reasoning capabilities. Unlike recurrent networks, Transformers lack recurrent connections, confining them to constant-depth computation. This restriction places them in the c...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 11. Recent studies have identified one aggravating factor of LLM hallucinations as the knowledge inconsistency between pre-training and fine-tuning, where unfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong outputs. In this paper, we propose a novel fine-tuning strategy called P...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 12. Bias assessment of news sources is paramount for professionals, organizations, and researchers who rely on truthful evidence for information gathering and reporting. While certain bias indicators are discernible from content analysis, descriptors like political bias and fake news pose greater challe...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 13. Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative sel...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 14. Videos of robots interacting with objects encode rich information about the objects' dynamics. However, existing video prediction approaches typically do not explicitly account for the 3D information from videos, such as robot actions and objects' 3D states, limiting their use in real-world robotic ...
[28.10.2024 22:11] ********************************************************************************
[28.10.2024 22:11] Abstract 15. The ability to adapt beliefs or behaviors in response to unexpected outcomes, reflection, is fundamental to intelligent systems' interaction with the world. From a cognitive science perspective, this serves as a core principle of intelligence applicable to both human and AI systems. To address the d...
[28.10.2024 22:11] Read previous papers.
[28.10.2024 22:11] Generating reviews via LLM API.
[28.10.2024 22:11] Using data from previous issue: {"categories": ["#agents", "#cv", "#multimodal", "#rl"], "emoji": "ğŸš€", "ru": {"title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ VLM Ğ² Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ
[28.10.2024 22:11] Using data from previous issue: {"categories": ["#audio", "#diffusion", "#benchmark"], "emoji": "ğŸ—£ï¸", "ru": {"title": "SALAD: Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸", "desc": "SALAD - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾
[28.10.2024 22:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#medicine"], "emoji": "â¤ï¸", "ru": {"title": "PULSE: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ­ĞšĞ“ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ­ĞšĞ“ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒ
[28.10.2024 22:11] Querying the API.
[28.10.2024 22:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The proliferation of large language models (LLMs) has led to the adoption of Mixture-of-Experts (MoE) architectures that dynamically leverage specialized subnetworks for improved efficiency and performance. Despite their benefits, MoE models face significant challenges during inference, including inefficient memory management and suboptimal batching, due to misaligned design choices between the model architecture and the system policies. Furthermore, the conventional approach of training MoEs from scratch is increasingly prohibitive in terms of cost. In this paper, we propose a novel framework Read-ME that transforms pre-trained dense LLMs into smaller MoE models (in contrast to "upcycling" generalist MoEs), avoiding the high costs of ground-up training. Our approach employs activation sparsity to extract experts. To compose experts, we examine the widely-adopted layer-wise router design and show its redundancy, and thus we introduce the pre-gating router decoupled from the MoE backbone that facilitates system-friendly pre-computing and lookahead scheduling, enhancing expert-aware batching and caching. Our codesign therefore addresses critical gaps on both the algorithmic and system fronts, establishing a scalable and efficient alternative for LLM inference in resource-constrained settings. Read-ME outperforms other popular open-source dense models of similar scales, achieving improvements of up to 10.1% on MMLU, and improving mean end-to-end latency up to 6.1%. Codes are available at: https://github.com/VITA-Group/READ-ME.
[28.10.2024 22:12] Response: {
  "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Read-ME Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mixture-of-Experts (MoE). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ MoE Ñ Ğ½ÑƒĞ»Ñ. Read-ME Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 10.1% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMLU Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ´Ğ¾ 6.1%.",
  "emoji": "ğŸ§ ",
  "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ MoE Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"
}
[28.10.2024 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: You are an expert classifier of machine learning research papers. Analyze the following research paper text and classify it into one or more relevant categories from the list below. Consider the paper's main contributions, methodologies, and applications.

Categories:
1. DATASET: Papers that introduce new datasets or make significant modifications to existing ones
2. DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
3. BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
4. AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
5. CV: Papers developing computer vision methods or visual processing systems
6. RL: Papers investigating reinforcement learning theory or applications
7. RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
8. RAG: Papers advancing retrieval-augmented generation techniques
9. PLP: Papers about Programming Language Processing models or programming benchmarks
10. INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
11. 3D: Papers on 3D content generation, processing, or understanding
12. AUDIO: Papers advancing speech/audio processing or generation
13. VIDEO: Papers on video analysis, generation, or understanding
14. MULTIMODAL: Papers combining multiple input/output modalities
15. MATH: Papers focused on mathematical theory and algorithms
16. MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities
17. ARCHITECTURE: Papers proposing novel neural architectures or components
18. MEDICINE: Papers applying ML to medical/healthcare domains
19. TRAINING: Papers improving model training or fine-tuning methods
20. ROBOTICS: Papers on robotic systems and embodied AI
21. AGI: Papers discussing artificial general intelligence concepts
22. GAMES: Papers applying ML to games or game development
23. INTERPRETABILITY: Papers analyzing model behavior and explanations
24. REASONING: Papers enhancing logical reasoning capabilities
25. TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
26. GRAPHS: Papers advancing graph neural networks and applications
27. ETHICS: Papers addressing AI ethics, fairness, and bias
28. SECURITY: Papers on model security and adversarial robustness
29. EDGE_COMPUTING: Papers on ML deployment for resource-constrained devices
30. OPTIMIZATION: Papers advancing training optimization methods
31. SURVEY: Papers comprehensively reviewing research areas
32. DIFFUSION: Papers on diffusion-based generative models
33. ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
34. STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
35. HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
36. LONG_CONTEXT: Papers about long context handling, including techniques to extend context length and process long sequences.

Return only JSON with flat array of categories that match the given text.

Paper text to classify:

"The proliferation of large language models (LLMs) has led to the adoption of Mixture-of-Experts (MoE) architectures that dynamically leverage specialized subnetworks for improved efficiency and performance. Despite their benefits, MoE models face significant challenges during inference, including inefficient memory management and suboptimal batching, due to misaligned design choices between the model architecture and the system policies. Furthermore, the conventional approach of training MoEs from scratch is increasingly prohibitive in terms of cost. In this paper, we propose a novel framework Read-ME that transforms pre-trained dense LLMs into smaller MoE models (in contrast to "upcycling" generalist MoEs), avoiding the high costs of ground-up training. Our approach employs activation sparsity to extract experts. To compose experts, we examine the widely-adopted layer-wise router design and show its redundancy, and thus we introduce the pre-gating router decoupled from the MoE backbone that facilitates system-friendly pre-computing and lookahead scheduling, enhancing expert-aware batching and caching. Our codesign therefore addresses critical gaps on both the algorithmic and system fronts, establishing a scalable and efficient alternative for LLM inference in resource-constrained settings. Read-ME outperforms other popular open-source dense models of similar scales, achieving improvements of up to 10.1% on MMLU, and improving mean end-to-end latency up to 6.1%. Codes are available at: https://github.com/VITA-Group/READ-ME."

[28.10.2024 22:12] Response: ```json
["ARCHITECTURE", "INFERENCE", "TRAINING", "OPTIMIZATION"]
```
[28.10.2024 22:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Read-ME, a framework that converts pre-trained dense large language models (LLMs) into smaller Mixture-of-Experts (MoE) models, which are more efficient and cost-effective. The authors address challenges in MoE models during inference, such as memory management and batching issues, by proposing a new pre-gating router design that enhances system performance. By utilizing activation sparsity, Read-ME effectively extracts and composes experts, leading to improved resource utilization. The results show that Read-ME significantly outperforms existing dense models, achieving better accuracy and reduced latency in resource-constrained environments.","title":"Transforming Dense Models into Efficient MoE with Read-ME"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper introduces Read-ME, a framework that converts pre-trained dense large language models (LLMs) into smaller Mixture-of-Experts (MoE) models, which are more efficient and cost-effective. The authors address challenges in MoE models during inference, such as memory management and batching issues, by proposing a new pre-gating router design that enhances system performance. By utilizing activation sparsity, Read-ME effectively extracts and composes experts, leading to improved resource utilization. The results show that Read-ME significantly outperforms existing dense models, achieving better accuracy and reduced latency in resource-constrained environments.', title='Transforming Dense Models into Efficient MoE with Read-ME'))
[28.10.2024 22:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶Read-MEï¼Œæ—¨åœ¨å°†é¢„è®­ç»ƒçš„å¯†é›†å‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è½¬åŒ–ä¸ºæ›´å°çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Œä»è€Œé¿å…ä»å¤´è®­ç»ƒçš„é«˜æˆæœ¬ã€‚æˆ‘ä»¬åˆ©ç”¨æ¿€æ´»ç¨€ç–æ€§æ¥æå–ä¸“å®¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä¸MoEä¸»å¹²è§£è€¦çš„é¢„é—¨æ§è·¯ç”±å™¨ï¼Œä»¥ä¼˜åŒ–ç³»ç»Ÿå‹å¥½çš„é¢„è®¡ç®—å’Œå‰ç»è°ƒåº¦ã€‚è¯¥æ–¹æ³•è§£å†³äº†æ¨¡å‹æ¶æ„ä¸ç³»ç»Ÿç­–ç•¥ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ï¼Œæé«˜äº†ä¸“å®¶æ„ŸçŸ¥çš„æ‰¹å¤„ç†å’Œç¼“å­˜æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRead-MEåœ¨MMLUä¸Šæ¯”å…¶ä»–æµè¡Œçš„å¼€æºå¯†é›†æ¨¡å‹æé«˜äº†10.1%çš„æ€§èƒ½ï¼Œå¹¶å°†å¹³å‡ç«¯åˆ°ç«¯å»¶è¿Ÿé™ä½äº†6.1%ã€‚","title":"Read-MEï¼šé«˜æ•ˆçš„æ··åˆä¸“å®¶æ¨¡å‹è½¬æ¢æ–¹æ¡ˆ"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶Read-MEï¼Œæ—¨åœ¨å°†é¢„è®­ç»ƒçš„å¯†é›†å‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è½¬åŒ–ä¸ºæ›´å°çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Œä»è€Œé¿å…ä»å¤´è®­ç»ƒçš„é«˜æˆæœ¬ã€‚æˆ‘ä»¬åˆ©ç”¨æ¿€æ´»ç¨€ç–æ€§æ¥æå–ä¸“å®¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä¸MoEä¸»å¹²è§£è€¦çš„é¢„é—¨æ§è·¯ç”±å™¨ï¼Œä»¥ä¼˜åŒ–ç³»ç»Ÿå‹å¥½çš„é¢„è®¡ç®—å’Œå‰ç»è°ƒåº¦ã€‚è¯¥æ–¹æ³•è§£å†³äº†æ¨¡å‹æ¶æ„ä¸ç³»ç»Ÿç­–ç•¥ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ï¼Œæé«˜äº†ä¸“å®¶æ„ŸçŸ¥çš„æ‰¹å¤„ç†å’Œç¼“å­˜æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRead-MEåœ¨MMLUä¸Šæ¯”å…¶ä»–æµè¡Œçš„å¼€æºå¯†é›†æ¨¡å‹æé«˜äº†10.1%çš„æ€§èƒ½ï¼Œå¹¶å°†å¹³å‡ç«¯åˆ°ç«¯å»¶è¿Ÿé™ä½äº†6.1%ã€‚', title='Read-MEï¼šé«˜æ•ˆçš„æ··åˆä¸“å®¶æ¨¡å‹è½¬æ¢æ–¹æ¡ˆ'))
[28.10.2024 22:12] Using data from previous issue: {"categories": ["#video", "#diffusion", "#inference"], "emoji": "ğŸš€", "ru": {"title": "FasterCache: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°", "desc": "FasterCache - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºÑÑˆ-Ğ¿Ğ¾
[28.10.2024 22:12] Using data from previous issue: {"categories": ["#dataset", "#data", "#multimodal"], "emoji": "ğŸ”", "ru": {"title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Infinity-MM - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 40 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ².
[28.10.2024 22:12] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark"], "emoji": "ğŸ·ï¸", "ru": {"title": "LLM ĞºĞ°Ğº ÑÑƒĞ´ÑŒÑ: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ² NLP-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ NLP-Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ 
[28.10.2024 22:12] Using data from previous issue: {"categories": ["#benchmark", "#audio", "#multimodal"], "emoji": "ğŸµ", "ru": {"title": "MMAU: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ´Ğ»Ñ Ğ˜Ğ˜", "desc": "MMAU - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 10 Ñ‚Ñ‹ÑÑÑ‡ Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑ‡
[28.10.2024 22:12] Using data from previous issue: {"categories": ["#rlhf", "#dataset", "#data", "#optimization"], "emoji": "ğŸ”€", "ru": {"title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´,
[28.10.2024 22:12] Using data from previous issue: {"categories": ["#interpretability", "#alignment", "#hallucinations"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ñ…Ñ€Ğ°Ğ½ÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ğ² Ğ¸Ñ… Ğ¿Ğ°
[28.10.2024 22:12] Using data from previous issue: {"categories": ["#reasoning", "#architecture"], "emoji": "ğŸ§®", "ru": {"title": "Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€
[28.10.2024 22:12] Using data from previous issue: {"categories": ["#hallucinations", "#alignment"], "emoji": "ğŸ§ ", "ru": {"title": "Prereq-Tune: Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Prereq-Tune, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ°
[28.10.2024 22:12] Using data from previous issue: {"categories": ["#dataset", "#rl", "#benchmark", "#data"], "emoji": "ğŸ”", "ru": {"title": "ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ğ°Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚
[28.10.2024 22:12] Using data from previous issue: {"categories": ["#rl", "#rlhf"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² RL Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SUPE Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. SUPE ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸
[28.10.2024 22:12] Using data from previous issue: {"categories": ["#3d", "#robots", "#cv"], "emoji": "ğŸ¤–", "ru": {"title": "3D-Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… RGB-Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·
[28.10.2024 22:12] Using data from previous issue: {"categories": ["#benchmark", "#interpretability"], "emoji": "ğŸ¤”", "ru": {"title": "Reflection-Bench: Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğº ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Reflection-Bench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡
[28.10.2024 22:12] Loading Chinese text from previous data.
[28.10.2024 22:12] Renaming data file.
[28.10.2024 22:12] Renaming previous data. hf_papers.json to ./d/2024-10-28.json
[28.10.2024 22:12] Saving new data file.
[28.10.2024 22:12] Generating page.
[28.10.2024 22:12] Renaming previous page.
[28.10.2024 22:12] Renaming previous data. index.html to ./d/2024-10-28.html
[28.10.2024 22:12] [Experimental] Generating Chinese page for reading.
[28.10.2024 22:12] Chinese vocab [{'word': 'è§†è§‰è¯­è¨€æ¨¡å‹', 'pinyin': 'shÃ¬juÃ© yÇ”yÃ¡n mÃ³xÃ­ng', 'trans': 'visual language models'}, {'word': 'å¼€æ”¾ç¯å¢ƒ', 'pinyin': 'kÄifÃ ng huÃ¡njÃ¬ng', 'trans': 'open environments'}, {'word': 'å†³ç­–èƒ½åŠ›', 'pinyin': 'juÃ©cÃ¨ nÃ©nglÃ¬', 'trans': 'decision-making ability'}, {'word': 'å¤šæ¨¡å¼ä»»åŠ¡', 'pinyin': 'duÅ mÃ³shÃ¬ rÃ¨nwÃ¹', 'trans': 'multimodal tasks'}, {'word': 'è¡¨ç°å‡ºè‰²', 'pinyin': 'biÇoxiÃ n chÅ«sÃ¨', 'trans': 'perform well'}, {'word': 'é¢ä¸´æŒ‘æˆ˜', 'pinyin': 'miÃ nlÃ­n tiÇozhÃ n', 'trans': 'face challenges'}, {'word': 'ä½å±‚æ¬¡è§‚å¯Ÿ', 'pinyin': 'dÄ« cÃ©ngcÃ¬ guÄnchÃ¡', 'trans': 'low-level observations'}, {'word': 'å•ä¸ªå®ä½“', 'pinyin': 'dÄn gÃ¨ shÃ­tÇ', 'trans': 'individual entities'}, {'word': 'è§„åˆ’æ‰€éœ€çš„æŠ½è±¡æ¦‚å¿µ', 'pinyin': 'guÄ«huÃ  suÇ’xÅ« de chÅuxiÃ ng gÃ iniÃ n', 'trans': 'abstract concepts required for planning'}, {'word': 'è§†è§‰æ—¶é—´ä¸Šä¸‹æ–‡æç¤º', 'pinyin': 'shÃ¬juÃ© shÃ­jiÄn shÃ ngxiÃ wÃ©n tÃ­shÃ¬', 'trans': 'visual temporal context prompts'}, {'word': 'æ–°é€šä¿¡åè®®', 'pinyin': 'xÄ«n tÅngxÃ¬n xiÃ©yÃ¬', 'trans': 'new communication protocol'}, {'word': 'è¿‡å»å’Œç°åœ¨çš„è§‚å¯Ÿ', 'pinyin': 'guÃ²qÃ¹ hÃ© xiÃ nzÃ i de guÄnchÃ¡', 'trans': 'past and present observations'}, {'word': 'å¯¹è±¡åˆ†å‰²', 'pinyin': 'duÃ¬xiÃ ng fÄ“ngÃ©', 'trans': 'object segmentation'}, {'word': 'æŒ‡å¯¼ç­–ç•¥ä¸ç¯å¢ƒçš„äº¤äº’', 'pinyin': 'zhÇdÇo cÃ¨lÃ¼Ã¨ yÇ” huÃ¡njÃ¬ng de jiÄohÃ¹', 'trans': 'guide the interaction of strategies with the environment'}, {'word': 'ä»£ç†', 'pinyin': 'dÃ ilÇ', 'trans': 'agent'}, {'word': 'å®Œæˆä»¥å‰æ— æ³•å®ç°çš„ä»»åŠ¡', 'pinyin': 'wÃ¡nchÃ©ng yÇqiÃ¡n wÃºfÇ shÃ­xiÃ n de rÃ¨nwÃ¹', 'trans': 'complete tasks that were previously impossible'}, {'word': 'ä¾èµ–ç©ºé—´ç†è§£çš„å¤æ‚ä»»åŠ¡', 'pinyin': 'yÄ«lÃ i kÅngjiÄn lÇjiÄ› de fÃ¹zÃ¡ rÃ¨nwÃ¹', 'trans': 'complex tasks that rely on spatial understanding'}]
[28.10.2024 22:12] Renaming previous Chinese page.
[28.10.2024 22:12] Renaming previous data. zh.html to ./d/2024-10-27_zh_reading_task.html
[28.10.2024 22:12] Writing result.
[28.10.2024 22:12] Writing Chinese reading task.
[28.10.2024 22:12] Renaming log file.
[28.10.2024 22:12] Renaming previous data. log.txt to ./logs/2024-10-28_last_log.txt
