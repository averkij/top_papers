[27.05.2025 07:12] Read previous papers.
[27.05.2025 07:12] Generating top page (month).
[27.05.2025 07:12] Writing top page (month).
[27.05.2025 08:15] Read previous papers.
[27.05.2025 08:15] Get feed.
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19147
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19457
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16348
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20258
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19914
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20259
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18675
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19815
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19209
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18545
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18536
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18601
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19949
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19788
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19752
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19439
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20256
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20152
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18759
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19602
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19590
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16972
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13426
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19427
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20254
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19640
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17652
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20278
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10887
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19706
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19630
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19443
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18384
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16312
[27.05.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2505.18773
[27.05.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15957
[27.05.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2505.18323
[27.05.2025 08:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.05.2025 08:15] No deleted papers detected.
[27.05.2025 08:15] Downloading and parsing papers (pdf, html). Total: 37.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19147.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19147.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19147.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19457.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19457.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19457.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.16348.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.16348.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.16348.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.20258.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.20258.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.20258.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19914.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19914.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19914.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.20259.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.20259.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.20259.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.18675.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.18675.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.18675.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19815.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19815.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19815.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19209.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19209.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19209.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.18545.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.18545.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.18545.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.18536.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.18536.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.18536.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.18601.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.18601.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.18601.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19949.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19949.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19949.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19788.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19788.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19788.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19752.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19752.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19752.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19439.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19439.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19439.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.20256.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.20256.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.20256.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.20152.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.20152.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.20152.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.18759.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.18759.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.18759.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19602.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19602.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19602.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19590.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19590.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19590.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.16972.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.16972.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.16972.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.13426.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.13426.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.13426.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19427.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19427.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19427.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.20254.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.20254.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.20254.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19640.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19640.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19640.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.17652.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.17652.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.17652.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.20278.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.20278.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.20278.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.10887.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.10887.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.10887.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19706.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19706.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19706.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19630.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19630.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19630.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.19443.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.19443.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.19443.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.18384.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.18384.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.18384.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.16312.
[27.05.2025 08:15] Extra JSON file exists (./assets/json/2505.16312.json), skip PDF parsing.
[27.05.2025 08:15] Paper image links file exists (./assets/img_data/2505.16312.json), skip HTML parsing.
[27.05.2025 08:15] Success.
[27.05.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2505.18773.
[27.05.2025 08:15] Downloading paper 2505.18773 from http://arxiv.org/pdf/2505.18773v1...
[27.05.2025 08:16] Extracting affiliations from text.
[27.05.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 3 7 7 8 1 . 5 0 5 2 : r Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models Jamie Hayes*1, Ilia Shumailov1, Christopher A. Choquette-Choo1, Matthew Jagielski1, George Kaissis1, Katherine Lee1, Milad Nasr1, Sahra Ghalebikesabi1, Niloofar Mireshghallah2, Meenatchi Sundaram Muthu Selva Annamalai3, Igor Shilov4, Matthieu Meeus4, Yves-Alexandre de Montjoye4, Franziska Boenisch5, Adam Dziedzic5 and A. Feder Cooper*6 1Google DeepMind, 2University of Washington, 3University College London, 4Imperial College London, 5CISPA Helmholtz Center for Information Security, 6Cornell University, *Equal contribution State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As result, prior research has either relied on weaker attacks that avoid training reference models (e.g., fine-tuning attacks), or on stronger attacks applied to small-scale models and datasets. However, weaker attacks have been shown to be brittleachieving close-to-arbitrary successand insights from strong attacks in simplified settings do not translate to todays LLMs. These challenges have prompted an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRAone of the strongest MIAsto GPT-2 architectures ranging from 10M to 1B parameters, training reference models on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their effectiveness, however, remains limited (e.g., AUC < 0.7) in practical settings; and, (3) the relationship between MIA success and related privacy metrics is not as straightforward as prior work has suggested. 1. Introduction In membership inference attack (MIA), an adversa"
[27.05.2025 08:16] Response: ```python
[
    "Google DeepMind",
    "University of Washington",
    "University College London",
    "Imperial College London",
    "CISPA Helmholtz Center for Information Security",
    "Cornell University"
]
```
[27.05.2025 08:16] Deleting PDF ./assets/pdf/2505.18773.pdf.
[27.05.2025 08:16] Success.
[27.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.15957.
[27.05.2025 08:16] Extra JSON file exists (./assets/json/2505.15957.json), skip PDF parsing.
[27.05.2025 08:16] Paper image links file exists (./assets/img_data/2505.15957.json), skip HTML parsing.
[27.05.2025 08:16] Success.
[27.05.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2505.18323.
[27.05.2025 08:16] Downloading paper 2505.18323 from http://arxiv.org/pdf/2505.18323v1...
[27.05.2025 08:17] Extracting affiliations from text.
[27.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 2 ] . [ 1 3 2 3 8 1 . 5 0 5 2 : r Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation Nicolas KÃ¼chler1,2,*, Ivan Petrov1, Conrad Grobler1 and Ilia Shumailov1 1Google DeepMind, 2ETH Zurich For nearly decade the academic community has investigated backdoors in neural networks, primarily focusing on classification tasks where adversaries manipulate the model prediction. While demonstrably malicious, the immediate real-world impact of such prediction-altering attacks has remained unclear. In this paper we introduce novel and significantly more potent class of backdoors that builds upon recent advancements in architectural backdoors. We demonstrate how these backdoors can be specifically engineered to exploit batched inference, common technique for hardware utilization, enabling largescale user data manipulation and theft. By targeting the batching process, these architectural backdoors facilitate information leakage between concurrent user requests and allow attackers to fully control model responses directed at other users within the same batch. In other words, an attacker who can change the model architecture can set and steal model inputs and outputs of other users within the same batch. We show that such attacks are not only feasible but also alarmingly effective, can be readily injected into prevalent model architectures, (e.g. Transformers), and represent truly malicious threat to user privacy and system integrity. Critically, to counteract this new class of vulnerabilities, we propose deterministic mitigation strategy that provides formal guarantees against this new attack vector, unlike prior work that relied on Large Language Models to find the backdoors. Our mitigation strategy employs novel Information Flow Control mechanism that analyzes the model graph and proves non-interference between different user inputs within the same batch. Using our mitigation strategy we perform large scale analysis of models hosted th"
[27.05.2025 08:17] Response: ```python
["Google DeepMind", "ETH Zurich"]
```
[27.05.2025 08:17] Deleting PDF ./assets/pdf/2505.18323.pdf.
[27.05.2025 08:17] Success.
[27.05.2025 08:17] Enriching papers with extra data.
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 0. The rapid advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on model-centric scaling through increasing parameter counts from millions to hundreds of billions to drive performance gains. However, as we approach hardware limits on model size, the dominan...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 1. BizFinBench is a benchmark for evaluating large language models in financial applications, revealing distinct performance patterns across various tasks.  					AI-generated summary 				 Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical do...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 2. MEMENTO evaluates personalized memory utilization in embodied agents, revealing limitations in understanding user semantics and routines.  					AI-generated summary 				 Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. H...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 3. Adaptive Reasoning Model (ARM) uses Ada-GRPO to reduce token usage and improve efficiency across different reasoning modes.  					AI-generated summary 				 While large reasoning models demonstrate strong performance on complex tasks, they lack the ability to adjust reasoning token usage based on tas...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 4. Enigmata is a comprehensive suite for improving LLMs in puzzle reasoning through scalable multi-task RL training, leading to better performance on benchmarks and advanced math tasks.  					AI-generated summary 				 Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at advance...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 5. A lifecycle safety alignment framework employs a Meta-Attacker and Defender to adapt LLMs to novel jailbreaking strategies, improving robustness in deployment.  					AI-generated summary 				 LLMs have made impressive progress, but their growing capabilities also expose them to highly flexible jailb...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 6. Multimodal large language models (MLLMs) have recently achieved significant progress in visual tasks, including semantic scene understanding and text-image alignment, with reasoning variants enhancing performance on complex tasks involving mathematics and logic. However, their capacity for reasoning...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 7. LLM reasoning is understood through a meta-learning framework, treating reasoning as pseudo-gradient descent and questions as individual tasks, which enhances generalization and provides practical insights for improvement.  					AI-generated summary 				 We propose a novel framework for comprehendin...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 8. A method is proposed to generate detailed scientific hypotheses using LLMs by defining and optimizing a latent reward landscape, outperforming baselines in benchmark evaluations.  					AI-generated summary 				 Large language models (LLMs) have shown promise in automating scientific hypothesis gener...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 9. Large language models (LLMs) often exhibit strong biases, e.g, against women or in favor of the number 7. We investigate whether LLMs would be able to output less biased answers when allowed to observe their prior answers to the same question in a multi-turn conversation. To understand which types o...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 10. Standing in 2025, at a critical juncture in the pursuit of Artificial General Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated significant potential in enhancing the reasoning capability of large language models (LLMs) and has led to the development of cutting-edge AI models such...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 11. Flex-Judge uses minimal textual reasoning data to generalize across multiple modalities and evaluation formats, outperforming state-of-the-art models in multimodal evaluations.  					AI-generated summary 				 Human-generated reward signals are critical for aligning generative models with human prefe...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 12. Influence functions are used to attribute LLMs' reasoning in math and coding to individual training elements, revealing cross-domain effects and enabling a reweighting strategy that improves model accuracy.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable rea...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 13. Multi-Turn Decomposition improves efficiency in large reasoning models by breaking down chain-of-thought into manageable turns, reducing token usage and latency while maintaining performance.  					AI-generated summary 				 Large Reasoning Models (LRMs) are criticized for the excessively lengthy Cha...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 14. A novel framework, Discrete Markov Bridge, is introduced for discrete data modeling with Matrix Learning and Score Learning components, demonstrating superior performance compared to existing methods on Text8 and CIFAR-10 datasets.  					AI-generated summary 				 Discrete diffusion has recently emer...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 15. Large Language Models have achieved remarkable success in natural language processing tasks, with Reinforcement Learning playing a key role in adapting them to specific applications. However, obtaining ground truth answers for training LLMs in mathematical problem-solving is often challenging, costl...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 16. An end-to-end reinforcement learning framework, Omni-R1, achieves superior performance in long-horizon video-audio reasoning and fine-grained pixel understanding tasks by combining global reasoning and detail understanding systems.  					AI-generated summary 				 Long-horizon video-audio reasoning a...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 17. A novel hard negative contrastive learning framework improves geometric reasoning in Large Multimodal Models, significantly enhancing their performance compared to existing models.  					AI-generated summary 				 Benefiting from contrastively trained visual encoders on large-scale natural scene imag...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 18. Data-centric distillation, including data augmentation, selection, and mixing, offers a promising path to creating smaller, more efficient student Large Language Models (LLMs) that retain strong reasoning abilities. However, there still lacks a comprehensive benchmark to systematically assess the ef...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 19. ScaleKV compresses the KV cache in Visual Autoregressive models by differentiating drafters and refiners across transformer layers, reducing memory consumption while maintaining high fidelity.  					AI-generated summary 				 Visual Autoregressive (VAR) modeling has garnered significant attention for...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 20. Intuitor, a Reinforcement Learning from Internal Feedback method, uses self-certainty as a reward signal to enable unsupervised learning of large language models, achieving performance comparable to GRPO on benchmarks and superior generalization.  					AI-generated summary 				 Training large langua...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 21. Recent advances in Automatic Speech Recognition (ASR) have been largely fueled by massive speech corpora. However, extending coverage to diverse languages with limited resources remains a formidable challenge. This paper introduces Speech Back-Translation, a scalable pipeline that improves multiling...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 22. VLM-Gym addresses the "knowing-doing" gap in Vision-Language Models by training them in a diverse RL environment, leading to enhanced perception and reasoning abilities that surpass existing models in interactive games.  					AI-generated summary 				 Vision-Language Models (VLMs) excel in many dire...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 23. WINA, a training-free sparse activation framework for large language models, improves inference accuracy by considering hidden state magnitudes and weight matrix norms, outperforming existing methods.  					AI-generated summary 				 The growing computational demands of large language models (LLMs) m...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 24. Prioritizing feature consistency in sparse autoencoders improves mechanistic interpretability of neural networks by ensuring reliable and interpretable features.  					AI-generated summary 				 Sparse Autoencoders (SAEs) are a prominent tool in mechanistic interpretability (MI) for decomposing neura...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 25. A reinforcement learning-guided training paradigm enhances large language models' reasoning efficiency and performance for multi-hop questions by interleaving thinking and answering.  					AI-generated summary 				 Long chain-of-thought (CoT) significantly enhances large language models' (LLM) reaso...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 26. CDAS addresses low sample efficiency in reinforcement learning by aligning model competence with problem difficulty, improving both accuracy and efficiency in mathematical benchmarks.  					AI-generated summary 				 Reinforcement learning exhibits potential in enhancing the reasoning abilities of la...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 27. Large language models excel at pattern matching, yet often fall short in systematic compositional generalization. We propose the coverage principle: a data-centric framework showing that models relying primarily on pattern matching for compositional tasks cannot reliably generalize beyond substituti...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 28. InfantAgent-Next is a multimodal agent that integrates tool-based and vision models in a modular architecture to solve various benchmarks, including OSWorld, GAIA, and SWE-Bench.  					AI-generated summary 				 This paper introduces InfantAgent-Next, a generalist agent capable of interacting with co...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 29. PathFinder-PRM, a hierarchical and error-aware Process Reward Model, improves mathematical problem-solving by fine-grained error classification and step correctness estimation, achieving state-of-the-art PRMScore with reduced data usage.  					AI-generated summary 				 Large Language Models (LLMs) a...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 30. Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Existing systems rely on a one-way information transmission mode where patients must fully desc...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 31. A review contrasts vibe coding and agentic coding paradigms, highlighting their differences in interaction, autonomy, and application areas in AI-assisted software development.  					AI-generated summary 				 This review presents a comprehensive analysis of two emerging paradigms in AI-assisted soft...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 32. Adversaries can significantly enhance foundation model capabilities in offensive cybersecurity with limited computational resources, underscoring the need for dynamic threat model assessments.  					AI-generated summary 				 Foundation models are increasingly becoming better autonomous programmers, ...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 33. EquivPruner reduces token consumption and improves reasoning accuracy by pruning semantically equivalent actions in LLM searches, leveraging a new dataset for mathematical equivalence.  					AI-generated summary 				 Large Language Models (LLMs) excel at complex reasoning through search algorithms, ...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 34. State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training reference models (e.g., f...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 35. A survey proposes a systematic taxonomy for evaluating large audio-language models across dimensions including auditory awareness, knowledge reasoning, dialogue ability, and fairness, to address fragmented benchmarks in the field.  					AI-generated summary 				 With advancements in large audio-lang...
[27.05.2025 08:17] ********************************************************************************
[27.05.2025 08:17] Abstract 36. For nearly a decade the academic community has investigated backdoors in neural networks, primarily focusing on classification tasks where adversaries manipulate the model prediction. While demonstrably malicious, the immediate real-world impact of such prediction-altering attacks has remained uncle...
[27.05.2025 08:17] Read previous papers.
[27.05.2025 08:17] Generating reviews via LLM API.
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#training", "#data", "#math", "#optimization", "#long_context", "#survey"], "emoji": "ðŸ—œï¸", "ru": {"title": "Ð¡Ð¶Ð°Ñ‚Ð¸Ðµ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²: Ð½Ð¾Ð²Ñ‹Ð¹ Ñ€ÑƒÐ±ÐµÐ¶ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð˜Ð˜", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ñ€Ð°ÑÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°ÐµÑ‚ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´ Ð¾Ñ‚ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ðº ÑÐ¶Ð°Ñ‚Ð¸ÑŽ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ Ñ€Ð°Ð·Ð²Ð¸Ñ‚Ð¸Ñ ÐºÑ€ÑƒÐ¿Ð½Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#benchmark", "#science", "#reasoning", "#dataset", "#open_source"], "emoji": "ðŸ’¹", "ru": {"title": "BizFinBench: ÐÐ¾Ð²Ñ‹Ð¹ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚ Ð¾Ñ†ÐµÐ½ÐºÐ¸ LLM Ð² Ñ„Ð¸Ð½Ð°Ð½ÑÐ°Ñ…", "desc": "BizFinBench - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ ÑÑ‚Ð°Ð»Ð¾Ð½Ð½Ñ‹Ð¹ Ñ‚ÐµÑÑ‚ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð² Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ñ‹Ñ… Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸ÑÑ…. ÐžÐ½ ÑÐ¾ÑÑ‚Ð¾Ð¸Ñ‚ Ð¸Ð· 67
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#agents", "#interpretability", "#multimodal", "#agi", "#reasoning"], "emoji": "ðŸ¤–", "ru": {"title": "ÐžÑ†ÐµÐ½ÐºÐ° Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ð˜Ð˜-Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð¾Ð²: Ð¿ÑƒÑ‚ÑŒ Ðº Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸", "desc": "MEMENTO - ÑÑ‚Ð¾ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð²Ð¾Ð¿Ð»Ð¾Ñ‰ÐµÐ½Ð½Ñ‹Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ð¸Ñ… ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð°Ð¼Ñ
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#reasoning", "#inference", "#optimization", "#training"], "emoji": "ðŸ§ ", "ru": {"title": "ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð´Ð»Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð² Ð˜Ð˜", "desc": "ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ (ARM) Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ada-GRPO Ð´Ð»Ñ ÑÐ½Ð¸Ð¶ÐµÐ½Ð¸Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð¸ Ð¿Ð¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ 
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#reasoning", "#math", "#training", "#rl", "#optimization", "#benchmark", "#dataset"], "emoji": "ðŸ§©", "ru": {"title": "Enigmata: Ð¿Ñ€Ð¾ÐºÐ°Ñ‡ÐºÐ° Ð»Ð¾Ð³Ð¸ÐºÐ¸ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ‡ÐµÑ€ÐµÐ· Ð³Ð¾Ð»Ð¾Ð²Ð¾Ð»Ð¾Ð¼ÐºÐ¸", "desc": "Enigmata - ÑÑ‚Ð¾ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ñ‹Ð¹ Ð½Ð°Ð±Ð¾Ñ€ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð½Ð°Ð²Ñ‹ÐºÐ¾Ð² Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð³Ð¾Ð»Ð¾Ð²Ð¾Ð»Ð¾Ð¼Ð¾Ðº Ñƒ Ð±Ð¾Ð»ÑŒ
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#inference", "#training", "#alignment", "#security"], "emoji": "ðŸ›¡ï¸", "ru": {"title": "ÐÐµÐ¿Ñ€ÐµÑ€Ñ‹Ð²Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð·Ð°Ñ‰Ð¸Ñ‚Ñ‹ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¾Ñ‚ Ð½Ð¾Ð²Ñ‹Ñ… Ð°Ñ‚Ð°Ðº", "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð½ÐµÐ¿Ñ€ÐµÑ€Ñ‹Ð²Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð¿Ð¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð² ÑƒÑÐ»Ð¾Ð²Ð¸ÑÑ… Ð½Ð¾Ð²Ñ‹Ñ… Ð°Ñ‚
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#cv", "#open_source"], "emoji": "ðŸ—ºï¸", "ru": {"title": "ReasonMap: Ð½Ð¾Ð²Ñ‹Ð¹ Ð²Ð·Ð³Ð»ÑÐ´ Ð½Ð° Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ðµ Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "ReasonMap - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ðº Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸ÑŽ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð¹
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#math", "#training"], "emoji": "ðŸ§ ", "ru": {"title": "Ð Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ LLM ÐºÐ°Ðº Ð¼ÐµÑ‚Ð°-Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ: Ð½Ð¾Ð²Ñ‹Ð¹ Ð²Ð·Ð³Ð»ÑÐ´ Ð½Ð° Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚", "desc": "Ð”Ð°Ð½Ð½Ð°Ñ ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸ÑŽ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÐµÐ¹ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ñ‡ÐµÑ€ÐµÐ·
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#science", "#rlhf", "#benchmark", "#multimodal", "#optimization"], "emoji": "ðŸ§ª", "ru": {"title": "Ð˜ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚ Ð½Ð° ÑÑ‚Ñ€Ð°Ð¶Ðµ Ð½Ð°ÑƒÑ‡Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ°: Ð¾Ñ‚ Ð¸Ð´ÐµÐ¸ Ðº ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñƒ", "desc": "ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑ‚Ð¾Ð´ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ñ… Ð³Ð¸Ð¿Ð¾Ñ‚ÐµÐ· Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LL
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#data", "#ethics", "#hallucinations", "#benchmark", "#rlhf"], "emoji": "ðŸ¤–", "ru": {"title": "Ð¡Ð°Ð¼Ð¾ÐºÐ¾Ñ€Ñ€ÐµÐºÑ†Ð¸Ñ Ð¿Ñ€ÐµÐ´Ð²Ð·ÑÑ‚Ð¾ÑÑ‚Ð¸ Ð² ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Ñ‡ÐµÑ€ÐµÐ· Ð¼Ð½Ð¾Ð³Ð¾ÑÑ‚Ð°Ð¿Ð½Ñ‹Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÑÐ²ÑÑ‰ÐµÐ½Ð¾ Ð¸Ð·ÑƒÑ‡ÐµÐ½Ð¸ÑŽ Ð¿Ñ€ÐµÐ´Ð²Ð·ÑÑ‚Ð¾ÑÑ‚Ð¸ Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ… (LLM) Ð¸ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ ÐµÐµ ÑƒÐ¼ÐµÐ½ÑŒÑˆÐµÐ½Ð¸Ñ Ð²
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#multimodal", "#training", "#reasoning", "#benchmark", "#rlhf", "#agi", "#rl"], "emoji": "ðŸ§ ", "ru": {"title": "RFT: ÐºÐ»ÑŽÑ‡ Ðº ÑƒÑÐ¸Ð»ÐµÐ½Ð¸ÑŽ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð² Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð˜Ð˜-Ð¼Ð¾Ð´ÐµÐ»ÑÑ…", "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ð¾ÑÐ²ÑÑ‰ÐµÐ½Ð° Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸ÑŽ Ð¼ÐµÑ‚Ð¾Ð´Ð° Ñ‚Ð¾Ð½ÐºÐ¾Ð¹ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ (RFT) Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#reasoning", "#rlhf", "#alignment", "#benchmark", "#multimodal", "#transfer_learning"], "emoji": "ðŸ§ ", "ru": {"title": "Ð Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ‚ÐµÐºÑÑ‚Ð° - ÐºÐ»ÑŽÑ‡ Ðº ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐµ", "desc": "Flex-Judge - ÑÑ‚Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð°Ñ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#training", "#reasoning", "#data", "#dataset", "#optimization", "#interpretability"], "emoji": "ðŸ§ ", "ru": {"title": "Ð¤ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð²Ð»Ð¸ÑÐ½Ð¸Ñ Ñ€Ð°ÑÐºÑ€Ñ‹Ð²Ð°ÑŽÑ‚ ÑÐµÐºÑ€ÐµÑ‚Ñ‹ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ LLM Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸ÐºÐµ Ð¸ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÑŽ", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð»Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð²Ð»Ð¸ÑÐ½Ð¸Ñ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð²ÐºÐ»Ð°Ð´Ð° Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÑÐ»ÐµÐ¼Ðµ
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#benchmark", "#optimization"], "emoji": "ðŸ§ ", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ Ð¿Ð¾ ÑˆÐ°Ð³Ð°Ð¼: MinD Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ LRM", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ Multi-Turn Decomposition (MinD) Ð´Ð»Ñ Ð¿Ð¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#training", "#data", "#benchmark", "#diffusion", "#dataset", "#optimization", "#architecture"], "emoji": "ðŸŒ‰", "ru": {"title": "Ð”Ð¸ÑÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ Ð¼Ð°Ñ€ÐºÐ¾Ð²ÑÐºÐ¸Ð¹ Ð¼Ð¾ÑÑ‚: Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÑŽ Ð´Ð¸ÑÐºÑ€ÐµÑ‚Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…", "desc": "ÐŸÑ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½ Ð½Ð¾Ð²Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Discrete Markov Bridge Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#reasoning", "#math", "#rl", "#optimization", "#training"], "emoji": "ðŸ§®", "ru": {"title": "ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð˜Ð˜ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸ÐºÐµ Ð±ÐµÐ· Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ñ… Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð²", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÑÐ²ÑÑ‰ÐµÐ½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÑŽ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð° Ð¸ Ð´Ð»Ð¸Ð½Ñ‹ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² Ð² ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ðµ Ð²ÑÐ¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÑÐ¸Ð³Ð½Ð°Ð»Ð¾Ð² Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ 
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#video", "#hallucinations", "#benchmark", "#multimodal", "#optimization"], "emoji": "ðŸ¤–", "ru": {"title": "Ð£Ð¼Ð½Ð¾Ðµ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ñ‚Ñ€ÑƒÐ´Ð°: Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ Ð¸ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ð² Ð¾Ð´Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸", "desc": "Omni-R1 - ÑÑ‚Ð¾ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ Ð·
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#training", "#reasoning", "#dataset", "#open_source"], "emoji": "ðŸ“", "ru": {"title": "ÐŸÑ€Ð¾Ñ€Ñ‹Ð² Ð² Ð³ÐµÐ¾Ð¼ÐµÑ‚Ñ€Ð¸Ñ‡ÐµÑÐºÐ¾Ð¼ Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ð¸ Ð˜Ð˜ Ñ‡ÐµÑ€ÐµÐ· ÑƒÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½ÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ ÐºÐ¾Ð½Ñ‚Ñ€Ð°ÑÑ‚Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ ÐºÐ¾Ð½Ñ‚Ñ€Ð°ÑÑ‚Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ ÑÐ»Ð¾
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#transfer_learning", "#dataset", "#optimization", "#training"], "emoji": "ðŸ§ ", "ru": {"title": "ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸Ð¸ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ‡ÐµÑ€ÐµÐ· Ð¼Ð°Ð½Ð¸Ð¿ÑƒÐ»ÑÑ†Ð¸Ð¸ Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸", "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ DC-CoT - Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð´Ð°Ñ‚Ð°-Ñ†ÐµÐ½Ñ‚Ñ€Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¼ÐµÑ‚Ð¾
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#inference", "#optimization", "#architecture"], "emoji": "ðŸ—œï¸", "ru": {"title": "ScaleKV: Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ ÑÐ¶Ð°Ñ‚Ð¸Ðµ ÐºÑÑˆÐ° Ð´Ð»Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð˜Ð˜-Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "ScaleKV - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ ÑÐ¶Ð°Ñ‚Ð¸Ñ KV-ÐºÑÑˆÐ° Ð´Ð»Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð°Ð²Ñ‚Ð¾Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. ÐžÐ½ Ñ€Ð°Ð·Ð´ÐµÐ»ÑÐµÑ‚ ÑÐ»Ð¾Ð¸ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼ÐµÑ€Ð° Ð½Ð° Ð´Ñ€Ð°Ñ„Ñ‚ÐµÑ€Ñ‹ Ð¸ Ñ€ÐµÑ„Ð°Ð¹
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#rlhf", "#optimization"], "emoji": "ðŸ§ ", "ru": {"title": "Ð¡Ð°Ð¼Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð˜Ð˜: Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½ÑÑ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ ÐºÐ°Ðº Ð´Ð²Ð¸Ð³Ð°Ñ‚ÐµÐ»ÑŒ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ°", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½ÐµÐ¹ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ²ÑÐ·Ð¸ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ Intuitor. Ð­Ñ‚Ð¾Ñ‚ Ð¼
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#data", "#multilingual", "#low_resource", "#dataset", "#audio", "#synthetic"], "emoji": "ðŸ—£ï¸", "ru": {"title": "Ð¡Ð¸Ð½Ñ‚ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ñ€ÐµÑ‡ÑŒ Ð¾Ñ‚ÐºÑ€Ñ‹Ð²Ð°ÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ðµ Ð³Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚Ñ‹ Ð´Ð»Ñ Ð¼Ð½Ð¾Ð³Ð¾ÑÐ·Ñ‹Ñ‡Ð½Ð¾Ð³Ð¾ ASR", "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´ Speech Back-Translation Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¼Ð½Ð¾Ð³Ð¾ÑÐ·Ñ‹Ñ‡Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼ Ð°Ð²Ñ‚Ð¾Ð¼Ð°
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#multimodal", "#training", "#reasoning", "#open_source", "#agents", "#games", "#optimization", "#rl"], "emoji": "ðŸŽ®", "ru": {"title": "ÐŸÑ€ÐµÐ¾Ð´Ð¾Ð»ÐµÐ½Ð¸Ðµ Ñ€Ð°Ð·Ñ€Ñ‹Ð²Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ð·Ð½Ð°Ð½Ð¸ÐµÐ¼ Ð¸ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸ÐµÐ¼ Ð² Vision-Language Models Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ RL", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ VLM-Gym - ÑÑ€ÐµÐ´Ñƒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#inference", "#training", "#open_source", "#optimization", "#architecture"], "emoji": "ðŸš€", "ru": {"title": "WINA: Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ñ€Ð°Ð·Ñ€ÐµÐ¶ÐµÐ½Ð½Ð°Ñ Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "WINA - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ñ€Ð°Ð·Ñ€ÐµÐ¶ÐµÐ½Ð½Ð¾Ð¹ Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‰Ð¸Ð¹ Ð´Ð¾Ð¿Ð¾Ð»
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#interpretability", "#training", "#math", "#architecture"], "emoji": "ðŸ”", "ru": {"title": "Ð¡Ñ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² - ÐºÐ»ÑŽÑ‡ Ðº Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾Ð¹ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ð¸ Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÐµÐ¹", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ñ€Ð°ÑÑÐ¼Ð°Ñ‚Ñ€Ð¸Ð²Ð°ÐµÑ‚ Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² Ð² Ñ€Ð°Ð·Ñ€ÐµÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð°Ð²Ñ‚Ð¾ÑÐ½ÐºÐ¾Ð´ÐµÑ€Ð°Ñ… Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¼ÐµÑ…Ð°Ð½Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð¸Ð½Ñ‚Ðµ
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#reasoning", "#math", "#rlhf", "#training", "#rl", "#optimization"], "emoji": "ðŸ§ ", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ðµ Ð˜Ð˜: Ð¼Ñ‹ÑÐ»ÑŒ Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚ Ð² Ð¾Ð´Ð½Ð¾Ð¼ Ð¿Ð¾Ñ‚Ð¾ÐºÐµ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÑŽ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð´Ð»Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð¼Ð½Ð¾Ð³Ð¾ÑÑ‚Ð°Ð¿Ð½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#math", "#optimization"], "emoji": "ðŸŽ¯", "ru": {"title": "Ð¢Ð¾Ñ‡Ð½Ð¾Ðµ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ: CDAS Ð¿Ð¾Ð²Ñ‹ÑˆÐ°ÐµÑ‚ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "CDAS (Competence-Difficulty Alignment Sampling) - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð² Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼, Ð½Ð°Ð¿Ñ€Ð°
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#interpretability", "#training", "#reasoning", "#data", "#architecture"], "emoji": "ðŸ§ ", "ru": {"title": "ÐŸÑ€Ð¸Ð½Ñ†Ð¸Ð¿ Ð¿Ð¾ÐºÑ€Ñ‹Ñ‚Ð¸Ñ: Ð½Ð¾Ð²Ñ‹Ð¹ Ð²Ð·Ð³Ð»ÑÐ´ Ð½Ð° ÐºÐ¾Ð¼Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ðµ Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ðµ Ð² Ð˜Ð˜", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ ÐºÐ¾Ð¼Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ð»Ð¸Ð·
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#agi", "#open_source", "#benchmark", "#agents", "#multimodal", "#architecture"], "emoji": "ðŸ¤–", "ru": {"title": "Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð°Ð³ÐµÐ½Ñ‚ Ð´Ð»Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡", "desc": "InfantAgent-Next - ÑÑ‚Ð¾ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð°Ð³ÐµÐ½Ñ‚, Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ð¹ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¸ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼Ð¾
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#reasoning", "#training", "#math", "#hallucinations", "#dataset", "#optimization"], "emoji": "ðŸ§®", "ru": {"title": "Ð¢Ð¾Ñ‡Ð½Ð°Ñ Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ñ Ð² Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸ÑÑ… Ñ PathFinder-PRM", "desc": "PathFinder-PRM - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ð°Ñ Ð¸ÐµÑ€Ð°Ñ€Ñ…Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°, ÑƒÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÑŽÑ‰Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ¸, 
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#training", "#reasoning", "#science", "#healthcare", "#dataset", "#games", "#optimization", "#rl"], "emoji": "ðŸ©º", "ru": {"title": "Ð£Ð¼Ð½Ñ‹Ð¹ Ð²Ð¸Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð´Ð¾ÐºÑ‚Ð¾Ñ€: Ð˜Ð˜ ÑƒÑ‡Ð¸Ñ‚ÑÑ Ð²ÐµÑÑ‚Ð¸ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ñ Ð¿Ð°Ñ†Ð¸ÐµÐ½Ñ‚Ð¾Ð¼", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ DoctorAgent-RL - ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ 
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#survey", "#agents", "#architecture", "#interpretability"], "emoji": "ðŸ¤–", "ru": {"title": "Ð’Ð°Ð¹Ð± vs ÐÐ³ÐµÐ½Ñ‚: ÐÐ¾Ð²Ñ‹Ðµ Ð³Ð¾Ñ€Ð¸Ð·Ð¾Ð½Ñ‚Ñ‹ Ð˜Ð˜-Ð°ÑÑÐ¸ÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸", "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÑ€Ð°Ð²Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ð´Ð²ÑƒÑ… Ð¿Ð°Ñ€Ð°Ð´Ð¸Ð³Ð¼ Ð² Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÐµÑÐ¿ÐµÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð˜Ð˜: Ð²Ð°Ð¹Ð±-ÐºÐ¾Ð´Ð¸
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#cybersecurity", "#agents", "#security"], "emoji": "ðŸ›¡ï¸", "ru": {"title": "Ð”Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ° ÑƒÐ³Ñ€Ð¾Ð· Ð˜Ð˜ Ð² ÐºÐ¸Ð±ÐµÑ€Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ Ð·Ð»Ð¾ÑƒÐ¼Ñ‹ÑˆÐ»ÐµÐ½Ð½Ð¸ÐºÐ¸ Ð¼Ð¾Ð³ÑƒÑ‚ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐ¸Ñ‚ÑŒ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ñ„ÑƒÐ½Ð´Ð°Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ Ð½Ð°ÑÑ‚ÑƒÐ¿Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ ÐºÐ¸Ð±ÐµÑ€Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#reasoning", "#data", "#dataset", "#optimization", "#training"], "emoji": "âœ‚ï¸", "ru": {"title": "EquivPruner: ÑƒÐ¼Ð½Ð¾Ðµ ÑÐ¾ÐºÑ€Ð°Ñ‰ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ñ… LLM", "desc": "EquivPruner - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð¿Ñ€Ð¸ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¸ Ð·Ð°Ð´Ð°Ñ‡, Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‰Ð¸Ñ… Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹. 
[27.05.2025 08:17] Querying the API.
[27.05.2025 08:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training reference models (e.g., fine-tuning attacks), or on stronger attacks applied to small-scale models and datasets. However, weaker attacks have been shown to be brittle - achieving close-to-arbitrary success - and insights from strong attacks in simplified settings do not translate to today's LLMs. These challenges have prompted an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures ranging from 10M to 1B parameters, training reference models on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their effectiveness, however, remains limited (e.g., AUC<0.7) in practical settings; and, (3) the relationship between MIA success and related privacy metrics is not as straightforward as prior work has suggested.
[27.05.2025 08:17] Response: {
  "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð»Ð¸ ÑÐ¸Ð»ÑŒÐ½ÑƒÑŽ Ð°Ñ‚Ð°ÐºÑƒ Ð²Ñ‹Ð²Ð¾Ð´Ð° Ñ‡Ð»ÐµÐ½ÑÑ‚Ð²Ð° (MIA) Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ LiRA Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ GPT-2 Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ñ€Ð°Ð·Ð¼ÐµÑ€Ð¾Ð². ÐžÐ½Ð¸ Ð¾Ð±ÑƒÑ‡Ð¸Ð»Ð¸ ÑÑ‚Ð°Ð»Ð¾Ð½Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° Ð±Ð¾Ð»ÐµÐµ Ñ‡ÐµÐ¼ 20 Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð°Ñ… Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð¸Ð· Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð° C4. Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ ÑÐ¸Ð»ÑŒÐ½Ñ‹Ðµ MIA Ð¼Ð¾Ð³ÑƒÑ‚ Ð±Ñ‹Ñ‚ÑŒ ÑƒÑÐ¿ÐµÑˆÐ½Ñ‹Ð¼Ð¸ Ð½Ð° Ð¿Ñ€ÐµÐ´Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ… (LLM), Ð½Ð¾ Ð¸Ñ… ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð¾ÑÑ‚Ð°ÐµÑ‚ÑÑ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð½Ð¾Ð¹ Ð² Ð¿Ñ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑÑ†ÐµÐ½Ð°Ñ€Ð¸ÑÑ…. Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ‚Ð°ÐºÐ¶Ðµ Ð²Ñ‹ÑÐ²Ð¸Ð»Ð¾, Ñ‡Ñ‚Ð¾ ÑÐ²ÑÐ·ÑŒ Ð¼ÐµÐ¶Ð´Ñƒ ÑƒÑÐ¿ÐµÑ…Ð¾Ð¼ MIA Ð¸ Ð´Ñ€ÑƒÐ³Ð¸Ð¼Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸ ÐºÐ¾Ð½Ñ„Ð¸Ð´ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð½Ðµ Ñ‚Ð°Ðº Ð¾Ð´Ð½Ð¾Ð·Ð½Ð°Ñ‡Ð½Ð°, ÐºÐ°Ðº Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ð»Ð°Ð³Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°Ð½ÐµÐµ.",
  "emoji": "ðŸ•µï¸",
  "title": "ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð°Ñ‚Ð°Ðº Ð²Ñ‹Ð²Ð¾Ð´Ð° Ñ‡Ð»ÐµÐ½ÑÑ‚Ð²Ð° Ð½Ð° Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸: Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð¸ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ"
}
[27.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training reference models (e.g., fine-tuning attacks), or on stronger attacks applied to small-scale models and datasets. However, weaker attacks have been shown to be brittle - achieving close-to-arbitrary success - and insights from strong attacks in simplified settings do not translate to today's LLMs. These challenges have prompted an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures ranging from 10M to 1B parameters, training reference models on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their effectiveness, however, remains limited (e.g., AUC<0.7) in practical settings; and, (3) the relationship between MIA success and related privacy metrics is not as straightforward as prior work has suggested."

[27.05.2025 08:17] Response: ```python
["DATASET", "DATA", "BENCHMARK"]
```
[27.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training reference models (e.g., fine-tuning attacks), or on stronger attacks applied to small-scale models and datasets. However, weaker attacks have been shown to be brittle - achieving close-to-arbitrary success - and insights from strong attacks in simplified settings do not translate to today's LLMs. These challenges have prompted an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures ranging from 10M to 1B parameters, training reference models on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their effectiveness, however, remains limited (e.g., AUC<0.7) in practical settings; and, (3) the relationship between MIA success and related privacy metrics is not as straightforward as prior work has suggested."

[27.05.2025 08:17] Response: ```python
['SECURITY', 'LEAKAGE']
```
[27.05.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates the effectiveness of membership inference attacks (MIAs) on large pre-trained language models (LLMs) like GPT-2. It highlights the challenges of scaling MIAs due to the need for training multiple reference models, which has limited previous research. The authors scale the LiRA attack to various GPT-2 architectures and find that while strong MIAs can be effective, their success rates are still relatively low in practical applications. Additionally, the study reveals that the relationship between MIA success and privacy metrics is more complex than previously thought.","title":"Scaling Membership Inference Attacks on Large Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates the effectiveness of membership inference attacks (MIAs) on large pre-trained language models (LLMs) like GPT-2. It highlights the challenges of scaling MIAs due to the need for training multiple reference models, which has limited previous research. The authors scale the LiRA attack to various GPT-2 architectures and find that while strong MIAs can be effective, their success rates are still relatively low in practical applications. Additionally, the study reveals that the relationship between MIA success and privacy metrics is more complex than previously thought.', title='Scaling Membership Inference Attacks on Large Language Models'))
[27.05.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬ç ”ç©¶æŽ¢è®¨äº†ä¼šå‘˜æŽ¨æ–­æ”»å‡»ï¼ˆMIAï¼‰åœ¨å¤§åž‹é¢„è®­ç»ƒè¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰ä¸Šçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æ‰©å±•äº†LiRAæ”»å‡»æ–¹æ³•ï¼Œåº”ç”¨äºŽä¸åŒè§„æ¨¡çš„GPT-2æ¨¡åž‹ï¼Œå¹¶åœ¨è¶…è¿‡200äº¿ä¸ªC4æ•°æ®é›†çš„æ ‡è®°ä¸Šè®­ç»ƒå‚è€ƒæ¨¡åž‹ã€‚ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼Œå°½ç®¡å¼ºå¤§çš„MIAå¯ä»¥åœ¨é¢„è®­ç»ƒçš„LLMä¸Šå–å¾—æˆåŠŸï¼Œä½†å…¶æœ‰æ•ˆæ€§åœ¨å®žé™…åº”ç”¨ä¸­ä»ç„¶æœ‰é™ï¼ˆä¾‹å¦‚ï¼ŒAUC<0.7ï¼‰ã€‚æ­¤å¤–ï¼ŒMIAæˆåŠŸä¸Žéšç§æŒ‡æ ‡ä¹‹é—´çš„å…³ç³»æ¯”ä¹‹å‰çš„ç ”ç©¶æ‰€æš—ç¤ºçš„è¦å¤æ‚å¾—å¤šã€‚","title":"æ­ç¤ºå¤§åž‹è¯­è¨€æ¨¡åž‹ä¸­çš„ä¼šå‘˜æŽ¨æ–­æ”»å‡»æŒ‘æˆ˜"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬ç ”ç©¶æŽ¢è®¨äº†ä¼šå‘˜æŽ¨æ–­æ”»å‡»ï¼ˆMIAï¼‰åœ¨å¤§åž‹é¢„è®­ç»ƒè¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰ä¸Šçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æ‰©å±•äº†LiRAæ”»å‡»æ–¹æ³•ï¼Œåº”ç”¨äºŽä¸åŒè§„æ¨¡çš„GPT-2æ¨¡åž‹ï¼Œå¹¶åœ¨è¶…è¿‡200äº¿ä¸ªC4æ•°æ®é›†çš„æ ‡è®°ä¸Šè®­ç»ƒå‚è€ƒæ¨¡åž‹ã€‚ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼Œå°½ç®¡å¼ºå¤§çš„MIAå¯ä»¥åœ¨é¢„è®­ç»ƒçš„LLMä¸Šå–å¾—æˆåŠŸï¼Œä½†å…¶æœ‰æ•ˆæ€§åœ¨å®žé™…åº”ç”¨ä¸­ä»ç„¶æœ‰é™ï¼ˆä¾‹å¦‚ï¼ŒAUC<0.7ï¼‰ã€‚æ­¤å¤–ï¼ŒMIAæˆåŠŸä¸Žéšç§æŒ‡æ ‡ä¹‹é—´çš„å…³ç³»æ¯”ä¹‹å‰çš„ç ”ç©¶æ‰€æš—ç¤ºçš„è¦å¤æ‚å¾—å¤šã€‚', title='æ­ç¤ºå¤§åž‹è¯­è¨€æ¨¡åž‹ä¸­çš„ä¼šå‘˜æŽ¨æ–­æ”»å‡»æŒ‘æˆ˜'))
[27.05.2025 08:17] Using data from previous issue: {"categories": ["#ethics", "#reasoning", "#audio", "#benchmark", "#multimodal", "#survey"], "emoji": "ðŸŽ§", "ru": {"title": "Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð¾Ñ†ÐµÐ½ÐºÐµ Ð°ÑƒÐ´Ð¸Ð¾-ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð˜Ð˜-Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ñ‚Ð°ÐºÑÐ¾Ð½Ð¾Ð¼Ð¸ÑŽ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð°ÑƒÐ´Ð¸Ð¾-ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LALM). ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð²Ñ‹
[27.05.2025 08:17] Querying the API.
[27.05.2025 08:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

For nearly a decade the academic community has investigated backdoors in neural networks, primarily focusing on classification tasks where adversaries manipulate the model prediction. While demonstrably malicious, the immediate real-world impact of such prediction-altering attacks has remained unclear. In this paper we introduce a novel and significantly more potent class of backdoors that builds upon recent advancements in architectural backdoors. We demonstrate how these backdoors can be specifically engineered to exploit batched inference, a common technique for hardware utilization, enabling large-scale user data manipulation and theft. By targeting the batching process, these architectural backdoors facilitate information leakage between concurrent user requests and allow attackers to fully control model responses directed at other users within the same batch. In other words, an attacker who can change the model architecture can set and steal model inputs and outputs of other users within the same batch. We show that such attacks are not only feasible but also alarmingly effective, can be readily injected into prevalent model architectures, and represent a truly malicious threat to user privacy and system integrity. Critically, to counteract this new class of vulnerabilities, we propose a deterministic mitigation strategy that provides formal guarantees against this new attack vector, unlike prior work that relied on Large Language Models to find the backdoors. Our mitigation strategy employs a novel Information Flow Control mechanism that analyzes the model graph and proves non-interference between different user inputs within the same batch. Using our mitigation strategy we perform a large scale analysis of models hosted through Hugging Face and find over 200 models that introduce (unintended) information leakage between batch entries due to the use of dynamic quantization.
[27.05.2025 08:17] Response: {
  "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ Ð±ÑÐºÐ´Ð¾Ñ€Ð¾Ð² Ð² Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ñ… ÑÐµÑ‚ÑÑ…, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ñ… Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð½Ñ‹Ðµ ÑƒÑÐ·Ð²Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ð´Ð»Ñ ÑÐºÑÐ¿Ð»ÑƒÐ°Ñ‚Ð°Ñ†Ð¸Ð¸ Ð¿Ð°ÐºÐµÑ‚Ð½Ð¾Ð³Ð¾ Ð²Ñ‹Ð²Ð¾Ð´Ð°. Ð­Ñ‚Ð¸ Ð±ÑÐºÐ´Ð¾Ñ€Ñ‹ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÑŽÑ‚ Ð·Ð»Ð¾ÑƒÐ¼Ñ‹ÑˆÐ»ÐµÐ½Ð½Ð¸ÐºÐ°Ð¼ Ð¼Ð°Ð½Ð¸Ð¿ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹ Ð¸ ÐºÑ€Ð°ÑÑ‚ÑŒ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð² Ñ€Ð°Ð¼ÐºÐ°Ñ… Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¿Ð°ÐºÐµÑ‚Ð° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð². ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÑŽÑ‚ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ñ‚Ð°ÐºÐ¸Ñ… Ð°Ñ‚Ð°Ðº Ð¸ Ð¸Ñ… Ð¿Ñ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ðº Ñ€Ð°ÑÐ¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ð¼ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð°Ð¼ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. Ð”Ð»Ñ Ð¿Ñ€Ð¾Ñ‚Ð¸Ð²Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ ÑÑ‚Ð¸Ð¼ ÑƒÑÐ·Ð²Ð¸Ð¼Ð¾ÑÑ‚ÑÐ¼ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ Ð´ÐµÑ‚ÐµÑ€Ð¼Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ ÑÐ¼ÑÐ³Ñ‡ÐµÐ½Ð¸Ñ, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð½Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ðµ Ð³Ñ€Ð°Ñ„Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ Ð´Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ðµ Ð½ÐµÐ²Ð¼ÐµÑˆÐ°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹ Ð² Ð¿Ð°ÐºÐµÑ‚Ðµ.",
  "emoji": "ðŸ•µï¸",
  "title": "ÐÐ¾Ð²Ñ‹Ðµ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð½Ñ‹Ðµ Ð±ÑÐºÐ´Ð¾Ñ€Ñ‹: ÑÐºÑ€Ñ‹Ñ‚Ð°Ñ ÑƒÐ³Ñ€Ð¾Ð·Ð° Ð¿Ð°ÐºÐµÑ‚Ð½Ð¾Ð¼Ñƒ Ð²Ñ‹Ð²Ð¾Ð´Ñƒ Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÐµÐ¹"
}
[27.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"For nearly a decade the academic community has investigated backdoors in neural networks, primarily focusing on classification tasks where adversaries manipulate the model prediction. While demonstrably malicious, the immediate real-world impact of such prediction-altering attacks has remained unclear. In this paper we introduce a novel and significantly more potent class of backdoors that builds upon recent advancements in architectural backdoors. We demonstrate how these backdoors can be specifically engineered to exploit batched inference, a common technique for hardware utilization, enabling large-scale user data manipulation and theft. By targeting the batching process, these architectural backdoors facilitate information leakage between concurrent user requests and allow attackers to fully control model responses directed at other users within the same batch. In other words, an attacker who can change the model architecture can set and steal model inputs and outputs of other users within the same batch. We show that such attacks are not only feasible but also alarmingly effective, can be readily injected into prevalent model architectures, and represent a truly malicious threat to user privacy and system integrity. Critically, to counteract this new class of vulnerabilities, we propose a deterministic mitigation strategy that provides formal guarantees against this new attack vector, unlike prior work that relied on Large Language Models to find the backdoors. Our mitigation strategy employs a novel Information Flow Control mechanism that analyzes the model graph and proves non-interference between different user inputs within the same batch. Using our mitigation strategy we perform a large scale analysis of models hosted through Hugging Face and find over 200 models that introduce (unintended) information leakage between batch entries due to the use of dynamic quantization."

[27.05.2025 08:17] Response: ```python
["ARCHITECTURE", "INFERENCE"]
```
[27.05.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"For nearly a decade the academic community has investigated backdoors in neural networks, primarily focusing on classification tasks where adversaries manipulate the model prediction. While demonstrably malicious, the immediate real-world impact of such prediction-altering attacks has remained unclear. In this paper we introduce a novel and significantly more potent class of backdoors that builds upon recent advancements in architectural backdoors. We demonstrate how these backdoors can be specifically engineered to exploit batched inference, a common technique for hardware utilization, enabling large-scale user data manipulation and theft. By targeting the batching process, these architectural backdoors facilitate information leakage between concurrent user requests and allow attackers to fully control model responses directed at other users within the same batch. In other words, an attacker who can change the model architecture can set and steal model inputs and outputs of other users within the same batch. We show that such attacks are not only feasible but also alarmingly effective, can be readily injected into prevalent model architectures, and represent a truly malicious threat to user privacy and system integrity. Critically, to counteract this new class of vulnerabilities, we propose a deterministic mitigation strategy that provides formal guarantees against this new attack vector, unlike prior work that relied on Large Language Models to find the backdoors. Our mitigation strategy employs a novel Information Flow Control mechanism that analyzes the model graph and proves non-interference between different user inputs within the same batch. Using our mitigation strategy we perform a large scale analysis of models hosted through Hugging Face and find over 200 models that introduce (unintended) information leakage between batch entries due to the use of dynamic quantization."

[27.05.2025 08:17] Response: ```python
['SECURITY', 'LEAKAGE']
```
[27.05.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores a new type of backdoor attack in neural networks that targets the batching process during inference, allowing attackers to manipulate and steal user data. Unlike traditional backdoor attacks that focus on altering model predictions, this method exploits architectural vulnerabilities to leak information between concurrent user requests. The authors demonstrate that these attacks can be easily integrated into existing model architectures, posing a significant threat to user privacy and system integrity. To combat this issue, they propose a deterministic mitigation strategy using Information Flow Control to ensure that user inputs remain isolated within the same batch, providing formal guarantees against such attacks.","title":"Exposing and Mitigating Architectural Backdoors in Neural Network Batching"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores a new type of backdoor attack in neural networks that targets the batching process during inference, allowing attackers to manipulate and steal user data. Unlike traditional backdoor attacks that focus on altering model predictions, this method exploits architectural vulnerabilities to leak information between concurrent user requests. The authors demonstrate that these attacks can be easily integrated into existing model architectures, posing a significant threat to user privacy and system integrity. To combat this issue, they propose a deterministic mitigation strategy using Information Flow Control to ensure that user inputs remain isolated within the same batch, providing formal guarantees against such attacks.', title='Exposing and Mitigating Architectural Backdoors in Neural Network Batching'))
[27.05.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬è®ºæ–‡æŽ¢è®¨äº†ä¸€ç§æ–°åž‹çš„ç¥žç»ç½‘ç»œåŽé—¨æ”»å‡»ï¼Œç‰¹åˆ«é’ˆå¯¹æ‰¹é‡æŽ¨ç†æŠ€æœ¯ã€‚è¿™ç§æ”»å‡»å¯ä»¥åœ¨åŒä¸€æ‰¹æ¬¡ä¸­æ“æŽ§ç”¨æˆ·æ•°æ®ï¼Œå¯¼è‡´ä¿¡æ¯æ³„éœ²å’Œæ¨¡åž‹å“åº”çš„æŽ§åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¡®å®šæ€§çš„ç¼“è§£ç­–ç•¥ï¼Œé€šè¿‡ä¿¡æ¯æµæŽ§åˆ¶æœºåˆ¶ï¼Œç¡®ä¿ä¸åŒç”¨æˆ·è¾“å…¥ä¹‹é—´çš„éžå¹²æ‰°æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜Žï¼Œè¿™ç§æ”»å‡»ä¸ä»…å¯è¡Œä¸”æœ‰æ•ˆï¼Œå¹¶ä¸”åœ¨çŽ°æœ‰æ¨¡åž‹æž¶æž„ä¸­æ™®éå­˜åœ¨ã€‚","title":"æ­ç¤ºç¥žç»ç½‘ç»œä¸­çš„æ–°åž‹åŽé—¨æ”»å‡»ä¸Žé˜²æŠ¤ç­–ç•¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬è®ºæ–‡æŽ¢è®¨äº†ä¸€ç§æ–°åž‹çš„ç¥žç»ç½‘ç»œåŽé—¨æ”»å‡»ï¼Œç‰¹åˆ«é’ˆå¯¹æ‰¹é‡æŽ¨ç†æŠ€æœ¯ã€‚è¿™ç§æ”»å‡»å¯ä»¥åœ¨åŒä¸€æ‰¹æ¬¡ä¸­æ“æŽ§ç”¨æˆ·æ•°æ®ï¼Œå¯¼è‡´ä¿¡æ¯æ³„éœ²å’Œæ¨¡åž‹å“åº”çš„æŽ§åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¡®å®šæ€§çš„ç¼“è§£ç­–ç•¥ï¼Œé€šè¿‡ä¿¡æ¯æµæŽ§åˆ¶æœºåˆ¶ï¼Œç¡®ä¿ä¸åŒç”¨æˆ·è¾“å…¥ä¹‹é—´çš„éžå¹²æ‰°æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜Žï¼Œè¿™ç§æ”»å‡»ä¸ä»…å¯è¡Œä¸”æœ‰æ•ˆï¼Œå¹¶ä¸”åœ¨çŽ°æœ‰æ¨¡åž‹æž¶æž„ä¸­æ™®éå­˜åœ¨ã€‚', title='æ­ç¤ºç¥žç»ç½‘ç»œä¸­çš„æ–°åž‹åŽé—¨æ”»å‡»ä¸Žé˜²æŠ¤ç­–ç•¥'))
[27.05.2025 08:17] Loading Chinese text from previous data.
[27.05.2025 08:17] Renaming data file.
[27.05.2025 08:17] Renaming previous data. hf_papers.json to ./d/2025-05-27.json
[27.05.2025 08:17] Saving new data file.
[27.05.2025 08:17] Generating page.
[27.05.2025 08:17] Renaming previous page.
[27.05.2025 08:17] Renaming previous data. index.html to ./d/2025-05-27.html
[27.05.2025 08:17] [Experimental] Generating Chinese page for reading.
[27.05.2025 08:17] Chinese vocab [{'word': 'è¿ç§»å­¦ä¹ ', 'pinyin': 'qiÄn yÃ­ xuÃ© xÃ­', 'trans': 'transfer learning'}, {'word': 'åˆ†ç±»ä»»åŠ¡', 'pinyin': 'fÄ“n lÃ¨i rÃ¨n wÃ¹', 'trans': 'classification task'}, {'word': 'å‚æ•°', 'pinyin': 'cÄn shÇ”', 'trans': 'parameter'}, {'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹n liÃ n', 'trans': 'pre-training'}, {'word': 'æ–‡æœ¬ç¼–ç å™¨', 'pinyin': 'wÃ©n bÄ›n biÄn mÇŽ qÃ¬', 'trans': 'text encoder'}, {'word': 'ç›®æ ‡ä»¤ç‰Œ', 'pinyin': 'mÃ¹ biÄo lÃ¬ng pÃ¡i', 'trans': 'target token'}, {'word': 'åµŒå…¥', 'pinyin': 'qiÃ n rÃ¹', 'trans': 'embedding'}, {'word': 'è¡¨çŽ°å‡ºè‰²', 'pinyin': 'biÇŽo xiÃ n chÅ« sÃ¨', 'trans': 'perform excellently'}, {'word': 'æ‰©å±•è§„å¾‹', 'pinyin': 'kuÃ² zhÇŽn guÄ« lÇœ', 'trans': 'expansion pattern'}]
[27.05.2025 08:17] Renaming previous Chinese page.
[27.05.2025 08:17] Renaming previous data. zh.html to ./d/2025-05-26_zh_reading_task.html
[27.05.2025 08:17] Writing Chinese reading task.
[27.05.2025 08:17] Writing result.
[27.05.2025 08:17] Renaming log file.
[27.05.2025 08:17] Renaming previous data. log.txt to ./logs/2025-05-27_last_log.txt
