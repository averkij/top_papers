[27.05.2025 03:49] Read previous papers.
[27.05.2025 03:49] Generating top page (month).
[27.05.2025 03:49] Writing top page (month).
[27.05.2025 04:17] Read previous papers.
[27.05.2025 04:17] Get feed.
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19147
[27.05.2025 04:17] Extract page data from URL. URL: https://huggingface.co/papers/2505.20259
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20258
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18675
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18536
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19815
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20256
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18545
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18601
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16972
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19752
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19590
[27.05.2025 04:17] Extract page data from URL. URL: https://huggingface.co/papers/2505.16348
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13426
[27.05.2025 04:17] Extract page data from URL. URL: https://huggingface.co/papers/2505.20152
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19602
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19427
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20254
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17652
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19706
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19630
[27.05.2025 04:17] Extract page data from URL. URL: https://huggingface.co/papers/2505.19457
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19443
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10887
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20278
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19788
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19209
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15957
[27.05.2025 04:17] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19949
[27.05.2025 04:17] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.05.2025 04:17] No deleted papers detected.
[27.05.2025 04:17] Downloading and parsing papers (pdf, html). Total: 29.
[27.05.2025 04:17] Downloading and parsing paper https://huggingface.co/papers/2505.19147.
[27.05.2025 04:17] Extra JSON file exists (./assets/json/2505.19147.json), skip PDF parsing.
[27.05.2025 04:17] Paper image links file exists (./assets/img_data/2505.19147.json), skip HTML parsing.
[27.05.2025 04:17] Success.
[27.05.2025 04:17] Downloading and parsing paper https://huggingface.co/papers/2505.20259.
[27.05.2025 04:17] Downloading paper 2505.20259 from http://arxiv.org/pdf/2505.20259v1...
[27.05.2025 04:18] Extracting affiliations from text.
[27.05.2025 04:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 1 9 5 2 0 2 . 5 0 5 2 : r a Haoyu Wang1,2, Zeyu Qin1,3, Yifei Zhao2, Chao Du1 Min Lin1, Xueqian Wang2, Tianyu Pang1 1Sea AI Lab, Singapore 2Tsinghua University 3The Hong Kong University of Science and Technology haoyu-wa22@mails.tsinghua.edu.cn; tianyupang@sea.com "
[27.05.2025 04:18] Response: ```python
["Sea AI Lab, Singapore", "Tsinghua University", "The Hong Kong University of Science and Technology"]
```
[27.05.2025 04:18] Deleting PDF ./assets/pdf/2505.20259.pdf.
[27.05.2025 04:18] Success.
[27.05.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2505.20258.
[27.05.2025 04:18] Extra JSON file exists (./assets/json/2505.20258.json), skip PDF parsing.
[27.05.2025 04:18] Paper image links file exists (./assets/img_data/2505.20258.json), skip HTML parsing.
[27.05.2025 04:18] Success.
[27.05.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2505.18675.
[27.05.2025 04:18] Extra JSON file exists (./assets/json/2505.18675.json), skip PDF parsing.
[27.05.2025 04:18] Paper image links file exists (./assets/img_data/2505.18675.json), skip HTML parsing.
[27.05.2025 04:18] Success.
[27.05.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2505.18536.
[27.05.2025 04:18] Extra JSON file exists (./assets/json/2505.18536.json), skip PDF parsing.
[27.05.2025 04:18] Paper image links file exists (./assets/img_data/2505.18536.json), skip HTML parsing.
[27.05.2025 04:18] Success.
[27.05.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2505.19815.
[27.05.2025 04:18] Extra JSON file exists (./assets/json/2505.19815.json), skip PDF parsing.
[27.05.2025 04:18] Paper image links file exists (./assets/img_data/2505.19815.json), skip HTML parsing.
[27.05.2025 04:18] Success.
[27.05.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2505.20256.
[27.05.2025 04:18] Extra JSON file exists (./assets/json/2505.20256.json), skip PDF parsing.
[27.05.2025 04:18] Paper image links file exists (./assets/img_data/2505.20256.json), skip HTML parsing.
[27.05.2025 04:18] Success.
[27.05.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2505.18545.
[27.05.2025 04:18] Extra JSON file exists (./assets/json/2505.18545.json), skip PDF parsing.
[27.05.2025 04:18] Paper image links file exists (./assets/img_data/2505.18545.json), skip HTML parsing.
[27.05.2025 04:18] Success.
[27.05.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2505.18601.
[27.05.2025 04:18] Extra JSON file exists (./assets/json/2505.18601.json), skip PDF parsing.
[27.05.2025 04:18] Paper image links file exists (./assets/img_data/2505.18601.json), skip HTML parsing.
[27.05.2025 04:18] Success.
[27.05.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2505.16972.
[27.05.2025 04:18] Extra JSON file exists (./assets/json/2505.16972.json), skip PDF parsing.
[27.05.2025 04:18] Paper image links file exists (./assets/img_data/2505.16972.json), skip HTML parsing.
[27.05.2025 04:18] Success.
[27.05.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2505.19752.
[27.05.2025 04:18] Extra JSON file exists (./assets/json/2505.19752.json), skip PDF parsing.
[27.05.2025 04:18] Paper image links file exists (./assets/img_data/2505.19752.json), skip HTML parsing.
[27.05.2025 04:18] Success.
[27.05.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2505.19590.
[27.05.2025 04:18] Extra JSON file exists (./assets/json/2505.19590.json), skip PDF parsing.
[27.05.2025 04:18] Paper image links file exists (./assets/img_data/2505.19590.json), skip HTML parsing.
[27.05.2025 04:18] Success.
[27.05.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2505.16348.
[27.05.2025 04:18] Downloading paper 2505.16348 from http://arxiv.org/pdf/2505.16348v1...
[27.05.2025 04:18] Extracting affiliations from text.
[27.05.2025 04:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 2 ] . [ 1 8 4 3 6 1 . 5 0 5 2 : r Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance Taeyoon Kwon 1 Dongwook Choi1 Sunghwan Kim1 Hyojun Kim Seungjun Moon1 Beong-woo Kwak1 Kuan-Hao Huang2 Jinyoung Yeo1 1Yonsei University 2Texas A&M University "
[27.05.2025 04:18] Response: ```python
["Yonsei University", "Texas A&M University"]
```
[27.05.2025 04:18] Deleting PDF ./assets/pdf/2505.16348.pdf.
[27.05.2025 04:18] Success.
[27.05.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2505.13426.
[27.05.2025 04:18] Extra JSON file exists (./assets/json/2505.13426.json), skip PDF parsing.
[27.05.2025 04:18] Paper image links file exists (./assets/img_data/2505.13426.json), skip HTML parsing.
[27.05.2025 04:18] Success.
[27.05.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2505.20152.
[27.05.2025 04:18] Downloading paper 2505.20152 from http://arxiv.org/pdf/2505.20152v1...
[27.05.2025 04:19] Extracting affiliations from text.
[27.05.2025 04:19] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models Kai Sun, Yushi Bai, Zhen Yang, Jiajie Zhang, Ji Qi, Lei Hou, Juanzi Li Tsinghua University 5 2 0 2 6 2 ] . [ 1 2 5 1 0 2 . 5 0 5 2 : r a "
[27.05.2025 04:19] Response: ```python
["Tsinghua University"]
```
[27.05.2025 04:19] Deleting PDF ./assets/pdf/2505.20152.pdf.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Downloading and parsing paper https://huggingface.co/papers/2505.19602.
[27.05.2025 04:19] Extra JSON file exists (./assets/json/2505.19602.json), skip PDF parsing.
[27.05.2025 04:19] Paper image links file exists (./assets/img_data/2505.19602.json), skip HTML parsing.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Downloading and parsing paper https://huggingface.co/papers/2505.19427.
[27.05.2025 04:19] Extra JSON file exists (./assets/json/2505.19427.json), skip PDF parsing.
[27.05.2025 04:19] Paper image links file exists (./assets/img_data/2505.19427.json), skip HTML parsing.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Downloading and parsing paper https://huggingface.co/papers/2505.20254.
[27.05.2025 04:19] Extra JSON file exists (./assets/json/2505.20254.json), skip PDF parsing.
[27.05.2025 04:19] Paper image links file exists (./assets/img_data/2505.20254.json), skip HTML parsing.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Downloading and parsing paper https://huggingface.co/papers/2505.17652.
[27.05.2025 04:19] Extra JSON file exists (./assets/json/2505.17652.json), skip PDF parsing.
[27.05.2025 04:19] Paper image links file exists (./assets/img_data/2505.17652.json), skip HTML parsing.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Downloading and parsing paper https://huggingface.co/papers/2505.19706.
[27.05.2025 04:19] Extra JSON file exists (./assets/json/2505.19706.json), skip PDF parsing.
[27.05.2025 04:19] Paper image links file exists (./assets/img_data/2505.19706.json), skip HTML parsing.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Downloading and parsing paper https://huggingface.co/papers/2505.19630.
[27.05.2025 04:19] Extra JSON file exists (./assets/json/2505.19630.json), skip PDF parsing.
[27.05.2025 04:19] Paper image links file exists (./assets/img_data/2505.19630.json), skip HTML parsing.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Downloading and parsing paper https://huggingface.co/papers/2505.19457.
[27.05.2025 04:19] Downloading paper 2505.19457 from http://arxiv.org/pdf/2505.19457v1...
[27.05.2025 04:19] Extracting affiliations from text.
[27.05.2025 04:19] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 1 7 5 4 9 1 . 5 0 5 2 : r BizFinBench: Business-Driven Real-World Financial Benchmark for Evaluating LLMs Guilong Lu1, Xuntao Guo1,2, Rongjunchen Zhang1, Wenqiao Zhu1, and Ji Liu1 1HiThink Research 2Harbin Institute of Technology "
[27.05.2025 04:19] Response: ```python
["HiThink Research", "Harbin Institute of Technology"]
```
[27.05.2025 04:19] Deleting PDF ./assets/pdf/2505.19457.pdf.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Downloading and parsing paper https://huggingface.co/papers/2505.19443.
[27.05.2025 04:19] Extra JSON file exists (./assets/json/2505.19443.json), skip PDF parsing.
[27.05.2025 04:19] Paper image links file exists (./assets/img_data/2505.19443.json), skip HTML parsing.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Downloading and parsing paper https://huggingface.co/papers/2505.10887.
[27.05.2025 04:19] Extra JSON file exists (./assets/json/2505.10887.json), skip PDF parsing.
[27.05.2025 04:19] Paper image links file exists (./assets/img_data/2505.10887.json), skip HTML parsing.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Downloading and parsing paper https://huggingface.co/papers/2505.20278.
[27.05.2025 04:19] Extra JSON file exists (./assets/json/2505.20278.json), skip PDF parsing.
[27.05.2025 04:19] Paper image links file exists (./assets/img_data/2505.20278.json), skip HTML parsing.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Downloading and parsing paper https://huggingface.co/papers/2505.19788.
[27.05.2025 04:19] Extra JSON file exists (./assets/json/2505.19788.json), skip PDF parsing.
[27.05.2025 04:19] Paper image links file exists (./assets/img_data/2505.19788.json), skip HTML parsing.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Downloading and parsing paper https://huggingface.co/papers/2505.19209.
[27.05.2025 04:19] Extra JSON file exists (./assets/json/2505.19209.json), skip PDF parsing.
[27.05.2025 04:19] Paper image links file exists (./assets/img_data/2505.19209.json), skip HTML parsing.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Downloading and parsing paper https://huggingface.co/papers/2505.15957.
[27.05.2025 04:19] Extra JSON file exists (./assets/json/2505.15957.json), skip PDF parsing.
[27.05.2025 04:19] Paper image links file exists (./assets/img_data/2505.15957.json), skip HTML parsing.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Downloading and parsing paper https://huggingface.co/papers/2505.19949.
[27.05.2025 04:19] Extra JSON file exists (./assets/json/2505.19949.json), skip PDF parsing.
[27.05.2025 04:19] Paper image links file exists (./assets/img_data/2505.19949.json), skip HTML parsing.
[27.05.2025 04:19] Success.
[27.05.2025 04:19] Enriching papers with extra data.
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 0. The rapid advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on model-centric scaling through increasing parameter counts from millions to hundreds of billions to drive performance gains. However, as we approach hardware limits on model size, the dominan...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 1. A lifecycle safety alignment framework employs a Meta-Attacker and Defender to adapt LLMs to novel jailbreaking strategies, improving robustness in deployment.  					AI-generated summary 				 LLMs have made impressive progress, but their growing capabilities also expose them to highly flexible jailb...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 2. Adaptive Reasoning Model (ARM) uses Ada-GRPO to reduce token usage and improve efficiency across different reasoning modes.  					AI-generated summary 				 While large reasoning models demonstrate strong performance on complex tasks, they lack the ability to adjust reasoning token usage based on tas...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 3. Multimodal large language models (MLLMs) have recently achieved significant progress in visual tasks, including semantic scene understanding and text-image alignment, with reasoning variants enhancing performance on complex tasks involving mathematics and logic. However, their capacity for reasoning...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 4. Standing in 2025, at a critical juncture in the pursuit of Artificial General Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated significant potential in enhancing the reasoning capability of large language models (LLMs) and has led to the development of cutting-edge AI models such...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 5. LLM reasoning is understood through a meta-learning framework, treating reasoning as pseudo-gradient descent and questions as individual tasks, which enhances generalization and provides practical insights for improvement.  					AI-generated summary 				 We propose a novel framework for comprehendin...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 6. An end-to-end reinforcement learning framework, Omni-R1, achieves superior performance in long-horizon video-audio reasoning and fine-grained pixel understanding tasks by combining global reasoning and detail understanding systems.  					AI-generated summary 				 Long-horizon video-audio reasoning a...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 7. Large language models (LLMs) often exhibit strong biases, e.g, against women or in favor of the number 7. We investigate whether LLMs would be able to output less biased answers when allowed to observe their prior answers to the same question in a multi-turn conversation. To understand which types o...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 8. Flex-Judge uses minimal textual reasoning data to generalize across multiple modalities and evaluation formats, outperforming state-of-the-art models in multimodal evaluations.  					AI-generated summary 				 Human-generated reward signals are critical for aligning generative models with human prefe...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 9. Recent advances in Automatic Speech Recognition (ASR) have been largely fueled by massive speech corpora. However, extending coverage to diverse languages with limited resources remains a formidable challenge. This paper introduces Speech Back-Translation, a scalable pipeline that improves multiling...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 10. A novel framework, Discrete Markov Bridge, is introduced for discrete data modeling with Matrix Learning and Score Learning components, demonstrating superior performance compared to existing methods on Text8 and CIFAR-10 datasets.  					AI-generated summary 				 Discrete diffusion has recently emer...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 11. Intuitor, a Reinforcement Learning from Internal Feedback method, uses self-certainty as a reward signal to enable unsupervised learning of large language models, achieving performance comparable to GRPO on benchmarks and superior generalization.  					AI-generated summary 				 Training large langua...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 12. MEMENTO evaluates personalized memory utilization in embodied agents, revealing limitations in understanding user semantics and routines.  					AI-generated summary 				 Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. H...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 13. VLM-Gym addresses the "knowing-doing" gap in Vision-Language Models by training them in a diverse RL environment, leading to enhanced perception and reasoning abilities that surpass existing models in interactive games.  					AI-generated summary 				 Vision-Language Models (VLMs) excel in many dire...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 14. A novel hard negative contrastive learning framework improves geometric reasoning in Large Multimodal Models, significantly enhancing their performance compared to existing models.  					AI-generated summary 				 Benefiting from contrastively trained visual encoders on large-scale natural scene imag...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 15. ScaleKV compresses the KV cache in Visual Autoregressive models by differentiating drafters and refiners across transformer layers, reducing memory consumption while maintaining high fidelity.  					AI-generated summary 				 Visual Autoregressive (VAR) modeling has garnered significant attention for...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 16. WINA, a training-free sparse activation framework for large language models, improves inference accuracy by considering hidden state magnitudes and weight matrix norms, outperforming existing methods.  					AI-generated summary 				 The growing computational demands of large language models (LLMs) m...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 17. Prioritizing feature consistency in sparse autoencoders improves mechanistic interpretability of neural networks by ensuring reliable and interpretable features.  					AI-generated summary 				 Sparse Autoencoders (SAEs) are a prominent tool in mechanistic interpretability (MI) for decomposing neura...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 18. CDAS addresses low sample efficiency in reinforcement learning by aligning model competence with problem difficulty, improving both accuracy and efficiency in mathematical benchmarks.  					AI-generated summary 				 Reinforcement learning exhibits potential in enhancing the reasoning abilities of la...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 19. PathFinder-PRM, a hierarchical and error-aware Process Reward Model, improves mathematical problem-solving by fine-grained error classification and step correctness estimation, achieving state-of-the-art PRMScore with reduced data usage.  					AI-generated summary 				 Large Language Models (LLMs) a...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 20. Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Existing systems rely on a one-way information transmission mode where patients must fully desc...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 21. BizFinBench is a benchmark for evaluating large language models in financial applications, revealing distinct performance patterns across various tasks.  					AI-generated summary 				 Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical do...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 22. A review contrasts vibe coding and agentic coding paradigms, highlighting their differences in interaction, autonomy, and application areas in AI-assisted software development.  					AI-generated summary 				 This review presents a comprehensive analysis of two emerging paradigms in AI-assisted soft...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 23. InfantAgent-Next is a multimodal agent that integrates tool-based and vision models in a modular architecture to solve various benchmarks, including OSWorld, GAIA, and SWE-Bench.  					AI-generated summary 				 This paper introduces InfantAgent-Next, a generalist agent capable of interacting with co...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 24. Large language models excel at pattern matching, yet often fall short in systematic compositional generalization. We propose the coverage principle: a data-centric framework showing that models relying primarily on pattern matching for compositional tasks cannot reliably generalize beyond substituti...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 25. Multi-Turn Decomposition improves efficiency in large reasoning models by breaking down chain-of-thought into manageable turns, reducing token usage and latency while maintaining performance.  					AI-generated summary 				 Large Reasoning Models (LRMs) are criticized for the excessively lengthy Cha...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 26. A method is proposed to generate detailed scientific hypotheses using LLMs by defining and optimizing a latent reward landscape, outperforming baselines in benchmark evaluations.  					AI-generated summary 				 Large language models (LLMs) have shown promise in automating scientific hypothesis gener...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 27. A survey proposes a systematic taxonomy for evaluating large audio-language models across dimensions including auditory awareness, knowledge reasoning, dialogue ability, and fairness, to address fragmented benchmarks in the field.  					AI-generated summary 				 With advancements in large audio-lang...
[27.05.2025 04:19] ********************************************************************************
[27.05.2025 04:19] Abstract 28. Influence functions are used to attribute LLMs' reasoning in math and coding to individual training elements, revealing cross-domain effects and enabling a reweighting strategy that improves model accuracy.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable rea...
[27.05.2025 04:19] Read previous papers.
[27.05.2025 04:19] Generating reviews via LLM API.
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#training", "#data", "#math", "#optimization", "#long_context", "#survey"], "emoji": "üóúÔ∏è", "ru": {"title": "–°–∂–∞—Ç–∏–µ —Ç–æ–∫–µ–Ω–æ–≤: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥ –æ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∫ —Å–∂–∞—Ç–∏—é –¥–∞–Ω–Ω—ã—Ö –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–∞–∑–≤–∏—Ç–∏—è –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (
[27.05.2025 04:19] Querying the API.
[27.05.2025 04:19] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A lifecycle safety alignment framework employs a Meta-Attacker and Defender to adapt LLMs to novel jailbreaking strategies, improving robustness in deployment.  					AI-generated summary 				 LLMs have made impressive progress, but their growing capabilities also expose them to highly flexible jailbreaking attacks designed to bypass safety alignment. While many existing defenses focus on known types of attacks, it is more critical to prepare LLMs for unseen attacks that may arise during deployment. To address this, we propose a lifelong safety alignment framework that enables LLMs to continuously adapt to new and evolving jailbreaking strategies. Our framework introduces a competitive setup between two components: a Meta-Attacker, trained to actively discover novel jailbreaking strategies, and a Defender, trained to resist them. To effectively warm up the Meta-Attacker, we first leverage the GPT-4o API to extract key insights from a large collection of jailbreak-related research papers. Through iterative training, the first iteration Meta-Attacker achieves a 73% attack success rate (ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks. Meanwhile, the Defender progressively improves its robustness and ultimately reduces the Meta-Attacker's success rate to just 7%, enabling safer and more reliable deployment of LLMs in open-ended environments. The code is available at https://github.com/sail-sg/LifelongSafetyAlignment.
[27.05.2025 04:19] Response: {
  "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º—É –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω–æ–≤—ã—Ö –∞—Ç–∞–∫ –Ω–∞ –∏—Ö –∑–∞—â–∏—Ç—É. –í –æ—Å–Ω–æ–≤–µ —Å–∏—Å—Ç–µ–º—ã –ª–µ–∂–∏—Ç —Å–æ—Ä–µ–≤–Ω–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –º–µ–∂–¥—É –ú–µ—Ç–∞-–ê—Ç–∞–∫—É—é—â–∏–º, –∫–æ—Ç–æ—Ä—ã–π –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã, –∏ –ó–∞—â–∏—Ç–Ω–∏–∫–æ–º, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—Å—è –ø—Ä–æ—Ç–∏–≤–æ—Å—Ç–æ—è—Ç—å —ç—Ç–∏–º –∞—Ç–∞–∫–∞–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ GPT-4 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç –ø–æ –≤–∑–ª–æ–º—É LLM –∏ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞—á–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –ú–µ—Ç–∞-–ê—Ç–∞–∫—É—é—â–µ–≥–æ. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ó–∞—â–∏—Ç–Ω–∏–∫ —Å–º–æ–≥ —Å–Ω–∏–∑–∏—Ç—å —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∞—Ç–∞–∫ —Å 73% –¥–æ 7%, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å LLM –ø—Ä–∏ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏.",
  "emoji": "üõ°Ô∏è",
  "title": "–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∑–∞—â–∏—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –æ—Ç –Ω–æ–≤—ã—Ö –∞—Ç–∞–∫"
}
[27.05.2025 04:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A lifecycle safety alignment framework employs a Meta-Attacker and Defender to adapt LLMs to novel jailbreaking strategies, improving robustness in deployment.  					AI-generated summary 				 LLMs have made impressive progress, but their growing capabilities also expose them to highly flexible jailbreaking attacks designed to bypass safety alignment. While many existing defenses focus on known types of attacks, it is more critical to prepare LLMs for unseen attacks that may arise during deployment. To address this, we propose a lifelong safety alignment framework that enables LLMs to continuously adapt to new and evolving jailbreaking strategies. Our framework introduces a competitive setup between two components: a Meta-Attacker, trained to actively discover novel jailbreaking strategies, and a Defender, trained to resist them. To effectively warm up the Meta-Attacker, we first leverage the GPT-4o API to extract key insights from a large collection of jailbreak-related research papers. Through iterative training, the first iteration Meta-Attacker achieves a 73% attack success rate (ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks. Meanwhile, the Defender progressively improves its robustness and ultimately reduces the Meta-Attacker's success rate to just 7%, enabling safer and more reliable deployment of LLMs in open-ended environments. The code is available at https://github.com/sail-sg/LifelongSafetyAlignment."

[27.05.2025 04:19] Response: ```python
["INFERENCE", "TRAINING"]
```
[27.05.2025 04:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A lifecycle safety alignment framework employs a Meta-Attacker and Defender to adapt LLMs to novel jailbreaking strategies, improving robustness in deployment.  					AI-generated summary 				 LLMs have made impressive progress, but their growing capabilities also expose them to highly flexible jailbreaking attacks designed to bypass safety alignment. While many existing defenses focus on known types of attacks, it is more critical to prepare LLMs for unseen attacks that may arise during deployment. To address this, we propose a lifelong safety alignment framework that enables LLMs to continuously adapt to new and evolving jailbreaking strategies. Our framework introduces a competitive setup between two components: a Meta-Attacker, trained to actively discover novel jailbreaking strategies, and a Defender, trained to resist them. To effectively warm up the Meta-Attacker, we first leverage the GPT-4o API to extract key insights from a large collection of jailbreak-related research papers. Through iterative training, the first iteration Meta-Attacker achieves a 73% attack success rate (ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks. Meanwhile, the Defender progressively improves its robustness and ultimately reduces the Meta-Attacker's success rate to just 7%, enabling safer and more reliable deployment of LLMs in open-ended environments. The code is available at https://github.com/sail-sg/LifelongSafetyAlignment."

[27.05.2025 04:19] Response: ```python
['SECURITY', 'ALIGNMENT']
```
[27.05.2025 04:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a lifecycle safety alignment framework designed to enhance the robustness of large language models (LLMs) against jailbreaking attacks. It introduces a competitive system involving a Meta-Attacker, which learns to identify new jailbreaking strategies, and a Defender, which evolves to counter these attacks. The framework emphasizes the importance of preparing LLMs for unforeseen threats rather than just known vulnerabilities. Through iterative training, the Defender significantly reduces the Meta-Attacker\'s success rate, ensuring safer deployment of LLMs in dynamic environments.","title":"Adapting LLMs for Unseen Threats: A Lifelong Safety Approach"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents a lifecycle safety alignment framework designed to enhance the robustness of large language models (LLMs) against jailbreaking attacks. It introduces a competitive system involving a Meta-Attacker, which learns to identify new jailbreaking strategies, and a Defender, which evolves to counter these attacks. The framework emphasizes the importance of preparing LLMs for unforeseen threats rather than just known vulnerabilities. Through iterative training, the Defender significantly reduces the Meta-Attacker's success rate, ensuring safer deployment of LLMs in dynamic environments.", title='Adapting LLMs for Unseen Threats: A Lifelong Safety Approach'))
[27.05.2025 04:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁîüÂëΩÂë®ÊúüÂÆâÂÖ®ÂØπÈΩêÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Èù¢ÂØπÊñ∞ÂûãË∂äÁã±ÊîªÂáªÊó∂ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂºïÂÖ•‰∏Ä‰∏™ÂÖÉÊîªÂáªËÄÖÂíå‰∏Ä‰∏™Èò≤Âæ°ËÄÖÁöÑÁ´û‰∫âÊú∫Âà∂Ôºå‰ΩøLLMsËÉΩÂ§üÊåÅÁª≠ÈÄÇÂ∫î‰∏çÊñ≠ÊºîÂèòÁöÑË∂äÁã±Á≠ñÁï•„ÄÇÂÖÉÊîªÂáªËÄÖË¥üË¥£‰∏ªÂä®ÂèëÁé∞Êñ∞ÁöÑË∂äÁã±Á≠ñÁï•ÔºåËÄåÈò≤Âæ°ËÄÖÂàôËá¥Âäõ‰∫éÊäµÂæ°Ëøô‰∫õÊîªÂáª„ÄÇÈÄöËøáËø≠‰ª£ËÆ≠ÁªÉÔºåÈò≤Âæ°ËÄÖÊòæËëóÊèêÈ´ò‰∫ÜÂØπÊäóËÉΩÂäõÔºåÂ∞ÜÂÖÉÊîªÂáªËÄÖÁöÑÊàêÂäüÁéáÈôç‰ΩéÂà∞‰ªÖ7%Ôºå‰ªéËÄåÂÆûÁé∞‰∫ÜLLMsÂú®ÂºÄÊîæÁéØÂ¢É‰∏≠ÁöÑÊõ¥ÂÆâÂÖ®ÂèØÈù†ÈÉ®ÁΩ≤„ÄÇ","title":"ÊèêÂçáLLMsÂÆâÂÖ®ÊÄßÁöÑÁîüÂëΩÂë®ÊúüÂØπÈΩêÊ°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁîüÂëΩÂë®ÊúüÂÆâÂÖ®ÂØπÈΩêÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Èù¢ÂØπÊñ∞ÂûãË∂äÁã±ÊîªÂáªÊó∂ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂºïÂÖ•‰∏Ä‰∏™ÂÖÉÊîªÂáªËÄÖÂíå‰∏Ä‰∏™Èò≤Âæ°ËÄÖÁöÑÁ´û‰∫âÊú∫Âà∂Ôºå‰ΩøLLMsËÉΩÂ§üÊåÅÁª≠ÈÄÇÂ∫î‰∏çÊñ≠ÊºîÂèòÁöÑË∂äÁã±Á≠ñÁï•„ÄÇÂÖÉÊîªÂáªËÄÖË¥üË¥£‰∏ªÂä®ÂèëÁé∞Êñ∞ÁöÑË∂äÁã±Á≠ñÁï•ÔºåËÄåÈò≤Âæ°ËÄÖÂàôËá¥Âäõ‰∫éÊäµÂæ°Ëøô‰∫õÊîªÂáª„ÄÇÈÄöËøáËø≠‰ª£ËÆ≠ÁªÉÔºåÈò≤Âæ°ËÄÖÊòæËëóÊèêÈ´ò‰∫ÜÂØπÊäóËÉΩÂäõÔºåÂ∞ÜÂÖÉÊîªÂáªËÄÖÁöÑÊàêÂäüÁéáÈôç‰ΩéÂà∞‰ªÖ7%Ôºå‰ªéËÄåÂÆûÁé∞‰∫ÜLLMsÂú®ÂºÄÊîæÁéØÂ¢É‰∏≠ÁöÑÊõ¥ÂÆâÂÖ®ÂèØÈù†ÈÉ®ÁΩ≤„ÄÇ', title='ÊèêÂçáLLMsÂÆâÂÖ®ÊÄßÁöÑÁîüÂëΩÂë®ÊúüÂØπÈΩêÊ°ÜÊû∂'))
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#reasoning", "#inference", "#optimization", "#training"], "emoji": "üß†", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ –ò–ò", "desc": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (ARM) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Ada-GRPO –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ 
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#cv", "#open_source"], "emoji": "üó∫Ô∏è", "ru": {"title": "ReasonMap: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ReasonMap - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ–π
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#multimodal", "#training", "#reasoning", "#benchmark", "#rlhf", "#agi", "#rl"], "emoji": "üß†", "ru": {"title": "RFT: –∫–ª—é—á –∫ —É—Å–∏–ª–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª—è—Ö", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é –º–µ—Ç–æ–¥–∞ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RFT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#math", "#training"], "emoji": "üß†", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM –∫–∞–∫ –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —á–µ—Ä–µ–∑
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#video", "#hallucinations", "#benchmark", "#multimodal", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–£–º–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä—É–¥–∞: –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "Omni-R1 - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∑
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#data", "#ethics", "#hallucinations", "#benchmark", "#rlhf"], "emoji": "ü§ñ", "ru": {"title": "–°–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–π –¥–∏–∞–ª–æ–≥", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –µ–µ —É–º–µ–Ω—å—à–µ–Ω–∏—è –≤
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#reasoning", "#rlhf", "#alignment", "#benchmark", "#multimodal", "#transfer_learning"], "emoji": "üß†", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ - –∫–ª—é—á –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–µ", "desc": "Flex-Judge - —ç—Ç–æ –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –º–∏–Ω–∏–º–∞–ª—å–Ω—ã
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#data", "#multilingual", "#low_resource", "#dataset", "#audio", "#synthetic"], "emoji": "üó£Ô∏è", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ—á—å –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ ASR", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Speech Back-Translation –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∞–≤—Ç–æ–º–∞
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#training", "#data", "#benchmark", "#diffusion", "#dataset", "#optimization", "#architecture"], "emoji": "üåâ", "ru": {"title": "–î–∏—Å–∫—Ä–µ—Ç–Ω—ã–π –º–∞—Ä–∫–æ–≤—Å–∫–∏–π –º–æ—Å—Ç: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Discrete Markov Bridge –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#rlhf", "#optimization"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –ò–ò: –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∫–∞–∫ –¥–≤–∏–≥–∞—Ç–µ–ª—å –ø—Ä–æ–≥—Ä–µ—Å—Å–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Intuitor. –≠—Ç–æ—Ç –º
[27.05.2025 04:19] Querying the API.
[27.05.2025 04:19] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MEMENTO evaluates personalized memory utilization in embodied agents, revealing limitations in understanding user semantics and routines.  					AI-generated summary 				 Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. However, these tasks primarily focus on single-turn interactions with simplified instructions, which do not truly reflect the challenges of providing meaningful assistance to users. To provide personalized assistance, embodied agents must understand the unique semantics that users assign to the physical world (e.g., favorite cup, breakfast routine) by leveraging prior interaction history to interpret dynamic, real-world instructions. Yet, the effectiveness of embodied agents in utilizing memory for personalized assistance remains largely underexplored. To address this gap, we present MEMENTO, a personalized embodied agent evaluation framework designed to comprehensively assess memory utilization capabilities to provide personalized assistance. Our framework consists of a two-stage memory evaluation process design that enables quantifying the impact of memory utilization on task performance. This process enables the evaluation of agents' understanding of personalized knowledge in object rearrangement tasks by focusing on its role in goal interpretation: (1) the ability to identify target objects based on personal meaning (object semantics), and (2) the ability to infer object-location configurations from consistent user patterns, such as routines (user patterns). Our experiments across various LLMs reveal significant limitations in memory utilization, with even frontier models like GPT-4o experiencing a 30.5% performance drop when required to reference multiple memories, particularly in tasks involving user patterns. These findings, along with our detailed analyses and case studies, provide valuable insights for future research in developing more effective personalized embodied agents. Project website: https://connoriginal.github.io/MEMENTO
[27.05.2025 04:19] Response: {
  "desc": "MEMENTO - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞–º—è—Ç—å –¥–ª—è –æ–∫–∞–∑–∞–Ω–∏—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π –ø–æ–º–æ—â–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞–º–∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—Ä–µ–¥–º–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏ –¥–∞–∂–µ —É –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ GPT-4. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã—è–≤–∏–ª–∏ 30.5% –ø–∞–¥–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è–º, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏.",
  "emoji": "ü§ñ",
  "title": "–û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤: –ø—É—Ç—å –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏"
}
[27.05.2025 04:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MEMENTO evaluates personalized memory utilization in embodied agents, revealing limitations in understanding user semantics and routines.  					AI-generated summary 				 Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. However, these tasks primarily focus on single-turn interactions with simplified instructions, which do not truly reflect the challenges of providing meaningful assistance to users. To provide personalized assistance, embodied agents must understand the unique semantics that users assign to the physical world (e.g., favorite cup, breakfast routine) by leveraging prior interaction history to interpret dynamic, real-world instructions. Yet, the effectiveness of embodied agents in utilizing memory for personalized assistance remains largely underexplored. To address this gap, we present MEMENTO, a personalized embodied agent evaluation framework designed to comprehensively assess memory utilization capabilities to provide personalized assistance. Our framework consists of a two-stage memory evaluation process design that enables quantifying the impact of memory utilization on task performance. This process enables the evaluation of agents' understanding of personalized knowledge in object rearrangement tasks by focusing on its role in goal interpretation: (1) the ability to identify target objects based on personal meaning (object semantics), and (2) the ability to infer object-location configurations from consistent user patterns, such as routines (user patterns). Our experiments across various LLMs reveal significant limitations in memory utilization, with even frontier models like GPT-4o experiencing a 30.5% performance drop when required to reference multiple memories, particularly in tasks involving user patterns. These findings, along with our detailed analyses and case studies, provide valuable insights for future research in developing more effective personalized embodied agents. Project website: https://connoriginal.github.io/MEMENTO"

[27.05.2025 04:19] Response: ```python
['AGENTS', 'MULTIMODAL']
```
[27.05.2025 04:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MEMENTO evaluates personalized memory utilization in embodied agents, revealing limitations in understanding user semantics and routines.  					AI-generated summary 				 Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. However, these tasks primarily focus on single-turn interactions with simplified instructions, which do not truly reflect the challenges of providing meaningful assistance to users. To provide personalized assistance, embodied agents must understand the unique semantics that users assign to the physical world (e.g., favorite cup, breakfast routine) by leveraging prior interaction history to interpret dynamic, real-world instructions. Yet, the effectiveness of embodied agents in utilizing memory for personalized assistance remains largely underexplored. To address this gap, we present MEMENTO, a personalized embodied agent evaluation framework designed to comprehensively assess memory utilization capabilities to provide personalized assistance. Our framework consists of a two-stage memory evaluation process design that enables quantifying the impact of memory utilization on task performance. This process enables the evaluation of agents' understanding of personalized knowledge in object rearrangement tasks by focusing on its role in goal interpretation: (1) the ability to identify target objects based on personal meaning (object semantics), and (2) the ability to infer object-location configurations from consistent user patterns, such as routines (user patterns). Our experiments across various LLMs reveal significant limitations in memory utilization, with even frontier models like GPT-4o experiencing a 30.5% performance drop when required to reference multiple memories, particularly in tasks involving user patterns. These findings, along with our detailed analyses and case studies, provide valuable insights for future research in developing more effective personalized embodied agents. Project website: https://connoriginal.github.io/MEMENTO"

[27.05.2025 04:19] Response: ```python
["AGI", "INTERPRETABILITY", "REASONING"]
```
[27.05.2025 04:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MEMENTO is a framework that evaluates how well embodied agents use memory to provide personalized assistance. It highlights the challenges these agents face in understanding the unique meanings users assign to objects and their routines. The framework includes a two-stage evaluation process that measures how effectively agents can identify objects based on personal significance and infer user patterns. Experiments show that even advanced models like GPT-4o struggle with memory utilization, particularly when dealing with multiple memories, leading to a notable drop in performance.","title":"Enhancing Personalized Assistance in Embodied Agents through Memory Utilization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MEMENTO is a framework that evaluates how well embodied agents use memory to provide personalized assistance. It highlights the challenges these agents face in understanding the unique meanings users assign to objects and their routines. The framework includes a two-stage evaluation process that measures how effectively agents can identify objects based on personal significance and infer user patterns. Experiments show that even advanced models like GPT-4o struggle with memory utilization, particularly when dealing with multiple memories, leading to a notable drop in performance.', title='Enhancing Personalized Assistance in Embodied Agents through Memory Utilization'))
[27.05.2025 04:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MEMENTOÊòØ‰∏Ä‰∏™ËØÑ‰º∞‰∏™ÊÄßÂåñËÆ∞ÂøÜÂà©Áî®ÁöÑÊ°ÜÊû∂Ôºå‰∏ìÊ≥®‰∫éÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÂú®ÁêÜËß£Áî®Êà∑ËØ≠‰πâÂíåÊó•Â∏∏‰π†ÊÉØÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄß„ÄÇÂ∞ΩÁÆ°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂÆ∂Â∫≠Áâ©ÂìÅÈáçÊéí‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜËøô‰∫õ‰ªªÂä°‰∏ªË¶ÅÊòØÂçïËΩÆ‰∫§‰∫íÔºåÊú™ËÉΩÁúüÂÆûÂèçÊò†Êèê‰æõÊúâÊÑè‰πâÂ∏ÆÂä©ÁöÑÊåëÊàò„ÄÇ‰∏∫‰∫ÜÊèê‰æõ‰∏™ÊÄßÂåñÁöÑÂ∏ÆÂä©ÔºåÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÈúÄË¶ÅÁêÜËß£Áî®Êà∑ÂØπÁâ©ÁêÜ‰∏ñÁïåÁöÑÁã¨ÁâπËØ≠‰πâÔºåÂπ∂Âà©Áî®ÂÖàÂâçÁöÑ‰∫§‰∫íÂéÜÂè≤Êù•Ëß£ÈáäÂä®ÊÄÅÁöÑÁé∞ÂÆûÊåá‰ª§„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÂç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑÊ®°ÂûãÔºåÂ¶ÇGPT-4oÔºåÂú®ÈúÄË¶ÅÂèÇËÄÉÂ§ö‰∏™ËÆ∞ÂøÜÊó∂ÔºåÊÄßËÉΩ‰πü‰ºö‰∏ãÈôç30.5%ÔºåËøô‰∏∫Êú™Êù•ÂºÄÂèëÊõ¥ÊúâÊïàÁöÑ‰∏™ÊÄßÂåñÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑËßÅËß£„ÄÇ","title":"‰∏™ÊÄßÂåñËÆ∞ÂøÜÂà©Áî®ÁöÑËØÑ‰º∞Ê°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MEMENTOÊòØ‰∏Ä‰∏™ËØÑ‰º∞‰∏™ÊÄßÂåñËÆ∞ÂøÜÂà©Áî®ÁöÑÊ°ÜÊû∂Ôºå‰∏ìÊ≥®‰∫éÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÂú®ÁêÜËß£Áî®Êà∑ËØ≠‰πâÂíåÊó•Â∏∏‰π†ÊÉØÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄß„ÄÇÂ∞ΩÁÆ°Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂÆ∂Â∫≠Áâ©ÂìÅÈáçÊéí‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜËøô‰∫õ‰ªªÂä°‰∏ªË¶ÅÊòØÂçïËΩÆ‰∫§‰∫íÔºåÊú™ËÉΩÁúüÂÆûÂèçÊò†Êèê‰æõÊúâÊÑè‰πâÂ∏ÆÂä©ÁöÑÊåëÊàò„ÄÇ‰∏∫‰∫ÜÊèê‰æõ‰∏™ÊÄßÂåñÁöÑÂ∏ÆÂä©ÔºåÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÈúÄË¶ÅÁêÜËß£Áî®Êà∑ÂØπÁâ©ÁêÜ‰∏ñÁïåÁöÑÁã¨ÁâπËØ≠‰πâÔºåÂπ∂Âà©Áî®ÂÖàÂâçÁöÑ‰∫§‰∫íÂéÜÂè≤Êù•Ëß£ÈáäÂä®ÊÄÅÁöÑÁé∞ÂÆûÊåá‰ª§„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÂç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑÊ®°ÂûãÔºåÂ¶ÇGPT-4oÔºåÂú®ÈúÄË¶ÅÂèÇËÄÉÂ§ö‰∏™ËÆ∞ÂøÜÊó∂ÔºåÊÄßËÉΩ‰πü‰ºö‰∏ãÈôç30.5%ÔºåËøô‰∏∫Êú™Êù•ÂºÄÂèëÊõ¥ÊúâÊïàÁöÑ‰∏™ÊÄßÂåñÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑËßÅËß£„ÄÇ', title='‰∏™ÊÄßÂåñËÆ∞ÂøÜÂà©Áî®ÁöÑËØÑ‰º∞Ê°ÜÊû∂'))
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#multimodal", "#training", "#reasoning", "#open_source", "#agents", "#games", "#optimization", "#rl"], "emoji": "üéÆ", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –∑–Ω–∞–Ω–∏–µ–º –∏ –¥–µ–π—Å—Ç–≤–∏–µ–º –≤ Vision-Language Models —Å –ø–æ–º–æ—â—å—é RL", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VLM-Gym - —Å—Ä–µ–¥—É –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø
[27.05.2025 04:19] Querying the API.
[27.05.2025 04:19] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel hard negative contrastive learning framework improves geometric reasoning in Large Multimodal Models, significantly enhancing their performance compared to existing models.  					AI-generated summary 				 Benefiting from contrastively trained visual encoders on large-scale natural scene images, Large Multimodal Models (LMMs) have achieved remarkable performance across various visual perception tasks. However, the inherent limitations of contrastive learning upon summarized descriptions fundamentally restrict the capabilities of models in meticulous reasoning, particularly in crucial scenarios of geometric problem-solving. To enhance geometric understanding, we propose a novel hard negative contrastive learning framework for the vision encoder, which combines image-based contrastive learning using generation-based hard negatives created by perturbing diagram generation code, and text-based contrastive learning using rule-based negatives derived from modified geometric descriptions and retrieval-based negatives selected based on caption similarity. We train CLIP using our strong negative learning method, namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for geometric problem-solving. Experiments show that our trained model, MMGeoLM, significantly outperforms other open-source models on three geometric reasoning benchmarks. Even with a size of 7B, it can rival powerful closed-source models like GPT-4o. We further study the impact of different negative sample construction methods and the number of negative samples on the geometric reasoning performance of LMM, yielding fruitful conclusions. The code and dataset are available at https://github.com/THU-KEG/MMGeoLM.
[27.05.2025 04:19] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–ª–æ–∂–Ω—ã—Ö –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∏ –ø—Ä–∞–≤–∏–ª–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å MMGeoLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–µ—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.",
  "emoji": "üìê",
  "title": "–ü—Ä–æ—Ä—ã–≤ –≤ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –ò–ò —á–µ—Ä–µ–∑ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
}
[27.05.2025 04:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel hard negative contrastive learning framework improves geometric reasoning in Large Multimodal Models, significantly enhancing their performance compared to existing models.  					AI-generated summary 				 Benefiting from contrastively trained visual encoders on large-scale natural scene images, Large Multimodal Models (LMMs) have achieved remarkable performance across various visual perception tasks. However, the inherent limitations of contrastive learning upon summarized descriptions fundamentally restrict the capabilities of models in meticulous reasoning, particularly in crucial scenarios of geometric problem-solving. To enhance geometric understanding, we propose a novel hard negative contrastive learning framework for the vision encoder, which combines image-based contrastive learning using generation-based hard negatives created by perturbing diagram generation code, and text-based contrastive learning using rule-based negatives derived from modified geometric descriptions and retrieval-based negatives selected based on caption similarity. We train CLIP using our strong negative learning method, namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for geometric problem-solving. Experiments show that our trained model, MMGeoLM, significantly outperforms other open-source models on three geometric reasoning benchmarks. Even with a size of 7B, it can rival powerful closed-source models like GPT-4o. We further study the impact of different negative sample construction methods and the number of negative samples on the geometric reasoning performance of LMM, yielding fruitful conclusions. The code and dataset are available at https://github.com/THU-KEG/MMGeoLM."

[27.05.2025 04:19] Response: ```python
['MULTIMODAL', 'BENCHMARK', 'DATASET', 'TRAINING']
```
[27.05.2025 04:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel hard negative contrastive learning framework improves geometric reasoning in Large Multimodal Models, significantly enhancing their performance compared to existing models.  					AI-generated summary 				 Benefiting from contrastively trained visual encoders on large-scale natural scene images, Large Multimodal Models (LMMs) have achieved remarkable performance across various visual perception tasks. However, the inherent limitations of contrastive learning upon summarized descriptions fundamentally restrict the capabilities of models in meticulous reasoning, particularly in crucial scenarios of geometric problem-solving. To enhance geometric understanding, we propose a novel hard negative contrastive learning framework for the vision encoder, which combines image-based contrastive learning using generation-based hard negatives created by perturbing diagram generation code, and text-based contrastive learning using rule-based negatives derived from modified geometric descriptions and retrieval-based negatives selected based on caption similarity. We train CLIP using our strong negative learning method, namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for geometric problem-solving. Experiments show that our trained model, MMGeoLM, significantly outperforms other open-source models on three geometric reasoning benchmarks. Even with a size of 7B, it can rival powerful closed-source models like GPT-4o. We further study the impact of different negative sample construction methods and the number of negative samples on the geometric reasoning performance of LMM, yielding fruitful conclusions. The code and dataset are available at https://github.com/THU-KEG/MMGeoLM."

[27.05.2025 04:19] Response: ```python
['REASONING', 'OPEN_SOURCE']
```
[27.05.2025 04:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new framework for hard negative contrastive learning that enhances geometric reasoning in Large Multimodal Models (LMMs). By utilizing contrastively trained visual encoders and generating hard negatives through perturbations and rule-based modifications, the framework improves the model\'s ability to solve geometric problems. The proposed model, MMGeoLM, demonstrates superior performance on geometric reasoning tasks compared to existing models, even rivaling larger closed-source models. The study also explores how different methods of constructing negative samples affect the model\'s reasoning capabilities, providing valuable insights for future research.","title":"Enhancing Geometric Reasoning with Hard Negative Contrastive Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces a new framework for hard negative contrastive learning that enhances geometric reasoning in Large Multimodal Models (LMMs). By utilizing contrastively trained visual encoders and generating hard negatives through perturbations and rule-based modifications, the framework improves the model's ability to solve geometric problems. The proposed model, MMGeoLM, demonstrates superior performance on geometric reasoning tasks compared to existing models, even rivaling larger closed-source models. The study also explores how different methods of constructing negative samples affect the model's reasoning capabilities, providing valuable insights for future research.", title='Enhancing Geometric Reasoning with Hard Negative Contrastive Learning'))
[27.05.2025 04:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂõ∞ÈöæË¥üÊ†∑Êú¨ÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂Ôºå‰ª•ÊèêÈ´òÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Âá†‰ΩïÊé®ÁêÜÊñπÈù¢ÁöÑË°®Áé∞„ÄÇÈÄöËøáÂØπÊØîÂ≠¶‰π†ËÆ≠ÁªÉÁöÑËßÜËßâÁºñÁ†ÅÂô®Âú®Ëá™ÁÑ∂Âú∫ÊôØÂõæÂÉè‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊïàÊûúÔºå‰ΩÜÂú®ÁªÜËá¥Êé®ÁêÜÊñπÈù¢Â≠òÂú®Â±ÄÈôê„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÁªìÂêà‰∫ÜÂü∫‰∫éÂõæÂÉèÁöÑÂØπÊØîÂ≠¶‰π†ÂíåÂü∫‰∫éÊñáÊú¨ÁöÑÂØπÊØîÂ≠¶‰π†ÔºåÂà©Áî®ÁîüÊàêÁöÑÂõ∞ÈöæË¥üÊ†∑Êú¨Âíå‰øÆÊîπÁöÑÂá†‰ΩïÊèèËø∞Êù•Â¢ûÂº∫Ê®°ÂûãÁöÑÂá†‰ΩïÁêÜËß£ËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®Âá†‰ΩïÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÂÖ∂‰ªñÂºÄÊ∫êÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âº∫Â§ßÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ","title":"ÊèêÂçáÂá†‰ΩïÊé®ÁêÜÁöÑÊñ∞ÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂõ∞ÈöæË¥üÊ†∑Êú¨ÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂Ôºå‰ª•ÊèêÈ´òÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Âá†‰ΩïÊé®ÁêÜÊñπÈù¢ÁöÑË°®Áé∞„ÄÇÈÄöËøáÂØπÊØîÂ≠¶‰π†ËÆ≠ÁªÉÁöÑËßÜËßâÁºñÁ†ÅÂô®Âú®Ëá™ÁÑ∂Âú∫ÊôØÂõæÂÉè‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊïàÊûúÔºå‰ΩÜÂú®ÁªÜËá¥Êé®ÁêÜÊñπÈù¢Â≠òÂú®Â±ÄÈôê„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÁªìÂêà‰∫ÜÂü∫‰∫éÂõæÂÉèÁöÑÂØπÊØîÂ≠¶‰π†ÂíåÂü∫‰∫éÊñáÊú¨ÁöÑÂØπÊØîÂ≠¶‰π†ÔºåÂà©Áî®ÁîüÊàêÁöÑÂõ∞ÈöæË¥üÊ†∑Êú¨Âíå‰øÆÊîπÁöÑÂá†‰ΩïÊèèËø∞Êù•Â¢ûÂº∫Ê®°ÂûãÁöÑÂá†‰ΩïÁêÜËß£ËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®Âá†‰ΩïÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÂÖ∂‰ªñÂºÄÊ∫êÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âº∫Â§ßÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ', title='ÊèêÂçáÂá†‰ΩïÊé®ÁêÜÁöÑÊñ∞ÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂'))
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#inference", "#optimization", "#architecture"], "emoji": "üóúÔ∏è", "ru": {"title": "ScaleKV: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –∫—ç—à–∞ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π", "desc": "ScaleKV - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è KV-–∫—ç—à–∞ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —Ä–∞–∑–¥–µ–ª—è–µ—Ç —Å–ª–æ–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –Ω–∞ –¥—Ä–∞—Ñ—Ç–µ—Ä—ã –∏ —Ä–µ—Ñ–∞–π
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#inference", "#training", "#open_source", "#optimization", "#architecture"], "emoji": "üöÄ", "ru": {"title": "WINA: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "WINA - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏–π –¥–æ–ø–æ–ª
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#interpretability", "#training", "#math", "#architecture"], "emoji": "üîç", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –∫–ª—é—á –∫ –Ω–∞–¥–µ–∂–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#math", "#optimization"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ: CDAS –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "CDAS (Competence-Difficulty Alignment Sampling) - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –Ω–∞–ø—Ä–∞
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#reasoning", "#training", "#math", "#hallucinations", "#dataset", "#optimization"], "emoji": "üßÆ", "ru": {"title": "–¢–æ—á–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö —Å PathFinder-PRM", "desc": "PathFinder-PRM - —ç—Ç–æ –Ω–æ–≤–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞, —É—á–∏—Ç—ã–≤–∞—é—â–∞—è –æ—à–∏–±–∫–∏, 
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#training", "#reasoning", "#science", "#healthcare", "#dataset", "#games", "#optimization", "#rl"], "emoji": "ü©∫", "ru": {"title": "–£–º–Ω—ã–π –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –¥–æ–∫—Ç–æ—Ä: –ò–ò —É—á–∏—Ç—Å—è –≤–µ—Å—Ç–∏ –¥–∏–∞–ª–æ–≥ —Å –ø–∞—Ü–∏–µ–Ω—Ç–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DoctorAgent-RL - —Å–∏—Å—Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º 
[27.05.2025 04:19] Querying the API.
[27.05.2025 04:19] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

BizFinBench is a benchmark for evaluating large language models in financial applications, revealing distinct performance patterns across various tasks.  					AI-generated summary 				 Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical domains like finance, law, and healthcare remains challenging. To address this, we introduce BizFinBench, the first benchmark specifically designed to evaluate LLMs in real-world financial applications. BizFinBench consists of 6,781 well-annotated queries in Chinese, spanning five dimensions: numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering, grouped into nine fine-grained categories. The benchmark includes both objective and subjective metrics. We also introduce IteraJudge, a novel LLM evaluation method that reduces bias when LLMs serve as evaluators in objective metrics. We benchmark 25 models, including both proprietary and open-source systems. Extensive experiments show that no model dominates across all tasks. Our evaluation reveals distinct capability patterns: (1) In Numerical Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning, proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with open-source models trailing by up to 19.49 points; (3) In Information Extraction, the performance spread is the largest, with DeepSeek-R1 scoring 71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition, performance variance is minimal, with top models scoring between 39.16 and 50.00. We find that while current LLMs handle routine finance queries competently, they struggle with complex scenarios requiring cross-concept reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future research. The code and dataset are available at https://github.com/HiThink-Research/BizFinBench.
[27.05.2025 04:19] Response: {
  "desc": "BizFinBench - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö. –û–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 6781 —Ö–æ—Ä–æ—à–æ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö –ø—è—Ç—å –∏–∑–º–µ—Ä–µ–Ω–∏–π: —á–∏—Å–ª–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–Ω–∞–Ω–∏–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ 25 –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –∏ –æ—Ç–∫—Ä—ã—Ç—ã–µ —Å–∏—Å—Ç–µ–º—ã, –∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –Ω–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç –≤–æ –≤—Å–µ—Ö –∑–∞–¥–∞—á–∞—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —Ä—É—Ç–∏–Ω–Ω—ã–º–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏, –Ω–æ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ —Å—Ü–µ–Ω–∞—Ä–∏—è–º–∏, —Ç—Ä–µ–±—É—é—â–∏–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏.",
  "emoji": "üíπ",
  "title": "BizFinBench: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ LLM –≤ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö"
}
[27.05.2025 04:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BizFinBench is a benchmark for evaluating large language models in financial applications, revealing distinct performance patterns across various tasks.  					AI-generated summary 				 Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical domains like finance, law, and healthcare remains challenging. To address this, we introduce BizFinBench, the first benchmark specifically designed to evaluate LLMs in real-world financial applications. BizFinBench consists of 6,781 well-annotated queries in Chinese, spanning five dimensions: numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering, grouped into nine fine-grained categories. The benchmark includes both objective and subjective metrics. We also introduce IteraJudge, a novel LLM evaluation method that reduces bias when LLMs serve as evaluators in objective metrics. We benchmark 25 models, including both proprietary and open-source systems. Extensive experiments show that no model dominates across all tasks. Our evaluation reveals distinct capability patterns: (1) In Numerical Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning, proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with open-source models trailing by up to 19.49 points; (3) In Information Extraction, the performance spread is the largest, with DeepSeek-R1 scoring 71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition, performance variance is minimal, with top models scoring between 39.16 and 50.00. We find that while current LLMs handle routine finance queries competently, they struggle with complex scenarios requiring cross-concept reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future research. The code and dataset are available at https://github.com/HiThink-Research/BizFinBench."

[27.05.2025 04:19] Response: ```python
['BENCHMARK', 'DATASET']
```
[27.05.2025 04:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"BizFinBench is a benchmark for evaluating large language models in financial applications, revealing distinct performance patterns across various tasks.  					AI-generated summary 				 Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical domains like finance, law, and healthcare remains challenging. To address this, we introduce BizFinBench, the first benchmark specifically designed to evaluate LLMs in real-world financial applications. BizFinBench consists of 6,781 well-annotated queries in Chinese, spanning five dimensions: numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering, grouped into nine fine-grained categories. The benchmark includes both objective and subjective metrics. We also introduce IteraJudge, a novel LLM evaluation method that reduces bias when LLMs serve as evaluators in objective metrics. We benchmark 25 models, including both proprietary and open-source systems. Extensive experiments show that no model dominates across all tasks. Our evaluation reveals distinct capability patterns: (1) In Numerical Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning, proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with open-source models trailing by up to 19.49 points; (3) In Information Extraction, the performance spread is the largest, with DeepSeek-R1 scoring 71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition, performance variance is minimal, with top models scoring between 39.16 and 50.00. We find that while current LLMs handle routine finance queries competently, they struggle with complex scenarios requiring cross-concept reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future research. The code and dataset are available at https://github.com/HiThink-Research/BizFinBench."

[27.05.2025 04:19] Response: ```python
["REASONING", "OPEN_SOURCE", "SCIENCE"]
```
[27.05.2025 04:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BizFinBench is a specialized benchmark designed to evaluate large language models (LLMs) in financial applications, highlighting their performance across various tasks. It includes 6,781 annotated queries in Chinese, covering areas such as numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering. The benchmark employs both objective and subjective metrics, and introduces IteraJudge to minimize bias in evaluations. Results show that no single model excels in all tasks, with distinct performance patterns observed among different models, particularly in complex reasoning scenarios.","title":"BizFinBench: Evaluating LLMs for Financial Precision"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='BizFinBench is a specialized benchmark designed to evaluate large language models (LLMs) in financial applications, highlighting their performance across various tasks. It includes 6,781 annotated queries in Chinese, covering areas such as numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering. The benchmark employs both objective and subjective metrics, and introduces IteraJudge to minimize bias in evaluations. Results show that no single model excels in all tasks, with distinct performance patterns observed among different models, particularly in complex reasoning scenarios.', title='BizFinBench: Evaluating LLMs for Financial Precision'))
[27.05.2025 04:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"BizFinBenchÊòØ‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÈáëËûçÂ∫îÁî®‰∏≠ÁöÑÂü∫ÂáÜÊµãËØï„ÄÇÂÆÉÂåÖÂê´6781‰∏™ÁªèËøáËâØÂ•ΩÊ†áÊ≥®ÁöÑÊü•ËØ¢ÔºåÊ∂µÁõñ‰∫ÜÊï∞ÂÄºËÆ°ÁÆó„ÄÅÊé®ÁêÜ„ÄÅ‰ø°ÊÅØÊèêÂèñ„ÄÅÈ¢ÑÊµãËØÜÂà´ÂíåÂü∫‰∫éÁü•ËØÜÁöÑÈóÆÈ¢òÂõûÁ≠îÁ≠â‰∫î‰∏™Áª¥Â∫¶„ÄÇÈÄöËøáÂØπ25‰∏™Ê®°ÂûãÁöÑËØÑ‰º∞ÔºåÂèëÁé∞Ê≤°Êúâ‰∏Ä‰∏™Ê®°ÂûãÂú®ÊâÄÊúâ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰∏î‰∏çÂêåÊ®°ÂûãÂú®ÂêÑ‰∏™‰ªªÂä°‰∏≠ÁöÑËÉΩÂäõÊ®°ÂºèÂêÑÂºÇ„ÄÇËØ•Âü∫ÂáÜÊµãËØï‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõ‰∫Ü‰∏•Ê†º‰∏î‰∏éÂïÜ‰∏öÁõ∏ÂÖ≥ÁöÑËØÑ‰º∞Ê†áÂáÜ„ÄÇ","title":"BizFinBenchÔºöÈáëËûçÂ∫îÁî®‰∏≠ÁöÑËØ≠Ë®ÄÊ®°ÂûãËØÑ‰º∞Êñ∞Âü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='BizFinBenchÊòØ‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÈáëËûçÂ∫îÁî®‰∏≠ÁöÑÂü∫ÂáÜÊµãËØï„ÄÇÂÆÉÂåÖÂê´6781‰∏™ÁªèËøáËâØÂ•ΩÊ†áÊ≥®ÁöÑÊü•ËØ¢ÔºåÊ∂µÁõñ‰∫ÜÊï∞ÂÄºËÆ°ÁÆó„ÄÅÊé®ÁêÜ„ÄÅ‰ø°ÊÅØÊèêÂèñ„ÄÅÈ¢ÑÊµãËØÜÂà´ÂíåÂü∫‰∫éÁü•ËØÜÁöÑÈóÆÈ¢òÂõûÁ≠îÁ≠â‰∫î‰∏™Áª¥Â∫¶„ÄÇÈÄöËøáÂØπ25‰∏™Ê®°ÂûãÁöÑËØÑ‰º∞ÔºåÂèëÁé∞Ê≤°Êúâ‰∏Ä‰∏™Ê®°ÂûãÂú®ÊâÄÊúâ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰∏î‰∏çÂêåÊ®°ÂûãÂú®ÂêÑ‰∏™‰ªªÂä°‰∏≠ÁöÑËÉΩÂäõÊ®°ÂºèÂêÑÂºÇ„ÄÇËØ•Âü∫ÂáÜÊµãËØï‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõ‰∫Ü‰∏•Ê†º‰∏î‰∏éÂïÜ‰∏öÁõ∏ÂÖ≥ÁöÑËØÑ‰º∞Ê†áÂáÜ„ÄÇ', title='BizFinBenchÔºöÈáëËûçÂ∫îÁî®‰∏≠ÁöÑËØ≠Ë®ÄÊ®°ÂûãËØÑ‰º∞Êñ∞Âü∫ÂáÜ'))
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#survey", "#agents", "#architecture", "#interpretability"], "emoji": "ü§ñ", "ru": {"title": "–í–∞–π–± vs –ê–≥–µ–Ω—Ç: –ù–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –ò–ò-–∞—Å—Å–∏—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–≤—É—Ö –ø–∞—Ä–∞–¥–∏–≥–º –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –ò–ò: –≤–∞–π–±-–∫–æ–¥–∏
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#agi", "#open_source", "#benchmark", "#agents", "#multimodal", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "InfantAgent-Next - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—â–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –º–æ
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#interpretability", "#training", "#reasoning", "#data", "#architecture"], "emoji": "üß†", "ru": {"title": "–ü—Ä–∏–Ω—Ü–∏–ø –ø–æ–∫—Ä—ã—Ç–∏—è: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –æ–±–ª–∞—Å—Ç–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞–ª–∏–∑
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#benchmark", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –ø–æ —à–∞–≥–∞–º: MinD –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É LRM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Multi-Turn Decomposition (MinD) –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#science", "#rlhf", "#benchmark", "#multimodal", "#optimization"], "emoji": "üß™", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –Ω–∞ —Å—Ç—Ä–∞–∂–µ –Ω–∞—É—á–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: –æ—Ç –∏–¥–µ–∏ –∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LL
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#ethics", "#reasoning", "#audio", "#benchmark", "#multimodal", "#survey"], "emoji": "üéß", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LALM). –ê–≤—Ç–æ—Ä—ã –≤—ã
[27.05.2025 04:19] Using data from previous issue: {"categories": ["#training", "#reasoning", "#data", "#dataset", "#optimization", "#interpretability"], "emoji": "üß†", "ru": {"title": "–§—É–Ω–∫—Ü–∏–∏ –≤–ª–∏—è–Ω–∏—è —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Å–µ–∫—Ä–µ—Ç—ã –æ–±—É—á–µ–Ω–∏—è LLM –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–ª–∏—è–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∫–ª–∞–¥–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ
[27.05.2025 04:19] Loading Chinese text from previous data.
[27.05.2025 04:19] Renaming data file.
[27.05.2025 04:19] Renaming previous data. hf_papers.json to ./d/2025-05-27.json
[27.05.2025 04:19] Saving new data file.
[27.05.2025 04:19] Generating page.
[27.05.2025 04:19] Renaming previous page.
[27.05.2025 04:19] Renaming previous data. index.html to ./d/2025-05-27.html
[27.05.2025 04:19] [Experimental] Generating Chinese page for reading.
[27.05.2025 04:19] Chinese vocab [{'word': 'ËøÅÁßªÂ≠¶‰π†', 'pinyin': 'qiƒÅn y√≠ xu√© x√≠', 'trans': 'transfer learning'}, {'word': 'ÂàÜÁ±ª‰ªªÂä°', 'pinyin': 'fƒìn l√®i r√®n w√π', 'trans': 'classification task'}, {'word': 'ÂèÇÊï∞', 'pinyin': 'cƒÅn sh«î', 'trans': 'parameter'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pre-training'}, {'word': 'ÊñáÊú¨ÁºñÁ†ÅÂô®', 'pinyin': 'w√©n bƒõn biƒÅn m«é q√¨', 'trans': 'text encoder'}, {'word': 'ÁõÆÊ†á‰ª§Áâå', 'pinyin': 'm√π biƒÅo l√¨ng p√°i', 'trans': 'target token'}, {'word': 'ÂµåÂÖ•', 'pinyin': 'qi√†n r√π', 'trans': 'embedding'}, {'word': 'Ë°®Áé∞Âá∫Ëâ≤', 'pinyin': 'bi«éo xi√†n ch≈´ s√®', 'trans': 'perform excellently'}, {'word': 'Êâ©Â±ïËßÑÂæã', 'pinyin': 'ku√≤ zh«én guƒ´ l«ú', 'trans': 'expansion pattern'}]
[27.05.2025 04:19] Renaming previous Chinese page.
[27.05.2025 04:19] Renaming previous data. zh.html to ./d/2025-05-26_zh_reading_task.html
[27.05.2025 04:19] Writing Chinese reading task.
[27.05.2025 04:19] Writing result.
[27.05.2025 04:19] Renaming log file.
[27.05.2025 04:19] Renaming previous data. log.txt to ./logs/2025-05-27_last_log.txt
