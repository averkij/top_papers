[27.05.2025 18:15] Read previous papers.
[27.05.2025 18:15] Generating top page (month).
[27.05.2025 18:15] Writing top page (month).
[27.05.2025 19:09] Read previous papers.
[27.05.2025 19:09] Get feed.
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17894
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19147
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19457
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19297
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19250
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16348
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20258
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19914
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19815
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18545
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20259
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19209
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18675
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19439
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18536
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19590
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18601
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19949
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19752
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20256
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20139
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19788
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18759
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13136
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20152
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18822
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20046
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19602
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16972
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13426
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19955
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19640
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19443
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19427
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19103
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20278
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19731
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19223
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18773
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17652
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.10887
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20254
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19386
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18384
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15804
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20294
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19630
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19084
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19056
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18926
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18323
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16984
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16886
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16312
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19706
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18283
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15957
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19800
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.19440
[27.05.2025 19:09] Extract page data from URL. URL: https://huggingface.co/papers/2505.19415
[27.05.2025 19:09] Extract page data from URL. URL: https://huggingface.co/papers/2505.18454
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18291
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12737
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20290
[27.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.20225
[27.05.2025 19:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.05.2025 19:09] No deleted papers detected.
[27.05.2025 19:09] Downloading and parsing papers (pdf, html). Total: 65.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17894.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17894.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17894.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19147.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19147.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19147.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19457.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19457.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19457.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19297.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19297.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19297.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19250.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19250.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19250.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16348.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16348.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16348.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.20258.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.20258.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.20258.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19914.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19914.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19914.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19815.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19815.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19815.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18545.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18545.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18545.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.20259.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.20259.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.20259.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19209.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19209.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19209.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18675.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18675.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18675.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19439.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19439.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19439.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18536.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18536.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18536.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19590.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19590.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19590.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18601.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18601.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18601.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19949.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19949.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19949.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19752.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19752.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19752.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.20256.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.20256.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.20256.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.20139.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.20139.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.20139.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19788.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19788.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19788.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18759.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18759.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18759.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.13136.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.13136.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.13136.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.20152.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.20152.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.20152.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18822.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18822.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18822.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.20046.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.20046.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.20046.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19602.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19602.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19602.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16972.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16972.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16972.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.13426.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.13426.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.13426.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19955.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19955.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19955.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19640.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19640.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19640.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19443.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19443.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19443.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19427.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19427.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19427.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19103.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19103.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19103.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.20278.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.20278.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.20278.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19731.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19731.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19731.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19223.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19223.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19223.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18773.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18773.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18773.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17652.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17652.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17652.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.10887.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.10887.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.10887.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.20254.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.20254.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.20254.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19386.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19386.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19386.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18384.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18384.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18384.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.15804.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.15804.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.15804.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.20294.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.20294.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.20294.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19630.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19630.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19630.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19084.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19084.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19084.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19056.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19056.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19056.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18926.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18926.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18926.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18323.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18323.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18323.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16984.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16984.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16984.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16886.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16886.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16886.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16312.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16312.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16312.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19706.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19706.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19706.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18283.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18283.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18283.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.15957.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.15957.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.15957.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19800.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19800.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19800.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19440.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.19440.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.19440.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.19415.
[27.05.2025 19:09] Downloading paper 2505.19415 from http://arxiv.org/pdf/2505.19415v1...
[27.05.2025 19:09] Extracting affiliations from text.
[27.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 1 5 1 4 9 1 . 5 0 5 2 : r MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models Hang Hua1 Ziyun Zeng1 Yizhi Song2 Liu He 2 1 University of Rochester Daniel Aliaga 2 Wei Xiong 3 2 Purdue University Yunlong Tang 1 Jiebo Luo1 3 NVIDIA {hhua2, jluo}@cs.rochester.edu, {zzeng24, ytang37}@ur.rochester.edu, {song630, he425, aliaga}@purdue.edu, wxiong@nvidia.com Figure 1: Overview of MMIG-Bench. We present unified multi-modal benchmark which contains 1,750 multi-view reference images with 4,850 richly annotated text prompts, covering both text-only and image-text-conditioned generation. We also propose comprehensive three-level evaluation framework: low-level of artifacts and identity preservation, mid-level of VQA-based Aspect Matching Score, and high-level of aesthetics and human preferencesdelivers holistic and interpretable scores. "
[27.05.2025 19:09] Response: ```python
["University of Rochester", "Purdue University", "NVIDIA"]
```
[27.05.2025 19:09] Deleting PDF ./assets/pdf/2505.19415.pdf.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18454.
[27.05.2025 19:09] Downloading paper 2505.18454 from http://arxiv.org/pdf/2505.18454v1...
[27.05.2025 19:09] Extracting affiliations from text.
[27.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 4 5 4 8 1 . 5 0 5 2 : r a Zhenrui Yue1, Bowen Jin1, Huimin Zeng1, Honglei Zhuang2, Zhen Qin2, Jinsung Yoon2, Lanyu Shang3, Jiawei Han1, Dong Wang1 1University of Illinois Urbana-Champaign, 2Google, 3LMU {zhenrui3,bowenj4,huiminz3,lshang3,hanj,dwang24}@illinois.edu, {hlz,zhenqin,jinsungyoon}@google.com, lanyu.shang@lmu.edu "
[27.05.2025 19:09] Response: ```python
["University of Illinois Urbana-Champaign", "Google", "LMU"]
```
[27.05.2025 19:09] Deleting PDF ./assets/pdf/2505.18454.pdf.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18291.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18291.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18291.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.12737.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.12737.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.12737.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.20290.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.20290.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.20290.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.20225.
[27.05.2025 19:09] Extra JSON file exists (./assets/json/2505.20225.json), skip PDF parsing.
[27.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.20225.json), skip HTML parsing.
[27.05.2025 19:09] Success.
[27.05.2025 19:09] Enriching papers with extra data.
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 0. Mutarjim is a compact Arabic-English translation model that outperforms larger models on established benchmarks and achieves state-of-the-art performance on a new comprehensive Tarjama-25 benchmark.  					AI-generated summary 				 We introduce Mutarjim, a compact yet powerful language model for bidi...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 1. The rapid advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on model-centric scaling through increasing parameter counts from millions to hundreds of billions to drive performance gains. However, as we approach hardware limits on model size, the dominan...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 2. BizFinBench is a benchmark for evaluating large language models in financial applications, revealing distinct performance patterns across various tasks.  					AI-generated summary 				 Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical do...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 3. A new method using a pre-trained generative model helps construct a high-impact SFT dataset, Alchemist, which improves the generative quality of text-to-image models while maintaining diversity.  					AI-generated summary 				 Pre-training equips text-to-image (T2I) models with broad world knowledge...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 4. PATS enhances LLM efficiency by dynamically adjusting reasoning strategies based on task difficulty, leveraging PRMs and Beam Search.  					AI-generated summary 				 Current large-language models (LLMs) typically adopt a fixed reasoning strategy, either simple or complex, for all questions, regardle...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 5. MEMENTO evaluates personalized memory utilization in embodied agents, revealing limitations in understanding user semantics and routines.  					AI-generated summary 				 Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. H...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 6. Adaptive Reasoning Model (ARM) uses Ada-GRPO to reduce token usage and improve efficiency across different reasoning modes.  					AI-generated summary 				 While large reasoning models demonstrate strong performance on complex tasks, they lack the ability to adjust reasoning token usage based on tas...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 7. Enigmata is a comprehensive suite for improving LLMs in puzzle reasoning through scalable multi-task RL training, leading to better performance on benchmarks and advanced math tasks.  					AI-generated summary 				 Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at advance...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 8. LLM reasoning is understood through a meta-learning framework, treating reasoning as pseudo-gradient descent and questions as individual tasks, which enhances generalization and provides practical insights for improvement.  					AI-generated summary 				 We propose a novel framework for comprehendin...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 9. Large language models (LLMs) often exhibit strong biases, e.g, against women or in favor of the number 7. We investigate whether LLMs would be able to output less biased answers when allowed to observe their prior answers to the same question in a multi-turn conversation. To understand which types o...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 10. A lifecycle safety alignment framework employs a Meta-Attacker and Defender to adapt LLMs to novel jailbreaking strategies, improving robustness in deployment.  					AI-generated summary 				 LLMs have made impressive progress, but their growing capabilities also expose them to highly flexible jailb...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 11. A method is proposed to generate detailed scientific hypotheses using LLMs by defining and optimizing a latent reward landscape, outperforming baselines in benchmark evaluations.  					AI-generated summary 				 Large language models (LLMs) have shown promise in automating scientific hypothesis gener...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 12. Multimodal large language models (MLLMs) have recently achieved significant progress in visual tasks, including semantic scene understanding and text-image alignment, with reasoning variants enhancing performance on complex tasks involving mathematics and logic. However, their capacity for reasoning...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 13. Large Language Models have achieved remarkable success in natural language processing tasks, with Reinforcement Learning playing a key role in adapting them to specific applications. However, obtaining ground truth answers for training LLMs in mathematical problem-solving is often challenging, costl...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 14. Standing in 2025, at a critical juncture in the pursuit of Artificial General Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated significant potential in enhancing the reasoning capability of large language models (LLMs) and has led to the development of cutting-edge AI models such...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 15. Intuitor, a Reinforcement Learning from Internal Feedback method, uses self-certainty as a reward signal to enable unsupervised learning of large language models, achieving performance comparable to GRPO on benchmarks and superior generalization.  					AI-generated summary 				 Training large langua...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 16. Flex-Judge uses minimal textual reasoning data to generalize across multiple modalities and evaluation formats, outperforming state-of-the-art models in multimodal evaluations.  					AI-generated summary 				 Human-generated reward signals are critical for aligning generative models with human prefe...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 17. Influence functions are used to attribute LLMs' reasoning in math and coding to individual training elements, revealing cross-domain effects and enabling a reweighting strategy that improves model accuracy.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remarkable rea...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 18. A novel framework, Discrete Markov Bridge, is introduced for discrete data modeling with Matrix Learning and Score Learning components, demonstrating superior performance compared to existing methods on Text8 and CIFAR-10 datasets.  					AI-generated summary 				 Discrete diffusion has recently emer...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 19. An end-to-end reinforcement learning framework, Omni-R1, achieves superior performance in long-horizon video-audio reasoning and fine-grained pixel understanding tasks by combining global reasoning and detail understanding systems.  					AI-generated summary 				 Long-horizon video-audio reasoning a...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 20. StructEval benchmarks Large Language Models for generating and converting structured outputs, highlighting performance gaps and challenges in producing visual content.  					AI-generated summary 				 As Large Language Models (LLMs) become integral to software development workflows, their ability to ...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 21. Multi-Turn Decomposition improves efficiency in large reasoning models by breaking down chain-of-thought into manageable turns, reducing token usage and latency while maintaining performance.  					AI-generated summary 				 Large Reasoning Models (LRMs) are criticized for the excessively lengthy Cha...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 22. Data-centric distillation, including data augmentation, selection, and mixing, offers a promising path to creating smaller, more efficient student Large Language Models (LLMs) that retain strong reasoning abilities. However, there still lacks a comprehensive benchmark to systematically assess the ef...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 23. ModernGBERT and LL\"aMmlein2Vec, new German encoder models, outperform existing models in terms of performance and parameter-efficiency across various NLP tasks.  					AI-generated summary 				 Despite the prominence of decoder-only language models, encoders remain crucial for resource-constrained a...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 24. A novel hard negative contrastive learning framework improves geometric reasoning in Large Multimodal Models, significantly enhancing their performance compared to existing models.  					AI-generated summary 				 Benefiting from contrastively trained visual encoders on large-scale natural scene imag...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 25. AdaCtrl, a novel framework, dynamically adjusts reasoning length based on problem difficulty and user control, improving performance and reducing response length across various datasets compared to standard training methods.  					AI-generated summary 				 Modern large reasoning models demonstrate i...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 26. REARANK, a reinforcement learning-enhanced large language model for listwise reasoning,outperforms baseline models and even surpasses GPT-4 on reasoning-intensive benchmarks with minimal data.  					AI-generated summary 				 We present REARANK, a large language model (LLM)-based listwise reasoning r...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 27. ScaleKV compresses the KV cache in Visual Autoregressive models by differentiating drafters and refiners across transformer layers, reducing memory consumption while maintaining high fidelity.  					AI-generated summary 				 Visual Autoregressive (VAR) modeling has garnered significant attention for...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 28. Recent advances in Automatic Speech Recognition (ASR) have been largely fueled by massive speech corpora. However, extending coverage to diverse languages with limited resources remains a formidable challenge. This paper introduces Speech Back-Translation, a scalable pipeline that improves multiling...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 29. VLM-Gym addresses the "knowing-doing" gap in Vision-Language Models by training them in a diverse RL environment, leading to enhanced perception and reasoning abilities that surpass existing models in interactive games.  					AI-generated summary 				 Vision-Language Models (VLMs) excel in many dire...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 30. MLR-Bench evaluates AI agents in scientific research through modular stages, revealing that while LLMs perform well in ideation and writing, coding agents often produce unreliable experimental results.  					AI-generated summary 				 Recent advancements in AI agents have demonstrated their growing p...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 31. A reinforcement learning-guided training paradigm enhances large language models' reasoning efficiency and performance for multi-hop questions by interleaving thinking and answering.  					AI-generated summary 				 Long chain-of-thought (CoT) significantly enhances large language models' (LLM) reaso...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 32. A review contrasts vibe coding and agentic coding paradigms, highlighting their differences in interaction, autonomy, and application areas in AI-assisted software development.  					AI-generated summary 				 This review presents a comprehensive analysis of two emerging paradigms in AI-assisted soft...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 33. WINA, a training-free sparse activation framework for large language models, improves inference accuracy by considering hidden state magnitudes and weight matrix norms, outperforming existing methods.  					AI-generated summary 				 The growing computational demands of large language models (LLMs) m...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 34. WHISTRESS is an alignment-free method for sentence stress detection trained on synthetic data, outperforming existing methods and generalizing well to diverse benchmarks.  					AI-generated summary 				 Spoken language conveys meaning not only through words but also through intonation, emotion, and ...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 35. Large language models excel at pattern matching, yet often fall short in systematic compositional generalization. We propose the coverage principle: a data-centric framework showing that models relying primarily on pattern matching for compositional tasks cannot reliably generalize beyond substituti...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 36. Nash Mirror Prox is an online algorithm for Nash Learning from Human Feedback that achieves linear convergence to the Nash equilibrium and is applicable for fine-tuning language models.  					AI-generated summary 				 Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on rewa...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 37. VRPO is a variance-reduced preference optimization framework for Masked Diffusion Models that significantly enhances their alignment with human preferences and performance across various benchmarks.  					AI-generated summary 				 While Masked Diffusion Models (MDMs), such as LLaDA, present a promis...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 38. State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training reference models (e.g., f...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 39. CDAS addresses low sample efficiency in reinforcement learning by aligning model competence with problem difficulty, improving both accuracy and efficiency in mathematical benchmarks.  					AI-generated summary 				 Reinforcement learning exhibits potential in enhancing the reasoning abilities of la...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 40. InfantAgent-Next is a multimodal agent that integrates tool-based and vision models in a modular architecture to solve various benchmarks, including OSWorld, GAIA, and SWE-Bench.  					AI-generated summary 				 This paper introduces InfantAgent-Next, a generalist agent capable of interacting with co...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 41. Prioritizing feature consistency in sparse autoencoders improves mechanistic interpretability of neural networks by ensuring reliable and interpretable features.  					AI-generated summary 				 Sparse Autoencoders (SAEs) are a prominent tool in mechanistic interpretability (MI) for decomposing neura...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 42. Force prompts enable video generation models to simulate realistic physical interactions using pretrained models and force conditioning from Blender-generated videos.  					AI-generated summary 				 Recent advances in video generation models have sparked interest in world models capable of simulatin...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 43. Adversaries can significantly enhance foundation model capabilities in offensive cybersecurity with limited computational resources, underscoring the need for dynamic threat model assessments.  					AI-generated summary 				 Foundation models are increasingly becoming better autonomous programmers, ...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 44. STAR-R1, a novel RL framework with a fine-grained reward mechanism, enhances spatial reasoning in multimodal large language models by addressing limitations in traditional SFT and sparse-reward RL.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have demonstrated remarkable ...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 45. A new benchmark and policy, GLEAM-Bench and GLEAM, improve the scalability and reliability of active mapping in complex environments through semantic representations and efficient exploration strategies.  					AI-generated summary 				 Generalizable active mapping in complex unknown environments rem...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 46. Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Existing systems rely on a one-way information transmission mode where patients must fully desc...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 47. Jodi, a diffusion framework using a linear diffusion transformer and role switch mechanism, unifies visual generation and understanding, performing joint, controllable, and perceptual tasks effectively across multiple visual domains.  					AI-generated summary 				 Visual generation and understandin...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 48. Modifying models to generate justified refusals through fine-tuning on an extended-refusal dataset mitigates ablation attacks while maintaining high refusal rates and general performance.  					AI-generated summary 				 Large language models (LLMs) are typically aligned to comply with safety guideli...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 49. A hybrid neural physics system with diffusion-based control achieves real-time, interactive fluid simulations with low latency and high fidelity.  					AI-generated summary 				 We propose a neural physics system for real-time, interactive fluid simulations. Traditional physics-based methods, while ...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 50. For nearly a decade the academic community has investigated backdoors in neural networks, primarily focusing on classification tasks where adversaries manipulate the model prediction. While demonstrably malicious, the immediate real-world impact of such prediction-altering attacks has remained uncle...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 51. A new post-training method, Unified Fine-Tuning (UFT), improves upon supervised and reinforcement fine-tuning for large language models by combining their benefits, achieving better generalization and faster convergence.  					AI-generated summary 				 Post-training has demonstrated its importance i...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 52. With the growing success of reasoning models across complex natural language tasks, researchers in the Information Retrieval (IR) community have begun exploring how similar reasoning capabilities can be integrated into passage rerankers built on Large Language Models (LLMs). These methods typically ...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 53. EquivPruner reduces token consumption and improves reasoning accuracy by pruning semantically equivalent actions in LLM searches, leveraging a new dataset for mathematical equivalence.  					AI-generated summary 				 Large Language Models (LLMs) excel at complex reasoning through search algorithms, ...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 54. PathFinder-PRM, a hierarchical and error-aware Process Reward Model, improves mathematical problem-solving by fine-grained error classification and step correctness estimation, achieving state-of-the-art PRMScore with reduced data usage.  					AI-generated summary 				 Large Language Models (LLMs) a...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 55. TAGS, a test-time framework combining generalist and specialist models with hierarchical retrieval and reliability scoring, enhances medical LLM reasoning without fine-tuning.  					AI-generated summary 				 Recent advances such as Chain-of-Thought prompting have significantly improved large languag...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 56. A survey proposes a systematic taxonomy for evaluating large audio-language models across dimensions including auditory awareness, knowledge reasoning, dialogue ability, and fairness, to address fragmented benchmarks in the field.  					AI-generated summary 				 With advancements in large audio-lang...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 57. A framework leveraging Large Language Models (LLMs) is introduced for automatic metadata extraction from scientific papers, with a focus on datasets from languages other than Arabic.  					AI-generated summary 				 Metadata extraction is essential for cataloging and preserving datasets, enabling eff...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 58. This paper studies the emergence of interpretable categorical features within large language models (LLMs), analyzing their behavior across training checkpoints (time), transformer layers (space), and varying model sizes (scale). Using sparse autoencoders for mechanistic interpretability, we identif...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 59. A comprehensive benchmark, MMIG-Bench, evaluates multi-modal image generators using text prompts and reference images, providing detailed insights through low-level, mid-level, and high-level metrics.  					AI-generated summary 				 Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 60. Hybrid reasoning policy optimization (HRPO) leverages reinforcement learning to integrate latent reasoning with large language models, enhancing performance in knowledge- and reasoning-intensive tasks while maintaining interpretability.  					AI-generated summary 				 Recent advances in large langua...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 61. A new benchmark, InstructPart, and a task-oriented part segmentation dataset are introduced to evaluate and improve the performance of Vision-Language Models in real-world contexts.  					AI-generated summary 				 Large multimodal foundation models, particularly in the domains of language and vision...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 62. Option-aware Temporally Abstracted (OTA) value learning improves offline goal-conditioned reinforcement learning performance by refining the high-level policy through better advantage estimates in long-horizon settings.  					AI-generated summary 				 Offline goal-conditioned reinforcement learning ...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 63. EgoZero learns robust manipulation policies for robots using in-the-wild human demonstrations and zero robot data, enabling zero-shot transfer across diverse tasks.  					AI-generated summary 				 Despite recent progress in general purpose robotics, robot policies still lag far behind basic human ca...
[27.05.2025 19:09] ********************************************************************************
[27.05.2025 19:09] Abstract 64. FLAME-MoE is an open-source research suite for MoE architectures in LLMs, providing tools to investigate scaling, routing, and expert behavior with reproducible experiments.  					AI-generated summary 				 Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt M...
[27.05.2025 19:09] Read previous papers.
[27.05.2025 19:09] Generating reviews via LLM API.
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#dataset", "#benchmark", "#machine_translation", "#multilingual", "#small_models", "#open_source"], "emoji": "", "ru": {"title": "  -    - ", "desc": "Mutarjim -       
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#data", "#math", "#optimization", "#long_context", "#survey"], "emoji": "", "ru": {"title": " :    ", "desc": "               (
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#benchmark", "#science", "#reasoning", "#dataset", "#open_source"], "emoji": "", "ru": {"title": "BizFinBench:    LLM  ", "desc": "BizFinBench -          (LLM)   .    67
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#synthetic", "#open_source", "#training", "#data"], "emoji": "", "ru": {"title": " :      ", "desc": "              .
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#reasoning", "#math", "#inference", "#optimization"], "emoji": "", "ru": {"title": "  LLM:   ", "desc": "          (LLM)   PATS.   
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#agents", "#interpretability", "#multimodal", "#agi", "#reasoning"], "emoji": "", "ru": {"title": "  -:   ", "desc": "MEMENTO -      ,      
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#inference", "#optimization", "#training"], "emoji": "", "ru": {"title": "      ", "desc": "   (ARM)  Ada-GRPO        
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#math", "#training", "#rl", "#optimization", "#benchmark", "#dataset"], "emoji": "", "ru": {"title": "Enigmata:      ", "desc": "Enigmata -           
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#math", "#training"], "emoji": "", "ru": {"title": " LLM  -:     ", "desc": "            (LLM) 
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#data", "#ethics", "#hallucinations", "#benchmark", "#rlhf"], "emoji": "", "ru": {"title": "       ", "desc": "        (LLM)     
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#inference", "#training", "#alignment", "#security"], "emoji": "", "ru": {"title": "        ", "desc": "           (LLM)    
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#science", "#rlhf", "#benchmark", "#multimodal", "#optimization"], "emoji": "", "ru": {"title": "     :    ", "desc": "           (LL
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#benchmark", "#cv", "#open_source"], "emoji": "", "ru": {"title": "ReasonMap:       ", "desc": "ReasonMap -            
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#math", "#rl", "#optimization", "#training"], "emoji": "", "ru": {"title": "     ", "desc": "                
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#multimodal", "#training", "#reasoning", "#benchmark", "#rlhf", "#agi", "#rl"], "emoji": "", "ru": {"title": "RFT:       -", "desc": "         (RFT)   
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#rlhf", "#optimization"], "emoji": "", "ru": {"title": " :     ", "desc": "             Intuitor.  
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#rlhf", "#alignment", "#benchmark", "#multimodal", "#transfer_learning"], "emoji": "", "ru": {"title": "    -     ", "desc": "Flex-Judge -     ,  
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#reasoning", "#data", "#dataset", "#optimization", "#interpretability"], "emoji": "", "ru": {"title": "     LLM   ", "desc": "        
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#data", "#benchmark", "#diffusion", "#dataset", "#optimization", "#architecture"], "emoji": "", "ru": {"title": "  :      ", "desc": "   Discrete Markov Bridge   
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#video", "#hallucinations", "#benchmark", "#multimodal", "#optimization"], "emoji": "", "ru": {"title": "  :        ", "desc": "Omni-R1 -        
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#open_source", "#benchmark"], "emoji": "", "ru": {"title": "StructEval:     ", "desc": "StructEval -           (LLM)  
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#benchmark", "#optimization"], "emoji": "", "ru": {"title": "   : MinD   LRM", "desc": "   Multi-Turn Decomposition (MinD)      
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#transfer_learning", "#dataset", "#optimization", "#training"], "emoji": "", "ru": {"title": "       ", "desc": "   DC-CoT -  -    
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#multilingual", "#benchmark", "#open_source", "#dataset", "#training", "#low_resource", "#architecture"], "emoji": "", "ru": {"title": "     : ModernGBERT  LL\"aMmlein2Vec", "desc": "      Mode
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#training", "#reasoning", "#dataset", "#open_source"], "emoji": "", "ru": {"title": "        ", "desc": "        
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#dataset"], "emoji": "", "ru": {"title": "       ", "desc": "AdaCtrl -   ,             
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#interpretability", "#optimization", "#reasoning", "#rlhf", "#rl"], "emoji": "", "ru": {"title": "REARANK:         ", "desc": "REARANK -    ,    
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#inference", "#optimization", "#architecture"], "emoji": "", "ru": {"title": "ScaleKV:      -", "desc": "ScaleKV -     KV-    .        
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#data", "#multilingual", "#low_resource", "#dataset", "#audio", "#synthetic"], "emoji": "", "ru": {"title": "       ASR", "desc": "    Speech Back-Translation     
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#multimodal", "#training", "#reasoning", "#open_source", "#agents", "#games", "#optimization", "#rl"], "emoji": "", "ru": {"title": "       Vision-Language Models   RL", "desc": "  VLM-Gym -    
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#agents", "#science", "#benchmark", "#open_source"], "emoji": "", "ru": {"title": "MLR-Bench:      ", "desc": "MLR-Bench -      -    .   201  
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#math", "#rlhf", "#training", "#rl", "#optimization"], "emoji": "", "ru": {"title": "  :      ", "desc": "         (LLM)     
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#survey", "#agents", "#architecture", "#interpretability"], "emoji": "", "ru": {"title": " vs :   - ", "desc": "             : -
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#inference", "#training", "#open_source", "#optimization", "#architecture"], "emoji": "", "ru": {"title": "WINA:       ", "desc": "WINA -         ,   
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#synthetic", "#audio", "#data", "#benchmark", "#alignment"], "emoji": "", "ru": {"title": "WHISTRESS:       ", "desc": "WHISTRESS -      ,     
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#interpretability", "#training", "#reasoning", "#data", "#architecture"], "emoji": "", "ru": {"title": " :       ", "desc": "          
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#games", "#rlhf", "#training", "#alignment", "#optimization"], "emoji": "", "ru": {"title": "Nash Mirror Prox:       ", "desc": "  Nash Mirror Prox - -       
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#rl", "#benchmark", "#architecture", "#optimization", "#alignment", "#rlhf"], "emoji": "", "ru": {"title": "       ", "desc": "VRPO -       
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#data", "#dataset", "#benchmark", "#security", "#leakage"], "emoji": "", "ru": {"title": "       :   ", "desc": "      (MIA)   LiRA  
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#rl", "#training", "#reasoning", "#math", "#optimization"], "emoji": "", "ru": {"title": " : CDAS     ", "desc": "CDAS (Competence-Difficulty Alignment Sampling) -        , 
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#agi", "#open_source", "#benchmark", "#agents", "#multimodal", "#architecture"], "emoji": "", "ru": {"title": "      ", "desc": "InfantAgent-Next -   ,     
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#interpretability", "#training", "#math", "#architecture"], "emoji": "", "ru": {"title": "  -     ", "desc": "           
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#video", "#synthetic", "#games", "#data"], "emoji": "", "ru": {"title": "       ", "desc": "   ' '       .  
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#cybersecurity", "#agents", "#security"], "emoji": "", "ru": {"title": "     ", "desc": " ,            
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#rl", "#multimodal", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": "          ", "desc": "STAR-R1 -           
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#benchmark", "#transfer_learning", "#3d", "#robotics", "#synthetic"], "emoji": "", "ru": {"title": "   : GLEAM   ", "desc": "GLEAM-Bench  GLEAM -          
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#reasoning", "#science", "#healthcare", "#dataset", "#games", "#optimization", "#rl"], "emoji": "", "ru": {"title": "  :      ", "desc": "  DoctorAgent-RL -       
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#diffusion", "#cv", "#multimodal", "#open_source"], "emoji": "", "ru": {"title": "Jodi:       ", "desc": "Jodi -   ,      .    
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#security", "#dataset", "#alignment", "#ethics"], "emoji": "", "ru": {"title": "       ", "desc": "       ,       
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#3d", "#agents", "#diffusion", "#open_source", "#optimization"], "emoji": "", "ru": {"title": "        ", "desc": "          . 
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#leakage", "#inference", "#security", "#architecture"], "emoji": "", "ru": {"title": "  :     ", "desc": "       ,     
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization"], "emoji": "", "ru": {"title": "UFT:    SFT  RFT    ", "desc": "   -   - Unified Fine-Tuning (UFT).    supervised fine-tun
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": "  :     ", "desc": "              (
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#data", "#dataset", "#optimization", "#training"], "emoji": "", "ru": {"title": "EquivPruner:     LLM", "desc": "EquivPruner -          (LLM)   ,  . 
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#training", "#math", "#hallucinations", "#dataset", "#optimization"], "emoji": "", "ru": {"title": "      PathFinder-PRM", "desc": "PathFinder-PRM -      ,  , 
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#multimodal", "#healthcare", "#optimization", "#reasoning", "#training"], "emoji": "", "ru": {"title": "TAGS:        LLM", "desc": "TAGS -          (LLM)  
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#ethics", "#reasoning", "#audio", "#benchmark", "#multimodal", "#survey"], "emoji": "", "ru": {"title": "    - -", "desc": "       -  (LALM).  
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#data", "#multilingual", "#science", "#open_source", "#low_resource"], "emoji": "", "ru": {"title": "MOLE:        ", "desc": "  MOLE,     
[27.05.2025 19:09] Using data from previous issue: {"categories": ["#interpretability", "#training", "#architecture"], "emoji": "", "ru": {"title": "      ", "desc": "           (LLM).   
[27.05.2025 19:09] Querying the API.
[27.05.2025 19:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A comprehensive benchmark, MMIG-Bench, evaluates multi-modal image generators using text prompts and reference images, providing detailed insights through low-level, mid-level, and high-level metrics.  					AI-generated summary 				 Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and Gemini 2.5 Pro excel at following complex instructions, editing images and maintaining concept consistency. However, they are still evaluated by disjoint toolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning, and customized image generation benchmarks that overlook compositional semantics and common knowledge. We propose MMIG-Bench, a comprehensive Multi-Modal Image Generation Benchmark that unifies these tasks by pairing 4,850 richly annotated text prompts with 1,750 multi-view reference images across 380 subjects, spanning humans, animals, objects, and artistic styles. MMIG-Bench is equipped with a three-level evaluation framework: (1) low-level metrics for visual artifacts and identity preservation of objects; (2) novel Aspect Matching Score (AMS): a VQA-based mid-level metric that delivers fine-grained prompt-image alignment and shows strong correlation with human judgments; and (3) high-level metrics for aesthetics and human preference. Using MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5 Pro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human ratings, yielding in-depth insights into architecture and data design. We will release the dataset and evaluation code to foster rigorous, unified evaluation and accelerate future innovations in multi-modal image generation.
[27.05.2025 19:09] Response: {
  "desc": "MMIG-Bench -         .     4850    1750    380  .     ,      ,   Aspect Matching Score (AMS)      ,     .   MMIG-Bench   17  ,  Gemini 2.5 Pro  FLUX.",
  "emoji": "",
  "title": "MMIG-Bench:      "
}
[27.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A comprehensive benchmark, MMIG-Bench, evaluates multi-modal image generators using text prompts and reference images, providing detailed insights through low-level, mid-level, and high-level metrics.  					AI-generated summary 				 Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and Gemini 2.5 Pro excel at following complex instructions, editing images and maintaining concept consistency. However, they are still evaluated by disjoint toolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning, and customized image generation benchmarks that overlook compositional semantics and common knowledge. We propose MMIG-Bench, a comprehensive Multi-Modal Image Generation Benchmark that unifies these tasks by pairing 4,850 richly annotated text prompts with 1,750 multi-view reference images across 380 subjects, spanning humans, animals, objects, and artistic styles. MMIG-Bench is equipped with a three-level evaluation framework: (1) low-level metrics for visual artifacts and identity preservation of objects; (2) novel Aspect Matching Score (AMS): a VQA-based mid-level metric that delivers fine-grained prompt-image alignment and shows strong correlation with human judgments; and (3) high-level metrics for aesthetics and human preference. Using MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5 Pro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human ratings, yielding in-depth insights into architecture and data design. We will release the dataset and evaluation code to foster rigorous, unified evaluation and accelerate future innovations in multi-modal image generation."

[27.05.2025 19:09] Response: ```python
['BENCHMARK', 'MULTIMODAL', 'DATASET']
```
[27.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A comprehensive benchmark, MMIG-Bench, evaluates multi-modal image generators using text prompts and reference images, providing detailed insights through low-level, mid-level, and high-level metrics.  					AI-generated summary 				 Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and Gemini 2.5 Pro excel at following complex instructions, editing images and maintaining concept consistency. However, they are still evaluated by disjoint toolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning, and customized image generation benchmarks that overlook compositional semantics and common knowledge. We propose MMIG-Bench, a comprehensive Multi-Modal Image Generation Benchmark that unifies these tasks by pairing 4,850 richly annotated text prompts with 1,750 multi-view reference images across 380 subjects, spanning humans, animals, objects, and artistic styles. MMIG-Bench is equipped with a three-level evaluation framework: (1) low-level metrics for visual artifacts and identity preservation of objects; (2) novel Aspect Matching Score (AMS): a VQA-based mid-level metric that delivers fine-grained prompt-image alignment and shows strong correlation with human judgments; and (3) high-level metrics for aesthetics and human preference. Using MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5 Pro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human ratings, yielding in-depth insights into architecture and data design. We will release the dataset and evaluation code to foster rigorous, unified evaluation and accelerate future innovations in multi-modal image generation."

[27.05.2025 19:09] Response: ```python
["SURVEY", "OPTIMIZATION", "OPEN_SOURCE"]
```
[27.05.2025 19:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces MMIG-Bench, a new benchmark designed to evaluate multi-modal image generators that use both text prompts and reference images. It addresses the limitations of existing evaluation methods by providing a unified framework that includes low-level, mid-level, and high-level metrics. The benchmark features a large dataset with 4,850 annotated text prompts and 1,750 reference images, allowing for comprehensive assessments of model performance. By testing 17 advanced models, the authors validate their metrics against human ratings, aiming to enhance the evaluation process in multi-modal image generation.","title":"Unifying Evaluation for Multi-Modal Image Generators"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces MMIG-Bench, a new benchmark designed to evaluate multi-modal image generators that use both text prompts and reference images. It addresses the limitations of existing evaluation methods by providing a unified framework that includes low-level, mid-level, and high-level metrics. The benchmark features a large dataset with 4,850 annotated text prompts and 1,750 reference images, allowing for comprehensive assessments of model performance. By testing 17 advanced models, the authors validate their metrics against human ratings, aiming to enhance the evaluation process in multi-modal image generation.', title='Unifying Evaluation for Multi-Modal Image Generators'))
[27.05.2025 19:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MMIG-Bench4,8501,750MMIG-BenchAspect Matching ScoreAMS17MMIG-Bench","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MMIG-Bench4,8501,750MMIG-BenchAspect Matching ScoreAMS17MMIG-Bench', title=''))
[27.05.2025 19:09] Querying the API.
[27.05.2025 19:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Hybrid reasoning policy optimization (HRPO) leverages reinforcement learning to integrate latent reasoning with large language models, enhancing performance in knowledge- and reasoning-intensive tasks while maintaining interpretability.  					AI-generated summary 				 Recent advances in large language models (LLMs) have introduced latent reasoning as a promising alternative to autoregressive reasoning. By performing internal computation with hidden states from previous steps, latent reasoning benefit from more informative features rather than sampling a discrete chain-of-thought (CoT) path. Yet latent reasoning approaches are often incompatible with LLMs, as their continuous paradigm conflicts with the discrete nature of autoregressive generation. Moreover, these methods rely on CoT traces for training and thus fail to exploit the inherent reasoning patterns of LLMs. In this work, we explore latent reasoning by leveraging the intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid latent reasoning approach that (1) integrates prior hidden states into sampled tokens with a learnable gating mechanism, and (2) initializes training with predominantly token embeddings while progressively incorporating more hidden features. This design maintains LLMs' generative capabilities and incentivizes hybrid reasoning using both discrete and continuous representations. In addition, the hybrid HRPO introduces stochasticity into latent reasoning via token sampling, thereby enabling RL-based optimization without requiring CoT trajectories. Extensive evaluations across diverse benchmarks show that HRPO outperforms prior methods in both knowledge- and reasoning-intensive tasks. Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing behaviors like cross-lingual patterns and shorter completion lengths, highlighting the potential of our RL-based approach and offer insights for future work in latent reasoning.
[27.05.2025 19:09] Response: {
  "desc": "        (HRPO),             . HRPO           ,     .      ,    ,     .  ,  HRPO       ,   - .",
  "emoji": "",
  "title": "    :    "
}
[27.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Hybrid reasoning policy optimization (HRPO) leverages reinforcement learning to integrate latent reasoning with large language models, enhancing performance in knowledge- and reasoning-intensive tasks while maintaining interpretability.  					AI-generated summary 				 Recent advances in large language models (LLMs) have introduced latent reasoning as a promising alternative to autoregressive reasoning. By performing internal computation with hidden states from previous steps, latent reasoning benefit from more informative features rather than sampling a discrete chain-of-thought (CoT) path. Yet latent reasoning approaches are often incompatible with LLMs, as their continuous paradigm conflicts with the discrete nature of autoregressive generation. Moreover, these methods rely on CoT traces for training and thus fail to exploit the inherent reasoning patterns of LLMs. In this work, we explore latent reasoning by leveraging the intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid latent reasoning approach that (1) integrates prior hidden states into sampled tokens with a learnable gating mechanism, and (2) initializes training with predominantly token embeddings while progressively incorporating more hidden features. This design maintains LLMs' generative capabilities and incentivizes hybrid reasoning using both discrete and continuous representations. In addition, the hybrid HRPO introduces stochasticity into latent reasoning via token sampling, thereby enabling RL-based optimization without requiring CoT trajectories. Extensive evaluations across diverse benchmarks show that HRPO outperforms prior methods in both knowledge- and reasoning-intensive tasks. Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing behaviors like cross-lingual patterns and shorter completion lengths, highlighting the potential of our RL-based approach and offer insights for future work in latent reasoning."

[27.05.2025 19:09] Response: ```python
["RL", "RLHF", "MULTILINGUAL", "TRAINING"]
```
[27.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Hybrid reasoning policy optimization (HRPO) leverages reinforcement learning to integrate latent reasoning with large language models, enhancing performance in knowledge- and reasoning-intensive tasks while maintaining interpretability.  					AI-generated summary 				 Recent advances in large language models (LLMs) have introduced latent reasoning as a promising alternative to autoregressive reasoning. By performing internal computation with hidden states from previous steps, latent reasoning benefit from more informative features rather than sampling a discrete chain-of-thought (CoT) path. Yet latent reasoning approaches are often incompatible with LLMs, as their continuous paradigm conflicts with the discrete nature of autoregressive generation. Moreover, these methods rely on CoT traces for training and thus fail to exploit the inherent reasoning patterns of LLMs. In this work, we explore latent reasoning by leveraging the intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid latent reasoning approach that (1) integrates prior hidden states into sampled tokens with a learnable gating mechanism, and (2) initializes training with predominantly token embeddings while progressively incorporating more hidden features. This design maintains LLMs' generative capabilities and incentivizes hybrid reasoning using both discrete and continuous representations. In addition, the hybrid HRPO introduces stochasticity into latent reasoning via token sampling, thereby enabling RL-based optimization without requiring CoT trajectories. Extensive evaluations across diverse benchmarks show that HRPO outperforms prior methods in both knowledge- and reasoning-intensive tasks. Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing behaviors like cross-lingual patterns and shorter completion lengths, highlighting the potential of our RL-based approach and offer insights for future work in latent reasoning."

[27.05.2025 19:09] Response: ```python
['REASONING', 'INTERPRETABILITY', 'OPTIMIZATION']
```
[27.05.2025 19:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Hybrid Reasoning Policy Optimization (HRPO) is a novel approach that combines reinforcement learning with large language models (LLMs) to enhance reasoning capabilities. It integrates latent reasoning by utilizing prior hidden states and a learnable gating mechanism, allowing for more informative feature extraction. Unlike traditional methods that rely on discrete chain-of-thought paths, HRPO maintains the generative nature of LLMs while introducing stochasticity through token sampling. This results in improved performance on knowledge- and reasoning-intensive tasks, while also preserving interpretability and revealing interesting behaviors in the models.","title":"Enhancing Reasoning in Language Models with Hybrid Optimization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Hybrid Reasoning Policy Optimization (HRPO) is a novel approach that combines reinforcement learning with large language models (LLMs) to enhance reasoning capabilities. It integrates latent reasoning by utilizing prior hidden states and a learnable gating mechanism, allowing for more informative feature extraction. Unlike traditional methods that rely on discrete chain-of-thought paths, HRPO maintains the generative nature of LLMs while introducing stochasticity through token sampling. This results in improved performance on knowledge- and reasoning-intensive tasks, while also preserving interpretability and revealing interesting behaviors in the models.', title='Enhancing Reasoning in Language Models with Hybrid Optimization'))
[27.05.2025 19:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HRPOHRPOHRPO","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HRPOHRPOHRPO', title=''))
[27.05.2025 19:10] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#robotics", "#benchmark", "#survey"], "emoji": "", "ru": {"title": "  :     ", "desc": "    InstructPart       , 
[27.05.2025 19:10] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#optimization", "#reasoning", "#benchmark"], "emoji": "", "ru": {"title": "      ", "desc": "   Option-aware Temporally Abstracted (OTA)      
[27.05.2025 19:10] Using data from previous issue: {"categories": ["#agents", "#optimization", "#robotics", "#transfer_learning"], "emoji": "", "ru": {"title": "     ", "desc": "EgoZero -  ,       ,     
[27.05.2025 19:10] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#training", "#architecture", "#optimization"], "emoji": "", "ru": {"title": "FLAME-MoE:     Mixture-of-Experts  LLM", "desc": "FLAME-MoE -       Mixture-of-Experts (MoE)  
[27.05.2025 19:10] Loading Chinese text from previous data.
[27.05.2025 19:10] Renaming data file.
[27.05.2025 19:10] Renaming previous data. hf_papers.json to ./d/2025-05-27.json
[27.05.2025 19:10] Saving new data file.
[27.05.2025 19:10] Generating page.
[27.05.2025 19:10] Renaming previous page.
[27.05.2025 19:10] Renaming previous data. index.html to ./d/2025-05-27.html
[27.05.2025 19:10] [Experimental] Generating Chinese page for reading.
[27.05.2025 19:10] Chinese vocab [{'word': '', 'pinyin': 'to ln', 'trans': 'discuss'}, {'word': '', 'pinyin': 'd xng', 'trans': 'large-scale'}, {'word': '', 'pinyin': 'y yn m xng', 'trans': 'language model'}, {'word': '', 'pinyin': 'du m ti', 'trans': 'multimodal'}, {'word': '', 'pinyin': 'kui s', 'trans': 'rapid'}, {'word': '', 'pinyin': 'f zhn', 'trans': 'development'}, {'word': '', 'pinyin': 'zh yo', 'trans': 'main'}, {'word': '', 'pinyin': 'tng gu', 'trans': 'through'}, {'word': '', 'pinyin': 'zng ji', 'trans': 'increase'}, {'word': '', 'pinyin': 'cn sh', 'trans': 'parameter'}, {'word': '', 'pinyin': 'sh ling', 'trans': 'quantity'}, {'word': '', 'pinyin': 't go', 'trans': 'improve'}, {'word': '', 'pinyin': 'xng nng', 'trans': 'performance'}, {'word': '', 'pinyin': 'yng jin', 'trans': 'hardware'}, {'word': '', 'pinyin': 'xin zh', 'trans': 'limit'}, {'word': '', 'pinyin': 'z zh y l', 'trans': 'self-attention'}, {'word': '', 'pinyin': 'chng bn', 'trans': 'cost'}, {'word': '', 'pinyin': 'png lng', 'trans': 'bottleneck'}, {'word': '', 'pinyin': 'yn ji', 'trans': 'research'}, {'word': '', 'pinyin': 'zhng din', 'trans': 'focus'}, {'word': '', 'pinyin': 'm xng y su', 'trans': 'model compression'}, {'word': '', 'pinyin': 'zhun xing', 'trans': 'turn to'}, {'word': '', 'pinyin': 'sh j y su', 'trans': 'data compression'}, {'word': '', 'pinyin': 'lng pi', 'trans': 'token'}, {'word': '', 'pinyin': 'y su', 'trans': 'compression'}, {'word': '', 'pinyin': 'jin sho', 'trans': 'reduce'}, {'word': '', 'pinyin': 'xn lin', 'trans': 'training'}, {'word': '', 'pinyin': 'tu l', 'trans': 'inference'}, {'word': '', 'pinyin': 'gu chng', 'trans': 'process'}, {'word': '', 'pinyin': 'xio l', 'trans': 'efficiency'}, {'word': '', 'pinyin': 'zu zh', 'trans': 'author'}, {'word': '', 'pinyin': 'fn x', 'trans': 'analyze'}, {'word': '', 'pinyin': 'chng shng xi wn', 'trans': 'long context'}, {'word': '', 'pinyin': 'sh xu kung ji', 'trans': 'mathematical framework'}, {'word': '', 'pinyin': 'tn to', 'trans': 'explore'}, {'word': '', 'pinyin': 'yu sh', 'trans': 'advantage'}, {'word': '', 'pinyin': 'tio zhn', 'trans': 'challenge'}]
[27.05.2025 19:10] Renaming previous Chinese page.
[27.05.2025 19:10] Renaming previous data. zh.html to ./d/2025-05-26_zh_reading_task.html
[27.05.2025 19:10] Writing Chinese reading task.
[27.05.2025 19:10] Writing result.
[27.05.2025 19:10] Renaming log file.
[27.05.2025 19:10] Renaming previous data. log.txt to ./logs/2025-05-27_last_log.txt
