[06.01.2026 03:43] Read previous papers.
[06.01.2026 03:43] Generating top page (month).
[06.01.2026 03:43] Writing top page (month).
[06.01.2026 04:41] Read previous papers.
[06.01.2026 04:41] Get feed.
[06.01.2026 04:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.01739
[06.01.2026 04:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.01425
[06.01.2026 04:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02204
[06.01.2026 04:41] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20578
[06.01.2026 04:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02256
[06.01.2026 04:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02356
[06.01.2026 04:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.01046
[06.01.2026 04:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02358
[06.01.2026 04:41] Extract page data from URL. URL: https://huggingface.co/papers/2601.02346
[06.01.2026 04:41] Extract page data from URL. URL: https://huggingface.co/papers/2601.02314
[06.01.2026 04:41] Get page data from previous paper. URL: https://huggingface.co/papers/2601.01576
[06.01.2026 04:41] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[06.01.2026 04:41] No deleted papers detected.
[06.01.2026 04:41] Downloading and parsing papers (pdf, html). Total: 11.
[06.01.2026 04:41] Downloading and parsing paper https://huggingface.co/papers/2601.01739.
[06.01.2026 04:41] Extra JSON file exists (./assets/json/2601.01739.json), skip PDF parsing.
[06.01.2026 04:41] Paper image links file exists (./assets/img_data/2601.01739.json), skip HTML parsing.
[06.01.2026 04:41] Success.
[06.01.2026 04:41] Downloading and parsing paper https://huggingface.co/papers/2601.01425.
[06.01.2026 04:41] Extra JSON file exists (./assets/json/2601.01425.json), skip PDF parsing.
[06.01.2026 04:41] Paper image links file exists (./assets/img_data/2601.01425.json), skip HTML parsing.
[06.01.2026 04:41] Success.
[06.01.2026 04:41] Downloading and parsing paper https://huggingface.co/papers/2601.02204.
[06.01.2026 04:41] Extra JSON file exists (./assets/json/2601.02204.json), skip PDF parsing.
[06.01.2026 04:41] Paper image links file exists (./assets/img_data/2601.02204.json), skip HTML parsing.
[06.01.2026 04:41] Success.
[06.01.2026 04:41] Downloading and parsing paper https://huggingface.co/papers/2512.20578.
[06.01.2026 04:41] Extra JSON file exists (./assets/json/2512.20578.json), skip PDF parsing.
[06.01.2026 04:41] Paper image links file exists (./assets/img_data/2512.20578.json), skip HTML parsing.
[06.01.2026 04:41] Success.
[06.01.2026 04:41] Downloading and parsing paper https://huggingface.co/papers/2601.02256.
[06.01.2026 04:41] Extra JSON file exists (./assets/json/2601.02256.json), skip PDF parsing.
[06.01.2026 04:41] Paper image links file exists (./assets/img_data/2601.02256.json), skip HTML parsing.
[06.01.2026 04:41] Success.
[06.01.2026 04:41] Downloading and parsing paper https://huggingface.co/papers/2601.02356.
[06.01.2026 04:41] Extra JSON file exists (./assets/json/2601.02356.json), skip PDF parsing.
[06.01.2026 04:41] Paper image links file exists (./assets/img_data/2601.02356.json), skip HTML parsing.
[06.01.2026 04:41] Success.
[06.01.2026 04:41] Downloading and parsing paper https://huggingface.co/papers/2601.01046.
[06.01.2026 04:41] Extra JSON file exists (./assets/json/2601.01046.json), skip PDF parsing.
[06.01.2026 04:41] Paper image links file exists (./assets/img_data/2601.01046.json), skip HTML parsing.
[06.01.2026 04:41] Success.
[06.01.2026 04:41] Downloading and parsing paper https://huggingface.co/papers/2601.02358.
[06.01.2026 04:41] Extra JSON file exists (./assets/json/2601.02358.json), skip PDF parsing.
[06.01.2026 04:41] Paper image links file exists (./assets/img_data/2601.02358.json), skip HTML parsing.
[06.01.2026 04:41] Success.
[06.01.2026 04:41] Downloading and parsing paper https://huggingface.co/papers/2601.02346.
[06.01.2026 04:41] Downloading paper 2601.02346 from https://arxiv.org/pdf/2601.02346v1...
[06.01.2026 04:41] Extracting affiliations from text.
[06.01.2026 04:41] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2026-01-06 Falcon-H1R: Pushing the Reasoning Frontiers with Hybrid Model for Efficient Test-Time Scaling Falcon LLM Team1 Iheb Chaabane Puneesh Khanna Slim Frikha Shi Hu Abdalgader Abubaker Reda Alami Mikhail Lubinets Mohamed El Amine Seddik Hakim Hacid Suhail Mohmad https://huggingface.co/tiiuae [Falcon-H1R collection] Abstract This work introduces Falcon-H1R, 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2 to 7 larger across variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance. Figure 1: DeepConf@512 (Fu et al., 2025a) average results over AIME24, AIME25, AMOBench, and GPQA-Diamond (Detailed results in Table 7). Falcon-H1R-7B achieves exceptional performance by pushing the reasoning frontiers in 3 dimensions: higher accuracy, token efficiency and fast inference in the parallel thinking setting."
[06.01.2026 04:41] Response: ```python
["Falcon LLM Team", "Technology Innovation Institute (TIIUAE)"]
```
[06.01.2026 04:41] Deleting PDF ./assets/pdf/2601.02346.pdf.
[06.01.2026 04:41] Success.
[06.01.2026 04:41] Downloading and parsing paper https://huggingface.co/papers/2601.02314.
[06.01.2026 04:41] Downloading paper 2601.02314 from https://arxiv.org/pdf/2601.02314v1...
[06.01.2026 04:41] Extracting affiliations from text.
[06.01.2026 04:41] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 5 ] . [ 1 4 1 3 2 0 . 1 0 6 2 : r Project Ariadne: Structural Causal Framework for Auditing Faithfulness in LLM Agents January "
[06.01.2026 04:41] Response: ```python
[]
```
[06.01.2026 04:41] Extracting affiliations from text.
[06.01.2026 04:41] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 5 ] . [ 1 4 1 3 2 0 . 1 0 6 2 : r Project Ariadne: Structural Causal Framework for Auditing Faithfulness in LLM AgentsJanuaryAs Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decisionmaking, the transparency of their reasoning processes has become critical safety concern. While Chain-ofThought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the models output or merely post-hoc rationalizations. We introduce Project Ariadne, novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodessystematically inverting logic, negating premises, and reversing factual claimsto measure the Causal Sensitivity (œï) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals persistent Faithfulness Gap. We define and detect widespread failure mode termed Causal Decoupling, where agents exhibit violation density (œÅ) of up to 0.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as Reasoning Theater while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as new benchmark for aligning stated logic with model action. The rapid proliferation of Large Language Model (LLM) agents has ushered in paradigm shift in autonomous problem-solving, moving beyond simple text generation toward complex, multi-step Chainof-Thought (CoT) reasoning. As these agents are increasingly deployed in high-stakes domainsranging from financial forecasting to autonomous scientific discoverythe transparency of their decision-making processes becomes critical safety frontier. However, significant sociotechnical challenge remains: the Faithfulness Gap. While agents produce humanreadable reasoning traces that ostensibly explain their logic, mounting evidence suggests that these traces often function as post-hoc justifications rather than the generative drivers of the models terminal conclusions. This phenomenon, which we term Causal Decoupling, represents fundamental failure in Explainable AI (XAI). When an agents internal thoughts are not causally linked to its final actions, the reasoning trace becomes hallucinated explanationa dangerous veneer of transparency that masks the underlying black-box heuristics of the transformer architecture. To address this, we introduce Project Ariadne, diagnostic framework designed to audit the causal integrity of agentic reasoning through the lens of Structural Causal Models (SCMs). Unlike traditional evaluation metrics that rely on surface-level textual similarity or static benchmarks, Project Ariadne utilizes counterfactual interventionist approach. By treating the reasoning trace as sequence of discrete causal nodes, we systematically 1 perform hard interventionsflipping logical operators, negating factual premises, or inverting causal directions. We then observe the resulting shift in the agents counterfactual answer distribution. By quantifying the Causal Sensitivity of the output to these perturbations, Ariadne provides formal mathematical basis for distinguishing between truly thinking agents and those merely performing reasoning theater. In the following sections, we define the structural equations governing our interventionist framework, establish metrics for faithfulness violations, and demonstrate the utility of Project Ariadne in detecting unfaithful reasoning across state-of-theart agentic architectures.The evaluation of faithfulness in Large Language Model (LLM) agents has emerged as primary bottleneck in AI safety. Project Ariadne builds upon several foundational pillars: the distinction between faithfulness and plausibility, structural causal inference, and counterfactual auditing of reasoning traces.A central challenge in eXplainable AI (XAI) is ensuring that an agents reasoning trace (q) reflects its actual decision-making process (faithfulness) rather than merely serving as human-convincing narrative (plausibility) [2]. Foundational work has demonstrated that reasoning traces frequently function as post-hoc justifications [3]. Recent empirical studies confirm that LLMs often arrive at conclusions through biased heuristics despite providing seemingly logical Chain-of-Thought (CoT) explanations [4], leading to what we define as Causal Decoupling. causal dependencies si = fstep(q, s<i, Œ∏) [?]. By modeling the agents response function fagent : as causal graph, we can rigorously define faithfulness as causal consistency: change in reasoning Œπ(sk) = sk must necessitate change in the final answer a(q) = aŒπ(q, k)."
[06.01.2026 04:41] Mistral response. {"id": "deed750f00fc44308b9f10b0a12696b2", "created": 1767674497, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1023, "total_tokens": 1029, "completion_tokens": 6}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[06.01.2026 04:41] Response: ```python
[]
```
[06.01.2026 04:41] Deleting PDF ./assets/pdf/2601.02314.pdf.
[06.01.2026 04:41] Success.
[06.01.2026 04:41] Downloading and parsing paper https://huggingface.co/papers/2601.01576.
[06.01.2026 04:41] Extra JSON file exists (./assets/json/2601.01576.json), skip PDF parsing.
[06.01.2026 04:41] Paper image links file exists (./assets/img_data/2601.01576.json), skip HTML parsing.
[06.01.2026 04:41] Success.
[06.01.2026 04:41] Enriching papers with extra data.
[06.01.2026 04:41] ********************************************************************************
[06.01.2026 04:41] Abstract 0. K-EXAONE is a multilingual language model with a Mixture-of-Experts architecture that achieves competitive performance on various benchmarks while supporting multiple languages and long-context windows.  					AI-generated summary 				 This technical report presents K-EXAONE, a large-scale multilingu...
[06.01.2026 04:41] ********************************************************************************
[06.01.2026 04:41] Abstract 1. A novel video face swapping framework combines image face swapping techniques with diffusion transformers and curriculum learning to achieve superior identity preservation and visual realism.  					AI-generated summary 				 Video Face Swapping (VFS) requires seamlessly injecting a source identity in...
[06.01.2026 04:41] ********************************************************************************
[06.01.2026 04:41] Abstract 2. NextFlow is a unified decoder-only autoregressive transformer that processes interleaved text-image tokens, enabling fast multimodal generation through novel next-token and next-scale prediction strategies.  					AI-generated summary 				 We present NextFlow, a unified decoder-only autoregressive tr...
[06.01.2026 04:41] ********************************************************************************
[06.01.2026 04:41] Abstract 3. Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with tr...
[06.01.2026 04:41] ********************************************************************************
[06.01.2026 04:41] Abstract 4. Visual autoregressive models face training instability due to asynchronous policy conflicts, which are addressed through a novel framework enhancing group relative policy optimization with intermediate rewards, dynamic time-step reweighting, and mask propagation algorithms.  					AI-generated summar...
[06.01.2026 04:41] ********************************************************************************
[06.01.2026 04:41] Abstract 5. Talk2Move presents a reinforcement learning-based diffusion framework that enables precise, semantically faithful spatial transformations of objects in scenes using natural language instructions.  					AI-generated summary 				 We introduce Talk2Move, a reinforcement learning (RL) based diffusion fr...
[06.01.2026 04:41] ********************************************************************************
[06.01.2026 04:41] Abstract 6. KV-Embedding enables training-free representation learning from frozen LLMs by utilizing key-value states for enhanced context access and automated layer selection.  					AI-generated summary 				 While LLMs are powerful embedding backbones, their application in training-free settings faces two stru...
[06.01.2026 04:41] ********************************************************************************
[06.01.2026 04:41] Abstract 7. VINO is a unified visual generator that uses a shared diffusion backbone with multimodal inputs to perform image and video generation and editing tasks.  					AI-generated summary 				 We present VINO, a unified visual generator that performs image and video generation and editing within a single fr...
[06.01.2026 04:41] ********************************************************************************
[06.01.2026 04:41] Abstract 8. Falcon-H1R is a 7B-parameter language model that achieves competitive reasoning performance through efficient training strategies and architectural design, enabling scalable reasoning capabilities in compact models.  					AI-generated summary 				 This work introduces Falcon-H1R, a 7B-parameter reas...
[06.01.2026 04:41] ********************************************************************************
[06.01.2026 04:41] Abstract 9. Project Ariadne uses structural causal models and counterfactual logic to evaluate the causal integrity of LLM reasoning, revealing a faithfulness gap where reasoning traces are not reliable drivers of outputs.  					AI-generated summary 				 As Large Language Model (LLM) agents are increasingly tas...
[06.01.2026 04:41] ********************************************************************************
[06.01.2026 04:41] Abstract 10. An LLM-powered agentic system for transparent, evidence-based novelty assessment in peer review that retrieves and analyzes prior work through semantic search and hierarchical taxonomy construction.  					AI-generated summary 				 Evaluating novelty is critical yet challenging in peer review, as rev...
[06.01.2026 04:41] Read previous papers.
[06.01.2026 04:41] Generating reviews via LLM API.
[06.01.2026 04:41] Using data from previous issue: {"categories": ["#benchmark", "#multilingual", "#reasoning", "#agents", "#architecture", "#long_context"], "emoji": "üåç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "desc": "K-EXAONE - —ç—Ç–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Mixture-of-Experts, —Ä
[06.01.2026 04:41] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#synthetic", "#video", "#diffusion", "#architecture", "#rl", "#training"], "emoji": "üé≠", "ru": {"title": "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –ø–æ–¥–º–µ–Ω—ã –ª–∏—Ü –≤ –≤–∏–¥–µ–æ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –ø–æ–¥–º–µ–Ω—ã –ª–∏—Ü –≤ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –∞–¥–∞–ø—Ç–∏—Ä—É–µ
[06.01.2026 04:41] Using data from previous issue: {"categories": ["#multimodal", "#video", "#rlhf", "#architecture", "#training"], "emoji": "‚ö°", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ —Å–∫–æ—Ä–æ—Å—Ç—å—é —Å–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ", "desc": "NextFlow ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –æ–±—É—á–∞–µ–º–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω –Ω–∞ 6 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞
[06.01.2026 04:41] Using data from previous issue: {"categories": ["#benchmark", "#interpretability", "#hallucinations", "#reasoning", "#inference", "#training"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Gnosis ‚Äî –º–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ–æ—Å–æ–∑–Ω–∞–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[06.01.2026 04:41] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#rl", "#cv", "#training"], "emoji": "üé®", "ru": {"title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞–º–∏ –ø–æ–ª–∏—Ç–∏–∫", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ
[06.01.2026 04:41] Using data from previous issue: {"categories": ["#rl", "#training", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫", "desc": "Talk2Move ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç—ã –≤ —Å—Ü–µ–Ω–µ –ø–æ —Ç–µ–∫—Å—Ç–æ
[06.01.2026 04:41] Using data from previous issue: {"categories": ["#multimodal", "#training"], "emoji": "üîë", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–∫—Ä—ã—Ç–æ–π –º–æ—â–∏ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—é –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ KV-Embedding –¥–ª—è –æ–±—É—á–µ–Ω–∏—è-—Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏–∑ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[06.01.2026 04:41] Using data from previous issue: {"categories": ["#multimodal", "#video", "#open_source", "#diffusion", "#architecture", "#training"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≤—Å–µ—Ö –≤–∏–¥–æ–≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞", "desc": "VINO ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –æ–±—â–µ–π –¥–∏—Ñ—Ñ—É
[06.01.2026 04:41] Querying the API.
[06.01.2026 04:41] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Falcon-H1R is a 7B-parameter language model that achieves competitive reasoning performance through efficient training strategies and architectural design, enabling scalable reasoning capabilities in compact models.  					AI-generated summary 				 This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2times to 7times larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.
[06.01.2026 04:41] Response: ```json
{
  "desc": "Falcon-H1R ‚Äî —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞ –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º –æ–±—É—á–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã–¥–∞—é—â—É—é—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—è—Å—å –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ 2-7 —Ä–∞–∑ –≤ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ö–ª—é—á–µ–≤–æ–π –≤–∫–ª–∞–¥ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (SFT –∏ RL) –∏ –≥–∏–±—Ä–∏–¥–Ω–æ-–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏—Ö –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å —Å –Ω–∏–∑–∫–æ–π –∑–∞—Ç—Ä–∞—Ç–æ–π –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ SLM –º–æ–≥—É—Ç –æ–±–µ—Å–ø–µ—á–∏—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.",
  "emoji": "‚ö°",
  "title": "–ú–æ—â–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"
}
```
[06.01.2026 04:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Falcon-H1R is a 7B-parameter language model that achieves competitive reasoning performance through efficient training strategies and architectural design, enabling scalable reasoning capabilities in compact models.  					AI-generated summary 				 This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2times to 7times larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance."

[06.01.2026 04:41] Response: ```python
['ARCHITECTURE', 'TRAINING', 'SMALL_MODELS', 'BENCHMARK', 'RL']
```
[06.01.2026 04:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Falcon-H1R is a 7B-parameter language model that achieves competitive reasoning performance through efficient training strategies and architectural design, enabling scalable reasoning capabilities in compact models.  					AI-generated summary 				 This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2times to 7times larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance."

[06.01.2026 04:41] Response: ```python
['REASONING', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[06.01.2026 04:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Falcon-H1R is a compact 7B-parameter language model designed to excel in reasoning tasks while maintaining efficiency. It achieves competitive performance by utilizing advanced training strategies and a hybrid-parallel architecture, allowing it to outperform larger models in various benchmarks. The model emphasizes the significance of data curation and targeted training methods, such as efficient supervised fine-tuning (SFT) and reinforcement learning (RL) scaling. Overall, Falcon-H1R showcases that smaller models can achieve high reasoning capabilities without the need for increased size, making them practical for complex reasoning applications.","title":"Compact Power: Reasoning Efficiency with Falcon-H1R"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Falcon-H1R is a compact 7B-parameter language model designed to excel in reasoning tasks while maintaining efficiency. It achieves competitive performance by utilizing advanced training strategies and a hybrid-parallel architecture, allowing it to outperform larger models in various benchmarks. The model emphasizes the significance of data curation and targeted training methods, such as efficient supervised fine-tuning (SFT) and reinforcement learning (RL) scaling. Overall, Falcon-H1R showcases that smaller models can achieve high reasoning capabilities without the need for increased size, making them practical for complex reasoning applications.', title='Compact Power: Reasoning Efficiency with Falcon-H1R'))
[06.01.2026 04:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Falcon-H1RÊòØ‰∏Ä‰∏™Êã•Êúâ70‰∫øÂèÇÊï∞ÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºåÈÄöËøáÈ´òÊïàÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÂíåÊû∂ÊûÑËÆæËÆ°ÔºåÂÆûÁé∞‰∫ÜÁ´û‰∫âÂäõÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãÂú®Â§ö‰∏™Êé®ÁêÜÂØÜÈõÜÂûãÂü∫ÂáÜÊµãËØï‰∏≠ÔºåË°®Áé∞Âá∫‰∏é2Âà∞7ÂÄçÊõ¥Â§ßÊ®°ÂûãÁõ∏ÂΩìÊàñÊõ¥‰ºòÁöÑÊÄßËÉΩÔºåÂ±ïÁ§∫‰∫ÜÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊΩúÂäõ„ÄÇÁ†îÁ©∂Âº∫Ë∞É‰∫ÜÊï∞ÊçÆÁ≤æÂøÉÁ≠ñÂàíÂíåÈíàÂØπÊÄßËÆ≠ÁªÉÁ≠ñÁï•ÁöÑÈáçË¶ÅÊÄßÔºåËøô‰ΩøÂæóÂú®‰∏çÂ¢ûÂä†Ê®°ÂûãËßÑÊ®°ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåFalcon-H1RÈÄöËøáÊ∑∑ÂêàÂπ∂Ë°åÊû∂ÊûÑËÆæËÆ°ÔºåÁªìÂêàÂø´ÈÄüÊé®ÁêÜÂíåÈ´òÂáÜÁ°ÆÊÄßÔºåÊé®Âä®‰∫ÜÊé®ÁêÜÊïàÁéáÁöÑÊûÅÈôê„ÄÇ","title":"Â∞èÂûãÊ®°Âûã‰πüËÉΩÂÆûÁé∞Âº∫Â§ßÊé®ÁêÜËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Falcon-H1RÊòØ‰∏Ä‰∏™Êã•Êúâ70‰∫øÂèÇÊï∞ÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºåÈÄöËøáÈ´òÊïàÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÂíåÊû∂ÊûÑËÆæËÆ°ÔºåÂÆûÁé∞‰∫ÜÁ´û‰∫âÂäõÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãÂú®Â§ö‰∏™Êé®ÁêÜÂØÜÈõÜÂûãÂü∫ÂáÜÊµãËØï‰∏≠ÔºåË°®Áé∞Âá∫‰∏é2Âà∞7ÂÄçÊõ¥Â§ßÊ®°ÂûãÁõ∏ÂΩìÊàñÊõ¥‰ºòÁöÑÊÄßËÉΩÔºåÂ±ïÁ§∫‰∫ÜÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊΩúÂäõ„ÄÇÁ†îÁ©∂Âº∫Ë∞É‰∫ÜÊï∞ÊçÆÁ≤æÂøÉÁ≠ñÂàíÂíåÈíàÂØπÊÄßËÆ≠ÁªÉÁ≠ñÁï•ÁöÑÈáçË¶ÅÊÄßÔºåËøô‰ΩøÂæóÂú®‰∏çÂ¢ûÂä†Ê®°ÂûãËßÑÊ®°ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåFalcon-H1RÈÄöËøáÊ∑∑ÂêàÂπ∂Ë°åÊû∂ÊûÑËÆæËÆ°ÔºåÁªìÂêàÂø´ÈÄüÊé®ÁêÜÂíåÈ´òÂáÜÁ°ÆÊÄßÔºåÊé®Âä®‰∫ÜÊé®ÁêÜÊïàÁéáÁöÑÊûÅÈôê„ÄÇ', title='Â∞èÂûãÊ®°Âûã‰πüËÉΩÂÆûÁé∞Âº∫Â§ßÊé®ÁêÜËÉΩÂäõ'))
[06.01.2026 04:41] Querying the API.
[06.01.2026 04:41] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Project Ariadne uses structural causal models and counterfactual logic to evaluate the causal integrity of LLM reasoning, revealing a faithfulness gap where reasoning traces are not reliable drivers of outputs.  					AI-generated summary 				 As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While Chain-of-Thought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the model's output or merely post-hoc rationalizations. We introduce Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the Causal Sensitivity (œÜ) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent Faithfulness Gap. We define and detect a widespread failure mode termed Causal Decoupling, where agents exhibit a violation density (œÅ) of up to 0.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.
[06.01.2026 04:41] Response: ```json
{
  "desc": "Project Ariadne ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç–∏—á–µ—Å–∫—É—é –ª–æ–≥–∏–∫—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç ¬´–ø—Ä–æ–ø–∞—Å—Ç—å –≤–µ—Ä–Ω–æ—Å—Ç–∏¬ª, –≥–¥–µ —Ü–µ–ø–æ—á–∫–∏ –º—ã—à–ª–µ–Ω–∏—è –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–∏—á–∏–Ω–∞–º–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ —Å–∫–æ—Ä–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –ø–æ—Å—Ç—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é ¬´–¢–µ–∞—Ç—Ä–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π¬ª ‚Äî —è–≤–ª–µ–Ω–∏—è, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –ø—Ä–∏—Ö–æ–¥–∏—Ç –∫ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –≤—ã–≤–æ–¥–∞–º –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—É—é –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –ª–æ–≥–∏–∫—É, —á—Ç–æ –¥–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è–º–∏ —Å–∫—Ä—ã—Ç—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞ ‚Äî Ariadne Score ‚Äî –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É —è–≤–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏ –º–æ–¥–µ–ª–∏.",
  "emoji": "üß©",
  "title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Ç–µ–∞—Ç—Ä–∞ –∏–ª–ª—é–∑–∏–π: –∫–æ–≥–¥–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM ‚Äî —ç—Ç–æ —Ç–æ–ª—å–∫–æ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞"
}
```
[06.01.2026 04:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Project Ariadne uses structural causal models and counterfactual logic to evaluate the causal integrity of LLM reasoning, revealing a faithfulness gap where reasoning traces are not reliable drivers of outputs.  					AI-generated summary 				 As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While Chain-of-Thought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the model's output or merely post-hoc rationalizations. We introduce Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the Causal Sensitivity (œÜ) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent Faithfulness Gap. We define and detect a widespread failure mode termed Causal Decoupling, where agents exhibit a violation density (œÅ) of up to 0.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action."

[06.01.2026 04:41] Response: ```python
["AGENTS", "BENCHMARK", "ARCHITECTURE"]
```
[06.01.2026 04:41] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Project Ariadne uses structural causal models and counterfactual logic to evaluate the causal integrity of LLM reasoning, revealing a faithfulness gap where reasoning traces are not reliable drivers of outputs.  					AI-generated summary 				 As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While Chain-of-Thought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the model's output or merely post-hoc rationalizations. We introduce Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the Causal Sensitivity (œÜ) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent Faithfulness Gap. We define and detect a widespread failure mode termed Causal Decoupling, where agents exhibit a violation density (œÅ) of up to 0.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action."

[06.01.2026 04:41] Response: ```python
['INTERPRETABILITY', 'REASONING', 'ALIGNMENT', 'SECURITY']
```
[06.01.2026 04:41] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Project Ariadne investigates how well Large Language Models (LLMs) explain their reasoning when making decisions. It uses Structural Causal Models and counterfactual logic to check if the reasoning provided by these models is genuinely linked to their outputs or just a facade. The study finds a significant \'Faithfulness Gap\', where models often reach the same conclusions despite having conflicting internal reasoning. This indicates that the reasoning traces may not accurately reflect the decision-making process, leading to the introduction of the Ariadne Score to better assess the alignment of logic and actions in LLMs.","title":"Bridging the Faithfulness Gap in LLM Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="Project Ariadne investigates how well Large Language Models (LLMs) explain their reasoning when making decisions. It uses Structural Causal Models and counterfactual logic to check if the reasoning provided by these models is genuinely linked to their outputs or just a facade. The study finds a significant 'Faithfulness Gap', where models often reach the same conclusions despite having conflicting internal reasoning. This indicates that the reasoning traces may not accurately reflect the decision-making process, leading to the introduction of the Ariadne Score to better assess the alignment of logic and actions in LLMs.", title='Bridging the Faithfulness Gap in LLM Reasoning'))
[06.01.2026 04:42] Response: ParsedChatCompletionMessage[Article](content='{"desc":"È°πÁõÆÈòøÈáåÈòøÂæ∑‰ΩøÁî®ÁªìÊûÑÂõ†ÊûúÊ®°ÂûãÂíåÂèç‰∫ãÂÆûÈÄªËæëÊù•ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜÁöÑÂõ†ÊûúÂÆåÊï¥ÊÄßÔºåÊè≠Á§∫‰∫ÜÊé®ÁêÜËΩ®Ëøπ‰∏éËæìÂá∫‰πãÈó¥Â≠òÂú®‰ø°‰ªªÂ∑ÆË∑ù„ÄÇÂ∞ΩÁÆ°ÈìæÂºèÊÄùÁª¥ÊèêÁ§∫ÂèØ‰ª•ÁîüÊàêÂèØËØªÁöÑÊé®ÁêÜËΩ®ËøπÔºå‰ΩÜËøô‰∫õËΩ®ËøπÊòØÂê¶ÁúüÂÆûÂèçÊò†Ê®°ÂûãËæìÂá∫‰ªç‰∏çÊòéÁ°Æ„ÄÇÈÄöËøáÂØπ‰∏≠Èó¥Êé®ÁêÜËäÇÁÇπËøõË°åÂπ≤È¢ÑÔºåÈ°πÁõÆÈòøÈáåÈòøÂæ∑ÊµãÈáè‰∫ÜÁªàÁ´ØÁ≠îÊ°àÁöÑÂõ†ÊûúÊïèÊÑüÊÄßÔºåÂèëÁé∞‰∫ÜÂõ†ÊûúËß£ËÄ¶ÁöÑÊôÆÈÅçÂ§±Ë¥•Ê®°Âºè„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÂâçÁöÑÊô∫ËÉΩ‰ΩìÊû∂ÊûÑÂÆπÊòì‰∫ßÁîü‰∏çÂèØ‰ø°ÁöÑËß£ÈáäÔºåÂπ∂ÊèêÂá∫‰∫ÜÈòøÈáåÈòøÂæ∑ËØÑÂàÜ‰Ωú‰∏∫ÂØπÈΩêÈÄªËæë‰∏éÊ®°ÂûãË°å‰∏∫ÁöÑÊñ∞Âü∫ÂáÜ„ÄÇ","title":"Êè≠Á§∫Êé®ÁêÜ‰ø°‰ªªÂ∑ÆË∑ùÁöÑÈòøÈáåÈòøÂæ∑È°πÁõÆ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='È°πÁõÆÈòøÈáåÈòøÂæ∑‰ΩøÁî®ÁªìÊûÑÂõ†ÊûúÊ®°ÂûãÂíåÂèç‰∫ãÂÆûÈÄªËæëÊù•ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜÁöÑÂõ†ÊûúÂÆåÊï¥ÊÄßÔºåÊè≠Á§∫‰∫ÜÊé®ÁêÜËΩ®Ëøπ‰∏éËæìÂá∫‰πãÈó¥Â≠òÂú®‰ø°‰ªªÂ∑ÆË∑ù„ÄÇÂ∞ΩÁÆ°ÈìæÂºèÊÄùÁª¥ÊèêÁ§∫ÂèØ‰ª•ÁîüÊàêÂèØËØªÁöÑÊé®ÁêÜËΩ®ËøπÔºå‰ΩÜËøô‰∫õËΩ®ËøπÊòØÂê¶ÁúüÂÆûÂèçÊò†Ê®°ÂûãËæìÂá∫‰ªç‰∏çÊòéÁ°Æ„ÄÇÈÄöËøáÂØπ‰∏≠Èó¥Êé®ÁêÜËäÇÁÇπËøõË°åÂπ≤È¢ÑÔºåÈ°πÁõÆÈòøÈáåÈòøÂæ∑ÊµãÈáè‰∫ÜÁªàÁ´ØÁ≠îÊ°àÁöÑÂõ†ÊûúÊïèÊÑüÊÄßÔºåÂèëÁé∞‰∫ÜÂõ†ÊûúËß£ËÄ¶ÁöÑÊôÆÈÅçÂ§±Ë¥•Ê®°Âºè„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÂâçÁöÑÊô∫ËÉΩ‰ΩìÊû∂ÊûÑÂÆπÊòì‰∫ßÁîü‰∏çÂèØ‰ø°ÁöÑËß£ÈáäÔºåÂπ∂ÊèêÂá∫‰∫ÜÈòøÈáåÈòøÂæ∑ËØÑÂàÜ‰Ωú‰∏∫ÂØπÈΩêÈÄªËæë‰∏éÊ®°ÂûãË°å‰∏∫ÁöÑÊñ∞Âü∫ÂáÜ„ÄÇ', title='Êè≠Á§∫Êé®ÁêÜ‰ø°‰ªªÂ∑ÆË∑ùÁöÑÈòøÈáåÈòøÂæ∑È°πÁõÆ'))
[06.01.2026 04:42] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#agents", "#rag", "#science"], "emoji": "üîç", "ru": {"title": "–ü—Ä–æ–∑—Ä–∞—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–æ–≤–∏–∑–Ω—ã —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏", "desc": "OpenNovelty ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç –æ—Ü–µ–Ω–∫—É –Ω–æ–≤–∏–∑–Ω—ã –Ω–∞—É
[06.01.2026 04:42] Renaming data file.
[06.01.2026 04:42] Renaming previous data. hf_papers.json to ./d/2026-01-06.json
[06.01.2026 04:42] Saving new data file.
[06.01.2026 04:42] Generating page.
[06.01.2026 04:42] Renaming previous page.
[06.01.2026 04:42] Renaming previous data. index.html to ./d/2026-01-06.html
[06.01.2026 04:42] Writing result.
[06.01.2026 04:42] Renaming log file.
[06.01.2026 04:42] Renaming previous data. log.txt to ./logs/2026-01-06_last_log.txt
