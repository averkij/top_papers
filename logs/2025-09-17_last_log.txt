[17.09.2025 17:10] Read previous papers.
[17.09.2025 17:10] Generating top page (month).
[17.09.2025 17:10] Writing top page (month).
[17.09.2025 18:15] Read previous papers.
[17.09.2025 18:15] Get feed.
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13312
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13310
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13305
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13311
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13309
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13313
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13232
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.12815
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13317
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.12603
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.12341
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06079
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.12521
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.11526
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.11177
[17.09.2025 18:15] Extract page data from URL. URL: https://huggingface.co/papers/2509.10687
[17.09.2025 18:15] Get page data from previous paper. URL: https://huggingface.co/papers/2509.13177
[17.09.2025 18:15] Extract page data from URL. URL: https://huggingface.co/papers/2509.11481
[17.09.2025 18:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.09.2025 18:15] No deleted papers detected.
[17.09.2025 18:15] Downloading and parsing papers (pdf, html). Total: 18.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.13312.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.13312.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.13312.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.13310.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.13310.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.13310.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.13305.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.13305.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.13305.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.13311.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.13311.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.13311.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.13309.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.13309.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.13309.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.13313.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.13313.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.13313.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.13232.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.13232.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.13232.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.12815.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.12815.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.12815.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.13317.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.13317.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.13317.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.12603.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.12603.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.12603.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.12341.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.12341.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.12341.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.06079.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.06079.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.06079.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.12521.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.12521.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.12521.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.11526.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.11526.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.11526.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.11177.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.11177.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.11177.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.10687.
[17.09.2025 18:15] Downloading paper 2509.10687 from http://arxiv.org/pdf/2509.10687v1...
[17.09.2025 18:15] Extracting affiliations from text.
[17.09.2025 18:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . [ 1 7 8 6 0 1 . 9 0 5 2 : r Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation Hao Zhang1,2 Chun-Han Yao1 Simon Donn√©1 Narendra Ahuja2 Varun Jampani1 1Stability AI 2University of Illinois Urbana-Champaign Figure 1: Left: Stable Part Diffusion 4D (SP4D) takes monocular input video and generates novelview RGB videos (bottom-left) as well as consistent part segmentation videos across all views. Right: SPD also supports single image input and synthesizes multi-view RGB images and corresponding part decompositions. These results can be lifted to 3D to produce riggable meshes with part-aware geometry and articulated structure. "
[17.09.2025 18:15] Response: ```python
["Stability AI", "University of Illinois Urbana-Champaign"]
```
[17.09.2025 18:15] Deleting PDF ./assets/pdf/2509.10687.pdf.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.13177.
[17.09.2025 18:15] Extra JSON file exists (./assets/json/2509.13177.json), skip PDF parsing.
[17.09.2025 18:15] Paper image links file exists (./assets/img_data/2509.13177.json), skip HTML parsing.
[17.09.2025 18:15] Success.
[17.09.2025 18:15] Downloading and parsing paper https://huggingface.co/papers/2509.11481.
[17.09.2025 18:15] Downloading paper 2509.11481 from http://arxiv.org/pdf/2509.11481v1...
[17.09.2025 18:16] Extracting affiliations from text.
[17.09.2025 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"RAPTOR: Foundation Policy for Quadrotor Control Jonas Eschmann1,2,, Dario Albani2, Giuseppe Loianno1 1Department of Electrical Engineering and Computer Sciences (EECS), UC Berkeley, Berkeley, CA 94720, USA. 2Autonomous Robotics Research Center, Technology Innovation Institute, Abu Dhabi, UAE. Corresponding author. Present Address. Email: jonas.eschmann@berkeley.edu Project page: https://raptor.rl.tools Video: https://youtu.be/hVzdWRFTX3k Humans are remarkably data-efficient when adapting to new unseen conditions, like driving new car. In contrast, modern robotic control systems, like neural network policies trained using Reinforcement Learning (RL), are highly specialized for single environments. Because of this overfitting, they are known to break down even under small differences like the Simulation-to-Reality (Sim2Real) gap and require system identification and retraining for even minimal changes to the system. In this work, we present RAPTOR, method for training highly adaptive foundation policy for quadrotor control. Our method enables training single, end-to-end neural-network policy to control wide variety of quadrotors. We test 10 different real quadrotors from 32 to 2.4 kg that also differ in motor type (brushed vs. brushless), frame type (soft vs. rigid), propeller type (2/3/4-blade), and flight controller (PX4/Betaflight/Crazyflie/M5StampFly). We find that tiny, three-layer policy with only 2084 parameters is sufficient for zero-shot adaptation to wide variety of platforms. The adaptation through InContext Learning is made possible by using recurrence in the hidden layer. The policy is trained through novel Meta-Imitation Learning algorithm, where we sample 1000 quadrotors and train teacher policy for each of them using Reinforcement Learning. Subsequently, the 1000 teachers are distilled into single, adaptive student policy. We find that within milliseconds, the resulting foundation policy adapts zero-shot to unseen quadrotors. We extensively test the ca"
[17.09.2025 18:16] Response: ```python
[
    "Department of Electrical Engineering and Computer Sciences (EECS), UC Berkeley, Berkeley, CA 94720, USA",
    "Autonomous Robotics Research Center, Technology Innovation Institute, Abu Dhabi, UAE"
]
```
[17.09.2025 18:16] Deleting PDF ./assets/pdf/2509.11481.pdf.
[17.09.2025 18:16] Success.
[17.09.2025 18:16] Enriching papers with extra data.
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 0. WebWeaver, a dual-agent framework, addresses open-ended deep research challenges by integrating adaptive planning and focused synthesis to produce high-quality, reliable reports.  					AI-generated summary 				 This paper tackles open-ended deep research (OEDR), a complex challenge where AI agents m...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 1. AgentFounder, a deep research agent model incorporating Agentic Continual Pre-training, achieves state-of-the-art performance in agentic tasks while maintaining strong tool-use ability.  					AI-generated summary 				 Large language models (LLMs) have evolved into agentic systems capable of autonomo...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 2. WebSailor, a post-training methodology, enhances open-source models with systematic uncertainty reduction, matching proprietary agents' performance in complex information-seeking tasks.  					AI-generated summary 				 Transcending human cognitive limitations represents a critical frontier in LLM tra...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 3. A scalable framework and two-phase fine-tuning strategy enhance function-calling capabilities of agents in diverse environments, improving performance on agentic benchmarks.  					AI-generated summary 				 Advanced agentic intelligence is a prerequisite for deploying Large Language Models in practic...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 4. WebResearcher, a deep-research framework, enhances AI agents' knowledge synthesis by reformulating research as a Markov Decision Process and using a scalable data synthesis engine, achieving superior performance across benchmarks.  					AI-generated summary 				 Recent advances in deep-research syst...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 5. ReSum, a novel paradigm with periodic context summarization, enhances web agents' performance on knowledge-intensive tasks by overcoming context window limitations, achieving significant improvements over ReAct.  					AI-generated summary 				 Large Language Model (LLM)-based web agents demonstrate ...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 6. Single-stream Policy Optimization (SPO) improves policy-gradient training for Large Language Models by eliminating group-based issues and providing a stable, low-variance learning signal, leading to better performance and efficiency.  					AI-generated summary 				 We revisit policy-gradient optimiz...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 7. Hunyuan3D Studio automates 3D asset creation using AI, integrating neural modules to transform concept images or text into high-quality, game-ready 3D models with optimized geometry and PBR textures.  					AI-generated summary 				 The creation of high-quality 3D assets, a cornerstone of modern game...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 8. A Spatial Region 3D (SR-3D) vision-language model unifies 2D and 3D representations by enriching 2D features with 3D positional embeddings, enabling flexible region prompting and accurate spatial reasoning across frames.  					AI-generated summary 				 We present Spatial Region 3D (SR-3D) aware visi...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 9. Two methods, dynamic CoT switching and Diverse parallel-scaled RL, reduce computational cost in ATP models while maintaining performance.  					AI-generated summary 				 Large Language Models (LLMs) have recently advanced the field of Automated Theorem Proving (ATP), attaining substantial performanc...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 10. A replacement for the domain-extension step in a quantum lattice algorithm uses a pair-shift difference construction to correct periodicity issues and enforce modular linear relations efficiently.  					AI-generated summary 				 We give a simple, fully correct, and assumption-light replacement for t...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 11. A caption-assisted reasoning framework bridges visual and textual modalities, achieving top performance in multimodal reasoning tasks like SeePhys and MathVerse.  					AI-generated summary 				 Multimodal reasoning remains a fundamental challenge in artificial intelligence. Despite substantial advan...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 12. A novel method, Preference Hijacking (Phi), manipulates Multimodal Large Language Model (MLLM) response preferences using specially crafted images, demonstrating significant effectiveness across various tasks.  					AI-generated summary 				 Recently, Multimodal Large Language Models (MLLMs) have ga...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 13. A novel MIL framework, MHIM-MIL, uses masked hard instance mining with a Siamese structure and momentum teacher to improve cancer diagnosis and subtyping accuracy.  					AI-generated summary 				 Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has opened new avenues for Compu...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 14. A framework combining quantization and pruning in LLMs through error compensation achieves significant speedup and memory reduction.  					AI-generated summary 				 Recent advances in Large Language Model (LLM) compression, such as quantization and pruning, have achieved notable success. However, as...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 15. SP4D generates paired RGB and kinematic part videos from monocular inputs using a dual-branch diffusion model with spatial color encoding and BiDiFuse module, demonstrating strong generalization to diverse scenarios.  					AI-generated summary 				 We present Stable Part Diffusion 4D (SP4D), a frame...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 16. ROOM is a simulation framework that generates photorealistic bronchoscopy training data using patient CT scans, enabling the development and validation of autonomy algorithms in medical robotics.  					AI-generated summary 				 Continuum robots are advancing bronchoscopy procedures by accessing comp...
[17.09.2025 18:16] ********************************************************************************
[17.09.2025 18:16] Abstract 17. A method called RAPTOR enables a single neural network policy to adapt zero-shot to various quadrotors using Meta-Imitation Learning and In-Context Learning with recurrence in the hidden layer.  					AI-generated summary 				 Humans are remarkably data-efficient when adapting to new unseen condition...
[17.09.2025 18:16] Read previous papers.
[17.09.2025 18:16] Generating reviews via LLM API.
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#hallucinations", "#long_context", "#multimodal", "#agents"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç WebWeaver - –¥–≤—É—Ö–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#agents", "#optimization", "#training", "#agi"], "emoji": "ü§ñ", "ru": {"title": "AgentFounder: –Ω–æ–≤—ã–π —à–∞–≥ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ AgentFounder - –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –ê–≥–µ–Ω—Ç
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#training", "#optimization", "#open_source", "#agents", "#agi"], "emoji": "üß≠", "ru": {"title": "WebSailor: –æ—Ç–∫—Ä—ã—Ç—ã–π –ø—É—Ç—å –∫ —Å–≤–µ—Ä—Ö—á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º—É –ø–æ–∏—Å–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "WebSailor - —ç—Ç–æ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–µ–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#agi", "#training", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—ã–∑–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏–π –≤ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é —Å–∏—Å—Ç–µ–º—É –∏ –¥–≤—É—Ö—Ñ–∞–∑–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–æ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#science", "#dataset", "#training", "#agents", "#transfer_learning"], "emoji": "üï∏Ô∏è", "ru": {"title": "WebResearcher: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–ª—è –ò–ò", "desc": "WebResearcher - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#benchmark", "#training", "#optimization", "#long_context", "#agents"], "emoji": "üß†", "ru": {"title": "ReSum: —Ä–∞–∑–¥–≤–∏–≥–∞—è –≥—Ä–∞–Ω–∏—Ü—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ ReSum, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#training", "#optimization", "#rl", "#reasoning"], "emoji": "üöÄ", "ru": {"title": "SPO: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Single-stream Policy Optimization (SPO). SPO 
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#games", "#optimization", "#3d"], "emoji": "üéÆ", "ru": {"title": "–ò–ò —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–∏–∑–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ 3D-–∞—Å—Å–µ—Ç–æ–≤ –¥–ª—è –∏–≥—Ä", "desc": "Hunyuan3D Studio - —ç—Ç–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è 3D-–∞—Å—Å–µ—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –°–∏—Å—Ç–µ–º–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏ 
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#benchmark", "#games", "#3d", "#cv", "#multimodal", "#reasoning"], "emoji": "üîç", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ 2D –∏ 3D –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "SR-3D - —ç—Ç–æ –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è 2D –∏ 3D –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∞ –æ–±–æ–≥–∞—â–∞–µ—Ç 2D –ø—Ä–∏–∑–Ω–∞–∫–∏ 
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#training", "#inference", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ —Ç–µ–æ—Ä–µ–º: –º–µ–Ω—å—à–µ –∑–∞—Ç—Ä–∞—Ç, —Ç–∞ –∂–µ –º–æ—â–Ω–æ—Å—Ç—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–≤–∞ –º–µ—Ç–æ–¥–∞ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –≤ –º–æ–¥–µ–ª—è—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫–∞
[17.09.2025 18:16] Using data from previous issue: {"categories": [], "emoji": "üî¨", "ru": {"title": "–£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Ä–µ—à–µ—Ç–∫–∏: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –∫–≤–∞–Ω—Ç–æ–≤–æ–π —Ä–µ—à–µ—Ç–∫–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∑–∞–º–µ–Ω—É –¥–ª—è —à–∞–≥–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–æ–º–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é —Ä–∞–∑–Ω–æ—Å—Ç–∏ –ø–∞—Ä–Ω—ã—Ö —Å–¥–≤–∏–≥–æ
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#benchmark", "#multimodal"], "emoji": "üß†", "ru": {"title": "–ú–æ—Å—Ç –º–µ–∂–¥—É –∑—Ä–µ–Ω–∏–µ–º –∏ —è–∑—ã–∫–æ–º: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º—É –ò–ò", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏.
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#multimodal", "#inference", "#ethics", "#security"], "emoji": "üé≠", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –ò–ò —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏", "desc": "–ù–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Preference Hijacking (Phi) –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#benchmark", "#science", "#optimization", "#training", "#healthcare"], "emoji": "üî¨", "ru": {"title": "–ù–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Ä–∞–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å —ç–∫–∑–µ–º–ø–ª—è—Ä–∞–º–∏ (MIL) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–∑–≥–∞: —Å–∂–∞—Ç–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å–æ—á–µ—Ç–∞—é—â–∏–π –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä—É–Ω–∏–Ω–≥. –ê–≤
[17.09.2025 18:16] Querying the API.
[17.09.2025 18:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SP4D generates paired RGB and kinematic part videos from monocular inputs using a dual-branch diffusion model with spatial color encoding and BiDiFuse module, demonstrating strong generalization to diverse scenarios.  					AI-generated summary 				 We present Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike conventional part segmentation methods that rely on appearance-based semantic cues, SP4D learns to produce kinematic parts - structural components aligned with object articulation and consistent across views and time. SP4D adopts a dual-branch diffusion model that jointly synthesizes RGB frames and corresponding part segmentation maps. To simplify the architecture and flexibly enable different part counts, we introduce a spatial color encoding scheme that maps part masks to continuous RGB-like images. This encoding allows the segmentation branch to share the latent VAE from the RGB branch, while enabling part segmentation to be recovered via straightforward post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a contrastive part consistency loss to promote spatial and temporal alignment of part predictions. We demonstrate that the generated 2D part maps can be lifted to 3D to derive skeletal structures and harmonic skinning weights with few manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K, a curated dataset of over 20K rigged objects selected and processed from Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part video sequences. Experiments show that SP4D generalizes strongly to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses, producing kinematic-aware outputs suitable for downstream animation and motion-related tasks.
[17.09.2025 18:16] Response: {
  "desc": "SP4D - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞—Ä–Ω—ã—Ö RGB –∏ –∫–∏–Ω–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ —á–∞—Å—Ç–µ–π –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö–≤–µ—Ç–≤–µ–≤—É—é –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç RGB-–∫–∞–¥—Ä—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–∞—Ä—Ç—ã —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —á–∞—Å—Ç–µ–π. –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –≥–∏–±–∫–æ–≥–æ –≤–∫–ª—é—á–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —á–∞—Å—Ç–µ–π –≤–≤–µ–¥–µ–Ω–∞ —Å—Ö–µ–º–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ü–≤–µ—Ç–æ–≤–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –ú–æ–¥—É–ª—å BiDiFuse —É–ª—É—á—à–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –≤–µ—Ç–≤—è–º–∏, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—É—é –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ—Ç–µ—Ä—å –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —á–∞—Å—Ç–µ–π.",
  "emoji": "ü§ñ",
  "title": "SP4D: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∏–Ω–µ–º–∞—Ç–∏—á–µ—Å–∫–∏-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
}
[17.09.2025 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SP4D generates paired RGB and kinematic part videos from monocular inputs using a dual-branch diffusion model with spatial color encoding and BiDiFuse module, demonstrating strong generalization to diverse scenarios.  					AI-generated summary 				 We present Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike conventional part segmentation methods that rely on appearance-based semantic cues, SP4D learns to produce kinematic parts - structural components aligned with object articulation and consistent across views and time. SP4D adopts a dual-branch diffusion model that jointly synthesizes RGB frames and corresponding part segmentation maps. To simplify the architecture and flexibly enable different part counts, we introduce a spatial color encoding scheme that maps part masks to continuous RGB-like images. This encoding allows the segmentation branch to share the latent VAE from the RGB branch, while enabling part segmentation to be recovered via straightforward post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a contrastive part consistency loss to promote spatial and temporal alignment of part predictions. We demonstrate that the generated 2D part maps can be lifted to 3D to derive skeletal structures and harmonic skinning weights with few manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K, a curated dataset of over 20K rigged objects selected and processed from Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part video sequences. Experiments show that SP4D generalizes strongly to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses, producing kinematic-aware outputs suitable for downstream animation and motion-related tasks."

[17.09.2025 18:16] Response: ```python
["DATASET", "CV", "3D"]
```
[17.09.2025 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SP4D generates paired RGB and kinematic part videos from monocular inputs using a dual-branch diffusion model with spatial color encoding and BiDiFuse module, demonstrating strong generalization to diverse scenarios.  					AI-generated summary 				 We present Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike conventional part segmentation methods that rely on appearance-based semantic cues, SP4D learns to produce kinematic parts - structural components aligned with object articulation and consistent across views and time. SP4D adopts a dual-branch diffusion model that jointly synthesizes RGB frames and corresponding part segmentation maps. To simplify the architecture and flexibly enable different part counts, we introduce a spatial color encoding scheme that maps part masks to continuous RGB-like images. This encoding allows the segmentation branch to share the latent VAE from the RGB branch, while enabling part segmentation to be recovered via straightforward post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a contrastive part consistency loss to promote spatial and temporal alignment of part predictions. We demonstrate that the generated 2D part maps can be lifted to 3D to derive skeletal structures and harmonic skinning weights with few manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K, a curated dataset of over 20K rigged objects selected and processed from Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part video sequences. Experiments show that SP4D generalizes strongly to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses, producing kinematic-aware outputs suitable for downstream animation and motion-related tasks."

[17.09.2025 18:16] Response: ```python
["DIFFUSION", "SYNTHETIC"]
```
[17.09.2025 18:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces Stable Part Diffusion 4D (SP4D), a novel framework that generates paired RGB and kinematic part videos from single-camera inputs. It utilizes a dual-branch diffusion model to synthesize both RGB frames and part segmentation maps, focusing on kinematic parts that reflect object movement. A unique spatial color encoding scheme allows for flexible part counts and efficient sharing of latent representations between branches. The framework is trained on a large dataset, KinematicParts20K, and demonstrates strong generalization across various scenarios, making it effective for animation and motion tasks.","title":"Generating Kinematic Videos with SP4D: A New Era in Motion Understanding"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces Stable Part Diffusion 4D (SP4D), a novel framework that generates paired RGB and kinematic part videos from single-camera inputs. It utilizes a dual-branch diffusion model to synthesize both RGB frames and part segmentation maps, focusing on kinematic parts that reflect object movement. A unique spatial color encoding scheme allows for flexible part counts and efficient sharing of latent representations between branches. The framework is trained on a large dataset, KinematicParts20K, and demonstrates strong generalization across various scenarios, making it effective for animation and motion tasks.', title='Generating Kinematic Videos with SP4D: A New Era in Motion Understanding'))
[17.09.2025 18:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SP4DÊòØ‰∏Ä‰∏™ÁîüÊàêÈÖçÂØπRGBÂíåËøêÂä®ÈÉ®‰ª∂ËßÜÈ¢ëÁöÑÊ°ÜÊû∂Ôºå‰ΩøÁî®ÂçïÁõÆËæìÂÖ•ÂíåÂèåÂàÜÊîØÊâ©Êï£Ê®°Âûã„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÂ§ñËßÇÁöÑÂàÜÂâ≤ÊñπÊ≥ï‰∏çÂêåÔºåSP4DÂ≠¶‰π†ÁîüÊàê‰∏éÁâ©‰ΩìÂÖ≥ËäÇËøêÂä®‰∏ÄËá¥ÁöÑËøêÂä®ÈÉ®‰ª∂„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®Á©∫Èó¥È¢úËâ≤ÁºñÁ†ÅÊñπÊ°àÔºå‰ΩøÂæóÂàÜÂâ≤ÂàÜÊîØËÉΩÂ§üÂÖ±‰∫´RGBÂàÜÊîØÁöÑÊΩúÂú®ÂèòÈáèÔºåÂπ∂ÈÄöËøáÁÆÄÂçïÁöÑÂêéÂ§ÑÁêÜÊÅ¢Â§çÂàÜÂâ≤„ÄÇÂÆûÈ™åË°®ÊòéÔºåSP4DÂú®Â§öÁßçÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÔºåÈÄÇÁî®‰∫éÂä®ÁîªÂíåËøêÂä®Áõ∏ÂÖ≥‰ªªÂä°„ÄÇ","title":"ÁîüÊàêËøêÂä®ÈÉ®‰ª∂ËßÜÈ¢ëÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SP4DÊòØ‰∏Ä‰∏™ÁîüÊàêÈÖçÂØπRGBÂíåËøêÂä®ÈÉ®‰ª∂ËßÜÈ¢ëÁöÑÊ°ÜÊû∂Ôºå‰ΩøÁî®ÂçïÁõÆËæìÂÖ•ÂíåÂèåÂàÜÊîØÊâ©Êï£Ê®°Âûã„ÄÇ‰∏é‰º†ÁªüÁöÑÂü∫‰∫éÂ§ñËßÇÁöÑÂàÜÂâ≤ÊñπÊ≥ï‰∏çÂêåÔºåSP4DÂ≠¶‰π†ÁîüÊàê‰∏éÁâ©‰ΩìÂÖ≥ËäÇËøêÂä®‰∏ÄËá¥ÁöÑËøêÂä®ÈÉ®‰ª∂„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®Á©∫Èó¥È¢úËâ≤ÁºñÁ†ÅÊñπÊ°àÔºå‰ΩøÂæóÂàÜÂâ≤ÂàÜÊîØËÉΩÂ§üÂÖ±‰∫´RGBÂàÜÊîØÁöÑÊΩúÂú®ÂèòÈáèÔºåÂπ∂ÈÄöËøáÁÆÄÂçïÁöÑÂêéÂ§ÑÁêÜÊÅ¢Â§çÂàÜÂâ≤„ÄÇÂÆûÈ™åË°®ÊòéÔºåSP4DÂú®Â§öÁßçÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÔºåÈÄÇÁî®‰∫éÂä®ÁîªÂíåËøêÂä®Áõ∏ÂÖ≥‰ªªÂä°„ÄÇ', title='ÁîüÊàêËøêÂä®ÈÉ®‰ª∂ËßÜÈ¢ëÁöÑÊñ∞ÊñπÊ≥ï'))
[17.09.2025 18:16] Using data from previous issue: {"categories": ["#cv", "#synthetic", "#data", "#robotics", "#training", "#transfer_learning", "#dataset"], "emoji": "ü´Å", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –±—Ä–æ–Ω—Ö–æ—Å–∫–æ–ø–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ", "desc": "ROOM - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ —Å–∏–º—É–ª—è—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã
[17.09.2025 18:16] Querying the API.
[17.09.2025 18:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A method called RAPTOR enables a single neural network policy to adapt zero-shot to various quadrotors using Meta-Imitation Learning and In-Context Learning with recurrence in the hidden layer.  					AI-generated summary 				 Humans are remarkably data-efficient when adapting to new unseen conditions, like driving a new car. In contrast, modern robotic control systems, like neural network policies trained using Reinforcement Learning (RL), are highly specialized for single environments. Because of this overfitting, they are known to break down even under small differences like the Simulation-to-Reality (Sim2Real) gap and require system identification and retraining for even minimal changes to the system. In this work, we present RAPTOR, a method for training a highly adaptive foundation policy for quadrotor control. Our method enables training a single, end-to-end neural-network policy to control a wide variety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg that also differ in motor type (brushed vs. brushless), frame type (soft vs. rigid), propeller type (2/3/4-blade), and flight controller (PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy with only 2084 parameters is sufficient for zero-shot adaptation to a wide variety of platforms. The adaptation through In-Context Learning is made possible by using a recurrence in the hidden layer. The policy is trained through a novel Meta-Imitation Learning algorithm, where we sample 1000 quadrotors and train a teacher policy for each of them using Reinforcement Learning. Subsequently, the 1000 teachers are distilled into a single, adaptive student policy. We find that within milliseconds, the resulting foundation policy adapts zero-shot to unseen quadrotors. We extensively test the capabilities of the foundation policy under numerous conditions (trajectory tracking, indoor/outdoor, wind disturbance, poking, different propellers).
[17.09.2025 18:16] Response: {
  "desc": "–ú–µ—Ç–æ–¥ RAPTOR –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–¥–∏–Ω–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–≤–∞–¥—Ä–æ–∫–æ–ø—Ç–µ—Ä–∞–º –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–∞-–∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å—é –≤ —Å–∫—Ä—ã—Ç–æ–º —Å–ª–æ–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ –º–µ—Ç–æ–¥ –Ω–∞ 10 —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–≤–∞–¥—Ä–æ–∫–æ–ø—Ç–µ—Ä–∞—Ö —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π. –ù–µ–±–æ–ª—å—à–∞—è —Ç—Ä–µ—Ö—Å–ª–æ–π–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ —Å –≤—Å–µ–≥–æ 2084 –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –æ–∫–∞–∑–∞–ª–∞—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ —à–∏—Ä–æ–∫–æ–º—É —Å–ø–µ–∫—Ç—Ä—É –ø–ª–∞—Ç—Ñ–æ—Ä–º.",
  "emoji": "üöÅ",
  "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–≤–∞–¥—Ä–æ–∫–æ–ø—Ç–µ—Ä–∞–º–∏ —Å –ø–æ–º–æ—â—å—é –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"
}
[17.09.2025 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A method called RAPTOR enables a single neural network policy to adapt zero-shot to various quadrotors using Meta-Imitation Learning and In-Context Learning with recurrence in the hidden layer.  					AI-generated summary 				 Humans are remarkably data-efficient when adapting to new unseen conditions, like driving a new car. In contrast, modern robotic control systems, like neural network policies trained using Reinforcement Learning (RL), are highly specialized for single environments. Because of this overfitting, they are known to break down even under small differences like the Simulation-to-Reality (Sim2Real) gap and require system identification and retraining for even minimal changes to the system. In this work, we present RAPTOR, a method for training a highly adaptive foundation policy for quadrotor control. Our method enables training a single, end-to-end neural-network policy to control a wide variety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg that also differ in motor type (brushed vs. brushless), frame type (soft vs. rigid), propeller type (2/3/4-blade), and flight controller (PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy with only 2084 parameters is sufficient for zero-shot adaptation to a wide variety of platforms. The adaptation through In-Context Learning is made possible by using a recurrence in the hidden layer. The policy is trained through a novel Meta-Imitation Learning algorithm, where we sample 1000 quadrotors and train a teacher policy for each of them using Reinforcement Learning. Subsequently, the 1000 teachers are distilled into a single, adaptive student policy. We find that within milliseconds, the resulting foundation policy adapts zero-shot to unseen quadrotors. We extensively test the capabilities of the foundation policy under numerous conditions (trajectory tracking, indoor/outdoor, wind disturbance, poking, different propellers)."

[17.09.2025 18:16] Response: ```python
['AGENTS', 'RL', 'TRAINING', 'ROBOTICS', 'SMALL_MODELS']
```
[17.09.2025 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A method called RAPTOR enables a single neural network policy to adapt zero-shot to various quadrotors using Meta-Imitation Learning and In-Context Learning with recurrence in the hidden layer.  					AI-generated summary 				 Humans are remarkably data-efficient when adapting to new unseen conditions, like driving a new car. In contrast, modern robotic control systems, like neural network policies trained using Reinforcement Learning (RL), are highly specialized for single environments. Because of this overfitting, they are known to break down even under small differences like the Simulation-to-Reality (Sim2Real) gap and require system identification and retraining for even minimal changes to the system. In this work, we present RAPTOR, a method for training a highly adaptive foundation policy for quadrotor control. Our method enables training a single, end-to-end neural-network policy to control a wide variety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg that also differ in motor type (brushed vs. brushless), frame type (soft vs. rigid), propeller type (2/3/4-blade), and flight controller (PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy with only 2084 parameters is sufficient for zero-shot adaptation to a wide variety of platforms. The adaptation through In-Context Learning is made possible by using a recurrence in the hidden layer. The policy is trained through a novel Meta-Imitation Learning algorithm, where we sample 1000 quadrotors and train a teacher policy for each of them using Reinforcement Learning. Subsequently, the 1000 teachers are distilled into a single, adaptive student policy. We find that within milliseconds, the resulting foundation policy adapts zero-shot to unseen quadrotors. We extensively test the capabilities of the foundation policy under numerous conditions (trajectory tracking, indoor/outdoor, wind disturbance, poking, different propellers)."

[17.09.2025 18:16] Response: ```python
["TRANSFER_LEARNING", "OPTIMIZATION"]
```
[17.09.2025 18:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces RAPTOR, a novel method that allows a single neural network policy to adapt quickly to different quadrotors without needing retraining. It leverages Meta-Imitation Learning and In-Context Learning, incorporating recurrence in the hidden layer to enhance adaptability. By training on a diverse set of 1000 quadrotors, the method distills knowledge into a compact policy with only 2084 parameters, enabling zero-shot adaptation to new environments. Extensive testing shows that this approach maintains performance across various conditions, demonstrating significant improvements over traditional specialized models.","title":"RAPTOR: One Policy to Control Them All!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces RAPTOR, a novel method that allows a single neural network policy to adapt quickly to different quadrotors without needing retraining. It leverages Meta-Imitation Learning and In-Context Learning, incorporating recurrence in the hidden layer to enhance adaptability. By training on a diverse set of 1000 quadrotors, the method distills knowledge into a compact policy with only 2084 parameters, enabling zero-shot adaptation to new environments. Extensive testing shows that this approach maintains performance across various conditions, demonstrating significant improvements over traditional specialized models.', title='RAPTOR: One Policy to Control Them All!'))
[17.09.2025 18:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫RAPTORÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÂÖÉÊ®°‰ªøÂ≠¶‰π†Âíå‰∏ä‰∏ãÊñáÂ≠¶‰π†Ôºå‰ΩøÂçï‰∏ÄÁ•ûÁªèÁΩëÁªúÁ≠ñÁï•ËÉΩÂ§üÂú®Èõ∂Ê†∑Êú¨ÊÉÖÂÜµ‰∏ãÈÄÇÂ∫îÂ§öÁßçÂõõÊóãÁøºÊó†‰∫∫Êú∫„ÄÇ‰∏é‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï‰∏çÂêåÔºåRAPTORËÉΩÂ§üÊúâÊïàÂ∫îÂØπ‰∏çÂêåÁöÑÈ£ûË°åÂô®Á±ªÂûãÂíåÁéØÂ¢ÉÂèòÂåñÔºåÈÅøÂÖç‰∫ÜËøáÊãüÂêàÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®ÈöêËóèÂ±Ç‰∏≠ÂºïÂÖ•ÈÄíÂΩíÁªìÊûÑÔºåÂÆûÁé∞‰∫ÜÂØπÊñ∞Âπ≥Âè∞ÁöÑÂø´ÈÄüÈÄÇÂ∫î„ÄÇÂÆûÈ™åË°®ÊòéÔºå‰ΩøÁî®‰ªÖ2084‰∏™ÂèÇÊï∞ÁöÑ‰∏âÂ±ÇÂ∞èÂûãÁ≠ñÁï•ÔºåRAPTORËÉΩÂ§üÂú®ÊØ´ÁßíÁ∫ßÂà´ÂÜÖÈÄÇÂ∫îÊú™ËßÅËøáÁöÑÂõõÊóãÁøºÊó†‰∫∫Êú∫„ÄÇ","title":"RAPTORÔºöÂõõÊóãÁøºÊéßÂà∂ÁöÑÈõ∂Ê†∑Êú¨ÈÄÇÂ∫îÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫RAPTORÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÂÖÉÊ®°‰ªøÂ≠¶‰π†Âíå‰∏ä‰∏ãÊñáÂ≠¶‰π†Ôºå‰ΩøÂçï‰∏ÄÁ•ûÁªèÁΩëÁªúÁ≠ñÁï•ËÉΩÂ§üÂú®Èõ∂Ê†∑Êú¨ÊÉÖÂÜµ‰∏ãÈÄÇÂ∫îÂ§öÁßçÂõõÊóãÁøºÊó†‰∫∫Êú∫„ÄÇ‰∏é‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï‰∏çÂêåÔºåRAPTORËÉΩÂ§üÊúâÊïàÂ∫îÂØπ‰∏çÂêåÁöÑÈ£ûË°åÂô®Á±ªÂûãÂíåÁéØÂ¢ÉÂèòÂåñÔºåÈÅøÂÖç‰∫ÜËøáÊãüÂêàÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®ÈöêËóèÂ±Ç‰∏≠ÂºïÂÖ•ÈÄíÂΩíÁªìÊûÑÔºåÂÆûÁé∞‰∫ÜÂØπÊñ∞Âπ≥Âè∞ÁöÑÂø´ÈÄüÈÄÇÂ∫î„ÄÇÂÆûÈ™åË°®ÊòéÔºå‰ΩøÁî®‰ªÖ2084‰∏™ÂèÇÊï∞ÁöÑ‰∏âÂ±ÇÂ∞èÂûãÁ≠ñÁï•ÔºåRAPTORËÉΩÂ§üÂú®ÊØ´ÁßíÁ∫ßÂà´ÂÜÖÈÄÇÂ∫îÊú™ËßÅËøáÁöÑÂõõÊóãÁøºÊó†‰∫∫Êú∫„ÄÇ', title='RAPTORÔºöÂõõÊóãÁøºÊéßÂà∂ÁöÑÈõ∂Ê†∑Êú¨ÈÄÇÂ∫îÊñ∞ÊñπÊ≥ï'))
[17.09.2025 18:17] Renaming data file.
[17.09.2025 18:17] Renaming previous data. hf_papers.json to ./d/2025-09-17.json
[17.09.2025 18:17] Saving new data file.
[17.09.2025 18:17] Generating page.
[17.09.2025 18:17] Renaming previous page.
[17.09.2025 18:17] Renaming previous data. index.html to ./d/2025-09-17.html
[17.09.2025 18:17] Writing result.
[17.09.2025 18:17] Renaming log file.
[17.09.2025 18:17] Renaming previous data. log.txt to ./logs/2025-09-17_last_log.txt
