[01.12.2024 12:42] Read previous papers.
[01.12.2024 12:42] Generating top page (month).
[01.12.2024 12:42] Writing top page (month).
[01.12.2024 18:29] Read previous papers.
[01.12.2024 18:29] Get feed.
[01.12.2024 18:29] Get page data from previous paper. URL: https://huggingface.co/papers/2411.18203
[01.12.2024 18:29] Get page data from previous paper. URL: https://huggingface.co/papers/2411.18350
[01.12.2024 18:29] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17176
[01.12.2024 18:29] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17041
[01.12.2024 18:29] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17863
[01.12.2024 18:29] Get page data from previous paper. URL: https://huggingface.co/papers/2411.15640
[01.12.2024 18:29] Get page data from previous paper. URL: https://huggingface.co/papers/2411.17190
[01.12.2024 18:29] Downloading and parsing papers (pdf, html). Total: 7.
[01.12.2024 18:29] Downloading and parsing paper https://huggingface.co/papers/2411.18203.
[01.12.2024 18:29] Extra JSON file exists (./assets/json/2411.18203.json), skip PDF parsing.
[01.12.2024 18:29] Paper image links file exists (./assets/img_data/2411.18203.json), skip HTML parsing.
[01.12.2024 18:29] Success.
[01.12.2024 18:29] Downloading and parsing paper https://huggingface.co/papers/2411.18350.
[01.12.2024 18:29] Extra JSON file exists (./assets/json/2411.18350.json), skip PDF parsing.
[01.12.2024 18:29] Paper image links file exists (./assets/img_data/2411.18350.json), skip HTML parsing.
[01.12.2024 18:29] Success.
[01.12.2024 18:29] Downloading and parsing paper https://huggingface.co/papers/2411.17176.
[01.12.2024 18:29] Extra JSON file exists (./assets/json/2411.17176.json), skip PDF parsing.
[01.12.2024 18:29] Paper image links file exists (./assets/img_data/2411.17176.json), skip HTML parsing.
[01.12.2024 18:29] Success.
[01.12.2024 18:29] Downloading and parsing paper https://huggingface.co/papers/2411.17041.
[01.12.2024 18:29] Extra JSON file exists (./assets/json/2411.17041.json), skip PDF parsing.
[01.12.2024 18:29] Paper image links file exists (./assets/img_data/2411.17041.json), skip HTML parsing.
[01.12.2024 18:29] Success.
[01.12.2024 18:29] Downloading and parsing paper https://huggingface.co/papers/2411.17863.
[01.12.2024 18:29] Extra JSON file exists (./assets/json/2411.17863.json), skip PDF parsing.
[01.12.2024 18:29] Paper image links file exists (./assets/img_data/2411.17863.json), skip HTML parsing.
[01.12.2024 18:29] Success.
[01.12.2024 18:29] Downloading and parsing paper https://huggingface.co/papers/2411.15640.
[01.12.2024 18:29] Extra JSON file exists (./assets/json/2411.15640.json), skip PDF parsing.
[01.12.2024 18:29] Paper image links file exists (./assets/img_data/2411.15640.json), skip HTML parsing.
[01.12.2024 18:29] Success.
[01.12.2024 18:29] Downloading and parsing paper https://huggingface.co/papers/2411.17190.
[01.12.2024 18:29] Extra JSON file exists (./assets/json/2411.17190.json), skip PDF parsing.
[01.12.2024 18:29] Paper image links file exists (./assets/img_data/2411.17190.json), skip HTML parsing.
[01.12.2024 18:29] Success.
[01.12.2024 18:29] Enriching papers with extra data.
[01.12.2024 18:29] ********************************************************************************
[01.12.2024 18:29] Abstract 0. Vision-language models~(VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V,...
[01.12.2024 18:29] ********************************************************************************
[01.12.2024 18:29] Abstract 1. This paper introduces Virtual Try-Off (VTOFF), a novel task focused on generating standardized garment images from single photos of clothed individuals. Unlike traditional Virtual Try-On (VTON), which digitally dresses models, VTOFF aims to extract a canonical garment image, posing unique challenges...
[01.12.2024 18:29] ********************************************************************************
[01.12.2024 18:29] Abstract 2. Despite the significant advancements in text-to-image (T2I) generative models, users often face a trial-and-error challenge in practical scenarios. This challenge arises from the complexity and uncertainty of tedious steps such as crafting suitable prompts, selecting appropriate models, and configur...
[01.12.2024 18:29] ********************************************************************************
[01.12.2024 18:29] Abstract 3. Diffusion models have achieved impressive results in generative tasks like text-to-image (T2I) and text-to-video (T2V) synthesis. However, achieving accurate text alignment in T2V generation remains challenging due to the complex temporal dependency across frames. Existing reinforcement learning (RL...
[01.12.2024 18:29] ********************************************************************************
[01.12.2024 18:29] Abstract 4. In an era of information overload, manually annotating the vast and growing corpus of documents and scholarly papers is increasingly impractical. Automated keyphrase extraction addresses this challenge by identifying representative terms within texts. However, most existing methods focus on short do...
[01.12.2024 18:29] ********************************************************************************
[01.12.2024 18:29] Abstract 5. Recent advancements in large language model(LLM) performance on medical multiple choice question (MCQ) benchmarks have stimulated interest from healthcare providers and patients globally. Particularly in low-and middle-income countries (LMICs) facing acute physician shortages and lack of specialists...
[01.12.2024 18:29] ********************************************************************************
[01.12.2024 18:29] Abstract 6. We propose SelfSplat, a novel 3D Gaussian Splatting model designed to perform pose-free and 3D prior-free generalizable 3D reconstruction from unposed multi-view images. These settings are inherently ill-posed due to the lack of ground-truth data, learned geometric information, and the need to achie...
[01.12.2024 18:29] Read previous papers.
[01.12.2024 18:29] Generating reviews via LLM API.
[01.12.2024 18:29] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#reasoning", "#multimodal", "#rlhf", "#hallucinations", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "Critic-V: —É–ª—É—á—à–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Critic-V - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø
[01.12.2024 18:29] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#dataset", "#cv", "#diffusion", "#optimization"], "emoji": "üëö", "ru": {"title": "–í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–∏–º–µ—Ä–∫–∞ –Ω–∞–∏–∑–Ω–∞–Ω–∫—É: –æ—Ç —Ñ–æ—Ç–æ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –æ–¥–µ–∂–¥—ã", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Virtual Try-Off (VTOFF), –∫–æ—Ç
[01.12.2024 18:29] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#reasoning", "#multimodal", "#optimization"], "emoji": "üé®", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É: –æ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É (Automatic T2I). –ê–≤—Ç
[01.12.2024 18:29] Using data from previous issue: {"categories": ["#video", "#multimodal", "#rl", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–°–≤–æ–±–æ–¥–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –¥–ª—è —Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Free^2Guide - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –≤–∏–¥–µ–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω
[01.12.2024 18:29] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#data", "#long_context"], "emoji": "üîë", "ru": {"title": "LongKey: –ü—Ä–æ—Ä—ã–≤ –≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –∏–∑ –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LongKey - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –∏–∑ –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. LongKey –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —è–∑—ã–∫–æ–≤—É—é 
[01.12.2024 18:29] Using data from previous issue: {"categories": ["#low_resource", "#benchmark", "#ethics", "#healthcare", "#dataset", "#science"], "emoji": "üåç", "ru": {"title": "AfriMed-QA: –û—Ü–µ–Ω–∫–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–æ–π –º–µ–¥–∏—Ü–∏–Ω–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AfriMed-QA - –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –ø–∞–Ω–∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö 
[01.12.2024 18:29] Using data from previous issue: {"categories": ["#3d"], "emoji": "üß†", "ru": {"title": "SelfSplat: –ü—Ä–æ—Ä—ã–≤ –≤ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π", "desc": "SelfSplat - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å 3D Gaussian Splatting –¥–ª—è –æ–±–æ–±—â–∞–µ–º–æ–π 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ –Ω–µ–ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —è–≤–Ω—ã–µ 3D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª
[01.12.2024 18:29] Loading Chinese text from previous data.
[01.12.2024 18:29] Renaming data file.
[01.12.2024 18:29] Renaming previous data. hf_papers.json to ./d/2024-11-29.json
[01.12.2024 18:29] Saving new data file.
[01.12.2024 18:29] Generating page.
[01.12.2024 18:29] Renaming previous page.
[01.12.2024 18:29] Renaming previous data. index.html to ./d/2024-11-29.html
[01.12.2024 18:29] [Experimental] Generating Chinese page for reading.
[01.12.2024 18:29] Chinese vocab [{'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ju√©', 'trans': 'vision'}, {'word': 'ËØ≠Ë®Ä', 'pinyin': 'y«îy√°n', 'trans': 'language'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'Áß∞‰∏∫', 'pinyin': 'chƒìngw√©i', 'trans': 'called'}, {'word': 'Êó®Âú®', 'pinyin': 'zh«êz√†i', 'trans': 'aim to'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈çm√≥shu√†i', 'trans': 'multimodal'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®nw√π', 'trans': 'task'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability'}, {'word': 'Áã¨Á´ã', 'pinyin': 'd√∫l√¨', 'trans': 'independent'}, {'word': 'ÁªÑ‰ª∂', 'pinyin': 'z«îji√†n', 'trans': 'component'}, {'word': 'ÁªÑÊàê', 'pinyin': 'zh«îch√©ng', 'trans': 'composed of'}, {'word': 'Êé®ÁêÜÂô®', 'pinyin': 'tuƒ´l«êq√¨', 'trans': 'reasoner'}, {'word': 'ËØÑËÆ∫Âô®', 'pinyin': 'p√≠ngl√πnq√¨', 'trans': 'critic'}, {'word': 'Ê†πÊçÆ', 'pinyin': 'gƒìnj√π', 'trans': 'based on'}, {'word': 'ËæìÂÖ•', 'pinyin': 'sh≈´r√π', 'trans': 'input'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'}, {'word': 'Ë∑ØÂæÑ', 'pinyin': 'l√πj√¨ng', 'trans': 'path'}, {'word': 'Âª∫ËÆæÊÄß', 'pinyin': 'ji√†nsh√®x√¨ng', 'trans': 'constructive'}, {'word': 'ÊâπËØÑ', 'pinyin': 'pƒ´p√≠ng', 'trans': 'criticism'}, {'word': 'ÁªÜÂåñ', 'pinyin': 'x√¨hu√†', 'trans': 'refine'}, {'word': 'Áõ¥Êé•', 'pinyin': 'zh√≠jiƒì', 'trans': 'direct'}, {'word': 'ÂÅèÂ•Ω', 'pinyin': 'piƒÅnh√†o', 'trans': 'preference'}, {'word': '‰ºòÂåñ', 'pinyin': 'y≈çuhu√†', 'trans': 'optimization'}, {'word': 'ËøõË°å', 'pinyin': 'j√¨nx√≠ng', 'trans': 'conduct'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πnli√†n', 'trans': 'training'}, {'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìngqi√°ng', 'trans': 'enhance'}, {'word': 'ËØÑËÆ∫', 'pinyin': 'p√≠ngl√πn', 'trans': 'comment'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√©gu«í', 'trans': 'result'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«énsh√¨', 'trans': 'show'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'}, {'word': '‰ºò‰∫é', 'pinyin': 'y≈çuy√∫', 'trans': 'superior to'}, {'word': 'Áé∞Êúâ', 'pinyin': 'xi√†ny«íu', 'trans': 'existing'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': 'ÁâπÂà´', 'pinyin': 't√®bi√©', 'trans': 'especially'}, {'word': 'ÂáÜÁ°ÆÊÄß', 'pinyin': 'zh«înqu√®x√¨ng', 'trans': 'accuracy'}, {'word': 'ÊïàÁéá', 'pinyin': 'xi√†ol«ú', 'trans': 'efficiency'}]
[01.12.2024 18:29] Renaming previous Chinese page.
[01.12.2024 18:29] Renaming previous data. zh.html to ./d/2024-11-30_zh_reading_task.html
[01.12.2024 18:29] Writing Chinese reading task.
[01.12.2024 18:29] Writing result.
[01.12.2024 18:29] Renaming log file.
[01.12.2024 18:29] Renaming previous data. log.txt to ./logs/2024-12-01_last_log.txt
