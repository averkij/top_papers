[25.10.2024 08:16] [Experimental] Generating an image for paper LOGO -- Long cOntext aliGnment via efficient preference Optimization.
[25.10.2024 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'LOGO -- Long cOntext aliGnment via efficient preference Optimization' Text: 'Long-context models(LCMs) have shown great potential in processing long input sequences(even more than 100M tokens) conveniently and effectively. With significant progress, recent research has pointed out that LCMs can accurately locate token-level salient information within the context. Yet, the generation performance of these LCMs is far from satisfactory and might result in misaligned responses, such as hallucinations. To enhance the generation capability of LCMs, existing works have investigated the effects of data size and quality for both pre-training and instruction tuning. Though achieving meaningful improvement, previous methods fall short in either effectiveness or efficiency. In this paper, we introduce LOGO(Long cOntext aliGnment via efficient preference Optimization), a training strategy that first introduces preference optimization for long-context alignment. To overcome the GPU memory-bound issue caused by the long sequence, LOGO employs a reference-free preference optimization strategy and adopts a position synthesis method to construct the training data. By training with only 0.3B data on a single 8timesA800 GPU machine for 16 hours, LOGO allows the Llama-3-8B-Instruct-80K model to achieve comparable performance with GPT-4 in real-world long-context tasks while preserving the model's original capabilities on other tasks, e.g., language modeling and MMLU. Moreover, LOGO can extend the model's context window size while enhancing its generation performance.'
[25.10.2024 08:16] Response: **Prompt:** Create a linear art piece on a white background featuring an abstract representation of a long pathway composed of interlocking tokens and fragmented data streams. Include floating geometric shapes symbolizing optimization and alignment, with a surreal twist where some shapes morph into graphical representations of high-performance GPUs. Integrate a sense of depth to illustrate the concept of long contexts, with layers of varying opacity. 

**Label on an object in the image:** "LOGO - Long cOntext aliGnment via efficient preference Optimization"
[25.10.2024 08:16] Generating image by prompt: **Prompt:** Create a linear art piece on a white background featuring an abstract representation of a long pathway composed of interlocking tokens and fragmented data streams. Include floating geometric shapes symbolizing optimization and alignment, with a surreal twist where some shapes morph into graphical representations of high-performance GPUs. Integrate a sense of depth to illustrate the concept of long contexts, with layers of varying opacity. 

**Label on an object in the image:** "LOGO - Long cOntext aliGnment via efficient preference Optimization".
[25.10.2024 08:16] Saving generated image from https://fal.media/files/panda/uL8esY1Cop_LIjBN54ZXp.png to 3094f5b6fc1c0168.jpg.
[25.10.2024 08:16] [Experimental] Generating an image for paper Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss.
[25.10.2024 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss' Text: 'Contrastive loss is a powerful approach for representation learning, where larger batch sizes enhance performance by providing more negative samples to better distinguish between similar and dissimilar data. However, scaling batch sizes is constrained by the quadratic growth in GPU memory consumption, primarily due to the full instantiation of the similarity matrix. To address this, we propose a tile-based computation strategy that partitions the contrastive loss calculation into arbitrary small blocks, avoiding full materialization of the similarity matrix. Furthermore, we introduce a multi-level tiling strategy to leverage the hierarchical structure of distributed systems, employing ring-based communication at the GPU level to optimize synchronization and fused kernels at the CUDA core level to reduce I/O overhead. Experimental results show that the proposed method scales batch sizes to unprecedented levels. For instance, it enables contrastive training of a CLIP-ViT-L/14 model with a batch size of 4M or 12M using 8 or 32 A800 80GB without sacrificing any accuracy. Compared to SOTA memory-efficient solutions, it achieves a two-order-of-magnitude reduction in memory while maintaining comparable speed. The code will be made publicly available.'
[25.10.2024 08:16] Response: **Prompt:** A surreal, linear art piece on a white background depicting a fragmented memory landscape. The landscape is composed of floating geometric shapes representing data points, interspersed with ethereal tiles symbolizing the tile-based computation strategy. Visualize a distorted similarity matrix as a cascading waterfall flowing down, embodying the concept of memory constraints. Incorporate abstract representations of GPUs as large, gleaming crystals, connected by intricate lines of light symbolizing ring-based communication. In the foreground, a giant, open book with pages fluttering like wings, symbolizing knowledge and innovation, rests atop a pedestal made of binary code. 

**Label:** 'Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss'
[25.10.2024 08:16] Generating image by prompt: **Prompt:** A surreal, linear art piece on a white background depicting a fragmented memory landscape. The landscape is composed of floating geometric shapes representing data points, interspersed with ethereal tiles symbolizing the tile-based computation strategy. Visualize a distorted similarity matrix as a cascading waterfall flowing down, embodying the concept of memory constraints. Incorporate abstract representations of GPUs as large, gleaming crystals, connected by intricate lines of light symbolizing ring-based communication. In the foreground, a giant, open book with pages fluttering like wings, symbolizing knowledge and innovation, rests atop a pedestal made of binary code. 

**Label:** 'Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss'.
[25.10.2024 08:16] Saving generated image from https://fal.media/files/elephant/0HsOxpnbLpF7hSEe2vQ1T.png to 29bd02d560e09f69.jpg.
[25.10.2024 10:13] Read previous papers.
[25.10.2024 10:13] Get feed.
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.17243
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18533
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18693
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16251
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18975
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18978
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18798
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18775
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18745
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18451
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18977
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18505
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18785
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.17779
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18976
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18362
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.15580
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18958
[25.10.2024 10:13] Extract page data from URL. URL: https://huggingface.co/papers/2410.17897
[25.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18234
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 0. Contrastive loss is a powerful approach for representation learning, where larger batch sizes enhance performance by providing more negative samples to better distinguish between similar and dissimilar data. However, scaling batch sizes is constrained by the quadratic growth in GPU memory consumptio...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 1. Long-context models(LCMs) have shown great potential in processing long input sequences(even more than 100M tokens) conveniently and effectively. With significant progress, recent research has pointed out that LCMs can accurately locate token-level salient information within the context. Yet, the ge...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 2. The availability of high-quality data is one of the most important factors in improving the reasoning capability of LLMs. Existing works have demonstrated the effectiveness of creating more instruction data from seed questions or knowledge bases. Recent research indicates that continually scaling up...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 3. Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks. Meanwhile, knowledge editing has been developed as a new popular paradigm to correct the erroneous factual knowledge encoded in LLMs...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 4. We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse's distinction between finite and infinite games, we leverage recent advances in generative AI to create...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 5. We propose Framer for interactive frame interpolation, which targets producing smoothly transitioning frames between two images as per user creativity. Concretely, besides taking the start and end frames as inputs, our approach supports customizing the transition process by tailoring the trajectory ...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 6. Solving complex chart Q&A tasks requires advanced visual reasoning abilities in multimodal large language models (MLLMs). Recent studies highlight that these abilities consist of two main parts: recognizing key information from visual inputs and conducting reasoning over it. Thus, a promising approa...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 7. Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. In this work, we introduce W-Bench, the first compre...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 8. Advancements in distributed training and efficient attention mechanisms have significantly expanded the context window sizes of large language models (LLMs). However, recent work reveals that the effective context lengths of open-source LLMs often fall short, typically not exceeding half of their tr...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 9. In this report, we introduce a collection of methods to enhance reward modeling for LLMs, focusing specifically on data-centric techniques. We propose effective data selection and filtering strategies for curating high-quality open-source preference datasets, culminating in the Skywork-Reward data c...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 10. This research delves into the problem of interactive editing of human motion generation. Previous motion diffusion models lack explicit modeling of the word-level text-motion correspondence and good explainability, hence restricting their fine-grained editing ability. To address this issue, we propo...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 11. We present CCI3.0-HQ (https://huggingface.co/datasets/BAAI/CCI3-HQ), a high-quality 500GB subset of the Chinese Corpora Internet 3.0 (CCI3.0)(https://huggingface.co/datasets/BAAI/CCI3-Data), developed using a novel two-stage hybrid filtering pipeline that significantly enhances data quality. To eval...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 12. Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality, with many methods excelling across these criteria. Some recent works disclose the pitfalls of these editi...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 13. Recent advancements in multimodal fusion have witnessed the remarkable success of vision-language (VL) models, which excel in various multimodal applications such as image captioning and visual question answering. However, building VL models requires substantial hardware resources, where efficiency ...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 14. Recent years have witnessed a significant interest in developing large multimodal models (LMMs) capable of performing various visual reasoning and understanding tasks. This has led to the introduction of multiple LMM benchmarks to evaluate LMMs on different tasks. However, most existing LMM evaluati...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 15. Web development involves turning UI designs into functional webpages, which can be difficult for both beginners and experienced developers due to the complexity of HTML's hierarchical structures and styles. While Large Language Models (LLMs) have shown promise in generating source code, two major ch...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 16. Large Language Models (LLMs) are thought to struggle with arithmetic learning due to the inherent differences between language modeling and numerical computation, but concrete evidence has been lacking. This work responds to this claim through a two-side experiment. We first investigate whether LLMs...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 17. Diffusion models achieve superior generation quality but suffer from slow generation speed due to the iterative nature of denoising. In contrast, consistency models, a new generative family, achieve competitive performance with significantly faster sampling. These models are trained either through c...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 18. Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earl...
[25.10.2024 10:13] ********************************************************************************
[25.10.2024 10:13] Abstract 19. We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model....
[25.10.2024 10:13] Read previous papers.
[25.10.2024 10:13] Generating reviews via LLM API.
[25.10.2024 10:13] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤—ã—á–∏—Å–ª–µ–Ω–∏—é –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –±–µ–∑ —Ä–æ—Å—Ç–∞ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ GPU. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∞–π–ª–∏–Ω–≥–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ä–∞—Å—á–µ—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏–µ –±–ª–æ–∫–∏.
[25.10.2024 10:13] Using data from previous issue: {"desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º LOGO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (LCM). LOGO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –±–µ–∑ —ç—Ç–∞–ª–æ–Ω–∞ –∏ –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –ø–æ–∑–∏—Ü–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å Llama-3-8B-Instruct-
[25.10.2024 10:13] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ScaleQuest - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –Ø–ë–ú –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –ò—Å–ø–æ–ª—å–∑—É—è –Ω–µ–±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, ScaleQuest –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å—ã —Å –Ω—É–ª—è –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –° –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –±—ã–ª —Å–æ–∑–¥–∞–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 1 –º–∏–ª
[25.10.2024 10:13] Using data from previous issue: {"desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –∏ –º–µ—Ç–æ–¥–∞–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –¥–ª—è –∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ HalluEditBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –≤ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –º
[25.10.2024 10:13] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π –∏–≥—Ä—ã, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ–∏–≥—Ä—ã, –≤—ã—Ö–æ–¥—è—â–µ–π –∑–∞ —Ä–∞–º–∫–∏ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–Ω–µ—á–Ω—ã—Ö —Å–∏—Å—Ç–µ–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∏–≥—Ä—É Unbounded - —Å–∏–º—É–ª—è—Ç–æ—Ä –∂–∏–∑–Ω–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞, –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –í –∏–≥—Ä–µ –∏
[25.10.2024 10:13] Using data from previous issue: {"desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Framer –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –∫–∞–¥—Ä–æ–≤, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø–ª–∞–≤–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É –¥–≤—É–º—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —Å —É—á–µ—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–µ—Ä–µ—Ö–æ–¥–∞ –ø—É—Ç–µ–º –∑–∞–¥–∞–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ 
[25.10.2024 10:13] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Code-as-Intermediary Translation (CIT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–¥ –∫–∞–∫ –ø–æ—Å—Ä–µ–¥–Ω–∏–∫ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç LLM –ø–æ–Ω–∏–º–∞—Ç—å –º–µ–∂–º–æ–¥–∞–ª—å
[25.10.2024 10:13] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ W-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ç–µ—Ö–Ω–∏–∫–∞–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ VINE, –∫–æ—Ç–æ—Ä—ã–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. VINE –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–Ω–∞–ª–∏–∑ 
[25.10.2024 10:13] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º STRING (ShifTed Rotray position embeddING) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ LLM —á–∞—Å—Ç–æ –Ω–µ –º–æ–≥—É—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ—Å—å –∑–∞—è–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑-–∑–∞ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ä–∞—Å–ø
[25.10.2024 10:13] Using data from previous issue: {"desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ç–µ—Ö–Ω–∏–∫–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –¥–∞–Ω–Ω—ã–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ—Ç–±–æ—Ä–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞
[25.10.2024 10:13] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MotionCLR - –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏ –∏ –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π
[25.10.2024 10:13] Using data from previous issue: {"desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω CCI3.0-HQ - –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–º–æ–º 500 –ì–ë, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –∏–∑ Chinese Corpora Internet 3.0. –î–ª—è –µ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—é—â–∏–π –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö. –ú–æ–¥–µ–ª—å —Å 0.5 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —ç—Ç–æ–º
[25.10.2024 10:13] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ–±—â–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –Ω–µ–∏–∑–±–µ–∂–Ω–æ–º—É —É—Ö—É–¥—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ–±—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø—Ä–∞–≤–æ–∫. –ú–æ–¥–µ–ª–∏, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã
[25.10.2024 10:13] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ADEM-VL - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏—è –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º —Å–ª–∏—è–Ω–∏–∏, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ
[25.10.2024 10:13] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CAMEL-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –Ω–∞ –∞—Ä–∞–±—Å–∫–æ–º —è–∑—ã–∫–µ. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 8 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∏ 38 –ø–æ–¥–æ–±–ª–∞—Å—Ç–µ–π, –≤–∫–ª—é—á–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–ª–æ–∂–Ω–æ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –∏ –∞–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ. CAMEL-Bench —Å–æ–¥–µ—Ä–∂–∏—Ç –æ
[25.10.2024 10:13] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ HTML-–∫–æ–¥–∞ –∏–∑ UI-–¥–∏–∑–∞–π–Ω–æ–≤. –ú–µ—Ç–æ–¥ Waffle –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É HTML, –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ UI –∏ HTML-–∫–æ–¥–æ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞
[25.10.2024 10:13] Using data from previous issue: {"desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –æ–±—É—á–µ–Ω–∏—é –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–µ. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Ö–æ—Ç—è LLM –º–æ–≥—É—Ç –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —á–∞—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è, –æ–Ω–∏ –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –¥–ª—è –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ LL
[25.10.2024 10:13] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –º–æ–¥–µ–ª–µ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—è –ø—Ä–æ—Ü–µ—Å—Å —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –∫–∞–∫ –º–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Stable Consistency Tuning (SCT), –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–∑–Ω–∏—Ü–µ–π –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä
[25.10.2024 10:13] Querying the API.
[25.10.2024 10:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding a residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer, reducing the KV cache by nearly 50%. Comprehensive empirical evidence demonstrates that ResFormer mitigates attention concentration problem in deeper layers and enhances representation across most layers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as downstream tasks. SVFormer trains significantly faster than the vanilla Transformer and performs better than other methods like GQA and CLA, with performance influenced by sequence length and cumulative learning rate.
[25.10.2024 10:13] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞–±–æ—Ç—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ResFormer. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –≥–ª—É–±–æ–∫–∏—Ö —Å–ª–æ—è—Ö –ø—É—Ç–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –æ—Ç –∑–Ω–∞—á–µ–Ω–∏–π –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è –∫–æ –≤—Å–µ–º –ø–æ—Å–ª–µ–¥—É—é—â–∏–º. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è SVFormer –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤, —á—Ç–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç KV-–∫—ç—à –ø–æ—á—Ç–∏ –Ω–∞ 50%. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ResFormer –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∏ –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ –æ—à–∏–±–∫–µ –æ–±—É—á–µ–Ω–∏—è –∏ –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.",
  "emoji": "üîÑ",
  "title": "ResFormer: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö"
}
[25.10.2024 10:13] OpenAI request. Model: gpt-4o-mini. Prompt: You are an expert classifier of machine learning research papers. Analyze the following research paper text and classify it into one or more relevant categories from the list below. Consider the paper's main contributions, methodologies, and applications.

Categories:
1. DATASET: Papers that introduce new datasets or make significant modifications to existing ones
2. DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
3. BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
4. AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
5. CV: Papers developing computer vision methods or visual processing systems
6. RL: Papers investigating reinforcement learning theory or applications
7. RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
8. RAG: Papers advancing retrieval-augmented generation techniques
9. PLP: Papers about Programming Language Processing models or programming benchmarks
10. INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
11. 3D: Papers on 3D content generation, processing, or understanding
12. AUDIO: Papers advancing speech/audio processing or generation
13. VIDEO: Papers on video analysis, generation, or understanding
14. MULTIMODAL: Papers combining multiple input/output modalities
15. MATH: Papers focused on mathematical theory and algorithms
16. MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities
17. ARCHITECTURE: Papers proposing novel neural architectures or components
18. MEDICINE: Papers applying ML to medical/healthcare domains
19. TRAINING: Papers improving model training or fine-tuning methods
20. ROBOTICS: Papers on robotic systems and embodied AI
21. AGI: Papers discussing artificial general intelligence concepts
22. GAMES: Papers applying ML to games or game development
23. INTERPRETABILITY: Papers analyzing model behavior and explanations
24. REASONING: Papers enhancing logical reasoning capabilities
25. TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
26. GRAPHS: Papers advancing graph neural networks and applications
27. ETHICS: Papers addressing AI ethics, fairness, and bias
28. SECURITY: Papers on model security and adversarial robustness
29. QUANTUM: Papers combining quantum computing and ML
30. EDGE_COMPUTING: Papers on ML deployment for resource-constrained devices
31. OPTIMIZATION: Papers advancing training optimization methods
32. SURVEY: Papers comprehensively reviewing research areas
33. DIFFUSION: Papers on diffusion-based generative models
34. ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
35. STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
36. HALLUCINATION: Papers about the hallucinations in language models, hallucinations analysis and mitigation

Return only JSON with flat array of categories that match the given text.

Paper text to classify:

"Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding a residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer, reducing the KV cache by nearly 50%. Comprehensive empirical evidence demonstrates that ResFormer mitigates attention concentration problem in deeper layers and enhances representation across most layers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as downstream tasks. SVFormer trains significantly faster than the vanilla Transformer and performs better than other methods like GQA and CLA, with performance influenced by sequence length and cumulative learning rate."

[25.10.2024 10:13] Response: ```json
["ARCHITECTURE", "TRAINING"]
```
[25.10.2024 10:13] Get embedding for a paper via LLM API.
[25.10.2024 10:13] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–µ–ª—å–Ω–æ–µ —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å—Ö–µ–º—É –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –≤–∞–∂–Ω–æ—Å—Ç–Ω—É—é –≤—ã–±–æ—Ä–∫—É. –î–ª—è —Å–ª—É—á–∞—è –¥–≤—É—Ö –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö —á–µ—Ä–Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã —É—Å–ª–æ–≤–∏—è –¥–ª—è 100% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω—è—Ç–∏—è –∏ –ø–æ
[25.10.2024 10:13] Trying to get texts in Chinese.
[25.10.2024 10:13] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Contrastive loss is a powerful approach for representation learning, where larger batch sizes enhance performance by providing more negative samples to better distinguish between similar and dissimilar data. However, scaling batch sizes is constrained by the quadratic growth in GPU memory consumption, primarily due to the full instantiation of the similarity matrix. To address this, we propose a tile-based computation strategy that partitions the contrastive loss calculation into arbitrary small blocks, avoiding full materialization of the similarity matrix. Furthermore, we introduce a multi-level tiling strategy to leverage the hierarchical structure of distributed systems, employing ring-based communication at the GPU level to optimize synchronization and fused kernels at the CUDA core level to reduce I/O overhead. Experimental results show that the proposed method scales batch sizes to unprecedented levels. For instance, it enables contrastive training of a CLIP-ViT-L/14 model with a batch size of 4M or 12M using 8 or 32 A800 80GB without sacrificing any accuracy. Compared to SOTA memory-efficient solutions, it achieves a two-order-of-magnitude reduction in memory while maintaining comparable speed. The code will be made publicly available.
[25.10.2024 10:13] Mistral response. {"id": "62a2350131944b119d5aeb4d2e417596", "object": "chat.completion", "created": 1729851208, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u5bf9\u6bd4\u635f\u5931\u5728\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002\u5927\u6279\u91cf\u6570\u636e\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u53d7\u9650\u4e8eGPU\u5185\u5b58\u6d88\u8017\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5757\u7684\u8ba1\u7b97\u7b56\u7565\uff0c\u907f\u514d\u5168\u90e8\u5b9e\u4f8b\u5316\u76f8\u4f3c\u5ea6\u77e9\u9635\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u5f15\u5165\u4e86\u591a\u7ea7\u5757\u7b56\u7565\uff0c\u5229\u7528\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5c42\u6b21\u7ed3\u6784\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6269\u5c55\u5230\u524d\u6240\u672a\u6709\u7684\u6279\u91cf\u5927\u5c0f\uff0c\u5e76\u4fdd\u6301\u7cbe\u5ea6\u3002\u4f8b\u5982\uff0c\u4f7f\u75288\u621632\u4e2aA800 80GB\uff0c\u53ef\u4ee5\u8bad\u7ec3CLIP-ViT-L/14\u6a21\u578b\uff0c\u6279\u91cf\u5927\u5c0f\u4e3a4M\u621612M\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u5185\u5b58\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u76f8\u6bd4\uff0c\u5b83\u5728\u4fdd\u6301\u76f8\u5f53\u901f\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u5185\u5b58\u51cf\u5c11\u3002\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\u3002", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 298, "total_tokens": 530, "completion_tokens": 232}}
[25.10.2024 10:13] Response: ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÂØπÊØîÊçüÂ§±Âú®Ë°®Á§∫Â≠¶‰π†‰∏≠ÁöÑÂ∫îÁî®„ÄÇÂ§ßÊâπÈáèÊï∞ÊçÆÂèØ‰ª•ÊèêÈ´òÊÄßËÉΩÔºå‰ΩÜÂèóÈôê‰∫éGPUÂÜÖÂ≠òÊ∂àËÄó„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂùóÁöÑËÆ°ÁÆóÁ≠ñÁï•ÔºåÈÅøÂÖçÂÖ®ÈÉ®ÂÆû‰æãÂåñÁõ∏‰ººÂ∫¶Áü©Èòµ„ÄÇÊ≠§Â§ñÔºå‰ªñ‰ª¨ÂºïÂÖ•‰∫ÜÂ§öÁ∫ßÂùóÁ≠ñÁï•ÔºåÂà©Áî®ÂàÜÂ∏ÉÂºèÁ≥ªÁªüÁöÑÂ±ÇÊ¨°ÁªìÊûÑ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Êâ©Â±ïÂà∞ÂâçÊâÄÊú™ÊúâÁöÑÊâπÈáèÂ§ßÂ∞èÔºåÂπ∂‰øùÊåÅÁ≤æÂ∫¶„ÄÇ‰æãÂ¶ÇÔºå‰ΩøÁî®8Êàñ32‰∏™A800 80GBÔºåÂèØ‰ª•ËÆ≠ÁªÉCLIP-ViT-L/14Ê®°ÂûãÔºåÊâπÈáèÂ§ßÂ∞è‰∏∫4MÊàñ12M„ÄÇ‰∏éÊúÄÂÖàËøõÁöÑÂÜÖÂ≠òÈ´òÊïàËß£ÂÜ≥ÊñπÊ°àÁõ∏ÊØîÔºåÂÆÉÂú®‰øùÊåÅÁõ∏ÂΩìÈÄüÂ∫¶ÁöÑÂêåÊó∂ÔºåÂÆûÁé∞‰∫Ü‰∏§‰∏™Êï∞ÈáèÁ∫ßÁöÑÂÜÖÂ≠òÂáèÂ∞ë„ÄÇ‰ª£Á†ÅÂ∞ÜÂÖ¨ÂºÄÂèëÂ∏É„ÄÇ
[25.10.2024 10:13] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÂØπÊØîÊçüÂ§±Âú®Ë°®Á§∫Â≠¶‰π†‰∏≠ÁöÑÂ∫îÁî®„ÄÇÂ§ßÊâπÈáèÊï∞ÊçÆÂèØ‰ª•ÊèêÈ´òÊÄßËÉΩÔºå‰ΩÜÂèóÈôê‰∫éGPUÂÜÖÂ≠òÊ∂àËÄó„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂùóÁöÑËÆ°ÁÆóÁ≠ñÁï•ÔºåÈÅøÂÖçÂÖ®ÈÉ®ÂÆû‰æãÂåñÁõ∏‰ººÂ∫¶Áü©Èòµ„ÄÇÊ≠§Â§ñÔºå‰ªñ‰ª¨ÂºïÂÖ•‰∫ÜÂ§öÁ∫ßÂùóÁ≠ñÁï•ÔºåÂà©Áî®ÂàÜÂ∏ÉÂºèÁ≥ªÁªüÁöÑÂ±ÇÊ¨°ÁªìÊûÑ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Êâ©Â±ïÂà∞ÂâçÊâÄÊú™ÊúâÁöÑÊâπÈáèÂ§ßÂ∞èÔºåÂπ∂‰øùÊåÅÁ≤æÂ∫¶„ÄÇ‰æãÂ¶ÇÔºå‰ΩøÁî®8Êàñ32‰∏™A800 80GBÔºåÂèØ‰ª•ËÆ≠ÁªÉCLIP-ViT-L/14Ê®°ÂûãÔºåÊâπÈáèÂ§ßÂ∞è‰∏∫4MÊàñ12M„ÄÇ‰∏éÊúÄÂÖàËøõÁöÑÂÜÖÂ≠òÈ´òÊïàËß£ÂÜ≥ÊñπÊ°àÁõ∏ÊØîÔºåÂÆÉÂú®‰øùÊåÅÁõ∏ÂΩìÈÄüÂ∫¶ÁöÑÂêåÊó∂ÔºåÂÆûÁé∞‰∫Ü‰∏§‰∏™Êï∞ÈáèÁ∫ßÁöÑÂÜÖÂ≠òÂáèÂ∞ë„ÄÇ‰ª£Á†ÅÂ∞ÜÂÖ¨ÂºÄÂèëÂ∏É„ÄÇ
[25.10.2024 10:14] Mistral response. {"id": "1e2745d59c714285afa47dd6f48cdfec", "object": "chat.completion", "created": 1729851220, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "zh\u00e8 pi\u0101n w\u00e9n zh\u0101ng t\u01ceo l\u00f9n le du\u00ec b\u01d0 s\u01d4n sh\u012b z\u00e0i bi\u01ceo sh\u00ec xu\u00e9 x\u00ed zh\u014dng de y\u00ecng y\u00f2ng. d\u00e0 p\u012b li\u00e0ng sh\u00f9 j\u00f9 k\u011b y\u01d0 t\u00ed g\u0101o x\u00ecng n\u00e9ng, d\u00e0n sh\u00f2u xi\u00e0n y\u00fa GPU n\u00e8i c\u00fan xi\u0101o h\u00e0o. zu\u00f2 zh\u011b t\u00ed ch\u016b le y\u012b zh\u01d2ng j\u012b z\u00e0i ku\u00e0i de j\u00ec su\u00e0n c\u00e8 l\u00fc\u00e8, b\u00ec mi\u01cen qu\u00e1n b\u00f9 sh\u00ed l\u00ec hu\u00ec xi\u0101ng s\u00ec d\u00fa j\u01d4 zh\u011bn. c\u01d0 w\u00e0i, t\u0101 men y\u01d0n r\u00f9 le du\u014d j\u00ed ku\u00e0i c\u00e8 l\u00fc\u00e8, l\u00ec y\u00f2ng f\u0113n b\u00f9 sh\u00ec x\u00ec t\u01d2ng de c\u00e9ng c\u00ec ji\u00e8 g\u00f2u. sh\u00ed y\u00e0n ji\u00e9 gu\u01d2 xi\u01cen sh\u00ec, g\u0101i f\u0101ng f\u01ce k\u011b y\u01d0 ku\u00f2 zh\u01cen d\u00e0o qi\u00e1n su\u01d2 w\u00e8i y\u01d2u de p\u012b li\u00e0ng d\u00e0 xi\u01ceo, \u00e9r b\u01ceo ch\u00ed j\u012bng du\u00f3. r\u00fa sh\u00ec, sh\u01d0 y\u00f2ng 8 hu\u00f2 32 g\u00e8 A800 80GB, k\u011b y\u01d0 x\u00f9n li\u00e0n CLIP-ViT-L/14 m\u00f3 x\u00edng, p\u012b li\u00e0ng d\u00e0 xi\u01ceo w\u00e9i 4M hu\u00f2 12M. y\u01d4 zu\u00ec xi\u0101n j\u00ecn de n\u00e8i c\u00fan g\u0101o xi\u00e0o ji\u011b ju\u00e9 f\u0101ng \u00e0n xi\u00e0ng b\u01d0, t\u0101 z\u00e0i b\u01ceo ch\u00ed xi\u0101ng d\u0101ng s\u00f9 d\u00f9 de sh\u00ed hu\u00e0, sh\u00ed xi\u00e0n le li\u01ceng g\u00e8 sh\u00f9 li\u00e0ng j\u00ed de n\u00e8i c\u00fan ji\u01cen sh\u01ceo. d\u00e0i m\u01ce ji\u0101ng g\u014dng k\u0101i f\u0101 b\u00f9.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 247, "total_tokens": 752, "completion_tokens": 505}}
[25.10.2024 10:14] Response: zh√® piƒÅn w√©n zhƒÅng t«éo l√πn le du√¨ b«ê s«în shƒ´ z√†i bi«éo sh√¨ xu√© x√≠ zh≈çng de y√¨ng y√≤ng. d√† pƒ´ li√†ng sh√π j√π kƒõ y«ê t√≠ gƒÅo x√¨ng n√©ng, d√†n sh√≤u xi√†n y√∫ GPU n√®i c√∫n xiƒÅo h√†o. zu√≤ zhƒõ t√≠ ch≈´ le yƒ´ zh«íng jƒ´ z√†i ku√†i de j√¨ su√†n c√® l√º√®, b√¨ mi«én qu√°n b√π sh√≠ l√¨ hu√¨ xiƒÅng s√¨ d√∫ j«î zhƒõn. c«ê w√†i, tƒÅ men y«ên r√π le du≈ç j√≠ ku√†i c√® l√º√®, l√¨ y√≤ng fƒìn b√π sh√¨ x√¨ t«íng de c√©ng c√¨ ji√® g√≤u. sh√≠ y√†n ji√© gu«í xi«én sh√¨, gƒÅi fƒÅng f«é kƒõ y«ê ku√≤ zh«én d√†o qi√°n su«í w√®i y«íu de pƒ´ li√†ng d√† xi«éo, √©r b«éo ch√≠ jƒ´ng du√≥. r√∫ sh√¨, sh«ê y√≤ng 8 hu√≤ 32 g√® A800 80GB, kƒõ y«ê x√πn li√†n CLIP-ViT-L/14 m√≥ x√≠ng, pƒ´ li√†ng d√† xi«éo w√©i 4M hu√≤ 12M. y«î zu√¨ xiƒÅn j√¨n de n√®i c√∫n gƒÅo xi√†o jiƒõ ju√© fƒÅng √†n xi√†ng b«ê, tƒÅ z√†i b«éo ch√≠ xiƒÅng dƒÅng s√π d√π de sh√≠ hu√†, sh√≠ xi√†n le li«éng g√® sh√π li√†ng j√≠ de n√®i c√∫n ji«én sh«éo. d√†i m«é jiƒÅng g≈çng kƒÅi fƒÅ b√π.
[25.10.2024 10:14] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÂØπÊØîÊçüÂ§±Âú®Ë°®Á§∫Â≠¶‰π†‰∏≠ÁöÑÂ∫îÁî®„ÄÇÂ§ßÊâπÈáèÊï∞ÊçÆÂèØ‰ª•ÊèêÈ´òÊÄßËÉΩÔºå‰ΩÜÂèóÈôê‰∫éGPUÂÜÖÂ≠òÊ∂àËÄó„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂùóÁöÑËÆ°ÁÆóÁ≠ñÁï•ÔºåÈÅøÂÖçÂÖ®ÈÉ®ÂÆû‰æãÂåñÁõ∏‰ººÂ∫¶Áü©Èòµ„ÄÇÊ≠§Â§ñÔºå‰ªñ‰ª¨ÂºïÂÖ•‰∫ÜÂ§öÁ∫ßÂùóÁ≠ñÁï•ÔºåÂà©Áî®ÂàÜÂ∏ÉÂºèÁ≥ªÁªüÁöÑÂ±ÇÊ¨°ÁªìÊûÑ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Êâ©Â±ïÂà∞ÂâçÊâÄÊú™ÊúâÁöÑÊâπÈáèÂ§ßÂ∞èÔºåÂπ∂‰øùÊåÅÁ≤æÂ∫¶„ÄÇ‰æãÂ¶ÇÔºå‰ΩøÁî®8Êàñ32‰∏™A800 80GBÔºåÂèØ‰ª•ËÆ≠ÁªÉCLIP-ViT-L/14Ê®°ÂûãÔºåÊâπÈáèÂ§ßÂ∞è‰∏∫4MÊàñ12M„ÄÇ‰∏éÊúÄÂÖàËøõÁöÑÂÜÖÂ≠òÈ´òÊïàËß£ÂÜ≥ÊñπÊ°àÁõ∏ÊØîÔºåÂÆÉÂú®‰øùÊåÅÁõ∏ÂΩìÈÄüÂ∫¶ÁöÑÂêåÊó∂ÔºåÂÆûÁé∞‰∫Ü‰∏§‰∏™Êï∞ÈáèÁ∫ßÁöÑÂÜÖÂ≠òÂáèÂ∞ë„ÄÇ‰ª£Á†ÅÂ∞ÜÂÖ¨ÂºÄÂèëÂ∏É„ÄÇ
[25.10.2024 10:14] Mistral response. {"id": "732122131b1848a2a39d4e78befea106", "object": "chat.completion", "created": 1729851245, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "[\n    {\"word\": \"\u8ba8\u8bba\", \"pinyin\": \"t\u01ceo l\u00f9n\", \"trans\": \"discuss\"},\n    {\"word\": \"\u5bf9\u6bd4\", \"pinyin\": \"du\u00ec b\u01d0\", \"trans\": \"contrast\"},\n    {\"word\": \"\u635f\u5931\", \"pinyin\": \"s\u01d4n sh\u012b\", \"trans\": \"loss\"},\n    {\"word\": \"\u8868\u793a\", \"pinyin\": \"bi\u01ceo sh\u00ec\", \"trans\": \"representation\"},\n    {\"word\": \"\u5b66\u4e60\", \"pinyin\": \"xu\u00e9 x\u00ed\", \"trans\": \"learning\"},\n    {\"word\": \"\u5927\u6279\u91cf\", \"pinyin\": \"d\u00e0 p\u012b li\u00e0ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"\u6027\u80fd\", \"pinyin\": \"x\u00ecng n\u00e9ng\", \"trans\": \"performance\"},\n    {\"word\": \"\u53d7\u9650\u4e8e\", \"pinyin\": \"sh\u00f2u xi\u00e0n y\u00fa\", \"trans\": \"limited by\"},\n    {\"word\": \"GPU\", \"pinyin\": \"GPU\", \"trans\": \"GPU\"},\n    {\"word\": \"\u5185\u5b58\", \"pinyin\": \"n\u00e8i c\u00fan\", \"trans\": \"memory\"},\n    {\"word\": \"\u6d88\u8017\", \"pinyin\": \"xi\u0101o h\u00e0o\", \"trans\": \"consumption\"},\n    {\"word\": \"\u63d0\u51fa\", \"pinyin\": \"t\u00ed ch\u016b\", \"trans\": \"propose\"},\n    {\"word\": \"\u57fa\u4e8e\", \"pinyin\": \"j\u012b y\u00fa\", \"trans\": \"based on\"},\n    {\"word\": \"\u5757\", \"pinyin\": \"ku\u00e0i\", \"trans\": \"block\"},\n    {\"word\": \"\u8ba1\u7b97\", \"pinyin\": \"j\u00ec su\u00e0n\", \"trans\": \"computation\"},\n    {\"word\": \"\u7b56\u7565\", \"pinyin\": \"c\u00e8 l\u00fc\u00e8\", \"trans\": \"strategy\"},\n    {\"word\": \"\u907f\u514d\", \"pinyin\": \"b\u00ec mi\u01cen\", \"trans\": \"avoid\"},\n    {\"word\": \"\u5168\u90e8\", \"pinyin\": \"qu\u00e1n b\u00f9\", \"trans\": \"entire\"},\n    {\"word\": \"\u5b9e\u4f8b\u5316\", \"pinyin\": \"sh\u00ed l\u00ec hu\u00e0\", \"trans\": \"instantiate\"},\n    {\"word\": \"\u76f8\u4f3c\u5ea6\", \"pinyin\": \"xi\u0101ng s\u00ec d\u00f9\", \"trans\": \"similarity\"},\n    {\"word\": \"\u77e9\u9635\", \"pinyin\": \"j\u01d4 zh\u00e8n\", \"trans\": \"matrix\"},\n    {\"word\": \"\u6b64\u5916\", \"pinyin\": \"c\u01d0 w\u00e0i\", \"trans\": \"moreover\"},\n    {\"word\": \"\u5f15\u5165\", \"pinyin\": \"y\u01d0n r\u00f9\", \"trans\": \"introduce\"},\n    {\"word\": \"\u591a\u7ea7\", \"pinyin\": \"du\u014d j\u00ed\", \"trans\": \"multi-level\"},\n    {\"word\": \"\u5206\u5e03\u5f0f\", \"pinyin\": \"f\u0113n b\u00f9 sh\u00ec\", \"trans\": \"distributed\"},\n    {\"word\": \"\u7cfb\u7edf\", \"pinyin\": \"x\u00ec t\u01d2ng\", \"trans\": \"system\"},\n    {\"word\": \"\u5c42\u6b21\", \"pinyin\": \"c\u00e9ng c\u00ec\", \"trans\": \"hierarchy\"},\n    {\"word\": \"\u7ed3\u6784\", \"pinyin\": \"ji\u00e9 g\u00f2u\", \"trans\": \"structure\"},\n    {\"word\": \"\u5b9e\u9a8c\", \"pinyin\": \"sh\u00ed y\u00e0n\", \"trans\": \"experiment\"},\n    {\"word\": \"\u7ed3\u679c\", \"pinyin\": \"ji\u00e9 gu\u01d2\", \"trans\": \"result\"},\n    {\"word\": \"\u663e\u793a\", \"pinyin\": \"xi\u01cen sh\u00ec\", \"trans\": \"show\"},\n    {\"word\": \"\u65b9\u6cd5\", \"pinyin\": \"f\u0101ng f\u01ce\", \"trans\": \"method\"},\n    {\"word\": \"\u6269\u5c55\", \"pinyin\": \"ku\u00f2 zh\u01cen\", \"trans\": \"scale\"},\n    {\"word\": \"\u524d\u6240\u672a\u6709\", \"pinyin\": \"qi\u00e1n su\u01d2 w\u00e8i y\u01d2u\", \"trans\": \"unprecedented\"},\n    {\"word\": \"\u6279\u91cf\", \"pinyin\": \"p\u012b li\u00e0ng\", \"trans\": \"batch\"},\n    {\"word\": \"\u5927\u5c0f\", \"pinyin\": \"d\u00e0 xi\u01ceo\", \"trans\": \"size\"},\n    {\"word\": \"\u4fdd\u6301\", \"pinyin\": \"b\u01ceo ch\u00ed\", \"trans\": \"maintain\"},\n    {\"word\": \"\u7cbe\u5ea6\", \"pinyin\": \"j\u012bng d\u00f9\", \"trans\": \"accuracy\"},\n    {\"word\": \"\u4f8b\u5982\", \"pinyin\": \"l\u00ec r\u00fa\", \"trans\": \"for example\"},\n    {\"word\": \"\u4f7f\u7528\", \"pinyin\": \"sh\u01d0 y\u00f2ng\", \"trans\": \"use\"},\n    {\"word\": \"A800\", \"pinyin\": \"A800\", \"trans\": \"A800\"},\n    {\"word\": \"80GB\", \"pinyin\": \"80GB\", \"trans\": \"80GB\"},\n    {\"word\": \"\u8bad\u7ec3\", \"pinyin\": \"x\u00f9n li\u00e0n\", \"trans\": \"train\"},\n    {\"word\": \"CLIP-ViT-L/14\", \"pinyin\": \"CLIP-ViT-L/14\", \"trans\": \"CLIP-ViT-L/14\"},\n    {\"word\": \"\u6a21\u578b\", \"pinyin\": \"m\u00f3 x\u00edng\", \"trans\": \"model\"},\n    {\"word\": \"\u4e0e\", \"pinyin\": \"y\u01d4\", \"trans\": \"with\"},\n    {\"word\": \"\u6700\u5148\u8fdb\", \"pinyin\": \"zu\u00ec xi\u0101n j\u00ecn\", \"trans\": \"most advanced\"},\n    {\"word\": \"\u5185\u5b58\u9ad8\u6548\", \"pinyin\": \"n\u00e8i c\u00fan g\u0101o xi\u00e0o\", \"trans\": \"memory-efficient\"},\n    {\"word\": \"\u89e3\u51b3\u65b9\u6848\", \"pinyin\": \"ji\u011b ju\u00e9 f\u0101ng \u00e0n\", \"trans\": \"solution\"},\n    {\"word\": \"\u76f8\u6bd4\", \"pinyin\": \"xi\u0101ng b\u01d0\", \"trans\": \"compared to\"},\n    {\"word\": \"\u76f8\u5f53\", \"pinyin\": \"xi\u0101ng d\u0101ng\", \"trans\": \"comparable\"},\n    {\"word\": \"\u901f\u5ea6\", \"pinyin\": \"s\u00f9 d\u00f9\", \"trans\": \"speed\"},\n    {\"word\": \"\u540c\u65f6\", \"pinyin\": \"t\u00f3ng sh\u00ed\", \"trans\": \"simultaneously\"},\n    {\"word\": \"\u5b9e\u73b0\", \"pinyin\": \"sh\u00ed xi\u00e0n\", \"trans\": \"achieve\"},\n    {\"word\": \"\u6570\u91cf\u7ea7\", \"pinyin\": \"sh\u00f9 li\u00e0ng j\u00ed\", \"trans\": \"order of magnitude\"},\n    {\"word\": \"\u51cf\u5c11\", \"pinyin\": \"ji\u01cen sh\u01ceo\", \"trans\": \"reduce\"},\n    {\"word\": \"\u4ee3\u7801\", \"pinyin\": \"d\u00e0i m\u01ce\", \"trans\": \"code\"},\n    {\"word\": \"\u5c06\", \"pinyin\": \"ji\u0101ng\", \"trans\": \"will\"},\n    {\"word\": \"\u516c\u5f00\", \"pinyin\": \"g\u014dng k\u0101i\", \"trans\": \"public\"},\n    {\"word\": \"\u53d1\u5e03\", \"pinyin\": \"f\u0101 b\u00f9\", \"trans\": \"release\"}\n]", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 279, "total_tokens": 2009, "completion_tokens": 1730}}
[25.10.2024 10:14] Response: [
    {"word": "ËÆ®ËÆ∫", "pinyin": "t«éo l√πn", "trans": "discuss"},
    {"word": "ÂØπÊØî", "pinyin": "du√¨ b«ê", "trans": "contrast"},
    {"word": "ÊçüÂ§±", "pinyin": "s«în shƒ´", "trans": "loss"},
    {"word": "Ë°®Á§∫", "pinyin": "bi«éo sh√¨", "trans": "representation"},
    {"word": "Â≠¶‰π†", "pinyin": "xu√© x√≠", "trans": "learning"},
    {"word": "Â§ßÊâπÈáè", "pinyin": "d√† pƒ´ li√†ng", "trans": "large-scale"},
    {"word": "ÊÄßËÉΩ", "pinyin": "x√¨ng n√©ng", "trans": "performance"},
    {"word": "ÂèóÈôê‰∫é", "pinyin": "sh√≤u xi√†n y√∫", "trans": "limited by"},
    {"word": "GPU", "pinyin": "GPU", "trans": "GPU"},
    {"word": "ÂÜÖÂ≠ò", "pinyin": "n√®i c√∫n", "trans": "memory"},
    {"word": "Ê∂àËÄó", "pinyin": "xiƒÅo h√†o", "trans": "consumption"},
    {"word": "ÊèêÂá∫", "pinyin": "t√≠ ch≈´", "trans": "propose"},
    {"word": "Âü∫‰∫é", "pinyin": "jƒ´ y√∫", "trans": "based on"},
    {"word": "Âùó", "pinyin": "ku√†i", "trans": "block"},
    {"word": "ËÆ°ÁÆó", "pinyin": "j√¨ su√†n", "trans": "computation"},
    {"word": "Á≠ñÁï•", "pinyin": "c√® l√º√®", "trans": "strategy"},
    {"word": "ÈÅøÂÖç", "pinyin": "b√¨ mi«én", "trans": "avoid"},
    {"word": "ÂÖ®ÈÉ®", "pinyin": "qu√°n b√π", "trans": "entire"},
    {"word": "ÂÆû‰æãÂåñ", "pinyin": "sh√≠ l√¨ hu√†", "trans": "instantiate"},
    {"word": "Áõ∏‰ººÂ∫¶", "pinyin": "xiƒÅng s√¨ d√π", "trans": "similarity"},
    {"word": "Áü©Èòµ", "pinyin": "j«î zh√®n", "trans": "matrix"},
    {"word": "Ê≠§Â§ñ", "pinyin": "c«ê w√†i", "trans": "moreover"},
    {"word": "ÂºïÂÖ•", "pinyin": "y«ên r√π", "trans": "introduce"},
    {"word": "Â§öÁ∫ß", "pinyin": "du≈ç j√≠", "trans": "multi-level"},
    {"word": "ÂàÜÂ∏ÉÂºè", "pinyin": "fƒìn b√π sh√¨", "trans": "distributed"},
    {"word": "Á≥ªÁªü", "pinyin": "x√¨ t«íng", "trans": "system"},
    {"word": "Â±ÇÊ¨°", "pinyin": "c√©ng c√¨", "trans": "hierarchy"},
    {"word": "ÁªìÊûÑ", "pinyin": "ji√© g√≤u", "trans": "structure"},
    {"word": "ÂÆûÈ™å", "pinyin": "sh√≠ y√†n", "trans": "experiment"},
    {"word": "ÁªìÊûú", "pinyin": "ji√© gu«í", "trans": "result"},
    {"word": "ÊòæÁ§∫", "pinyin": "xi«én sh√¨", "trans": "show"},
    {"word": "ÊñπÊ≥ï", "pinyin": "fƒÅng f«é", "trans": "method"},
    {"word": "Êâ©Â±ï", "pinyin": "ku√≤ zh«én", "trans": "scale"},
    {"word": "ÂâçÊâÄÊú™Êúâ", "pinyin": "qi√°n su«í w√®i y«íu", "trans": "unprecedented"},
    {"word": "ÊâπÈáè", "pinyin": "pƒ´ li√†ng", "trans": "batch"},
    {"word": "Â§ßÂ∞è", "pinyin": "d√† xi«éo", "trans": "size"},
    {"word": "‰øùÊåÅ", "pinyin": "b«éo ch√≠", "trans": "maintain"},
    {"word": "Á≤æÂ∫¶", "pinyin": "jƒ´ng d√π", "trans": "accuracy"},
    {"word": "‰æãÂ¶Ç", "pinyin": "l√¨ r√∫", "trans": "for example"},
    {"word": "‰ΩøÁî®", "pinyin": "sh«ê y√≤ng", "trans": "use"},
    {"word": "A800", "pinyin": "A800", "trans": "A800"},
    {"word": "80GB", "pinyin": "80GB", "trans": "80GB"},
    {"word": "ËÆ≠ÁªÉ", "pinyin": "x√πn li√†n", "trans": "train"},
    {"word": "CLIP-ViT-L/14", "pinyin": "CLIP-ViT-L/14", "trans": "CLIP-ViT-L/14"},
    {"word": "Ê®°Âûã", "pinyin": "m√≥ x√≠ng", "trans": "model"},
    {"word": "‰∏é", "pinyin": "y«î", "trans": "with"},
    {"word": "ÊúÄÂÖàËøõ", "pinyin": "zu√¨ xiƒÅn j√¨n", "trans": "most advanced"},
    {"word": "ÂÜÖÂ≠òÈ´òÊïà", "pinyin": "n√®i c√∫n gƒÅo xi√†o", "trans": "memory-efficient"},
    {"word": "Ëß£ÂÜ≥ÊñπÊ°à", "pinyin": "jiƒõ ju√© fƒÅng √†n", "trans": "solution"},
    {"word": "Áõ∏ÊØî", "pinyin": "xiƒÅng b«ê", "trans": "compared to"},
    {"word": "Áõ∏ÂΩì", "pinyin": "xiƒÅng dƒÅng", "trans": "comparable"},
    {"word": "ÈÄüÂ∫¶", "pinyin": "s√π d√π", "trans": "speed"},
    {"word": "ÂêåÊó∂", "pinyin": "t√≥ng sh√≠", "trans": "simultaneously"},
    {"word": "ÂÆûÁé∞", "pinyin": "sh√≠ xi√†n", "trans": "achieve"},
    {"word": "Êï∞ÈáèÁ∫ß", "pinyin": "sh√π li√†ng j√≠", "trans": "order of magnitude"},
    {"word": "ÂáèÂ∞ë", "pinyin": "ji«én sh«éo", "trans": "reduce"},
    {"word": "‰ª£Á†Å", "pinyin": "d√†i m«é", "trans": "code"},
    {"word": "Â∞Ü", "pinyin": "jiƒÅng", "trans": "will"},
    {"word": "ÂÖ¨ÂºÄ", "pinyin": "g≈çng kƒÅi", "trans": "public"},
    {"word": "ÂèëÂ∏É", "pinyin": "fƒÅ b√π", "trans": "release"}
]
[25.10.2024 10:14] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÂØπÊØîÊçüÂ§±Âú®Ë°®Á§∫Â≠¶‰π†‰∏≠ÁöÑÂ∫îÁî®„ÄÇÂ§ßÊâπÈáèÊï∞ÊçÆÂèØ‰ª•ÊèêÈ´òÊÄßËÉΩÔºå‰ΩÜÂèóÈôê‰∫éGPUÂÜÖÂ≠òÊ∂àËÄó„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂùóÁöÑËÆ°ÁÆóÁ≠ñÁï•ÔºåÈÅøÂÖçÂÖ®ÈÉ®ÂÆû‰æãÂåñÁõ∏‰ººÂ∫¶Áü©Èòµ„ÄÇÊ≠§Â§ñÔºå‰ªñ‰ª¨ÂºïÂÖ•‰∫ÜÂ§öÁ∫ßÂùóÁ≠ñÁï•ÔºåÂà©Áî®ÂàÜÂ∏ÉÂºèÁ≥ªÁªüÁöÑÂ±ÇÊ¨°ÁªìÊûÑ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËØ•ÊñπÊ≥ïÂèØ‰ª•Êâ©Â±ïÂà∞ÂâçÊâÄÊú™ÊúâÁöÑÊâπÈáèÂ§ßÂ∞èÔºåÂπ∂‰øùÊåÅÁ≤æÂ∫¶„ÄÇ‰æãÂ¶ÇÔºå‰ΩøÁî®8Êàñ32‰∏™A800 80GBÔºåÂèØ‰ª•ËÆ≠ÁªÉCLIP-ViT-L/14Ê®°ÂûãÔºåÊâπÈáèÂ§ßÂ∞è‰∏∫4MÊàñ12M„ÄÇ‰∏éÊúÄÂÖàËøõÁöÑÂÜÖÂ≠òÈ´òÊïàËß£ÂÜ≥ÊñπÊ°àÁõ∏ÊØîÔºåÂÆÉÂú®‰øùÊåÅÁõ∏ÂΩìÈÄüÂ∫¶ÁöÑÂêåÊó∂ÔºåÂÆûÁé∞‰∫Ü‰∏§‰∏™Êï∞ÈáèÁ∫ßÁöÑÂÜÖÂ≠òÂáèÂ∞ë„ÄÇ‰ª£Á†ÅÂ∞ÜÂÖ¨ÂºÄÂèëÂ∏É„ÄÇ
[25.10.2024 10:15] Mistral response. {"id": "0c3d5bcadbff4682b3fd59a3f465859a", "object": "chat.completion", "created": 1729851294, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "This article discusses the application of contrastive loss in representation learning. Large-scale data can improve performance but is limited by GPU memory consumption. The authors propose a block-based computation strategy to avoid fully instantiating the similarity matrix. Additionally, they introduce a multi-level block strategy that leverages the hierarchical structure of distributed systems. Experimental results show that this method can scale to unprecedented batch sizes while maintaining accuracy. For example, using 8 or 32 A800 80GB GPUs, the CLIP-ViT-L/14 model can be trained with batch sizes of 4M or 12M. Compared to state-of-the-art memory-efficient solutions, it achieves a two-order-of-magnitude reduction in memory usage while maintaining comparable speed. The code will be made publicly available.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 245, "total_tokens": 429, "completion_tokens": 184}}
[25.10.2024 10:15] Response: This article discusses the application of contrastive loss in representation learning. Large-scale data can improve performance but is limited by GPU memory consumption. The authors propose a block-based computation strategy to avoid fully instantiating the similarity matrix. Additionally, they introduce a multi-level block strategy that leverages the hierarchical structure of distributed systems. Experimental results show that this method can scale to unprecedented batch sizes while maintaining accuracy. For example, using 8 or 32 A800 80GB GPUs, the CLIP-ViT-L/14 model can be trained with batch sizes of 4M or 12M. Compared to state-of-the-art memory-efficient solutions, it achieves a two-order-of-magnitude reduction in memory usage while maintaining comparable speed. The code will be made publicly available.
[25.10.2024 10:15] Renaming data file.
[25.10.2024 10:15] Renaming previous data. hf_papers.json to ./d/2024-10-25.json
[25.10.2024 10:15] Saving new data file.
[25.10.2024 10:15] Generating page.
[25.10.2024 10:15] Renaming previous page.
[25.10.2024 10:15] Renaming previous data. index.html to ./d/2024-10-25.html
[25.10.2024 10:15] [Experimental] Generating Chinese page for reading.
[25.10.2024 10:15] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'ÂØπÊØî', 'pinyin': 'du√¨ b«ê', 'trans': 'contrast'}, {'word': 'ÊçüÂ§±', 'pinyin': 's«în shƒ´', 'trans': 'loss'}, {'word': 'Ë°®Á§∫', 'pinyin': 'bi«éo sh√¨', 'trans': 'representation'}, {'word': 'Â≠¶‰π†', 'pinyin': 'xu√© x√≠', 'trans': 'learning'}, {'word': 'Â§ßÊâπÈáè', 'pinyin': 'd√† pƒ´ li√†ng', 'trans': 'large-scale'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'ÂèóÈôê‰∫é', 'pinyin': 'sh√≤u xi√†n y√∫', 'trans': 'limited by'}, {'word': 'GPU', 'pinyin': 'GPU', 'trans': 'GPU'}, {'word': 'ÂÜÖÂ≠ò', 'pinyin': 'n√®i c√∫n', 'trans': 'memory'}, {'word': 'Ê∂àËÄó', 'pinyin': 'xiƒÅo h√†o', 'trans': 'consumption'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´ y√∫', 'trans': 'based on'}, {'word': 'Âùó', 'pinyin': 'ku√†i', 'trans': 'block'}, {'word': 'ËÆ°ÁÆó', 'pinyin': 'j√¨ su√†n', 'trans': 'computation'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'ÈÅøÂÖç', 'pinyin': 'b√¨ mi«én', 'trans': 'avoid'}, {'word': 'ÂÖ®ÈÉ®', 'pinyin': 'qu√°n b√π', 'trans': 'entire'}, {'word': 'ÂÆû‰æãÂåñ', 'pinyin': 'sh√≠ l√¨ hu√†', 'trans': 'instantiate'}, {'word': 'Áõ∏‰ººÂ∫¶', 'pinyin': 'xiƒÅng s√¨ d√π', 'trans': 'similarity'}, {'word': 'Áü©Èòµ', 'pinyin': 'j«î zh√®n', 'trans': 'matrix'}, {'word': 'Ê≠§Â§ñ', 'pinyin': 'c«ê w√†i', 'trans': 'moreover'}, {'word': 'ÂºïÂÖ•', 'pinyin': 'y«ên r√π', 'trans': 'introduce'}, {'word': 'Â§öÁ∫ß', 'pinyin': 'du≈ç j√≠', 'trans': 'multi-level'}, {'word': 'ÂàÜÂ∏ÉÂºè', 'pinyin': 'fƒìn b√π sh√¨', 'trans': 'distributed'}, {'word': 'Á≥ªÁªü', 'pinyin': 'x√¨ t«íng', 'trans': 'system'}, {'word': 'Â±ÇÊ¨°', 'pinyin': 'c√©ng c√¨', 'trans': 'hierarchy'}, {'word': 'ÁªìÊûÑ', 'pinyin': 'ji√© g√≤u', 'trans': 'structure'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«én sh√¨', 'trans': 'show'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'Êâ©Â±ï', 'pinyin': 'ku√≤ zh«én', 'trans': 'scale'}, {'word': 'ÂâçÊâÄÊú™Êúâ', 'pinyin': 'qi√°n su«í w√®i y«íu', 'trans': 'unprecedented'}, {'word': 'ÊâπÈáè', 'pinyin': 'pƒ´ li√†ng', 'trans': 'batch'}, {'word': 'Â§ßÂ∞è', 'pinyin': 'd√† xi«éo', 'trans': 'size'}, {'word': '‰øùÊåÅ', 'pinyin': 'b«éo ch√≠', 'trans': 'maintain'}, {'word': 'Á≤æÂ∫¶', 'pinyin': 'jƒ´ng d√π', 'trans': 'accuracy'}, {'word': '‰æãÂ¶Ç', 'pinyin': 'l√¨ r√∫', 'trans': 'for example'}, {'word': '‰ΩøÁî®', 'pinyin': 'sh«ê y√≤ng', 'trans': 'use'}, {'word': 'A800', 'pinyin': 'A800', 'trans': 'A800'}, {'word': '80GB', 'pinyin': '80GB', 'trans': '80GB'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πn li√†n', 'trans': 'train'}, {'word': 'CLIP-ViT-L/14', 'pinyin': 'CLIP-ViT-L/14', 'trans': 'CLIP-ViT-L/14'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': '‰∏é', 'pinyin': 'y«î', 'trans': 'with'}, {'word': 'ÊúÄÂÖàËøõ', 'pinyin': 'zu√¨ xiƒÅn j√¨n', 'trans': 'most advanced'}, {'word': 'ÂÜÖÂ≠òÈ´òÊïà', 'pinyin': 'n√®i c√∫n gƒÅo xi√†o', 'trans': 'memory-efficient'}, {'word': 'Ëß£ÂÜ≥ÊñπÊ°à', 'pinyin': 'jiƒõ ju√© fƒÅng √†n', 'trans': 'solution'}, {'word': 'Áõ∏ÊØî', 'pinyin': 'xiƒÅng b«ê', 'trans': 'compared to'}, {'word': 'Áõ∏ÂΩì', 'pinyin': 'xiƒÅng dƒÅng', 'trans': 'comparable'}, {'word': 'ÈÄüÂ∫¶', 'pinyin': 's√π d√π', 'trans': 'speed'}, {'word': 'ÂêåÊó∂', 'pinyin': 't√≥ng sh√≠', 'trans': 'simultaneously'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}, {'word': 'Êï∞ÈáèÁ∫ß', 'pinyin': 'sh√π li√†ng j√≠', 'trans': 'order of magnitude'}, {'word': 'ÂáèÂ∞ë', 'pinyin': 'ji«én sh«éo', 'trans': 'reduce'}, {'word': '‰ª£Á†Å', 'pinyin': 'd√†i m«é', 'trans': 'code'}, {'word': 'Â∞Ü', 'pinyin': 'jiƒÅng', 'trans': 'will'}, {'word': 'ÂÖ¨ÂºÄ', 'pinyin': 'g≈çng kƒÅi', 'trans': 'public'}, {'word': 'ÂèëÂ∏É', 'pinyin': 'fƒÅ b√π', 'trans': 'release'}]
[25.10.2024 10:15] Renaming previous Chinese page.
[25.10.2024 10:15] Renaming previous data. zh.html to ./d/2024-10-24_zh_reading_task.html
[25.10.2024 10:15] Writing result.
[25.10.2024 10:15] Writing Chinese reading task.
[25.10.2024 10:15] Renaming log file.
[25.10.2024 10:15] Renaming previous data. log.txt to ./logs/2024-10-25_last_log.txt
