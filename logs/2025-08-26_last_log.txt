[26.08.2025 22:11] Read previous papers.
[26.08.2025 22:11] Generating top page (month).
[26.08.2025 22:11] Writing top page (month).
[26.08.2025 23:10] Read previous papers.
[26.08.2025 23:10] Get feed.
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18265
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18032
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.16577
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17472
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.16949
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.16745
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18264
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17188
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18255
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17580
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17290
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17298
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.16790
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18190
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18159
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17973
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17821
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17811
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18076
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17326
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17061
[26.08.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2508.16838
[26.08.2025 23:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.08.2025 23:10] No deleted papers detected.
[26.08.2025 23:10] Downloading and parsing papers (pdf, html). Total: 22.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.18265.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.18265.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.18265.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.18032.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.18032.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.18032.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.16577.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.16577.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.16577.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.17472.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.17472.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.17472.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.16949.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.16949.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.16949.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.16745.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.16745.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.16745.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.18264.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.18264.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.18264.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.17188.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.17188.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.17188.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.18255.
[26.08.2025 23:10] Downloading paper 2508.18255 from http://arxiv.org/pdf/2508.18255v1...
[26.08.2025 23:10] Extracting affiliations from text.
[26.08.2025 23:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"HERMES 4 TECHNICAL REPORT 5 2 0 2 5 2 ] . [ 1 5 5 2 8 1 . 8 0 5 2 : r Ryan Teknium* X: @Teknium1 Roger Jin X: @rogershijin Jai Suphavadeeprasit X: @JSupa15 Dakota Mahan X: @dmayhem93 Jeffrey Quesnelle X: @theemozilla Joe Li X: @JoeLi Chen Guang X: @nullvaluetensor Shannon Sands X: @max_paperclips Karan Malhotra X: @karan4d "
[26.08.2025 23:10] Response: ```python
[]
```
[26.08.2025 23:10] Extracting affiliations from text.
[26.08.2025 23:10] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"HERMES 4 TECHNICAL REPORT 5 2 0 2 5 2 ] . [ 1 5 5 2 8 1 . 8 0 5 2 : r Ryan Teknium* X: @Teknium1 Roger Jin X: @rogershijin Jai Suphavadeeprasit X: @JSupa15 Dakota Mahan X: @dmayhem93 Jeffrey Quesnelle X: @theemozilla Joe Li X: @JoeLi Chen Guang X: @nullvaluetensor Shannon Sands X: @max_paperclips Karan Malhotra X: @karan4dWe present Hermes 4, family of hybrid reasoning models that combine structured, multi-step reasoning with broad instruction-following ability. We describe the challenges encountered during data curation, synthesis, training, and evaluation, and outline the solutions employed to address these challenges at scale. We comprehensively evaluate across mathematical reasoning, coding, knowledge, comprehension, and alignment benchmarks, and we report both quantitative performance and qualitative behavioral analysis. To support open research, all model weights are published publicly at https://huggingface.co/NousResearch2.LLMs, at this point, need no introduction. Their effectiveness at approximating aspects of human cognition despite comparatively simple training objectives has made them subject of both academic investigation and practical deployment. In recent years, growing body of work has explored inference-time scaling mechanisms (often described as reasoner" models) which dynamically adjust the amount of computation devoted to solving task [35, 15]. Initially, reasoner models were available only from proprietary providers. More recently, number of open-weight reasoning systems have been released, enabling the community to study, evaluate, and improve upon these methods in an open setting [57, 15, 48]. In this report we introduce Hermes 4, family of neutrally-aligned generalist models trained to integrate self-reflective reasoning with broad instructional competence. The contributions of this work are threefold: data synthesis and curation strategy that produces large-scale hybrid dataset consisting of both reasoning-focused and general purpose instruction examples (Section2); training methodology that incorporates loss-masking, length-control fine-tuning, and efficient packing strategies for large-scale heterogeneous data (Section 3); Correspondence to teknium@nousresearch.com. 2https://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728 Hermes 4 Technical Report comprehensive evaluation across reasoning, code, knowledge, alignment, and qualitative benchmarks (Section 4, 5). Together, these contributions demonstrate how open-weight reasoning models can be effectively trained and rigorously evaluated, yielding models that are comparable to frontier systems while remaining transparent and reproducible.The Hermes 4 dataset consists primarily of newly synthesized reasoning and non-reasoning data, totaling approximately 5 million samples and 19 billion tokens. It was designed to imbue the model with advanced reasoning capabilities while maintaining broad base of general knowledge and instruction-following proficiency. The data strategy focused on hybrid approach, combining substantial volume of reasoning-focused data with diverse set of non-reasoning instructions. The dataset is composite of 3.5 million reasoning samples and 1.6 million non-reasoning samples. significant portion of the Hermes 3 [51] dataset3 was retained to ensure continuity in the models capabilities. The reasoning samples were intentionally token-heavy, with an average of five times more tokens per sample than their non-reasoning counterparts, accommodating thinking traces up to 16 thousand tokens long.We process pre-training seed data through graph-based synthetic data generator called DataForge. Inspired by AgentInstruct [31] , DataForge generates conversational data across wide variety of tasks. Each datapoint is generated via random walk through directed acyclic graph (DAG) where each node implements struct struct map. Specifically, each node implements the PDDL [9] action interface, defining preconditions and postconditions which determine data flow. The edges in this DAG are thus implicit, i.e. directed edge exists from node to node if and only if the postconditions guaranteed by satisfy the preconditions of B. This design enables declarative construction of agent graphs, facilitating the specification of large and complex structures. We show an example graph in Figure 1 for single-turn QA data."
[26.08.2025 23:10] Mistral response. {"object": "error", "message": "Service tier capacity exceeded for this model.", "type": "service_tier_capacity_exceeded", "param": null, "code": "3505"}
[26.08.2025 23:10] Failed to download and parse paper https://huggingface.co/papers/2508.18255: 'choices'
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.17580.
[26.08.2025 23:10] Downloading paper 2508.17580 from http://arxiv.org/pdf/2508.17580v1...
[26.08.2025 23:10] Failed to download and parse paper https://huggingface.co/papers/2508.17580: maximum recursion depth exceeded
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.17290.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.17290.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.17290.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.17298.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.17298.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.17298.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.16790.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.16790.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.16790.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.18190.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.18190.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.18190.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.18159.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.18159.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.18159.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.17973.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.17973.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.17973.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.17821.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.17821.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.17821.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.17811.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.17811.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.17811.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.18076.
[26.08.2025 23:10] Extra JSON file exists (./assets/json/2508.18076.json), skip PDF parsing.
[26.08.2025 23:10] Paper image links file exists (./assets/img_data/2508.18076.json), skip HTML parsing.
[26.08.2025 23:10] Success.
[26.08.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2508.17326.
[26.08.2025 23:10] Downloading paper 2508.17326 from http://arxiv.org/pdf/2508.17326v1...
[26.08.2025 23:11] Extracting affiliations from text.
[26.08.2025 23:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . e [ 1 6 2 3 7 1 . 8 0 5 2 : r a Tristan S.W. Stevens , Ois√≠n Nolan , and Ruud J.G. van Sloun Eindhoven University of Technology, the Netherlands {t.s.w.stevens,o.i.nolan,r.j.g.v.sloun}@tue.nl Abstract. Echocardiography plays central role in cardiac imaging, offering dynamic views of the heart that are essential for diagnosis and monitoring. However, image quality can be significantly degraded by haze arising from multipath reverberations, particularly in difficult-to-image patients. In this work, we propose semantic-guided, diffusion-based dehazing algorithm developed for the MICCAI Dehazing Echocardiography Challenge (DehazingEcho2025). Our method integrates pixel-wise noise model, derived from semantic segmentation of hazy inputs into diffusion posterior sampling framework guided by generative prior trained on clean ultrasound data. Quantitative evaluation on the challenge dataset demonstrates strong performance across contrast and fidelity metrics. Code for the submitted algorithm is available on GitHub.1. Keywords: Cardiac Ultrasound Imaging Dehazing Diffusion Models. Ultrasound is popular modality for cardiac imaging due to its high temporal resolution, cost effectiveness, and real-time imaging capabilities, enabling the detection of variety of cardiac abnormalities [2]. An ongoing challenge in echocardiography is that of clutter or haze resulting from multipath reverberations [12,14], which can prevent accurate measurement from B-Mode images. This has motivated the development dehazing algorithms, which aim to recover clean images from hazy input images y. Recently, number of dehazing algorithms leveraging deep generative models (DGMs) have been proposed, using prior knowledge of the clean image distribution to infer sets of clean images corresponding to observed hazy images. One such approach involves using Generative Adversarial Networks (GANs) to perform domain adaptation, wherein the style of one dataset is transferred to samples from anoth"
[26.08.2025 23:11] Response: ```python
["Eindhoven University of Technology, the Netherlands"]
```
[26.08.2025 23:11] Deleting PDF ./assets/pdf/2508.17326.pdf.
[26.08.2025 23:11] Success.
[26.08.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2508.17061.
[26.08.2025 23:11] Extra JSON file exists (./assets/json/2508.17061.json), skip PDF parsing.
[26.08.2025 23:11] Paper image links file exists (./assets/img_data/2508.17061.json), skip HTML parsing.
[26.08.2025 23:11] Success.
[26.08.2025 23:11] Downloading and parsing paper https://huggingface.co/papers/2508.16838.
[26.08.2025 23:11] Extra JSON file exists (./assets/json/2508.16838.json), skip PDF parsing.
[26.08.2025 23:11] Paper image links file exists (./assets/img_data/2508.16838.json), skip HTML parsing.
[26.08.2025 23:11] Success.
[26.08.2025 23:11] Enriching papers with extra data.
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 0. InternVL 3.5 introduces Cascade RL, ViR, and DvD to enhance reasoning, efficiency, and performance in multimodal models.  					AI-generated summary 				 We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inferen...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 1. The Visual-Chain of Guidance (Visual-CoG) paradigm enhances text-to-image generation by providing stage-aware rewards, improving performance across multiple benchmarks.  					AI-generated summary 				 Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, t...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 2. MV-RAG enhances text-to-3D generation by retrieving 2D images and conditioning a multiview diffusion model to improve consistency and accuracy, especially for out-of-domain concepts.  					AI-generated summary 				 Text-to-3D generation approaches have advanced significantly by leveraging pretrained...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 3. T2I-ReasonBench evaluates the reasoning capabilities of text-to-image models across four dimensions using a two-stage protocol, analyzing their performance comprehensively.  					AI-generated summary 				 We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of text-to-image (T2I...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 4. RuscaRL, a novel instructional scaffolding framework, enhances LLM reasoning by using rubrics for exploration and verifiable rewards, significantly improving performance on reasoning tasks.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) have underscored the potential...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 5. Models trained on random Boolean functions in a cellular automata framework show that increasing depth, recurrence, memory, and test-time compute scaling enhances multi-step reasoning capabilities.  					AI-generated summary 				 Reasoning is a core capability of large language models, yet understan...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 6. A multimodal method leverages both vision and text tokens to optimize vision token selection, improving inference efficiency in vision-language models.  					AI-generated summary 				 Vision-Language Models (VLMs) demonstrate impressive performance in understanding visual content with language instr...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 7. PosterGen, a multi-agent framework using large language models, automates paper-to-poster generation with high design quality and minimal manual refinement.  					AI-generated summary 				 Multi-agent systems built upon large language models (LLMs) have demonstrated remarkable capabilities in tackli...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 8. Hermes 4, a hybrid reasoning model, integrates structured multi-turn reasoning with broad instruction-following, evaluated across various benchmarks including math, coding, knowledge, comprehension, and alignment.  					AI-generated summary 				 We present Hermes 4, a family of hybrid reasoning mode...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 9. UQ is a benchmark for evaluating AI models on unsolved questions, combining difficulty and realism to assess capabilities like reasoning, factuality, and browsing.  					AI-generated summary 				 Benchmarks shape progress in AI research. A useful benchmark should be both difficult and realistic: que...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 10. MEENA, a Persian-English dataset, evaluates vision-language models across scientific, reasoning, and human-level understanding tasks, enhancing capabilities beyond English.  					AI-generated summary 				 Recent advancements in large vision-language models (VLMs) have primarily focused on English, w...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 11. A comprehensive survey of compositional visual reasoning from 2023 to 2025 reviews advancements in multimodal AI, highlighting architectural designs, benchmarks, and future directions.  					AI-generated summary 				 Compositional visual reasoning has emerged as a key research frontier in multimodal...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 12. TaDiCodec, a Text-aware Diffusion Transformer Speech Codec, achieves low frame rates and bitrates with superior speech generation performance using end-to-end optimization and text guidance.  					AI-generated summary 				 Speech tokenizers serve as foundational components for speech language models...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 13. ST-Raptor, a tree-based framework using large language models, addresses challenges in answering questions from semi-structured tables by introducing a Hierarchical Orthogonal Tree and a two-stage verification mechanism.  					AI-generated summary 				 Semi-structured tables, widely used in real-wor...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 14. SpotEdit is a benchmark for evaluating visually-guided image editing methods, revealing performance disparities and hallucination issues across diffusion, autoregressive, and hybrid generative models.  					AI-generated summary 				 Visually-guided image editing, where edits are conditioned on both ...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 15. A large-scale German dataset and model for readability-controlled paraphrasing are introduced, achieving state-of-the-art performance in text simplification.  					AI-generated summary 				 The ability to paraphrase texts across different complexity levels is essential for creating accessible texts ...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 16. Theoretical and empirical analysis of softmax normalization in attention mechanisms reveals limitations in token selection and gradient sensitivity, highlighting the need for improved normalization strategies.  					AI-generated summary 				 This paper investigates the limitations of the normalizati...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 17. MeshSplat uses Gaussian Splatting and a feed-forward network to reconstruct surfaces from sparse views, improving accuracy with a Weighted Chamfer Distance Loss and normal prediction.  					AI-generated summary 				 Surface reconstruction has been widely studied in computer vision and graphics. Howe...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 18. The paper critiques the use of large language models as judges for evaluating natural language generation systems, questioning their reliability, capabilities, scalability, and cost-effectiveness.  					AI-generated summary 				 Evaluating natural language generation (NLG) systems remains a core cha...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 19. A semantic-guided diffusion-based dehazing algorithm improves echocardiography image quality by integrating a pixel-wise noise model and a generative prior.  					AI-generated summary 				 Echocardiography plays a central role in cardiac imaging, offering dynamic views of the heart that are essentia...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 20. A dual-stage generative network framework enhances photorealism in real-time video game rendering by improving inference speed and visual quality.  					AI-generated summary 				 Photorealism is an important aspect of modern video games since it can shape the player experience and simultaneously imp...
[26.08.2025 23:11] ********************************************************************************
[26.08.2025 23:11] Abstract 21. A structured claim verification framework reduces prompt sensitivity and presupposition issues in large language models, improving performance by 2-5%.  					AI-generated summary 				 Prior work has shown that presupposition in generated questions can introduce unverified assumptions, leading to inc...
[26.08.2025 23:11] Read previous papers.
[26.08.2025 23:11] Generating reviews via LLM API.
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#training", "#agents", "#reasoning", "#architecture", "#inference", "#open_source", "#multimodal", "#rl"], "emoji": "üß†", "ru": {"title": "InternVL 3.5: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò —á–µ—Ä–µ–∑ –∫–∞—Å–∫–∞–¥–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "InternVL 3.5 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ 
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#games", "#benchmark", "#reasoning", "#rl", "#optimization", "#cv"], "emoji": "üé®", "ru": {"title": "–ü–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É Visual-Chain of Guidance (Visual-CoG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#synthetic", "#diffusion", "#3d", "#multimodal", "#benchmark", "#rag"], "emoji": "üñºÔ∏è", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –∏–∑–≤–ª–µ—á–µ–Ω–∏—è 2D-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "MV-RAG - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ú–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑—É–º–Ω–æ—Å—Ç–∏ text-to-image –º–æ–¥–µ–ª–µ–π", "desc": "T2I-ReasonBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π text-to-image –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç —á–µ—Ç—ã—Ä–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è: –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –∏–¥–∏–æ–º, —Ç–µ–∫—Å—Ç
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#reasoning", "#benchmark"], "emoji": "üß†", "ru": {"title": "RuscaRL: –£—Å–∏–ª–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "RuscaRL - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ–¥–∫—Ä–µ–ø–ª—è—é—â–µ–≥–æ –æ–±—É—á–µ
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#reasoning", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–ì–ª—É–±–∏–Ω–∞ –∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å - –∫–ª—é—á –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –∫–∞–∫ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è –≤–ª–∏—è—é—Ç –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ 
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#inference", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ú–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã–±–æ—Ä–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#optimization", "#agi", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "PosterGen: –ò–ò-–¥–∏–∑–∞–π–Ω–µ—Ä –Ω–∞—É—á–Ω—ã—Ö –ø–æ—Å—Ç–µ—Ä–æ–≤", "desc": "PosterGen - —ç—Ç–æ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å—Ç–µ—Ä–æ–≤ –∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π. –°–∏—Å—Ç–µ
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#data", "#alignment", "#dataset", "#open_source", "#reasoning", "#training", "#benchmark", "#architecture"], "emoji": "üß†", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–π –ò–ò: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Hermes 4, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–Ω–æ–≥–æ—ç—Ç–∞
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#data", "#survey", "#reasoning", "#open_source"], "emoji": "üß†", "ru": {"title": "UQ: –û—Ü–µ–Ω–∫–∞ –ò–ò –Ω–∞ –≥—Ä–∞–Ω–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π", "desc": "UQ - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –Ω–∞ –Ω–µ—Ä–µ—à–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö, —Å–æ—á–µ—Ç–∞—é—â–∏–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏ —Ä–µ–∞–ª–∏—Å
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#low_resource", "#dataset", "#reasoning", "#hallucinations", "#multilingual", "#benchmark"], "emoji": "üåê", "ru": {"title": "MEENA: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö MEENA –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–µ—Ä—Å–∏–¥—Å–∫–æ-–∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –º
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#benchmark", "#hallucinations", "#survey", "#multimodal", "#interpretability"], "emoji": "üß†", "ru": {"title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –æ—Ç –ø–æ–¥—Å–∫–∞–∑–æ–∫ –∫ –∞–≥–µ–Ω—Ç–∞–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –æ–±–∑–æ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ –æ–±–ª–∞—Å—Ç–∏
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#audio", "#open_source"], "emoji": "üó£Ô∏è", "ru": {"title": "TaDiCodec: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ —Ç–µ–∫—Å—Ç–∞", "desc": "TaDiCodec - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é —Ä–µ—á–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ. –û–Ω 
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#reasoning", "#multimodal", "#data", "#interpretability"], "emoji": "üå≥", "ru": {"title": "ST-Raptor: –î—Ä–µ–≤–æ–≤–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–Ω–∞–ª–∏–∑—É —Å–ª–æ–∂–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "ST-Raptor - —ç—Ç–æ –¥—Ä–µ–≤–æ–≤–∏–¥–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –ø–æ–ª—É—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç–∞–±–ª
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#benchmark", "#diffusion", "#open_source", "#hallucinations", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "SpotEdit: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "SpotEdit ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —É–ø—Ä–∞–≤–ª—è–µ–º—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏. –û–Ω
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#data", "#training", "#synthetic"], "emoji": "üìö", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤: German4All –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –ø–∞—Ä–∞—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω German4All - –ø–µ—Ä–≤—ã–π –º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–µ–º–µ—Ü–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –ø–∞—Ä–∞—Ñ—Ä–∞–∑–æ–≤ —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#architecture", "#interpretability", "#optimization", "#training", "#math"], "emoji": "üîç", "ru": {"title": "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å–æ—Ñ—Ç–º–∞–∫—Å-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –º–µ—Ö–∞–Ω–∏–∑–º–∞—Ö –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –º–µ—Ö–∞–Ω–∏–∑–º–∞—Ö –≤–Ω–∏–º–∞–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ç–µ–æ—Ä–µ—Ç
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#cv", "#3d"], "emoji": "üî¨", "ru": {"title": "–¢–æ—á–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è 3D-–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º", "desc": "MeshSplat - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π –ø–æ –º–∞–ª–æ–º—É —á–∏—Å–ª—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π Gaussian Splatting –∏ –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è. –î–ª—è –ø
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#ethics", "#benchmark", "#alignment", "#multimodal", "#interpretability"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ —Å—É–¥—å–∏: –ø—Ä–µ–∂–¥–µ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —ç–Ω—Ç—É–∑–∏–∞–∑–º?", "desc": "–°—Ç–∞—Ç—å—è –∫—Ä–∏—Ç–∏–∫—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å—É–¥–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º –≥–µ–Ω–µ—Ä–∞
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#cv", "#open_source", "#dataset", "#healthcare", "#diffusion"], "emoji": "ü´Ä", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–µ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥—ã–º–∫–∏ –¥–ª—è —á–µ—Ç–∫–∏—Ö —ç—Ö–æ–∫–∞—Ä–¥–∏–æ–≥—Ä–∞–º–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —ç—Ö–æ–∫–∞—Ä–¥–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —É–ø
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#architecture", "#open_source", "#cv", "#games", "#video", "#optimization", "#inference"], "emoji": "üéÆ", "ru": {"title": "–§–æ—Ç–æ—Ä–µ–∞–ª–∏–∑–º –≤ –∏–≥—Ä–∞—Ö –Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å–≤–µ—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –≤ –≤–∏–¥–µ–æ–∏–≥—Ä–∞—Ö —Å –ø–æ–º–æ—â—å—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω
[26.08.2025 23:11] Using data from previous issue: {"categories": ["#benchmark", "#hallucinations", "#multimodal", "#reasoning", "#data"], "emoji": "üîç", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —Å
[26.08.2025 23:11] Renaming data file.
[26.08.2025 23:11] Renaming previous data. hf_papers.json to ./d/2025-08-26.json
[26.08.2025 23:11] Saving new data file.
[26.08.2025 23:11] Generating page.
[26.08.2025 23:11] Renaming previous page.
[26.08.2025 23:11] Renaming previous data. index.html to ./d/2025-08-26.html
[26.08.2025 23:11] Writing result.
[26.08.2025 23:11] Renaming log file.
[26.08.2025 23:11] Renaming previous data. log.txt to ./logs/2025-08-26_last_log.txt
