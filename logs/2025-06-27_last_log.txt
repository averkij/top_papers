[27.06.2025 12:21] Read previous papers.
[27.06.2025 12:21] Generating top page (month).
[27.06.2025 12:21] Writing top page (month).
[27.06.2025 13:26] Read previous papers.
[27.06.2025 13:26] Get feed.
[27.06.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20670
[27.06.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21520
[27.06.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21539
[27.06.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21551
[27.06.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21506
[27.06.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21547
[27.06.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20911
[27.06.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21552
[27.06.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16655
[27.06.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21272
[27.06.2025 13:26] Extract page data from URL. URL: https://huggingface.co/papers/2506.21263
[27.06.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20430
[27.06.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15196
[27.06.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21103
[27.06.2025 13:26] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18729
[27.06.2025 13:26] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.06.2025 13:26] No deleted papers detected.
[27.06.2025 13:26] Downloading and parsing papers (pdf, html). Total: 15.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.20670.
[27.06.2025 13:26] Extra JSON file exists (./assets/json/2506.20670.json), skip PDF parsing.
[27.06.2025 13:26] Paper image links file exists (./assets/img_data/2506.20670.json), skip HTML parsing.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.21520.
[27.06.2025 13:26] Extra JSON file exists (./assets/json/2506.21520.json), skip PDF parsing.
[27.06.2025 13:26] Paper image links file exists (./assets/img_data/2506.21520.json), skip HTML parsing.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.21539.
[27.06.2025 13:26] Extra JSON file exists (./assets/json/2506.21539.json), skip PDF parsing.
[27.06.2025 13:26] Paper image links file exists (./assets/img_data/2506.21539.json), skip HTML parsing.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.21551.
[27.06.2025 13:26] Extra JSON file exists (./assets/json/2506.21551.json), skip PDF parsing.
[27.06.2025 13:26] Paper image links file exists (./assets/img_data/2506.21551.json), skip HTML parsing.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.21506.
[27.06.2025 13:26] Extra JSON file exists (./assets/json/2506.21506.json), skip PDF parsing.
[27.06.2025 13:26] Paper image links file exists (./assets/img_data/2506.21506.json), skip HTML parsing.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.21547.
[27.06.2025 13:26] Extra JSON file exists (./assets/json/2506.21547.json), skip PDF parsing.
[27.06.2025 13:26] Paper image links file exists (./assets/img_data/2506.21547.json), skip HTML parsing.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.20911.
[27.06.2025 13:26] Extra JSON file exists (./assets/json/2506.20911.json), skip PDF parsing.
[27.06.2025 13:26] Paper image links file exists (./assets/img_data/2506.20911.json), skip HTML parsing.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.21552.
[27.06.2025 13:26] Extra JSON file exists (./assets/json/2506.21552.json), skip PDF parsing.
[27.06.2025 13:26] Paper image links file exists (./assets/img_data/2506.21552.json), skip HTML parsing.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.16655.
[27.06.2025 13:26] Extra JSON file exists (./assets/json/2506.16655.json), skip PDF parsing.
[27.06.2025 13:26] Paper image links file exists (./assets/img_data/2506.16655.json), skip HTML parsing.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.21272.
[27.06.2025 13:26] Extra JSON file exists (./assets/json/2506.21272.json), skip PDF parsing.
[27.06.2025 13:26] Paper image links file exists (./assets/img_data/2506.21272.json), skip HTML parsing.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.21263.
[27.06.2025 13:26] Downloading paper 2506.21263 from http://arxiv.org/pdf/2506.21263v1...
[27.06.2025 13:26] Extracting affiliations from text.
[27.06.2025 13:26] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster Ji Qi 1 WenPeng Zhu 1 Li Li 1 Ming Wu 2 YingJun Wu 1 Wu He 1 Xun Gao 1 Jason Zeng 2 Michael Heinrich "
[27.06.2025 13:26] Response: ```python
[]
```
[27.06.2025 13:26] Extracting affiliations from text.
[27.06.2025 13:26] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster Ji Qi 1 WenPeng Zhu 1 Li Li 1 Ming Wu 2 YingJun Wu 1 Wu He 1 Xun Gao 1 Jason Zeng 2 Michael HeinrichThe distributed training of foundation models, particularly large language models (LLMs), demands high level of communication. Consequently, it is highly dependent on centralized cluster with fast and reliable interconnects. Can we conduct training on slow networks and thereby unleash the power of decentralized clusters when dealing with models exceeding 100 billion parameters? In this paper, we propose DiLoCoX, low-communication large-scale decentralized cluster training framework. It combines Pipeline Parallelism with Dual Optimizer Policy, OneStep-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme. This combination significantly improves the scale of parameters and the speed of model pre-training. We justify the benefits of one-stepdelay overlap of communication and local training, as well as the adaptive gradient compression scheme, through theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training 107B foundation model over 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve 357x speedup in distributed training while maintaining negligible degradation in model convergence. To the best of our knowledge, this is the first decentralized training framework successfully applied to models with over 100 billion parameters. 5 2 0 2 6 2 ] . [ 1 3 6 2 1 2 . 6 0 5 2 : r 1. Introduction LLMs have quickly become dominant in the field of AI due to their exceptional capabilities. Their effectiveness in areas such as automated dialogue generation, machine translation, content summarization, and recommendation systems clearly demonstrates their superior advantages (Zhao et al., 1China Mobile(Suzhou) Software Technology, JiangSu, Ji Qi Correspondence to: China 2Zero Gravity Labs. <qiji@cmss.chinamobile.com>. 2023a; Wang et al., 2022; Bommasani et al., 2021). However, as efforts to improve the accuracy of these models continue, both their complexity and scale have grown exponentially. In recent years, the number of parameters has increased from billions (Devlin, 2018) to trillions (Fedus et al., 2022; Du et al., 2022), introducing significant challenges in model training. For example, pre-training model with 175B (Brown, 2020) parameters can require 12,000 GPUs operating for 118 days (Narayanan et al., 2021). In distributed foundation model training, communication is the key bottleneck. As an example, fine-tuning GPT-J-6B over 10B tokens with 262K batch size across 4 machines (2 A100 GPUs each) demands 915.5 TB data communication in total training(Wang et al., 2023). As result, the highspeed centralized cluster is currently the dominant solution for training foundation model(Rendle et al., 2016). Decentralized clusters usually have significant amount of computing resources than centralized cluster, but the bandwidth between clusters is relatively slower, with only hundreds of Mbps to 1 Gbps in common. When using training framework such as Megatron-LM(Shoeybi et al., 2019) for distributed cluster training, the bandwidth between clusters often becomes training bottleneck(Dai et al., 2024; Zhang et al., 2022), resulting in the inability to fully utilize the computing resources of decentralized clusters for model training. Recently, there has been an exciting collection of work focusing on the decentralized training of foundation models. By introducing outer Nesterov momentum optimizers, DiLoCo (Douillard et al., 2023) can train model with up to 400M parameters using only data parallelism and achieve even better performance than fully synchronous model. OpenDiLoCo(Jaghouar et al., 2024) is an opensource implementation of DiLoCo. With FP16 gradient compression and Hivemind optimized collective communication operators, it can train the 1.1B model over poorly connected networks with bandwidth of hundreds of Mbps and negligible degradation in convergence. CocktailSGD (Wang et al., 2023) combines three distinct compression techniques, achieving 117 aggressive compression in finetuning LLMs with up to 20B parameters over 500Mbps network. 1 DiLoCoX: Low-Communication Large-Scale Training Framework for Decentralized Cluster However, despite these recent efforts, the scale of model parameters and communication bottleneck is still challenge when pretraining over 100B parameter models. DiLoCo and OpenDiLoCo (Douillard et al., 2023; Jaghouar et al., 2024) have comparable model convergence with the fully synchronous model. However, they only use data parallelism and do not support FSDP (Zhao et al., 2023b) or deepspeed (Rajbhandari et al., 2020), thus the VRAM of GPU capacity limits the scale of model parameters. Furthermore, DiLoCo and OpenDiloCo (Douillard et al., 2023; Jaghouar et al., 2024) employ pseudo-gradients synchronous mechanism. During the synchronization of pseudo-gradients, local training is in an idle state. When the model scale is large and network bandwidth is limited, the idle time for synchronizing pseudo-gradients becomes unacceptable. CocktailSGD (Wang et al., 2023) uses pipeline parallelism to support 20B model training, but the compression ratio of data parallelism is aggressive up to 117 times and does not use local training like DiLoCo or OpenDiLoCo in decentralized clusters, which has potential impact on model convergence. In order to train models with scale of more than 100B parameters on low-bandwidth decentralized clusters while having comparative model convergence, we have identified the following key challenges: 1. Introduce model parallelism to address the limitation of VRAM which has to accommodate the whole model parameters. 2. The overlap between the synchronization of pseudo-gradients and local training to avoid the idleness of computing resources. 3. Design an efficient gradient compression algorithm and balance it with the number of local training steps to ensure the convergence of model training. To address the above challenges, we propose low communication large-scale model training framework DiLoCoX for decentralized cluster. Experiments demonstrate that DiLoCoX can pre-train 107B model and significantly hide communication overhead while ensuring model convergence on decentralized clusters with only 1Gbps network bandwidth. To the best of our knowledge, this is currently the largest-sc"
[27.06.2025 13:26] Mistral response. {"id": "4137294f030748358d536c349c77cac8", "object": "chat.completion", "created": 1751030772, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['China Mobile(Suzhou) Software Technology, JiangSu', 'Zero Gravity Labs']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1711, "total_tokens": 1741, "completion_tokens": 30}}
[27.06.2025 13:26] Response: ```python
['China Mobile(Suzhou) Software Technology, JiangSu', 'Zero Gravity Labs']
```
[27.06.2025 13:26] Deleting PDF ./assets/pdf/2506.21263.pdf.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.20430.
[27.06.2025 13:26] Extra JSON file exists (./assets/json/2506.20430.json), skip PDF parsing.
[27.06.2025 13:26] Paper image links file exists (./assets/img_data/2506.20430.json), skip HTML parsing.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.15196.
[27.06.2025 13:26] Extra JSON file exists (./assets/json/2506.15196.json), skip PDF parsing.
[27.06.2025 13:26] Paper image links file exists (./assets/img_data/2506.15196.json), skip HTML parsing.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.21103.
[27.06.2025 13:26] Extra JSON file exists (./assets/json/2506.21103.json), skip PDF parsing.
[27.06.2025 13:26] Paper image links file exists (./assets/img_data/2506.21103.json), skip HTML parsing.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Downloading and parsing paper https://huggingface.co/papers/2506.18729.
[27.06.2025 13:26] Extra JSON file exists (./assets/json/2506.18729.json), skip PDF parsing.
[27.06.2025 13:26] Paper image links file exists (./assets/img_data/2506.18729.json), skip HTML parsing.
[27.06.2025 13:26] Success.
[27.06.2025 13:26] Enriching papers with extra data.
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 0. MMSearch-R1, a reinforcement learning framework, enables large multimodal models to perform efficient, on-demand, multi-turn search in real-world environments, outperforming existing approaches.  					AI-generated summary 				 Robust deployment of large multimodal models (LMMs) in real-world scenari...
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 1. MADrive enhances scene reconstruction for autonomous driving by integrating visually similar 3D car assets from an external memory bank to achieve photorealistic synthesis of altered scenarios.  					AI-generated summary 				 Recent advances in scene reconstruction have pushed toward highly realisti...
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 2. WorldVLA, an autoregressive action world model integrating vision-language-action (VLA) and world models, enhances performance through mutual understanding and generation, improving action prediction and sequence generation with an attention mask strategy.  					AI-generated summary 				 We present ...
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 3. Grokking, or continued test performance improvement after training loss convergence, is observed during pretraining of a large language model, showcasing a memorization-to-generalization process.  					AI-generated summary 				 Grokking, i.e., test performance keeps improving long after training los...
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 4. Mind2Web 2 benchmark evaluates agentic search systems with a suite of realistic, long-horizon tasks, introducing an Agent-as-a-Judge framework to assess accuracy and source attribution.  					AI-generated summary 				 Agentic search such as Deep Research systems, where large language models autonomo...
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 5. SAM4D is a multi-modal and temporal foundation model for segmentation in autonomous driving using Unified Multi-modal Positional Encoding and Motion-aware Cross-modal Memory Attention, with a multi-modal automated data engine generating pseudo-labels.  					AI-generated summary 				 We present SAM4D...
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 6. A neurosymbolic agent combines language models for fast subtask planning with A$^*$ search for detailed toolpaths, creating a cost-efficient multi-turn image editing solution.  					AI-generated summary 				 We develop a cost-efficient neurosymbolic agent to address challenging multi-turn image edit...
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 7. A model trained on real-world egocentric video and body pose predicts video from human actions using an auto-regressive conditional diffusion transformer, evaluated with a hierarchical protocol of tasks.  					AI-generated summary 				 We train models to Predict Ego-centric Video from human Actions ...
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 8. A preference-aligned routing framework using a compact 1.5B model effectively matches queries to user-defined domains and action types, outperforming proprietary models in subjective evaluation criteria.  					AI-generated summary 				 With the rapid proliferation of large language models (LLMs) -- ...
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 9. FairyGen generates story-driven cartoon videos from a single drawing by disentangling character modeling and background styling, employing MLLM for storyboards, style propagation for consistency, and MMDiT-based diffusion models for motion.  					AI-generated summary 				 We propose FairyGen, an aut...
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 10. DiLoCoX, a decentralized cluster training framework, enhances the training of large-scale models over slow networks by utilizing pipeline parallelism, dual optimizer policy, and gradient compression, achieving significant speed improvements and effective scalability.  					AI-generated summary 				 ...
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 11. DeepRare, a large language model-based system, provides accurate rare disease diagnoses using heterogeneous clinical inputs and outperforms other diagnostic methods across various datasets.  					AI-generated summary 				 Rare diseases collectively affect over 300 million individuals worldwide, yet ...
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 12. HeurAgenix, a two-stage hyper-heuristic framework using large language models, evolves and selects heuristics dynamically for combinatorial optimization problems, achieving performance on par with specialized solvers.  					AI-generated summary 				 Heuristic algorithms play a vital role in solving ...
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 13. A novel conditional computation architecture for Transformers dynamically skips middle layers based on input and a gating mechanism, but does not outperform dense baselines in reducing computational cost or improving validation performance.  					AI-generated summary 				 Conditional computation is ...
[27.06.2025 13:26] ********************************************************************************
[27.06.2025 13:26] Abstract 14. Rotary positional embeddings enhance time-varying control in text-to-music generation models with fewer parameters.  					AI-generated summary 				 We propose MuseControlLite, a lightweight mechanism designed to fine-tune text-to-music generation models for precise conditioning using various time-va...
[27.06.2025 13:26] Read previous papers.
[27.06.2025 13:26] Generating reviews via LLM API.
[27.06.2025 13:26] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#reasoning", "#games", "#rl", "#rag", "#multimodal"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "MMSearch-R1 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º –º—É–ª—å—Ç–∏–º–æ–¥
[27.06.2025 13:26] Using data from previous issue: {"categories": ["#3d", "#multimodal", "#games", "#synthetic", "#dataset"], "emoji": "üöó", "ru": {"title": "–§–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ü–µ–Ω –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –≤–Ω–µ—à–Ω–µ–π –ø–∞–º—è—Ç–∏", "desc": "MADrive - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º –≤–æ–∂–¥–µ–Ω–∏–∏. –û–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤–∏–∑—É–∞
[27.06.2025 13:26] Using data from previous issue: {"categories": ["#optimization", "#rl", "#multimodal", "#games", "#cv"], "emoji": "üåê", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –∏ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "WorldVLA - —ç—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –¥–µ–π—Å—Ç–≤–∏–π. –û–Ω–∞
[27.06.2025 13:26] Using data from previous issue: {"categories": ["#math", "#data", "#optimization", "#reasoning", "#benchmark", "#training"], "emoji": "üß†", "ru": {"title": "–û—Ç –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫ –æ–±–æ–±—â–µ–Ω–∏—é: —Ä–∞—Å–∫—Ä—ã–≤–∞—è —Ç–∞–π–Ω—ã –≥—Ä–æ–∫–∏–Ω–≥–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —è–≤–ª–µ–Ω–∏–µ –≥—Ä–æ–∫–∏–Ω–≥–∞ (grokking) –ø—Ä–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ
[27.06.2025 13:26] Using data from previous issue: {"categories": ["#interpretability", "#optimization", "#agents", "#agi", "#benchmark"], "emoji": "üïµÔ∏è", "ru": {"title": "Mind2Web 2: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞", "desc": "Mind2Web 2 - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, –≤–∫–ª—é—á–∞—é—â–∏–π 130 —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á —Å –¥–ª–∏—Ç–µ–ª—å–Ω—ã–º –≥–æ—Ä
[27.06.2025 13:26] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#games", "#multimodal", "#cv"], "emoji": "üöó", "ru": {"title": "SAM4D: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –±–µ—Å–ø–∏–ª–æ—Ç–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π", "desc": "SAM4D - —ç—Ç–æ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–∞—è –∏ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–∞—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º –≤–æ–∂–¥–µ–Ω–∏–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–Ω
[27.06.2025 13:26] Using data from previous issue: {"categories": ["#cv", "#agents", "#optimization", "#reasoning"], "emoji": "üé®", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–≥–æ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–π –∞–≥–µ–Ω—Ç –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω —Å–æ—á–µ—Ç–∞–µ—Ç –±—ã—Å—Ç
[27.06.2025 13:26] Using data from previous issue: {"categories": ["#diffusion", "#agents", "#games", "#dataset", "#video"], "emoji": "üé•", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –≤–∏–¥–µ–æ –ø–æ –¥–µ–π—Å—Ç–≤–∏—è–º —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å PEVA, –æ–±—É—á–µ–Ω–Ω—É—é –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–µ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–π—Å—Ç–≤–∏–π —á–µ–ª–æ–≤–µ–∫–∞. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 
[27.06.2025 13:26] Using data from previous issue: {"categories": ["#training", "#alignment", "#small_models", "#multimodal"], "emoji": "üîÄ", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –ò–ò —Å —É—á–µ—Ç–æ–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
[27.06.2025 13:26] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#video", "#diffusion", "#story_generation"], "emoji": "üé®", "ru": {"title": "–û—Ç –¥–µ—Ç—Å–∫–æ–≥–æ —Ä–∏—Å—É–Ω–∫–∞ –∫ –º—É–ª—å—Ç—Ñ–∏–ª—å–º—É: FairyGen –æ–∂–∏–≤–ª—è–µ—Ç –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "desc": "FairyGen - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º—É–ª—å—Ç—Ñ–∏–ª—å–º–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É –¥–µ—Ç—Å–∫–æ–º—É —Ä–∏—Å—É–Ω–∫—É, —Å–æ—Ö—Ä–∞–Ω—è—é—â–∞—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ö—É–¥–æ–∂–µ—Å—Ç–≤
[27.06.2025 13:26] Querying the API.
[27.06.2025 13:26] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DiLoCoX, a decentralized cluster training framework, enhances the training of large-scale models over slow networks by utilizing pipeline parallelism, dual optimizer policy, and gradient compression, achieving significant speed improvements and effective scalability.  					AI-generated summary 				 The distributed training of foundation models, particularly large language models (LLMs), demands a high level of communication. Consequently, it is highly dependent on a centralized cluster with fast and reliable interconnects. Can we conduct training on slow networks and thereby unleash the power of decentralized clusters when dealing with models exceeding 100 billion parameters? In this paper, we propose DiLoCoX, a low-communication large-scale decentralized cluster training framework. It combines Pipeline Parallelism with Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme. This combination significantly improves the scale of parameters and the speed of model pre-training. We justify the benefits of one-step-delay overlap of communication and local training, as well as the adaptive gradient compression scheme, through a theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training a 107B foundation model over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x speedup in distributed training while maintaining negligible degradation in model convergence. To the best of our knowledge, this is the first decentralized training framework successfully applied to models with over 100 billion parameters.
[27.06.2025 13:26] Response: {
  "desc": "DiLoCoX - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –º–µ–¥–ª–µ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω–≤–µ–π–µ—Ä–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º, –ø–æ–ª–∏—Ç–∏–∫—É –¥–≤–æ–π–Ω–æ–≥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Å–∂–∞—Ç–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤. DiLoCoX –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏ —Å –±–æ–ª–µ–µ —á–µ–º 100 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ —Å–µ—Ç—è—Ö —Å–æ —Å–∫–æ—Ä–æ—Å—Ç—å—é 1 –ì–±–∏—Ç/—Å, –¥–æ—Å—Ç–∏–≥–∞—è 357-–∫—Ä–∞—Ç–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—ã—á–Ω—ã–º AllReduce.",

  "emoji": "üöÄ",

  "title": "DiLoCoX: –ü—Ä–æ—Ä—ã–≤ –≤ –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
[27.06.2025 13:26] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DiLoCoX, a decentralized cluster training framework, enhances the training of large-scale models over slow networks by utilizing pipeline parallelism, dual optimizer policy, and gradient compression, achieving significant speed improvements and effective scalability.  					AI-generated summary 				 The distributed training of foundation models, particularly large language models (LLMs), demands a high level of communication. Consequently, it is highly dependent on a centralized cluster with fast and reliable interconnects. Can we conduct training on slow networks and thereby unleash the power of decentralized clusters when dealing with models exceeding 100 billion parameters? In this paper, we propose DiLoCoX, a low-communication large-scale decentralized cluster training framework. It combines Pipeline Parallelism with Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme. This combination significantly improves the scale of parameters and the speed of model pre-training. We justify the benefits of one-step-delay overlap of communication and local training, as well as the adaptive gradient compression scheme, through a theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training a 107B foundation model over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x speedup in distributed training while maintaining negligible degradation in model convergence. To the best of our knowledge, this is the first decentralized training framework successfully applied to models with over 100 billion parameters."

[27.06.2025 13:26] Response: ```python
["TRAINING", "ARCHITECTURE"]
```
[27.06.2025 13:26] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DiLoCoX, a decentralized cluster training framework, enhances the training of large-scale models over slow networks by utilizing pipeline parallelism, dual optimizer policy, and gradient compression, achieving significant speed improvements and effective scalability.  					AI-generated summary 				 The distributed training of foundation models, particularly large language models (LLMs), demands a high level of communication. Consequently, it is highly dependent on a centralized cluster with fast and reliable interconnects. Can we conduct training on slow networks and thereby unleash the power of decentralized clusters when dealing with models exceeding 100 billion parameters? In this paper, we propose DiLoCoX, a low-communication large-scale decentralized cluster training framework. It combines Pipeline Parallelism with Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme. This combination significantly improves the scale of parameters and the speed of model pre-training. We justify the benefits of one-step-delay overlap of communication and local training, as well as the adaptive gradient compression scheme, through a theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training a 107B foundation model over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x speedup in distributed training while maintaining negligible degradation in model convergence. To the best of our knowledge, this is the first decentralized training framework successfully applied to models with over 100 billion parameters."

[27.06.2025 13:26] Response: ```python
["OPTIMIZATION"]
```
[27.06.2025 13:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DiLoCoX is a decentralized training framework designed to improve the training of large-scale models, especially those with over 100 billion parameters, over slow networks. It employs techniques like pipeline parallelism and a dual optimizer policy to enhance communication efficiency and speed. The framework also introduces an adaptive gradient compression scheme, which helps in reducing the amount of data that needs to be communicated during training. Empirical results show that DiLoCoX can achieve a remarkable 357x speedup in distributed training compared to traditional methods, while still ensuring effective model convergence.","title":"Revolutionizing Large-Scale Model Training on Slow Networks"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DiLoCoX is a decentralized training framework designed to improve the training of large-scale models, especially those with over 100 billion parameters, over slow networks. It employs techniques like pipeline parallelism and a dual optimizer policy to enhance communication efficiency and speed. The framework also introduces an adaptive gradient compression scheme, which helps in reducing the amount of data that needs to be communicated during training. Empirical results show that DiLoCoX can achieve a remarkable 357x speedup in distributed training compared to traditional methods, while still ensuring effective model convergence.', title='Revolutionizing Large-Scale Model Training on Slow Networks'))
[27.06.2025 13:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DiLoCoXÊòØ‰∏ÄÁßçÂéª‰∏≠ÂøÉÂåñÁöÑÈõÜÁæ§ËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßËßÑÊ®°Ê®°ÂûãÂú®ÊÖ¢ÈÄüÁΩëÁªú‰∏äÁöÑËÆ≠ÁªÉÊïàÁéá„ÄÇÂÆÉÁªìÂêà‰∫ÜÁÆ°ÈÅìÂπ∂Ë°å„ÄÅÂèå‰ºòÂåñÂô®Á≠ñÁï•ÂíåÊ¢ØÂ∫¶ÂéãÁº©Á≠âÊäÄÊúØÔºåÊòæËëóÊèêÂçá‰∫ÜËÆ≠ÁªÉÈÄüÂ∫¶ÂíåÂèØÊâ©Â±ïÊÄß„ÄÇÈÄöËøáÁêÜËÆ∫ÂàÜÊûêÂíåÂÆûËØÅÈ™åËØÅÔºåDiLoCoXËÉΩÂ§üÂú®1GbpsÁΩëÁªú‰∏äÊàêÂäüÈ¢ÑËÆ≠ÁªÉË∂ÖËøá107‰∫øÂèÇÊï∞ÁöÑÂü∫Á°ÄÊ®°Âûã„ÄÇ‰∏é‰º†ÁªüÁöÑAllReduceÊñπÊ≥ïÁõ∏ÊØîÔºåDiLoCoXÂú®ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ‰∏≠ÂÆûÁé∞‰∫Ü357ÂÄçÁöÑÂä†ÈÄüÔºåÂêåÊó∂Ê®°ÂûãÊî∂ÊïõÊÄßÂá†‰πéÊ≤°Êúâ‰∏ãÈôç„ÄÇ","title":"Âéª‰∏≠ÂøÉÂåñÈõÜÁæ§ËÆ≠ÁªÉÁöÑÈÄüÂ∫¶Èù©ÂëΩ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DiLoCoXÊòØ‰∏ÄÁßçÂéª‰∏≠ÂøÉÂåñÁöÑÈõÜÁæ§ËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßËßÑÊ®°Ê®°ÂûãÂú®ÊÖ¢ÈÄüÁΩëÁªú‰∏äÁöÑËÆ≠ÁªÉÊïàÁéá„ÄÇÂÆÉÁªìÂêà‰∫ÜÁÆ°ÈÅìÂπ∂Ë°å„ÄÅÂèå‰ºòÂåñÂô®Á≠ñÁï•ÂíåÊ¢ØÂ∫¶ÂéãÁº©Á≠âÊäÄÊúØÔºåÊòæËëóÊèêÂçá‰∫ÜËÆ≠ÁªÉÈÄüÂ∫¶ÂíåÂèØÊâ©Â±ïÊÄß„ÄÇÈÄöËøáÁêÜËÆ∫ÂàÜÊûêÂíåÂÆûËØÅÈ™åËØÅÔºåDiLoCoXËÉΩÂ§üÂú®1GbpsÁΩëÁªú‰∏äÊàêÂäüÈ¢ÑËÆ≠ÁªÉË∂ÖËøá107‰∫øÂèÇÊï∞ÁöÑÂü∫Á°ÄÊ®°Âûã„ÄÇ‰∏é‰º†ÁªüÁöÑAllReduceÊñπÊ≥ïÁõ∏ÊØîÔºåDiLoCoXÂú®ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉ‰∏≠ÂÆûÁé∞‰∫Ü357ÂÄçÁöÑÂä†ÈÄüÔºåÂêåÊó∂Ê®°ÂûãÊî∂ÊïõÊÄßÂá†‰πéÊ≤°Êúâ‰∏ãÈôç„ÄÇ', title='Âéª‰∏≠ÂøÉÂåñÈõÜÁæ§ËÆ≠ÁªÉÁöÑÈÄüÂ∫¶Èù©ÂëΩ'))
[27.06.2025 13:26] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#benchmark", "#science", "#dataset", "#healthcare"], "emoji": "üî¨", "ru": {"title": "DeepRare: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ —Ä–µ–¥–∫–∏—Ö –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "DeepRare - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Ä–µ–¥–∫–∏—Ö –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π, –∏—Å–ø–æ
[27.06.2025 13:26] Using data from previous issue: {"categories": ["#benchmark", "#training", "#optimization", "#agents", "#architecture"], "emoji": "üß†", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —ç–≤–æ–ª—é—Ü–∏—è –∏ –≤—ã–±–æ—Ä —ç–≤—Ä–∏—Å—Ç–∏–∫ —Å –ø–æ–º–æ—â—å—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "HeurAgenix - —ç—Ç–æ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –≥–∏–ø–µ—Ä-—ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ
[27.06.2025 13:26] Using data from previous issue: {"categories": ["#interpretability", "#architecture", "#optimization", "#training"], "emoji": "‚è≠Ô∏è", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–ø—É—Å–∫ —Å—Ä–µ–¥–Ω–∏—Ö —Å–ª–æ–µ–≤: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —É—Å–ª–æ–≤–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫
[27.06.2025 13:26] Using data from previous issue: {"categories": ["#optimization", "#audio", "#diffusion", "#training", "#games", "#data"], "emoji": "üéµ", "ru": {"title": "–õ–µ–≥–∫–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –º—É–∑—ã–∫–∏ —Å –ø–æ–º–æ—â—å—é –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –≤–ª–æ–∂–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ MuseControlLite - –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –¥–ª—è —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä
[27.06.2025 13:26] Renaming data file.
[27.06.2025 13:26] Renaming previous data. hf_papers.json to ./d/2025-06-27.json
[27.06.2025 13:26] Saving new data file.
[27.06.2025 13:26] Generating page.
[27.06.2025 13:26] Renaming previous page.
[27.06.2025 13:26] Renaming previous data. index.html to ./d/2025-06-27.html
[27.06.2025 13:26] Writing result.
[27.06.2025 13:26] Renaming log file.
[27.06.2025 13:26] Renaming previous data. log.txt to ./logs/2025-06-27_last_log.txt
