[26.06.2025 23:11] Read previous papers.
[26.06.2025 23:11] Generating top page (month).
[26.06.2025 23:11] Writing top page (month).
[27.06.2025 00:56] Read previous papers.
[27.06.2025 00:56] Get feed.
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18095
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.19103
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.19697
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20512
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.16012
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18088
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18315
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20544
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20452
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20495
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18674
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20480
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20331
[27.06.2025 00:56] Extract page data from URL. URL: https://huggingface.co/papers/2506.20920
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.19502
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.18403
[27.06.2025 00:56] Get page data from previous paper. URL: https://huggingface.co/papers/2506.19143
[27.06.2025 00:56] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.06.2025 00:56] No deleted papers detected.
[27.06.2025 00:56] Downloading and parsing papers (pdf, html). Total: 17.
[27.06.2025 00:56] Downloading and parsing paper https://huggingface.co/papers/2506.18095.
[27.06.2025 00:56] Extra JSON file exists (./assets/json/2506.18095.json), skip PDF parsing.
[27.06.2025 00:56] Paper image links file exists (./assets/img_data/2506.18095.json), skip HTML parsing.
[27.06.2025 00:56] Success.
[27.06.2025 00:56] Downloading and parsing paper https://huggingface.co/papers/2506.19103.
[27.06.2025 00:56] Extra JSON file exists (./assets/json/2506.19103.json), skip PDF parsing.
[27.06.2025 00:56] Paper image links file exists (./assets/img_data/2506.19103.json), skip HTML parsing.
[27.06.2025 00:56] Success.
[27.06.2025 00:56] Downloading and parsing paper https://huggingface.co/papers/2506.19697.
[27.06.2025 00:56] Extra JSON file exists (./assets/json/2506.19697.json), skip PDF parsing.
[27.06.2025 00:56] Paper image links file exists (./assets/img_data/2506.19697.json), skip HTML parsing.
[27.06.2025 00:56] Success.
[27.06.2025 00:56] Downloading and parsing paper https://huggingface.co/papers/2506.20512.
[27.06.2025 00:56] Extra JSON file exists (./assets/json/2506.20512.json), skip PDF parsing.
[27.06.2025 00:56] Paper image links file exists (./assets/img_data/2506.20512.json), skip HTML parsing.
[27.06.2025 00:56] Success.
[27.06.2025 00:56] Downloading and parsing paper https://huggingface.co/papers/2506.16012.
[27.06.2025 00:56] Extra JSON file exists (./assets/json/2506.16012.json), skip PDF parsing.
[27.06.2025 00:56] Paper image links file exists (./assets/img_data/2506.16012.json), skip HTML parsing.
[27.06.2025 00:56] Success.
[27.06.2025 00:56] Downloading and parsing paper https://huggingface.co/papers/2506.18088.
[27.06.2025 00:56] Extra JSON file exists (./assets/json/2506.18088.json), skip PDF parsing.
[27.06.2025 00:56] Paper image links file exists (./assets/img_data/2506.18088.json), skip HTML parsing.
[27.06.2025 00:56] Success.
[27.06.2025 00:56] Downloading and parsing paper https://huggingface.co/papers/2506.18315.
[27.06.2025 00:56] Extra JSON file exists (./assets/json/2506.18315.json), skip PDF parsing.
[27.06.2025 00:56] Paper image links file exists (./assets/img_data/2506.18315.json), skip HTML parsing.
[27.06.2025 00:56] Success.
[27.06.2025 00:56] Downloading and parsing paper https://huggingface.co/papers/2506.20544.
[27.06.2025 00:56] Extra JSON file exists (./assets/json/2506.20544.json), skip PDF parsing.
[27.06.2025 00:56] Paper image links file exists (./assets/img_data/2506.20544.json), skip HTML parsing.
[27.06.2025 00:56] Success.
[27.06.2025 00:56] Downloading and parsing paper https://huggingface.co/papers/2506.20452.
[27.06.2025 00:56] Extra JSON file exists (./assets/json/2506.20452.json), skip PDF parsing.
[27.06.2025 00:56] Paper image links file exists (./assets/img_data/2506.20452.json), skip HTML parsing.
[27.06.2025 00:56] Success.
[27.06.2025 00:56] Downloading and parsing paper https://huggingface.co/papers/2506.20495.
[27.06.2025 00:56] Extra JSON file exists (./assets/json/2506.20495.json), skip PDF parsing.
[27.06.2025 00:56] Paper image links file exists (./assets/img_data/2506.20495.json), skip HTML parsing.
[27.06.2025 00:56] Success.
[27.06.2025 00:56] Downloading and parsing paper https://huggingface.co/papers/2506.18674.
[27.06.2025 00:56] Extra JSON file exists (./assets/json/2506.18674.json), skip PDF parsing.
[27.06.2025 00:56] Paper image links file exists (./assets/img_data/2506.18674.json), skip HTML parsing.
[27.06.2025 00:56] Success.
[27.06.2025 00:56] Downloading and parsing paper https://huggingface.co/papers/2506.20480.
[27.06.2025 00:56] Extra JSON file exists (./assets/json/2506.20480.json), skip PDF parsing.
[27.06.2025 00:56] Paper image links file exists (./assets/img_data/2506.20480.json), skip HTML parsing.
[27.06.2025 00:56] Success.
[27.06.2025 00:56] Downloading and parsing paper https://huggingface.co/papers/2506.20331.
[27.06.2025 00:56] Extra JSON file exists (./assets/json/2506.20331.json), skip PDF parsing.
[27.06.2025 00:56] Paper image links file exists (./assets/img_data/2506.20331.json), skip HTML parsing.
[27.06.2025 00:56] Success.
[27.06.2025 00:56] Downloading and parsing paper https://huggingface.co/papers/2506.20920.
[27.06.2025 00:56] Downloading paper 2506.20920 from http://arxiv.org/pdf/2506.20920v1...
[27.06.2025 00:57] Extracting affiliations from text.
[27.06.2025 00:57] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Preprint. Under review. FineWeb2: One Pipeline to Scale Them All Adapting Pre-Training Data Processing to Every Language Guilherme Penedo Hynek Kydlƒ±Àácek Negar Foroutan Leandro Von Werra Vinko SabolÀácec 5 2 0 2 6 2 ] . [ 1 0 2 9 0 2 . 6 0 5 2 : r a Pipeline code: https://github.com/huggingface/fineweb-2 FineWeb2 dataset: https://hf.co/datasets/HuggingFaceFW/fineweb- "
[27.06.2025 00:57] Response: []
[27.06.2025 00:57] Extracting affiliations from text.
[27.06.2025 00:57] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Preprint. Under review. FineWeb2: One Pipeline to Scale Them All Adapting Pre-Training Data Processing to Every Language Guilherme Penedo Hynek Kydlƒ±Àácek Negar Foroutan Leandro Von WerraVinko SabolÀácec5 2 0 2 6 2 ] . [ 1 0 2 9 0 2 . 6 0 5 2 : r aPipeline code: https://github.com/huggingface/fineweb-2 FineWeb2 dataset: https://hf.co/datasets/HuggingFaceFW/fineweb-Pre-training state-of-the-art large language models (LLMs) requires vast amounts of clean and diverse text data. While the open development of large high-quality English pre-training datasets has seen substantial recent progress, training performant multilingual LLMs remains challenge, in large part due to the inherent difficulty of tailoring filtering and deduplication pipelines to large number of languages. In this work, we introduce new pre-training dataset curation pipeline based on FineWeb (Penedo et al., 2024) that can be automatically adapted to support any language. We extensively ablate our pipeline design choices on set of nine diverse languages, guided by set of meaningful and informative evaluation tasks that were chosen through novel selection process based on measurable criteria. Ultimately, we show that our pipeline can be used to create nonEnglish corpora that produce more performant models than prior datasets. We additionally introduce straightforward and principled approach to rebalance datasets that takes into consideration both duplication count and quality, providing an additional performance uplift. Finally, we scale our pipeline to over 1000 languages using almost 100 Common Crawl snapshots to produce FineWeb2, new 20 terabyte (5 billion document) multilingual dataset which we release along with our pipeline, training, and evaluation codebases.One of the main drivers of the improving capabilities of large language models (LLMs) is increased scale, in terms of both model and pre-training dataset size. To satiate the ever-growing hunger for text data, most pretraining datasets include large amounts of text scraped from the public internet (Raffel et al., 2020; Penedo et al., 2023; 2024). Consequently, pre-training data tends to be most readily available in the high-resource languages (English, Chinese, etc.) that are most prevalent on the internet. Since LLM capabilities largely stem from the data they were trained on (Grosse et al., 2023; Roberts et al., 2020; Razeghi et al., 2022), this has resulted in language models having better performance on high-resource languages. Furthermore, commercial and open language model development frequently only targets these languages (Grattafiori et al., 2024; Jiang et al., 2024; 01.AI et al., 2025). This state of affairs leaves the majority of the worlds population Figure 1: The FineWeb2 pipeline: Evaluation results of models trained on 350 billion tokens show that each pipeline step Language Identification (LID), Deduplication (Dedup), Filtering, and Dedup-informed upsampling (Rehydration) improves performance. Correspondence to guilherme at huggingface dot co 1 Preprint. Under review. (speaking over 7,000 languages (Eberhard et al., 2024)) unable to interact with state-of-the-art LLMs in their native tongue. Why not just curate datasets in underrepresented languages and train LLMs on them? Putting aside the possible lack of data (recent LLM training runs typically require trillions of tokens (AI@Meta, 2024; DeepSeek-AI et al., 2024)), key challenge is that high-resource languages benefit from the existence of well-tuned and battle-tested data processing and curation pipelines, whereas low-resource languages face vastly different landscape: evaluating corpora quality, ensuring accurate language identification, customizing filtering recipes, and even separating words can be major challenges for many languages. While some past work has successfully curated single-language pre-training datasets and used them to produce strong language-specific models (Carmo et al., 2020; de Vries et al., 2019; Le et al., 2020; Martin et al., 2020; Delobelle et al., 2020; Luukkonen et al., 2023; PLLuM Consortium, 2025; Pipatanakul et al., 2023, etc.), hand-designing different pipeline for each language does not scale. Consequently, most past work on multilingual datasets (e.g. Xue et al., 2021; Wenzek et al., 2020; De Gibert et al., 2024) has used (mostly) fixed pipeline across all languages. This one-size-fits-all approach risks applying inappropriate filtering to different languages, obviating the goal of creating performant data in many languages. In this work, we introduce new data processing pipeline based on the approach used for the state-of-the-art English pre-training dataset FineWeb (Penedo et al., 2024). Importantly, our pipeline can be automatically adapted based on language-specific statistics to produce high-quality pre-training corpora in any language. We follow data-driven approach and validate our design choices by running extensive ablation experiments where we train monolingual models on set of nine diverse languages and evaluate on tasks chosen through novel selection process based on measurable criteria that ensure meaningful signal. In addition, we introduce straightforward and principled approach to rebalance datasets using the original duplication counts and quality signals that allows globally neardeduplicated datasets to obtain performance uplift. Ultimately, we show that models trained on language-specific corpora produced by our pipeline perform better than those trained on other public web-based multilingual datasets by training models on additional unseen languages that were not used to inform pipeline design decisions. Finally, we use our pipeline to process almost 100 Common Crawl1 snapshots spanning the summer of 2013 to April 2024 to create FineWeb2, new 20 terabyte (5 billion document) dataset covering over 1000 languages. FineWeb2 is released under the permissive ODC-By License, and we additionally release the pipeline, training, and evaluation codebases, as well as the preliminary version of the dataset obtained after the deduplication stage, to facilitate further research on multilingual pre-training dataset curation.Before detailing our dataset creation process, we first establish critical considerations that arise when dealing with massively multilingual data. Notation When considering thousands of languages, its important to have an unambiguous way of referring to languages and scripts. In our work we identify languages by their official IS"
[27.06.2025 00:57] Mistral response. {"id": "3a2946ed5edc4b5d9a58883e9adcd9c1", "object": "chat.completion", "created": 1750985822, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1663, "total_tokens": 1671, "completion_tokens": 8}}
[27.06.2025 00:57] Response: ```python
[]
```
[27.06.2025 00:57] Deleting PDF ./assets/pdf/2506.20920.pdf.
[27.06.2025 00:57] Success.
[27.06.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2506.19502.
[27.06.2025 00:57] Extra JSON file exists (./assets/json/2506.19502.json), skip PDF parsing.
[27.06.2025 00:57] Paper image links file exists (./assets/img_data/2506.19502.json), skip HTML parsing.
[27.06.2025 00:57] Success.
[27.06.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2506.18403.
[27.06.2025 00:57] Extra JSON file exists (./assets/json/2506.18403.json), skip PDF parsing.
[27.06.2025 00:57] Paper image links file exists (./assets/img_data/2506.18403.json), skip HTML parsing.
[27.06.2025 00:57] Success.
[27.06.2025 00:57] Downloading and parsing paper https://huggingface.co/papers/2506.19143.
[27.06.2025 00:57] Extra JSON file exists (./assets/json/2506.19143.json), skip PDF parsing.
[27.06.2025 00:57] Paper image links file exists (./assets/img_data/2506.19143.json), skip HTML parsing.
[27.06.2025 00:57] Success.
[27.06.2025 00:57] Enriching papers with extra data.
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 0. ShareGPT-4o-Image and Janus-4o enable open research in photorealistic, instruction-aligned image generation through a large dataset and multimodal model.  					AI-generated summary 				 Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 1. A new framework using consistency models enhances image inversion and editing efficiency, achieving top performance with fewer steps.  					AI-generated summary 				 Recent advances in image editing with diffusion models have achieved impressive results, offering fine-grained control over the genera...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 2. Outlier-Safe Pre-Training improves large language model quantization performance by preventing extreme activation outliers through innovative training techniques.  					AI-generated summary 				 Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, ...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 3. Investigating mid-training strategies reveals that high-quality mathematical corpora and well-formatted chain-of-thought reasoning examples enhance reinforcement learning performance in language models, leading to the development of OctoThinker.  					AI-generated summary 				 Different base languag...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 4. A simulator named DualTHOR for training dual-arm humanoid robots integrates real-world assets and physics to enhance the robustness and generalization of Vision Language Models.  					AI-generated summary 				 Developing embodied agents capable of performing complex interactive tasks in real-world s...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 5. RoboTwin 2.0 is a scalable simulation framework for bimanual robotic manipulation that uses expert data synthesis and structured domain randomization to generate diverse and realistic synthetic data, improving sim-to-real transfer and generalization.  					AI-generated summary 				 Simulation-based ...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 6. A novel framework using Property-Based Testing and collaborative LLM-based agents improves code generation correctness and generalization.  					AI-generated summary 				 Large Language Models (LLMs) excel at code generation, but ensuring their outputs to be functionally correct, especially in compl...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 7. The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.  					AI-generated summary 				 Recent advancements i...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 8. HiWave enhances ultra-high-resolution image synthesis using pretrained diffusion models through a two-stage pipeline involving DDIM inversion and wavelet-based detail enhancement, improving visual fidelity and reducing artifacts.  					AI-generated summary 				 Diffusion models have emerged as the l...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 9. ReCode, a rule-based reinforcement learning framework, enhances large language models' adaptation to API updates without compromising their general code generation capabilities.  					AI-generated summary 				 Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter wh...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 10. Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.  					AI-generated summary 				 The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 11. A new strategy merges layers from fine-tuned model variants to compress large language models with minimal performance loss.  					AI-generated summary 				 Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability t...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 12. A biomedical text dataset, constructed from PubMed, uses a two-stage annotation process involving large and small language models to fine-tune and extract subsets for clinical NLP, improving pretraining efficiency and performance.  					AI-generated summary 				 We introduce Biomed-Enriched, a biome...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 13. A new pre-training dataset curation pipeline based on FineWeb supports multilingual LLMs by automatically adapting to any language, improving model performance and balancing dataset quality.  					AI-generated summary 				 Pre-training state-of-the-art large language models (LLMs) requires vast amou...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 14. MATE, a multimodal accessibility multi-agent system, converts data into understandable formats based on user needs, supporting various disabilities and integrating with institutional technologies.  					AI-generated summary 				 Accessibility remains a critical concern in today's society, as many te...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 15. The Debugging Decay Index (DDI) quantifies and optimizes the effectiveness of iterative AI debugging by predicting intervention points to revive and enhance debugging capability.  					AI-generated summary 				 The effectiveness of AI debugging follows a predictable exponential decay pattern; most m...
[27.06.2025 00:57] ********************************************************************************
[27.06.2025 00:57] Abstract 16. Sentence-level attribution methods uncover critical thought anchors in large language models' reasoning processes, enhancing interpretability.  					AI-generated summary 				 Reasoning large language models have recently achieved state-of-the-art performance in many fields. However, their long-form ...
[27.06.2025 00:57] Read previous papers.
[27.06.2025 00:57] Generating reviews via LLM API.
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#cv", "#dataset", "#open_source", "#multimodal", "#synthetic"], "emoji": "üñºÔ∏è", "ru": {"title": "–î–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ShareGPT-4o-Image - –ø–µ—Ä–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 45 —Ç—ã—Å—è—á –ø—Ä–∏–º–µ
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#open_source", "#training", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –±—ã—Å—Ç—Ä–æ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏–Ω–≤–µ—Ä—Å–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π —Å–æ–≥
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization", "#open_source"], "emoji": "üöÄ", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ LLM –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Outlier-Safe Pre-Traini
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#rl", "#training", "#open_source", "#optimization", "#dataset", "#reasoning"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#transfer_learning", "#cv", "#games", "#robotics", "#agents"], "emoji": "ü§ñ", "ru": {"title": "DualTHOR: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Å—Ä–µ–¥–µ", "desc": "DualTHOR - —ç—Ç–æ —Å–∏–º—É–ª—è—Ç–æ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–≤—É—Ä—É–∫–∏—Ö –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—â–∏–π —Ä–µ–∞–ª—å–Ω—ã–µ –∞–∫—Ç–∏–≤—ã –∏ —Ñ–∏–∑–∏–∫—É –¥–ª—è —É–ª—É—á—à–µ–Ω
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#transfer_learning", "#synthetic", "#dataset", "#benchmark", "#optimization", "#data", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–í–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –±–ª–∏–∑–Ω–µ—Ü –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤-–º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–≤", "desc": "RoboTwin 2.0 - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∏–º—É–ª—è—Ü–∏–∏ –¥–ª—è –±–∏–º–∞–Ω—É–∞–ª—å–Ω–æ–π —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#plp", "#benchmark", "#optimization", "#training", "#agents"], "emoji": "üß™", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é Property-Based Testing –∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Property-Generated Solver –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#multilingual", "#inference", "#low_resource"], "emoji": "üåê", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö LLM —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≤—ã–≤–æ–¥–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –Ω–æ–≤—ã–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º –≤—ã–±–æ—Ä–∫–∏ –∏ –æ—Ç–±–æ—Ä–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∏ –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã—Ö 
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#dataset", "#diffusion", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "HiWave: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∏–Ω—Ç–µ–∑–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–≤–µ—Ä—Ö–≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è", "desc": "HiWave - —ç—Ç–æ –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–≤–µ—Ä—Ö–≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –¥–≤—É
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#rl", "#training", "#rlhf", "#optimization", "#dataset", "#reasoning"], "emoji": "üîÑ", "ru": {"title": "ReCode: –ê–¥–∞–ø—Ç–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º API –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ ReCode –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –æ
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#training", "#optimization", "#data"], "emoji": "ü§ñ", "ru": {"title": "–≠–∫–æ–Ω–æ–º–∏—è —ç–Ω–µ—Ä–≥–∏–∏ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è —á–∞—Ç-–±–æ—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —á–∞—Ç-–±–æ—Ç–æ–≤ –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –∏ —ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –Ω–∞ 5-10%. –ê–≤—Ç–æ—Ä
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#inference", "#small_models", "#optimization", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —Å–ª–æ–µ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å–∂–∞—Ç–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—É—Ç–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å–ª–æ–µ–≤
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#science", "#training", "#data", "#healthcare"], "emoji": "üß¨", "ru": {"title": "Biomed-Enriched: –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—ã", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç Biomed-Enriched, —Å–æ–∑–¥–∞–Ω–Ω—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Pub
[27.06.2025 00:57] Querying the API.
[27.06.2025 00:57] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A new pre-training dataset curation pipeline based on FineWeb supports multilingual LLMs by automatically adapting to any language, improving model performance and balancing dataset quality.  					AI-generated summary 				 Pre-training state-of-the-art large language models (LLMs) requires vast amounts of clean and diverse text data. While the open development of large high-quality English pre-training datasets has seen substantial recent progress, training performant multilingual LLMs remains a challenge, in large part due to the inherent difficulty of tailoring filtering and deduplication pipelines to a large number of languages. In this work, we introduce a new pre-training dataset curation pipeline based on FineWeb that can be automatically adapted to support any language. We extensively ablate our pipeline design choices on a set of nine diverse languages, guided by a set of meaningful and informative evaluation tasks that were chosen through a novel selection process based on measurable criteria. Ultimately, we show that our pipeline can be used to create non-English corpora that produce more performant models than prior datasets. We additionally introduce a straightforward and principled approach to rebalance datasets that takes into consideration both duplication count and quality, providing an additional performance uplift. Finally, we scale our pipeline to over 1000 languages using almost 100 Common Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document) multilingual dataset which we release along with our pipeline, training, and evaluation codebases.
[27.06.2025 00:57] Response: {
  "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ FineWeb. –≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –ª—é–±–æ–º—É —è–∑—ã–∫—É, —É–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –±–∞–ª–∞–Ω—Å–∏—Ä—É—è –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –æ–±—à–∏—Ä–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –¥–µ–≤—è—Ç–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç FineWeb2 –æ–±—ä–µ–º–æ–º 20 —Ç–µ—Ä–∞–±–∞–π—Ç, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π –±–æ–ª–µ–µ 1000 —è–∑—ã–∫–æ–≤.",
  "emoji": "üåê",
  "title": "FineWeb2: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π"
}
[27.06.2025 00:57] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new pre-training dataset curation pipeline based on FineWeb supports multilingual LLMs by automatically adapting to any language, improving model performance and balancing dataset quality.  					AI-generated summary 				 Pre-training state-of-the-art large language models (LLMs) requires vast amounts of clean and diverse text data. While the open development of large high-quality English pre-training datasets has seen substantial recent progress, training performant multilingual LLMs remains a challenge, in large part due to the inherent difficulty of tailoring filtering and deduplication pipelines to a large number of languages. In this work, we introduce a new pre-training dataset curation pipeline based on FineWeb that can be automatically adapted to support any language. We extensively ablate our pipeline design choices on a set of nine diverse languages, guided by a set of meaningful and informative evaluation tasks that were chosen through a novel selection process based on measurable criteria. Ultimately, we show that our pipeline can be used to create non-English corpora that produce more performant models than prior datasets. We additionally introduce a straightforward and principled approach to rebalance datasets that takes into consideration both duplication count and quality, providing an additional performance uplift. Finally, we scale our pipeline to over 1000 languages using almost 100 Common Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document) multilingual dataset which we release along with our pipeline, training, and evaluation codebases."

[27.06.2025 00:57] Response: ```python
['DATASET', 'DATA', 'MULTILINGUAL']
```
[27.06.2025 00:57] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new pre-training dataset curation pipeline based on FineWeb supports multilingual LLMs by automatically adapting to any language, improving model performance and balancing dataset quality.  					AI-generated summary 				 Pre-training state-of-the-art large language models (LLMs) requires vast amounts of clean and diverse text data. While the open development of large high-quality English pre-training datasets has seen substantial recent progress, training performant multilingual LLMs remains a challenge, in large part due to the inherent difficulty of tailoring filtering and deduplication pipelines to a large number of languages. In this work, we introduce a new pre-training dataset curation pipeline based on FineWeb that can be automatically adapted to support any language. We extensively ablate our pipeline design choices on a set of nine diverse languages, guided by a set of meaningful and informative evaluation tasks that were chosen through a novel selection process based on measurable criteria. Ultimately, we show that our pipeline can be used to create non-English corpora that produce more performant models than prior datasets. We additionally introduce a straightforward and principled approach to rebalance datasets that takes into consideration both duplication count and quality, providing an additional performance uplift. Finally, we scale our pipeline to over 1000 languages using almost 100 Common Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document) multilingual dataset which we release along with our pipeline, training, and evaluation codebases."

[27.06.2025 00:57] Response: ```python
['OPEN_SOURCE', 'LOW_RESOURCE']
```
[27.06.2025 00:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new method for creating pre-training datasets for multilingual large language models (LLMs) using a pipeline called FineWeb. The pipeline automatically adapts to various languages, enhancing model performance by ensuring high-quality and diverse text data. The authors demonstrate that their approach outperforms existing datasets by evaluating it across nine different languages and introducing a method to rebalance datasets based on duplication and quality. Additionally, they scale their pipeline to generate FineWeb2, a massive 20 terabyte dataset that supports over 1000 languages, which they make publicly available.","title":"Empowering Multilingual LLMs with FineWeb Dataset Curation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new method for creating pre-training datasets for multilingual large language models (LLMs) using a pipeline called FineWeb. The pipeline automatically adapts to various languages, enhancing model performance by ensuring high-quality and diverse text data. The authors demonstrate that their approach outperforms existing datasets by evaluating it across nine different languages and introducing a method to rebalance datasets based on duplication and quality. Additionally, they scale their pipeline to generate FineWeb2, a massive 20 terabyte dataset that supports over 1000 languages, which they make publicly available.', title='Empowering Multilingual LLMs with FineWeb Dataset Curation'))
[27.06.2025 00:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂü∫‰∫éFineWebÁöÑÊñ∞È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁ≠ñÂàíÊµÅÁ®ãÔºåÊó®Âú®ÊîØÊåÅÂ§öËØ≠Ë®ÄÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ„ÄÇËØ•ÊµÅÁ®ãËÉΩÂ§üËá™Âä®ÈÄÇÂ∫î‰ªª‰ΩïËØ≠Ë®ÄÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÊÄßËÉΩÂπ∂Âπ≥Ë°°Êï∞ÊçÆÈõÜË¥®Èáè„ÄÇÊàë‰ª¨ÈÄöËøáÂØπ‰πùÁßç‰∏çÂêåËØ≠Ë®ÄËøõË°åÂπøÊ≥õÁöÑÂÆûÈ™åÔºåÈ™åËØÅ‰∫ÜËØ•ÊµÅÁ®ãÁöÑËÆæËÆ°ÈÄâÊã©ÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÂêàÁêÜÁöÑÊï∞ÊçÆÈõÜÈáçÂπ≥Ë°°ÊñπÊ≥ï„ÄÇÊúÄÁªàÔºåÊàë‰ª¨Êâ©Â±ï‰∫ÜËØ•ÊµÅÁ®ãÔºåÁîüÊàê‰∫Ü‰∏Ä‰∏™ÂåÖÂê´1000Â§öÁßçËØ≠Ë®ÄÁöÑ20TBÂ§öËØ≠Ë®ÄÊï∞ÊçÆÈõÜFineWeb2ÔºåÂπ∂ÂÖ¨ÂºÄ‰∫ÜÁõ∏ÂÖ≥‰ª£Á†Å„ÄÇ","title":"Â§öËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞Êï∞ÊçÆÈõÜÁ≠ñÂàíÊñπÊ°à"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂü∫‰∫éFineWebÁöÑÊñ∞È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁ≠ñÂàíÊµÅÁ®ãÔºåÊó®Âú®ÊîØÊåÅÂ§öËØ≠Ë®ÄÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ„ÄÇËØ•ÊµÅÁ®ãËÉΩÂ§üËá™Âä®ÈÄÇÂ∫î‰ªª‰ΩïËØ≠Ë®ÄÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÊÄßËÉΩÂπ∂Âπ≥Ë°°Êï∞ÊçÆÈõÜË¥®Èáè„ÄÇÊàë‰ª¨ÈÄöËøáÂØπ‰πùÁßç‰∏çÂêåËØ≠Ë®ÄËøõË°åÂπøÊ≥õÁöÑÂÆûÈ™åÔºåÈ™åËØÅ‰∫ÜËØ•ÊµÅÁ®ãÁöÑËÆæËÆ°ÈÄâÊã©ÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÂêàÁêÜÁöÑÊï∞ÊçÆÈõÜÈáçÂπ≥Ë°°ÊñπÊ≥ï„ÄÇÊúÄÁªàÔºåÊàë‰ª¨Êâ©Â±ï‰∫ÜËØ•ÊµÅÁ®ãÔºåÁîüÊàê‰∫Ü‰∏Ä‰∏™ÂåÖÂê´1000Â§öÁßçËØ≠Ë®ÄÁöÑ20TBÂ§öËØ≠Ë®ÄÊï∞ÊçÆÈõÜFineWeb2ÔºåÂπ∂ÂÖ¨ÂºÄ‰∫ÜÁõ∏ÂÖ≥‰ª£Á†Å„ÄÇ', title='Â§öËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞Êï∞ÊçÆÈõÜÁ≠ñÂàíÊñπÊ°à'))
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#healthcare", "#agents", "#multimodal", "#ethics", "#open_source"], "emoji": "‚ôø", "ru": {"title": "MATE: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –±–∞—Ä—å–µ—Ä–æ–≤ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏", "desc": "MATE - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ 
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#optimization", "#training", "#math"], "emoji": "üîç", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –æ—Ç–ª–∞–¥–∫–∏ –ò–ò: –∏–∑–º–µ—Ä–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∑–∞—Ç—É—Ö–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ò–Ω–¥–µ–∫—Å –ó–∞—Ç—É—Ö–∞–Ω–∏—è –û—Ç–ª–∞–¥–∫–∏ (DDI), –∫–æ—Ç–æ—Ä—ã–π –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –æ—Ç–ª–∞–¥–∫–∏ –ò–ò. –ê–≤—Ç–æ—Ä—ã –æ
[27.06.2025 00:57] Using data from previous issue: {"categories": ["#inference", "#reasoning", "#interpretability", "#data", "#open_source", "#training"], "emoji": "üß†", "ru": {"title": "–†–∞—Å–∫—Ä—ã–≤–∞—è —Ç–∞–π–Ω—ã –º—ã—à–ª–µ–Ω–∏—è –ò–ò: —è–∫–æ—Ä—è –º—ã—Å–ª–∏ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ç—Ä–∏ –º–µ—Ç–æ–¥–∞ –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω
[27.06.2025 00:57] Renaming data file.
[27.06.2025 00:57] Renaming previous data. hf_papers.json to ./d/2025-06-27.json
[27.06.2025 00:57] Saving new data file.
[27.06.2025 00:57] Generating page.
[27.06.2025 00:57] Renaming previous page.
[27.06.2025 00:57] Renaming previous data. index.html to ./d/2025-06-27.html
[27.06.2025 00:57] Writing result.
[27.06.2025 00:57] Renaming log file.
[27.06.2025 00:57] Renaming previous data. log.txt to ./logs/2025-06-27_last_log.txt
