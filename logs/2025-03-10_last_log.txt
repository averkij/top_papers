[10.03.2025 05:09] Read previous papers.
[10.03.2025 05:09] Generating top page (month).
[10.03.2025 05:09] Writing top page (month).
[10.03.2025 06:13] Read previous papers.
[10.03.2025 06:13] Get feed.
[10.03.2025 06:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.05236
[10.03.2025 06:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.05179
[10.03.2025 06:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.02130
[10.03.2025 06:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.05592
[10.03.2025 06:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.05652
[10.03.2025 06:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.05379
[10.03.2025 06:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.04872
[10.03.2025 06:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.05315
[10.03.2025 06:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.05132
[10.03.2025 06:13] Get page data from previous paper. URL: https://huggingface.co/papers/2503.04808
[10.03.2025 06:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[10.03.2025 06:13] No deleted papers detected.
[10.03.2025 06:13] Downloading and parsing papers (pdf, html). Total: 10.
[10.03.2025 06:13] Downloading and parsing paper https://huggingface.co/papers/2503.05236.
[10.03.2025 06:13] Extra JSON file exists (./assets/json/2503.05236.json), skip PDF parsing.
[10.03.2025 06:13] Paper image links file exists (./assets/img_data/2503.05236.json), skip HTML parsing.
[10.03.2025 06:13] Success.
[10.03.2025 06:13] Downloading and parsing paper https://huggingface.co/papers/2503.05179.
[10.03.2025 06:13] Extra JSON file exists (./assets/json/2503.05179.json), skip PDF parsing.
[10.03.2025 06:13] Paper image links file exists (./assets/img_data/2503.05179.json), skip HTML parsing.
[10.03.2025 06:13] Success.
[10.03.2025 06:13] Downloading and parsing paper https://huggingface.co/papers/2503.02130.
[10.03.2025 06:13] Extra JSON file exists (./assets/json/2503.02130.json), skip PDF parsing.
[10.03.2025 06:13] Paper image links file exists (./assets/img_data/2503.02130.json), skip HTML parsing.
[10.03.2025 06:13] Success.
[10.03.2025 06:13] Downloading and parsing paper https://huggingface.co/papers/2503.05592.
[10.03.2025 06:13] Extra JSON file exists (./assets/json/2503.05592.json), skip PDF parsing.
[10.03.2025 06:13] Paper image links file exists (./assets/img_data/2503.05592.json), skip HTML parsing.
[10.03.2025 06:13] Success.
[10.03.2025 06:13] Downloading and parsing paper https://huggingface.co/papers/2503.05652.
[10.03.2025 06:13] Extra JSON file exists (./assets/json/2503.05652.json), skip PDF parsing.
[10.03.2025 06:13] Paper image links file exists (./assets/img_data/2503.05652.json), skip HTML parsing.
[10.03.2025 06:13] Success.
[10.03.2025 06:13] Downloading and parsing paper https://huggingface.co/papers/2503.05379.
[10.03.2025 06:13] Extra JSON file exists (./assets/json/2503.05379.json), skip PDF parsing.
[10.03.2025 06:13] Paper image links file exists (./assets/img_data/2503.05379.json), skip HTML parsing.
[10.03.2025 06:13] Success.
[10.03.2025 06:13] Downloading and parsing paper https://huggingface.co/papers/2503.04872.
[10.03.2025 06:13] Extra JSON file exists (./assets/json/2503.04872.json), skip PDF parsing.
[10.03.2025 06:13] Paper image links file exists (./assets/img_data/2503.04872.json), skip HTML parsing.
[10.03.2025 06:13] Success.
[10.03.2025 06:13] Downloading and parsing paper https://huggingface.co/papers/2503.05315.
[10.03.2025 06:13] Extra JSON file exists (./assets/json/2503.05315.json), skip PDF parsing.
[10.03.2025 06:13] Paper image links file exists (./assets/img_data/2503.05315.json), skip HTML parsing.
[10.03.2025 06:13] Success.
[10.03.2025 06:13] Downloading and parsing paper https://huggingface.co/papers/2503.05132.
[10.03.2025 06:13] Extra JSON file exists (./assets/json/2503.05132.json), skip PDF parsing.
[10.03.2025 06:13] Paper image links file exists (./assets/img_data/2503.05132.json), skip HTML parsing.
[10.03.2025 06:13] Success.
[10.03.2025 06:13] Downloading and parsing paper https://huggingface.co/papers/2503.04808.
[10.03.2025 06:13] Extra JSON file exists (./assets/json/2503.04808.json), skip PDF parsing.
[10.03.2025 06:13] Paper image links file exists (./assets/img_data/2503.04808.json), skip HTML parsing.
[10.03.2025 06:13] Success.
[10.03.2025 06:13] Enriching papers with extra data.
[10.03.2025 06:13] ********************************************************************************
[10.03.2025 06:13] Abstract 0. Recent advances in human preference alignment have significantly enhanced multimodal generation and understanding. A key approach is training reward models to guide preference optimization. However, existing models are often task-specific, limiting their adaptability across diverse visual applicatio...
[10.03.2025 06:13] ********************************************************************************
[10.03.2025 06:13] Abstract 1. Recent advances in large language models have demonstrated remarkable reasoning capabilities through Chain of Thought (CoT) prompting, but often at the cost of excessive verbosity in their intermediate outputs, which increases computational overhead. We introduce Sketch-of-Thought (SoT), a novel pro...
[10.03.2025 06:13] ********************************************************************************
[10.03.2025 06:13] Abstract 2. An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name th...
[10.03.2025 06:13] ********************************************************************************
[10.03.2025 06:13] Abstract 3. Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their intern...
[10.03.2025 06:13] ********************************************************************************
[10.03.2025 06:13] Abstract 4. Real-world household tasks present significant challenges for mobile manipulation robots. An analysis of existing robotics benchmarks reveals that successful task performance hinges on three key whole-body control capabilities: bimanual coordination, stable and precise navigation, and extensive end-...
[10.03.2025 06:13] ********************************************************************************
[10.03.2025 06:13] Abstract 5. In this work, we present the first application of Reinforcement Learning with Verifiable Reward (RLVR) to an Omni-multimodal large language model in the context of emotion recognition, a task where both visual and audio modalities play crucial roles. We leverage RLVR to optimize the Omni model, sign...
[10.03.2025 06:13] ********************************************************************************
[10.03.2025 06:13] Abstract 6. The challenge of reducing the size of Large Language Models (LLMs) while maintaining their performance has gained significant attention. However, existing methods, such as model distillation and transfer learning, often fail to achieve high accuracy. To address this limitation, we introduce the Bran...
[10.03.2025 06:13] ********************************************************************************
[10.03.2025 06:13] Abstract 7. Code embeddings are essential for semantic code search; however, current approaches often struggle to capture the precise syntactic and contextual nuances inherent in code. Open-source models such as CodeBERT and UniXcoder exhibit limitations in scalability and efficiency, while high-performing prop...
[10.03.2025 06:13] ********************************************************************************
[10.03.2025 06:13] Abstract 8. Recently DeepSeek R1 demonstrated how reinforcement learning with simple rule-based incentives can enable autonomous development of complex reasoning in large language models, characterized by the "aha moment", in which the model manifest self-reflection and increased response length during training...
[10.03.2025 06:13] ********************************************************************************
[10.03.2025 06:13] Abstract 9. Recent advancements in reinforcement learning (RL) for large language models (LLMs), exemplified by DeepSeek R1, have shown that even a simple question-answering task can substantially improve an LLM's reasoning capabilities. In this work, we extend this approach by modifying the task into a multi-a...
[10.03.2025 06:13] Read previous papers.
[10.03.2025 06:13] Generating reviews via LLM API.
[10.03.2025 06:13] Using data from previous issue: {"categories": ["#multimodal", "#video", "#cv", "#alignment", "#dataset", "#rlhf", "#rag"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö AI-—Å–∏—Å—Ç–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç UnifiedReward - –ø–µ—Ä–≤—É—é —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç
[10.03.2025 06:13] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#reasoning", "#multilingual", "#optimization", "#open_source", "#math", "#training"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–æ–∫–µ–Ω–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Sketch-of-
[10.03.2025 06:13] Using data from previous issue: {"categories": ["#long_context", "#architecture", "#optimization", "#training"], "emoji": "üß†", "ru": {"title": "Forgetting Transformer: –£–ª—É—á—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Forgetting Transformer (FoX), –∫–æ—Ç–æ—Ä–∞
[10.03.2025 06:13] Using data from previous issue: {"categories": ["#hallucinations", "#optimization", "#rl", "#rag", "#reasoning"], "emoji": "üîç", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "R1-Searcher - —ç—Ç–æ –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —É–ª—É—á—à–∞—é—â–∏–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö
[10.03.2025 06:13] Using data from previous issue: {"categories": ["#robotics", "#open_source", "#dataset", "#training"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –¥–æ–º–∞—à–Ω–∏–º –∑–∞–¥–∞—á–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç BEHAVIOR Robot Suite (BRS) - –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤ –≤ –¥–æ–º–∞—à–Ω–∏—Ö —É—Å–ª–æ–≤–∏—è—Ö. BRS –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ 
[10.03.2025 06:13] Using data from previous issue: {"categories": ["#optimization", "#rl", "#multimodal", "#audio", "#cv", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "RLVR: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ —ç–º–æ—Ü–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –ø–µ—Ä–≤–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ–º (RLVR) –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[10.03.2025 06:13] Using data from previous issue: {"categories": ["#small_models", "#training", "#transfer_learning", "#optimization"], "emoji": "üå≥", "ru": {"title": "–í–µ—Ç–≤–ª–µ–Ω–∏–µ –∏ —Å–ª–∏—è–Ω–∏–µ: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–º –∏ –º–æ—â–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∂–∞—Ç–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Branch-Merge d
[10.03.2025 06:13] Using data from previous issue: {"categories": ["#data", "#open_source", "#training", "#optimization", "#plp"], "emoji": "üîç", "ru": {"title": "LoRA: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∫–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∫–æ–¥–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ Low-Rank Adaptation (LoR
[10.03.2025 06:13] Using data from previous issue: {"categories": ["#rl", "#multimodal", "#training", "#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑ DeepSeek R1 –ø–æ–∫–∞–∑–∞–ª–∏, –∫–∞–∫ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å LLM —Ä–∞–∑–≤–∏–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –º–æ
[10.03.2025 06:13] Using data from previous issue: {"categories": ["#optimization", "#rl", "#math", "#training", "#rlhf", "#reasoning"], "emoji": "üîÅ", "ru": {"title": "–ú–Ω–æ–≥–æ–ø–æ–ø—ã—Ç–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –ø—É—Ç—å –∫ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –ø–æ–¥—Ö–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤–Ω–µ–¥—Ä—è—è –º–Ω–æ–≥–æ–ø–æ
[10.03.2025 06:13] Loading Chinese text from previous data.
[10.03.2025 06:13] Renaming data file.
[10.03.2025 06:13] Renaming previous data. hf_papers.json to ./d/2025-03-10.json
[10.03.2025 06:13] Saving new data file.
[10.03.2025 06:13] Generating page.
[10.03.2025 06:13] Renaming previous page.
[10.03.2025 06:13] Renaming previous data. index.html to ./d/2025-03-10.html
[10.03.2025 06:13] [Experimental] Generating Chinese page for reading.
[10.03.2025 06:13] Chinese vocab [{'word': '‰ªãÁªç', 'pinyin': 'ji√®sh√†o', 'trans': 'introduce'}, {'word': 'Â§ßÂûã', 'pinyin': 'd√†x√≠ng', 'trans': 'large-scale'}, {'word': 'ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'y«îy√°n m√≥x√≠ng', 'trans': 'language model'}, {'word': 'ÁªìÂêà', 'pinyin': 'ji√©h√©', 'trans': 'combine'}, {'word': 'Â§ñÈÉ®', 'pinyin': 'w√†ib√π', 'trans': 'external'}, {'word': 'Â∑•ÂÖ∑', 'pinyin': 'g≈çngj√π', 'trans': 'tool'}, {'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìngqi√°ng', 'trans': 'enhance'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√πz√°', 'trans': 'complex'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®nw√π', 'trans': 'task'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability'}, {'word': 'ÊâßË°å', 'pinyin': 'zh√≠x√≠ng', 'trans': 'execute'}, {'word': '‰ª£Á†Å', 'pinyin': 'd√†im«é', 'trans': 'code'}, {'word': 'ËÆ°ÁÆó', 'pinyin': 'j√¨su√†n', 'trans': 'calculation'}, {'word': 'Ëá™ÊàëÊ£ÄÊü•', 'pinyin': 'z√¨w«í ji«énch√°', 'trans': 'self-check'}, {'word': 'Êé¢Á¥¢', 'pinyin': 't√†nsu«í', 'trans': 'explore'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': 'Ëá™ÊàëË∞ÉËØï', 'pinyin': 'z√¨w«í ti√°osh√¨', 'trans': 'self-debug'}, {'word': 'Ëß£ÂÜ≥', 'pinyin': 'jiƒõju√©', 'trans': 'solve'}, {'word': 'Â±ÄÈôêÊÄß', 'pinyin': 'j√∫xi√†nx√¨ng', 'trans': 'limitation'}, {'word': 'ÂàõÊñ∞ÁÇπ', 'pinyin': 'chu√†ngxƒ´n di«én', 'trans': 'innovation'}, {'word': 'Ëá™Â≠¶‰π†', 'pinyin': 'z√¨ xu√©x√≠', 'trans': 'self-learning'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'ÊäÄÊúØ', 'pinyin': 'j√¨sh√π', 'trans': 'technology'}, {'word': 'È´òÊïà', 'pinyin': 'gƒÅoxi√†o', 'trans': 'efficient'}, {'word': 'Âà©Áî®', 'pinyin': 'l√¨y√≤ng', 'trans': 'utilize'}, {'word': 'Âü∫ÂáÜÊµãËØï', 'pinyin': 'jƒ´zh«în c√®sh√¨', 'trans': 'benchmark test'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éoxi√†n', 'trans': 'performance'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´s√®', 'trans': 'outstanding'}]
[10.03.2025 06:13] Renaming previous Chinese page.
[10.03.2025 06:13] Renaming previous data. zh.html to ./d/2025-03-09_zh_reading_task.html
[10.03.2025 06:13] Writing Chinese reading task.
[10.03.2025 06:13] Writing result.
[10.03.2025 06:13] Renaming log file.
[10.03.2025 06:13] Renaming previous data. log.txt to ./logs/2025-03-10_last_log.txt
