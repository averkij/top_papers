[04.02.2026 23:21] Read previous papers.
[04.02.2026 23:21] Generating top page (month).
[04.02.2026 23:21] Writing top page (month).
[05.02.2026 01:18] Read previous papers.
[05.02.2026 01:18] Get feed.
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01785
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03786
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02103
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02660
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03796
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02619
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01630
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03048
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03139
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03419
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03411
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03845
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03619
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02444
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02380
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02636
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2601.21244
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03086
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01362
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03216
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03798
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03747
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03709
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02676
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00747
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03677
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03647
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01053
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00359
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02537
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03806
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02905
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01753
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2601.19103
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03454
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03295
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02419
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03837
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02914
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02751
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01212
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03320
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03238
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03183
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02494
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02405
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02220
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01405
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00682
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.00398
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03817
[05.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01519
[05.02.2026 01:18] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.02.2026 01:18] No deleted papers detected.
[05.02.2026 01:18] Downloading and parsing papers (pdf, html). Total: 52.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.01785.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.01785.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.01785.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03786.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03786.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03786.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02103.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02103.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02103.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02660.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02660.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02660.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03796.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03796.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03796.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02619.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02619.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02619.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.01630.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.01630.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.01630.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03048.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03048.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03048.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03139.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03139.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03139.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03419.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03419.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03419.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03411.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03411.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03411.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03845.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03845.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03845.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03619.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03619.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03619.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02444.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02444.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02444.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02380.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02380.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02380.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02636.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02636.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02636.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2601.21244.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2601.21244.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2601.21244.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03086.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03086.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03086.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.01362.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.01362.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.01362.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03216.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03216.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03216.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03798.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03798.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03798.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03747.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03747.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03747.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03709.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03709.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03709.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02676.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02676.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02676.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.00747.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.00747.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.00747.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03677.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03677.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03677.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03647.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03647.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03647.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.01053.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.01053.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.01053.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.00359.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.00359.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.00359.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02537.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02537.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02537.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03806.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03806.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03806.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02905.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02905.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02905.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.01753.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.01753.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.01753.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2601.19103.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2601.19103.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2601.19103.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03454.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03454.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03454.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03295.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03295.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03295.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02419.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02419.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02419.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03837.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03837.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03837.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02914.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02914.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02914.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02751.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02751.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02751.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.01212.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.01212.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.01212.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03320.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03320.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03320.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03238.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03238.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03238.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03183.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03183.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03183.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02494.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02494.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02494.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02405.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02405.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02405.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.02220.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.02220.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.02220.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.01405.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.01405.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.01405.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.00682.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.00682.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.00682.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.00398.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.00398.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.00398.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.03817.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.03817.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.03817.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.01519.
[05.02.2026 01:18] Extra JSON file exists (./assets/json/2602.01519.json), skip PDF parsing.
[05.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.01519.json), skip HTML parsing.
[05.02.2026 01:18] Success.
[05.02.2026 01:18] Enriching papers with extra data.
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 0. Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.  					AI-generated summary 				 Large Language Models (LLMs) have achieved rema...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 1. AOrchestra is a framework-agnostic agentic system that uses a tuple-based abstraction to dynamically create specialized task executors, achieving improved performance on complex benchmarks through automated agent creation and resource management.  					AI-generated summary 				 Language agents have ...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 2. Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition.  					AI-generated summary 				 This work stems from prior complementary obser...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 3. MARS is a modular AI research automation framework that uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance in autonomous machine learning research.  					AI-generated summary 				 Automating AI research differs from general software engine...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 4. 3DiMo enables view-agnostic human motion control in video generation by training a motion encoder alongside a pretrained video generator to distill driving frames into compact motion tokens that align with the generator's spatial priors.  					AI-generated summary 				 Existing methods for human mot...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 5. Large language models face challenges in long-horizon agentic workflows due to lack of authentic long-dependency training data, which is addressed by leveraging pull request sequences for structured supervision through progressive decomposition, consistency enforcement, and refinement from bug-fix h...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 6. Current world models lack unified frameworks despite task-specific advances, necessitating a comprehensive approach integrating interaction, perception, symbolic reasoning, and spatial representation.  					AI-generated summary 				 World models have emerged as a critical frontier in AI research, ai...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 7. CoBA-RL adapts rollout budget allocation for LLM training by evaluating sample training value and optimizing resource distribution through a capability-oriented value function and greedy strategy.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a ...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 8. A novel distillation framework called DP-DMD is introduced that preserves sample diversity in text-to-image generation by separating the roles of distilled steps, using v-prediction for diversity and standard DMD loss for quality refinement without additional computational overhead.  					AI-generat...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 9. A Docker-free framework replaces physical execution environments with learned surrogates for training software engineering agents, enabling efficient training and test-time scaling without costly container setup.  					AI-generated summary 				 Recent advances in large language models (LLMs) have en...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 10. SWE-Master presents a reproducible framework for developing software engineering agents through systematic optimization across multiple stages of agent development, achieving superior performance on software task resolution benchmarks.  					AI-generated summary 				 In this technical report, we pre...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 11. Parallel-Probe is a training-free controller that optimizes parallel thinking by using consensus-based early stopping and deviation-based branch pruning to reduce computational costs while maintaining accuracy.  					AI-generated summary 				 Parallel thinking has emerged as a promising paradigm for...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 12. DeepResearch report generation is improved through human-preference-aligned rubric generators trained via reinforcement learning with hybrid rewards and enhanced with multi-agent Markov-state workflows.  					AI-generated summary 				 Nowadays, training and evaluating DeepResearch-generated reports ...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 13. RANKVIDEO is a reasoning-based video retrieval system that improves upon traditional two-stage frameworks through explicit query-video pair analysis and a multi-objective training approach.  					AI-generated summary 				 Reranking is a critical component of modern retrieval systems, which typically...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 14. UnifiedReward-Flex combines reward modeling with flexible, context-adaptive reasoning to improve visual generation by dynamically constructing hierarchical assessments based on semantic intent and visual evidence.  					AI-generated summary 				 Recent advancements in multimodal reward models (RMs) ...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 15. Wide Research advances search intelligence through a dedicated benchmark and multi-agent architecture that enables parallel information retrieval under complex constraints.  					AI-generated summary 				 Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for r...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 16. LENS framework improves reinforcement learning with verifiable rewards by identifying and removing interference tokens to enhance exploration efficiency and training stability.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remai...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 17. Neural Predictor-Corrector framework unifies homotopy methods across multiple domains and outperforms classical approaches through learned policies and amortized training.  					AI-generated summary 				 The Homotopy paradigm, a general principle for solving challenging problems, appears across dive...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 18. XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality.  					AI-generated summary 				 In discrete generative modeling, two dominant paradigms d...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 19. Token Sparse Attention enables efficient long-context inference by dynamically compressing and decompressing attention tensors at the token level, achieving significant speedup with minimal accuracy loss.  					AI-generated summary 				 The quadratic complexity of attention remains the central bottl...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 20. A unified agent system called FullStack-Agent is introduced to assist non-expert users in developing complex interactive websites by addressing full-stack development challenges through enhanced planning, code editing, and self-improving capabilities.  					AI-generated summary 				 Assisting non-ex...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 21. LIVE is a long-horizon video world model that uses cycle-consistency and diffusion loss to control error accumulation during extended video generation.  					AI-generated summary 				 Autoregressive video world models predict future visual observations conditioned on actions. While effective over sh...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 22. Multi-hop question answering dataset ID-MoCQA assesses cultural understanding in large language models through Indonesian traditions with diverse reasoning chains.  					AI-generated summary 				 Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far b...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 23. AdaptMMBench presents a comprehensive benchmark for evaluating adaptive multimodal reasoning in Vision-Language Models, measuring reasoning mode selection rationality through dynamic difficulty assessment and multi-dimensional process evaluation.  					AI-generated summary 				 Adaptive multimodal r...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 24. DeMix is a framework that uses model merging to predict optimal data ratios for LLM pre-training, decoupling search from training costs to improve mixture discovery efficiency.  					AI-generated summary 				 Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-tra...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 25. Research reveals that instruction tokens act as structural anchors in multimodal large language models, with shallow layers performing non-selective information transfer and deep layers resolving modality competition guided by instruction intent.  					AI-generated summary 				 Modality following se...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 26. Search-R2 framework improves language agent reasoning through Actor-Refiner collaboration with targeted interventions and fine-grained reward supervision for better credit assignment in reinforcement learning.  					AI-generated summary 				 Search-integrated reasoning enables language agents to tra...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 27. LRAgent is a KV cache sharing framework for multi-LoRA agents that decomposes cache into shared and adapter-dependent components, reducing memory and compute overhead while maintaining accuracy.  					AI-generated summary 				 Role specialization in multi-LLM agent systems is often realized via mult...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 28. Large language models face limitations in adapting to changing real-world environments, necessitating a new approach called agentic evolution that treats deployment-time improvement as a goal-directed optimization process.  					AI-generated summary 				 As Large Language Models (LLMs) move from cur...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 29. WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts.  					AI-generated summary 				 We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world kn...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 30. Offline reinforcement learning method combines contextual bandit learning with partial trajectories to improve multi-turn code generation performance while reducing training costs.  					AI-generated summary 				 Recently, there have been significant research interests in training large language mod...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 31. Researchers developed FIRE-Bench, a comprehensive evaluation framework that challenges autonomous agents to rediscover established scientific findings through complete research cycles involving hypothesis generation, experimentation, coding, and evidence-based conclusion drawing.  					AI-generated ...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 32. ObjEmbed is a novel multimodal language-model embedding approach that decomposes images into regional embeddings for improved object-level visual understanding and retrieval tasks.  					AI-generated summary 				 Aligning objects with corresponding textual descriptions is a fundamental challenge and...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 33. A reinforcement learning framework with glance and focus models improves pan-cancer screening in CT scans by addressing foreground-background imbalance and reducing false positives through group relative learning.  					AI-generated summary 				 Pan-cancer screening in large-scale CT scans remains c...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 34. CoViP addresses contextualized visual personalization by treating personalized image captioning as a core task and improving capabilities through reinforcement-learning-based post-training and caption-augmented generation.  					AI-generated summary 				 Despite recent progress in vision-language mo...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 35. Stage-aware pruning method for large language and vision-language models that improves efficiency by selectively removing layers during different processing phases while maintaining accuracy.  					AI-generated summary 				 Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstr...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 36. SafeGround is a uncertainty-aware framework for GUI grounding models that uses distribution-aware uncertainty quantification and calibration to enable risk-aware predictions with controlled false discovery rates.  					AI-generated summary 				 Graphical User Interface (GUI) grounding aims to transl...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 37. Advanced AI models demonstrate capability in supporting expert-level mathematical discovery and scientific research through collaborative approaches involving proof verification and automated code execution.  					AI-generated summary 				 Recent advances in large language models (LLMs) have opened ...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 38. FaceLinkGen attack demonstrates that current privacy-preserving face recognition methods fail to protect identity information despite pixel-level distortion metrics suggesting adequate protection.  					AI-generated summary 				 Transformation-based privacy-preserving face recognition (PPFR) aims to...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 39. Small language models struggle with complex tasks but can be effectively coordinated through a marketplace-inspired framework that reduces costs and improves performance through strategic bidding and self-improvement mechanisms.  					AI-generated summary 				 Small language models are increasingly ...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 40. SimpleNorm normalization strategy stabilizes activation scales and enables larger stable learning rates in Transformer models by reducing Hessian spectral norm, leading to improved training performance.  					AI-generated summary 				 In this work, we revisit Transformer optimization through the len...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 41. MedSAM-Agent reformulates medical image segmentation as a multi-step decision-making process using hybrid prompting and a two-stage training pipeline with process rewards to improve autonomous reasoning and optimization.  					AI-generated summary 				 Medical image segmentation is evolving from tas...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 42. Large Language Models have advanced general-purpose agents, but current evaluation benchmarks suffer from confounding factors and lack of standardization, necessitating a unified framework for rigorous assessment.  					AI-generated summary 				 With the advent of Large Language Models (LLMs), gener...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 43. A large-scale synthetic dataset called Privasis is introduced to address privacy concerns in AI research, enabling more effective text sanitization with compact models that outperform existing large language models.  					AI-generated summary 				 Research involving privacy-sensitive data has always...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 44. MEG-XL demonstrates improved brain-to-text decoding performance through extended pre-training with 2.5-minute MEG context, significantly outperforming previous models with less contextual data.  					AI-generated summary 				 Clinical brain-to-text interfaces are designed for paralysed patients who ...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 45. Distribution Aligned Imitation Learning (DAIL) improves LLM reasoning by transforming expert solutions into in-distribution traces and using contrastive learning to focus on expert methodologies, achieving significant performance gains with minimal expert data.  					AI-generated summary 				 Improv...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 46. HieraNav presents a multi-granularity, open-vocabulary navigation task with LangMap benchmark that enables agents to follow natural language instructions across different semantic levels in 3D environments.  					AI-generated summary 				 The relationships between objects and language are fundamenta...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 47. High-quality feedback is essential for effective human-AI interaction. It bridges knowledge gaps, corrects digressions, and shapes system behavior; both during interaction and throughout model development. Yet despite its importance, human feedback to AI is often infrequent and low quality. This gap...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 48. A novel dual semantic alignment framework for LLM-enhanced multimodal recommendation that addresses representational divergence between large models and recommendation systems through graph attention networks and cross-modal contrastive learning.  					AI-generated summary 				 Multimodal recommenda...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 49. MemoryLLM decouples feed-forward networks from self-attention in transformers, enabling context-free token-wise neural retrieval memory that improves inference efficiency through pre-computed lookups.  					AI-generated summary 				 Understanding how transformer components operate in LLMs is importa...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 50. A fusion framework called FINCH combines audio and spatiotemporal predictors for bioacoustic classification by adaptively weighting evidence based on reliability estimates, outperforming fixed-weight methods and audio-only approaches.  					AI-generated summary 				 Many machine learning systems hav...
[05.02.2026 01:18] ********************************************************************************
[05.02.2026 01:18] Abstract 51. Native position-independent caching enhances LLM inference efficiency by reintroducing encoders and developing a caching system that reduces latency while maintaining accuracy.  					AI-generated summary 				 The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly ...
[05.02.2026 01:18] Read previous papers.
[05.02.2026 01:18] Generating reviews via LLM API.
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#plp", "#multimodal", "#inference"], "emoji": "üñºÔ∏è", "ru": {"title": "–ö–æ–¥ –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: —Å–∂–∞—Ç–∏–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –≤ –≤–∏–¥–µ —Å–∂–∞—Ç—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤–º–µ—Å—Ç–æ —Ç
[05.02.2026 01:18] Using data from previous issue: {"categories": [], "emoji": "üéº", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∫–æ—Ä—Ç–µ–∂–Ω—É—é –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—é", "desc": "AOrchestra ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—é –≤ –≤–∏–¥–µ –∫–æ—Ä—Ç–µ–∂–∞ (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –º–æ–¥–µ–ª—å) –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å–ø
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#reasoning", "#training", "#open_source", "#interpretability", "#benchmark"], "emoji": "üîç", "ru": {"title": "–°–∫—Ä—ã—Ç–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ LLM: –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å –≤–º–µ—Å—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏", "desc": "–†–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥ –∑–æ–Ω
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#training", "#agents", "#open_source", "#science", "#benchmark"], "emoji": "üî¨", "ru": {"title": "–ú–æ–¥—É–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç —Å —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–æ–π –ø–∞–º—è—Ç—å—é –¥–ª—è —ç–∫–æ–Ω–æ–º–Ω–æ–≥–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ ML-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "desc": "MARS ‚Äî —ç—Ç–æ –º–æ–¥—É–ª—å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#video", "#training", "#architecture", "#multimodal", "#optimization", "#3d"], "emoji": "üé¨", "ru": {"title": "–ù–µ—è–≤–Ω–æ–µ 3D –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —É–≥–ª–∞ –æ–±–∑–æ—Ä–∞", "desc": "3DiMo ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–µ–º —á–µ–ª–æ–≤–µ–∫–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#training", "#synthetic", "#alignment", "#data", "#long_context", "#reasoning", "#agents"], "emoji": "üîÑ", "ru": {"title": "–û—Ç –∫–æ–¥–∞ –∫ –∞–≥–µ–Ω—Ç–∞–º: —Å–∏–Ω—Ç–µ–∑ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∞
[05.02.2026 01:18] Using data from previous issue: {"categories": [], "emoji": "üåç", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ü–µ–ª–æ—Å—Ç–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Å–µ–≥–æ–¥–Ω—è —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á –ø—Ä–µ–¥—Å–∫–∞–∑–∞
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "CoBA-RL ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –∞–¥–∞–ø—Ç–∏–≤–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#multimodal", "#training", "#inference", "#diffusion", "#optimization"], "emoji": "üé®", "ru": {"title": "–î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ DP-DMD –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É, –∫–æ—Ç–æ—Ä
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#training", "#benchmark", "#rl", "#plp", "#agents"], "emoji": "üöÄ", "ru": {"title": "–í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ Docker –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω SWE-World ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–µ–∑ Docker, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–º–µ–Ω—è–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞ –æ–±—É—á
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#open_source", "#training", "#benchmark", "#long_context", "#rl", "#plp", "#reasoning", "#agents", "#optimization"], "emoji": "üõ†Ô∏è", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏", "desc": "SWE-Master –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Ç–∫—Ä—ã—Ç—É—é –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "üå≥", "ru": {"title": "–£–º–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∫–æ–Ω—Å–µ–Ω—Å—É—Å –∏ –¥–µ–≤–∏–∞—Ü–∏—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω Parallel-Probe ‚Äî –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç 2D-
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#rlhf", "#benchmark", "#dataset", "#agents"], "emoji": "üìã", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—Ü–µ–Ω–∫–∏ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç—á—ë—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã—Ä–∞–≤–Ω–µ–Ω—ã —Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ —á
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#training", "#benchmark", "#video", "#reasoning", "#synthetic", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–£–º–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ ‚Äî –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "RANKVIDEO ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç 
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#video", "#reasoning", "#multimodal", "#alignment", "#cv", "#rlhf"], "emoji": "üé®", "ru": {"title": "–ì–∏–±–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ UnifiedReward-Flex ‚Äî —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#training", "#agents", "#rl", "#benchmark", "#dataset"], "emoji": "üîç", "ru": {"title": "–®–∏—Ä–æ–∫–∏–π –ø–æ–∏—Å–∫: –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Wide Research ‚Äî –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏–∑
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#optimization", "#training"], "emoji": "üßπ", "ru": {"title": "–û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –æ—Ç –ø–æ–º–µ—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ LENS - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É 
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#architecture", "#rl", "#training"], "emoji": "üîÑ", "ru": {"title": "–ù–µ–π—Ä–æ—Å–µ—Ç—å –≤–º–µ—Å—Ç–æ —ç–≤—Ä–∏—Å—Ç–∏–∫: —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–æ–º–æ—Ç–æ–ø–∏—á–µ—Å–∫–∏–º –º–µ—Ç–æ–¥–∞–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Neural Predictor-Corrector (NPC) - –µ–¥–∏–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—èÊ°ÜÊû∂–¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#optimization"], "emoji": "üîÄ", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–¥–∏–≥–º: –µ–¥–∏–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω XDLM ‚Äî –µ–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –¥–≤–µ –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏–µ –ø–∞—Ä–∞–¥–∏–≥–º—ã –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#inference", "#long_context", "#architecture", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Å–ø–∞—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ inference –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Token Sparse Attention ‚Äî –º–µ—Ö–∞–Ω–∏–∑–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Å–ø–∞—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ —É—Ä
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agents", "#training", "#plp"], "emoji": "üèóÔ∏è", "ru": {"title": "–ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ FullStack-Agent - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–≥–∞–µ—Ç –Ω–µ–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ä–∞–∑—Ä–∞
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#training", "#benchmark", "#diffusion", "#long_context", "#optimization", "#video"], "emoji": "üîÑ", "ru": {"title": "–¶–∏–∫–ª–∏—á–Ω–æ—Å—Ç—å –ø—Ä–æ—Ç–∏–≤ –æ—à–∏–±–æ–∫: –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—å –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "LIVE ‚Äî —ç—Ç–æ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#benchmark", "#low_resource", "#open_source", "#multilingual"], "emoji": "üåè", "ru": {"title": "–ú–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫—É–ª—å—Ç—É—Ä—ã –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω ID-MoCQA ‚Äî –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –º–Ω–æ–≥–æ—Ö–æ–¥
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#cv"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ: –æ—Ü–µ–Ω–∫–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ä–µ–∂–∏–º–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "AdaptMMBench ‚Äî —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ Vision-Language Mod
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#training", "#optimization", "#data", "#open_source", "#dataset"], "emoji": "üß©", "ru": {"title": "–ü–æ–∏—Å–∫ –∏–¥–µ–∞–ª—å–Ω–æ–π —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "DeMix ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ LLM, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—ä–µ–¥–∏–Ω–µ
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#architecture"], "emoji": "üß≠", "ru": {"title": "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∫–∞–∫ —è–∫–æ—Ä—è: –∫–∞–∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—ã–±–∏—Ä–∞—é—Ç –Ω—É–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM) —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ 
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#agents", "#rl", "#rag"], "emoji": "üîç", "ru": {"title": "–î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Ä–µ—Ñ–∏–Ω–∏—Ä–æ–≤–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞ –≤ –∞–≥–µ–Ω—Ç–∞—Ö –ø–æ–∏—Å–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Search-R2, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –∞
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#agents", "#inference", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–£–º–Ω–æ–µ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∞ –¥–ª—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏", "desc": "LRAgent ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è KV –∫—ç—à–∞ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#agents", "#training"], "emoji": "üß¨", "ru": {"title": "–û—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫ –∂–∏–≤–æ–π —ç–≤–æ–ª—é—Ü–∏–∏: –∞–≥–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —É—á–∞—Ç—Å—è –Ω–∞ –ª–µ—Ç—É", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∏–∑–º–µ–Ω—è—é—â–∏—Ö—Å—è —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π –∞–≥–µ–Ω—Ç–∏–≤–Ω–æ–π —ç–≤–æ–ª—é—Ü–∏–µ–π. –ê–≤—Ç–æ
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#cv"], "emoji": "üåç", "ru": {"title": "–ò–∑–º–µ—Ä–µ–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–æ–º–Ω–∏–ª–∞ –æ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –º–∏—Ä–µ", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç WorldVQA ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ì–ª–∞–≤–Ω–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å –∑–∞–∫–ª
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#training", "#rl", "#plp"], "emoji": "üíª", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –±–∞–Ω–¥–∏—Ç—ã –∏ –æ—Ñ—Ñ–ª–∞–π–Ω —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Cobalt, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –±–∞–Ω–¥–∏—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#reasoning", "#science"], "emoji": "üî¨", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –Ω–∞—É—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –ø–µ—Ä–µ–æ—Ç–∫—Ä—ã—Ç–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ FIRE-Bench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã –ø–µ—Ä–µ–æ—Ç–∫—Ä—ã—Ç—å 
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#benchmark", "#cv"], "emoji": "üéØ", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "ObjEmbed ‚Äî —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–±–∏–≤–∞–µ—Ç –∏–∑–æ–±
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#rl", "#cv", "#healthcare"], "emoji": "üîç", "ru": {"title": "–î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Å–∫—Ä–∏–Ω–∏–Ω–≥ —Ä–∞–∫–∞ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –≥—Ä—É–ø–ø–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ GF-Screen –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Å–∫—Ä–∏–Ω–∏–Ω–≥–∞ —Ä–∞–∫–∞ –ø–æ –ö–¢-—Å–Ω–∏–º–∫–∞–º, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –¥–∏—Å–±–∞–ª–∞–Ω—Å
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#rl", "#multimodal", "#benchmark", "#cv"], "emoji": "üì∏", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∑—Ä–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π vision-language", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ CoViP –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#inference", "#optimization", "#architecture", "#multimodal"], "emoji": "‚ö°", "ru": {"title": "–ò–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ —Å–ª–æ–µ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —ç—Ç–∞–ø–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –æ–±—Ä–µ–∑–∫–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç 
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#benchmark", "#inference", "#agents", "#security"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å", "desc": "SafeGround ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–æ–¥–µ–ª–µ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥—ã –∫–æ–ª–∏
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#reasoning", "#science", "#transfer_learning"], "emoji": "ü§ù", "ru": {"title": "AI –∫–∞–∫ –Ω–∞—É—á–Ω—ã–π –ø–∞—Ä—Ç–Ω—ë—Ä: –æ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫ –ø–æ–¥–ª–∏–Ω–Ω–æ–º—É —Å–æ—Ç–≤–æ—Ä—á–µ—Å—Ç–≤—É –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã —É—Å–ø–µ—à–Ω–æ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, —Ç–∞–∫–∏–º–∏ –∫
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#ethics", "#benchmark", "#cv", "#security"], "emoji": "üîì", "ru": {"title": "–ü–∏–∫—Å–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å –ª–∏—Ü", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∞—Ç–∞–∫–∞ FaceLinkGen, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–∏–≤–∞—Ç–Ω–æ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü - –æ–Ω–∏ –Ω–µ –∑–∞—â
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#small_models", "#agents", "#training"], "emoji": "üè™", "ru": {"title": "–ú–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å –∞–≥–µ–Ω—Ç–æ–≤: –∫–∞–∫ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è –º–∞–ª—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ä–∞–∑–º–µ—Ä", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#training", "#open_source", "#architecture", "#optimization", "#math"], "emoji": "üìà", "ru": {"title": "–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: –ø—É—Ç—å –∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ–º—É –∏ –±—ã—Å—Ç—Ä–æ–º—É –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –≤–≤–æ–¥–∏—Ç SimpleNorm ‚Äî –ø—Ä–æ—Å—Ç—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è —Å—Ç–∞–±–∏–ª–∏
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#optimization", "#healthcare", "#rl", "#cv", "#rlhf", "#science", "#training", "#agents", "#reasoning", "#open_source"], "emoji": "üè•", "ru": {"title": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "MedSAM-Agent –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á—É —Å–µ–≥–º–µ–Ω
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#benchmark"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –æ–±—Å—É–∂–¥–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–µ–Ω—á–º
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#synthetic", "#open_source", "#security"], "emoji": "üîê", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –≥–∏–≥–∞–Ω—Ç—Å–∫–∏–µ LLM –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É –ø—Ä–∏–≤–∞—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Privasis ‚Äî –ø–µ—Ä–≤—ã–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –º–∏–ª–ª–∏–æ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#science", "#transfer_learning", "#long_context", "#open_source"], "emoji": "üß†", "ru": {"title": "–î–æ–ª–≥–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Äî –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é –º–æ–∑–≥–∞", "desc": "MEG-XL ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏–∑ —Å–∏–≥–Ω–∞–ª–æ–≤ –º–æ–∑–≥–∞, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤ 2.
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#rlhf", "#training", "#optimization", "#reasoning"], "emoji": "üéØ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç–æ–¥ Distribution Aligned Imitation Learning (DAIL), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#3d", "#multimodal", "#benchmark", "#dataset", "#agents", "#open_source"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –ø–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É —è–∑—ã–∫—É –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö 3D-—Å—Ü–µ–Ω–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∑–∞–¥–∞—á–∞ HieraNav –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ 3D-–æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç
[05.02.2026 01:18] Using data from previous issue: {"categories": [], "emoji": "üîÑ", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –±–∞—Ä—å–µ—Ä–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –≤ —á–µ–ª–æ–≤–µ–∫–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —Å AI", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∫ —Å–∏—Å—Ç–µ–º–∞–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –¥–≤–∞ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#benchmark", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–ì–∞—Ä–º–æ–Ω–∏–∑–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —á–µ—Ä–µ–∑ –¥–≤–æ–π–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ RecGOAT ‚Äî –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç
[05.02.2026 01:18] Using data from previous issue: {"categories": [], "emoji": "üíæ", "ru": {"title": "–ü–∞–º—è—Ç—å –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: –æ—Ç–¥–µ–ª–µ–Ω–∏–µ FFN –æ—Ç –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç MemoryLLM ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–µ—Ç–∏ (FFN) –∏ –º–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö, –ø–æ–∑–≤–æ–ª—è—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å FFN –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å
[05.02.2026 01:18] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#benchmark"], "emoji": "üê¶", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –∑–≤—É–∫–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç FINCH ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–∏–æ–∞–∫—É—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø
[05.02.2026 01:18] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ-–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ-–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è (PIC) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Key-Value –∫–µ—à –±–µ–∑ —É—á
[05.02.2026 01:18] Renaming data file.
[05.02.2026 01:18] Renaming previous data. hf_papers.json to ./d/2026-02-05.json
[05.02.2026 01:18] Saving new data file.
[05.02.2026 01:18] Generating page.
[05.02.2026 01:18] Renaming previous page.
[05.02.2026 01:18] Renaming previous data. index.html to ./d/2026-02-05.html
[05.02.2026 01:18] Writing result.
[05.02.2026 01:18] Renaming log file.
[05.02.2026 01:18] Renaming previous data. log.txt to ./logs/2026-02-05_last_log.txt
