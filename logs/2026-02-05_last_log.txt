[05.02.2026 06:49] Read previous papers.
[05.02.2026 06:49] Generating top page (month).
[05.02.2026 06:49] Writing top page (month).
[05.02.2026 07:45] Read previous papers.
[05.02.2026 07:45] Get feed.
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04705
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04145
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04804
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02958
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02402
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03560
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04515
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04634
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04879
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03510
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02196
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22954
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04575
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03907
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03442
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2601.22859
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18207
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04735
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03973
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04284
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04816
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.01640
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03143
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02140
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02554
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04486
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02350
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04805
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03979
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04883
[05.02.2026 07:45] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04605
[05.02.2026 07:45] Extract page data from URL. URL: https://huggingface.co/papers/2601.20499
[05.02.2026 07:45] Extract page data from URL. URL: https://huggingface.co/papers/2602.04547
[05.02.2026 07:45] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[05.02.2026 07:45] No deleted papers detected.
[05.02.2026 07:45] Downloading and parsing papers (pdf, html). Total: 33.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04705.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.04705.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.04705.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04145.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.04145.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.04145.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04804.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.04804.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.04804.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.02958.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.02958.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.02958.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.02402.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.02402.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.02402.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.03560.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.03560.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.03560.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04515.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.04515.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.04515.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04634.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.04634.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.04634.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04879.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.04879.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.04879.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.03510.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.03510.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.03510.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.02196.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.02196.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.02196.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2601.22954.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2601.22954.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2601.22954.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04575.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.04575.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.04575.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.03907.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.03907.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.03907.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.03442.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.03442.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.03442.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2601.22859.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2601.22859.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2601.22859.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2601.18207.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2601.18207.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2601.18207.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04735.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.04735.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.04735.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.03973.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.03973.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.03973.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04284.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.04284.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.04284.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04816.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.04816.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.04816.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.01640.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.01640.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.01640.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.03143.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.03143.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.03143.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.02140.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.02140.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.02140.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.02554.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.02554.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.02554.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04486.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.04486.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.04486.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.02350.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.02350.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.02350.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04805.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.04805.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.04805.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.03979.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.03979.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.03979.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04883.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.04883.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.04883.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04605.
[05.02.2026 07:45] Extra JSON file exists (./assets/json/2602.04605.json), skip PDF parsing.
[05.02.2026 07:45] Paper image links file exists (./assets/img_data/2602.04605.json), skip HTML parsing.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2601.20499.
[05.02.2026 07:45] Downloading paper 2601.20499 from https://arxiv.org/pdf/2601.20499v1...
[05.02.2026 07:45] Extracting affiliations from text.
[05.02.2026 07:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Hang Guo 1 2 Zhaoyang Jia 2 3 Jiahao Li 2 Bin Li 2 Yuanhao Cai 4 Jiangshan Wang 1 Yawei Li 5 Yan Lu 2 6 2 0 2 8 2 ] . [ 1 9 9 4 0 2 . 1 0 6 2 : r a "
[05.02.2026 07:45] Response: ```python
[]
```
[05.02.2026 07:45] Extracting affiliations from text.
[05.02.2026 07:45] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Hang Guo 1 2 Zhaoyang Jia 2 3 Jiahao Li 2 Bin Li 2 Yuanhao Cai 4 Jiangshan Wang 1 Yawei Li 5 Yan Lu 2 6 2 0 2 8 2 ] . [ 1 9 9 4 0 2 . 1 0 6 2 : r aThe autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head selfattention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0 speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/ project/DummyForcing/. 1. Introduction Video diffusion models (Brooks et al., 2024; Peebles & Xie, 2023; Wan et al., 2025) have achieved remarkable progress in recent years, with the state-of-the-art models now being able to generate realistic shots with complex motions. Early works typically rely on bidirectional attention in diffusion transformers to generate all video frames at once. Under this paradigm, users have to wait until the model processes all frames before they can watch the video, which is slow and prevents interactive generation. Recently, autoregressive video diffusion models (Chen et al., 2024; Yin et al., 2025; Huang et al., 2025) have shifted the 1Tsinghua University 2Microsoft Research Asia 3University of Science and Technology of China 4Johns Hopkins University 5ETH Zurich. Work done during Hang Guo and Zhaoyang Jias internship at Microsoft Research Asia. Preprint. January 29, 2026. 1 Figure 1. Weak context utilization in the multi-head attention of existing methods, e.g., Diffusion Forcing (Chen et al., 2024), Self Forcing (Huang et al., 2025). Naively pruning all KV caches of 25% heads results in only marginal performance drop (84.0 vs. 83.78) while speedup inference from 17.6FPS to 19.6FPS. paradigm to frame-by-frame generation, where each autoregressive step iteratively denoises to produce clean frames. Unlike previous bidirectional counterparts, autoregressive video diffusion supports caching mechanism and can aggregate historical context stored in the KV cache through self-attention. By combining the strengths of autoregressive modeling and diffusion denoising, current methods allow sequential frame modeling while maintaining high visual quality. However, existing methods still face efficiency challenges when processing long visual token sequences. For instance, the KV cache length for past frames increases significantly in computation-dense tasks such as long videos or high-resolution videos. Although most current works (Yang et al., 2025a; Liu et al., 2025) improve efficiency at the input level by employing sliding window strategy to restrict the models attention only to recent frames, the models internal utilization on contextual frames still remains black box and has been largely unexplored. To this end, we delve deep into the multi-head self-attention layers of existing models, as they are responsible for context aggregation. Surprisingly, we found most autoregressive video diffusion pipelines (Chen et al., 2024; Huang et al., 2025; Liu et al., 2025) are inefficient in context utilization: about 25% attention heads disproportionately allocate large attention scores (over 80%) to the current frame, even with access to historical frames (Sec. 3.2 gives detailed discussion). Furthermore, as shown in Fig. 1, we remove the KV cache corresponding to these heads during inference, and this simple modification causes only 0.26% performance drop. These observations suggest that the pre-trained Efficient Autoregressive Video Diffusion with Dummy Head Figure 2. The proposed Dummy Forcing can be applied to (1) efficiently generate videos, (2) overcome quadratic complexity in high-resolution video generation, and (3) enlarge context lengths without increasing computational overhead. models have learned shortcut to prioritize certain attention heads to perform context aggregation, while leaving the other heads to mainly refine the current frame. Since the latter heads do not actually work on context aggregation, we refer to them as dummy heads in this paper. Based on the above insight, we propose Dummy Forcing to compress redundant contextual information for efficient autoregressive video diffusion models. Specifically, we develop the Heterogeneous Memory Allocation (HMA), which assigns adaptive context lengths based on distinct head types to eliminate redundant historical frames. Subsequently, we introduce Dynamic Head Programming (DHP), which maximizes an information retention objective and uses dynamic programming algorithm to derive optimal head classification results. Finally, we employ Packed Attention Forward (PAF) to achieve more aggressive dummy head numbers through adjust the classification boundary between dummy and non-dummy heads. In short, we make the following key contributions: I. We identify that current autoregressive video diffusion models exhibit inefficient context utilization. Furthermore, we observe dummy heads in these models whose attentions are exclusively focused on the current frame, even though past frames are available. II. We introduce Dummy Forcing to efficiently compress the redundant context of dummy heads. The proposed method enables adaptive cache management and head classification while achieving high compression ratios. III. As shown in Fig. 2, we apply Dummy Forcing to multiple video generation applications. Without any training, the proposed method achieves up to 2.0 end-to-end acceleration compared to the baseline and can generate videos at speeds exceeding 24 FPS. 2. Related Work Video Diffusion Models. Early video diffusion models (Polyak et al., 2024; Chen et al., 2023; Brooks et al., 2024; Zhang et al., 2025a; Zhang & Agrawala, 2025) typically feed all frames into the model at once and employ bidirectional attention during the denoising process. With largescale training, the"
[05.02.2026 07:45] Mistral response. {"id": "04b3fbec79954a54a11729e77db43df0", "created": 1770277546, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1470, "total_tokens": 1513, "completion_tokens": 43, "num_cached_tokens": 1469}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Tsinghua University\",\n    \"Microsoft Research Asia\",\n    \"University of Science and Technology of China\",\n    \"Johns Hopkins University\",\n    \"ETH Zurich\"\n]\n```"}}]}
[05.02.2026 07:45] Response: ```python
[
    "Tsinghua University",
    "Microsoft Research Asia",
    "University of Science and Technology of China",
    "Johns Hopkins University",
    "ETH Zurich"
]
```
[05.02.2026 07:45] Deleting PDF ./assets/pdf/2601.20499.pdf.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Downloading and parsing paper https://huggingface.co/papers/2602.04547.
[05.02.2026 07:45] Downloading paper 2602.04547 from https://arxiv.org/pdf/2602.04547v1...
[05.02.2026 07:45] Extracting affiliations from text.
[05.02.2026 07:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 4 ] . [ 1 7 4 5 4 0 . 2 0 6 2 : r OMNIRAD: RADIOLOGICAL FOUNDATION MODEL FOR MULTI-TASK MEDICAL IMAGE ANALYSIS Luca Zedda , Andrea Loddo , Cecilia Di Ruberto Department of Mathematics and Computer Science, University of Cagliari, Cagliari, Italy {luca.zedda,andrea.loddo,cecilia.dir}@unica.it "
[05.02.2026 07:45] Response: ```python
["Department of Mathematics and Computer Science, University of Cagliari, Cagliari, Italy"]
```
[05.02.2026 07:45] Deleting PDF ./assets/pdf/2602.04547.pdf.
[05.02.2026 07:45] Success.
[05.02.2026 07:45] Enriching papers with extra data.
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 0. ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.  					AI-generated summary 				 In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desing...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 1. Training multimodal process reward models efficiently through balanced-information scoring that prioritizes label mixture and reliability while achieving full-data performance with only 10% of training data.  					AI-generated summary 				 Multimodal Process Reward Models (MPRMs) are central to step...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 2. OmniSIFT is a modality-asymmetric token compression framework for Omni-LLMs that reduces computational overhead through spatio-temporal video pruning and vision-guided audio selection while maintaining superior performance.  					AI-generated summary 				 Omni-modal Large Language Models (Omni-LLMs)...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 3. Quant VideoGen addresses KV cache memory limitations in autoregressive video diffusion models through semantic-aware smoothing and progressive residual quantization, achieving significant memory reduction with minimal latency impact.  					AI-generated summary 				 Despite rapid progress in autoregr...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 4. SoMA is a 3D Gaussian Splat simulator that enables stable, long-horizon manipulation of soft bodies by coupling deformable dynamics, environmental forces, and robot actions in a unified latent neural space.  					AI-generated summary 				 Simulating deformable objects under rich interactions remains...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 5. Hybrid Sparse Attention architecture interleaves full and sparse attention layers, using full attention output to guide sparse layer token selection and cache reuse for improved efficiency and performance.  					AI-generated summary 				 This work introduces Hybrid Sparse Attention (HySparse), a new...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 6. EgoActor is a unified vision-language model that translates high-level instructions into precise humanoid robot actions through integrated perception and execution across simulated and real-world environments.  					AI-generated summary 				 Deploying humanoid robots in real-world settings is fundam...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 7. Multi-agent systems using reinforcement learning enable parallel information seeking with scalable orchestration, achieving performance comparable to larger single agents.  					AI-generated summary 				 Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 8. DPPO addresses limitations in PPO for LLM fine-tuning by replacing ratio clipping with direct policy divergence constraints, improving training stability and efficiency.  					AI-generated summary 				 Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs),...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 9. Text conditioning in DiT-based models is enhanced through a unified normalized convex fusion framework that optimizes multi-layer LLM hidden states via depth-wise semantic routing, improving text-image alignment and compositional generation.  					AI-generated summary 				 Recent DiT-based text-to-i...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 10. Test-Time Improvement (TTI) in autonomous LLM agents involves iterative environmental interaction that enhances performance, but current evaluation methods inadequately capture task optimization efficiency and memory utilization.  					AI-generated summary 				 Recent advances in autonomous LLM agen...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 11. Residual Context Diffusion (RCD) enhances diffusion large language models by recycling discarded token information through contextual residuals, improving accuracy with minimal computational overhead.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) have emerged as a promising...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 12. Vibe AIGC introduces a new generative AI paradigm where users provide high-level aesthetic and functional preferences, which are then orchestrated through multi-agent workflows to bridge the gap between human intent and machine execution.  					AI-generated summary 				 For the past decade, the traj...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 13. HY3D-Bench presents an open-source ecosystem for 3D content creation that provides high-fidelity 3D objects and synthetic assets to advance 3D generation capabilities.  					AI-generated summary 				 While recent advances in neural representations and generative models have revolutionized 3D content...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 14. Agentic RAG framework enables models to dynamically adapt retrieval decisions across multiple granularities, outperforming traditional approaches while scaling efficiently with model improvements.  					AI-generated summary 				 Frontier language models have demonstrated strong reasoning and long-ho...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 15. MEnvAgent is a multi-language framework that automates environment construction for software engineering tasks using a planning-execution-verification architecture and environment reuse mechanism, achieving improved performance on a new benchmark and creating the largest open-source polyglot dataset...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 16. Search agents trained on scientific paper corpora demonstrate advanced reasoning capabilities for technical question-answering tasks, outperforming traditional retrieval methods through reinforcement learning with verifiable rewards.  					AI-generated summary 				 Search agents are language models ...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 17. Data2Behavior predicts unintended model behaviors before training using MDF, a lightweight method that analyzes data features to reveal potential biases without parameter updates.  					AI-generated summary 				 Large Language Models (LLMs) can acquire unintended biases from seemingly benign trainin...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 18. Pretrained diffusion and flow-matching policies fail under test-time shifts due to tight coupling with training configurations, prompting the development of Vision-Language Steering (VLS) for training-free inference-time adaptation through vision-language model-guided trajectory steering.  					AI-g...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 19. Agent-Omit is a training framework that enables LLM agents to adaptively omit redundant thoughts and observations during multi-turn interactions, achieving superior effectiveness-efficiency trade-offs compared to existing methods.  					AI-generated summary 				 Managing agent thought and observatio...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 20. Horizon-LM enables large-model training on single GPUs by redefining CPU-GPU roles and eliminating persistent GPU memory usage through explicit recomputation and pipelined execution.  					AI-generated summary 				 The rapid growth of large language models (LLMs) has outpaced the evolution of single...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 21. Agentic automatic evaluation framework automates embodied vision-language model assessment through collaborative agents that reduce evaluation costs and improve ranking accuracy.  					AI-generated summary 				 Current embodied VLM evaluation relies on static, expert-defined, manually annotated benc...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 22. SAGE is an on-policy reinforcement learning framework that enhances GRPO by injecting self-hints during training to increase outcome diversity under sparse rewards, improving alignment of large language models.  					AI-generated summary 				 Group Relative Policy Optimization (GRPO) has recently em...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 23. Unified multimodal models exhibit a persistent gap between understanding and generation capabilities, indicating only surface-level integration rather than deep cognitive convergence.  					AI-generated summary 				 Recent advances in unified multimodal models (UMM) have demonstrated remarkable prog...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 24. BatCoder is a self-supervised reinforcement learning framework that jointly optimizes code and documentation generation through back-translation, achieving superior performance on code-related benchmarks.  					AI-generated summary 				 Training LLMs for code-related tasks typically depends on high-...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 25. MLLMs suffer from modality bias in GMNER tasks, which is addressed through a proposed method that enforces cross-modal reasoning via multi-style reasoning schema injection and constraint-guided verifiable optimization.  					AI-generated summary 				 Grounded Multimodal Named Entity Recognition (GMN...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 26. Multi-Agent Discussion methods suffer from inconsistency due to individual context misalignment, which is addressed through a context learning approach that dynamically generates context instructions for each agent to improve consensus reaching and performance.  					AI-generated summary 				 Multi-...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 27. Generative 3D models face challenges in animation rigging, which this work addresses by introducing SkinTokens‚Äîa learned discrete representation for skinning weights‚Äîand TokenRig, a unified autoregressive framework that models skeletons and skin deformations together, improving rigging accuracy thro...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 28. Log-probability rewards derived from the reference answer's likelihood outperform binary rewards in chain-of-thought fine-tuning across both verifiable and non-verifiable reasoning benchmarks.  					AI-generated summary 				 Fine-tuning large language models (LLMs) on reasoning benchmarks via reinfo...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 29. PAR is a multi-scale autoregressive framework for protein backbone generation that uses hierarchical structure modeling, autoregressive transformers, and flow-based decoding to produce high-quality protein structures with improved generalization and reduced exposure bias.  					AI-generated summary ...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 30. RexBERT, a family of BERT-style encoders designed for e-commerce semantics, achieves superior performance on domain-specific tasks through specialized pretraining and high-quality in-domain data.  					AI-generated summary 				 Encoder-only transformers remain indispensable in retrieval, classificat...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 31. Autoregressive video diffusion models suffer from inefficient attention mechanisms that underutilize historical frames, but a new method called Dummy Forcing improves efficiency through heterogeneous memory allocation and dynamic head programming while maintaining quality.  					AI-generated summary...
[05.02.2026 07:45] ********************************************************************************
[05.02.2026 07:45] Abstract 32. OmniRad is a self-supervised radiological foundation model pretrained on 1.2 million medical images that demonstrates improved performance in classification and segmentation tasks through representation reuse and cross-task transferability.  					AI-generated summary 				 Radiological analysis incre...
[05.02.2026 07:45] Read previous papers.
[05.02.2026 07:45] Generating reviews via LLM API.
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#multimodal", "#training", "#inference", "#architecture"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Ç—Ä–∏–ª–ª–∏–æ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ —Å –≥–∏–±–∫–∏–º –æ–±—É—á–µ–Ω–∏–µ–º –∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏", "desc": "ERNIE 5.0 ‚Äî —ç—Ç–æ —Ç—Ä–∏–ª–ª–∏–æ–Ω–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤–∞—è –∞–≤—Ç–µ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–æ–Ω–∏
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#data", "#benchmark", "#training", "#multimodal"], "emoji": "‚öñÔ∏è", "ru": {"title": "–£–º–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ (MPRM), –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#video", "#inference"], "emoji": "‚ö°", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "OmniSIFT ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Omni-LLM), –∫–æ—Ç–æ—Ä—ã–π —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#optimization", "#inference", "#long_context", "#video"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Quant VideoGen –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ KV –∫–µ—à–∞ –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –≤–∏–¥–µ
[05.02.2026 07:45] Using data from previous issue: {"categories": [], "emoji": "ü§ñ", "ru": {"title": "–ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π —Å–∏–º—É–ª—è—Ç–æ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º—è–≥–∫–∏–º–∏ —Ç–µ–ª–∞–º–∏ –±–µ–∑ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "SoMA ‚Äî —ç—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π —Å–∏–º—É–ª—è—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–≥–æ Gaussian Splat, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ—Ñ–æ—Ä–º–∏—Ä—É–µ–º—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–π –æ–∫—Ä—É–∂–∞—é—â–µ–π 
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#inference", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–ü–æ–ª–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∫–∞–∫ —É—á–∏—Ç–µ–ª—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Hybrid Sparse Attention (HySparse), –∫–æ—Ç–æ—Ä–∞—è —á–µ—Ä–µ–¥—É–µ—Ç —Å–ª–æ–∏ –ø–æ–ª–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–ª–æ—è–º–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ 
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#training", "#robotics", "#cv", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∫ –¥–µ–π—Å—Ç–≤–∏—è–º: —è–∑—ã–∫ —Ä–æ–±–æ—Ç–∞ –ø—Ä—è–º–æ –∏–∑ –≤–∏–¥–µ–Ω–∏—è", "desc": "EgoActor ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≤–∏–¥–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ —Ç–æ—á–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –≥—É–º–∞–Ω–æ–∏–¥–Ω–æ–≥–æ —Ä–æ–±–æ
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#small_models", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ —à–∏—Ä–∏–Ω–µ: –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è –≤–º–µ—Å—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞ WideSeek-R1, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ä–µ—à–µ–Ω–∏—è
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#training"], "emoji": "üéØ", "ru": {"title": "–û—Ç –æ–±—Ä–µ–∑–∞–Ω–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –∫ –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω—ã–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º: —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ LLM —á–µ—Ä–µ–∑ DPPO", "desc": "DPPO ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ PPO –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#diffusion"], "emoji": "üé®", "ru": {"title": "–ì–ª—É–±–∏–Ω–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –¥–∏—Ñ—Ñ—É–∑–∏–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —É—Å–ª–æ–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≤ DiT-–º–æ–¥–µ–ª—è—Ö –ø—É—Ç—ë–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ—ë–≤ —è–∑—ã–∫
[05.02.2026 07:45] Using data from previous issue: {"categories": [], "emoji": "üîÑ", "ru": {"title": "–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è: –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è LLM –∞–≥–µ–Ω—Ç–æ–≤ —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç Test-Time Improvement (TTI) ‚Äî –ø—Ä–æ—Ü–µ—Å—Å, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ LLM –∞–≥–µ–Ω—Ç—ã —É–ª—É—á—à–∞—é—Ç —Å–≤–æ—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#reasoning"], "emoji": "‚ôªÔ∏è", "ru": {"title": "–í–æ–∑—Ä–æ–∂–¥–µ–Ω–∏–µ –æ—Ç–±—Ä–æ—à–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Residual Context Diffusion (RCD), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø—É—Ç—ë–º –ø–µ—Ä–µ–∏—Å–ø–æ–ª
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#agents", "#multimodal"], "emoji": "üé≠", "ru": {"title": "–û—Ç —Å–ª—É—á–∞–π–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏: –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –∑–∞–º—ã—Å–ª–∞ –∏ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è", "desc": "Vibe AIGC –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ AI, –≥–¥–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —ç—Å
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#data", "#synthetic", "#open_source", "#3d"], "emoji": "üé®", "ru": {"title": "–ï–¥–∏–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–≤–æ–ª—é—Ü–∏–∏ –≤ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "HY3D-Bench ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π —ç–∫–æ—Å–∏—Å—Ç–µ–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∏–π –±–æ–ª—å—à—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É –≤—ã—Å
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#rag", "#agents", "#reasoning"], "emoji": "üîç", "ru": {"title": "–£–º–Ω—ã–π –∞–≥–µ–Ω—Ç —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ–∏—Å–∫–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ A-RAG ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —è–∑—ã–∫–æ–≤—ã–º
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#open_source", "#benchmark", "#plp", "#agents", "#low_resource"], "emoji": "üê≥", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∏—Å–ø–æ–ª–Ω—è–µ–º—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏–π –¥–ª—è –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è", "desc": "MEnvAgent ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#rag", "#science", "#dataset", "#agents", "#reasoning", "#rl"], "emoji": "üî¨", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —É—á–∞—Ç—Å—è –∏—Å–∫–∞—Ç—å –∏—Å—Ç–∏–Ω—É –≤ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –º–µ—Ç–æ–¥–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å
[05.02.2026 07:45] Using data from previous issue: {"categories": [], "emoji": "üîç", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–∏—Å–∫–∞ –¥–æ –æ–±—É—á–µ–Ω–∏—è: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤–º–µ—Å—Ç–æ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–µ–π –æ—Ü–µ–Ω–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Data2Behavior ‚Äî –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–æ —ç—Ç–∞–ø–∞ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#robotics", "#optimization", "#diffusion", "#training", "#multimodal", "#inference"], "emoji": "ü§ñ", "ru": {"title": "–ê–¥–∞–ø—Ç–∞—Ü–∏—è –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –ø–æ–ª–∏—Ç–∏–∫ —Ä–æ–±–æ—Ç–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ —á–µ—Ä–µ–∑ –≤–∏–¥–µ–æ—è–∑—ã–∫–æ–≤–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞, –∫–æ–≥–¥–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –ø–æ–ª
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agents", "#training", "#rl"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ –ø—Ä–æ–ø—É—Å–∫–∞–Ω–∏–µ: –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –æ–ø—É—Å–∫–∞—Ç—å –ª–∏—à–Ω–∏–µ –º—ã—Å–ª–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Agent-Omit, —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º –∞–¥–∞–ø—Ç–∏–≤–Ω
[05.02.2026 07:45] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–ü–µ—Ä–µ–≤–æ—Ä–æ—Ç —Ä–æ–ª–µ–π: –æ–±—É—á–µ–Ω–∏–µ –≥–∏–≥–∞–Ω—Ç—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–Ω–æ–º GPU —á–µ—Ä–µ–∑ —Ö–æ—Å—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É", "desc": "Horizon-LM ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–Ω–æ–º GPU, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–æ–ª–∏ CPU –∏ GPU, —Å–¥–µ–ª–∞–≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–º—è—Ç—å—é —Ö–æ—Å—Ç-–º
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#cv", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–∏–¥–µ–Ω–∏–µ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—é—â–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Agentic Automatic Evaluation (A2Eval) ‚Äî –ø–µ—Ä–≤–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#alignment", "#open_source", "#training", "#rl"], "emoji": "üéØ", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ GRPO —Å–∞–º–æ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "SAGE ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º GRPO 
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark"], "emoji": "üîÄ", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#low_resource", "#plp", "#training", "#rl"], "emoji": "üîÑ", "ru": {"title": "–î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –ø–µ—Ä–µ–≤–æ–¥ –∫–æ–¥–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "BatCoder –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º reinforcement learning, –∫–æ—Ç
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#training", "#multimodal", "#rlhf"], "emoji": "üîç", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–Ω–æ–π –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –úLLM —á–µ—Ä–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á–∏ —Ä–∞—Å–ø–æ–∑–Ω
[05.02.2026 07:45] Using data from previous issue: {"categories": [], "emoji": "ü§ù", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞ –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–º –æ–±—Å—É–∂–¥–µ–Ω–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –º–µ—Ç–æ–¥–∞—Ö –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –æ–±—Å—É–∂–¥–µ–Ω–∏—è, –≥–¥–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–æ–≤–º–µ—Å—Ç–Ω–æ 
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#rl", "#3d", "#architecture", "#optimization"], "emoji": "üé≠", "ru": {"title": "–î–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∏–≥–≥–∏–Ω–≥–∞ 3D-–ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π", "desc": "–†–∞–±–æ—Ç–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∏–≥–≥–∏–Ω–≥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π, –ø—Ä–µ–¥–ª–æ–∂–∏–≤ SkinTokens ‚Äî –æ–±—É—á–µ–Ω–Ω–æ–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#training", "#benchmark", "#rl"], "emoji": "üéØ", "ru": {"title": "–õ–æ–≥–∞—Ä–∏—Ñ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[05.02.2026 07:45] Using data from previous issue: {"categories": ["#benchmark", "#training", "#architecture"], "emoji": "üß¨", "ru": {"title": "–û—Ç –≥—Ä—É–±—ã—Ö —Ñ–æ—Ä–º –∫ —Ç–æ—á–Ω—ã–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º: –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Å–∫—É–ª—å–ø—Ç—É—Ä–∞ –±–µ–ª–∫–æ–≤", "desc": "PAR ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–∞—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–ª–∫–æ–≤—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –æ—Ç –≥—Ä—É–±—ã—Ö
[05.02.2026 07:45] Using data from previous issue: {"categories": [], "emoji": "üõçÔ∏è", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã –ª—É—á—à–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö ‚Äî –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∂–Ω–µ–µ –º–∞—Å—à—Ç–∞–±–∞", "desc": "RexBERT ‚Äî —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ BERT-–ø–æ–¥–æ–±–Ω—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ Ecom-niverse, –∫–æ—Ä–ø—É—Å
[05.02.2026 07:45] Querying the API.
[05.02.2026 07:45] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Autoregressive video diffusion models suffer from inefficient attention mechanisms that underutilize historical frames, but a new method called Dummy Forcing improves efficiency through heterogeneous memory allocation and dynamic head programming while maintaining quality.  					AI-generated summary 				 The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.
[05.02.2026 07:45] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –≤—ã—è–≤–ª—è—é—Ç—Å—è –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –º–µ—Ö–∞–Ω–∏–∑–º–µ multi-head self-attention, –≥–¥–µ –æ–∫–æ–ª–æ 25% –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –ø–æ—á—Ç–∏ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ —Ç–µ–∫—É—â–µ–º –∫–∞–¥—Ä–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Dummy Forcing, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ–ª–æ–≤ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –¢–µ—Ö–Ω–∏–∫–∞ –ø–∞–∫–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –∫—ç—à–∞ KV –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 2 —Ä–∞–∑–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ (–ø–æ—Ç–µ—Ä—è –º–µ–Ω–µ–µ 0.5%) –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ 24.3 FPS.",
  "emoji": "‚ö°",
  "title": "Dummy Forcing: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≤–Ω–∏–º–∞–Ω–∏—è"
}
```
[05.02.2026 07:45] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Autoregressive video diffusion models suffer from inefficient attention mechanisms that underutilize historical frames, but a new method called Dummy Forcing improves efficiency through heterogeneous memory allocation and dynamic head programming while maintaining quality.  					AI-generated summary 				 The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/."

[05.02.2026 07:45] Response: ```python
["VIDEO", "INFERENCE", "ARCHITECTURE"]
```
[05.02.2026 07:45] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Autoregressive video diffusion models suffer from inefficient attention mechanisms that underutilize historical frames, but a new method called Dummy Forcing improves efficiency through heterogeneous memory allocation and dynamic head programming while maintaining quality.  					AI-generated summary 				 The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/."

[05.02.2026 07:46] Response: ```python
['DIFFUSION', 'OPTIMIZATION']
```
[05.02.2026 07:46] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the inefficiencies in autoregressive video diffusion models caused by their attention mechanisms, which often do not fully utilize historical frames. The authors introduce a method called Dummy Forcing, which enhances efficiency by implementing heterogeneous memory allocation and dynamic head programming. This approach reduces redundancy in head-wise context and allows for better management of attention across different frames. As a result, Dummy Forcing achieves significant speed improvements in video generation while maintaining high quality, demonstrating a 2.0x speedup with minimal quality loss.","title":"Enhancing Video Diffusion Efficiency with Dummy Forcing"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the inefficiencies in autoregressive video diffusion models caused by their attention mechanisms, which often do not fully utilize historical frames. The authors introduce a method called Dummy Forcing, which enhances efficiency by implementing heterogeneous memory allocation and dynamic head programming. This approach reduces redundancy in head-wise context and allows for better management of attention across different frames. As a result, Dummy Forcing achieves significant speed improvements in video generation while maintaining high quality, demonstrating a 2.0x speedup with minimal quality loss.', title='Enhancing Video Diffusion Efficiency with Dummy Forcing'))
[05.02.2026 07:46] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Ëá™ÂõûÂΩíËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÂú®Â§ÑÁêÜÂéÜÂè≤Â∏ßÊó∂ÊïàÁéá‰∏çÈ´òÔºåÂØºËá¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂà©Áî®‰∏çË∂≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÁß∞‰∏∫Dummy ForcingÔºåÈÄöËøáÂºÇÊûÑÂÜÖÂ≠òÂàÜÈÖçÂíåÂä®ÊÄÅÂ§¥ÁºñÁ®ãÊù•ÊèêÈ´òÊïàÁéáÔºåÂêåÊó∂‰øùÊåÅÁîüÊàêË¥®Èáè„ÄÇËØ•ÊñπÊ≥ïÂáèÂ∞ë‰∫ÜÂ§¥ÈÉ®‰∏ä‰∏ãÊñáÁöÑÂÜó‰ΩôÔºåÂπ∂ÈÄöËøá‰∏ä‰∏ãÊñáÊâìÂåÖÊäÄÊúØÂÆûÁé∞‰∫ÜÊõ¥Âº∫ÁöÑÁºìÂ≠òÂéãÁº©„ÄÇDummy ForcingÂú®‰∏çÂ¢ûÂä†È¢ùÂ§ñËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÂ∞ÜÈÄüÂ∫¶ÊèêÂçáËá≥Âü∫Á∫øÁöÑ2.0ÂÄçÔºåÊîØÊåÅ‰ª•24.3Â∏ßÊØèÁßíÁöÑÈÄüÂ∫¶ÁîüÊàêËßÜÈ¢ëÔºå‰∏îË¥®Èáè‰∏ãÈôç‰∏çÂà∞0.5%„ÄÇ","title":"ÊèêÂçáËßÜÈ¢ëÁîüÊàêÊïàÁéáÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Ëá™ÂõûÂΩíËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÂú®Â§ÑÁêÜÂéÜÂè≤Â∏ßÊó∂ÊïàÁéá‰∏çÈ´òÔºåÂØºËá¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂà©Áî®‰∏çË∂≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÁß∞‰∏∫Dummy ForcingÔºåÈÄöËøáÂºÇÊûÑÂÜÖÂ≠òÂàÜÈÖçÂíåÂä®ÊÄÅÂ§¥ÁºñÁ®ãÊù•ÊèêÈ´òÊïàÁéáÔºåÂêåÊó∂‰øùÊåÅÁîüÊàêË¥®Èáè„ÄÇËØ•ÊñπÊ≥ïÂáèÂ∞ë‰∫ÜÂ§¥ÈÉ®‰∏ä‰∏ãÊñáÁöÑÂÜó‰ΩôÔºåÂπ∂ÈÄöËøá‰∏ä‰∏ãÊñáÊâìÂåÖÊäÄÊúØÂÆûÁé∞‰∫ÜÊõ¥Âº∫ÁöÑÁºìÂ≠òÂéãÁº©„ÄÇDummy ForcingÂú®‰∏çÂ¢ûÂä†È¢ùÂ§ñËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÂ∞ÜÈÄüÂ∫¶ÊèêÂçáËá≥Âü∫Á∫øÁöÑ2.0ÂÄçÔºåÊîØÊåÅ‰ª•24.3Â∏ßÊØèÁßíÁöÑÈÄüÂ∫¶ÁîüÊàêËßÜÈ¢ëÔºå‰∏îË¥®Èáè‰∏ãÈôç‰∏çÂà∞0.5%„ÄÇ', title='ÊèêÂçáËßÜÈ¢ëÁîüÊàêÊïàÁéáÁöÑÊñ∞ÊñπÊ≥ï'))
[05.02.2026 07:46] Querying the API.
[05.02.2026 07:46] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

OmniRad is a self-supervised radiological foundation model pretrained on 1.2 million medical images that demonstrates improved performance in classification and segmentation tasks through representation reuse and cross-task transferability.  					AI-generated summary 				 Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation.
[05.02.2026 07:46] Response: ```json
{
  "desc": "OmniRad ‚Äî —ç—Ç–æ —Å–∞–º–æ–æ–±—É—á–∞–µ–º–∞—è —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è foundation model, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ 1.2 –º–∏–ª–ª–∏–æ–Ω–∞—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–¥–∏–æ–ª–æ–≥–∏–∏, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏. –ü—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ OmniRad –ø–æ–∫–∞–∑–∞–ª–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ 2.05% –ø–æ –º–µ—Ç—Ä–∏–∫–µ F1 –∏ –¥–æ—Å—Ç–∏–≥–ª–∞ –ª—É—á—à–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —Å—Ä–µ–¥–Ω–µ–π –º–µ—Ç—Ä–∏–∫–∏ Dice –¥–ª—è –∑–∞–¥–∞—á —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –∞–Ω–∞–ª–∏–∑–æ–º –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –ø–æ —Ä–∞–∑–Ω—ã–º —Ç–∏–ø–∞–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.",
  "emoji": "ü©ª",
  "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º"
}
```
[05.02.2026 07:46] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"OmniRad is a self-supervised radiological foundation model pretrained on 1.2 million medical images that demonstrates improved performance in classification and segmentation tasks through representation reuse and cross-task transferability.  					AI-generated summary 				 Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation."

[05.02.2026 07:46] Response: ```python
['HEALTHCARE', 'CV', 'DATASET', 'BENCHMARK', 'TRAINING']
```
[05.02.2026 07:46] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"OmniRad is a self-supervised radiological foundation model pretrained on 1.2 million medical images that demonstrates improved performance in classification and segmentation tasks through representation reuse and cross-task transferability.  					AI-generated summary 				 Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation."

[05.02.2026 07:46] Response: ```python
["TRANSFER_LEARNING", "SCIENCE"]
```
[05.02.2026 07:46] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OmniRad is a self-supervised foundation model specifically designed for radiology, trained on a vast dataset of 1.2 million medical images. It enhances performance in both classification and segmentation tasks by leveraging representation reuse and enabling cross-task transferability. The model is evaluated through various adaptation methods, including lightweight task-specific adapters and full fine-tuning, demonstrating its versatility and effectiveness. Results show that OmniRad outperforms existing models in classification and segmentation benchmarks, indicating its potential for improving radiological analysis.","title":"OmniRad: Revolutionizing Radiology with Self-Supervised Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OmniRad is a self-supervised foundation model specifically designed for radiology, trained on a vast dataset of 1.2 million medical images. It enhances performance in both classification and segmentation tasks by leveraging representation reuse and enabling cross-task transferability. The model is evaluated through various adaptation methods, including lightweight task-specific adapters and full fine-tuning, demonstrating its versatility and effectiveness. Results show that OmniRad outperforms existing models in classification and segmentation benchmarks, indicating its potential for improving radiological analysis.', title='OmniRad: Revolutionizing Radiology with Self-Supervised Learning'))
[05.02.2026 07:46] Response: ParsedChatCompletionMessage[Article](content='{"desc":"OmniRadÊòØ‰∏ÄÁßçËá™ÁõëÁù£ÁöÑÊîæÂ∞ÑÂ≠¶Âü∫Á°ÄÊ®°ÂûãÔºåÁªèËøá120‰∏áÂº†ÂåªÂ≠¶ÂõæÂÉèÁöÑÈ¢ÑËÆ≠ÁªÉÔºåËÉΩÂ§üÂú®ÂàÜÁ±ªÂíåÂàÜÂâ≤‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãÂº∫Ë∞ÉË°®Á§∫ÈáçÁî®ÂíåË∑®‰ªªÂä°ÂèØËΩ¨ÁßªÊÄßÔºåÈÄÇÁî®‰∫éÂ§öÁßçÊàêÂÉèÊ®°ÂºèÁöÑ‰∏ãÊ∏∏‰ªªÂä°„ÄÇÊàë‰ª¨Âú®Â§ö‰∏™‰∏ãÊ∏∏ÈÄÇÂ∫îÊñπÊ°à‰∏ãËØÑ‰º∞‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑÁºñÁ†ÅÂô®ÔºåÂåÖÊã¨‰ΩøÁî®ÂÜªÁªì‰∏ªÂπ≤ÁöÑËΩªÈáèÁ∫ß‰ªªÂä°ÁâπÂÆöÈÄÇÈÖçÂô®ÂíåÂÖ®Á´ØÂà∞Á´ØÂæÆË∞É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOmniRadÂú®ÂàÜÁ±ªÂíåÂàÜÂâ≤‰ªªÂä°‰∏äÂùá‰ºò‰∫éÂÖ∂‰ªñÂü∫Á°ÄÊ®°ÂûãÔºåÊòæÁ§∫Âá∫Êõ¥Â•ΩÁöÑÁâπÂæÅËÅöÁ±ªÂíåÊ®°ÊÄÅÂàÜÁ¶ª„ÄÇ","title":"OmniRadÔºöÊîæÂ∞ÑÂ≠¶‰ªªÂä°ÁöÑËá™ÁõëÁù£Âü∫Á°ÄÊ®°Âûã"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='OmniRadÊòØ‰∏ÄÁßçËá™ÁõëÁù£ÁöÑÊîæÂ∞ÑÂ≠¶Âü∫Á°ÄÊ®°ÂûãÔºåÁªèËøá120‰∏áÂº†ÂåªÂ≠¶ÂõæÂÉèÁöÑÈ¢ÑËÆ≠ÁªÉÔºåËÉΩÂ§üÂú®ÂàÜÁ±ªÂíåÂàÜÂâ≤‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãÂº∫Ë∞ÉË°®Á§∫ÈáçÁî®ÂíåË∑®‰ªªÂä°ÂèØËΩ¨ÁßªÊÄßÔºåÈÄÇÁî®‰∫éÂ§öÁßçÊàêÂÉèÊ®°ÂºèÁöÑ‰∏ãÊ∏∏‰ªªÂä°„ÄÇÊàë‰ª¨Âú®Â§ö‰∏™‰∏ãÊ∏∏ÈÄÇÂ∫îÊñπÊ°à‰∏ãËØÑ‰º∞‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑÁºñÁ†ÅÂô®ÔºåÂåÖÊã¨‰ΩøÁî®ÂÜªÁªì‰∏ªÂπ≤ÁöÑËΩªÈáèÁ∫ß‰ªªÂä°ÁâπÂÆöÈÄÇÈÖçÂô®ÂíåÂÖ®Á´ØÂà∞Á´ØÂæÆË∞É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOmniRadÂú®ÂàÜÁ±ªÂíåÂàÜÂâ≤‰ªªÂä°‰∏äÂùá‰ºò‰∫éÂÖ∂‰ªñÂü∫Á°ÄÊ®°ÂûãÔºåÊòæÁ§∫Âá∫Êõ¥Â•ΩÁöÑÁâπÂæÅËÅöÁ±ªÂíåÊ®°ÊÄÅÂàÜÁ¶ª„ÄÇ', title='OmniRadÔºöÊîæÂ∞ÑÂ≠¶‰ªªÂä°ÁöÑËá™ÁõëÁù£Âü∫Á°ÄÊ®°Âûã'))
[05.02.2026 07:46] Renaming data file.
[05.02.2026 07:46] Renaming previous data. hf_papers.json to ./d/2026-02-05.json
[05.02.2026 07:46] Saving new data file.
[05.02.2026 07:46] Generating page.
[05.02.2026 07:46] Renaming previous page.
[05.02.2026 07:46] Renaming previous data. index.html to ./d/2026-02-05.html
[05.02.2026 07:46] Writing result.
[05.02.2026 07:46] Renaming log file.
[05.02.2026 07:46] Renaming previous data. log.txt to ./logs/2026-02-05_last_log.txt
