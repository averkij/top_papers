[19.03.2025 02:22] Read previous papers.
[19.03.2025 02:22] Generating top page (month).
[19.03.2025 02:22] Writing top page (month).
[19.03.2025 03:24] Read previous papers.
[19.03.2025 03:24] Get feed.
[19.03.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14456
[19.03.2025 03:24] Extract page data from URL. URL: https://huggingface.co/papers/2503.12797
[19.03.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14378
[19.03.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2503.13424
[19.03.2025 03:24] Extract page data from URL. URL: https://huggingface.co/papers/2503.14478
[19.03.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14125
[19.03.2025 03:24] Extract page data from URL. URL: https://huggingface.co/papers/2503.14476
[19.03.2025 03:24] Extract page data from URL. URL: https://huggingface.co/papers/2503.14499
[19.03.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2503.12545
[19.03.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2503.12505
[19.03.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10410
[19.03.2025 03:24] Extract page data from URL. URL: https://huggingface.co/papers/2503.14492
[19.03.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10546
[19.03.2025 03:24] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.03.2025 03:24] No deleted papers detected.
[19.03.2025 03:24] Downloading and parsing papers (pdf, html). Total: 13.
[19.03.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2503.14456.
[19.03.2025 03:24] Extra JSON file exists (./assets/json/2503.14456.json), skip PDF parsing.
[19.03.2025 03:24] Paper image links file exists (./assets/img_data/2503.14456.json), skip HTML parsing.
[19.03.2025 03:24] Success.
[19.03.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2503.12797.
[19.03.2025 03:24] Downloading paper 2503.12797 from http://arxiv.org/pdf/2503.12797v2...
[19.03.2025 03:24] Extracting affiliations from text.
[19.03.2025 03:24] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 2 7 9 7 2 1 . 3 0 5 2 : r DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding Xinyu Ma1, Ziyang Ding4, Zhicong Luo3, Chi Chen2, , Zonghao Guo2, Derek F. Wong1, , Xiaoyi Feng3, Maosong Sun2 1University of Macau, 2Tsinghua University, 3Northwestern Polytechnical University, 4Shandong University chenchithu@gmail.com, derekfw@um.edu.mo Figure 1. (a) DeepPerception employs knowledge-driven reasoning to derive answers, while the baseline model directly outputs predictions without cognitive processing. (b) DeepPerception demonstrates superior cognitive visual perception capabilities that cannot be elicited in the foundation model through simplistic zero-shot CoT prompting. "
[19.03.2025 03:24] Response: ```python
["University of Macau", "Tsinghua University", "Northwestern Polytechnical University", "Shandong University"]
```
[19.03.2025 03:24] Deleting PDF ./assets/pdf/2503.12797.pdf.
[19.03.2025 03:24] Success.
[19.03.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2503.14378.
[19.03.2025 03:24] Extra JSON file exists (./assets/json/2503.14378.json), skip PDF parsing.
[19.03.2025 03:24] Paper image links file exists (./assets/img_data/2503.14378.json), skip HTML parsing.
[19.03.2025 03:24] Success.
[19.03.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2503.13424.
[19.03.2025 03:24] Extra JSON file exists (./assets/json/2503.13424.json), skip PDF parsing.
[19.03.2025 03:24] Paper image links file exists (./assets/img_data/2503.13424.json), skip HTML parsing.
[19.03.2025 03:24] Success.
[19.03.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2503.14478.
[19.03.2025 03:24] Downloading paper 2503.14478 from http://arxiv.org/pdf/2503.14478v1...
[19.03.2025 03:24] Extracting affiliations from text.
[19.03.2025 03:24] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 8 7 4 4 1 . 3 0 5 2 : r Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLMs Xinyu Fang1,2*, Zhijian Chen3*, Kai Lan3, Lixin Ma3, Shengyuan Ding2,4, Yingji Liang5, Xiangyu Zhao2,6, Farong Wen6, Zicheng Zhang2,6, Guofeng Zhang1, Haodong Duan2, Kai Chen2, Dahua Lin2,7 Zhejiang University1 Shanghai AI Laboratory2 Tongji University3 Nanjing University4 East China Normal University Shanghai Jiaotong University6 The Chinese University of Hong Kong7 Figure 1. Our Motivation for Creation-MMBench. The triarchic theory of intelligence divides intelligence into three forms. Current MLLM benchmarks have significant gaps in evaluating visual-creative intelligence compared to the other forms. Additionally, existing benchmarks feature simple questions that fail to assess model performance in real-life creative tasks. Therefore, we proposed CreationMMBench, which includes four categories, more creative and discriminative questions, and better evaluation of visual creative intelligence. "
[19.03.2025 03:24] Response: ```python
[
    "Zhejiang University",
    "Shanghai AI Laboratory",
    "Tongji University",
    "Nanjing University",
    "East China Normal University",
    "Shanghai Jiaotong University",
    "The Chinese University of Hong Kong"
]
```
[19.03.2025 03:24] Deleting PDF ./assets/pdf/2503.14478.pdf.
[19.03.2025 03:24] Success.
[19.03.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2503.14125.
[19.03.2025 03:24] Extra JSON file exists (./assets/json/2503.14125.json), skip PDF parsing.
[19.03.2025 03:24] Paper image links file exists (./assets/img_data/2503.14125.json), skip HTML parsing.
[19.03.2025 03:24] Success.
[19.03.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2503.14476.
[19.03.2025 03:24] Downloading paper 2503.14476 from http://arxiv.org/pdf/2503.14476v1...
[19.03.2025 03:24] Extracting affiliations from text.
[19.03.2025 03:24] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DAPO: An Open-Source LLM Reinforcement Learning System at Scale 1ByteDance Seed 2Institute for AI Industry Research (AIR), Tsinghua University 3The University of Hong Kong 4SIA-Lab of Tsinghua AIR and ByteDance Seed Full author list in Contributions "
[19.03.2025 03:24] Response: ```python
["ByteDance", "Institute for AI Industry Research (AIR), Tsinghua University", "The University of Hong Kong", "SIA-Lab of Tsinghua AIR and ByteDance"]
```
[19.03.2025 03:24] Deleting PDF ./assets/pdf/2503.14476.pdf.
[19.03.2025 03:24] Success.
[19.03.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2503.14499.
[19.03.2025 03:24] Downloading paper 2503.14499 from http://arxiv.org/pdf/2503.14499v1...
[19.03.2025 03:24] Extracting affiliations from text.
[19.03.2025 03:24] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 9 9 4 4 1 . 3 0 5 2 : r a Thomas Kwa, Ben West, Joel Becker, Amy Deng, Katharyn Garcia, Max Hasin, Sami Jawhar, Megan Kinniment, Nate Rush, Sydney Von Arx Ryan Bloom, Thomas Broadley, Haoxing Du, Brian Goodrich, Nikola Jurkovic, Luke Harold Miles, Seraphina Nix, Tao Lin, Neev Parikh, David Rein, Lucas Jun Koba Sato, Hjalmar Wijk, Daniel M. Ziegler Elizabeth Barnes, Lawrence Chan Model Evaluation & Threat Research (METR) "
[19.03.2025 03:24] Response: ```python
[]
```
[19.03.2025 03:24] Extracting affiliations from text.
[19.03.2025 03:24] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 9 9 4 4 1 . 3 0 5 2 : r aThomas Kwa, Ben West, Joel Becker, Amy Deng, Katharyn Garcia, Max Hasin, Sami Jawhar, Megan Kinniment, Nate Rush, Sydney Von Arx Ryan Bloom, Thomas Broadley, Haoxing Du, Brian Goodrich, Nikola Jurkovic, Luke Harold Miles, Seraphina Nix, Tao Lin, Neev Parikh, David Rein, Lucas Jun Koba Sato, Hjalmar Wijk, Daniel M. Ziegler Elizabeth Barnes, Lawrence Chan Model Evaluation & Threat Research (METR)Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have 50% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our resultsincluding their degree of external validityand the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans month.In the last five years, frontier AI systems have undergone dramatic transformation in capabilities, evolving from basic text generation [1] to autonomously executing complex multi-hour machine learning research projects [2]. Sufficiently capable AIs could perform dangerous, highly complex actions like autonomous development of chemical, biological, radiological or nuclear weapons (CBRN) and self-replication and adaptation outside human control [3]. Understanding AI capabilities helps inform the development of safety guardrails as systems become increasingly powerful. In particular, many frontier AI developers have committed to using measures of specific AI capabilities to determine the necessary risk mitigations for their frontier AI systems.1 Robust benchmarks that can accurately track and forecast AI capabilities thus form the foundation for responsibd AI governance and risk mitigation. Equal contribution. Corresponding author, ben@metr.org. Ohm Chip. Work done at METR. Anthropic. Work done at METR. 1An updated list of frontier AI safety policies can be found at: https://metr.org/faisc. Figure 1: The length of tasks (measured by how long they take human professionals) that generalist autonomous frontier model agents can complete with 50% reliability has been doubling approximately every 7 months for the last 6 years (Section 4). The shaded region represents 95% CI calculated by hierarchical bootstrap over task families, tasks, and task attempts. Even if the absolute measurements are off by factor of 10, the trend predicts that in under decade we will see AI agents that can independently complete large fraction of software tasks that currently take humans days or weeks(Section 7). However, existing benchmarks face several key limitations. First, they often consist of artificial rather than economically valuable tasks. Second, benchmarks are often adversarially selected for tasks that current models struggle with compared to humans,2 biasing the comparison to human performance. Most critically, individual benchmarks saturate increasingly quickly [6], and we lack more general, intuitive, and quantitative way to compare between different benchmarks,3 which prevents meaningful comparison between models of vastly different capabilities (e.g., GPT-2 versus o1). As consequence, while the last few years have seen dramatic increases in AI performance on many individual benchmarks, understanding the progress of AI capabilities in general has required estimating the qualitative difficulty of the latest benchmarks AI systems can pass. We propose tracking AI progress over time using the task completion time horizon: the duration of tasks that models can complete at certain success probability, providing an intuitive measure of real-world capability compared to humans. As models may not reliably complete all tasks of given length, we operationalize this by measuring the X%-(task completion) time horizonthe length of tasks that models can complete approximately X% of the time. We prototype this methodology using three datasets designed to capture skills required for research or software engineering (Section 3.1), totaling 170 tasks with wide range of difficulty: HCAST [8], RE-Bench [2], and Software Atomic Actions (SWAA), new suite of shorter software tasks that can measure pre-2023 models (Section 3.1.3). Using skilled human baseliners, we estimate the duration that domain knowledgeable human (without task-specific context) takes to complete these tasks (Section 3.2). We evaluate the performance of 13 frontier models from 2019 to 2025 on these tasks (Section 3.3). Using methodology inspired by human psychometric studies, we then estimate the duration of tasks that models can complete with 50% success ratethe 50% time horizon (Section 4.1). We find that the 50% time horizon has been growing exponentially from 20192025 on our tasks, with doubling time of approximately seven months (Figure 1). We compare our main result with 2For example, HellaSwag [4] and Humanitys Last Exam [5] were both generated by adversarially filtering problems against the best performing language models available at the time. 3As with our tasks, SWE-bench Verified [7] does come with human-estimated task completion times. We use SWE-bench Verified tasks and accompanying time estimates to validate our main result in Section 6.3. Figure 2: Our methodology for measuring AI agent time horizon. First, we create diverse task suite of 170 tasks. Second, we have both humans and AI agents (consisting of an AI model and scaffold) attempt these tasks, recording the time taken by successful humans and the success rate for AI agents. Third, we fit logistic model to find the time horizon at which each AI agent has 50% chance of success, and plot"
[19.03.2025 03:24] Mistral response. {"id": "66d56155bc4449348b67e7551846f24c", "object": "chat.completion", "created": 1742354686, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Model Evaluation & Threat Research (METR)\", \"Anthropic\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1555, "total_tokens": 1578, "completion_tokens": 23}}
[19.03.2025 03:24] Response: ```python
["Model Evaluation & Threat Research (METR)", "Anthropic"]
```
[19.03.2025 03:24] Deleting PDF ./assets/pdf/2503.14499.pdf.
[19.03.2025 03:24] Success.
[19.03.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2503.12545.
[19.03.2025 03:24] Extra JSON file exists (./assets/json/2503.12545.json), skip PDF parsing.
[19.03.2025 03:24] Paper image links file exists (./assets/img_data/2503.12545.json), skip HTML parsing.
[19.03.2025 03:24] Success.
[19.03.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2503.12505.
[19.03.2025 03:24] Extra JSON file exists (./assets/json/2503.12505.json), skip PDF parsing.
[19.03.2025 03:24] Paper image links file exists (./assets/img_data/2503.12505.json), skip HTML parsing.
[19.03.2025 03:24] Success.
[19.03.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2503.10410.
[19.03.2025 03:24] Downloading paper 2503.10410 from http://arxiv.org/pdf/2503.10410v1...
[19.03.2025 03:24] Failed to download and parse paper https://huggingface.co/papers/2503.10410: max() arg is an empty sequence
[19.03.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2503.14492.
[19.03.2025 03:24] Downloading paper 2503.14492 from http://arxiv.org/pdf/2503.14492v1...
[19.03.2025 03:24] Extracting affiliations from text.
[19.03.2025 03:24] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control 2025-3-19 NVIDIA "
[19.03.2025 03:24] Response: []
[19.03.2025 03:24] Extracting affiliations from text.
[19.03.2025 03:24] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control 2025-3-19 NVIDIA5 2 0 2 8 1 ] . [ 1 2 9 4 4 1 . 3 0 5 2 : r We introduce Cosmos-Transfer1, conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at https://github.com/nvidia-cosmos/cosmos-transfer1. 1. Introduction Multimodal controllable world generation refers to the problem of generating world simulation videos based on multimodal video inputs such as segmentation, depth, and edge. These inputs help define specifics of the target world at different spatial locations at different time instances, which simplifies the generation problem. Such controllable generation capability is valuable. One can leverage the capability to mitigate the synthetic-to-real domain gap problem of CG-based simulator. We can make the renderings more realistic while preserving the scene structure and semantic through using depth and segmentation, often freely available in the CG-based simulator, as the multimodal inputs in world generation. In this paper, we propose Cosmos-Transfer1, diffusion-based conditional world model for the multimodal controllable world generation problem. Our model is built on top of Cosmos-Predict1 (NVIDIA, 2025), which consists of set of diffusion transformer-based (DiT-based) (Peebles and Xie, 2023) world models. We add multimodal control branches to the DiT through novel ControlNet design (Zhang et al., 2023). We build control branch per modality. If there are 𝑁 multimodal video inputs, then we will have 𝑁 control branches. We train the 𝑁 control branches separately and fuse them in the inference time. Our multimodal control is spatially and temporally adaptive. This is achieved by applying spatiotemporal control map to the outputs of the control branches. The spatiotemporal control map specifies the weight for each modality at each location and time instance. The higher the weight, the more influence the modality has on the generation output at the location and the time instance. This adaptive weighting scheme enables various ways to control the generation output. For example, one can choose to favor depth modality input more across all the locations in order to favor preservation of the input scene geometry. One can also give edge modality more weight to the foreground object to preserve the fine-grained details of the foreground object and less weight to the background to allow diverse background generation. The spatiotemporal control map could also be inferred by separate neural module. We conduct extensive empirical evaluations to verify the effectiveness of Cosmos-Transfer1. We measure its generation quality and controllability on several Physical AI related world generation tasks. We also discuss 1A detailed list of contributors and acknowledgments can be found in App. of this paper. 2025 NVIDIA. All rights reserved. Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control Figure 1: (a) Base model is the base DiT-based diffusion model. It consists of sequence of transformer blocks and learns to predict the added noise in the input noisy tokens. (b) ControlNet extends the base model to conditional diffusion model. The main addition is the control branch, which contains few transformer blocks. The outputs of the transformer blocks are passed to zero-initialized linear layers before added back to the main branch. During the ControlNet training, the base model weights are frozen. applications of Cosmos-Transfer1 for robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help advance the field, our code, model weights, and example scripts are open-sourced at https://github.com/nvidia-cosmos/cosmos-transfer1. 2. Preliminary The main component of diffusion model is denoiser. common approach to implement the denoiser is to use the DiT architecture as visualized in Fig. 1(a). The architecture consists of sequence of transformer blocks, trained to predict the noise added to the input video tokens. Let 𝐷 denote the denoiser. In the diffusion model, it takes noisy video tokens x𝜎 and noise deviation 𝜎 as inputs and uses them to predict the noise. That is = 𝐷(x𝜎, 𝜎). (1) ControlNet is popular approach to extend the base diffusion model to conditional diffusion model. The original ControlNet was developed for the UNet model. Chen et al. (2024) extended it to the DiT architecture. Based on the prior works, we design our DiT-based ControlNet visualized in Fig. 1(b). It has the base model and control branch. The base model is the base diffusion model. The control branch consists of few transformer blocks for conditional inputs. These conditional inputs serve as the control signal. In our ControlNet design, we use three conditional blocks. We choose three as it empirically offers good balance between control effectiveness and inference efficiency. These transformer blocks are initialized by inheriting the weights from the corresponding transformer blocks in the base diffusion model. Outputs from these blocks are passed to corresponding linear layers, respectively. These linear layers are zero-initialized. The linear layer outputs are then added to the activations of the corresponding transformer block in the base model. During training, these 2 Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control Figure 2: Cosmos-Transfer1 is world generator with adaptive multimodal control. It contains multiple control branches to extract control information from different modality inputs such as segmentation, depth, and edge. We apply spatiotempor"
[19.03.2025 03:24] Mistral response. {"id": "637a26e9fc794d828a481fa650c20b90", "object": "chat.completion", "created": 1742354697, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"NVIDIA\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1555, "total_tokens": 1566, "completion_tokens": 11}}
[19.03.2025 03:24] Response: ```python
["NVIDIA"]
```
[19.03.2025 03:24] Deleting PDF ./assets/pdf/2503.14492.pdf.
[19.03.2025 03:24] Success.
[19.03.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2503.10546.
[19.03.2025 03:24] Extra JSON file exists (./assets/json/2503.10546.json), skip PDF parsing.
[19.03.2025 03:24] Paper image links file exists (./assets/img_data/2503.10546.json), skip HTML parsing.
[19.03.2025 03:24] Success.
[19.03.2025 03:24] Enriching papers with extra data.
[19.03.2025 03:24] ********************************************************************************
[19.03.2025 03:24] Abstract 0. We present RWKV-7 "Goose", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on...
[19.03.2025 03:24] ********************************************************************************
[19.03.2025 03:24] Abstract 1. Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning ...
[19.03.2025 03:24] ********************************************************************************
[19.03.2025 03:24] Abstract 2. Synthetic videos nowadays is widely used to complement data scarcity and diversity of real-world videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) C...
[19.03.2025 03:24] ********************************************************************************
[19.03.2025 03:24] Abstract 3. Large-scale articulated objects with high quality are desperately needed for multiple tasks related to embodied AI. Most existing methods for creating articulated objects are either data-driven or simulation based, which are limited by the scale and quality of the training data or the fidelity and h...
[19.03.2025 03:24] ********************************************************************************
[19.03.2025 03:24] Abstract 4. Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs...
[19.03.2025 03:24] ********************************************************************************
[19.03.2025 03:24] Abstract 5. Residual connections are central to modern deep learning architectures, enabling the training of very deep networks by mitigating gradient vanishing. Hyper-Connections recently generalized residual connections by introducing multiple connection strengths at different depths, thereby addressing the s...
[19.03.2025 03:24] ********************************************************************************
[19.03.2025 03:24] Abstract 6. Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the ...
[19.03.2025 03:24] ********************************************************************************
[19.03.2025 03:24] Abstract 7. Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks tha...
[19.03.2025 03:24] ********************************************************************************
[19.03.2025 03:24] Abstract 8. In recent years, Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in tasks such as visual question answering, visual understanding, and reasoning. However, this impressive progress relies on vast amounts of data collected from the internet, raising significant conce...
[19.03.2025 03:24] ********************************************************************************
[19.03.2025 03:24] Abstract 9. Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning...
[19.03.2025 03:24] ********************************************************************************
[19.03.2025 03:24] Abstract 10. Roadside Collaborative Perception refers to a system where multiple roadside units collaborate to pool their perceptual data, assisting vehicles in enhancing their environmental awareness. Existing roadside perception methods concentrate on model design but overlook data issues like calibration erro...
[19.03.2025 03:24] ********************************************************************************
[19.03.2025 03:24] Abstract 11. We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting...
[19.03.2025 03:24] ********************************************************************************
[19.03.2025 03:24] Abstract 12. With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to m...
[19.03.2025 03:24] Read previous papers.
[19.03.2025 03:24] Generating reviews via LLM API.
[19.03.2025 03:24] Using data from previous issue: {"categories": ["#architecture", "#training", "#open_source", "#dataset", "#multilingual"], "emoji": "🦢", "ru": {"title": "RWKV-7: Эффективная архитектура для многоязычного моделирования последовательностей", "desc": "RWKV-7 'Goose' - это новая архитектура моделирования последовательностей, которая 
[19.03.2025 03:24] Querying the API.
[19.03.2025 03:24] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08\% accuracy improvements on KVG-Bench and exhibiting +4.60\% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at https://github.com/thunlp/DeepPerception.
[19.03.2025 03:25] Response: {
  "desc": "Эта статья представляет новый подход к улучшению визуального восприятия мультимодальных языковых моделей (MLLM). Авторы вводят задачу knowledge-intensive visual grounding (KVG), требующую тонкого визуального различения и интеграции специализированных знаний. Предлагается модель DeepPerception, использующая автоматическую генерацию обучающих данных и двухэтапное обучение для улучшения когнитивных способностей MLLM. Эксперименты на созданном датасете KVG-Bench показывают значительное улучшение точности и обобщающей способности модели.",
  "emoji": "🔬",
  "title": "DeepPerception: Улучшение визуального восприятия ИИ через интеграцию знаний"
}
[19.03.2025 03:25] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08\% accuracy improvements on KVG-Bench and exhibiting +4.60\% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at https://github.com/thunlp/DeepPerception."

[19.03.2025 03:25] Response: ```python
['DATASET', 'DATA', 'BENCHMARK', 'MULTIMODAL', 'TRAINING']
```
[19.03.2025 03:25] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08\% accuracy improvements on KVG-Bench and exhibiting +4.60\% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at https://github.com/thunlp/DeepPerception."

[19.03.2025 03:25] Response: ```python
["REASONING", "TRANSFER_LEARNING", "SYNTHETIC"]
```
[19.03.2025 03:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in visual perception and reasoning. It introduces a new task called knowledge-intensive visual grounding (KVG), which combines fine-grained visual discrimination with domain-specific knowledge. The authors propose DeepPerception, an enhanced MLLM that utilizes a data synthesis pipeline and a two-stage training framework to improve cognitive reasoning and perception integration. Experimental results show that DeepPerception outperforms existing methods, demonstrating the potential for MLLMs to achieve human-like visual understanding.","title":"Enhancing Visual Perception in MLLMs with DeepPerception"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in visual perception and reasoning. It introduces a new task called knowledge-intensive visual grounding (KVG), which combines fine-grained visual discrimination with domain-specific knowledge. The authors propose DeepPerception, an enhanced MLLM that utilizes a data synthesis pipeline and a two-stage training framework to improve cognitive reasoning and perception integration. Experimental results show that DeepPerception outperforms existing methods, demonstrating the potential for MLLMs to achieve human-like visual understanding.', title='Enhancing Visual Perception in MLLMs with DeepPerception'))
[19.03.2025 03:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"人类专家在细粒度视觉辨别方面表现出色，能够利用领域知识来优化感知特征，而当前的多模态大型语言模型（MLLMs）在这方面仍显不足。尽管拥有丰富的专家级知识，MLLMs在视觉感知中整合推理的能力较弱，常常直接生成响应而缺乏深入分析。为了解决这一问题，我们提出了知识密集型视觉定位（KVG），这是一项新颖的视觉定位任务，要求同时具备细粒度感知和领域特定知识的整合。我们提出的DeepPerception模型增强了认知视觉感知能力，通过自动化数据合成和两阶段训练框架，显著提高了在KVG-Bench数据集上的准确性和跨领域泛化能力。","title":"提升视觉感知的认知整合能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='人类专家在细粒度视觉辨别方面表现出色，能够利用领域知识来优化感知特征，而当前的多模态大型语言模型（MLLMs）在这方面仍显不足。尽管拥有丰富的专家级知识，MLLMs在视觉感知中整合推理的能力较弱，常常直接生成响应而缺乏深入分析。为了解决这一问题，我们提出了知识密集型视觉定位（KVG），这是一项新颖的视觉定位任务，要求同时具备细粒度感知和领域特定知识的整合。我们提出的DeepPerception模型增强了认知视觉感知能力，通过自动化数据合成和两阶段训练框架，显著提高了在KVG-Bench数据集上的准确性和跨领域泛化能力。', title='提升视觉感知的认知整合能力'))
[19.03.2025 03:25] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#synthetic", "#video"], "emoji": "🎬", "ru": {"title": "IPV-Bench: Новый рубеж в понимании и генерации невозможных видео", "desc": "Эта статья представляет IPV-Bench - новый бенчмарк для оценки и развития понимания и генерации видео. Он основан на таксоном
[19.03.2025 03:25] Using data from previous issue: {"categories": ["#3d", "#open_source", "#dataset", "#synthetic"], "emoji": "🤖", "ru": {"title": "Процедурная генерация сочлененных объектов для воплощенного ИИ", "desc": "Статья представляет новый метод под названием Infinite Mobility для синтеза высококачественных сочлененных объектов с помощью про
[19.03.2025 03:25] Querying the API.
[19.03.2025 03:25] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs) in this domain remains largely unexplored. To address this gap, we introduce Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks. The benchmark comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous evaluation, we define instance-specific evaluation criteria for each test case, guiding the assessment of both general response quality and factual consistency with visual inputs. Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks. Furthermore, our analysis demonstrates that visual fine-tuning can negatively impact the base LLM's creative abilities. Creation-MMBench provides valuable insights for advancing MLLM creativity and establishes a foundation for future improvements in multimodal generative intelligence. Full data and evaluation code is released on https://github.com/open-compass/Creation-MMBench.
[19.03.2025 03:25] Response: {
  "desc": "Статья представляет Creation-MMBench - новый мультимодальный бенчмарк для оценки творческих способностей мультимодальных больших языковых моделей (MLLM) в задачах, основанных на изображениях. Бенчмарк содержит 765 тестовых примеров, охватывающих 51 детализированную задачу, с критериями оценки для каждого случая. Эксперименты показали, что открытые MLLM значительно уступают проприетарным моделям в творческих задачах. Исследование также выявило, что визуальная дообучение может негативно влиять на творческие способности базовой языковой модели.",
  "emoji": "🎨",
  "title": "Измеряя творчество искусственного интеллекта: новый бенчмарк для мультимодальных моделей"
}
[19.03.2025 03:25] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs) in this domain remains largely unexplored. To address this gap, we introduce Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks. The benchmark comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous evaluation, we define instance-specific evaluation criteria for each test case, guiding the assessment of both general response quality and factual consistency with visual inputs. Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks. Furthermore, our analysis demonstrates that visual fine-tuning can negatively impact the base LLM's creative abilities. Creation-MMBench provides valuable insights for advancing MLLM creativity and establishes a foundation for future improvements in multimodal generative intelligence. Full data and evaluation code is released on https://github.com/open-compass/Creation-MMBench."

[19.03.2025 03:25] Response: ```python
["BENCHMARK", "MULTIMODAL"]
```
[19.03.2025 03:25] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs) in this domain remains largely unexplored. To address this gap, we introduce Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks. The benchmark comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous evaluation, we define instance-specific evaluation criteria for each test case, guiding the assessment of both general response quality and factual consistency with visual inputs. Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks. Furthermore, our analysis demonstrates that visual fine-tuning can negatively impact the base LLM's creative abilities. Creation-MMBench provides valuable insights for advancing MLLM creativity and establishes a foundation for future improvements in multimodal generative intelligence. Full data and evaluation code is released on https://github.com/open-compass/Creation-MMBench."

[19.03.2025 03:25] Response: ```python
['CREATIVITY', 'OPEN_SOURCE']
```
[19.03.2025 03:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces Creation-MMBench, a new benchmark designed to evaluate the creative capabilities of Multimodal Large Language Models (MLLMs) in tasks that involve both text and images. The benchmark includes 765 test cases across 51 specific tasks, each with tailored evaluation criteria to assess the quality of responses and their consistency with visual inputs. Experimental findings indicate that current open-source MLLMs perform poorly in creative tasks compared to proprietary models, and that visual fine-tuning may hinder the creative performance of base LLMs. Creation-MMBench aims to enhance understanding and development of MLLM creativity, providing a resource for future research in multimodal generative intelligence.","title":"Unlocking Creativity in Multimodal AI with Creation-MMBench"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces Creation-MMBench, a new benchmark designed to evaluate the creative capabilities of Multimodal Large Language Models (MLLMs) in tasks that involve both text and images. The benchmark includes 765 test cases across 51 specific tasks, each with tailored evaluation criteria to assess the quality of responses and their consistency with visual inputs. Experimental findings indicate that current open-source MLLMs perform poorly in creative tasks compared to proprietary models, and that visual fine-tuning may hinder the creative performance of base LLMs. Creation-MMBench aims to enhance understanding and development of MLLM creativity, providing a resource for future research in multimodal generative intelligence.', title='Unlocking Creativity in Multimodal AI with Creation-MMBench'))
[19.03.2025 03:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"创造力是智能的一个基本方面，涉及在不同情境中生成新颖且适当的解决方案。虽然大型语言模型（LLMs）在创造能力方面得到了广泛评估，但多模态大型语言模型（MLLMs）的评估仍然相对缺乏。为了解决这一问题，我们推出了Creation-MMBench，这是一个专门设计用于评估MLLMs在基于图像的实际任务中创造能力的多模态基准。实验结果表明，当前的开源MLLMs在创造性任务中显著低于专有模型，而视觉微调可能会对基础LLM的创造能力产生负面影响。","title":"评估多模态大型语言模型的创造力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='创造力是智能的一个基本方面，涉及在不同情境中生成新颖且适当的解决方案。虽然大型语言模型（LLMs）在创造能力方面得到了广泛评估，但多模态大型语言模型（MLLMs）的评估仍然相对缺乏。为了解决这一问题，我们推出了Creation-MMBench，这是一个专门设计用于评估MLLMs在基于图像的实际任务中创造能力的多模态基准。实验结果表明，当前的开源MLLMs在创造性任务中显著低于专有模型，而视觉微调可能会对基础LLM的创造能力产生负面影响。', title='评估多模态大型语言模型的创造力'))
[19.03.2025 03:25] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "🧩", "ru": {"title": "Frac-Connections: оптимизация глубоких нейросетей без расширения скрытых состояний", "desc": "Статья представляет новый подход к архитектуре глубоких нейронных сетей под названием Frac-Connections. Этот ме
[19.03.2025 03:25] Querying the API.
[19.03.2025 03:25] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.
[19.03.2025 03:25] Response: {
  "desc": "Статья представляет алгоритм DAPO для обучения с подкреплением крупномасштабных языковых моделей. Авторы открыто публикуют систему, достигающую 50 баллов на AIME 2024 с использованием базовой модели Qwen2.5-32B. В работе раскрываются четыре ключевые техники, делающие успешным RL для больших ЯМ. Исследователи также предоставляют код обучения и подготовленный набор данных для воспроизводимости результатов.",
  "emoji": "🧠",
  "title": "Открытая система RL для улучшения рассуждений больших языковых моделей"
}
[19.03.2025 03:25] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL."

[19.03.2025 03:25] Response: ```python
['RL', 'TRAINING', 'DATASET']
```
[19.03.2025 03:25] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL."

[19.03.2025 03:25] Response: ```python
['REASONING', 'OPEN_SOURCE', 'OPTIMIZATION']
```
[19.03.2025 03:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, which enhances the reasoning capabilities of large language models (LLMs) through reinforcement learning (RL). The authors address the lack of transparency in existing state-of-the-art LLMs by providing detailed insights into their training processes and techniques. They report achieving a significant performance score of 50 points on the AIME 2024 benchmark using the Qwen2.5-32B model. By open-sourcing their training code and dataset, they aim to improve reproducibility and foster further advancements in large-scale LLM reinforcement learning research.","title":"Unlocking LLM Potential with Open-Source Reinforcement Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, which enhances the reasoning capabilities of large language models (LLMs) through reinforcement learning (RL). The authors address the lack of transparency in existing state-of-the-art LLMs by providing detailed insights into their training processes and techniques. They report achieving a significant performance score of 50 points on the AIME 2024 benchmark using the Qwen2.5-32B model. By open-sourcing their training code and dataset, they aim to improve reproducibility and foster further advancements in large-scale LLM reinforcement learning research.', title='Unlocking LLM Potential with Open-Source Reinforcement Learning'))
[19.03.2025 03:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文提出了一种新的算法，称为解耦剪辑和动态采样策略优化（DAPO），旨在提升大型语言模型（LLM）的推理能力。我们通过强化学习技术，成功实现了在AIME 2024上获得50分的成绩，使用的是Qwen2.5-32B基础模型。与以往研究不同，我们公开了四个关键技术细节，帮助社区更好地理解和复现我们的训练结果。此外，我们还开源了训练代码和经过精心处理的数据集，以促进大型LLM强化学习的可重复性和未来研究。","title":"解锁大型语言模型的推理潜力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文提出了一种新的算法，称为解耦剪辑和动态采样策略优化（DAPO），旨在提升大型语言模型（LLM）的推理能力。我们通过强化学习技术，成功实现了在AIME 2024上获得50分的成绩，使用的是Qwen2.5-32B基础模型。与以往研究不同，我们公开了四个关键技术细节，帮助社区更好地理解和复现我们的训练结果。此外，我们还开源了训练代码和经过精心处理的数据集，以促进大型LLM强化学习的可重复性和未来研究。', title='解锁大型语言模型的推理潜力'))
[19.03.2025 03:25] Querying the API.
[19.03.2025 03:25] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models' time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results -- including their degree of external validity -- and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.
[19.03.2025 03:25] Response: {
  "desc": "Статья предлагает новую метрику для оценки возможностей ИИ-систем - горизонт времени 50%-ного выполнения задач. Этот показатель измеряет время, которое обычно требуется людям для выполнения задач, которые модели ИИ могут выполнить с 50%-ной вероятностью успеха. Исследование показало, что современные передовые модели ИИ, такие как Claude 3.7 Sonnet, имеют горизонт времени около 50 минут. Авторы отмечают, что горизонт времени ИИ удваивается примерно каждые семь месяцев с 2019 года, и обсуждают потенциальные последствия повышения автономности ИИ-систем.",
  "emoji": "⏳",
  "title": "Время на вашей стороне: ИИ догоняет человека в скорости выполнения задач"
}
[19.03.2025 03:25] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models' time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results -- including their degree of external validity -- and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month."

[19.03.2025 03:25] Response: ```python
["BENCHMARK"]
```
[19.03.2025 03:25] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models' time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results -- including their degree of external validity -- and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month."

[19.03.2025 03:25] Response: ```python
["REASONING", "ETHICS"]
```
[19.03.2025 03:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new metric called the 50%-task-completion time horizon to evaluate AI systems based on human performance. It measures the time it takes for humans to complete tasks that AI can accomplish with a 50% success rate. The study found that leading AI models, like Claude 3.7 Sonnet, have a time horizon of about 50 minutes, and this capability has been improving rapidly, doubling approximately every seven months. The authors highlight the implications of this trend, suggesting that AI could soon automate complex software tasks that currently require significant human effort.","title":"Measuring AI Progress: The 50%-Task-Completion Time Horizon"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new metric called the 50%-task-completion time horizon to evaluate AI systems based on human performance. It measures the time it takes for humans to complete tasks that AI can accomplish with a 50% success rate. The study found that leading AI models, like Claude 3.7 Sonnet, have a time horizon of about 50 minutes, and this capability has been improving rapidly, doubling approximately every seven months. The authors highlight the implications of this trend, suggesting that AI could soon automate complex software tasks that currently require significant human effort.', title='Measuring AI Progress: The 50%-Task-Completion Time Horizon'))
[19.03.2025 03:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"尽管人工智能在基准测试上取得了快速进展，但其实际表现的意义仍不明确。我们提出了一种新的度量标准：50%任务完成时间范围，旨在量化AI系统与人类能力的对比。通过对人类专家在多个任务上的完成时间进行测量，我们发现当前的前沿AI模型在这些任务上的50%时间范围约为50分钟。我们的研究表明，AI模型的时间范围自2019年以来大约每七个月翻倍，未来五年内，AI系统可能能够自动化许多目前需要人类一个月才能完成的软件任务。","title":"AI能力的新度量：50%任务完成时间"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='尽管人工智能在基准测试上取得了快速进展，但其实际表现的意义仍不明确。我们提出了一种新的度量标准：50%任务完成时间范围，旨在量化AI系统与人类能力的对比。通过对人类专家在多个任务上的完成时间进行测量，我们发现当前的前沿AI模型在这些任务上的50%时间范围约为50分钟。我们的研究表明，AI模型的时间范围自2019年以来大约每七个月翻倍，未来五年内，AI系统可能能够自动化许多目前需要人类一个月才能完成的软件任务。', title='AI能力的新度量：50%任务完成时间'))
[19.03.2025 03:25] Using data from previous issue: {"categories": ["#multimodal", "#ethics", "#security", "#benchmark", "#dataset"], "emoji": "🔒", "ru": {"title": "PEBench: Новый стандарт для оценки безопасности мультимодальных ИИ-моделей", "desc": "Статья представляет новый бенчмарк PEBench для оценки эффективности методов машинного разобучения (MU
[19.03.2025 03:25] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#multimodal", "#rl"], "emoji": "🧠", "ru": {"title": "MPBench: Комплексная оценка рассуждений языковых моделей", "desc": "Статья представляет новый бенчмарк MPBench для оценки эффективности моделей вознаграждения на уровне процесса (PRM) в различных сценар
[19.03.2025 03:25] Using data from previous issue: {"categories": ["#3d", "#optimization", "#data", "#dataset", "#cv", "#open_source"], "emoji": "🚗", "ru": {"title": "RoCo-Sim: прорыв в симуляции дорожного восприятия", "desc": "RoCo-Sim - это первая система симуляции для совместного восприятия дорожной обстановки. Она решает проблемы калибровки, раз
[19.03.2025 03:25] Querying the API.
[19.03.2025 03:25] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at https://github.com/nvidia-cosmos/cosmos-transfer1.
[19.03.2025 03:25] Response: {
  "desc": "Cosmos-Transfer - это условная модель генерации миров, способная создавать симуляции на основе нескольких пространственных входных данных разных модальностей. Модель позволяет гибко настраивать веса различных условных входов в разных пространственных локациях. Это обеспечивает высоко контролируемую генерацию миров и находит применение в различных сценариях переноса между мирами, включая Sim2Real. Авторы провели обширные оценки модели и продемонстрировали ее применение для физического ИИ, в том числе для робототехники Sim2Real и обогащения данных для беспилотных автомобилей.",
  "emoji": "🌌",
  "title": "Адаптивная генерация миров с пространственным контролем"
}
[19.03.2025 03:25] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at https://github.com/nvidia-cosmos/cosmos-transfer1."

[19.03.2025 03:25] Response: ```python
["3D", "AGENTS", "INFERENCE", "ROBOTICS"]
```
[19.03.2025 03:25] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at https://github.com/nvidia-cosmos/cosmos-transfer1."

[19.03.2025 03:25] Response: ```python
['TRANSFER_LEARNING', 'OPEN_SOURCE']
```
[19.03.2025 03:25] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Cosmos-Transfer is a novel model designed for generating world simulations using various spatial control inputs like segmentation, depth, and edge maps. It features an adaptive and customizable spatial conditional scheme that allows for different weights to be assigned to inputs based on their spatial locations. This flexibility enhances the controllability of world generation, making it suitable for applications such as Sim2Real in robotics and autonomous vehicles. The model has been extensively evaluated, and its code is open-sourced to foster further research in the field.","title":"Empowering World Generation with Adaptive Control"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Cosmos-Transfer is a novel model designed for generating world simulations using various spatial control inputs like segmentation, depth, and edge maps. It features an adaptive and customizable spatial conditional scheme that allows for different weights to be assigned to inputs based on their spatial locations. This flexibility enhances the controllability of world generation, making it suitable for applications such as Sim2Real in robotics and autonomous vehicles. The model has been extensively evaluated, and its code is open-sourced to foster further research in the field.', title='Empowering World Generation with Adaptive Control'))
[19.03.2025 03:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"我们介绍了Cosmos-Transfer，这是一种条件世界生成模型，可以根据多种空间控制输入（如分割、深度和边缘）生成世界模拟。在设计上，空间条件方案是自适应和可定制的，允许在不同空间位置对不同条件输入进行加权。这使得世界生成具有高度可控性，并在多种世界到世界的转移应用中找到用途，包括Sim2Real。我们进行了广泛的评估，分析了所提出的模型，并展示了其在物理人工智能中的应用，包括机器人Sim2Real和自动驾驶车辆数据增强。","title":"可控的世界生成，助力物理人工智能"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='我们介绍了Cosmos-Transfer，这是一种条件世界生成模型，可以根据多种空间控制输入（如分割、深度和边缘）生成世界模拟。在设计上，空间条件方案是自适应和可定制的，允许在不同空间位置对不同条件输入进行加权。这使得世界生成具有高度可控性，并在多种世界到世界的转移应用中找到用途，包括Sim2Real。我们进行了广泛的评估，分析了所提出的模型，并展示了其在物理人工智能中的应用，包括机器人Sim2Real和自动驾驶车辆数据增强。', title='可控的世界生成，助力物理人工智能'))
[19.03.2025 03:26] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#robotics", "#open_source", "#agents"], "emoji": "🤖", "ru": {"title": "KUDA: Динамическое планирование для роботов с открытым словарем", "desc": "KUDA - это система манипуляции с открытым словарем, которая объединяет обучение динамике и визуальное упр
[19.03.2025 03:26] Loading Chinese text from previous data.
[19.03.2025 03:26] Renaming data file.
[19.03.2025 03:26] Renaming previous data. hf_papers.json to ./d/2025-03-19.json
[19.03.2025 03:26] Saving new data file.
[19.03.2025 03:26] Generating page.
[19.03.2025 03:26] Renaming previous page.
[19.03.2025 03:26] Renaming previous data. index.html to ./d/2025-03-19.html
[19.03.2025 03:26] [Experimental] Generating Chinese page for reading.
[19.03.2025 03:26] Chinese vocab [{'word': '视频生成', 'pinyin': 'shìpín shēngchéng', 'trans': 'video generation'}, {'word': '时空一致性', 'pinyin': 'shíkōng yīzhìxìng', 'trans': 'spatiotemporal consistency'}, {'word': '合格', 'pinyin': 'hégé', 'trans': 'qualified'}, {'word': '情节合理性', 'pinyin': 'qíngjié hélǐxìng', 'trans': 'plot rationality'}, {'word': '连贯性', 'pinyin': 'liánhéngxìng', 'trans': 'coherence'}, {'word': '视觉一致性', 'pinyin': 'shìjué yīzhìxìng', 'trans': 'visual consistency'}, {'word': '摄像机', 'pinyin': 'shèxiàngjī', 'trans': 'camera'}, {'word': '协同作用', 'pinyin': 'xiétóng zuòyòng', 'trans': 'synergistic effect'}, {'word': '长期影响', 'pinyin': 'chángqī yǐngxiǎng', 'trans': 'long-term impact'}, {'word': '数据集', 'pinyin': 'shùjùjí', 'trans': 'dataset'}, {'word': '模型', 'pinyin': 'móxíng', 'trans': 'model'}, {'word': '构建', 'pinyin': 'gòujiàn', 'trans': 'construct'}, {'word': '训练', 'pinyin': 'xùnliàn', 'trans': 'train'}, {'word': '访问', 'pinyin': 'fǎngwèn', 'trans': 'access'}]
[19.03.2025 03:26] Renaming previous Chinese page.
[19.03.2025 03:26] Renaming previous data. zh.html to ./d/2025-03-18_zh_reading_task.html
[19.03.2025 03:26] Writing Chinese reading task.
[19.03.2025 03:26] Writing result.
[19.03.2025 03:26] Renaming log file.
[19.03.2025 03:26] Renaming previous data. log.txt to ./logs/2025-03-19_last_log.txt
