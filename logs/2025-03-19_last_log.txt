[19.03.2025 16:15] Read previous papers.
[19.03.2025 16:15] Generating top page (month).
[19.03.2025 16:15] Writing top page (month).
[19.03.2025 17:10] Read previous papers.
[19.03.2025 17:10] Get feed.
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14456
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14378
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14478
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14476
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.12797
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.12329
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.13424
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14125
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14492
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14504
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.12505
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14499
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.13265
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14495
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14151
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.13111
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.12545
[19.03.2025 17:10] Extract page data from URL. URL: https://huggingface.co/papers/2503.10522
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.09443
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.12303
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.12271
[19.03.2025 17:10] Extract page data from URL. URL: https://huggingface.co/papers/2503.12355
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10546
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.10410
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.08893
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.14002
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.13661
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.12127
[19.03.2025 17:10] Extract page data from URL. URL: https://huggingface.co/papers/2503.10905
[19.03.2025 17:10] Get page data from previous paper. URL: https://huggingface.co/papers/2503.08683
[19.03.2025 17:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.03.2025 17:10] No deleted papers detected.
[19.03.2025 17:10] Downloading and parsing papers (pdf, html). Total: 30.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.14456.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.14456.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.14456.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.14378.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.14378.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.14378.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.14478.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.14478.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.14478.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.14476.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.14476.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.14476.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.12797.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.12797.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.12797.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.12329.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.12329.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.12329.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.13424.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.13424.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.13424.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.14125.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.14125.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.14125.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.14492.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.14492.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.14492.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.14504.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.14504.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.14504.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.12505.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.12505.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.12505.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.14499.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.14499.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.14499.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.13265.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.13265.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.13265.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.14495.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.14495.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.14495.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.14151.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.14151.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.14151.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.13111.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.13111.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.13111.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.12545.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.12545.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.12545.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.10522.
[19.03.2025 17:10] Downloading paper 2503.10522 from http://arxiv.org/pdf/2503.10522v1...
[19.03.2025 17:10] Extracting affiliations from text.
[19.03.2025 17:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 1 ] . [ 1 2 2 5 0 1 . 3 0 5 2 : r AudioX: Diffusion Transformer for Anything-to-Audio Generation Zeyue Tian1, Yizhu Jin1, Zhaoyang Liu1, Ruibin Yuan1, Xu Tan2, Qifeng Chen1, Wei Xue1, Yike Guo1 1Hong Kong University of Science and Technology 2Moonshot AI Figure 1. (a) Overview of AudioX, illustrating its capabilities across various tasks. (b) Radar chart comparing the performance of different methods across multiple benchmarks. AudioX demonstrates superior Inception Scores (IS) across diverse set of datasets in audio and music generation tasks. "
[19.03.2025 17:10] Response: ```python
["Hong Kong University of Science and Technology", "Moonshot AI"]
```
[19.03.2025 17:10] Deleting PDF ./assets/pdf/2503.10522.pdf.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.09443.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.09443.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.09443.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.12303.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.12303.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.12303.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.12271.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.12271.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.12271.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.12355.
[19.03.2025 17:10] Downloading paper 2503.12355 from http://arxiv.org/pdf/2503.12355v1...
[19.03.2025 17:10] Extracting affiliations from text.
[19.03.2025 17:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 5 5 3 2 1 . 3 0 5 2 : r Atlas: Multi-Scale Attention Improves Long Context Image Modeling Kumar Krishna Agrawal * 1 Long Lian * 1 Longchao Liu 1 Natalia Harguindeguy 1 2 Boyi Li 1 Alexander Bick 3 Maggie Chung 2 Trevor Darrell 1 Adam Yala "
[19.03.2025 17:10] Response: ```python
[]
```
[19.03.2025 17:10] Extracting affiliations from text.
[19.03.2025 17:10] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 5 5 3 2 1 . 3 0 5 2 : r Atlas: Multi-Scale Attention Improves Long Context Image Modeling Kumar Krishna Agrawal * 1 Long Lian * 1 Longchao Liu 1 Natalia Harguindeguy 1 2 Boyi Li 1 Alexander Bick 3 Maggie Chung 2 Trevor Darrell 1 Adam YalaEfficiently modeling massive images is longstanding challenge in machine learning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on two key ideas, (i) multi-scale representations (ii) bi-directional cross-scale communication. MSA creates O(logN ) scales to represent the image across progressively coarser features and leverages cross-attention to propagate information across scales. We then introduce Atlas, novel neural network architecture based on MSA. We demonstrate that Atlas significantly improves the compute-performance tradeoff of long-context image modeling in high-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves 91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster. Atlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96% better than LongViT. In comparisons against MambaVision-S, we find Atlas-S achieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px respectively, while obtaining similar runtimes. Code for reproducing our experiments and pretrained models is available at https://github.com/yalalab/atlas. 1. Introduction Long-context image modeling remains fundamental challenge in computer vision with broad applications to biomedicine (Xu et al., 2024), satellite imagery (Rad, 2024), and vision-language modeling (Gemini-Team et al., 2023; Wang et al., 2024; Qwen-Team, 2025; Chen et al., 2024). At the core of this challenge is compute expressivity trade-off; we aim to develop models that efficiently scale to massive input sequences while capturing arbitrary pair-wise depen- *Equal contribution, Project lead 1University of California, Berkeley 2University of California San Francisco 3Vanderbilt University . Correspondence to: Kumar Krishna Agrawal <kagrawal@berkeley.edu>, Adam Yala <yala@berkeley.edu>. Figure 1. (a) Training efficiency comparison of different vision architectures on HR-IN100 across increasing input resolutions (1024-4096px). (b) Atlas exhibits similar runtime scaling as MambaVision while obtaining significantly better accuracy. dencies between input tokens. As shown in Figure 1(a), self-attention, as used in Vision Transformers, is highly expressive, but its computational cost scales poorly (i.e., quadratically) with sequence length. It remains infeasible to train end-to-end Vision Transformers on massive imaging modalities such as mammograms or whole-slide pathology images. At another end of the spectrum, state space models (SSMs) and recurrent architectures are highly efficient, achieving linear computational complexity; however, SSM-based models perform poorly in long-context imaging modeling (Figure 1b). 1 Multi-Scale Attention Improves Long Context Image Modeling Long-context image modeling requires novel neural primitives and new benchmarks to guide their development. Recent work in efficient architecture design, such as FasterViT (Hatamizadeh et al., 2023) and MambaVision (Hatamizadeh & Kautz, 2024), has primarily focused on improving the throughput vs accuracy trade-offs in the context of standard resolution ImageNet experiments (224 224). While valuable, this setting yields little insight into how methods scale to larger input resolutions. To this end, we propose new high-resolution benchmark based on ImageNet-100 (HR-IN 100). We evaluate the speed vs accuracy trade-off of different neural networks at progressively larger resolutions, ranging from 1024 1024 to 4096 4096 images. As input resolution increases, long-range communication across distant parts of the image becomes more essential for image classification, and asymptotic computational complexity begins to dominate model runtime. In designing novel neural primitive, we aim to enable arbitrary cross-token interaction with minimal intermediate steps (i.e., communication complexity) while minimizing computational complexity as function of input sequence length. To this end, we propose Multiscale Attention (MSA), novel primitive for high-resolution image modeling. MSA is built on two key ideas: multiscale representations and cross-scale communication. In each MSA block, we leverage simple S-token max-pooling kernel to summarize small spatial regions (e.g., 4x4 input region), into progressively coarser summary representations across O(logS ) spatial scales, where is the total sequence length. We then leverage windowed cross-attention mechanism to enable information-sharing between tokens at different scales. At each scale, tokens attend to nearby tokens of the same scale and tokens from all coarser scales. This top-down communication enables MSA to integrate information across the entire sequence. Each scales tokens also cross-attend to its parent finer-grain scale tokens, allowing each coarse token to refine its representation through bottom-up communication. Altogether, this bi-directional communication pattern enables information mixing between all input tokens through O(log ) intermediate tokens (i.e. coarse scale representations) and within O(N log ) runtime. In controlled block-level experiments (see Table 3), we find that MSA outperforms alternative neural network primitives in long-context modeling, including LongNets dilated attention (Ding et al., 2023), MambaVision Mixer (Hatamizadeh & Kautz, 2024), and FasterViTs Hierarchical Attention (Hatamizadeh et al., 2023). We propose Atlas, novel architecture designed around the unique advantages of MSA. Given sequence length N, which defines log scales within MSA, Atlas leverages log macro-stages to progressively down-sample the input until MSA recovers only single scale. We leverage the rich scale-2 representations of our MSA block as down-sampling mechanism, enabling both faster and more performant down-sampling than traditional approaches. We demonstrate that Atlas significantly improves the Pareto In 1024 1024 exfrontier in long-context modeling. periments, as shown in Table 1, Atlas obtains comparable runtime to MambaVision (23.1hr vs 22.6hr) on the same hardware, while obtaining 6.1% higher accuracy (91.04 vs 84.86). Compared to FasterViT and LongViT, Atlas is 2.95 and 2.25 faster, obtaining 7.38% (91.04 vs 83.66) and 4.96% (91.04 vs 86.08) higher accuracy, respectively. Moreover, th"
[19.03.2025 17:10] Mistral response. {"id": "dd7dbbfe92674d2c88e3079228fc7b2d", "object": "chat.completion", "created": 1742404238, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"University of California, Berkeley\", \"University of California San Francisco\", \"Vanderbilt University\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1793, "total_tokens": 1822, "completion_tokens": 29}}
[19.03.2025 17:10] Response: ```python
["University of California, Berkeley", "University of California San Francisco", "Vanderbilt University"]
```
[19.03.2025 17:10] Deleting PDF ./assets/pdf/2503.12355.pdf.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.10546.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.10546.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.10546.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.10410.
[19.03.2025 17:10] Downloading paper 2503.10410 from http://arxiv.org/pdf/2503.10410v1...
[19.03.2025 17:10] Failed to download and parse paper https://huggingface.co/papers/2503.10410: max() arg is an empty sequence
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.08893.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.08893.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.08893.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.14002.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.14002.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.14002.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.13661.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.13661.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.13661.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.12127.
[19.03.2025 17:10] Extra JSON file exists (./assets/json/2503.12127.json), skip PDF parsing.
[19.03.2025 17:10] Paper image links file exists (./assets/img_data/2503.12127.json), skip HTML parsing.
[19.03.2025 17:10] Success.
[19.03.2025 17:10] Downloading and parsing paper https://huggingface.co/papers/2503.10905.
[19.03.2025 17:10] Downloading paper 2503.10905 from http://arxiv.org/pdf/2503.10905v2...
[19.03.2025 17:10] Extracting affiliations from text.
[19.03.2025 17:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Zhuoyan Xu1*, Khoi Duc Nguyen1*, Preeti Mukherjee2, Saurabh Bagchi2,Somali Chaterji2, Yingyu Liang1,3, Yin Li1 1University of Wisconsin-Madison 2Purdue University 3The University of Hong Kong 5 2 0 2 7 1 ] . [ 2 5 0 9 0 1 . 3 0 5 2 : r a "
[19.03.2025 17:11] Response: ```python
["University of Wisconsin-Madison", "Purdue University", "The University of Hong Kong"]
```
[19.03.2025 17:11] Deleting PDF ./assets/pdf/2503.10905.pdf.
[19.03.2025 17:11] Success.
[19.03.2025 17:11] Downloading and parsing paper https://huggingface.co/papers/2503.08683.
[19.03.2025 17:11] Extra JSON file exists (./assets/json/2503.08683.json), skip PDF parsing.
[19.03.2025 17:11] Paper image links file exists (./assets/img_data/2503.08683.json), skip HTML parsing.
[19.03.2025 17:11] Success.
[19.03.2025 17:11] Enriching papers with extra data.
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 0. We present RWKV-7 "Goose", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 1. Synthetic videos nowadays is widely used to complement data scarcity and diversity of real-world videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) C...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 2. Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 3. Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the ...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 4. Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning ...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 5. Image captioning has been a longstanding challenge in vision-language research. With the rise of LLMs, modern Vision-Language Models (VLMs) generate detailed and comprehensive image descriptions. However, benchmarking the quality of such captions remains unresolved. This paper addresses two key ques...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 6. Large-scale articulated objects with high quality are desperately needed for multiple tasks related to embodied AI. Most existing methods for creating articulated objects are either data-driven or simulation based, which are limited by the scale and quality of the training data or the fidelity and h...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 7. Residual connections are central to modern deep learning architectures, enabling the training of very deep networks by mitigating gradient vanishing. Hyper-Connections recently generalized residual connections by introducing multiple connection strengths at different depths, thereby addressing the s...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 8. We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 9. Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textua...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 10. Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 11. Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks tha...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 12. Generating flexible-view 3D scenes, including 360{\deg} rotation and zooming, from single images is challenging due to a lack of 3D data. To this end, we introduce FlexWorld, a novel framework consisting of two key components: (1) a strong video-to-video (V2V) diffusion model to generate high-qualit...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 13. Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequen...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 14. We present Concat-ID, a unified framework for identity-preserving video generation. Concat-ID employs Variational Autoencoders to extract image features, which are concatenated with video latents along the sequence dimension, leveraging solely 3D self-attention mechanisms without the need for additi...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 15. Multimodal large language models (MLLMs) excel at 2D visual understanding but remain limited in their ability to reason about 3D space. In this work, we leverage large-scale high-quality 3D scene data with open-set annotations to introduce 1) a novel supervised fine-tuning dataset and 2) a new evalu...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 16. In recent years, Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in tasks such as visual question answering, visual understanding, and reasoning. However, this impressive progress relies on vast amounts of data collected from the internet, raising significant conce...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 17. Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integ...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 18. Cross-lingual transfer enables vision-language models (VLMs) to perform vision tasks in various languages with training data only in one language. Current approaches rely on large pre-trained multilingual language models. However, they face the curse of multilinguality, sacrificing downstream task p...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 19. Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) face challenges with fine-grained perception and complex reasoning. Prevalent multimodal pre-training approaches focus on enhancing perception by training on high-quality image captions due to the extremely high cost of ...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 20. The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to imp...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 21. Efficiently modeling massive images is a long-standing challenge in machine learning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on two key ideas, (i) multi-scale representations (ii) bi-directional cross-scale communication. MSA creates O(log N) scales to represent the image ...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 22. With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to m...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 23. Roadside Collaborative Perception refers to a system where multiple roadside units collaborate to pool their perceptual data, assisting vehicles in enhancing their environmental awareness. Existing roadside perception methods concentrate on model design but overlook data issues like calibration erro...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 24. An ideal model evaluation should achieve two goals: identifying where the model fails and providing actionable improvement guidance. Toward these goals for Language Model (LM) evaluations, we formulate the problem of generating a weakness profile, a set of weaknesses expressed in natural language, g...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 25. Generative models have recently made remarkable progress in the field of 3D objects. However, their practical application in fields like engineering remains limited since they fail to deliver the accuracy, quality, and controllability needed for domain-specific tasks. Fine-tuning large generative mo...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 26. Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, achieving strong performance in specialized domains like mathematical reasoning and non-English languages often requires extensive training on massive datasets. This paper in...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 27. Addressing the retrieval of unsafe content from vision-language models such as CLIP is an important step towards real-world integration. Current efforts have relied on unlearning techniques that try to erase the model's knowledge of unsafe concepts. While effective in reducing unwanted outputs, unle...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 28. Multimodal Large Language Models (MLLMs) have shown impressive capabilities in reasoning, yet come with substantial computational cost, limiting their deployment in resource-constrained settings. Despite recent efforts on improving the efficiency of MLLMs, prior solutions fall short in responding to...
[19.03.2025 17:11] ********************************************************************************
[19.03.2025 17:11] Abstract 29. Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise for improving safety by addressing the perception and prediction uncertainties inherent in single-agent systems. However, traditional cooperative methods are constrained by rigid collaboration protocols and limited generaliz...
[19.03.2025 17:11] Read previous papers.
[19.03.2025 17:11] Generating reviews via LLM API.
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#architecture", "#training", "#open_source", "#dataset", "#multilingual"], "emoji": "🦢", "ru": {"title": "RWKV-7: Эффективная архитектура для многоязычного моделирования последовательностей", "desc": "RWKV-7 'Goose' - это новая архитектура моделирования последовательностей, которая 
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#synthetic", "#video"], "emoji": "🎬", "ru": {"title": "IPV-Bench: Новый рубеж в понимании и генерации невозможных видео", "desc": "Эта статья представляет IPV-Bench - новый бенчмарк для оценки и развития понимания и генерации видео. Он основан на таксоном
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#creativity", "#open_source", "#benchmark", "#multimodal"], "emoji": "🎨", "ru": {"title": "Измеряя творчество искусственного интеллекта: новый бенчмарк для мультимодальных моделей", "desc": "Статья представляет Creation-MMBench - новый мультимодальный бенчмарк для оценки творческих 
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#rl", "#dataset", "#reasoning", "#optimization", "#training", "#open_source"], "emoji": "🧠", "ru": {"title": "Открытая система RL для улучшения рассуждений больших языковых моделей", "desc": "Статья представляет алгоритм DAPO для обучения с подкреплением крупномасштабных языковых мо
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#multimodal", "#data", "#dataset", "#reasoning", "#synthetic", "#benchmark", "#transfer_learning", "#training"], "emoji": "🔬", "ru": {"title": "DeepPerception: Улучшение визуального восприятия ИИ через интеграцию знаний", "desc": "Эта статья представляет новый подход к улучшению виз
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#benchmark", "#games", "#open_source", "#cv"], "emoji": "📸", "ru": {"title": "Новый рубеж в генерации подписей к изображениям: ИИ догоняет человека", "desc": "Статья посвящена проблеме оценки качества генерации подписей к изображениям с помощью современных моделей компьютерного зрен
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#3d", "#open_source", "#dataset", "#synthetic"], "emoji": "🤖", "ru": {"title": "Процедурная генерация сочлененных объектов для воплощенного ИИ", "desc": "Статья представляет новый метод под названием Infinite Mobility для синтеза высококачественных сочлененных объектов с помощью про
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "🧩", "ru": {"title": "Frac-Connections: оптимизация глубоких нейросетей без расширения скрытых состояний", "desc": "Статья представляет новый подход к архитектуре глубоких нейронных сетей под названием Frac-Connections. Этот ме
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#agents", "#robotics", "#transfer_learning", "#open_source", "#inference", "#3d"], "emoji": "🌌", "ru": {"title": "Адаптивная генерация миров с пространственным контролем", "desc": "Cosmos-Transfer - это условная модель генерации миров, способная создавать симуляции на основе несколь
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#survey", "#multimodal", "#benchmark", "#dataset", "#alignment"], "emoji": "🤖", "ru": {"title": "Выравнивание мультимодальных ИИ: путь к безопасности и эффективности", "desc": "Эта статья представляет собой всесторонний обзор алгоритмов выравнивания для мультимодальных больших языко
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#multimodal", "#rl"], "emoji": "🧠", "ru": {"title": "MPBench: Комплексная оценка рассуждений языковых моделей", "desc": "Статья представляет новый бенчмарк MPBench для оценки эффективности моделей вознаграждения на уровне процесса (PRM) в различных сценар
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#ethics", "#benchmark", "#reasoning"], "emoji": "⏳", "ru": {"title": "Время на вашей стороне: ИИ догоняет человека в скорости выполнения задач", "desc": "Статья предлагает новую метрику для оценки возможностей ИИ-систем - горизонт времени 50%-ного выполнения задач. Этот показатель и
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#diffusion", "#3d", "#video"], "emoji": "🌐", "ru": {"title": "Создание гибких 3D-миров из одного изображения", "desc": "FlexWorld - это новая система для создания гибких 3D-сцен из одиночных изображений, включая вращение на 360° и масштабирование. Она состоит из мощной модели диффуз
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#math", "#benchmark", "#optimization", "#reasoning", "#training", "#architecture"], "emoji": "🧮", "ru": {"title": "Повышение точности верификации математических рассуждений через временную согласованность", "desc": "Статья представляет новый метод временной согласованности для вериф
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#benchmark", "#3d", "#video"], "emoji": "🎬", "ru": {"title": "Concat-ID: Новый стандарт в генерации видео с сохранением идентичности", "desc": "Concat-ID - это унифицированная система для генерации видео с сохранением идентичности. Она использует вариационные автоэнкодеры для извлеч
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#3d", "#multimodal", "#benchmark", "#dataset"], "emoji": "🏠", "ru": {"title": "Новый шаг к пониманию 3D-пространства искусственным интеллектом", "desc": "Статья представляет новый подход к обучению мультимодальных языковых моделей пониманию трехмерного
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#multimodal", "#ethics", "#security", "#benchmark", "#dataset"], "emoji": "🔒", "ru": {"title": "PEBench: Новый стандарт для оценки безопасности мультимодальных ИИ-моделей", "desc": "Статья представляет новый бенчмарк PEBench для оценки эффективности методов машинного разобучения (MU
[19.03.2025 17:11] Querying the API.
[19.03.2025 17:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX, a unified Diffusion Transformer model for Anything-to-Audio and Music Generation. Unlike previous domain-specific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various modalities including text, video, image, music, and audio. Its key innovation is a multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified cross-modal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audio captions based on the VGGSound dataset, and V2M-caps with 6 million music captions derived from the V2M dataset. Extensive experiments demonstrate that AudioX not only matches or outperforms state-of-the-art specialized models, but also offers remarkable versatility in handling diverse input modalities and generation tasks within a unified architecture. The code and datasets will be available at https://zeyuet.github.io/AudioX/
[19.03.2025 17:11] Response: {
  "desc": "AudioX - это унифицированная модель Diffusion Transformer для генерации аудио и музыки на основе различных входных данных. Модель использует стратегию мультимодального маскированного обучения для создания надежных кросс-модальных представлений. Для обучения были созданы два больших набора данных: vggsound-caps и V2M-caps. Эксперименты показывают, что AudioX не уступает специализированным моделям и обладает высокой универсальностью в обработке различных модальностей входных данных.",
  "emoji": "🎵",
  "title": "AudioX: универсальная модель для генерации аудио из чего угодно"
}
[19.03.2025 17:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX, a unified Diffusion Transformer model for Anything-to-Audio and Music Generation. Unlike previous domain-specific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various modalities including text, video, image, music, and audio. Its key innovation is a multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified cross-modal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audio captions based on the VGGSound dataset, and V2M-caps with 6 million music captions derived from the V2M dataset. Extensive experiments demonstrate that AudioX not only matches or outperforms state-of-the-art specialized models, but also offers remarkable versatility in handling diverse input modalities and generation tasks within a unified architecture. The code and datasets will be available at https://zeyuet.github.io/AudioX/"

[19.03.2025 17:11] Response: ```python
['DATASET', 'MULTIMODAL', 'AUDIO', 'ARCHITECTURE']
```
[19.03.2025 17:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX, a unified Diffusion Transformer model for Anything-to-Audio and Music Generation. Unlike previous domain-specific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various modalities including text, video, image, music, and audio. Its key innovation is a multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified cross-modal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audio captions based on the VGGSound dataset, and V2M-caps with 6 million music captions derived from the V2M dataset. Extensive experiments demonstrate that AudioX not only matches or outperforms state-of-the-art specialized models, but also offers remarkable versatility in handling diverse input modalities and generation tasks within a unified architecture. The code and datasets will be available at https://zeyuet.github.io/AudioX/"

[19.03.2025 17:11] Response: ```python
["DIFFUSION", "SYNTHETIC"]
```
[19.03.2025 17:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces AudioX, a novel Diffusion Transformer model designed for generating audio and music from various input types. Unlike traditional models that focus on specific tasks, AudioX integrates multiple modalities, allowing it to process text, video, images, and audio seamlessly. The model employs a unique multi-modal masked training strategy, which enhances its ability to learn robust representations by masking inputs across different modalities. Additionally, the authors address the challenge of limited high-quality training data by creating two extensive datasets, enabling AudioX to outperform existing specialized models in versatility and performance.","title":"AudioX: Unifying Audio and Music Generation Across Modalities"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces AudioX, a novel Diffusion Transformer model designed for generating audio and music from various input types. Unlike traditional models that focus on specific tasks, AudioX integrates multiple modalities, allowing it to process text, video, images, and audio seamlessly. The model employs a unique multi-modal masked training strategy, which enhances its ability to learn robust representations by masking inputs across different modalities. Additionally, the authors address the challenge of limited high-quality training data by creating two extensive datasets, enabling AudioX to outperform existing specialized models in versatility and performance.', title='AudioX: Unifying Audio and Music Generation Across Modalities'))
[19.03.2025 17:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"音频和音乐生成在许多应用中变得越来越重要，但现有方法存在显著局限性。我们提出了AudioX，这是一种统一的扩散变换器模型，能够实现多种输入到音频和音乐的生成。AudioX的创新之处在于其多模态掩蔽训练策略，使模型能够从不同模态的掩蔽输入中学习，从而生成高质量的音频和音乐。通过构建两个全面的数据集，AudioX在处理多样化输入模态和生成任务方面展现了卓越的灵活性和性能。","title":"统一音频与音乐生成的创新模型"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='音频和音乐生成在许多应用中变得越来越重要，但现有方法存在显著局限性。我们提出了AudioX，这是一种统一的扩散变换器模型，能够实现多种输入到音频和音乐的生成。AudioX的创新之处在于其多模态掩蔽训练策略，使模型能够从不同模态的掩蔽输入中学习，从而生成高质量的音频和音乐。通过构建两个全面的数据集，AudioX在处理多样化输入模态和生成任务方面展现了卓越的灵活性和性能。', title='统一音频与音乐生成的创新模型'))
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#dataset", "#multilingual", "#multimodal", "#training", "#synthetic", "#machine_translation", "#long_context", "#transfer_learning"], "emoji": "🌐", "ru": {"title": "Монолингвальные модели преодолевают языковые барьеры в мультимодальных задачах", "desc": "Статья исследует масштабируе
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#multimodal", "#transfer_learning", "#training"], "emoji": "🧠", "ru": {"title": "Самосовершенствующийся ИИ: новый подход к обучению мультимодальных языковых моделей", "desc": "Статья представляет SIcog - фреймворк для улучшения когнитивных способностей му
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#optimization", "#cv", "#benchmark", "#inference", "#diffusion", "#reasoning"], "emoji": "🖼️", "ru": {"title": "Reflect-DiT: Умное улучшение генерации изображений через анализ и рефлексию", "desc": "Эта статья представляет новый метод Reflect-DiT для улучшения генерации изображений 
[19.03.2025 17:11] Querying the API.
[19.03.2025 17:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Efficiently modeling massive images is a long-standing challenge in machine learning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on two key ideas, (i) multi-scale representations (ii) bi-directional cross-scale communication. MSA creates O(log N) scales to represent the image across progressively coarser features and leverages cross-attention to propagate information across scales. We then introduce Atlas, a novel neural network architecture based on MSA. We demonstrate that Atlas significantly improves the compute-performance tradeoff of long-context image modeling in a high-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves 91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster. Atlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96% better than LongViT. In comparisons against MambaVision-S, we find Atlas-S achieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px respectively, while obtaining similar runtimes. Code for reproducing our experiments and pretrained models is available at https://github.com/yalalab/atlas.
[19.03.2025 17:11] Response: {
  "desc": "Статья представляет новый подход к эффективному моделированию больших изображений - Multi-Scale Attention (MSA). MSA использует многомасштабные представления и двунаправленную коммуникацию между масштабами, создавая O(log N) уровней для представления изображения. На основе MSA авторы разработали нейросетевую архитектуру Atlas, которая значительно улучшает соотношение производительности и вычислительных затрат при моделировании изображений с большим контекстом. Atlas демонстрирует высокую точность и скорость работы по сравнению с современными моделями на задаче классификации изображений высокого разрешения.",
  "emoji": "🔍",
  "title": "Масштабируемое внимание для эффективной обработки гигантских изображений"
}
[19.03.2025 17:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Efficiently modeling massive images is a long-standing challenge in machine learning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on two key ideas, (i) multi-scale representations (ii) bi-directional cross-scale communication. MSA creates O(log N) scales to represent the image across progressively coarser features and leverages cross-attention to propagate information across scales. We then introduce Atlas, a novel neural network architecture based on MSA. We demonstrate that Atlas significantly improves the compute-performance tradeoff of long-context image modeling in a high-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves 91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster. Atlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96% better than LongViT. In comparisons against MambaVision-S, we find Atlas-S achieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px respectively, while obtaining similar runtimes. Code for reproducing our experiments and pretrained models is available at https://github.com/yalalab/atlas."

[19.03.2025 17:11] Response: ```python
['ARCHITECTURE', 'CV']
```
[19.03.2025 17:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Efficiently modeling massive images is a long-standing challenge in machine learning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on two key ideas, (i) multi-scale representations (ii) bi-directional cross-scale communication. MSA creates O(log N) scales to represent the image across progressively coarser features and leverages cross-attention to propagate information across scales. We then introduce Atlas, a novel neural network architecture based on MSA. We demonstrate that Atlas significantly improves the compute-performance tradeoff of long-context image modeling in a high-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves 91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster. Atlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96% better than LongViT. In comparisons against MambaVision-S, we find Atlas-S achieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px respectively, while obtaining similar runtimes. Code for reproducing our experiments and pretrained models is available at https://github.com/yalalab/atlas."

[19.03.2025 17:11] Response: ```python
["LONG_CONTEXT", "OPTIMIZATION", "OPEN_SOURCE"]
```
[19.03.2025 17:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents Multi-Scale Attention (MSA), a method designed to efficiently model large images in machine learning. MSA utilizes multi-scale representations and bi-directional cross-scale communication to enhance image feature extraction. The authors introduce Atlas, a neural network architecture that implements MSA, achieving significant improvements in speed and accuracy for high-resolution image tasks. Atlas outperforms existing models like ConvNext and FasterViT, demonstrating a superior compute-performance tradeoff while maintaining high accuracy on the ImageNet dataset.","title":"Revolutionizing Image Modeling with Multi-Scale Attention"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents Multi-Scale Attention (MSA), a method designed to efficiently model large images in machine learning. MSA utilizes multi-scale representations and bi-directional cross-scale communication to enhance image feature extraction. The authors introduce Atlas, a neural network architecture that implements MSA, achieving significant improvements in speed and accuracy for high-resolution image tasks. Atlas outperforms existing models like ConvNext and FasterViT, demonstrating a superior compute-performance tradeoff while maintaining high accuracy on the ImageNet dataset.', title='Revolutionizing Image Modeling with Multi-Scale Attention'))
[19.03.2025 17:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"在机器学习中，高效建模大规模图像一直是一个挑战。为此，我们提出了多尺度注意力（MSA），它依赖于多尺度表示和双向跨尺度通信两个关键思想。MSA通过创建O(log N)的尺度来表示图像，并利用交叉注意力在不同尺度之间传播信息。我们还介绍了一种基于MSA的新型神经网络架构Atlas，实验表明Atlas在高分辨率图像建模中显著提高了计算性能的平衡。","title":"高效图像建模的新突破：多尺度注意力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='在机器学习中，高效建模大规模图像一直是一个挑战。为此，我们提出了多尺度注意力（MSA），它依赖于多尺度表示和双向跨尺度通信两个关键思想。MSA通过创建O(log N)的尺度来表示图像，并利用交叉注意力在不同尺度之间传播信息。我们还介绍了一种基于MSA的新型神经网络架构Atlas，实验表明Atlas在高分辨率图像建模中显著提高了计算性能的平衡。', title='高效图像建模的新突破：多尺度注意力'))
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#multimodal", "#optimization", "#robotics", "#open_source", "#agents"], "emoji": "🤖", "ru": {"title": "KUDA: Динамическое планирование для роботов с открытым словарем", "desc": "KUDA - это система манипуляции с открытым словарем, которая объединяет обучение динамике и визуальное упр
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#3d", "#optimization", "#data", "#dataset", "#cv", "#open_source"], "emoji": "🚗", "ru": {"title": "RoCo-Sim: прорыв в симуляции дорожного восприятия", "desc": "RoCo-Sim - это первая система симуляции для совместного восприятия дорожной обстановки. Она решает проблемы калибровки, раз
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#data", "#interpretability", "#training", "#open_source"], "emoji": "🌳", "ru": {"title": "EvalTree: точное профилирование слабостей языковых моделей", "desc": "Статья представляет метод EvalTree для создания профиля слабостей языковых моделей. EvalTree
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#dataset", "#synthetic", "#optimization", "#data", "#3d"], "emoji": "🚗", "ru": {"title": "MeshFleet: Автоматизированная фильтрация данных для 3D генеративных моделей транспортных средств", "desc": "Статья представляет MeshFleet - отфильтрованный и аннотированный набор данных 3D моде
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#multilingual", "#data", "#training", "#transfer_learning", "#low_resource"], "emoji": "🧠", "ru": {"title": "Малые данные, большой результат: оптимизация языковых моделей без масштабирования", "desc": "Данная статья исследует стратегический подход к улучшен
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#open_source", "#cv", "#security"], "emoji": "🛡️", "ru": {"title": "Гиперболическая иерархия для осознанной безопасности в мультимодальных моделях", "desc": "Статья представляет новый подход к обработке небезопасного контента в мультимодальных мод
[19.03.2025 17:11] Querying the API.
[19.03.2025 17:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Multimodal Large Language Models (MLLMs) have shown impressive capabilities in reasoning, yet come with substantial computational cost, limiting their deployment in resource-constrained settings. Despite recent efforts on improving the efficiency of MLLMs, prior solutions fall short in responding to varying runtime conditions, in particular changing resource availability (e.g., contention due to the execution of other programs on the device). To bridge this gap, we introduce AdaLLaVA, an adaptive inference framework that learns to dynamically reconfigure operations in an MLLM during inference, accounting for the input data and a latency budget. We conduct extensive experiments across benchmarks involving question-answering, reasoning, and hallucination. Our results show that AdaLLaVA effectively adheres to input latency budget, achieving varying accuracy and latency tradeoffs at runtime. Further, we demonstrate that AdaLLaVA adapts to both input latency and content, can be integrated with token selection for enhanced efficiency, and generalizes across MLLMs. Our project webpage with code release is at https://zhuoyan-xu.github.io/ada-llava/.
[19.03.2025 17:11] Response: {
  "desc": "AdaLLaVA - это адаптивная система вывода для мультимодальных больших языковых моделей (MLLM), которая динамически реконфигурирует операции во время выполнения с учетом входных данных и ограничений по задержке. Эта система позволяет эффективно использовать MLLM в условиях ограниченных ресурсов, адаптируясь к меняющимся условиям выполнения. Эксперименты показали, что AdaLLaVA успешно соблюдает заданные ограничения по задержке, достигая различных компромиссов между точностью и скоростью работы. Система также демонстрирует способность адаптироваться к входным данным и обобщаться на различные MLLM.",
  "emoji": "⚡",
  "title": "AdaLLaVA: Адаптивный вывод для эффективных мультимодальных языковых моделей"
}
[19.03.2025 17:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multimodal Large Language Models (MLLMs) have shown impressive capabilities in reasoning, yet come with substantial computational cost, limiting their deployment in resource-constrained settings. Despite recent efforts on improving the efficiency of MLLMs, prior solutions fall short in responding to varying runtime conditions, in particular changing resource availability (e.g., contention due to the execution of other programs on the device). To bridge this gap, we introduce AdaLLaVA, an adaptive inference framework that learns to dynamically reconfigure operations in an MLLM during inference, accounting for the input data and a latency budget. We conduct extensive experiments across benchmarks involving question-answering, reasoning, and hallucination. Our results show that AdaLLaVA effectively adheres to input latency budget, achieving varying accuracy and latency tradeoffs at runtime. Further, we demonstrate that AdaLLaVA adapts to both input latency and content, can be integrated with token selection for enhanced efficiency, and generalizes across MLLMs. Our project webpage with code release is at https://zhuoyan-xu.github.io/ada-llava/."

[19.03.2025 17:11] Response: ```python
['INFERENCE', 'MULTIMODAL', 'BENCHMARK']
```
[19.03.2025 17:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Multimodal Large Language Models (MLLMs) have shown impressive capabilities in reasoning, yet come with substantial computational cost, limiting their deployment in resource-constrained settings. Despite recent efforts on improving the efficiency of MLLMs, prior solutions fall short in responding to varying runtime conditions, in particular changing resource availability (e.g., contention due to the execution of other programs on the device). To bridge this gap, we introduce AdaLLaVA, an adaptive inference framework that learns to dynamically reconfigure operations in an MLLM during inference, accounting for the input data and a latency budget. We conduct extensive experiments across benchmarks involving question-answering, reasoning, and hallucination. Our results show that AdaLLaVA effectively adheres to input latency budget, achieving varying accuracy and latency tradeoffs at runtime. Further, we demonstrate that AdaLLaVA adapts to both input latency and content, can be integrated with token selection for enhanced efficiency, and generalizes across MLLMs. Our project webpage with code release is at https://zhuoyan-xu.github.io/ada-llava/."

[19.03.2025 17:11] Response: ```python
['REASONING', 'HALLUCINATIONS', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[19.03.2025 17:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents AdaLLaVA, an adaptive inference framework designed for Multimodal Large Language Models (MLLMs) to optimize their performance under varying computational resources. It addresses the challenge of maintaining efficiency while responding to different runtime conditions, such as resource contention from other applications. AdaLLaVA dynamically adjusts the operations of MLLMs during inference based on the input data and a specified latency budget, allowing for flexible accuracy and latency trade-offs. The framework has been tested across various benchmarks, demonstrating its ability to adapt to both input characteristics and latency requirements, while also integrating with token selection for improved efficiency.","title":"Adaptive Inference for Efficient Multimodal Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents AdaLLaVA, an adaptive inference framework designed for Multimodal Large Language Models (MLLMs) to optimize their performance under varying computational resources. It addresses the challenge of maintaining efficiency while responding to different runtime conditions, such as resource contention from other applications. AdaLLaVA dynamically adjusts the operations of MLLMs during inference based on the input data and a specified latency budget, allowing for flexible accuracy and latency trade-offs. The framework has been tested across various benchmarks, demonstrating its ability to adapt to both input characteristics and latency requirements, while also integrating with token selection for improved efficiency.', title='Adaptive Inference for Efficient Multimodal Language Models'))
[19.03.2025 17:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"多模态大型语言模型（MLLMs）在推理方面表现出色，但其计算成本高，限制了在资源受限环境中的应用。尽管近期对提高MLLMs效率的努力有所增加，但现有解决方案在应对不同运行时条件方面仍显不足，特别是在资源可用性变化时。为了解决这个问题，我们提出了AdaLLaVA，这是一种自适应推理框架，能够在推理过程中动态重新配置MLLM的操作，考虑输入数据和延迟预算。我们的实验表明，AdaLLaVA能够有效遵循输入延迟预算，实现运行时的准确性和延迟之间的权衡。","title":"自适应推理，提升多模态模型效率"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='多模态大型语言模型（MLLMs）在推理方面表现出色，但其计算成本高，限制了在资源受限环境中的应用。尽管近期对提高MLLMs效率的努力有所增加，但现有解决方案在应对不同运行时条件方面仍显不足，特别是在资源可用性变化时。为了解决这个问题，我们提出了AdaLLaVA，这是一种自适应推理框架，能够在推理过程中动态重新配置MLLM的操作，考虑输入数据和延迟预算。我们的实验表明，AdaLLaVA能够有效遵循输入延迟预算，实现运行时的准确性和延迟之间的权衡。', title='自适应推理，提升多模态模型效率'))
[19.03.2025 17:11] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#optimization", "#rl", "#agents"], "emoji": "🚗", "ru": {"title": "Языковые модели открывают новую эру в кооперативном автономном вождении", "desc": "CoLMDriver - это первая система кооперативного вождения на основе языковых моделей (LLM), обеспечивающая э
[19.03.2025 17:11] Loading Chinese text from previous data.
[19.03.2025 17:11] Renaming data file.
[19.03.2025 17:11] Renaming previous data. hf_papers.json to ./d/2025-03-19.json
[19.03.2025 17:11] Saving new data file.
[19.03.2025 17:11] Generating page.
[19.03.2025 17:11] Renaming previous page.
[19.03.2025 17:11] Renaming previous data. index.html to ./d/2025-03-19.html
[19.03.2025 17:11] [Experimental] Generating Chinese page for reading.
[19.03.2025 17:11] Chinese vocab [{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'}, {'word': '序列', 'pinyin': 'xù liè', 'trans': 'sequence'}, {'word': '建模', 'pinyin': 'jiàn mó', 'trans': 'modeling'}, {'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'}, {'word': '多语言', 'pinyin': 'duō yǔ yán', 'trans': 'multilingual'}, {'word': '任务', 'pinyin': 'rèn wu', 'trans': 'task'}, {'word': '达到', 'pinyin': 'dá dào', 'trans': 'achieve'}, {'word': '最佳', 'pinyin': 'zuì jiā', 'trans': 'best'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '尽管', 'pinyin': 'jìn guǎn', 'trans': 'although'}, {'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'training'}, {'word': '令牌', 'pinyin': 'lìng pài', 'trans': 'token'}, {'word': '数量', 'pinyin': 'shù liàng', 'trans': 'quantity'}, {'word': '少', 'pinyin': 'shǎo', 'trans': 'few'}, {'word': '内存', 'pinyin': 'nèi cún', 'trans': 'memory'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'}, {'word': '时间', 'pinyin': 'shí jiān', 'trans': 'time'}, {'word': '常数', 'pinyin': 'cháng shù', 'trans': 'constant'}, {'word': '引入', 'pinyin': 'yǐn rù', 'trans': 'introduce'}, {'word': '规则', 'pinyin': 'guī zé', 'trans': 'rule'}, {'word': '放松', 'pinyin': 'fàng sōng', 'trans': 'relax'}, {'word': '值', 'pinyin': 'zhí', 'trans': 'value'}, {'word': '替换', 'pinyin': 'tì huàn', 'trans': 'replace'}, {'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'demonstrate'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '发布', 'pinyin': 'fā bù', 'trans': 'release'}, {'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'}]
[19.03.2025 17:11] Renaming previous Chinese page.
[19.03.2025 17:11] Renaming previous data. zh.html to ./d/2025-03-18_zh_reading_task.html
[19.03.2025 17:11] Writing Chinese reading task.
[19.03.2025 17:11] Writing result.
[19.03.2025 17:11] Renaming log file.
[19.03.2025 17:11] Renaming previous data. log.txt to ./logs/2025-03-19_last_log.txt
