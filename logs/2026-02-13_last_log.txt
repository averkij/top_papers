[13.02.2026 04:20] Read previous papers.
[13.02.2026 04:20] Generating top page (month).
[13.02.2026 04:20] Writing top page (month).
[13.02.2026 05:55] Read previous papers.
[13.02.2026 05:55] Get feed.
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09877
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12125
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12099
[13.02.2026 05:55] Extract page data from URL. URL: https://huggingface.co/papers/2602.12280
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12056
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05548
[13.02.2026 05:55] Extract page data from URL. URL: https://huggingface.co/papers/2602.10934
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12153
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11541
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11337
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08194
[13.02.2026 05:55] Extract page data from URL. URL: https://huggingface.co/papers/2602.12164
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11964
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11683
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11298
[13.02.2026 05:55] Extract page data from URL. URL: https://huggingface.co/papers/2602.12262
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11761
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11636
[13.02.2026 05:55] Extract page data from URL. URL: https://huggingface.co/papers/2602.11543
[13.02.2026 05:55] Extract page data from URL. URL: https://huggingface.co/papers/2602.11509
[13.02.2026 05:55] Extract page data from URL. URL: https://huggingface.co/papers/2602.08277
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11598
[13.02.2026 05:55] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10585
[13.02.2026 05:55] Extract page data from URL. URL: https://huggingface.co/papers/2602.09070
[13.02.2026 05:55] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.02.2026 05:55] No deleted papers detected.
[13.02.2026 05:55] Downloading and parsing papers (pdf, html). Total: 24.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.09877.
[13.02.2026 05:55] Extra JSON file exists (./assets/json/2602.09877.json), skip PDF parsing.
[13.02.2026 05:55] Paper image links file exists (./assets/img_data/2602.09877.json), skip HTML parsing.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.12125.
[13.02.2026 05:55] Extra JSON file exists (./assets/json/2602.12125.json), skip PDF parsing.
[13.02.2026 05:55] Paper image links file exists (./assets/img_data/2602.12125.json), skip HTML parsing.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.12099.
[13.02.2026 05:55] Extra JSON file exists (./assets/json/2602.12099.json), skip PDF parsing.
[13.02.2026 05:55] Paper image links file exists (./assets/img_data/2602.12099.json), skip HTML parsing.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.12280.
[13.02.2026 05:55] Downloading paper 2602.12280 from https://arxiv.org/pdf/2602.12280v1...
[13.02.2026 05:55] Extracting affiliations from text.
[13.02.2026 05:55] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching Huai-Hsun Cheng Siang-Ling Zhang Yu-Lun Liu 6 2 0 2 2 1 ] . [ 1 0 8 2 2 1 . 2 0 6 2 : r Figure 1. Progressive semantic illusions from text. Given pair of text prompts (a), our method generates vector sketch that evolves over time. The initial generated sketch (b) depicts the first concept (e.g., pig). By adding further generated strokes (c), the drawing is transformed into totally different object (e.g., angel). This creates Stroke of Surprise: the process subverts the viewers expectation of the initial concept, triggering dramatic semantic reversal as the final strokes re-contextualize the entire composition. "
[13.02.2026 05:55] Response: ```python
[]
```
[13.02.2026 05:55] Extracting affiliations from text.
[13.02.2026 05:55] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching Huai-Hsun Cheng Siang-Ling Zhang Yu-Lun Liu6 2 0 2 2 1 ] . [ 1 0 8 2 2 1 . 2 0 6 2 : r Figure 1. Progressive semantic illusions from text. Given pair of text prompts (a), our method generates vector sketch that evolves over time. The initial generated sketch (b) depicts the first concept (e.g., pig). By adding further generated strokes (c), the drawing is transformed into totally different object (e.g., angel). This creates Stroke of Surprise: the process subverts the viewers expectation of the initial concept, triggering dramatic semantic reversal as the final strokes re-contextualize the entire composition.Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, novel vector sketching task where single sketch undergoes dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the dual-constraint: initial prefix strokes must form coherent object (e.g., duck) while simultaneously serving as the structural foundation for second concept (e.g., sheep) upon adding delta strokes. To address this, we propose sequence-aware joint optimization 1 framework driven by dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover common structural subspace valid for both targets. Furthermore, we introduce novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/ (e.g., SketchAgent) employ greedy strategy, optimizing strokes solely for A. This renders the fixed prefix as semantic noise when extending to B, resulting in clutter. Crucially, these baselines fail to find Common Subspace, which is shared geometric configuration valid for both semantic interpretations. To overcome these limitations, we propose Stroke of Surprise, sequence-aware joint optimization framework designed to discover this common structural subspace  (Fig. 3)  . Unlike sequential approaches, we optimize parameters for both the prefix (Object A) and full phase (Object B) simultaneously using dual-branch Score Distillation Sampling (SDS) mechanism. This guidance ensures prefix strokes are recognizable as the initial concept yet primed for re-interpretation. Furthermore, we introduce geometric Overlay Loss to enforce spatial complementarity and prevent occlusion. This enables delta strokes to structurally integrate with and re-contextualize the prefix. For example, it can transform pig ears into angel wings, creating seamless illusion. Our main contributions are summarized as follows: Task: We introduce Progressive Semantic Illusion, extending visual illusions from the spatial to the temporal dimension. This task requires single vector sketch to reveal distinct semantic interpretations through progressive stroke accumulation. Method: We propose sequence-aware joint optimization framework that optimizes shared stroke parameters under simultaneous semantic constraints and novel Overlay Loss, ensuring the prefix strokes form robust structural foundation for the final illusion. Insight & Scalability: We demonstrate that joint optimization identifies Common Subspace that resolves conflicts between early semantic clarity and later structural integration. Our method scales to multi-phase illusions (A C) and significantly outperforms baselines in recognizability and coherence. 2. Related Work Sketch Generation and Sequential Modeling. Vector sketch synthesis evolved from category-specific to openvocabulary generation, progressing from edge detection [16, 118] and sketch datasets [29, 39, 54, 99] through RNNs [39], Transformers [17, 70, 96], GANs [34, 71], and autoregressive models [120, 130]. CLIP enabled text-driven synthesis [32, 108, 109], while diffusion-based Score Distillation [90] was adapted for SVG [52, 93, 106, 125, 127, 136, 138], with recent feed-forward [4, 22] and LLM-based methods [89, 121, 128]. Stroke ordering encodes semantics, as shown in foundational rendering [41, 42] and sequential modeling via attention [37], VAEs [39], embeddings [1], completion [71, 104], transformers [11], Bezier curves [25], temporality [55], and diffusion [114]. Stroke semantics are Figure 2. Challenges in progressive illusion sketching. (a) Rasterbased methods (e.g., Nano Banana Pro) rely on destructive editing, modifying the initial structure to fit the final target and thus violating the progressive constraint. (b) Vector-based baselines (e.g., SketchDreamer [93] or SketchAgent [110]) employ greedy strategy, where specific Phase 1 details become semantic noise or clutter in Phase 2. (c) Ours achieves dual-semantic coherency by jointly optimizing for common structural subspace, ensuring the initial strokes are valid building blocks for both interpretations (e.g., rabbit elephant). 1. Introduction Visual illusions traditionally exploit spatial ambiguities, requiring viewers to change viewpoints to perceive hidden meanings (e.g., Visual Anagrams [36]). In this work, we introduce new dimension to this artistic interplay: time. We propose Progressive Semantic Illusions, novel vector sketching task where the drawing process itself drives semantic transformation. As illustrated in Fig. 1, our method generates coherent initial sketch (e.g., pig) that is subsequently re-contextualized by additional strokes into distinct concept (e.g., an angel). This Stroke of Surprise subverts expectations, achieving perceptual shift through sequential stroke accumulation rather than spatial manipulation. Sketch generation has evolved from category-specific RNNs [39] to open-vocabulary models leveraging CLIP [94] and diffusion priors [97]. Methods like CLIPasso [108] and VectorFusion [52] utilize differentiable rasterization for highfidelity sketching, while sequential approaches like SketchAgent [110] and SketchDreamer [93] mimic step-by-step human drawing. "
[13.02.2026 05:55] Mistral response. {"id": "a01c5f83267e4450a99b1564658eb570", "created": 1770962141, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1449, "total_tokens": 1455, "completion_tokens": 6, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[13.02.2026 05:55] Response: ```python
[]
```
[13.02.2026 05:55] Deleting PDF ./assets/pdf/2602.12280.pdf.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.12056.
[13.02.2026 05:55] Extra JSON file exists (./assets/json/2602.12056.json), skip PDF parsing.
[13.02.2026 05:55] Paper image links file exists (./assets/img_data/2602.12056.json), skip HTML parsing.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.05548.
[13.02.2026 05:55] Extra JSON file exists (./assets/json/2602.05548.json), skip PDF parsing.
[13.02.2026 05:55] Paper image links file exists (./assets/img_data/2602.05548.json), skip HTML parsing.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.10934.
[13.02.2026 05:55] Downloading paper 2602.10934 from https://arxiv.org/pdf/2602.10934v2...
[13.02.2026 05:55] Extracting affiliations from text.
[13.02.2026 05:55] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 1 ] . [ 2 4 3 9 0 1 . 2 0 6 2 : r MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models MOSI.AI* "
[13.02.2026 05:55] Response: ```python
[]
```
[13.02.2026 05:55] Extracting affiliations from text.
[13.02.2026 05:55] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 1 ] . [ 2 4 3 9 0 1 . 2 0 6 2 : r MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models MOSI.AI*Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully endto-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSSAudio-Tokenizer consistently outperforms prior codecs over wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as unified, scalable interface for the next generation of native audio foundation models. Code: https://github.com/OpenMOSS/MOSS-Audio-Tokenizer Model: https://huggingface.co/OpenMOSS-Team/MOSS-Audio-TokenizerRecent advances in large language models [18] have demonstrated the effectiveness of autoregressive modeling over discrete token sequences. By providing unified discrete interface, text tokenizers [9, 10] allow large language models to operate directly on raw text, serving as the foundation upon which compression, understanding, generation, and in-context learning capabilities emerge within single autoregressive modeling framework. Extending this paradigm to audio requires Full contributors can be found in the Contributors section. Figure 1 Audio reconstruction quality comparison. 1 Table 1 Comparison of representative audio tokenizers with respect to architectural design and functional capabilities. 4 indicates support, 8 indicates not supported, and indicates not specified. Trans. denotes Transformer, and Hybrid denotes hybrid architecture combining CNN and Transformer. End-to-end optimize indicates whether all modules are jointly optimized under unified objective. Model Frame rate Encoder Arch. Decoder Arch. Stream ing Variable Bitrate Semantic rich Reconstruction Speech Sound Music Pretrained encoder free End-to-end optimize Encodec DAC SpeechTokenizer Mimi BigCodec StableCodec XCodec2.0 XY-Tokenizer DualCodec Higgs-Audio-Tokenizer MiMo-Audio-Tokenizer Qwen3-TTS-Tokenizer MOSS-Audio-Tokenizer 75 75 50 12.5 80 25 50 12.5 12.5 25 25 12.5 12. CNN CNN CNN CNN CNN CNN Hybrid Hybrid CNN CNN Hybrid Hybrid Hybrid Hybrid Hybrid Hybrid CNN Hybrid Hybrid CNN Hybrid Hybrid Hybrid Hybrid Trans. Trans. 4 8 8 4 8 8 8 8 8 8 8 4 4 4 4 4 4 8 4 8 8 4 4 4 8 8 4 4 8 8 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 8 4 8 8 8 8 8 4 4 4 4 4 8 4 8 8 8 8 8 4 4 4 4 4 8 8 4 4 8 8 8 8 4 8 4 4 8 8 4 4 8 8 8 8 8 8 4 unified discrete audio tokenizer that can serve as native interface for autoregressive modeling [1114]. Unlike text, audio contains both fine-grained acoustic details and long-range structure, making discrete tokenization more challenging [11]. unified audio tokenizer should enable high-fidelity reconstruction of diverse audio signals while remaining compatible with autoregressive sequence modeling [1416]. Existing approaches typically address these requirements through pretrained encoders [1722], multi-stage training pipelines [14, 23, 24], or architecture-specific inductive biases [2527], achieving strong performance under particular design choices. However, such designs introduce additional dependencies and architectural constraints that make it difficult to scale models, data, and quantization capacity in unified manner. From this perspective, we draw inspiration from the success of large language models, where simple and efficient architectures trained on large-scale data have proven critical for achieving strong performance [28, 29]. We posit that enabling an audio tokenizer to reach higher performance ceiling similarly requires simple and scalable architecture that can be trained end-to-end on large amounts of data. Such design emphasizes joint optimization and scale, while minimizing reliance on external priors, pretrained components, or complex architectural heuristics. In this work, we propose MOSS-Audio-Tokenizer, fully end-to-end audio tokenizer that serves as unified discrete interface for autoregressive audio language models. Our tokenizer, build on CAT (Causal Audio Tokenizer with Transformer) architecture, operates at 24 kHz sampling rate with low token frame rate of 12.5 Hz, and jointly optimizes the encoder, quantizer, decoder, and discriminator within single training pipeline, without relying on pretrained encoders, distillation, or separate optimization of individual components. Both the encoder and decoder are built entirely from causal Transformer blocks, resulting in simple and scalable architecture that is naturally aligned with autoregressive modeling [30]. All components of MOSS-Audio-Tokenizer are designed to operate in streaming manner, enabling low-latency inference and consistent traininginference behavior [15, 25, 26]. By scaling large amounts of paired audiotext data, MOSS-Audio-Tokenizer learns discrete representations that are both structurally rich and acoustically expressive, while remaining robust across wide range of bitrates. As result, the tokenizer achieves high-quality reconstruction of general audio, including speech, sound, and music, from very low to high bitrate regimes, providing strong lower bound and high upper bound for downstream audio language models. Across speech, sound, and music, MOSS-Audio-Tokenizer achieve"
[13.02.2026 05:55] Mistral response. {"id": "12fdd352cbdf4bf9ac64e269528f52ea", "created": 1770962149, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1533, "total_tokens": 1545, "completion_tokens": 12, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"MOSI.AI\"]\n```"}}]}
[13.02.2026 05:55] Response: ```python
["MOSI.AI"]
```
[13.02.2026 05:55] Deleting PDF ./assets/pdf/2602.10934.pdf.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.12153.
[13.02.2026 05:55] Extra JSON file exists (./assets/json/2602.12153.json), skip PDF parsing.
[13.02.2026 05:55] Paper image links file exists (./assets/img_data/2602.12153.json), skip HTML parsing.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.11541.
[13.02.2026 05:55] Extra JSON file exists (./assets/json/2602.11541.json), skip PDF parsing.
[13.02.2026 05:55] Paper image links file exists (./assets/img_data/2602.11541.json), skip HTML parsing.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.11337.
[13.02.2026 05:55] Extra JSON file exists (./assets/json/2602.11337.json), skip PDF parsing.
[13.02.2026 05:55] Paper image links file exists (./assets/img_data/2602.11337.json), skip HTML parsing.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.08194.
[13.02.2026 05:55] Extra JSON file exists (./assets/json/2602.08194.json), skip PDF parsing.
[13.02.2026 05:55] Paper image links file exists (./assets/img_data/2602.08194.json), skip HTML parsing.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.12164.
[13.02.2026 05:55] Downloading paper 2602.12164 from https://arxiv.org/pdf/2602.12164v1...
[13.02.2026 05:55] Extracting affiliations from text.
[13.02.2026 05:55] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision Xiaohan He * 1 2 Shiyang Feng * 1 Songtao Huang 1 2 Lei Bai 1 Bin Wang 2 Bo Zhang 1 6 2 0 2 2 1 ] . [ 1 4 6 1 2 1 . 2 0 6 2 : r Abstract Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, twostage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through transition from sparse supervision to unsupervised learning. In the first stage, the model uses small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github. com/InternScience/Sci-CoE. 1. Introduction Self-evolving Reinforcement Learning (RL) has emerged as transformative paradigm for Large Language Models (LLMs) to refine their reasoning trajectories through feedback. notable milestone is the Zero RL paradigm introduced by DeepSeek-R1 (Guo et al., 2025), which elicits sophisticated reasoning behaviors without prior supervised imitation learning. However, this approach remains fundamentally dependent on annotated datasets for reward calculation. To mitigate this reliance, self-play mechanisms have been adopted to facilitate autonomous evolution. In these 1Shanghai Artificial Intelligence Laboratory 2"
[13.02.2026 05:55] Response: ```python
["Shanghai Artificial Intelligence Laboratory"]
```
[13.02.2026 05:55] Deleting PDF ./assets/pdf/2602.12164.pdf.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.11964.
[13.02.2026 05:55] Extra JSON file exists (./assets/json/2602.11964.json), skip PDF parsing.
[13.02.2026 05:55] Paper image links file exists (./assets/img_data/2602.11964.json), skip HTML parsing.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.11683.
[13.02.2026 05:55] Extra JSON file exists (./assets/json/2602.11683.json), skip PDF parsing.
[13.02.2026 05:55] Paper image links file exists (./assets/img_data/2602.11683.json), skip HTML parsing.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.11298.
[13.02.2026 05:55] Extra JSON file exists (./assets/json/2602.11298.json), skip PDF parsing.
[13.02.2026 05:55] Paper image links file exists (./assets/img_data/2602.11298.json), skip HTML parsing.
[13.02.2026 05:55] Success.
[13.02.2026 05:55] Downloading and parsing paper https://huggingface.co/papers/2602.12262.
[13.02.2026 05:55] Downloading paper 2602.12262 from https://arxiv.org/pdf/2602.12262v1...
[13.02.2026 05:55] Extracting affiliations from text.
[13.02.2026 05:55] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization Tunyu Zhang * 1 Xinxi Zhang * 1 Ligong Han 2 3 Haizhou Shi 1 Xiaoxiao He 1 Zhuowei Li 4 Hao Wang 2 3 Kai Xu 2 3 Akash Srivastava 2 3 Hao Wang 1 Vladimir Pavlovic 1 Dimitris N. Metaxas 1 6 2 0 2 2 1 ] . [ 1 2 6 2 2 1 . 2 0 6 2 : r a "
[13.02.2026 05:55] Response: ```python
[]
```
[13.02.2026 05:55] Extracting affiliations from text.
[13.02.2026 05:55] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization Tunyu Zhang * 1 Xinxi Zhang * 1 Ligong Han 2 3 Haizhou Shi 1 Xiaoxiao He 1 Zhuowei Li 4 Hao Wang 2 3 Kai Xu 2 3 Akash Srivastava 2 3 Hao Wang 1 Vladimir Pavlovic 1 Dimitris N. Metaxas 1 6 2 0 2 2 1 ] . [ 1 2 6 2 2 1 . 2 0 6 2 : r aDiffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to substantial degradation in generation quality. To alleviate this, we propose trajectory self-distillation framework that improves few-step decoding by distilling the models own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on highprobability teacher modes. Across benchmarks, our approach consistently outperforms strong fewstep baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D. 1. Introduction Inference-time efficiency is central challenge in large language modeling, especially for real-time and computeconstrained applications (Zhen et al., 2025; Miao et al., 2025; Alizadeh et al., 2024). Many practical deployments, such as real-time decision-making and on-device AI, require consistently low-latency responses. Diffusion large language models (DLLMs) (Labs et al., 2025; Song et al., 2025; Nie et al., 2025; Cheng et al., 2025; Ye et al., 2025) *Equal contribution Rutgers University 1Department of Computer SciInnovation ence, 3MIT-IBM Watson AI Lab 4Amazon. Correspondence to: Ligong Han <ligong.han@rutgers.edu>, Dimitris N. Metaxas <dnm@cs.rutgers.edu>. Tunyu Zhang <ty.zhang@rutgers.edu>, 2Red Hat AI Preprint. February 13, 2026. 1 offer promising direction by enabling parallel token generation. However, existing DLLMs rely on long decoding chains consisting of many diffusion steps (Sahoo et al., 2024; Schiff et al.; Nie et al., 2025; Ye et al., 2025), which significantly limits their efficiency gains. When decoding is made more aggressive by reducing the number of steps, these models struggle to accurately generate multiple tokens simultaneously (Cheng et al., 2025). Recent work (Yoo et al., 2025; Chen et al., 2025; Xu et al., 2024a; Qian et al., 2026; Kim et al., 2025; Zhang et al., 2025b) has explored fine-tuning DLLMs to support few-step decoding, aiming to further reduce inference latency beyond standard diffusion sampling. However, few-step decoding is fundamentally challenged by the common mean-field (tokenfactorized) parameterization used in masked diffusion models (Xu et al., 2024a; Yoo et al., 2025; Zhang et al., 2025b), which induces factorization (approximation) error that typically grows as the number of denoising steps decreases. Moreover, training and inference often operate on different intermediate-state distributions: training commonly uses random masking, while inference frequently relies on nonrandom heuristics (e.g., confidence-based schedules) that change the masked states that the model encounters (Wang et al., 2025b; Qian et al., 2026). Together, this motivates the following important research question: Can we reduce mean-field approximation error under tight step budgets for DLLMs? We answer this question affirmatively by introducing two complementary training ingredients. (i) Trajectory supervision. First, we introduce trajectory self-distillation, which trains the student on the teachers own rollout trajectories. This directly addresses traintest mismatch in intermediate states: training often relies on random masking, while inference typically uses non-random heuristics (e.g., confidence-based schedules) that induce different masking patterns and introduce dependencies among simultaneously decoded tokens. By distilling on rollout trajecotires produced by the teacher under the target decoding procedure, the student is supervised on-policy and can use its limited step budget more effectively, matching the Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization trajectory distribution encountered at inference time rather than optimizing only endpoint transitions as previous selfdistillation frameworks (Yoo et al., 2025; Chen et al., 2025). (ii) Better training objective for few-step decoding. To strengthen learning under aggressive step compression, we incorporate Direct Discriminative Optimization (DDO) (Zheng et al., 2025) into the distillation objective. With only few decoding steps, the denoising posterior becomes highly multimodal (Xiao et al., 2021; Xu et al., 2024b), and trajectory matching with forward-KL objective tends to be mode-covering, yielding over-smoothed predictions and weak alignment with teacher trajectories. DDO mitigates this issue via GAN-inspired likelihood-ratio loss that can be interpreted as reverse-KL style, mode-seeking distillation: by contrasting the fine-tuned model against its initialization, it emphasizes high-probability teacher modes rather than uniformly matching likelihood across all continuations. Moreover, for masked diffusion, this likelihood-ratio objective can be evaluated directly from the models predicted probabilities, avoiding Monte Carlo ELBO estimation (and its associated gradient variance). Finally, we introduce path-consistency regularizer that reweights token-level losses based on decoding order, mitigating error propagation from early mispredictions in blockwise few-step decoding. We refer to the resulting approach as Self-Trajectory Distillation via DDO (T3D). T3D provides simple yet effective solution for few-step diffusion language modeling. By distilling directly from teacher-generated trajectories, T3D eliminates distribution shift and stabilizes training in the few-step regime, without requiring any additional ground-truth supervision. Our theoretical analysis shows that trajectory-level distillation reduces conditional dependencies in the reverse process, leading to lower factorization error. Extensive experiments on standard reasoning and code-generation benchmarks demonstrate"
[13.02.2026 05:55] Mistral response. {"id": "430a34a1227b452e86307bfc3966c5df", "created": 1770962159, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1532, "total_tokens": 1572, "completion_tokens": 40, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Rutgers University\",\n    \"Department of Computer Science\",\n    \"MIT-IBM Watson AI Lab\",\n    \"Amazon\",\n    \"Red Hat AI\"\n]\n```"}}]}
[13.02.2026 05:55] Response: ```python
[
    "Rutgers University",
    "Department of Computer Science",
    "MIT-IBM Watson AI Lab",
    "Amazon",
    "Red Hat AI"
]
```
[13.02.2026 05:55] Deleting PDF ./assets/pdf/2602.12262.pdf.
[13.02.2026 05:56] Success.
[13.02.2026 05:56] Downloading and parsing paper https://huggingface.co/papers/2602.11761.
[13.02.2026 05:56] Extra JSON file exists (./assets/json/2602.11761.json), skip PDF parsing.
[13.02.2026 05:56] Paper image links file exists (./assets/img_data/2602.11761.json), skip HTML parsing.
[13.02.2026 05:56] Success.
[13.02.2026 05:56] Downloading and parsing paper https://huggingface.co/papers/2602.11636.
[13.02.2026 05:56] Extra JSON file exists (./assets/json/2602.11636.json), skip PDF parsing.
[13.02.2026 05:56] Paper image links file exists (./assets/img_data/2602.11636.json), skip HTML parsing.
[13.02.2026 05:56] Success.
[13.02.2026 05:56] Downloading and parsing paper https://huggingface.co/papers/2602.11543.
[13.02.2026 05:56] Downloading paper 2602.11543 from https://arxiv.org/pdf/2602.11543v1...
[13.02.2026 05:56] Extracting affiliations from text.
[13.02.2026 05:56] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Pretraining Large Language Model using Distributed GPUs: Memory-Efficient Decentralized Paradigm Jinrui Zhang 1 2 Chaodong Xiao 1 2 Aoqi Wu 1 2 Xindong Zhang 2 Lei Zhang 1 2 Abstract Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100).Recent decentralized training methods reduce communication overhead by employing federated opthey still need to train timization; however, the entire model on each node, remaining constrained by GPU memory limitations. In this work, we propose SParse Expert Synchronization (SPES), memory-efficient decentralized framework for pretraining mixture-of-experts (MoE) LLMs. SPES trains only subset of experts per node, substantially lowering the memory footprint. Each node updates its local experts and periodically synchronizes with other nodes, eliminating full-parameter transmission while ensuring efficient knowledge sharing. To accelerate convergence, we introduce an expert-merging warm-up strategy, where experts exchange knowledge early in training, to rapidly establish foundational capabilities. With SPES, we train 2B-parameter MoE LLM using 16 standalone 48GB GPUs over internet connections, which achieves competitive performance with centrally trained LLMs under similar computational budgets. We further demonstrate scalability by training 7B model from scratch and 9B model upcycled from dense checkpoint, both of which match prior centralized baselines. Our code is available at https://github.com/zjr2000/SPES. 6 2 0 2 2 1 ] . [ 1 3 4 5 1 1 . 2 0 6 2 : r 1. Introduction Large language models (LLMs) (Achiam et al., 2023; Grattafiori et al., 2024; Yang et al., 2025; Liu et al., 2024; Muennighoff et al., 2024) have shown strong generalization capabilities across various downstream tasks, establishing 1Department of Computing, The Hong Kong Polytechnic University. 2OPPO Research Institute. Correspondence to: Lei Zhang <cslzhang@comp.polyu.edu.hk>. Preprint. Febru"
[13.02.2026 05:56] Response: ```python
[
    "Department of Computing, The Hong Kong Polytechnic University",
    "OPPO Research Institute"
]
```
[13.02.2026 05:56] Deleting PDF ./assets/pdf/2602.11543.pdf.
[13.02.2026 05:56] Success.
[13.02.2026 05:56] Downloading and parsing paper https://huggingface.co/papers/2602.11509.
[13.02.2026 05:56] Downloading paper 2602.11509 from https://arxiv.org/pdf/2602.11509v1...
[13.02.2026 05:56] Extracting affiliations from text.
[13.02.2026 05:56] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Multimodal Fact-Level Attribution for Verifiable Reasoning David Wan * 1 Han Wang * 1 Ziyang Wang 1 Elias Stengel-Eskin 2 Hyunji Lee 1 Mohit Bansal "
[13.02.2026 05:56] Response: ```python
[]
```
[13.02.2026 05:56] Extracting affiliations from text.
[13.02.2026 05:56] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Multimodal Fact-Level Attribution for Verifiable Reasoning David Wan * 1 Han Wang * 1 Ziyang Wang 1 Elias Stengel-Eskin 2 Hyunji Lee 1 Mohit BansalMultimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observationbased scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning. We introduce MURGAT (Multimodal Reasoning with Grounded Attribution), benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MURGAT requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting significant gap between internal reasoning and verifiable attribution. Code and data are available at https: //github.com/meetdavidwan/murgat. 6 2 0 2 2 1 ] . [ 1 9 0 5 1 1 . 2 0 6 2 : r 1. Introduction Reliable and trustworthy real-world deployment of multimodal large language models (MLLMs) requires outputs that are verifiable and grounded in models input sources. *Equal contribution 1UNC Chapel Hill 2The University of Texas at Austin. Correspondence to: David Wan <davidwan@cs.unc.edu>. Preprint. February 13, 2026. 1 This grounding is particularly important when problems require multi-step reasoning, which amplifies the risk of hallucinations from propagating errors (Ji et al., 2023; Min et al., 2023), and when producing long-form responses which are harder and more time-consuming to verify (Song et al., 2025; Li et al., 2022). While prior work in temporal video grounding (Hendricks et al., 2017; Lei et al., 2021) and multimodal retrieval-augmented generation (Dong et al., 2025; Yu et al., 2025; Chen et al., 2022) has explored grounding multimodal models outputs to their inputs using citations or timestamps, existing studies often focus on simplified settings. Many grounding tasks emphasize observational or retrieval-based grounding, where questions can be answered by directly grounding to relevant evidence in the input source (e.g., How many flags are in front of the U.S. Capitol dome? given an image of the Capitol). In contrast, real-world questions frequently require not only grounding to evidence, but also reasoning over grounded information to synthesize an answer (e.g., the question in Figure 1). Moreover, prior work is typically limited to narrow set of modalities, most commonly visual information. Even in video grounding settings (Hendricks et al., 2017; Wang et al., 2025a; Lei et al., 2019; 2021), existing methods mostly ground to visual inputs or rely on automatically-transcribed text rather than original audio, overlooking modalities such as audio and figures and failing to evaluate joint grounding across heterogeneous multimodal sources. To evaluate MLLMs in more realistic settings requiring reasoning grounded in heterogeneous multimodal inputs, we introduce Multimodal Reasoning with Grounded Attribution (MURGAT). We measure different models ability to perform fact-level multimodal attribution in settings that require reasoning beyond direct observation. As shown in Figure 1 (top), given multimodal inputs including video, audio, and graphs, models should generate answers with explicit reasoning and precise citations that refer to the specific modality and temporal segments supporting each claim. To assess models ability to identify and attribute supporting evidence, we decompose response evaluation into three subtasks (Figure 1, bottom). (1) Verifiable claim identification identifies sentences that contain directly observable claims requiring grounding, as opposed to sentences that reflect reasoning steps. This allows attribution quality to be evaluated only over verifiable claims without penalizing unMultimodal Fact-Level Attribution for Verifiable Reasoning Figure 1. Overview of MURGAT and the evaluation protocol. The model is given question and multimodal sources and is asked to generate response containing explicit reasoning and precise citations, including the specific modality and timestamp. To evaluate the response, we apply fact-level multimodal attribution protocol. The generated response and its citations are processed through three subtasks: (1) verifiable claim identification, (2) atomic fact decomposition, and (3) attribution quality. grounded reasoning or rewarding unnecessary citations. (2) Atomic fact decomposition further breaks each verifiable sentence into atomic facts, enabling fine-grained evaluation, as single sentence often contains multiple claims (Min et al., 2023). (3) Attribution quality evaluates whether each atomic fact is entailed by the multimodal evidence cited for it. Following text attribution work (Gao et al., 2023b), we measure recall (whether the union of cited segments fully entails the fact) and precision (whether each cited segment is strictly necessary) while accounting for both temporal alignment and modality. To establish reliable reference points for these tasks, we first collect human annotations for all three subtasks of the evaluation pipeline on two datasets, WorldSense (Hong et al., 2025) and Video-MMMU (Hu et al., 2025b), which cover diverse range of multimodal inputs and question types, including those requiring reasoning beyond direct observation. Using these annotations as ground truth, we evaluate range of MLLM variants (e.g., Gemini-2.5-Flash (Comanici et al., 2025), Gemini-3-Flash and Pro (Google, 2025), and Qwen-3-Omni-Instruct and Thinking (Xu et al., 2025)). We observe that even strong MLLMs perform poorly on the MURGAT task  (Table 1)  : while they are often able to answer questions correctly, they frequently fail to provide sufficient and accurate attribution to the underlying sources. These findings motivate th"
[13.02.2026 05:56] Mistral response. {"id": "a45fdfd83a60438986872c77b4325056", "created": 1770962172, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1440, "total_tokens": 1458, "completion_tokens": 18, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"UNC Chapel Hill\", \"The University of Texas at Austin\"]\n```"}}]}
[13.02.2026 05:56] Response: ```python
["UNC Chapel Hill", "The University of Texas at Austin"]
```
[13.02.2026 05:56] Deleting PDF ./assets/pdf/2602.11509.pdf.
[13.02.2026 05:56] Success.
[13.02.2026 05:56] Downloading and parsing paper https://huggingface.co/papers/2602.08277.
[13.02.2026 05:56] Downloading paper 2602.08277 from https://arxiv.org/pdf/2602.08277v1...
[13.02.2026 05:56] Extracting affiliations from text.
[13.02.2026 05:56] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 ] . [ 1 7 7 2 8 0 . 2 0 6 2 : r PISCO: Precise Video Instance Insertion with Sparse Control Xiangbo Gao1, Renjie Li1, Xinghao Chen1, Yuheng Wu3, Suofei Feng4, Qing Yin2, Zhengzhong Tu1 1Texas A&M University 2Visko Platform 3KAIST 4Stanford University Abstract. The landscape of AI video generation is undergoing pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, the core requirement is the ability to perform precise, targeted modifications. cornerstone of this transition is video instance insertion, which requires inserting specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction (e.g., shadows and reflections), and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify single keyframe, start-andend keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms stron"
[13.02.2026 05:56] Response: ```python
[
    "Texas A&M University",
    "Visko Platform",
    "KAIST",
    "Stanford University"
]
```
[13.02.2026 05:56] Deleting PDF ./assets/pdf/2602.08277.pdf.
[13.02.2026 05:56] Success.
[13.02.2026 05:56] Downloading and parsing paper https://huggingface.co/papers/2602.11598.
[13.02.2026 05:56] Extra JSON file exists (./assets/json/2602.11598.json), skip PDF parsing.
[13.02.2026 05:56] Paper image links file exists (./assets/img_data/2602.11598.json), skip HTML parsing.
[13.02.2026 05:56] Success.
[13.02.2026 05:56] Downloading and parsing paper https://huggingface.co/papers/2602.10585.
[13.02.2026 05:56] Extra JSON file exists (./assets/json/2602.10585.json), skip PDF parsing.
[13.02.2026 05:56] Paper image links file exists (./assets/img_data/2602.10585.json), skip HTML parsing.
[13.02.2026 05:56] Success.
[13.02.2026 05:56] Downloading and parsing paper https://huggingface.co/papers/2602.09070.
[13.02.2026 05:56] Downloading paper 2602.09070 from https://arxiv.org/pdf/2602.09070v2...
[13.02.2026 05:56] Extracting affiliations from text.
[13.02.2026 05:56] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control Zhaocheng Liu ByteDance Beijing, China lio.h.zen@gmail.com Yufan Wen Tsinghua University Shenzhen, China wenyf24@mails.tsinghua.edu.cn YeGuo Hua ByteDance Beijing, China huayeguo@bytedance.com 6 2 0 2 2 1 ] . [ 2 0 7 0 9 0 . 2 0 6 2 : r Ziyi Guo ByteDance Shenzhen, China ziyi.94@bytedance.com Lihua Zhang ByteDance Beijing, China lizhiyu.0@bytedance.com Jian Wu ByteDance Beijing, China wujian@bytedance.com Chun Yuan Tsinghua University Shenzhen, China yuanc@sz.tsinghua.edu.cn Abstract Synthesizing coherent soundtracks for long-form videos remains formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, hierarchical framework predicated on the core insight that emotion serves as high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs Dual-Branch Injection strategy to reconcile global structure with local dynamism: Global Semantic Anchor ensures stylistic stability, while surgical Token-Level Affective Adapter modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing fully autonomous paradigm for long-video soundtrack generation. CCS Concepts Applied computing Sound and music computing. Keywords Video-to-Music Generation, Long-form Video Soundtrack Ge"
[13.02.2026 05:56] Response: ```python
[
    "ByteDance Beijing, China",
    "Tsinghua University Shenzhen, China",
    "ByteDance Shenzhen, China"
]
```
[13.02.2026 05:56] Deleting PDF ./assets/pdf/2602.09070.pdf.
[13.02.2026 05:56] Success.
[13.02.2026 05:56] Enriching papers with extra data.
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 0. Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution.  					AI-generated summary 				 The emergence of multi-agent systems built from large language models (LLMs)...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 1. On-policy distillation is extended through a generalized framework that introduces flexible reference models and reward scaling factors, demonstrating improved performance through reward extrapolation and reward correction techniques.  					AI-generated summary 				 On-policy distillation (OPD), whi...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 2. A vision-language-action model enhanced with world model-based reinforcement learning demonstrates improved performance and long-horizon execution capabilities for robotic manipulation tasks.  					AI-generated summary 				 Vision-language-action (VLA) models that directly predict multi-step action ...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 3. Progressive Semantic Illusions use a generative framework with dual-branch Score Distillation Sampling to create vector sketches that transform semantically through sequential stroke additions, achieving superior recognizability and illusion strength.  					AI-generated summary 				 Visual illusions...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 4. LawThinker is an autonomous legal research agent that uses an Explore-Verify-Memorize strategy with a DeepVerifier module to ensure accurate and procedurally compliant legal reasoning through dynamic verification of intermediate steps.  					AI-generated summary 				 Legal reasoning requires not onl...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 5. Asymmetric Group Relative Advantage Estimation addresses exploration and difficulty adaptation challenges in reinforcement learning with large language models by dynamically modulating exploration incentives and sample difficulty focus.  					AI-generated summary 				 Reinforcement Learning with Ver...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 6. A fully end-to-end Transformer-based audio tokenizer architecture achieves high-fidelity reconstruction across diverse audio domains and enables superior text-to-speech and automatic speech recognition performance.  					AI-generated summary 				 Discrete audio tokenizers are fundamental to empoweri...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 7. Diffusion large language models enable parallel token generation and efficient reasoning enhancement through a voting technique that identifies and refines uncertain predictions across multiple samples.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) represent a new paradigm ...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 8. Budget-constrained tool-augmented agents use a hierarchical world model and intent-aware planning to optimize multi-step task completion under monetary constraints.  					AI-generated summary 				 We study budget-constrained tool-augmented agents, where a large language model must solve multi-step t...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 9. MolmoSpaces presents an open ecosystem with diverse indoor environments and annotated objects for large-scale robot policy benchmarking across multiple tasks and simulators.  					AI-generated summary 				 Deploying robots at scale demands robustness to the long tail of everyday situations. The coun...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 10. Foundation models generate executable environment code to scaffold learning progress in open-ended worlds, enabling agents to acquire long-horizon skills through curriculum control.  					AI-generated summary 				 Open-ended learning frames intelligence as emerging from continual interaction with an...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 11. Sci-CoE is a two-stage scientific co-evolving framework that enables large language models to self-evolve as both solver and verifier through sparse-to-unsupervised learning transitions, improving scientific reasoning capabilities and evaluation system robustness.  					AI-generated summary 				 Lar...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 12. Gaia2 presents a benchmark for evaluating large language model agents in asynchronous, dynamic environments with temporal constraints and multi-agent collaboration, featuring a write-action verifier for reinforcement learning and revealing trade-offs between reasoning, efficiency, and robustness.  	...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 13. ThinkRouter is a confidence-aware routing mechanism that improves reasoning efficiency by switching between discrete token and latent spaces based on model confidence, achieving better accuracy and faster generation.  					AI-generated summary 				 Recent work explores latent reasoning to improve re...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 14. Voxtral Realtime is a streaming speech recognition model trained end-to-end for sub-second latency with performance matching offline systems.  					AI-generated summary 				 We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription qual...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 15. A trajectory self-distillation framework with direct discriminative optimization improves few-step decoding efficiency in diffusion large language models while maintaining generation quality.  					AI-generated summary 				 Diffusion large language models (DLLMs) have the potential to enable fast te...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 16. MiniCPM-SALA combines sparse and linear attention mechanisms in a hybrid architecture to enable efficient processing of ultra-long contexts while maintaining model performance and reducing training costs.  					AI-generated summary 				 The evolution of large language models (LLMs) towards applicati...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 17. ScalSelect is a scalable training-free method for selecting representative multimodal data that achieves near-full-dataset performance with significantly reduced computational requirements.  					AI-generated summary 				 Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for adva...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 18. A memory-efficient decentralized framework for training mixture-of-experts language models using sparse expert synchronization and expert-merging warm-up strategies.  					AI-generated summary 				 Pretraining large language models (LLMs) typically requires centralized clusters with thousands of hig...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 19. MuRGAt is a benchmark for evaluating fact-level multimodal attribution in complex reasoning tasks, requiring models to provide precise citations for their answers across video, audio, and other modalities.  					AI-generated summary 				 Multimodal large language models (MLLMs) are increasingly used...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 20. Video diffusion model PISCO enables precise instance insertion with sparse keyframe control through variable-information guidance and distribution-preserving temporal masking.  					AI-generated summary 				 The landscape of AI video generation is undergoing a pivotal shift: moving beyond general ge...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 21. A unified Vision-Language-Action model with a hierarchical architecture combining semantic reasoning and continuous trajectory generation achieves state-of-the-art performance across multiple embodied navigation tasks.  					AI-generated summary 				 Embodied navigation has long been fragmented by t...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 22. Neural Additive Experts combines multiple specialized networks with a dynamic gating mechanism to balance predictive accuracy and feature interpretability in machine learning models.  					AI-generated summary 				 The trade-off between interpretability and accuracy remains a core challenge in machi...
[13.02.2026 05:56] ********************************************************************************
[13.02.2026 05:56] Abstract 23. NarraScore presents a hierarchical framework that uses frozen Vision-Language Models as affective sensors to generate coherent soundtracks for long-form videos by combining global semantic anchors with token-level adaptive modulation.  					AI-generated summary 				 Synthesizing coherent soundtracks...
[13.02.2026 05:56] Read previous papers.
[13.02.2026 05:56] Generating reviews via LLM API.
[13.02.2026 05:56] Using data from previous issue: {"categories": ["#alignment", "#training", "#agents", "#security"], "emoji": "", "ru": {"title": "        ", "desc": "   ,      LLM     
[13.02.2026 05:56] Using data from previous issue: {"categories": ["#alignment", "#training", "#rl", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": " :       ", "desc": "      (on-policy distillation),  
[13.02.2026 05:56] Using data from previous issue: {"categories": ["#robotics", "#rl", "#cv", "#benchmark", "#multimodal"], "emoji": "", "ru": {"title": "      ", "desc": "   GigaBrain-0.5M*,  Vision-Language-Action,         
[13.02.2026 05:56] Querying the API.
[13.02.2026 05:56] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Progressive Semantic Illusions use a generative framework with dual-branch Score Distillation Sampling to create vector sketches that transform semantically through sequential stroke additions, achieving superior recognizability and illusion strength.  					AI-generated summary 				 Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the "dual-constraint": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a "common structural subspace" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/
[13.02.2026 05:56] Response: ```json
{
  "desc": "          ,        .      Stroke of Surprise,    Score Distillation Sampling (SDS)         .            ,     .    Overlay Loss    ,         .",
  "emoji": "",
  "title": "   :      "
}
```
[13.02.2026 05:56] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Progressive Semantic Illusions use a generative framework with dual-branch Score Distillation Sampling to create vector sketches that transform semantically through sequential stroke additions, achieving superior recognizability and illusion strength.  					AI-generated summary 				 Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the "dual-constraint": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a "common structural subspace" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/"

[13.02.2026 05:56] Response: ```python
["CV", "MULTIMODAL"]
```
[13.02.2026 05:56] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Progressive Semantic Illusions use a generative framework with dual-branch Score Distillation Sampling to create vector sketches that transform semantically through sequential stroke additions, achieving superior recognizability and illusion strength.  					AI-generated summary 				 Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the "dual-constraint": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a "common structural subspace" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/"

[13.02.2026 05:56] Response: ```python
['DIFFUSION']
```
[13.02.2026 05:56] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents Progressive Semantic Illusions, a new approach to vector sketching that allows a single sketch to change its meaning through the addition of strokes. The authors introduce a generative framework called Stroke of Surprise, which uses dual-branch Score Distillation Sampling to optimize the strokes for different semantic interpretations. A key challenge is to ensure that the initial strokes represent one object while also allowing for a transformation into another object with subsequent strokes. The proposed method includes a novel Overlay Loss to maintain structural integrity, leading to improved recognizability and illusion strength compared to existing methods.","title":"Transforming Sketches: From One Meaning to Another!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents Progressive Semantic Illusions, a new approach to vector sketching that allows a single sketch to change its meaning through the addition of strokes. The authors introduce a generative framework called Stroke of Surprise, which uses dual-branch Score Distillation Sampling to optimize the strokes for different semantic interpretations. A key challenge is to ensure that the initial strokes represent one object while also allowing for a transformation into another object with subsequent strokes. The proposed method includes a novel Overlay Loss to maintain structural integrity, leading to improved recognizability and illusion strength compared to existing methods.', title='Transforming Sketches: From One Meaning to Another!'))
[13.02.2026 05:56] Response: ParsedChatCompletionMessage[Article](content='{"desc":"","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='', title=''))
[13.02.2026 05:56] Using data from previous issue: {"categories": [], "emoji": "", "ru": {"title": "       ", "desc": "LawThinker       ,   Explore-Verify-Memorize   DeepVerifier    
[13.02.2026 05:56] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#optimization", "#reasoning"], "emoji": "", "ru": {"title": " :   exploration     LLM", "desc": "    Asymmetric GRAE       
[13.02.2026 05:56] Querying the API.
[13.02.2026 05:56] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A fully end-to-end Transformer-based audio tokenizer architecture achieves high-fidelity reconstruction across diverse audio domains and enables superior text-to-speech and automatic speech recognition performance.  					AI-generated summary 				 Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models.
[13.02.2026 05:56] Response: ```json
{
  "desc": "   CAT (Causal Audio Tokenizer with Transformer)   -     ,     ,      .   CAT  MOSS-Audio-Tokenizer  1.6  ,   3    ,       .           ,   ,            .   ,   -                .",
  "emoji": "",
  "title": "        -"
}
```
[13.02.2026 05:56] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A fully end-to-end Transformer-based audio tokenizer architecture achieves high-fidelity reconstruction across diverse audio domains and enables superior text-to-speech and automatic speech recognition performance.  					AI-generated summary 				 Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models."

[13.02.2026 05:56] Response: ```python
['AUDIO', 'ARCHITECTURE', 'TRAINING']
```
[13.02.2026 05:56] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A fully end-to-end Transformer-based audio tokenizer architecture achieves high-fidelity reconstruction across diverse audio domains and enables superior text-to-speech and automatic speech recognition performance.  					AI-generated summary 				 Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models."

[13.02.2026 05:56] Response: ```python
['OPTIMIZATION']
```
[13.02.2026 05:56] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new audio tokenizer architecture called MOSS-Audio-Tokenizer, which is based entirely on Transformers and designed for high-fidelity audio reconstruction. Unlike previous methods that depend on fixed architectures or pretrained models, this approach learns the entire process end-to-end, optimizing the encoder, quantizer, and decoder together. The model, with 1.6 billion parameters, is trained on a vast dataset of diverse audio, allowing it to perform exceptionally well in text-to-speech (TTS) and automatic speech recognition (ASR) tasks. The results demonstrate that this architecture not only scales effectively but also consistently outperforms existing audio codecs across various audio types and bitrates.","title":"Transforming Audio: High-Fidelity Tokenization with Transformers"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new audio tokenizer architecture called MOSS-Audio-Tokenizer, which is based entirely on Transformers and designed for high-fidelity audio reconstruction. Unlike previous methods that depend on fixed architectures or pretrained models, this approach learns the entire process end-to-end, optimizing the encoder, quantizer, and decoder together. The model, with 1.6 billion parameters, is trained on a vast dataset of diverse audio, allowing it to perform exceptionally well in text-to-speech (TTS) and automatic speech recognition (ASR) tasks. The results demonstrate that this architecture not only scales effectively but also consistently outperforms existing audio codecs across various audio types and bitrates.', title='Transforming Audio: High-Fidelity Tokenization with Transformers'))
[13.02.2026 05:56] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TransformerTTSASRCATCATMOSS-Audio-Tokenizer16300","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TransformerTTSASRCATCATMOSS-Audio-Tokenizer16300', title=''))
[13.02.2026 05:56] Using data from previous issue: {"categories": ["#reasoning", "#diffusion", "#open_source"], "emoji": "", "ru": {"title": "       ", "desc": "      (dLLM),        
[13.02.2026 05:56] Using data from previous issue: {"categories": [], "emoji": "", "ru": {"title": "     ", "desc": "    INTENT        ,           
[13.02.2026 05:56] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#open_source", "#benchmark", "#multimodal"], "emoji": "", "ru": {"title": "       ", "desc": "MolmoSpaces       230  
[13.02.2026 05:56] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#training", "#agents"], "emoji": "", "ru": {"title": "        ", "desc": "  Dreaming in Code (DiCode)  ,        
[13.02.2026 05:56] Querying the API.
[13.02.2026 05:56] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Sci-CoE is a two-stage scientific co-evolving framework that enables large language models to self-evolve as both solver and verifier through sparse-to-unsupervised learning transitions, improving scientific reasoning capabilities and evaluation system robustness.  					AI-generated summary 				 Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.
[13.02.2026 05:56] Response: ```json
{
  "desc": "Sci-CoE      -     .            ,              .         ,       .  ,  Sci-CoE           .",
  "emoji": "",
  "title": "    :       "
}
```
[13.02.2026 05:56] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sci-CoE is a two-stage scientific co-evolving framework that enables large language models to self-evolve as both solver and verifier through sparse-to-unsupervised learning transitions, improving scientific reasoning capabilities and evaluation system robustness.  					AI-generated summary 				 Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE."

[13.02.2026 05:56] Response: ```python
["TRAINING", "BENCHMARK", "MATH"]
```
[13.02.2026 05:56] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Sci-CoE is a two-stage scientific co-evolving framework that enables large language models to self-evolve as both solver and verifier through sparse-to-unsupervised learning transitions, improving scientific reasoning capabilities and evaluation system robustness.  					AI-generated summary 				 Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE."

[13.02.2026 05:56] Response: ```python
["REASONING", "SCIENCE", "OPEN_SOURCE"]
```
[13.02.2026 05:56] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Sci-CoE is a novel framework designed to enhance the reasoning abilities of large language models (LLMs) in scientific tasks. It operates in two stages: first, it uses a small amount of labeled data to train a Verifier that can assess the correctness of solutions. In the second stage, it employs a geometric reward system that encourages the model to improve through self-iteration on unlabeled data, focusing on consensus, reliability, and diversity. This approach not only strengthens the model\'s reasoning capabilities but also builds a more robust evaluation system for scientific reasoning tasks.","title":"Empowering Language Models to Self-Evolve in Scientific Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="Sci-CoE is a novel framework designed to enhance the reasoning abilities of large language models (LLMs) in scientific tasks. It operates in two stages: first, it uses a small amount of labeled data to train a Verifier that can assess the correctness of solutions. In the second stage, it employs a geometric reward system that encourages the model to improve through self-iteration on unlabeled data, focusing on consensus, reliability, and diversity. This approach not only strengthens the model's reasoning capabilities but also builds a more robust evaluation system for scientific reasoning tasks.", title='Empowering Language Models to Self-Evolve in Scientific Reasoning'))
[13.02.2026 05:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Sci-CoESci-CoE","title":"Sci-CoE"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Sci-CoESci-CoE', title='Sci-CoE'))
[13.02.2026 05:57] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#rl", "#benchmark", "#optimization", "#agents", "#reasoning"], "emoji": "", "ru": {"title": "    :   ,   ", "desc": "  Gaia2      
[13.02.2026 05:57] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "", "ru": {"title": " :        ", "desc": "ThinkRouter    ,    ,     
[13.02.2026 05:57] Using data from previous issue: {"categories": ["#audio", "#open_source", "#low_resource", "#training", "#architecture", "#multilingual"], "emoji": "", "ru": {"title": "      ", "desc": "  Voxtral Realtime     ,  end-to-en
[13.02.2026 05:57] Querying the API.
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A trajectory self-distillation framework with direct discriminative optimization improves few-step decoding efficiency in diffusion large language models while maintaining generation quality.  					AI-generated summary 				 Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.
[13.02.2026 05:57] Response: ```json
{
  "desc": "            .   Direct Discriminative Optimization  reverse-KL  ,         .        ,        .            .",
  "emoji": "",
  "title": "       "
}
```
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A trajectory self-distillation framework with direct discriminative optimization improves few-step decoding efficiency in diffusion large language models while maintaining generation quality.  					AI-generated summary 				 Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D."

[13.02.2026 05:57] Response: ```python
["INFERENCE", "TRAINING", "ARCHITECTURE"]
```

**Justification:**

- **INFERENCE**: The paper explicitly focuses on optimizing model deployment and decoding efficiency, specifically addressing "few-step decoding efficiency" and inference constraints in diffusion language models.

- **TRAINING**: The paper proposes a trajectory self-distillation framework and incorporates Direct Discriminative Optimization (DDO), which are training/fine-tuning methodologies to improve model performance.

- **ARCHITECTURE**: The paper discusses diffusion large language models (DLLMs) as a specific architectural approach to language modeling that enables parallel token decoding.
[13.02.2026 05:57] Error. Failed to parse JSON from LLM. ["INFERENCE", "TRAINING", "ARCHITECTURE"]


**Justification:**

- **INFERENCE**: The paper explicitly focuses on optimizing model deployment and decoding efficiency, specifically addressing "few-step decoding efficiency" and inference constraints in diffusion language models.

- **TRAINING**: The paper proposes a trajectory self-distillation framework and incorporates Direct Discriminative Optimization (DDO), which are training/fine-tuning methodologies to improve model performance.

- **ARCHITECTURE**: The paper discusses diffusion large language models (DLLMs) as a specific architectural approach to language modeling that enables parallel token decoding.
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A trajectory self-distillation framework with direct discriminative optimization improves few-step decoding efficiency in diffusion large language models while maintaining generation quality.  					AI-generated summary 				 Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D."

[13.02.2026 05:57] Response: ```python
['DIFFUSION', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[13.02.2026 05:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a trajectory self-distillation framework aimed at enhancing the efficiency of few-step decoding in diffusion large language models (DLLMs). By utilizing Direct Discriminative Optimization (DDO), the framework distills the model\'s own generative paths, allowing it to focus on the most probable outputs. This method significantly improves the balance between decoding speed and generation quality, outperforming existing few-step approaches. While full-step decoding is still more effective, this work narrows the performance gap, paving the way for more practical applications of DLLMs.","title":"Enhancing Few-Step Decoding in DLLMs with Self-Distillation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces a trajectory self-distillation framework aimed at enhancing the efficiency of few-step decoding in diffusion large language models (DLLMs). By utilizing Direct Discriminative Optimization (DDO), the framework distills the model's own generative paths, allowing it to focus on the most probable outputs. This method significantly improves the balance between decoding speed and generation quality, outperforming existing few-step approaches. While full-step decoding is still more effective, this work narrows the performance gap, paving the way for more practical applications of DLLMs.", title='Enhancing Few-Step Decoding in DLLMs with Self-Distillation'))
[13.02.2026 05:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DLLMsDDODLLMs","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DLLMsDDODLLMs', title=''))
[13.02.2026 05:57] Using data from previous issue: {"categories": ["#small_models", "#training", "#architecture", "#long_context", "#optimization", "#inference"], "emoji": "", "ru": {"title": "   -      ", "desc": "MiniCPM-SALA   ,  
[13.02.2026 05:57] Using data from previous issue: {"categories": ["#multimodal", "#training", "#data"], "emoji": "", "ru": {"title": "         ", "desc": "ScalSelect           -   
[13.02.2026 05:57] Querying the API.
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A memory-efficient decentralized framework for training mixture-of-experts language models using sparse expert synchronization and expert-merging warm-up strategies.  					AI-generated summary 				 Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100). Recent decentralized training methods reduce communication overhead by employing federated optimization; however, they still need to train the entire model on each node, remaining constrained by GPU memory limitations. In this work, we propose SParse Expert Synchronization (SPES), a memory-efficient decentralized framework for pretraining mixture-of-experts (MoE) LLMs. SPES trains only a subset of experts per node, substantially lowering the memory footprint. Each node updates its local experts and periodically synchronizes with other nodes, eliminating full-parameter transmission while ensuring efficient knowledge sharing. To accelerate convergence, we introduce an expert-merging warm-up strategy, where experts exchange knowledge early in training, to rapidly establish foundational capabilities. With SPES, we train a 2B-parameter MoE LLM using 16 standalone 48GB GPUs over internet connections, which achieves competitive performance with centrally trained LLMs under similar computational budgets. We further demonstrate scalability by training a 7B model from scratch and a 9B model upcycled from a dense checkpoint, both of which match prior centralized baselines. Our code is available at https://github.com/zjr2000/SPES.
[13.02.2026 05:57] Response: ```json
{
  "desc": "   SPES          mixture-of-experts,       GPU        .            ,          .    expert-merging warm-up,              .    MoE LLM  2    16 GPU   48GB  -,  ,        .",
  "emoji": "",
  "title": "      "
}
```
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A memory-efficient decentralized framework for training mixture-of-experts language models using sparse expert synchronization and expert-merging warm-up strategies.  					AI-generated summary 				 Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100). Recent decentralized training methods reduce communication overhead by employing federated optimization; however, they still need to train the entire model on each node, remaining constrained by GPU memory limitations. In this work, we propose SParse Expert Synchronization (SPES), a memory-efficient decentralized framework for pretraining mixture-of-experts (MoE) LLMs. SPES trains only a subset of experts per node, substantially lowering the memory footprint. Each node updates its local experts and periodically synchronizes with other nodes, eliminating full-parameter transmission while ensuring efficient knowledge sharing. To accelerate convergence, we introduce an expert-merging warm-up strategy, where experts exchange knowledge early in training, to rapidly establish foundational capabilities. With SPES, we train a 2B-parameter MoE LLM using 16 standalone 48GB GPUs over internet connections, which achieves competitive performance with centrally trained LLMs under similar computational budgets. We further demonstrate scalability by training a 7B model from scratch and a 9B model upcycled from a dense checkpoint, both of which match prior centralized baselines. Our code is available at https://github.com/zjr2000/SPES."

[13.02.2026 05:57] Response: ```python
["TRAINING", "ARCHITECTURE", "SMALL_MODELS"]
```

**Justification:**

- **TRAINING**: The paper focuses on improving model training methods, specifically proposing SPES (SParse Expert Synchronization) as a memory-efficient decentralized framework for pretraining LLMs with novel strategies like expert-merging warm-up.

- **ARCHITECTURE**: The paper proposes novel architectural components and approaches for mixture-of-experts (MoE) language models, including sparse expert synchronization mechanisms.

- **SMALL_MODELS**: The paper demonstrates training models with 2B, 7B, and 9B parameters, which fall within the scope of smaller models being optimized for resource-constrained settings (using 16 GPUs with 48GB memory each rather than thousands of high-memory GPUs).
[13.02.2026 05:57] Error. Failed to parse JSON from LLM. ["TRAINING", "ARCHITECTURE", "SMALL_MODELS"]


**Justification:**

- **TRAINING**: The paper focuses on improving model training methods, specifically proposing SPES (SParse Expert Synchronization) as a memory-efficient decentralized framework for pretraining LLMs with novel strategies like expert-merging warm-up.

- **ARCHITECTURE**: The paper proposes novel architectural components and approaches for mixture-of-experts (MoE) language models, including sparse expert synchronization mechanisms.

- **SMALL_MODELS**: The paper demonstrates training models with 2B, 7B, and 9B parameters, which fall within the scope of smaller models being optimized for resource-constrained settings (using 16 GPUs with 48GB memory each rather than thousands of high-memory GPUs).
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A memory-efficient decentralized framework for training mixture-of-experts language models using sparse expert synchronization and expert-merging warm-up strategies.  					AI-generated summary 				 Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100). Recent decentralized training methods reduce communication overhead by employing federated optimization; however, they still need to train the entire model on each node, remaining constrained by GPU memory limitations. In this work, we propose SParse Expert Synchronization (SPES), a memory-efficient decentralized framework for pretraining mixture-of-experts (MoE) LLMs. SPES trains only a subset of experts per node, substantially lowering the memory footprint. Each node updates its local experts and periodically synchronizes with other nodes, eliminating full-parameter transmission while ensuring efficient knowledge sharing. To accelerate convergence, we introduce an expert-merging warm-up strategy, where experts exchange knowledge early in training, to rapidly establish foundational capabilities. With SPES, we train a 2B-parameter MoE LLM using 16 standalone 48GB GPUs over internet connections, which achieves competitive performance with centrally trained LLMs under similar computational budgets. We further demonstrate scalability by training a 7B model from scratch and a 9B model upcycled from a dense checkpoint, both of which match prior centralized baselines. Our code is available at https://github.com/zjr2000/SPES."

[13.02.2026 05:57] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```

**Justification:**

- **OPTIMIZATION**: The paper focuses on advancing training optimization methods through a memory-efficient decentralized framework (SPES) with sparse expert synchronization and expert-merging warm-up strategies to improve convergence and reduce memory footprint during LLM pretraining.

- **OPEN_SOURCE**: The paper explicitly states "Our code is available at https://github.com/zjr2000/SPES," indicating the authors are releasing their framework and code to the public.
[13.02.2026 05:57] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

- **OPTIMIZATION**: The paper focuses on advancing training optimization methods through a memory-efficient decentralized framework (SPES) with sparse expert synchronization and expert-merging warm-up strategies to improve convergence and reduce memory footprint during LLM pretraining.

- **OPEN_SOURCE**: The paper explicitly states "Our code is available at https://github.com/zjr2000/SPES," indicating the authors are releasing their framework and code to the public.
[13.02.2026 05:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new decentralized framework called SParse Expert Synchronization (SPES) for training mixture-of-experts (MoE) language models in a memory-efficient way. Instead of requiring high-memory GPUs to train the entire model, SPES allows each node to train only a subset of experts, significantly reducing memory usage. The framework also incorporates a novel expert-merging warm-up strategy to enhance knowledge sharing among experts early in the training process. As a result, SPES enables the training of large language models with competitive performance using fewer resources and demonstrates scalability for even larger models.","title":"Efficient Decentralized Training for Large Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new decentralized framework called SParse Expert Synchronization (SPES) for training mixture-of-experts (MoE) language models in a memory-efficient way. Instead of requiring high-memory GPUs to train the entire model, SPES allows each node to train only a subset of experts, significantly reducing memory usage. The framework also incorporates a novel expert-merging warm-up strategy to enhance knowledge sharing among experts early in the training process. As a result, SPES enables the training of large language models with competitive performance using fewer resources and demonstrates scalability for even larger models.', title='Efficient Decentralized Training for Large Language Models'))
[13.02.2026 05:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SPESMoESPES1648GB GPU2BMoE LLM","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SPESMoESPES1648GB GPU2BMoE LLM', title=''))
[13.02.2026 05:57] Querying the API.
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MuRGAt is a benchmark for evaluating fact-level multimodal attribution in complex reasoning tasks, requiring models to provide precise citations for their answers across video, audio, and other modalities.  					AI-generated summary 				 Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning. We introduce MuRGAt (Multimodal Reasoning with Grounded Attribution), a benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MuRGAt requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe a key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting a significant gap between internal reasoning and verifiable attribution.
[13.02.2026 05:57] Response: ```json
{
  "desc": "MuRGAt           (MLLM)            ,    .  ,                ,   .      ,             .    :  MLLM     ,         .",
  "emoji": "",
  "title": "      "
}
```
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MuRGAt is a benchmark for evaluating fact-level multimodal attribution in complex reasoning tasks, requiring models to provide precise citations for their answers across video, audio, and other modalities.  					AI-generated summary 				 Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning. We introduce MuRGAt (Multimodal Reasoning with Grounded Attribution), a benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MuRGAt requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe a key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting a significant gap between internal reasoning and verifiable attribution."

[13.02.2026 05:57] Response: ```python
['BENCHMARK', 'MULTIMODAL', 'VIDEO', 'AUDIO']
```
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MuRGAt is a benchmark for evaluating fact-level multimodal attribution in complex reasoning tasks, requiring models to provide precise citations for their answers across video, audio, and other modalities.  					AI-generated summary 				 Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning. We introduce MuRGAt (Multimodal Reasoning with Grounded Attribution), a benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MuRGAt requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe a key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting a significant gap between internal reasoning and verifiable attribution."

[13.02.2026 05:57] Response: ```python
['HALLUCINATIONS', 'INTERPRETABILITY', 'LONG_CONTEXT']
```
[13.02.2026 05:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MuRGAt is a new benchmark designed to evaluate how well models can attribute facts in complex reasoning tasks using multiple types of data, like video and audio. It challenges models to not only provide answers but also to cite their sources accurately, specifying the type of data and the exact time segments used. The benchmark reveals that even advanced multimodal large language models (MLLMs) often make mistakes in citing sources, even when their reasoning is correct. Additionally, the study highlights a trade-off where deeper reasoning can lead to less accurate citations, indicating a gap between a model\'s internal logic and its ability to provide verifiable information.","title":"MuRGAt: Grounding Reasoning in Multimodal Attribution"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="MuRGAt is a new benchmark designed to evaluate how well models can attribute facts in complex reasoning tasks using multiple types of data, like video and audio. It challenges models to not only provide answers but also to cite their sources accurately, specifying the type of data and the exact time segments used. The benchmark reveals that even advanced multimodal large language models (MLLMs) often make mistakes in citing sources, even when their reasoning is correct. Additionally, the study highlights a trade-off where deeper reasoning can lead to less accurate citations, indicating a gap between a model's internal logic and its ability to provide verifiable information.", title='MuRGAt: Grounding Reasoning in Multimodal Attribution'))
[13.02.2026 05:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MuRGAtMuRGAt","title":"MuRGAt"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MuRGAtMuRGAt', title='MuRGAt'))
[13.02.2026 05:57] Querying the API.
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Video diffusion model PISCO enables precise instance insertion with sparse keyframe control through variable-information guidance and distribution-preserving temporal masking.  					AI-generated summary 				 The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.
[13.02.2026 05:57] Response: ```json
{
  "desc": "PISCO    -,                .    Variable-Information Guidance       Distribution-Preserving Temporal Masking     .     ,      ,       .     PISCO-Bench  ,         -.",
  "emoji": "",
  "title": "         "
}
```
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Video diffusion model PISCO enables precise instance insertion with sparse keyframe control through variable-information guidance and distribution-preserving temporal masking.  					AI-generated summary 				 The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO."

[13.02.2026 05:57] Response: ```python
['VIDEO', 'BENCHMARK', 'ARCHITECTURE']
```
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Video diffusion model PISCO enables precise instance insertion with sparse keyframe control through variable-information guidance and distribution-preserving temporal masking.  					AI-generated summary 				 The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO."

[13.02.2026 05:57] Response: ```python
["DIFFUSION"]
```
[13.02.2026 05:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces PISCO, a video diffusion model designed for precise video instance insertion, allowing users to control the insertion of specific instances into existing footage with minimal effort. It addresses challenges in maintaining scene integrity and dynamics while ensuring accurate spatial-temporal placement and interaction of objects. PISCO employs Variable-Information Guidance and Distribution-Preserving Temporal Masking to enhance the robustness of the model under sparse keyframe conditions. The model is evaluated using a new benchmark, PISCO-Bench, demonstrating superior performance compared to traditional video editing methods, especially as more control signals are utilized.","title":"PISCO: Precision in Video Instance Insertion with Minimal Effort"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces PISCO, a video diffusion model designed for precise video instance insertion, allowing users to control the insertion of specific instances into existing footage with minimal effort. It addresses challenges in maintaining scene integrity and dynamics while ensuring accurate spatial-temporal placement and interaction of objects. PISCO employs Variable-Information Guidance and Distribution-Preserving Temporal Masking to enhance the robustness of the model under sparse keyframe conditions. The model is evaluated using a new benchmark, PISCO-Bench, demonstrating superior performance compared to traditional video editing methods, especially as more control signals are utilized.', title='PISCO: Precision in Video Instance Insertion with Minimal Effort'))
[13.02.2026 05:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PISCOPISCOPISCO","title":"PISCO"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PISCOPISCOPISCO', title='PISCO'))
[13.02.2026 05:57] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#transfer_learning", "#3d", "#architecture", "#synthetic", "#benchmark", "#multimodal", "#agents", "#reasoning"], "emoji": "", "ru": {"title": "      ", "desc": "   Vision-Language-Action (VLA)
[13.02.2026 05:57] Using data from previous issue: {"categories": ["#interpretability", "#open_source"], "emoji": "", "ru": {"title": "       ", "desc": "  Neural Additive Experts (NAE)    ,      
[13.02.2026 05:57] Querying the API.
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

NarraScore presents a hierarchical framework that uses frozen Vision-Language Models as affective sensors to generate coherent soundtracks for long-form videos by combining global semantic anchors with token-level adaptive modulation.  					AI-generated summary 				 Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a Global Semantic Anchor ensures stylistic stability, while a surgical Token-Level Affective Adapter modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation.
[13.02.2026 05:57] Response: ```json
{
  "desc": "NarraScore            ,    Vision-Language     .     ,        .    :      ,             .             ,   ,      .",
  "emoji": "",
  "title": "   :     "
}
```
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"NarraScore presents a hierarchical framework that uses frozen Vision-Language Models as affective sensors to generate coherent soundtracks for long-form videos by combining global semantic anchors with token-level adaptive modulation.  					AI-generated summary 				 Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a Global Semantic Anchor ensures stylistic stability, while a surgical Token-Level Affective Adapter modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation."

[13.02.2026 05:57] Response: ```python
["VIDEO", "AUDIO", "MULTIMODAL", "ARCHITECTURE"]
```
[13.02.2026 05:57] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"NarraScore presents a hierarchical framework that uses frozen Vision-Language Models as affective sensors to generate coherent soundtracks for long-form videos by combining global semantic anchors with token-level adaptive modulation.  					AI-generated summary 				 Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a Global Semantic Anchor ensures stylistic stability, while a surgical Token-Level Affective Adapter modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation."

[13.02.2026 05:57] Response: ```python
['LONG_CONTEXT']
```
[13.02.2026 05:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"NarraScore is a new framework designed to create soundtracks for long videos by understanding the emotions in the narrative. It uses frozen Vision-Language Models as sensors to capture the emotional flow of the video, translating visual information into emotional trajectories. The framework combines a stable global structure with dynamic local adjustments to ensure the soundtrack matches the evolving story. This approach not only improves coherence and alignment with the narrative but also reduces the computational demands typically associated with such tasks.","title":"NarraScore: Emotion-Driven Soundtrack Generation for Long Videos"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='NarraScore is a new framework designed to create soundtracks for long videos by understanding the emotions in the narrative. It uses frozen Vision-Language Models as sensors to capture the emotional flow of the video, translating visual information into emotional trajectories. The framework combines a stable global structure with dynamic local adjustments to ensure the soundtrack matches the evolving story. This approach not only improves coherence and alignment with the narrative but also reduces the computational demands typically associated with such tasks.', title='NarraScore: Emotion-Driven Soundtrack Generation for Long Videos'))
[13.02.2026 05:57] Response: ParsedChatCompletionMessage[Article](content='{"desc":"NarraScore-NarraScoreNarraScore","title":""}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='NarraScore-NarraScoreNarraScore', title=''))
[13.02.2026 05:57] Renaming data file.
[13.02.2026 05:57] Renaming previous data. hf_papers.json to ./d/2026-02-13.json
[13.02.2026 05:57] Saving new data file.
[13.02.2026 05:57] Generating page.
[13.02.2026 05:57] Renaming previous page.
[13.02.2026 05:57] Renaming previous data. index.html to ./d/2026-02-13.html
[13.02.2026 05:57] Writing result.
[13.02.2026 05:57] Renaming log file.
[13.02.2026 05:57] Renaming previous data. log.txt to ./logs/2026-02-13_last_log.txt
