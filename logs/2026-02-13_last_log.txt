[13.02.2026 08:35] Read previous papers.
[13.02.2026 08:35] Generating top page (month).
[13.02.2026 08:35] Writing top page (month).
[13.02.2026 09:37] Read previous papers.
[13.02.2026 09:37] Get feed.
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09877
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10934
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12125
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12099
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12056
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12280
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11748
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12153
[13.02.2026 09:37] Extract page data from URL. URL: https://huggingface.co/papers/2602.12036
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11731
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05548
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12092
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09070
[13.02.2026 09:37] Extract page data from URL. URL: https://huggingface.co/papers/2602.09021
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11761
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11683
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08194
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12262
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12205
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11541
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11337
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11298
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12164
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12116
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11964
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11636
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11598
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11543
[13.02.2026 09:37] Extract page data from URL. URL: https://huggingface.co/papers/2602.12203
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11792
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11509
[13.02.2026 09:37] Extract page data from URL. URL: https://huggingface.co/papers/2602.11075
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10585
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10575
[13.02.2026 09:37] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08277
[13.02.2026 09:37] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.02.2026 09:37] No deleted papers detected.
[13.02.2026 09:37] Downloading and parsing papers (pdf, html). Total: 35.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.09877.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.09877.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.09877.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.10934.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.10934.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.10934.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.12125.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.12125.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.12125.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.12099.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.12099.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.12099.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.12056.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.12056.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.12056.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.12280.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.12280.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.12280.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.11748.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.11748.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.11748.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.12153.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.12153.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.12153.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.12036.
[13.02.2026 09:37] Downloading paper 2602.12036 from https://arxiv.org/pdf/2602.12036v1...
[13.02.2026 09:37] Extracting affiliations from text.
[13.02.2026 09:37] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models February 13, 2026 Xin Xu 1 2 Clive Bai 1 Kai Yang 1 Tianhao Chen 2 Yangkun Chen 1 Weijie Liu 1 Hao Chen 2 Yang Wang 3 Saiyong Yang 1 Can Yang "
[13.02.2026 09:37] Response: ```python
[]
```
[13.02.2026 09:37] Extracting affiliations from text.
[13.02.2026 09:37] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models February 13, 2026 Xin Xu 1 2 Clive Bai 1 Kai Yang 1 Tianhao Chen 2 Yangkun Chen 1 Weijie Liu 1 Hao Chen 2 Yang Wang 3 Saiyong Yang 1 Can YangLarge-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that CompositionRL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXUUSTC/Composition-RL. 6 2 0 2 2 1 ] . [ 1 6 3 0 2 1 . 2 0 6 2 : r 1. Introduction After the advent of OpenAI-o1 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025), Reinforcement Learning with Verifiable Rewards (RLVR) has reshaped the training lifecycle of large language models (LLMs), improv1HY, Tencent 2The Hong Kong University of Science and Technology 3The University of Hong Kong. Correspondence to: Can Yang <macyang@ust.hk>, Saiyong Yang <stevesyang@tencent.com>. Preprint. February 13, 2026. 1 ing both text-only reasoning (Luo et al., 2025; Yang et al., 2025a; Liu et al., 2025b; Cai et al., 2025) and multimodal question answering (Meng et al., 2025; Xiao et al., 2025). Rapid progress in RLVR, including improved optimization algorithms (Nan et al., 2025; Yu et al., 2025; Chen et al., 2025a; Liu et al., 2025b), more efficient training frameworks (Sheng et al., 2024; Fu et al., 2025; Zhu et al., 2025b), and techniques to mitigate traininginference mismatch (Yao et al., 2025; Qi et al., 2025), has contributed to the strong slow-thinking ability of large reasoning models (LRMs), often manifested as longer chain of thought (CoT) (Wei et al., 2022). At its core, RLVR relies on large collections of training prompts paired with ground-truth answers to enable verifiable reward computation during training (Hu et al., 2025; He et al., 2025b; Luo et al., 2025). Prompts with 0/1 rollout accuracy yield zero gradient signals in RLVR algorithms (Yu et al., 2025), substantially reducing the number of available informative prompts during training. However, collecting and cleaning additional high-quality training prompts is often expensive (He et al., 2025b; Zeng et al., 2025). To mitigate this, prior work has primarily focused on better leveraging hard prompts with zero success rate, via advantage shaping (Le et al., 2025; Nan et al., 2025), allocating more rollouts (Yang et al., 2025c; Li et al., 2025c), and hint-based augmentation (Chen et al., 2025b; Li et al., 2025a). Nevertheless, while all-zero prompts constitute some fraction of the training set, as training progresses, an increasing proportion of prompts attain rollout accuracy of 1. This motivates the need for methods that can better exploit these easy prompts. In this work, we propose Composition-RL, simple yet effective approach for better utilizing limited verifiable training prompts by transforming simple prompts into more challenging ones. We first introduce procedure for composing existing prompts into new prompts (Section 3.1) and empirically show that prompt composition can, to some extent, mitigate the growing number of too-easy prompts (Section 3.2). We then formalize Composition-RL as RL training on compositional prompts (Section 3.3); an overview is provided in Figure 1. As shown in Figure 1, CompositionRL outperforms RL training on the original prompts, with Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models Figure 1. Overview of Composition-RL. Top: an example of composing two math problems, illustrating the high-level idea of CompositionRL. Bottom left: pass@1 (%) on AIME24 versus training steps for different methods, summarizing key findings in Sections 4.2 and 4.3. Bottom right: cross-topic results on MMLU-Pro subjects with the top-5 largest sample sizes, highlighting the main finding in Section 4.4. increasing performance gains when combined with curriculum over compositional depth K. Moreover, composing prompts from different domains shows strong potential for cross-domain RL training. Our contributions can be summarized as follows: ‚ù∂ We propose Composition-RL, an approach that performs RL on composed prompts that are automatically transformed from existing ones. ‚ù∑ Extensive experiments on 4B-30B LLMs demonstrate the effectiveness of Composition-RL and the curriculum variant of Composition-RL. ‚ù∏ We show that RL on composed prompts spanning physics and math is more effective than simply mixing training problems, regardless of whether sequential or joint training. ‚ùπ We analyze the reasons behind the success of Composition-RL through the lenses of compositional generalization and implicit process supervision. 2. Preliminary Notation. We denote an LLM parameterized by Œ∏ as policy œÄŒ∏. Let be an input query (i.e., prompt) and be the set of all queries. Given response = (r1, . . . , rr) to q, the policy likelihood can be written as œÄŒ∏(r q) = (cid:81)r t=1 œÄŒ∏(rt q, r<t), where r<t = (r1, . . . , rt1) and is the number of tokens in r. Each (q, r) can be evaluated by verifier v(q, r) {0, 1}, which indicates whether matches the ground-truth answer of (denoted as gt). RLVR. RLVR optimizes the expected verifiable reward: (cid:2)v(q, r)(cid:3)). (cid:2)JRLVR(Œ∏, q)(cid:3) (= EqD, rœÄŒ∏(q) maxŒ∏ EqD standard policy gradient estimator (Sutton et al., 1999) is: gŒ∏(q, r) = A(q, r) Œ∏ log œÄŒ∏(rq), (1) where A(q, r) = v(q, r) b(q) is called advantage and b(q) is baseline function that depends only on the query q. Group Rel"
[13.02.2026 09:37] Mistral response. {"id": "469c18fcd03141b5b226f7fc1eff74b6", "created": 1770975437, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1749, "total_tokens": 1783, "completion_tokens": 34, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"HY, Tencent\",\n    \"The Hong Kong University of Science and Technology\",\n    \"The University of Hong Kong\"\n]\n```"}}]}
[13.02.2026 09:37] Response: ```python
[
    "HY, Tencent",
    "The Hong Kong University of Science and Technology",
    "The University of Hong Kong"
]
```
[13.02.2026 09:37] Deleting PDF ./assets/pdf/2602.12036.pdf.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.11731.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.11731.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.11731.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.05548.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.05548.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.05548.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.12092.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.12092.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.12092.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.09070.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.09070.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.09070.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.09021.
[13.02.2026 09:37] Downloading paper 2602.09021 from https://arxiv.org/pdf/2602.09021v1...
[13.02.2026 09:37] Extracting affiliations from text.
[13.02.2026 09:37] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"œá0: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies Checheng Yu, Chonghao Sima, Gangcheng Jiang, Hai Zhang, Haoguang Mai, Hongyang Li, Huijie Wang, Jin Chen, Kaiyang Wu, Li Chen, Lirui Zhao, Modi Shi, Ping Luo, Qingwen Bu, Shijia Peng, Tianyu Li, Yibo Yuan Kinetix AI Code: https://github.com/OpenDriveLab/kai0 Blog: https://mmlab.hk/research/kai0 6 2 0 2 9 ] . [ 1 1 2 0 9 0 . 2 0 6 2 : r Fig. 1: Top: System overview. robot teamwork system with two dual-arm ALOHA robots performing long-horizon collaborative garment manipulation, including flattening, folding and hanging. Bottom: Technical philosophy and performance. Distributional inconsistencies are inherent to robot learning (Ptrain: expert demonstrations; Qmodel: policy inductive bias; Ptest: deployment trajectories). œá0 systematically resolves these pairwise mismatches: Model Arithmetic aligns Qmodel with Ptrain; Train-Deploy Alignment bridges Ptrain and Ptest; and Stage Advantage optimizes Qmodel for Ptest. The contributions of these modules collectively enable œá0 to surpass the baseline œÄ0.5 [3] in terms of success rate by approximately 250%. AbstractHigh-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distributiona systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose œá0, resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varyi"
[13.02.2026 09:37] Response: ```python
["Kinetix AI"]
```
[13.02.2026 09:37] Deleting PDF ./assets/pdf/2602.09021.pdf.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.11761.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.11761.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.11761.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.11683.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.11683.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.11683.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.08194.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.08194.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.08194.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.12262.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.12262.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.12262.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.12205.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.12205.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.12205.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.11541.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.11541.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.11541.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.11337.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.11337.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.11337.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.11298.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.11298.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.11298.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.12164.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.12164.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.12164.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.12116.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.12116.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.12116.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.11964.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.11964.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.11964.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.11636.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.11636.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.11636.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.11598.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.11598.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.11598.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.11543.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.11543.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.11543.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.12203.
[13.02.2026 09:37] Downloading paper 2602.12203 from https://arxiv.org/pdf/2602.12203v1...
[13.02.2026 09:37] Extracting affiliations from text.
[13.02.2026 09:37] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"EXSTRUCTINY: Benchmark for Schema-Variable Structured Information Extraction from Document Images Mathieu Sibue Andres Mu√±oz Garza Zhiqiang Ma Xiaomo Liu Manuela Veloso J.P. Morgan AI Research {name}.{surname}@jpmchase.com 6 2 0 2 2 1 ] . [ 1 3 0 2 2 1 . 2 0 6 2 : r a "
[13.02.2026 09:37] Response: ```python
["J.P. Morgan AI Research"]
```
[13.02.2026 09:37] Deleting PDF ./assets/pdf/2602.12203.pdf.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.11792.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.11792.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.11792.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.11509.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.11509.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.11509.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.11075.
[13.02.2026 09:37] Downloading paper 2602.11075 from https://arxiv.org/pdf/2602.11075v1...
[13.02.2026 09:37] Extracting affiliations from text.
[13.02.2026 09:37] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"RISE: Self-Improving Robot Policy with Compositional World Model Jiazhi Yang1,2 Kunyang Lin2 Jinwei Li2,6 Wencong Zhang2 Zhizhong Su5 Hao Zhao6 Ya-Qin Zhang6 Li Chen3 Tianwei Lin5 Longyan Wu4 Ping Luo3 Xiangyu Yue1 Hongyang Li3 1 The Chinese University of Hong Kong 4 Shanghai Innovation Institute Equal Contribution 2 Kinetix AI 5 Horizon Robotics Project lead https://opendrivelab.com/kai0-rl Equal Advising 3 The University of Hong Kong 6 Tsinghua University 6 2 0 F 1 1 ] . [ 1 5 7 0 1 1 . 2 0 6 2 : r Fig. 1: We present RISE, framework for Reinforcement learning via Imagination for SElf-improving robots. (a) Conventional physical-world RL is bottlenecked by hardware cost, slow serial interaction, and the need for manual reset. (b) RISE shifts the learning environment to Compositional World Model, which first emulates future observations for proposed actions, then evaluates imagined states to derive advantage for policy improvement. (c) Training on massive imaginative rollouts effectively bootstraps RISEs performance across variety of complex, contact-rich tasks, surpassing prior art by non-trivial margin. AbstractDespite the sustained scaling on model capacity and data acquisition, VisionLanguageAction (VLA) models remain brittle in contact-rich and dynamic manipulation tasks, where minor execution deviations can compound into failures. While reinforcement learning (RL) offers principled path to robustness, on-policy RL in the physical world is constrained by safety risk, hardware cost, and environment reset. To bridge this gap, we present RISE, scalable framework of robotic reinforcement learning via imagination. At its core is Compositional World Model that (i) predicts multi-view future via controllable dynamics model, and (ii) evaluates imagined outcomes with progress value model, producing informative advantages for the policy improvement. Such compositional design allows state and value to be tailored by best-suited yet distinct architectures and objectives. Th"
[13.02.2026 09:37] Response: ```python
[
    "The Chinese University of Hong Kong",
    "Kinetix AI",
    "The University of Hong Kong",
    "Shanghai Innovation Institute",
    "Horizon Robotics",
    "Tsinghua University"
]
```
[13.02.2026 09:37] Deleting PDF ./assets/pdf/2602.11075.pdf.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.10585.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.10585.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.10585.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.10575.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.10575.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.10575.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Downloading and parsing paper https://huggingface.co/papers/2602.08277.
[13.02.2026 09:37] Extra JSON file exists (./assets/json/2602.08277.json), skip PDF parsing.
[13.02.2026 09:37] Paper image links file exists (./assets/img_data/2602.08277.json), skip HTML parsing.
[13.02.2026 09:37] Success.
[13.02.2026 09:37] Enriching papers with extra data.
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 0. Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution.  					AI-generated summary 				 The emergence of multi-agent systems built from large language models (LLMs)...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 1. A fully end-to-end Transformer-based audio tokenizer architecture achieves high-fidelity reconstruction across diverse audio domains and enables superior text-to-speech and automatic speech recognition performance.  					AI-generated summary 				 Discrete audio tokenizers are fundamental to empoweri...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 2. On-policy distillation is extended through a generalized framework that introduces flexible reference models and reward scaling factors, demonstrating improved performance through reward extrapolation and reward correction techniques.  					AI-generated summary 				 On-policy distillation (OPD), whi...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 3. A vision-language-action model enhanced with world model-based reinforcement learning demonstrates improved performance and long-horizon execution capabilities for robotic manipulation tasks.  					AI-generated summary 				 Vision-language-action (VLA) models that directly predict multi-step action ...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 4. LawThinker is an autonomous legal research agent that uses an Explore-Verify-Memorize strategy with a DeepVerifier module to ensure accurate and procedurally compliant legal reasoning through dynamic verification of intermediate steps.  					AI-generated summary 				 Legal reasoning requires not onl...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 5. Progressive Semantic Illusions use a generative framework with dual-branch Score Distillation Sampling to create vector sketches that transform semantically through sequential stroke additions, achieving superior recognizability and illusion strength.  					AI-generated summary 				 Visual illusions...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 6. Models require in-context exploration capabilities to scale effectively at test time, but autoregressive generation faces exponential decay in sampling long sequences, which is addressed by a length-incentivized exploration method that improves performance on both in-domain and out-of-domain tasks. ...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 7. Diffusion large language models enable parallel token generation and efficient reasoning enhancement through a voting technique that identifies and refines uncertain predictions across multiple samples.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) represent a new paradigm ...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 8. Composition-RL improves reasoning capabilities by automatically composing multiple problems into new verifiable questions for reinforcement learning training.  					AI-generated summary 				 Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR),...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 9. Visual reasoning is enhanced by reconstructing logical structures from compressed visual tokens through a DSL-based approach that generates deterministic visual proofs for verification.  					AI-generated summary 				 Existing multimodal large language models have achieved high-fidelity visual perce...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 10. Asymmetric Group Relative Advantage Estimation addresses exploration and difficulty adaptation challenges in reinforcement learning with large language models by dynamically modulating exploration incentives and sample difficulty focus.  					AI-generated summary 				 Reinforcement Learning with Ver...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 11. DeepSight is an open-source project that integrates safety evaluation and diagnosis for large language and multimodal models, enabling white-box insights through unified protocols and specialized toolkits.  					AI-generated summary 				 As the development of Large Models (LMs) progresses rapidly, t...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 12. NarraScore presents a hierarchical framework that uses frozen Vision-Language Models as affective sensors to generate coherent soundtracks for long-form videos by combining global semantic anchors with token-level adaptive modulation.  					AI-generated summary 				 Synthesizing coherent soundtracks...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 13. A resource-efficient robotic manipulation framework addresses distributional shifts through model arithmetic, stage-aware advantage estimation, and train-deploy alignment to achieve long-horizon task reliability.  					AI-generated summary 				 High-reliability long-horizon robotic manipulation has ...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 14. MiniCPM-SALA combines sparse and linear attention mechanisms in a hybrid architecture to enable efficient processing of ultra-long contexts while maintaining model performance and reducing training costs.  					AI-generated summary 				 The evolution of large language models (LLMs) towards applicati...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 15. ThinkRouter is a confidence-aware routing mechanism that improves reasoning efficiency by switching between discrete token and latent spaces based on model confidence, achieving better accuracy and faster generation.  					AI-generated summary 				 Recent work explores latent reasoning to improve re...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 16. Foundation models generate executable environment code to scaffold learning progress in open-ended worlds, enabling agents to acquire long-horizon skills through curriculum control.  					AI-generated summary 				 Open-ended learning frames intelligence as emerging from continual interaction with an...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 17. A trajectory self-distillation framework with direct discriminative optimization improves few-step decoding efficiency in diffusion large language models while maintaining generation quality.  					AI-generated summary 				 Diffusion large language models (DLLMs) have the potential to enable fast te...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 18. A lightweight 5B unified multimodal model achieves competitive performance through hierarchical feature extraction, learnable think tokens, and progressive training strategies including alignment pre-training, joint supervised fine-tuning, and reinforcement learning with MR-GRPO.  					AI-generated ...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 19. Budget-constrained tool-augmented agents use a hierarchical world model and intent-aware planning to optimize multi-step task completion under monetary constraints.  					AI-generated summary 				 We study budget-constrained tool-augmented agents, where a large language model must solve multi-step t...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 20. MolmoSpaces presents an open ecosystem with diverse indoor environments and annotated objects for large-scale robot policy benchmarking across multiple tasks and simulators.  					AI-generated summary 				 Deploying robots at scale demands robustness to the long tail of everyday situations. The coun...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 21. Voxtral Realtime is a streaming speech recognition model trained end-to-end for sub-second latency with performance matching offline systems.  					AI-generated summary 				 We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription qual...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 22. Sci-CoE is a two-stage scientific co-evolving framework that enables large language models to self-evolve as both solver and verifier through sparse-to-unsupervised learning transitions, improving scientific reasoning capabilities and evaluation system robustness.  					AI-generated summary 				 Lar...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 23. Personalized generative reward models address challenges in adapting language model responses to individual user preferences by using structured evaluation chains and dual-granularity scaling mechanisms for improved generalization and accuracy.  					AI-generated summary 				 Personalized alignment ...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 24. Gaia2 presents a benchmark for evaluating large language model agents in asynchronous, dynamic environments with temporal constraints and multi-agent collaboration, featuring a write-action verifier for reinforcement learning and revealing trade-offs between reasoning, efficiency, and robustness.  	...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 25. ScalSelect is a scalable training-free method for selecting representative multimodal data that achieves near-full-dataset performance with significantly reduced computational requirements.  					AI-generated summary 				 Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for adva...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 26. A unified Vision-Language-Action model with a hierarchical architecture combining semantic reasoning and continuous trajectory generation achieves state-of-the-art performance across multiple embodied navigation tasks.  					AI-generated summary 				 Embodied navigation has long been fragmented by t...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 27. A memory-efficient decentralized framework for training mixture-of-experts language models using sparse expert synchronization and expert-merging warm-up strategies.  					AI-generated summary 				 Pretraining large language models (LLMs) typically requires centralized clusters with thousands of hig...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 28. A new benchmark dataset called ExStrucTiny is introduced for structured information extraction from document images, addressing limitations of existing datasets and evaluating vision-language models on diverse document types and flexible schemas.  					AI-generated summary 				 Enterprise documents,...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 29. Reinforcement learning with verifiable rewards induces behavioral signatures that can be detected using a black-box method based on prompt generation diversity, outperforming existing contamination detection approaches.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 30. MuRGAt is a benchmark for evaluating fact-level multimodal attribution in complex reasoning tasks, requiring models to provide precise citations for their answers across video, audio, and other modalities.  					AI-generated summary 				 Multimodal large language models (MLLMs) are increasingly used...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 31. RISE is a robotic reinforcement learning framework that uses a compositional world model to predict multi-view futures and evaluate imagined outcomes, enabling policy improvement through virtual interactions rather than physical trials.  					AI-generated summary 				 Despite the sustained scaling o...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 32. Neural Additive Experts combines multiple specialized networks with a dynamic gating mechanism to balance predictive accuracy and feature interpretability in machine learning models.  					AI-generated summary 				 The trade-off between interpretability and accuracy remains a core challenge in machi...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 33. MetaphorStar, an end-to-end visual reinforcement learning framework, significantly enhances metaphor comprehension in images through a specialized dataset, RL method, and benchmark, achieving state-of-the-art performance on multiple visual reasoning tasks.  					AI-generated summary 				 Metaphorica...
[13.02.2026 09:37] ********************************************************************************
[13.02.2026 09:37] Abstract 34. Video diffusion model PISCO enables precise instance insertion with sparse keyframe control through variable-information guidance and distribution-preserving temporal masking.  					AI-generated summary 				 The landscape of AI video generation is undergoing a pivotal shift: moving beyond general ge...
[13.02.2026 09:37] Read previous papers.
[13.02.2026 09:37] Generating reviews via LLM API.
[13.02.2026 09:37] Using data from previous issue: {"categories": ["#alignment", "#training", "#agents", "#security"], "emoji": "‚öñÔ∏è", "ru": {"title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–µ–π –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞, —á—Ç–æ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –Ω–µ –º–æ–≥—É—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –¥–æ—Å—Ç–∏—á—å 
[13.02.2026 09:37] Using data from previous issue: {"categories": ["#optimization", "#audio", "#training", "#architecture"], "emoji": "üéµ", "ru": {"title": "–°–∫–≤–æ–∑–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é –æ–¥–Ω–æ—Ä–æ–¥–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ CAT (Causal Audio Tokenizer with Transformer) ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–Ω–∞
[13.02.2026 09:37] Using data from previous issue: {"categories": ["#alignment", "#training", "#rl", "#optimization", "#reasoning"], "emoji": "üéì", "ru": {"title": "–£–º–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è: –∫–∞–∫ —É—á–µ–Ω–∏–∫ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —É—á–∏—Ç–µ–ª—è —á–µ—Ä–µ–∑ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è", "desc": "–†–∞–±–æ—Ç–∞ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ ¬´–Ω–∞ –ø–æ–ª–∏—Ç–∏–∫–µ¬ª (on-policy distillation), –ø—Ä–µ–¥–ª–∞–≥–∞—è –æ–±–æ–±—â—ë–Ω
[13.02.2026 09:37] Using data from previous issue: {"categories": ["#robotics", "#rl", "#cv", "#benchmark", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ú–∏—Ä–æ–≤–∞—è –º–æ–¥–µ–ª—å –∫–∞–∫ –æ—Å–Ω–æ–≤–∞ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è GigaBrain-0.5M*, –º–æ–¥–µ–ª—å Vision-Language-Action, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–µ–Ω–∞ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω
[13.02.2026 09:37] Using data from previous issue: {"categories": [], "emoji": "‚öñÔ∏è", "ru": {"title": "–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–≥–µ–Ω—Ç —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "LawThinker ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Explore-Verify-Memorize —Å –º–æ–¥—É–ª–µ–º DeepVerifier –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ –∏
[13.02.2026 09:37] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#multimodal"], "emoji": "üé®", "ru": {"title": "–û—Ç —É—Ç–∫–∏ –∫ –æ–≤—Ü–µ: –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∏–ª–ª—é–∑–∏–∏ —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —à—Ç—Ä–∏—Ö–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∏–ª–ª—é–∑–∏–π ‚Äî –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç—Å–∫–∏–∑–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–µ–Ω—è—é—Ç —Å–≤–æ—ë –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ 
[13.02.2026 09:37] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "üîç", "ru": {"title": "–ü–æ–æ—â—Ä–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤–æ –≤—Ä–µ–º—è –∞–≤—Ç–æ–º–∞
[13.02.2026 09:37] Using data from previous issue: {"categories": ["#reasoning", "#diffusion", "#open_source"], "emoji": "üó≥Ô∏è", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (dLLM), –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç—Ö–æ–¥—è—Ç –æ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –≥–µ–Ω–µ—Ä–∏
[13.02.2026 09:37] Querying the API.
[13.02.2026 09:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Composition-RL improves reasoning capabilities by automatically composing multiple problems into new verifiable questions for reinforcement learning training.  					AI-generated summary 				 Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL.
[13.02.2026 09:37] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Composition-RL ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –ø—É—Ç—ë–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–¥–∞—á –≤ –Ω–æ–≤—ã–µ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ (RLVR) –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–≥–æ –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –ª–µ–≥–∫–∏—Ö –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—é—Ç –æ–±—É—á–µ–Ω–∏—é. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—ë–≥–∫–∏–µ –∑–∞–¥–∞—á–∏ –ø—É—Ç—ë–º –∏—Ö –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏, —Å–æ–∑–¥–∞–≤–∞—è –Ω–æ–≤—ã–µ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ RL –∞–≥–µ–Ω—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—è—Ö —Ä–∞–∑–º–µ—Ä–æ–º –æ—Ç 4B –¥–æ 30B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Composition-RL –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.",
  "emoji": "üß©",
  "title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏—è –∑–∞–¥–∞—á –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º"
}
```
[13.02.2026 09:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Composition-RL improves reasoning capabilities by automatically composing multiple problems into new verifiable questions for reinforcement learning training.  					AI-generated summary 				 Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL."

[13.02.2026 09:37] Response: ```python
["RL", "TRAINING", "DATASET"]
```
[13.02.2026 09:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Composition-RL improves reasoning capabilities by automatically composing multiple problems into new verifiable questions for reinforcement learning training.  					AI-generated summary 				 Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL."

[13.02.2026 09:37] Response: ```python
["REASONING", "OPTIMIZATION", "OPEN_SOURCE"]
```
[13.02.2026 09:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Composition-RL is a novel approach that enhances reasoning abilities in reinforcement learning (RL) by creating new, verifiable questions from existing problems. This method addresses the challenge of limited training data by focusing on hard prompts that have a rollout pass rate of 0, while also managing the abundance of easy prompts with a pass rate of 1. By composing multiple problems into a single question, Composition-RL effectively increases the diversity and utility of training data. Experiments demonstrate that this technique improves reasoning performance across various model sizes and supports cross-domain RL applications.","title":"Enhancing Reasoning in RL through Problem Composition"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Composition-RL is a novel approach that enhances reasoning abilities in reinforcement learning (RL) by creating new, verifiable questions from existing problems. This method addresses the challenge of limited training data by focusing on hard prompts that have a rollout pass rate of 0, while also managing the abundance of easy prompts with a pass rate of 1. By composing multiple problems into a single question, Composition-RL effectively increases the diversity and utility of training data. Experiments demonstrate that this technique improves reasoning performance across various model sizes and supports cross-domain RL applications.', title='Enhancing Reasoning in RL through Problem Composition'))
[13.02.2026 09:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Composition-RLÊòØ‰∏ÄÁßçÂ¢ûÂº∫Êé®ÁêÜËÉΩÂäõÁöÑÊñπÊ≥ïÔºåÈÄöËøáËá™Âä®ÁªÑÂêàÂ§ö‰∏™ÈóÆÈ¢òÁîüÊàêÊñ∞ÁöÑÂèØÈ™åËØÅÈóÆÈ¢òÔºåÁî®‰∫éÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ„ÄÇËØ•ÊñπÊ≥ïÊó®Âú®Êõ¥Â•ΩÂú∞Âà©Áî®ÊúâÈôêÁöÑÂèØÈ™åËØÅÊèêÁ§∫ÔºåÁâπÂà´ÊòØÈíàÂØπÈÄöËøáÁéá‰∏∫1ÁöÑÊèêÁ§∫„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåComposition-RLÂú®‰∏çÂêåËßÑÊ®°ÁöÑÊ®°Âûã‰∏äÂùáËÉΩÊòæËëóÊèêÈ´òÊé®ÁêÜËÉΩÂäõÔºåÂπ∂‰∏îÈÄöËøáÈÄêÊ≠•Â¢ûÂä†ÁªÑÂêàÊ∑±Â∫¶ÁöÑËØæÁ®ãÂèò‰ΩìÔºåÊÄßËÉΩËøõ‰∏ÄÊ≠•ÊèêÂçá„ÄÇÊ≠§Â§ñÔºåComposition-RLËøòÊîØÊåÅË∑®È¢ÜÂüüÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåÈÄöËøáÁªÑÂêàÊù•Ëá™‰∏çÂêåÈ¢ÜÂüüÁöÑÊèêÁ§∫Êù•ÂÆûÁé∞„ÄÇ","title":"Composition-RLÔºöÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑËá™Âä®ÁªÑÂêàÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Composition-RLÊòØ‰∏ÄÁßçÂ¢ûÂº∫Êé®ÁêÜËÉΩÂäõÁöÑÊñπÊ≥ïÔºåÈÄöËøáËá™Âä®ÁªÑÂêàÂ§ö‰∏™ÈóÆÈ¢òÁîüÊàêÊñ∞ÁöÑÂèØÈ™åËØÅÈóÆÈ¢òÔºåÁî®‰∫éÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ„ÄÇËØ•ÊñπÊ≥ïÊó®Âú®Êõ¥Â•ΩÂú∞Âà©Áî®ÊúâÈôêÁöÑÂèØÈ™åËØÅÊèêÁ§∫ÔºåÁâπÂà´ÊòØÈíàÂØπÈÄöËøáÁéá‰∏∫1ÁöÑÊèêÁ§∫„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåComposition-RLÂú®‰∏çÂêåËßÑÊ®°ÁöÑÊ®°Âûã‰∏äÂùáËÉΩÊòæËëóÊèêÈ´òÊé®ÁêÜËÉΩÂäõÔºåÂπ∂‰∏îÈÄöËøáÈÄêÊ≠•Â¢ûÂä†ÁªÑÂêàÊ∑±Â∫¶ÁöÑËØæÁ®ãÂèò‰ΩìÔºåÊÄßËÉΩËøõ‰∏ÄÊ≠•ÊèêÂçá„ÄÇÊ≠§Â§ñÔºåComposition-RLËøòÊîØÊåÅË∑®È¢ÜÂüüÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåÈÄöËøáÁªÑÂêàÊù•Ëá™‰∏çÂêåÈ¢ÜÂüüÁöÑÊèêÁ§∫Êù•ÂÆûÁé∞„ÄÇ', title='Composition-RLÔºöÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑËá™Âä®ÁªÑÂêàÊñπÊ≥ï'))
[13.02.2026 09:37] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#cv", "#interpretability", "#architecture", "#reasoning"], "emoji": "üî¨", "ru": {"title": "–û—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤—É —á–µ—Ä–µ–∑ –∫–æ–¥", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫
[13.02.2026 09:37] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#optimization", "#reasoning"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ê—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è exploration –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Asymmetric GRAE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑
[13.02.2026 09:37] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#security", "#benchmark", "#alignment", "#open_source", "#dataset"], "emoji": "üîç", "ru": {"title": "–û—Ç —á—ë—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞ –∫ –±–µ–ª–æ–º—É: –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "DeepSight ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π –ø—Ä–æ–µ–∫—Ç –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω
[13.02.2026 09:37] Using data from previous issue: {"categories": ["#multimodal", "#long_context", "#audio", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–≠–º–æ—Ü–∏–∏ –∫–∞–∫ –∫–æ–º–ø—Ä–µ—Å—Å –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è: —É–º–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º—É–∑—ã–∫–∏ –¥–ª—è –≤–∏–¥–µ–æ", "desc": "NarraScore ‚Äî —ç—Ç–æ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –º—É–∑—ã–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä—Ç–∏—Ç—É—Ä –∫ –¥–ª–∏–Ω–Ω—ã–º –≤–∏–¥–µ–æ
[13.02.2026 09:37] Querying the API.
[13.02.2026 09:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A resource-efficient robotic manipulation framework addresses distributional shifts through model arithmetic, stage-aware advantage estimation, and train-deploy alignment to achieve long-horizon task reliability.  					AI-generated summary 				 High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose œá_{0}, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. œá_{0} enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that œá_{0} surpasses the state-of-the-art œÄ_{0.5} in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.
[13.02.2026 09:37] Response: ```json
{
  "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç œá‚ÇÄ ‚Äî —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º-–º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–º –Ω–∞ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–µ–π—Å—Ç–≤–∏–π. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–µ –≤ –æ–±—ä—ë–º–µ –¥–∞–Ω–Ω—ã—Ö, –∞ –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª—å–Ω–æ–º —Å–¥–≤–∏–≥–µ –º–µ–∂–¥—É –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è–º–∏ —á–µ–ª–æ–≤–µ–∫–∞, –∏–Ω–¥—É–∫—Ç–∏–≤–Ω—ã–º–∏ —Å–º–µ—â–µ–Ω–∏—è–º–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ö–Ω–∏–∫–∏: –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫—É –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–ª–∏—è–Ω–∏—è –≤–µ—Å–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π, —ç—Ç–∞–ø–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è-—Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é. –°–∏—Å—Ç–µ–º–∞ –¥–æ—Å—Ç–∏–≥–ª–∞ –Ω–∞–¥—ë–∂–Ω–æ–π –∞–≤—Ç–æ–Ω–æ–º–∏–∏ –ø—Ä–∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ç–∫–∞–Ω—è–º–∏ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ 24 —á–∞—Å–æ–≤, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —Å –º–µ–Ω—å—à–∏–º –æ–±—ä—ë–º–æ–º –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.",
  "emoji": "ü§ñ",
  "title": "–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç-–º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏"
}
```
[13.02.2026 09:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A resource-efficient robotic manipulation framework addresses distributional shifts through model arithmetic, stage-aware advantage estimation, and train-deploy alignment to achieve long-horizon task reliability.  					AI-generated summary 				 High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose œá_{0}, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. œá_{0} enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that œá_{0} surpasses the state-of-the-art œÄ_{0.5} in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community."

[13.02.2026 09:37] Response: ```python
["ROBOTICS", "TRAINING", "RL"]
```
[13.02.2026 09:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A resource-efficient robotic manipulation framework addresses distributional shifts through model arithmetic, stage-aware advantage estimation, and train-deploy alignment to achieve long-horizon task reliability.  					AI-generated summary 				 High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose œá_{0}, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. œá_{0} enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that œá_{0} surpasses the state-of-the-art œÄ_{0.5} in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community."

[13.02.2026 09:38] Response: ```python
["OPTIMIZATION", "TRANSFER_LEARNING", "OPEN_SOURCE"]
```

**Justification:**

1. **OPTIMIZATION**: The paper proposes optimization methods including "Model Arithmetic" (weight-space merging strategy), "Stage Advantage" (advantage estimator for stable training signals), and "Train-Deploy Alignment" techniques to improve training efficiency and robustness.

2. **TRANSFER_LEARNING**: The paper addresses distributional shifts between human demonstration distribution, learned inductive bias, and test-time execution distribution - a core transfer learning problem of adapting models across different data distributions.

3. **OPEN_SOURCE**: The paper explicitly states "Code, data and models will be released to facilitate the community," indicating a commitment to open-source contribution.
[13.02.2026 09:38] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "TRANSFER_LEARNING", "OPEN_SOURCE"]


**Justification:**

1. **OPTIMIZATION**: The paper proposes optimization methods including "Model Arithmetic" (weight-space merging strategy), "Stage Advantage" (advantage estimator for stable training signals), and "Train-Deploy Alignment" techniques to improve training efficiency and robustness.

2. **TRANSFER_LEARNING**: The paper addresses distributional shifts between human demonstration distribution, learned inductive bias, and test-time execution distribution - a core transfer learning problem of adapting models across different data distributions.

3. **OPEN_SOURCE**: The paper explicitly states "Code, data and models will be released to facilitate the community," indicating a commitment to open-source contribution.
[13.02.2026 09:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new framework called œá_{0} for improving the reliability of robotic manipulation tasks over long periods. It addresses the issue of distributional shifts that occur between training and real-world execution, which can lead to errors in multi-stage tasks. The framework incorporates three key components: Model Arithmetic for merging diverse demonstration data, Stage Advantage for providing stable feedback during task execution, and Train-Deploy Alignment to ensure consistency between training and deployment environments. The results show that œá_{0} significantly outperforms existing methods in success rates while using fewer resources, demonstrating its effectiveness in real-world applications.","title":"Achieving Reliable Robotic Manipulation with Resource Efficiency"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new framework called œá_{0} for improving the reliability of robotic manipulation tasks over long periods. It addresses the issue of distributional shifts that occur between training and real-world execution, which can lead to errors in multi-stage tasks. The framework incorporates three key components: Model Arithmetic for merging diverse demonstration data, Stage Advantage for providing stable feedback during task execution, and Train-Deploy Alignment to ensure consistency between training and deployment environments. The results show that œá_{0} significantly outperforms existing methods in success rates while using fewer resources, demonstrating its effectiveness in real-world applications.', title='Achieving Reliable Robotic Manipulation with Resource Efficiency'))
[13.02.2026 09:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËµÑÊ∫êÈ´òÊïàÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥ÂàÜÂ∏ÉËΩ¨ÁßªÈóÆÈ¢òÔºå‰ª•ÂÆûÁé∞ÈïøÊúü‰ªªÂä°ÁöÑÂèØÈù†ÊÄß„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÁé∞ÂÆû‰∏ñÁïåÁöÑÈ≤ÅÊ£íÊÄßÁì∂È¢à‰∏ç‰ªÖ‰ªÖÂú®‰∫éËµÑÊ∫êËßÑÊ®°ÔºåËøòÂú®‰∫é‰∫∫Á±ªÁ§∫ËåÉÂàÜÂ∏É„ÄÅÁ≠ñÁï•Â≠¶‰π†ÁöÑÂΩíÁ∫≥ÂÅèÂ∑ÆÂíåÊµãËØïÊó∂ÊâßË°åÂàÜÂ∏É‰πãÈó¥ÁöÑÁ≥ªÁªü‰∏ç‰∏ÄËá¥ÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õ‰∏ç‰∏ÄËá¥ÊÄßÔºåÊàë‰ª¨ÊèêÂá∫‰∫Üœá_{0}Ê°ÜÊû∂ÔºåÂåÖÂê´Ê®°ÂûãÁÆóÊúØ„ÄÅÈò∂ÊÆµ‰ºòÂäø‰º∞ËÆ°ÂíåËÆ≠ÁªÉ-ÈÉ®ÁΩ≤ÂØπÈΩêÁ≠âÊäÄÊúØÊîØÊü±„ÄÇÂÆûÈ™åË°®ÊòéÔºåœá_{0}Âú®ÊàêÂäüÁéá‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÈ´òÂèØÈù†ÊÄßÁöÑËá™‰∏ªÊìç‰ΩúËÉΩÂäõ„ÄÇ","title":"È´òÊïàÊú∫Âô®‰∫∫Êìç‰ΩúÔºåË∑®Ë∂äÂàÜÂ∏ÉËΩ¨ÁßªÁöÑÊåëÊàò"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËµÑÊ∫êÈ´òÊïàÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥ÂàÜÂ∏ÉËΩ¨ÁßªÈóÆÈ¢òÔºå‰ª•ÂÆûÁé∞ÈïøÊúü‰ªªÂä°ÁöÑÂèØÈù†ÊÄß„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÁé∞ÂÆû‰∏ñÁïåÁöÑÈ≤ÅÊ£íÊÄßÁì∂È¢à‰∏ç‰ªÖ‰ªÖÂú®‰∫éËµÑÊ∫êËßÑÊ®°ÔºåËøòÂú®‰∫é‰∫∫Á±ªÁ§∫ËåÉÂàÜÂ∏É„ÄÅÁ≠ñÁï•Â≠¶‰π†ÁöÑÂΩíÁ∫≥ÂÅèÂ∑ÆÂíåÊµãËØïÊó∂ÊâßË°åÂàÜÂ∏É‰πãÈó¥ÁöÑÁ≥ªÁªü‰∏ç‰∏ÄËá¥ÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õ‰∏ç‰∏ÄËá¥ÊÄßÔºåÊàë‰ª¨ÊèêÂá∫‰∫Üœá_{0}Ê°ÜÊû∂ÔºåÂåÖÂê´Ê®°ÂûãÁÆóÊúØ„ÄÅÈò∂ÊÆµ‰ºòÂäø‰º∞ËÆ°ÂíåËÆ≠ÁªÉ-ÈÉ®ÁΩ≤ÂØπÈΩêÁ≠âÊäÄÊúØÊîØÊü±„ÄÇÂÆûÈ™åË°®ÊòéÔºåœá_{0}Âú®ÊàêÂäüÁéá‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂ±ïÁ§∫‰∫ÜÈ´òÂèØÈù†ÊÄßÁöÑËá™‰∏ªÊìç‰ΩúËÉΩÂäõ„ÄÇ', title='È´òÊïàÊú∫Âô®‰∫∫Êìç‰ΩúÔºåË∑®Ë∂äÂàÜÂ∏ÉËΩ¨ÁßªÁöÑÊåëÊàò'))
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#small_models", "#training", "#architecture", "#long_context", "#optimization", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —É–ª—å—Ç—Ä–∞-–¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "MiniCPM-SALA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ—Ç–æ—Ä–∞—è –æ
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "üõ£Ô∏è", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è: –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É —è–≤–Ω—ã–º –∏ —Å–∫—Ä—ã—Ç—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏", "desc": "ThinkRouter ‚Äî —ç—Ç–æ –º–µ—Ö–∞–Ω–∏–∑–º –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –º–µ–∂–¥—É –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#training", "#agents"], "emoji": "üß†", "ru": {"title": "–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–∏—Ä–æ–≤ —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –∫–æ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Dreaming in Code (DiCode) ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –≤ –∫–æ—Ç–æ—Ä–æ–º –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π 
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–°–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–Ω–æ–≥–æ—Ç–æ–∫–µ–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ —Å–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#rlhf", "#training", "#alignment", "#open_source", "#small_models", "#dataset", "#optimization"], "emoji": "üé®", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–æ–π –º–æ—â—å—é: 5B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±–≥–æ–Ω—è—é—Ç 80B –≤ –∫–∞—á–µ—Å—Ç–≤–µ", "desc": "DeepGen 1.0 ‚Äî —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è 5-–º–∏–ª–ª–∏–∞—Ä–¥–Ω–∞
[13.02.2026 09:38] Using data from previous issue: {"categories": [], "emoji": "üí∞", "ru": {"title": "–≠–∫–æ–Ω–æ–º–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –Ω–∞–º–µ—Ä–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ INTENT –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ—à–∞—é—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–Ω–µ—à–Ω–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏ —Å—Ç—Ä–æ–≥–æ–º –±—é
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#open_source", "#benchmark", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–∏—Ç–∏–∫", "desc": "MolmoSpaces –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Ç–∫—Ä—ã—Ç—É—é —ç–∫–æ—Å–∏—Å—Ç–µ–º—É —Å –±–æ–ª–µ–µ —á–µ–º 230 —Ç—ã—Å—è—á–∞–º–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#audio", "#open_source", "#low_resource", "#training", "#architecture", "#multilingual"], "emoji": "üéôÔ∏è", "ru": {"title": "–ü–æ—Ç–æ–∫–æ–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –≤ –ø–æ–ª—Å–µ–∫—É–Ω–¥—ã", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Voxtral Realtime ‚Äî –ø–æ—Ç–æ–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏, –æ–±—É—á–µ–Ω–Ω–∞—è end-to-en
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#reasoning", "#math", "#science", "#open_source", "#benchmark", "#training"], "emoji": "üß¨", "ru": {"title": "–ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —Å–∞–º–∞ —Å–µ–±—è –ø—Ä–æ–≤–µ—Ä—è—Ç—å: –∫–æ—ç–≤–æ–ª—é—Ü–∏—è —Ä–µ—à–∞—Ç–µ–ª—è –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "Sci-CoE –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å–∏—Å—Ç–µ–º—É –Ω–∞—É—á–Ω–æ–≥–æ —Å–æ-—ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–≥
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#alignment", "#transfer_learning"], "emoji": "üéØ", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ –æ—Ü–µ–Ω–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç P-GenRM ‚Äî –ø–µ—Ä–≤—É—é –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –±–æ–ª—å—à
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#rl", "#benchmark", "#optimization", "#agents", "#reasoning"], "emoji": "‚è±Ô∏è", "ru": {"title": "–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –ø–æ–¥ –¥–∞–≤–ª–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏: –∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã –º–µ–∂–¥—É —Ä–∞–∑—É–º–æ–º, —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å—é", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Gaia2 ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ 
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#multimodal", "#training", "#data"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ScalSelect ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã—Ö –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ 
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#transfer_learning", "#3d", "#architecture", "#synthetic", "#benchmark", "#multimodal", "#agents", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å Vision-Language-Action (VLA)
[13.02.2026 09:38] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Å —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –±–µ–∑ —Å—É–ø–µ—Ä–∫–ª–∞—Å—Ç–µ—Ä–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ SPES ‚Äî –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π mixture-of-experts, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –Ω–∞ GPU –ø—É—Ç—ë–º
[13.02.2026 09:38] Querying the API.
[13.02.2026 09:38] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A new benchmark dataset called ExStrucTiny is introduced for structured information extraction from document images, addressing limitations of existing datasets and evaluating vision-language models on diverse document types and flexible schemas.  					AI-generated summary 				 Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents.
[13.02.2026 09:38] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ExStrucTiny –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∑–∞–¥–∞—á–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π, –≤—ã–¥–µ–ª–µ–Ω–∏—è –æ—Ç–Ω–æ—à–µ–Ω–∏–π –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –æ—Ö–≤–∞—Ç—ã–≤–∞—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –≥–∏–±–∫–∏–µ —Å—Ö–µ–º—ã. –ê–≤—Ç–æ—Ä—ã –æ—Ü–µ–Ω–∏–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ vision-language –º–æ–¥–µ–ª–∏ –∏ –≤—ã—è–≤–∏–ª–∏ –∫–ª—é—á–µ–≤—ã–µ –≤—ã–∑–æ–≤—ã, –≤–∫–ª—é—á–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—é –∫ –Ω–æ–≤—ã–º —Å—Ö–µ–º–∞–º –∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–æ–≤. –†–∞–±–æ—Ç–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ –æ–±–æ–±—â–∞—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.",
  "emoji": "üìã",
  "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–∏–±–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
}
```
[13.02.2026 09:38] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new benchmark dataset called ExStrucTiny is introduced for structured information extraction from document images, addressing limitations of existing datasets and evaluating vision-language models on diverse document types and flexible schemas.  					AI-generated summary 				 Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents."

[13.02.2026 09:38] Response: ```python
["DATASET", "BENCHMARK", "CV", "MULTIMODAL"]
```
[13.02.2026 09:38] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A new benchmark dataset called ExStrucTiny is introduced for structured information extraction from document images, addressing limitations of existing datasets and evaluating vision-language models on diverse document types and flexible schemas.  					AI-generated summary 				 Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents."

[13.02.2026 09:38] Response: ```python
['SYNTHETIC', 'OPEN_SOURCE']
```
[13.02.2026 09:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces ExStrucTiny, a new benchmark dataset designed for structured information extraction from document images. This dataset aims to overcome the limitations of existing datasets by providing a diverse range of document types and flexible extraction schemas. It combines elements of Key Entity Extraction, Relation Extraction, and Visual Question Answering to facilitate comprehensive evaluation of Vision Language Models. The authors analyze the performance of various models on this dataset, identifying challenges related to schema adaptation and answer localization, ultimately aiming to enhance structured information extraction capabilities in AI.","title":"ExStrucTiny: A New Frontier for Structured Information Extraction"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces ExStrucTiny, a new benchmark dataset designed for structured information extraction from document images. This dataset aims to overcome the limitations of existing datasets by providing a diverse range of document types and flexible extraction schemas. It combines elements of Key Entity Extraction, Relation Extraction, and Visual Question Answering to facilitate comprehensive evaluation of Vision Language Models. The authors analyze the performance of various models on this dataset, identifying challenges related to schema adaptation and answer localization, ultimately aiming to enhance structured information extraction capabilities in AI.', title='ExStrucTiny: A New Frontier for Structured Information Extraction'))
[13.02.2026 09:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜExStrucTinyÔºåÊó®Âú®‰ªéÊñáÊ°£ÂõæÂÉè‰∏≠ÊèêÂèñÁªìÊûÑÂåñ‰ø°ÊÅØÔºåËß£ÂÜ≥Áé∞ÊúâÊï∞ÊçÆÈõÜÁöÑÂ±ÄÈôêÊÄß„ÄÇËØ•Êï∞ÊçÆÈõÜËØÑ‰º∞‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÊ†∑ÂåñÊñáÊ°£Á±ªÂûãÂíåÁÅµÊ¥ªÊ®°Âºè‰∏ãÁöÑË°®Áé∞„ÄÇÈÄöËøáÁªìÂêàÊâãÂä®ÂíåÂêàÊàêÁöÑ‰∫∫Â∑•È™åËØÅÊ†∑Êú¨ÔºåExStrucTinyË¶ÜÁõñ‰∫ÜÊõ¥Â§öÁöÑÊñáÊ°£Á±ªÂûãÂíåÊèêÂèñÂú∫ÊôØ„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫ÜÂºÄÊîæÂíåÂ∞ÅÈó≠ÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®ËØ•Âü∫ÂáÜ‰∏äÁöÑË°®Áé∞ÔºåÂº∫Ë∞É‰∫ÜÊ®°ÂºèÈÄÇÂ∫î„ÄÅÊü•ËØ¢‰∏çÊòéÁ°ÆÂíåÁ≠îÊ°àÂÆö‰ΩçÁ≠âÊåëÊàò„ÄÇ","title":"ExStrucTinyÔºöÊñáÊ°£‰ø°ÊÅØÊèêÂèñÁöÑÊñ∞Âü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜExStrucTinyÔºåÊó®Âú®‰ªéÊñáÊ°£ÂõæÂÉè‰∏≠ÊèêÂèñÁªìÊûÑÂåñ‰ø°ÊÅØÔºåËß£ÂÜ≥Áé∞ÊúâÊï∞ÊçÆÈõÜÁöÑÂ±ÄÈôêÊÄß„ÄÇËØ•Êï∞ÊçÆÈõÜËØÑ‰º∞‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÊ†∑ÂåñÊñáÊ°£Á±ªÂûãÂíåÁÅµÊ¥ªÊ®°Âºè‰∏ãÁöÑË°®Áé∞„ÄÇÈÄöËøáÁªìÂêàÊâãÂä®ÂíåÂêàÊàêÁöÑ‰∫∫Â∑•È™åËØÅÊ†∑Êú¨ÔºåExStrucTinyË¶ÜÁõñ‰∫ÜÊõ¥Â§öÁöÑÊñáÊ°£Á±ªÂûãÂíåÊèêÂèñÂú∫ÊôØ„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫ÜÂºÄÊîæÂíåÂ∞ÅÈó≠ÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®ËØ•Âü∫ÂáÜ‰∏äÁöÑË°®Áé∞ÔºåÂº∫Ë∞É‰∫ÜÊ®°ÂºèÈÄÇÂ∫î„ÄÅÊü•ËØ¢‰∏çÊòéÁ°ÆÂíåÁ≠îÊ°àÂÆö‰ΩçÁ≠âÊåëÊàò„ÄÇ', title='ExStrucTinyÔºöÊñáÊ°£‰ø°ÊÅØÊèêÂèñÁöÑÊñ∞Âü∫ÂáÜ'))
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#security", "#benchmark", "#reasoning", "#rl"], "emoji": "üîç", "ru": {"title": "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–º–∏–Ω–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∫–æ–Ω—Ç–∞–º–∏–Ω–∞—Ü–∏–∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –≤—ã–∑–≤–∞–Ω–Ω–æ–π –æ–±—É—á–µ–Ω–∏–µ–º —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#long_context", "#hallucinations", "#audio", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º–æ—Å—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ç–æ—á–Ω–æ–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", "desc": "MuRGAt ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ
[13.02.2026 09:38] Querying the API.
[13.02.2026 09:38] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

RISE is a robotic reinforcement learning framework that uses a compositional world model to predict multi-view futures and evaluate imagined outcomes, enabling policy improvement through virtual interactions rather than physical trials.  					AI-generated summary 				 Despite the sustained scaling on model capacity and data acquisition, Vision-Language-Action (VLA) models remain brittle in contact-rich and dynamic manipulation tasks, where minor execution deviations can compound into failures. While reinforcement learning (RL) offers a principled path to robustness, on-policy RL in the physical world is constrained by safety risk, hardware cost, and environment reset. To bridge this gap, we present RISE, a scalable framework of robotic reinforcement learning via imagination. At its core is a Compositional World Model that (i) predicts multi-view future via a controllable dynamics model, and (ii) evaluates imagined outcomes with a progress value model, producing informative advantages for the policy improvement. Such compositional design allows state and value to be tailored by best-suited yet distinct architectures and objectives. These components are integrated into a closed-loop self-improving pipeline that continuously generates imaginary rollouts, estimates advantages, and updates the policy in imaginary space without costly physical interaction. Across three challenging real-world tasks, RISE yields significant improvement over prior art, with more than +35% absolute performance increase in dynamic brick sorting, +45% for backpack packing, and +35% for box closing, respectively.
[13.02.2026 09:38] Response: ```json
{
  "desc": "RISE ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –≤–æ–æ–±—Ä–∞–∂–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–µ–π—Å—Ç–≤–∏–π. –í–º–µ—Å—Ç–æ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Ä–æ–±–æ—Ç–∞—Ö, —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤ –≤–æ–æ–±—Ä–∞–∂–∞–µ–º–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –∏—Å–ø–æ–ª—å–∑—É—è —É–ø—Ä–∞–≤–ª—è–µ–º—É—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å –∏ –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –ø–æ–ª–∏—Ç–∏–∫–∏. –¢–∞–∫–æ–π –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –∑–Ω–∞—á–µ–Ω–∏—è, –∏–∑–±–µ–≥–∞—è –∑–∞—Ç—Ä–∞—Ç –Ω–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ —Ä–∏—Å–∫–∏ –¥–ª—è –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏, –≤–∫–ª—é—á–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É –∫–∏—Ä–ø–∏—á–µ–π (+35%), —É–ø–∞–∫–æ–≤–∫—É —Ä—é–∫–∑–∞–∫–∞ (+45%) –∏ –∑–∞–∫—Ä—ã—Ç–∏–µ –∫–æ—Ä–æ–±–æ–∫ (+35%).",
  "emoji": "ü§ñ",
  "title": "–í–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –æ–ø—ã—Ç–∞: –æ–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞ –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ"
}
```
[13.02.2026 09:38] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RISE is a robotic reinforcement learning framework that uses a compositional world model to predict multi-view futures and evaluate imagined outcomes, enabling policy improvement through virtual interactions rather than physical trials.  					AI-generated summary 				 Despite the sustained scaling on model capacity and data acquisition, Vision-Language-Action (VLA) models remain brittle in contact-rich and dynamic manipulation tasks, where minor execution deviations can compound into failures. While reinforcement learning (RL) offers a principled path to robustness, on-policy RL in the physical world is constrained by safety risk, hardware cost, and environment reset. To bridge this gap, we present RISE, a scalable framework of robotic reinforcement learning via imagination. At its core is a Compositional World Model that (i) predicts multi-view future via a controllable dynamics model, and (ii) evaluates imagined outcomes with a progress value model, producing informative advantages for the policy improvement. Such compositional design allows state and value to be tailored by best-suited yet distinct architectures and objectives. These components are integrated into a closed-loop self-improving pipeline that continuously generates imaginary rollouts, estimates advantages, and updates the policy in imaginary space without costly physical interaction. Across three challenging real-world tasks, RISE yields significant improvement over prior art, with more than +35% absolute performance increase in dynamic brick sorting, +45% for backpack packing, and +35% for box closing, respectively."

[13.02.2026 09:38] Response: ```python
["RL", "ROBOTICS", "TRAINING", "MULTIMODAL"]
```
[13.02.2026 09:38] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RISE is a robotic reinforcement learning framework that uses a compositional world model to predict multi-view futures and evaluate imagined outcomes, enabling policy improvement through virtual interactions rather than physical trials.  					AI-generated summary 				 Despite the sustained scaling on model capacity and data acquisition, Vision-Language-Action (VLA) models remain brittle in contact-rich and dynamic manipulation tasks, where minor execution deviations can compound into failures. While reinforcement learning (RL) offers a principled path to robustness, on-policy RL in the physical world is constrained by safety risk, hardware cost, and environment reset. To bridge this gap, we present RISE, a scalable framework of robotic reinforcement learning via imagination. At its core is a Compositional World Model that (i) predicts multi-view future via a controllable dynamics model, and (ii) evaluates imagined outcomes with a progress value model, producing informative advantages for the policy improvement. Such compositional design allows state and value to be tailored by best-suited yet distinct architectures and objectives. These components are integrated into a closed-loop self-improving pipeline that continuously generates imaginary rollouts, estimates advantages, and updates the policy in imaginary space without costly physical interaction. Across three challenging real-world tasks, RISE yields significant improvement over prior art, with more than +35% absolute performance increase in dynamic brick sorting, +45% for backpack packing, and +35% for box closing, respectively."

[13.02.2026 09:38] Response: ```python
['OPTIMIZATION', 'REASONING']
```

**Justification:**

- **OPTIMIZATION**: The paper presents RISE, a framework focused on improving training efficiency through reinforcement learning via imagination/simulation rather than physical trials. This directly addresses optimization of the learning process by reducing costly physical interactions and enabling policy improvement through virtual interactions.

- **REASONING**: The framework involves evaluating imagined outcomes and estimating advantages to guide policy improvement, which relates to enhancing decision-making and logical reasoning capabilities in robotic systems for complex manipulation tasks.
[13.02.2026 09:38] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "REASONING"]


**Justification:**

- **OPTIMIZATION**: The paper presents RISE, a framework focused on improving training efficiency through reinforcement learning via imagination/simulation rather than physical trials. This directly addresses optimization of the learning process by reducing costly physical interactions and enabling policy improvement through virtual interactions.

- **REASONING**: The framework involves evaluating imagined outcomes and estimating advantages to guide policy improvement, which relates to enhancing decision-making and logical reasoning capabilities in robotic systems for complex manipulation tasks.
[13.02.2026 09:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RISE is a robotic reinforcement learning framework that enhances policy learning by using a compositional world model to simulate future scenarios. This model predicts multiple views of potential outcomes and evaluates them to improve decision-making without needing physical trials. By leveraging a closed-loop system, RISE generates imaginary rollouts and updates policies based on these simulations, which reduces risks and costs associated with real-world interactions. The framework has shown significant performance improvements in various manipulation tasks, demonstrating its effectiveness in dynamic environments.","title":"Reinforcement Learning Through Imagination: RISE Framework"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RISE is a robotic reinforcement learning framework that enhances policy learning by using a compositional world model to simulate future scenarios. This model predicts multiple views of potential outcomes and evaluates them to improve decision-making without needing physical trials. By leveraging a closed-loop system, RISE generates imaginary rollouts and updates policies based on these simulations, which reduces risks and costs associated with real-world interactions. The framework has shown significant performance improvements in various manipulation tasks, demonstrating its effectiveness in dynamic environments.', title='Reinforcement Learning Through Imagination: RISE Framework'))
[13.02.2026 09:38] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RISEÊòØ‰∏Ä‰∏™Êú∫Âô®‰∫∫Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂà©Áî®ÁªÑÂêà‰∏ñÁïåÊ®°ÂûãÊù•È¢ÑÊµãÂ§öËßÜËßíÁöÑÊú™Êù•ÔºåÂπ∂ËØÑ‰º∞ÊÉ≥Ë±°ÁöÑÁªìÊûúÔºå‰ªéËÄåÈÄöËøáËôöÊãü‰∫§‰∫íËÄåÈùûÁâ©ÁêÜËØïÈ™åÊù•ÊîπËøõÁ≠ñÁï•„ÄÇËØ•Ê°ÜÊû∂ÁöÑÊ†∏ÂøÉÊòØ‰∏Ä‰∏™ÂèØÊéßÁöÑÂä®ÊÄÅÊ®°ÂûãÔºåËÉΩÂ§üÈ¢ÑÊµãÊú™Êù•Áä∂ÊÄÅÔºåÂπ∂ÈÄöËøáËøõÂ±ïÂÄºÊ®°ÂûãËØÑ‰º∞ÊÉ≥Ë±°ÁöÑÁªìÊûúÔºå‰∏∫Á≠ñÁï•ÊîπËøõÊèê‰æõÊúâ‰ª∑ÂÄºÁöÑ‰ø°ÊÅØ„ÄÇRISEÁöÑÁªÑÂêàËÆæËÆ°‰ΩøÂæóÁä∂ÊÄÅÂíå‰ª∑ÂÄºÂèØ‰ª•ÈÄöËøáÊúÄÂêàÈÄÇÁöÑÊû∂ÊûÑÂíåÁõÆÊ†áËøõË°åÂÆöÂà∂„ÄÇÈÄöËøáÂú®‰∏â‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂÆûÈôÖ‰ªªÂä°‰∏≠ÊµãËØïÔºåRISEÂú®Âä®ÊÄÅÁ†ñÂùóÊéíÂ∫è„ÄÅËÉåÂåÖÊâìÂåÖÂíåÁÆ±Â≠êÂÖ≥Èó≠Á≠â‰ªªÂä°‰∏äÂùáÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩÔºåÂàÜÂà´ÊèêÂçáË∂ÖËøá35%Âíå45%„ÄÇ","title":"ÈÄöËøáÊÉ≥Ë±°ÊèêÂçáÊú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÊú™Êù•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RISEÊòØ‰∏Ä‰∏™Êú∫Âô®‰∫∫Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂà©Áî®ÁªÑÂêà‰∏ñÁïåÊ®°ÂûãÊù•È¢ÑÊµãÂ§öËßÜËßíÁöÑÊú™Êù•ÔºåÂπ∂ËØÑ‰º∞ÊÉ≥Ë±°ÁöÑÁªìÊûúÔºå‰ªéËÄåÈÄöËøáËôöÊãü‰∫§‰∫íËÄåÈùûÁâ©ÁêÜËØïÈ™åÊù•ÊîπËøõÁ≠ñÁï•„ÄÇËØ•Ê°ÜÊû∂ÁöÑÊ†∏ÂøÉÊòØ‰∏Ä‰∏™ÂèØÊéßÁöÑÂä®ÊÄÅÊ®°ÂûãÔºåËÉΩÂ§üÈ¢ÑÊµãÊú™Êù•Áä∂ÊÄÅÔºåÂπ∂ÈÄöËøáËøõÂ±ïÂÄºÊ®°ÂûãËØÑ‰º∞ÊÉ≥Ë±°ÁöÑÁªìÊûúÔºå‰∏∫Á≠ñÁï•ÊîπËøõÊèê‰æõÊúâ‰ª∑ÂÄºÁöÑ‰ø°ÊÅØ„ÄÇRISEÁöÑÁªÑÂêàËÆæËÆ°‰ΩøÂæóÁä∂ÊÄÅÂíå‰ª∑ÂÄºÂèØ‰ª•ÈÄöËøáÊúÄÂêàÈÄÇÁöÑÊû∂ÊûÑÂíåÁõÆÊ†áËøõË°åÂÆöÂà∂„ÄÇÈÄöËøáÂú®‰∏â‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂÆûÈôÖ‰ªªÂä°‰∏≠ÊµãËØïÔºåRISEÂú®Âä®ÊÄÅÁ†ñÂùóÊéíÂ∫è„ÄÅËÉåÂåÖÊâìÂåÖÂíåÁÆ±Â≠êÂÖ≥Èó≠Á≠â‰ªªÂä°‰∏äÂùáÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩÔºåÂàÜÂà´ÊèêÂçáË∂ÖËøá35%Âíå45%„ÄÇ', title='ÈÄöËøáÊÉ≥Ë±°ÊèêÂçáÊú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÊú™Êù•'))
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#interpretability", "#open_source"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å—é —á–µ—Ä–µ–∑ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ —Å–º–µ—Å–∏", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Neural Additive Experts (NAE) ‚Äî –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫—É—é –¥–∏–ª–µ–º–º—É –º–µ–∂–¥—É –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#training", "#benchmark", "#multimodal", "#rl", "#open_source", "#optimization", "#dataset", "#reasoning"], "emoji": "üé®", "ru": {"title": "–ù–∞—É—á–∏—Ç—å –ò–ò –ø–æ–Ω–∏–º–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ —Å–º—ã—Å–ª—ã: –º–µ—Ç–∞—Ñ–æ—Ä—ã –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "MetaphorStar ‚Äî —ç—Ç–æ –ø–µ—Ä–≤—ã–π end-to-end —Ñ—Ä–µ–π–º
[13.02.2026 09:38] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–¢–æ—á–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å —Ä–µ–¥–∫–∏–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–ª—é—á–µ–≤—ã–º–∏ –∫–∞–¥—Ä–∞–º–∏", "desc": "PISCO ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—Å—Ç–∞–≤–ª—è—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –≤–∏–¥–µ–æ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã
[13.02.2026 09:38] Renaming data file.
[13.02.2026 09:38] Renaming previous data. hf_papers.json to ./d/2026-02-13.json
[13.02.2026 09:38] Saving new data file.
[13.02.2026 09:38] Generating page.
[13.02.2026 09:38] Renaming previous page.
[13.02.2026 09:38] Renaming previous data. index.html to ./d/2026-02-13.html
[13.02.2026 09:38] Writing result.
[13.02.2026 09:38] Renaming log file.
[13.02.2026 09:38] Renaming previous data. log.txt to ./logs/2026-02-13_last_log.txt
