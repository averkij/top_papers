[13.02.2026 09:38] Read previous papers.
[13.02.2026 09:38] Generating top page (month).
[13.02.2026 09:38] Writing top page (month).
[13.02.2026 10:33] Read previous papers.
[13.02.2026 10:33] Get feed.
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09877
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10934
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12125
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12099
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12056
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12280
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12036
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11748
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11731
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12153
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09021
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05548
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12092
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11075
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09070
[13.02.2026 10:33] Extract page data from URL. URL: https://huggingface.co/papers/2602.10106
[13.02.2026 10:33] Extract page data from URL. URL: https://huggingface.co/papers/2602.05827
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12205
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11761
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11683
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11337
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08194
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12262
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12116
[13.02.2026 10:33] Extract page data from URL. URL: https://huggingface.co/papers/2602.11733
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11541
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11298
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12164
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11964
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11636
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11598
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11543
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11509
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10575
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12203
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11792
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10585
[13.02.2026 10:33] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08277
[13.02.2026 10:33] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.02.2026 10:33] No deleted papers detected.
[13.02.2026 10:33] Downloading and parsing papers (pdf, html). Total: 38.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.09877.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.09877.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.09877.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.10934.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.10934.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.10934.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.12125.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.12125.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.12125.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.12099.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.12099.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.12099.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.12056.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.12056.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.12056.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.12280.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.12280.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.12280.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.12036.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.12036.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.12036.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11748.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.11748.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.11748.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11731.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.11731.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.11731.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.12153.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.12153.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.12153.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.09021.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.09021.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.09021.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.05548.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.05548.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.05548.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.12092.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.12092.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.12092.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11075.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.11075.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.11075.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.09070.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.09070.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.09070.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.10106.
[13.02.2026 10:33] Downloading paper 2602.10106 from https://arxiv.org/pdf/2602.10106v1...
[13.02.2026 10:33] Extracting affiliations from text.
[13.02.2026 10:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 0 1 ] . [ 1 6 0 1 0 1 . 2 0 6 2 : r EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration Modi Shi2,3 Shijia Peng4 Jin Chen2 Haoran Jiang2 Yinghui Li1,4 Di Huang3 Ping Luo1 Hongyang Li1 Li Chen1 1 The University of Hong Kong 2 Shanghai Innovation Institute 3 Beihang University 4 Kinetix AI Equal Contribution Project lead https://opendrivelab.com/EgoHumanoid Fig. 1: Introducing EGOHUMANOID, the first investigation on human-to-humanoid transfer for whole-body loco-manipulation. Robot teleoperation data collection is constrained to laboratory environment due to hardware and safety limitations, while inthe-wild human demonstrations offer scalable diversity in objects, scenes, lighting, and viewpoints. Our alignment pipeline bridges the embodiment gap through view and action alignment, enabling vision-language-action (VLA) co-training on both data sources. Real-world loco-manipulation deployment validates that egocentric human demonstrations invigorate generalization without scene-specific robot data, outperforming robot-only baselines by 51% with consistent scaling behavior. AbstractHuman demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robotarm manipulation, its potential for the more challenging, datahungry problem of humanoid loco-manipulation remains largely unexplored. We present EGOHUMANOID, the first framework to co-train vision-language-action policy using abundant egocentric human demonstrations together with limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce systematic alignment pipeline spanning from hardware design to data processing. portable system for scalable human data collection is developed, and "
[13.02.2026 10:33] Response: ```python
[
    "The University of Hong Kong",
    "Shanghai Innovation Institute",
    "Beihang University",
    "Kinetix AI"
]
```
[13.02.2026 10:33] Deleting PDF ./assets/pdf/2602.10106.pdf.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.05827.
[13.02.2026 10:33] Downloading paper 2602.05827 from https://arxiv.org/pdf/2602.05827v1...
[13.02.2026 10:33] Extracting affiliations from text.
[13.02.2026 10:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation Hai Zhang* Siqi Liang* Li Chen Yuxian Li Yukuan Xu Yichao Zhong Fu Zhang Hongyang Li The University of Hong Kong https://github.com/OpenDriveLab/SparseVideoNav 6 2 0 2 5 ] . [ 1 7 2 8 5 0 . 2 0 6 2 : r Fig. 1: In this work, we investigate the beyond-the-view navigation task in the real world, where agents must locate distant, unseen targets without step-by-step guidance. Traditional large language model-based methods suffer from short-horizon supervision, leading to short-sighted behaviors, e.g., unexpected turning and dead-end trapping. We address this challenge from new perspective, by introducing the video generation model to this field for the first time. The whole training pipeline is sparsified further for the sake of extended prediction horizon and computational efficiency. AbstractWhy must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and stepby-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on shorthorizon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latenc"
[13.02.2026 10:33] Response: ```python
["The University of Hong Kong"]
```
[13.02.2026 10:33] Deleting PDF ./assets/pdf/2602.05827.pdf.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.12205.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.12205.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.12205.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11761.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.11761.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.11761.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11683.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.11683.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.11683.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11337.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.11337.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.11337.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.08194.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.08194.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.08194.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.12262.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.12262.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.12262.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.12116.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.12116.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.12116.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11733.
[13.02.2026 10:33] Downloading paper 2602.11733 from https://arxiv.org/pdf/2602.11733v1...
[13.02.2026 10:33] Extracting affiliations from text.
[13.02.2026 10:33] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Adapting Vision-Language Models for E-commerce Understanding at Scale Matteo Nulli1,2, Vladimir Orshulevich1, Tala Bazazo1, Christian Herold1, Michael Kozielski1, Marcin Mazur1, Szymon Tuzel1, Cees G. M. Snoek2, Seyyed Hadi Hashemi1, Omar Javed1, Yannick Versley1 and Shahram Khadivi1 1eBay Inc., 2University of Amsterdam {mnulli, tbazazo}@ebay.com 6 2 0 2 2 1 ] . [ 1 3 3 7 1 1 . 2 0 6 2 : r a "
[13.02.2026 10:33] Response: ```python
['eBay Inc.', 'University of Amsterdam']
```
[13.02.2026 10:33] Deleting PDF ./assets/pdf/2602.11733.pdf.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11541.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.11541.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.11541.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11298.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.11298.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.11298.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.12164.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.12164.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.12164.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11964.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.11964.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.11964.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11636.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.11636.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.11636.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11598.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.11598.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.11598.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11543.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.11543.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.11543.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11509.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.11509.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.11509.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.10575.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.10575.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.10575.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.12203.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.12203.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.12203.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.11792.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.11792.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.11792.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.10585.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.10585.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.10585.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Downloading and parsing paper https://huggingface.co/papers/2602.08277.
[13.02.2026 10:33] Extra JSON file exists (./assets/json/2602.08277.json), skip PDF parsing.
[13.02.2026 10:33] Paper image links file exists (./assets/img_data/2602.08277.json), skip HTML parsing.
[13.02.2026 10:33] Success.
[13.02.2026 10:33] Enriching papers with extra data.
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 0. Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution.  					AI-generated summary 				 The emergence of multi-agent systems built from large language models (LLMs)...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 1. A fully end-to-end Transformer-based audio tokenizer architecture achieves high-fidelity reconstruction across diverse audio domains and enables superior text-to-speech and automatic speech recognition performance.  					AI-generated summary 				 Discrete audio tokenizers are fundamental to empoweri...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 2. On-policy distillation is extended through a generalized framework that introduces flexible reference models and reward scaling factors, demonstrating improved performance through reward extrapolation and reward correction techniques.  					AI-generated summary 				 On-policy distillation (OPD), whi...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 3. A vision-language-action model enhanced with world model-based reinforcement learning demonstrates improved performance and long-horizon execution capabilities for robotic manipulation tasks.  					AI-generated summary 				 Vision-language-action (VLA) models that directly predict multi-step action ...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 4. LawThinker is an autonomous legal research agent that uses an Explore-Verify-Memorize strategy with a DeepVerifier module to ensure accurate and procedurally compliant legal reasoning through dynamic verification of intermediate steps.  					AI-generated summary 				 Legal reasoning requires not onl...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 5. Progressive Semantic Illusions use a generative framework with dual-branch Score Distillation Sampling to create vector sketches that transform semantically through sequential stroke additions, achieving superior recognizability and illusion strength.  					AI-generated summary 				 Visual illusions...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 6. Composition-RL improves reasoning capabilities by automatically composing multiple problems into new verifiable questions for reinforcement learning training.  					AI-generated summary 				 Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR),...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 7. Models require in-context exploration capabilities to scale effectively at test time, but autoregressive generation faces exponential decay in sampling long sequences, which is addressed by a length-incentivized exploration method that improves performance on both in-domain and out-of-domain tasks. ...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 8. Visual reasoning is enhanced by reconstructing logical structures from compressed visual tokens through a DSL-based approach that generates deterministic visual proofs for verification.  					AI-generated summary 				 Existing multimodal large language models have achieved high-fidelity visual perce...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 9. Diffusion large language models enable parallel token generation and efficient reasoning enhancement through a voting technique that identifies and refines uncertain predictions across multiple samples.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) represent a new paradigm ...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 10. A resource-efficient robotic manipulation framework addresses distributional shifts through model arithmetic, stage-aware advantage estimation, and train-deploy alignment to achieve long-horizon task reliability.  					AI-generated summary 				 High-reliability long-horizon robotic manipulation has ...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 11. Asymmetric Group Relative Advantage Estimation addresses exploration and difficulty adaptation challenges in reinforcement learning with large language models by dynamically modulating exploration incentives and sample difficulty focus.  					AI-generated summary 				 Reinforcement Learning with Ver...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 12. DeepSight is an open-source project that integrates safety evaluation and diagnosis for large language and multimodal models, enabling white-box insights through unified protocols and specialized toolkits.  					AI-generated summary 				 As the development of Large Models (LMs) progresses rapidly, t...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 13. RISE is a robotic reinforcement learning framework that uses a compositional world model to predict multi-view futures and evaluate imagined outcomes, enabling policy improvement through virtual interactions rather than physical trials.  					AI-generated summary 				 Despite the sustained scaling o...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 14. NarraScore presents a hierarchical framework that uses frozen Vision-Language Models as affective sensors to generate coherent soundtracks for long-form videos by combining global semantic anchors with token-level adaptive modulation.  					AI-generated summary 				 Synthesizing coherent soundtracks...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 15. EgoHumanoid enables humanoid loco-manipulation through co-training vision-language-action policies using egocentric human demonstrations and limited robot data, addressing embodiment gaps via view and action alignment techniques.  					AI-generated summary 				 Human demonstrations offer rich enviro...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 16. Vision-language navigation systems traditionally require detailed instructions but can be improved by incorporating video generation models with sparse future planning for faster, more efficient real-world deployment.  					AI-generated summary 				 Why must vision-language navigation be bound to de...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 17. A lightweight 5B unified multimodal model achieves competitive performance through hierarchical feature extraction, learnable think tokens, and progressive training strategies including alignment pre-training, joint supervised fine-tuning, and reinforcement learning with MR-GRPO.  					AI-generated ...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 18. MiniCPM-SALA combines sparse and linear attention mechanisms in a hybrid architecture to enable efficient processing of ultra-long contexts while maintaining model performance and reducing training costs.  					AI-generated summary 				 The evolution of large language models (LLMs) towards applicati...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 19. ThinkRouter is a confidence-aware routing mechanism that improves reasoning efficiency by switching between discrete token and latent spaces based on model confidence, achieving better accuracy and faster generation.  					AI-generated summary 				 Recent work explores latent reasoning to improve re...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 20. MolmoSpaces presents an open ecosystem with diverse indoor environments and annotated objects for large-scale robot policy benchmarking across multiple tasks and simulators.  					AI-generated summary 				 Deploying robots at scale demands robustness to the long tail of everyday situations. The coun...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 21. Foundation models generate executable environment code to scaffold learning progress in open-ended worlds, enabling agents to acquire long-horizon skills through curriculum control.  					AI-generated summary 				 Open-ended learning frames intelligence as emerging from continual interaction with an...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 22. A trajectory self-distillation framework with direct discriminative optimization improves few-step decoding efficiency in diffusion large language models while maintaining generation quality.  					AI-generated summary 				 Diffusion large language models (DLLMs) have the potential to enable fast te...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 23. Personalized generative reward models address challenges in adapting language model responses to individual user preferences by using structured evaluation chains and dual-granularity scaling mechanisms for improved generalization and accuracy.  					AI-generated summary 				 Personalized alignment ...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 24. General-purpose Vision-Language Models can be effectively adapted for e-commerce applications through targeted techniques that enhance product understanding while maintaining broad multimodal capabilities.  					AI-generated summary 				 E-commerce product understanding demands by nature, strong mul...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 25. Budget-constrained tool-augmented agents use a hierarchical world model and intent-aware planning to optimize multi-step task completion under monetary constraints.  					AI-generated summary 				 We study budget-constrained tool-augmented agents, where a large language model must solve multi-step t...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 26. Voxtral Realtime is a streaming speech recognition model trained end-to-end for sub-second latency with performance matching offline systems.  					AI-generated summary 				 We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription qual...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 27. Sci-CoE is a two-stage scientific co-evolving framework that enables large language models to self-evolve as both solver and verifier through sparse-to-unsupervised learning transitions, improving scientific reasoning capabilities and evaluation system robustness.  					AI-generated summary 				 Lar...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 28. Gaia2 presents a benchmark for evaluating large language model agents in asynchronous, dynamic environments with temporal constraints and multi-agent collaboration, featuring a write-action verifier for reinforcement learning and revealing trade-offs between reasoning, efficiency, and robustness.  	...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 29. ScalSelect is a scalable training-free method for selecting representative multimodal data that achieves near-full-dataset performance with significantly reduced computational requirements.  					AI-generated summary 				 Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for adva...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 30. A unified Vision-Language-Action model with a hierarchical architecture combining semantic reasoning and continuous trajectory generation achieves state-of-the-art performance across multiple embodied navigation tasks.  					AI-generated summary 				 Embodied navigation has long been fragmented by t...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 31. A memory-efficient decentralized framework for training mixture-of-experts language models using sparse expert synchronization and expert-merging warm-up strategies.  					AI-generated summary 				 Pretraining large language models (LLMs) typically requires centralized clusters with thousands of hig...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 32. MuRGAt is a benchmark for evaluating fact-level multimodal attribution in complex reasoning tasks, requiring models to provide precise citations for their answers across video, audio, and other modalities.  					AI-generated summary 				 Multimodal large language models (MLLMs) are increasingly used...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 33. MetaphorStar, an end-to-end visual reinforcement learning framework, significantly enhances metaphor comprehension in images through a specialized dataset, RL method, and benchmark, achieving state-of-the-art performance on multiple visual reasoning tasks.  					AI-generated summary 				 Metaphorica...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 34. A new benchmark dataset called ExStrucTiny is introduced for structured information extraction from document images, addressing limitations of existing datasets and evaluating vision-language models on diverse document types and flexible schemas.  					AI-generated summary 				 Enterprise documents,...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 35. Reinforcement learning with verifiable rewards induces behavioral signatures that can be detected using a black-box method based on prompt generation diversity, outperforming existing contamination detection approaches.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 36. Neural Additive Experts combines multiple specialized networks with a dynamic gating mechanism to balance predictive accuracy and feature interpretability in machine learning models.  					AI-generated summary 				 The trade-off between interpretability and accuracy remains a core challenge in machi...
[13.02.2026 10:33] ********************************************************************************
[13.02.2026 10:33] Abstract 37. Video diffusion model PISCO enables precise instance insertion with sparse keyframe control through variable-information guidance and distribution-preserving temporal masking.  					AI-generated summary 				 The landscape of AI video generation is undergoing a pivotal shift: moving beyond general ge...
[13.02.2026 10:33] Read previous papers.
[13.02.2026 10:33] Generating reviews via LLM API.
[13.02.2026 10:33] Using data from previous issue: {"categories": ["#alignment", "#training", "#agents", "#security"], "emoji": "‚öñÔ∏è", "ru": {"title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–µ–π –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞, —á—Ç–æ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –Ω–µ –º–æ–≥—É—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –¥–æ—Å—Ç–∏—á—å 
[13.02.2026 10:33] Using data from previous issue: {"categories": ["#optimization", "#audio", "#training", "#architecture"], "emoji": "üéµ", "ru": {"title": "–°–∫–≤–æ–∑–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é –æ–¥–Ω–æ—Ä–æ–¥–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ CAT (Causal Audio Tokenizer with Transformer) ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–Ω–∞
[13.02.2026 10:33] Using data from previous issue: {"categories": ["#alignment", "#training", "#rl", "#optimization", "#reasoning"], "emoji": "üéì", "ru": {"title": "–£–º–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è: –∫–∞–∫ —É—á–µ–Ω–∏–∫ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —É—á–∏—Ç–µ–ª—è —á–µ—Ä–µ–∑ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è", "desc": "–†–∞–±–æ—Ç–∞ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ ¬´–Ω–∞ –ø–æ–ª–∏—Ç–∏–∫–µ¬ª (on-policy distillation), –ø—Ä–µ–¥–ª–∞–≥–∞—è –æ–±–æ–±—â—ë–Ω
[13.02.2026 10:33] Using data from previous issue: {"categories": ["#robotics", "#rl", "#cv", "#benchmark", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ú–∏—Ä–æ–≤–∞—è –º–æ–¥–µ–ª—å –∫–∞–∫ –æ—Å–Ω–æ–≤–∞ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è GigaBrain-0.5M*, –º–æ–¥–µ–ª—å Vision-Language-Action, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–µ–Ω–∞ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω
[13.02.2026 10:33] Using data from previous issue: {"categories": [], "emoji": "‚öñÔ∏è", "ru": {"title": "–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–≥–µ–Ω—Ç —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "LawThinker ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Explore-Verify-Memorize —Å –º–æ–¥—É–ª–µ–º DeepVerifier –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ –∏
[13.02.2026 10:33] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#multimodal"], "emoji": "üé®", "ru": {"title": "–û—Ç —É—Ç–∫–∏ –∫ –æ–≤—Ü–µ: –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∏–ª–ª—é–∑–∏–∏ —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —à—Ç—Ä–∏—Ö–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∏–ª–ª—é–∑–∏–π ‚Äî –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç—Å–∫–∏–∑–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–µ–Ω—è—é—Ç —Å–≤–æ—ë –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ 
[13.02.2026 10:33] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#rl", "#training", "#reasoning", "#optimization"], "emoji": "üß©", "ru": {"title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏—è –∑–∞–¥–∞—á –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Composition-RL ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –ª–æ–≥–∏—á–µ—Å–∫
[13.02.2026 10:33] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "üîç", "ru": {"title": "–ü–æ–æ—â—Ä–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤–æ –≤—Ä–µ–º—è –∞–≤—Ç–æ–º–∞
[13.02.2026 10:33] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#cv", "#interpretability", "#architecture", "#reasoning"], "emoji": "üî¨", "ru": {"title": "–û—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤—É —á–µ—Ä–µ–∑ –∫–æ–¥", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫
[13.02.2026 10:33] Using data from previous issue: {"categories": ["#reasoning", "#diffusion", "#open_source"], "emoji": "üó≥Ô∏è", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (dLLM), –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç—Ö–æ–¥—è—Ç –æ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –≥–µ–Ω–µ—Ä–∏
[13.02.2026 10:33] Using data from previous issue: {"categories": ["#training", "#robotics", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç-–º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç œá‚ÇÄ ‚Äî —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º-–º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–º –Ω–∞ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–µ–π—Å—Ç–≤–∏–π. –û
[13.02.2026 10:33] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#optimization", "#reasoning"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ê—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è exploration –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Asymmetric GRAE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑
[13.02.2026 10:33] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#security", "#benchmark", "#alignment", "#open_source", "#dataset"], "emoji": "üîç", "ru": {"title": "–û—Ç —á—ë—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞ –∫ –±–µ–ª–æ–º—É: –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π", "desc": "DeepSight ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π –ø—Ä–æ–µ–∫—Ç –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω
[13.02.2026 10:33] Using data from previous issue: {"categories": ["#training", "#robotics", "#multimodal", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–í–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –æ–ø—ã—Ç–∞: –æ–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞ –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "RISE ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É
[13.02.2026 10:33] Using data from previous issue: {"categories": ["#multimodal", "#long_context", "#audio", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–≠–º–æ—Ü–∏–∏ –∫–∞–∫ –∫–æ–º–ø—Ä–µ—Å—Å –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è: —É–º–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º—É–∑—ã–∫–∏ –¥–ª—è –≤–∏–¥–µ–æ", "desc": "NarraScore ‚Äî —ç—Ç–æ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –º—É–∑—ã–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä—Ç–∏—Ç—É—Ä –∫ –¥–ª–∏–Ω–Ω—ã–º –≤–∏–¥–µ–æ
[13.02.2026 10:33] Querying the API.
[13.02.2026 10:33] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

EgoHumanoid enables humanoid loco-manipulation through co-training vision-language-action policies using egocentric human demonstrations and limited robot data, addressing embodiment gaps via view and action alignment techniques.  					AI-generated summary 				 Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.
[13.02.2026 10:33] Response: ```json
{
  "desc": "EgoHumanoid –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è vision-language-action –ø–æ–ª–∏—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—è —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–Ω—ã–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–æ–±–æ—Ç–∞. –î–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è —Ä–∞–∑–Ω–∏—Ü—ã –≤ –≤–æ–ø–ª–æ—â–µ–Ω–∏–∏ –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –≥—É–º–∞–Ω–æ–∏–¥–æ–º –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ pipeline –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—é—â–∏–π –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–Ω–∏—è –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞. –ü–æ—Ä—Ç–∞—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ —Å–æ–±–∏—Ä–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∏—Ä—É–µ–º–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —Ä–æ–±–æ—Ç–∞ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ 51% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ–±—É—á–µ–Ω–Ω—ã–º–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä–æ–±–æ—Ç–µ.",
  "emoji": "ü§ñ",
  "title": "–û—Ç —Ä—É–∫ —á–µ–ª–æ–≤–µ–∫–∞ –∫ –ª–æ–≤–∫–æ—Å—Ç–∏ –≥—É–º–∞–Ω–æ–∏–¥–∞ —á–µ—Ä–µ–∑ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"
}
```
[13.02.2026 10:33] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"EgoHumanoid enables humanoid loco-manipulation through co-training vision-language-action policies using egocentric human demonstrations and limited robot data, addressing embodiment gaps via view and action alignment techniques.  					AI-generated summary 				 Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data."

[13.02.2026 10:33] Response: ```python
["ROBOTICS", "MULTIMODAL", "TRAINING", "DATA"]
```
[13.02.2026 10:33] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"EgoHumanoid enables humanoid loco-manipulation through co-training vision-language-action policies using egocentric human demonstrations and limited robot data, addressing embodiment gaps via view and action alignment techniques.  					AI-generated summary 				 Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data."

[13.02.2026 10:33] Response: ```python
['TRANSFER_LEARNING', 'SYNTHETIC']
```
[13.02.2026 10:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"EgoHumanoid is a framework that enhances humanoid robots\' ability to move and manipulate objects by using human demonstrations and limited robot data. It addresses the challenges of differences in physical form and viewpoint between humans and robots through a systematic alignment process. The framework co-trains a vision-language-action policy, allowing robots to learn from rich, egocentric human data, which is more diverse than traditional robot teleoperation methods. Experiments show that this approach significantly improves performance in new environments compared to using only robot data.","title":"Bridging the Gap: Learning Loco-Manipulation from Humans to Humanoids"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="EgoHumanoid is a framework that enhances humanoid robots' ability to move and manipulate objects by using human demonstrations and limited robot data. It addresses the challenges of differences in physical form and viewpoint between humans and robots through a systematic alignment process. The framework co-trains a vision-language-action policy, allowing robots to learn from rich, egocentric human data, which is more diverse than traditional robot teleoperation methods. Experiments show that this approach significantly improves performance in new environments compared to using only robot data.", title='Bridging the Gap: Learning Loco-Manipulation from Humans to Humanoids'))
[13.02.2026 10:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"EgoHumanoidÊòØ‰∏Ä‰∏™Êñ∞Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂÖ±ÂêåËÆ≠ÁªÉËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÁ≠ñÁï•ÔºåÂà©Áî®‰∫∫Á±ªÁöÑÁ¨¨‰∏Ä‰∫∫Áß∞Á§∫ËåÉÂíåÊúâÈôêÁöÑÊú∫Âô®‰∫∫Êï∞ÊçÆÔºåÂÆûÁé∞Á±ª‰∫∫Êú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑËøêÂä®ÊìçÊéß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËßÜËßíÂíåÂä®‰ΩúÂØπÈΩêÊäÄÊúØÔºåËß£ÂÜ≥‰∫Ü‰∫∫Á±ª‰∏éÊú∫Âô®‰∫∫‰πãÈó¥ÁöÑ‰ΩìÁé∞Â∑ÆË∑ùÔºå‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊâßË°å‰∫∫Á±ªÁöÑÂä®‰Ωú„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßç‰æøÊê∫ÂºèÁ≥ªÁªüÔºåËÉΩÂ§üÈ´òÊïàÊî∂ÈõÜ‰∫∫Á±ªÊï∞ÊçÆÔºåÂπ∂Âª∫Á´ã‰∫ÜÂÆûÁî®ÁöÑÊî∂ÈõÜÂçèËÆÆÔºå‰ª•ÊèêÈ´òÊï∞ÊçÆÁöÑÂèØËΩ¨ÁßªÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®‰∫∫Á±ªÊï∞ÊçÆÁöÑÊ®°ÂûãÂú®Êú™ËßÅÁéØÂ¢É‰∏≠ÁöÑË°®Áé∞ÊØî‰ªÖ‰ΩøÁî®Êú∫Âô®‰∫∫Êï∞ÊçÆÁöÑÂü∫Á∫øÊèêÈ´ò‰∫Ü51%„ÄÇ","title":"EgoHumanoidÔºö‰∫∫Á±ªÁ§∫ËåÉÂä©ÂäõÁ±ª‰∫∫Êú∫Âô®‰∫∫ËøêÂä®ÊìçÊéß"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='EgoHumanoidÊòØ‰∏Ä‰∏™Êñ∞Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂÖ±ÂêåËÆ≠ÁªÉËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÁ≠ñÁï•ÔºåÂà©Áî®‰∫∫Á±ªÁöÑÁ¨¨‰∏Ä‰∫∫Áß∞Á§∫ËåÉÂíåÊúâÈôêÁöÑÊú∫Âô®‰∫∫Êï∞ÊçÆÔºåÂÆûÁé∞Á±ª‰∫∫Êú∫Âô®‰∫∫Âú®Â§çÊùÇÁéØÂ¢É‰∏≠ÁöÑËøêÂä®ÊìçÊéß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËßÜËßíÂíåÂä®‰ΩúÂØπÈΩêÊäÄÊúØÔºåËß£ÂÜ≥‰∫Ü‰∫∫Á±ª‰∏éÊú∫Âô®‰∫∫‰πãÈó¥ÁöÑ‰ΩìÁé∞Â∑ÆË∑ùÔºå‰ΩøÊú∫Âô®‰∫∫ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÊâßË°å‰∫∫Á±ªÁöÑÂä®‰Ωú„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßç‰æøÊê∫ÂºèÁ≥ªÁªüÔºåËÉΩÂ§üÈ´òÊïàÊî∂ÈõÜ‰∫∫Á±ªÊï∞ÊçÆÔºåÂπ∂Âª∫Á´ã‰∫ÜÂÆûÁî®ÁöÑÊî∂ÈõÜÂçèËÆÆÔºå‰ª•ÊèêÈ´òÊï∞ÊçÆÁöÑÂèØËΩ¨ÁßªÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®‰∫∫Á±ªÊï∞ÊçÆÁöÑÊ®°ÂûãÂú®Êú™ËßÅÁéØÂ¢É‰∏≠ÁöÑË°®Áé∞ÊØî‰ªÖ‰ΩøÁî®Êú∫Âô®‰∫∫Êï∞ÊçÆÁöÑÂü∫Á∫øÊèêÈ´ò‰∫Ü51%„ÄÇ', title='EgoHumanoidÔºö‰∫∫Á±ªÁ§∫ËåÉÂä©ÂäõÁ±ª‰∫∫Êú∫Âô®‰∫∫ËøêÂä®ÊìçÊéß'))
[13.02.2026 10:34] Querying the API.
[13.02.2026 10:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Vision-language navigation systems traditionally require detailed instructions but can be improved by incorporating video generation models with sparse future planning for faster, more efficient real-world deployment.  					AI-generated summary 				 Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.
[13.02.2026 10:34] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö, –≥–¥–µ —Ä–æ–±–æ—Ç –¥–æ–ª–∂–µ–Ω –Ω–∞–π—Ç–∏ –¥–∞–ª–µ–∫—É—é, –Ω–µ–≤–∏–¥–∏–º—É—é —Ü–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –ø–æ—à–∞–≥–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, —á–µ–º —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ LLM-–º–µ—Ç–æ–¥—ã, —Ç–∞–∫ –∫–∞–∫ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞—Ö. –î–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SparseVideoNav, –∫–æ—Ç–æ—Ä—ã–π –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–¥–∫–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –∫–∞–¥—Ä—ã –≤–º–µ—Å—Ç–æ –ø–æ–ª–Ω–æ–≥–æ –≤–∏–¥–µ–æ, –¥–æ—Å—Ç–∏–≥–∞—è 27-–∫—Ä–∞—Ç–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∫–∞–∑–∞–ª–∞ –≤ 2,5 —Ä–∞–∑–∞ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è —Å–ª–æ–∂–Ω—ã–µ –Ω–æ—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è.",
  "emoji": "ü§ñ",
  "title": "–û—Ç –≤–∏–¥–µ–æ–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫ —Ä–µ–∞–ª—å–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏: —Ä–µ–¥–∫–∏–µ –±—É–¥—É—â–∏–µ –∫–∞–¥—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º"
}
```
[13.02.2026 10:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Vision-language navigation systems traditionally require detailed instructions but can be improved by incorporating video generation models with sparse future planning for faster, more efficient real-world deployment.  					AI-generated summary 				 Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes."

[13.02.2026 10:34] Response: ```python
['AGENTS', 'VIDEO', 'MULTIMODAL', 'INFERENCE']
```
[13.02.2026 10:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Vision-language navigation systems traditionally require detailed instructions but can be improved by incorporating video generation models with sparse future planning for faster, more efficient real-world deployment.  					AI-generated summary 				 Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes."

[13.02.2026 10:34] Response: ```python
['OPTIMIZATION']
```

**Reasoning:** The paper focuses on optimizing vision-language navigation systems through sparse video generation to achieve significant speed improvements (27x speed-up) for real-world deployment. The core contribution is an optimization technique (SparseVideoNav) that reduces latency while maintaining performance, which directly relates to the OPTIMIZATION topic of advancing training and inference efficiency methods.
[13.02.2026 10:34] Error. Failed to parse JSON from LLM. ["OPTIMIZATION"]


**Reasoning:** The paper focuses on optimizing vision-language navigation systems through sparse video generation to achieve significant speed improvements (27x speed-up) for real-world deployment. The core contribution is an optimization technique (SparseVideoNav) that reduces latency while maintaining performance, which directly relates to the OPTIMIZATION topic of advancing training and inference efficiency methods.
[13.02.2026 10:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the limitations of traditional vision-language navigation systems that rely on detailed instructions. It introduces a novel approach called SparseVideoNav, which integrates video generation models to enhance navigation efficiency in unknown environments. By leveraging long-horizon supervision, this method allows agents to navigate using simple, high-level intents rather than verbose commands. The results show that SparseVideoNav significantly outperforms existing models, achieving faster and more successful navigation in challenging scenarios.","title":"Empowering Navigation with Simplicity: SparseVideoNav Revolutionizes Real-World Exploration"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the limitations of traditional vision-language navigation systems that rely on detailed instructions. It introduces a novel approach called SparseVideoNav, which integrates video generation models to enhance navigation efficiency in unknown environments. By leveraging long-horizon supervision, this method allows agents to navigate using simple, high-level intents rather than verbose commands. The results show that SparseVideoNav significantly outperforms existing models, achieving faster and more successful navigation in challenging scenarios.', title='Empowering Navigation with Simplicity: SparseVideoNav Revolutionizes Real-World Exploration'))
[13.02.2026 10:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜËßÜËßâ-ËØ≠Ë®ÄÂØºËà™Á≥ªÁªüÂ¶Ç‰ΩïÈÄöËøáÂºïÂÖ•ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂíåÁ®ÄÁñèÊú™Êù•ËßÑÂàíÊù•ÊèêÈ´òÊïàÁéá„ÄÇ‰º†ÁªüÁöÑÂØºËà™Á≥ªÁªü‰æùËµñËØ¶ÁªÜÁöÑËØ≠Ë®ÄÊåá‰ª§Ôºå‰ΩÜËøô‰∏éÂú®ÁúüÂÆûÁéØÂ¢É‰∏≠ÂØºËà™ÁöÑÁõÆÊ†áÁõ∏ÊÇñ„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑSparseVideoNavÊ®°ÂûãËÉΩÂ§üÂú®Áü≠Êó∂Èó¥ÂÜÖÁîüÊàêÁ®ÄÁñèÁöÑÊú™Êù•ËΩ®ËøπÔºå‰ªéËÄåÂÆûÁé∞Âø´ÈÄüÁöÑÂØºËà™ÂÜ≥Á≠ñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê®°ÂûãÂú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÊàêÂäüÁéáÊòæËëóÈ´ò‰∫éÁé∞ÊúâÁöÑËØ≠Ë®ÄÊ®°ÂûãÂü∫Á∫ø„ÄÇ","title":"ÂºïÂÖ•ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºåÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÂØºËà™ÊïàÁéá"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜËßÜËßâ-ËØ≠Ë®ÄÂØºËà™Á≥ªÁªüÂ¶Ç‰ΩïÈÄöËøáÂºïÂÖ•ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂíåÁ®ÄÁñèÊú™Êù•ËßÑÂàíÊù•ÊèêÈ´òÊïàÁéá„ÄÇ‰º†ÁªüÁöÑÂØºËà™Á≥ªÁªü‰æùËµñËØ¶ÁªÜÁöÑËØ≠Ë®ÄÊåá‰ª§Ôºå‰ΩÜËøô‰∏éÂú®ÁúüÂÆûÁéØÂ¢É‰∏≠ÂØºËà™ÁöÑÁõÆÊ†áÁõ∏ÊÇñ„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑSparseVideoNavÊ®°ÂûãËÉΩÂ§üÂú®Áü≠Êó∂Èó¥ÂÜÖÁîüÊàêÁ®ÄÁñèÁöÑÊú™Êù•ËΩ®ËøπÔºå‰ªéËÄåÂÆûÁé∞Âø´ÈÄüÁöÑÂØºËà™ÂÜ≥Á≠ñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê®°ÂûãÂú®Â§çÊùÇÂú∫ÊôØ‰∏ãÁöÑÊàêÂäüÁéáÊòæËëóÈ´ò‰∫éÁé∞ÊúâÁöÑËØ≠Ë®ÄÊ®°ÂûãÂü∫Á∫ø„ÄÇ', title='ÂºïÂÖ•ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºåÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÂØºËà™ÊïàÁéá'))
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#rlhf", "#training", "#alignment", "#open_source", "#small_models", "#dataset", "#optimization"], "emoji": "üé®", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–æ–π –º–æ—â—å—é: 5B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±–≥–æ–Ω—è—é—Ç 80B –≤ –∫–∞—á–µ—Å—Ç–≤–µ", "desc": "DeepGen 1.0 ‚Äî —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è 5-–º–∏–ª–ª–∏–∞—Ä–¥–Ω–∞
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#small_models", "#training", "#architecture", "#long_context", "#optimization", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —É–ª—å—Ç—Ä–∞-–¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "MiniCPM-SALA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ—Ç–æ—Ä–∞—è –æ
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "üõ£Ô∏è", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è: –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É —è–≤–Ω—ã–º –∏ —Å–∫—Ä—ã—Ç—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏", "desc": "ThinkRouter ‚Äî —ç—Ç–æ –º–µ—Ö–∞–Ω–∏–∑–º –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –º–µ–∂–¥—É –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#open_source", "#benchmark", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–∏—Ç–∏–∫", "desc": "MolmoSpaces –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Ç–∫—Ä—ã—Ç—É—é —ç–∫–æ—Å–∏—Å—Ç–µ–º—É —Å –±–æ–ª–µ–µ —á–µ–º 230 —Ç—ã—Å—è—á–∞–º–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#training", "#agents"], "emoji": "üß†", "ru": {"title": "–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–∏—Ä–æ–≤ —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –∫–æ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Dreaming in Code (DiCode) ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –≤ –∫–æ—Ç–æ—Ä–æ–º –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π 
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–°–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–Ω–æ–≥–æ—Ç–æ–∫–µ–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ —Å–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#alignment", "#transfer_learning"], "emoji": "üéØ", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ –æ—Ü–µ–Ω–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç P-GenRM ‚Äî –ø–µ—Ä–≤—É—é –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –±–æ–ª—å—à
[13.02.2026 10:34] Querying the API.
[13.02.2026 10:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

General-purpose Vision-Language Models can be effectively adapted for e-commerce applications through targeted techniques that enhance product understanding while maintaining broad multimodal capabilities.  					AI-generated summary 				 E-commerce product understanding demands by nature, strong multimodal comprehension from text, images, and structured attributes. General-purpose Vision-Language Models (VLMs) enable generalizable multimodal latent modelling, yet there is no documented, well-known strategy for adapting them to the attribute-centric, multi-image, and noisy nature of e-commerce data, without sacrificing general performance. In this work, we show through a large-scale experimental study, how targeted adaptation of general VLMs can substantially improve e-commerce performance while preserving broad multimodal capabilities. Furthermore, we propose a novel extensive evaluation suite covering deep product understanding, strict instruction following, and dynamic attribute extraction.
[13.02.2026 10:34] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö Vision-Language –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —à–∏—Ä–æ–∫–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ü–µ–ª–µ–≤–∞—è –¥–æ–∞–¥–∞–ø—Ç–∞—Ü–∏—è VLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤, —Å–ø—Ä–∞–≤–ª—è—è—Å—å —Å —à—É–º–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä –æ—Ü–µ–Ω–∫–∏, –≤–∫–ª—é—á–∞—é—â–∏–π –≥–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Ç–æ–≤–∞—Ä–æ–≤, —Ç–æ—á–Ω–æ–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–±—ã–≤–∞–Ω–∏—è –æ–±—â–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏.",
  "emoji": "üõçÔ∏è",
  "title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –æ–±—â–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π"
}
```
[13.02.2026 10:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"General-purpose Vision-Language Models can be effectively adapted for e-commerce applications through targeted techniques that enhance product understanding while maintaining broad multimodal capabilities.  					AI-generated summary 				 E-commerce product understanding demands by nature, strong multimodal comprehension from text, images, and structured attributes. General-purpose Vision-Language Models (VLMs) enable generalizable multimodal latent modelling, yet there is no documented, well-known strategy for adapting them to the attribute-centric, multi-image, and noisy nature of e-commerce data, without sacrificing general performance. In this work, we show through a large-scale experimental study, how targeted adaptation of general VLMs can substantially improve e-commerce performance while preserving broad multimodal capabilities. Furthermore, we propose a novel extensive evaluation suite covering deep product understanding, strict instruction following, and dynamic attribute extraction."

[13.02.2026 10:34] Response: ```python
["MULTIMODAL", "BENCHMARK", "TRAINING"]
```

**Justification:**

1. **MULTIMODAL**: The paper explicitly discusses Vision-Language Models (VLMs) that combine multiple modalities (text, images, and structured attributes) for e-commerce applications.

2. **BENCHMARK**: The paper proposes "a novel extensive evaluation suite covering deep product understanding, strict instruction following, and dynamic attribute extraction," which constitutes a new evaluation framework/benchmark.

3. **TRAINING**: The paper focuses on "targeted adaptation of general VLMs" and improving model performance through adaptation techniques, which relates to fine-tuning and training methodologies.
[13.02.2026 10:34] Error. Failed to parse JSON from LLM. ["MULTIMODAL", "BENCHMARK", "TRAINING"]


**Justification:**

1. **MULTIMODAL**: The paper explicitly discusses Vision-Language Models (VLMs) that combine multiple modalities (text, images, and structured attributes) for e-commerce applications.

2. **BENCHMARK**: The paper proposes "a novel extensive evaluation suite covering deep product understanding, strict instruction following, and dynamic attribute extraction," which constitutes a new evaluation framework/benchmark.

3. **TRAINING**: The paper focuses on "targeted adaptation of general VLMs" and improving model performance through adaptation techniques, which relates to fine-tuning and training methodologies.
[13.02.2026 10:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"General-purpose Vision-Language Models can be effectively adapted for e-commerce applications through targeted techniques that enhance product understanding while maintaining broad multimodal capabilities.  					AI-generated summary 				 E-commerce product understanding demands by nature, strong multimodal comprehension from text, images, and structured attributes. General-purpose Vision-Language Models (VLMs) enable generalizable multimodal latent modelling, yet there is no documented, well-known strategy for adapting them to the attribute-centric, multi-image, and noisy nature of e-commerce data, without sacrificing general performance. In this work, we show through a large-scale experimental study, how targeted adaptation of general VLMs can substantially improve e-commerce performance while preserving broad multimodal capabilities. Furthermore, we propose a novel extensive evaluation suite covering deep product understanding, strict instruction following, and dynamic attribute extraction."

[13.02.2026 10:34] Response: ```python
["TRANSFER_LEARNING"]
```

The paper discusses adapting general-purpose Vision-Language Models (VLMs) to e-commerce applications, which is a clear example of transfer learning - applying knowledge from a general model to a specific domain while maintaining broad capabilities.
[13.02.2026 10:34] Error. Failed to parse JSON from LLM. ["TRANSFER_LEARNING"]


The paper discusses adapting general-purpose Vision-Language Models (VLMs) to e-commerce applications, which is a clear example of transfer learning - applying knowledge from a general model to a specific domain while maintaining broad capabilities.
[13.02.2026 10:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses how general-purpose Vision-Language Models (VLMs) can be tailored for e-commerce applications to improve product understanding. It highlights the need for strong multimodal comprehension, integrating text, images, and structured attributes specific to e-commerce. The authors present a large-scale experimental study demonstrating that targeted adaptations of VLMs can enhance performance in e-commerce settings without losing their general capabilities. Additionally, they introduce a comprehensive evaluation suite designed to assess deep product understanding, adherence to instructions, and dynamic extraction of attributes.","title":"Enhancing E-commerce with Adapted Vision-Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses how general-purpose Vision-Language Models (VLMs) can be tailored for e-commerce applications to improve product understanding. It highlights the need for strong multimodal comprehension, integrating text, images, and structured attributes specific to e-commerce. The authors present a large-scale experimental study demonstrating that targeted adaptations of VLMs can enhance performance in e-commerce settings without losing their general capabilities. Additionally, they introduce a comprehensive evaluation suite designed to assess deep product understanding, adherence to instructions, and dynamic extraction of attributes.', title='Enhancing E-commerce with Adapted Vision-Language Models'))
[13.02.2026 10:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÊúâÊïàÂú∞Â∞ÜÈÄöÁî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂ∫îÁî®‰∫éÁîµÂ≠êÂïÜÂä°È¢ÜÂüü„ÄÇÁîµÂ≠êÂïÜÂä°‰∫ßÂìÅÁêÜËß£ÈúÄË¶ÅÂº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£ËÉΩÂäõÔºåÂåÖÊã¨ÊñáÊú¨„ÄÅÂõæÂÉèÂíåÁªìÊûÑÂåñÂ±ûÊÄß„ÄÇÊàë‰ª¨ÈÄöËøáÂ§ßËßÑÊ®°ÂÆûÈ™åÁ†îÁ©∂ÔºåÂ±ïÁ§∫‰∫ÜÈíàÂØπÊÄßÂú∞Ë∞ÉÊï¥ÈÄöÁî®VLMsÂèØ‰ª•ÊòæËëóÊèêÂçáÁîµÂ≠êÂïÜÂä°ÊÄßËÉΩÔºåÂêåÊó∂‰øùÊåÅÂπøÊ≥õÁöÑÂ§öÊ®°ÊÄÅËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏ÄÂ•óÊñ∞ÁöÑËØÑ‰º∞Â∑•ÂÖ∑ÔºåÊ∂µÁõñÊ∑±Â∫¶‰∫ßÂìÅÁêÜËß£„ÄÅ‰∏•Ê†ºÁöÑÊåá‰ª§ÈÅµÂæ™ÂíåÂä®ÊÄÅÂ±ûÊÄßÊèêÂèñ„ÄÇ","title":"ÊèêÂçáÁîµÂ≠êÂïÜÂä°ÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£ËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÊúâÊïàÂú∞Â∞ÜÈÄöÁî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂ∫îÁî®‰∫éÁîµÂ≠êÂïÜÂä°È¢ÜÂüü„ÄÇÁîµÂ≠êÂïÜÂä°‰∫ßÂìÅÁêÜËß£ÈúÄË¶ÅÂº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£ËÉΩÂäõÔºåÂåÖÊã¨ÊñáÊú¨„ÄÅÂõæÂÉèÂíåÁªìÊûÑÂåñÂ±ûÊÄß„ÄÇÊàë‰ª¨ÈÄöËøáÂ§ßËßÑÊ®°ÂÆûÈ™åÁ†îÁ©∂ÔºåÂ±ïÁ§∫‰∫ÜÈíàÂØπÊÄßÂú∞Ë∞ÉÊï¥ÈÄöÁî®VLMsÂèØ‰ª•ÊòæËëóÊèêÂçáÁîµÂ≠êÂïÜÂä°ÊÄßËÉΩÔºåÂêåÊó∂‰øùÊåÅÂπøÊ≥õÁöÑÂ§öÊ®°ÊÄÅËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏ÄÂ•óÊñ∞ÁöÑËØÑ‰º∞Â∑•ÂÖ∑ÔºåÊ∂µÁõñÊ∑±Â∫¶‰∫ßÂìÅÁêÜËß£„ÄÅ‰∏•Ê†ºÁöÑÊåá‰ª§ÈÅµÂæ™ÂíåÂä®ÊÄÅÂ±ûÊÄßÊèêÂèñ„ÄÇ', title='ÊèêÂçáÁîµÂ≠êÂïÜÂä°ÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£ËÉΩÂäõ'))
[13.02.2026 10:34] Using data from previous issue: {"categories": [], "emoji": "üí∞", "ru": {"title": "–≠–∫–æ–Ω–æ–º–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –Ω–∞–º–µ—Ä–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ INTENT –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ—à–∞—é—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–Ω–µ—à–Ω–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏ —Å—Ç—Ä–æ–≥–æ–º –±—é
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#audio", "#open_source", "#low_resource", "#training", "#architecture", "#multilingual"], "emoji": "üéôÔ∏è", "ru": {"title": "–ü–æ—Ç–æ–∫–æ–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –≤ –ø–æ–ª—Å–µ–∫—É–Ω–¥—ã", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Voxtral Realtime ‚Äî –ø–æ—Ç–æ–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏, –æ–±—É—á–µ–Ω–Ω–∞—è end-to-en
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#reasoning", "#math", "#science", "#open_source", "#benchmark", "#training"], "emoji": "üß¨", "ru": {"title": "–ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —Å–∞–º–∞ —Å–µ–±—è –ø—Ä–æ–≤–µ—Ä—è—Ç—å: –∫–æ—ç–≤–æ–ª—é—Ü–∏—è —Ä–µ—à–∞—Ç–µ–ª—è –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "Sci-CoE –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å–∏—Å—Ç–µ–º—É –Ω–∞—É—á–Ω–æ–≥–æ —Å–æ-—ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–≥
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#rl", "#benchmark", "#optimization", "#agents", "#reasoning"], "emoji": "‚è±Ô∏è", "ru": {"title": "–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –ø–æ–¥ –¥–∞–≤–ª–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏: –∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã –º–µ–∂–¥—É —Ä–∞–∑—É–º–æ–º, —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å—é", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Gaia2 ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ 
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#multimodal", "#training", "#data"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ScalSelect ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã—Ö –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ 
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#transfer_learning", "#3d", "#architecture", "#synthetic", "#benchmark", "#multimodal", "#agents", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å Vision-Language-Action (VLA)
[13.02.2026 10:34] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Å —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –±–µ–∑ —Å—É–ø–µ—Ä–∫–ª–∞—Å—Ç–µ—Ä–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ SPES ‚Äî –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π mixture-of-experts, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –Ω–∞ GPU –ø—É—Ç—ë–º
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#long_context", "#hallucinations", "#audio", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º–æ—Å—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ç–æ—á–Ω–æ–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", "desc": "MuRGAt ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#training", "#benchmark", "#multimodal", "#rl", "#open_source", "#optimization", "#dataset", "#reasoning"], "emoji": "üé®", "ru": {"title": "–ù–∞—É—á–∏—Ç—å –ò–ò –ø–æ–Ω–∏–º–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–µ —Å–º—ã—Å–ª—ã: –º–µ—Ç–∞—Ñ–æ—Ä—ã –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "MetaphorStar ‚Äî —ç—Ç–æ –ø–µ—Ä–≤—ã–π end-to-end —Ñ—Ä–µ–π–º
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#benchmark", "#synthetic", "#multimodal", "#cv"], "emoji": "üìã", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–∏–±–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ ExStrucTiny –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#security", "#benchmark", "#reasoning", "#rl"], "emoji": "üîç", "ru": {"title": "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–º–∏–Ω–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∫–æ–Ω—Ç–∞–º–∏–Ω–∞—Ü–∏–∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –≤—ã–∑–≤–∞–Ω–Ω–æ–π –æ–±—É—á–µ–Ω–∏–µ–º —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#interpretability", "#open_source"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å—é —á–µ—Ä–µ–∑ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ —Å–º–µ—Å–∏", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Neural Additive Experts (NAE) ‚Äî –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫—É—é –¥–∏–ª–µ–º–º—É –º–µ–∂–¥—É –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç
[13.02.2026 10:34] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–¢–æ—á–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å —Ä–µ–¥–∫–∏–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–ª—é—á–µ–≤—ã–º–∏ –∫–∞–¥—Ä–∞–º–∏", "desc": "PISCO ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—Å—Ç–∞–≤–ª—è—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –≤–∏–¥–µ–æ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã
[13.02.2026 10:34] Renaming data file.
[13.02.2026 10:34] Renaming previous data. hf_papers.json to ./d/2026-02-13.json
[13.02.2026 10:34] Saving new data file.
[13.02.2026 10:34] Generating page.
[13.02.2026 10:34] Renaming previous page.
[13.02.2026 10:34] Renaming previous data. index.html to ./d/2026-02-13.html
[13.02.2026 10:34] Writing result.
[13.02.2026 10:34] Renaming log file.
[13.02.2026 10:34] Renaming previous data. log.txt to ./logs/2026-02-13_last_log.txt
