[12.02.2026 23:22] Read previous papers.
[12.02.2026 23:22] Generating top page (month).
[12.02.2026 23:22] Writing top page (month).
[13.02.2026 01:25] Read previous papers.
[13.02.2026 01:25] Get feed.
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10604
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11124
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11144
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.04935
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10177
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08253
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10622
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10560
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08711
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10975
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11008
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10224
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11089
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11103
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10609
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11149
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07106
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10999
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10367
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09514
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08099
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10231
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09713
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10179
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09901
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.02192
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10229
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08489
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08030
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10748
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07954
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.06008
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09014
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07900
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11137
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10652
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08995
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.03773
[13.02.2026 01:25] Extract page data from URL. URL: https://huggingface.co/papers/2602.02459
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10870
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10778
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10699
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08741
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08052
[13.02.2026 01:25] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08934
[13.02.2026 01:25] Extract page data from URL. URL: https://huggingface.co/papers/2602.06841
[13.02.2026 01:25] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.02.2026 01:25] No deleted papers detected.
[13.02.2026 01:25] Downloading and parsing papers (pdf, html). Total: 46.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10604.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10604.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10604.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.11124.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.11124.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.11124.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.11144.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.11144.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.11144.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.04935.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.04935.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.04935.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10177.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10177.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10177.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.08253.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.08253.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.08253.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10622.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10622.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10622.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10560.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10560.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10560.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.08711.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.08711.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.08711.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10975.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10975.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10975.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.11008.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.11008.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.11008.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10224.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10224.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10224.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.11089.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.11089.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.11089.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.11103.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.11103.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.11103.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10609.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10609.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10609.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.11149.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.11149.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.11149.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.07106.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.07106.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.07106.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10999.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10999.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10999.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10367.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10367.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10367.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.09514.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.09514.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.09514.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.08099.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.08099.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.08099.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10231.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10231.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10231.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.09713.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.09713.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.09713.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10179.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10179.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10179.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.09901.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.09901.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.09901.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.02192.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.02192.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.02192.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10229.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10229.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10229.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.08489.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.08489.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.08489.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.08030.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.08030.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.08030.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10748.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10748.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10748.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.07954.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.07954.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.07954.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.06008.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.06008.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.06008.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.09014.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.09014.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.09014.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.07900.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.07900.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.07900.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.11137.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.11137.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.11137.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10652.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10652.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10652.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.08995.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.08995.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.08995.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.03773.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.03773.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.03773.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.02459.
[13.02.2026 01:25] Downloading paper 2602.02459 from https://arxiv.org/pdf/2602.02459v1...
[13.02.2026 01:25] Extracting affiliations from text.
[13.02.2026 01:25] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"TIC-VLA: Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments Zhiyu Huang * 1 Yun Zhang * 1 Johnson Liu 1 Rui Song 1 Chen Tang 1 Jiaqi Ma 1 Abstract Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-languageaction (VLA) models offer promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines delayed semanticcontrol interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on real robot show that TICVLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/ 6 2 0 2 2 ] . [ 1 9 5 4 2 0 . 2 0 6 2 : r 1. Introduction Robots operating in real-world, human-centric environments must react to dynamic scenes while following high-level natural language instructions (Chen et al., 2025b). Visionlanguage-action (VLA) models (Hirose et al., 2025; Xu et al., 2024; Driess et al., 2025) have emerged as promis- *Equal contribution 1University of California, Los Angeles. Correspondence to: Zhiyu Huang <zhiyuh@ucla.edu>, Yun Zhang <yun666@"
[13.02.2026 01:25] Response: ```python
["University of California, Los Angeles"]
```
[13.02.2026 01:25] Deleting PDF ./assets/pdf/2602.02459.pdf.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10870.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10870.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10870.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10778.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10778.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10778.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.10699.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.10699.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.10699.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.08741.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.08741.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.08741.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.08052.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.08052.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.08052.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.08934.
[13.02.2026 01:25] Extra JSON file exists (./assets/json/2602.08934.json), skip PDF parsing.
[13.02.2026 01:25] Paper image links file exists (./assets/img_data/2602.08934.json), skip HTML parsing.
[13.02.2026 01:25] Success.
[13.02.2026 01:25] Downloading and parsing paper https://huggingface.co/papers/2602.06841.
[13.02.2026 01:25] Downloading paper 2602.06841 from https://arxiv.org/pdf/2602.06841v2...
[13.02.2026 01:26] Extracting affiliations from text.
[13.02.2026 01:26] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 9 ] . [ 2 1 4 8 6 0 . 2 0 6 2 : r From Features to Actions: Explainability in Traditional and Agentic AI Systems Sindhuja Chaduvula1 , Jessee Ho1 , Kina Kim2 , Aravind Narayanan1 , Mahshid Alinoori1 , Muskan Garg3 , Dhanesh Ramachandram1 , and Shaina Raza1 1 Vector Institute for Artificial Intelligence, Toronto, Canada {sindhuja.chaduvula, jessee.ho, aravind.narayanan, mahshid.alinoori, dhanesh.ramachandram, shaina.raza}@vectorinstitute.ai 2 Independent Researcher 3Mayo Clinic, Rochester, MN, USA Abstract. Over the last decade, XAI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman Ï = 0.86), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7 more prevalent in failed runs and reduces success probability by 49%. These findings motivate shift towards trajectory-level e"
[13.02.2026 01:26] Response: ```python
[
    "Vector Institute for Artificial Intelligence, Toronto, Canada",
    "Independent Researcher",
    "Mayo Clinic, Rochester, MN, USA"
]
```
[13.02.2026 01:26] Deleting PDF ./assets/pdf/2602.06841.pdf.
[13.02.2026 01:26] Success.
[13.02.2026 01:26] Enriching papers with extra data.
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 0. Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.  					AI-generated summary 				 We introduce Step 3.5 Flash, ...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 1. PhyCritic is a multimodal critic model designed for physical AI tasks through a two-stage RLVR pipeline that enhances perception and reasoning capabilities.  					AI-generated summary 				 With the rapid development of large multimodal models, reliable judge and critic models have become essential f...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 2. GENIUS evaluates multimodal models' generative fluid intelligence through pattern induction, constraint execution, and contextual adaptation tasks, revealing deficiencies in context comprehension rather than generative capability.  					AI-generated summary 				 Unified Multimodal Models (UMMs) have...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 3. A training-free method called Activation Steering Adapter corrects tool calling behavior in language models by using mid-layer activation interventions guided by a probe and router-conditioned steering vectors.  					AI-generated summary 				 Adapting LLM agents to domain-specific tool calling remai...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 4. Aletheia, a math research agent, demonstrates advanced reasoning capabilities by generating and verifying solutions end-to-end in natural language, achieving autonomous research outcomes from Olympiad problems to PhD-level exercises and contributing to AI-assisted mathematical research.  					AI-gen...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 5. A generative evolutionary framework extends large language models for automated design of large neighborhood search operators in combinatorial optimization problems.  					AI-generated summary 				 While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), ex...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 6. Research investigates the impact of different attention masking strategies on user embedding quality in decoder-only language models, proposing a gradient-guided soft masking technique to improve training stability and representation quality for user behavior analysis.  					AI-generated summary 			...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 7. GRU-Mem addresses long-context reasoning challenges in LLMs by incorporating text-controlled gates and reinforcement learning rewards to stabilize memory updates and improve computational efficiency.  					AI-generated summary 				 While reasoning over long context is crucial for various real-world ...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 8. Omni Dense Captioning introduces a six-dimensional structural schema for generating time-aware audio-visual narratives with explicit timestamps, along with a unified evaluation metric and strong baseline model.  					AI-generated summary 				 This paper proposes Omni Dense Captioning, a novel task d...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 9. FeatureBench evaluates agentic coding performance in comprehensive feature-oriented development through execution-based assessments and automated task derivation from code repositories.  					AI-generated summary 				 Agents powered by large language models (LLMs) are increasingly adopted in the sof...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 10. ROCKET is a training-free model compression method that formulates layer-wise compression as a multi-choice knapsack problem and uses sparse matrix factorization for efficient weight sparsification without iterative optimization.  					AI-generated summary 				 We present ROCKET, a training-free mod...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 11. Meta-Experience Learning enhances LLM reasoning by incorporating self-distilled error representations into parametric memory through contrastive trajectory analysis and language-modeled reward signals.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged ...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 12. DataChef-32B automates data recipe generation for LLM adaptation through reinforcement learning with proxy rewards, achieving performance comparable to human-crafted recipes.  					AI-generated summary 				 In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-q...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 13. GameDevBench is introduced as the first benchmark for evaluating agents on game development tasks that combine software development complexity with deep multimodal understanding requirements.  					AI-generated summary 				 Despite rapid progress on coding agents, progress on their multimodal counte...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 14. Online Causal Kalman Filtering addresses high-variance token-level importance sampling in reinforcement learning for large language models by modeling IS ratios as evolving latent states and using Kalman filtering for stable policy optimization.  					AI-generated summary 				 Reinforcement learning...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 15. Training reasoning language models with repeated examples on smaller datasets yields better performance than single-pass training on larger datasets, with token accuracy serving as a reliable indicator for optimal training duration.  					AI-generated summary 				 Supervised fine-tuning (SFT) on cha...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 16. Ex-Omni is an open-source framework that enhances omni-modal large language models with speech-accompanied 3D facial animation by decoupling semantic reasoning from temporal generation and using speech units as temporal scaffolding.  					AI-generated summary 				 Omni-modal large language models (O...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 17. CLI-Gym enables scalable derivation of environment-intensive tasks by simulating and exploring environment histories, while LiberCoder achieves significant performance improvements on Terminal-Bench through fine-tuning.  					AI-generated summary 				 Agentic coding requires agents to effectively in...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 18. LiveMedBench addresses limitations in medical LLM evaluation by providing a continuously updated, contamination-free benchmark with rubric-based evaluation that better aligns with expert clinical reasoning.  					AI-generated summary 				 The deployment of Large Language Models (LLMs) in high-stakes...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 19. EcoGym presents a generalizable benchmark for evaluating long-horizon planning capabilities of LLM-based agents in interactive economic environments with persistent dynamics and multi-scenario evaluation.  					AI-generated summary 				 Long-horizon planning is widely recognized as a core capability...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 20. Generative multimodal large language models are adapted for video-text embedding and retrieval through intermediate-layer analysis and text-based alignment without visual supervision.  					AI-generated summary 				 Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 21. Blockwise Advantage Estimation addresses reward interference in structured generations by assigning separate advantages to different text blocks, using outcome-conditioned baselines to avoid expensive nested rollouts.  					AI-generated summary 				 Group Relative Policy Optimization (GRPO) assigns ...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 22. Stroke3D generates rigged 3D meshes from 2D strokes and text prompts through a two-stage pipeline combining controllable skeleton generation with enhanced mesh synthesis.  					AI-generated summary 				 Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 23. Visual-to-visual jailbreak attacks compromise image editing models through malicious visual inputs, necessitating new safety benchmarks and defense mechanisms.  					AI-generated summary 				 Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vis...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 24. A unified generative large language model approach for social network search query processing that improves semantic understanding through multi-task learning and reinforcement learning while enhancing downstream task performance.  					AI-generated summary 				 Query Processing (QP) bridges user in...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 25. ECHO-2 is a distributed reinforcement learning framework that enables efficient post-training of large language models by overlapping rollout generation, dissemination, and training while managing policy staleness and network latency.  					AI-generated summary 				 Reinforcement learning (RL) is a ...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 26. Latent Thoughts Tuning introduces a novel framework for reasoning in continuous latent space by combining contextual hidden states with predictive semantic guidance, enabling robust inference through a progressive curriculum learning approach.  					AI-generated summary 				 While explicit Chain-of-...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 27. Reinforcement Learning with Transferable Reward (RLTR) enhances LLM reasoning robustness by ensuring reasoning stability and generalizability through transfer rewards that test cross-model guidance capabilities.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) ha...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 28. Free()LM addresses reasoning model limitations by introducing a self-forgetting mechanism through a Free-Module plug-and-play LoRA adapter, improving performance across scales and long-horizon tasks.  					AI-generated summary 				 Reasoning models enhance problem-solving by scaling test-time comput...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 29. Large language models show promise but lack stability and reliability for knowledge graph fact validation, with retrieval-augmented generation and multi-model consensus approaches yielding inconsistent improvements.  					AI-generated summary 				 Knowledge Graphs (KGs) store structured factual know...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 30. Bielik Guard is a compact Polish language safety classifier family with two variants that effectively categorize content across five safety domains while maintaining high efficiency and accuracy.  					AI-generated summary 				 As Large Language Models (LLMs) become increasingly deployed in Polish l...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 31. AgenticPay presents a benchmark and simulation framework for evaluating multi-agent language-mediated economic interactions, focusing on negotiation performance and strategic reasoning challenges in complex market scenarios.  					AI-generated summary 				 Large language model (LLM)-based agents are...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 32. ArcFlow is a few-step distillation framework that uses non-linear flow trajectories to approximate teacher diffusion models, achieving fast inference with minimal quality loss through lightweight adapter training.  					AI-generated summary 				 Diffusion models have achieved remarkable generation q...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 33. Empirical analysis of LLM code agents reveals that test writing provides limited improvement in issue resolution and is often replaced by observation-based debugging methods.  					AI-generated summary 				 Large Language Model (LLM) code agents increasingly resolve repository-level issues by iterat...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 34. Pretraining with larger weight decay values improves model plasticity and downstream fine-tuning performance by encouraging linearly separable representations and reducing overfitting.  					AI-generated summary 				 The prevailing paradigm in large language model (LLM) development is to pretrain a ...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 35. A unified framework for memory extraction and management in LLM-based agents that improves generalization through semantic neighborhood modeling and marginal utility rewards.  					AI-generated summary 				 Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-base...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 36. Computer-use agents face safety risks from misaligned actions caused by external attacks or internal limitations, prompting the development of DeAction, a guardrail that detects and corrects such actions before execution.  					AI-generated summary 				 Computer-use agents (CUAs) have made tremendou...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 37. RC, an iterative decoding algorithm, enables large language models to extrapolate and continuously improve beyond training budgets by constructing reasoning chains that enhance across iterations, achieving superior performance on long-horizon tasks.  					AI-generated summary 				 Large Language Mod...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 38. Vision-language-action models for robotics are enhanced with a latency-aware framework that compensates for delayed semantic reasoning during real-time action generation through delayed semantic-control interfaces and latency-consistent training.  					AI-generated summary 				 Robots in dynamic, hu...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 39. FedPS is a federated data preprocessing framework that uses aggregated statistics and data-sketching techniques to enable efficient and privacy-preserving data preparation for collaborative machine learning across distributed datasets.  					AI-generated summary 				 Federated Learning (FL) enables ...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 40. GoodVibe is a neuron-level framework that enhances code language model security through targeted fine-tuning of security-relevant neurons while maintaining model utility and significantly reducing training costs.  					AI-generated summary 				 Large language models (LLMs) are increasingly used for ...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 41. V-STAR addresses limitations in generative recommendation by combining value-guided decoding and tree-structured advantage reinforcement to improve exploration and reward signal quality.  					AI-generated summary 				 Generative recommendation via autoregressive models has unified retrieval and ran...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 42. Attack method exploits expert routing dynamics in Mixture-of-Experts language models to compromise safety alignment while maintaining language utility.  					AI-generated summary 				 The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 43. A Deep Reinforcement Learning framework combining Proximal Policy Optimization and Graph Neural Networks addresses multi-objective scheduling challenges by effectively balancing total weighted tardiness and total setup time.  					AI-generated summary 				 The Unrelated Parallel Machine Scheduling P...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 44. StealthRL uses reinforcement learning with LoRA adapters to create adversarial paraphrases that evade multiple AI text detectors while preserving meaning, demonstrating significant robustness gaps in current detection systems.  					AI-generated summary 				 AI-text detectors face a critical robustn...
[13.02.2026 01:26] ********************************************************************************
[13.02.2026 01:26] Abstract 45. Static and agentic explainability approaches differ in their ability to interpret model behavior, with attribution methods effective for individual predictions but inadequate for diagnosing failures in multi-step decision processes, where trace-based diagnostics prove more reliable.  					AI-generat...
[13.02.2026 01:26] Read previous papers.
[13.02.2026 01:26] Generating reviews via LLM API.
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#architecture", "#agents", "#benchmark", "#inference", "#rl"], "emoji": "âš¡", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð³Ñ€Ð°Ð½Ð¸Ñ†Ð° Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸: Ð°Ð³ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ Ð˜Ð˜ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ€Ð°Ð·Ñ€ÐµÐ¶ÐµÐ½Ð½Ð¾Ð¹ ÑÐ¼ÐµÑÐ¸ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð¾Ð²", "desc": "ÐŸÑ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ Step 3.5 Flash, Ð¾Ñ‚Ð½Ð¾ÑÑÑ‰Ð°ÑÑÑ Ðº Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ðµ Mixture-of-Experts, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#rlhf", "#robotics", "#multimodal", "#alignment", "#benchmark", "#reasoning"], "emoji": "ðŸ¤–", "ru": {"title": "ÐšÑ€Ð¸Ñ‚Ð¸Ðº Ñ Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ñ‡ÑƒÑ‚ÑŒÑ‘Ð¼: ÑÐ°Ð¼Ð¾ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ° Ñ‡ÐµÑ€ÐµÐ· Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½Ð¸Ðµ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ", "desc": "PhyCritic â€” ÑÑ‚Ð¾ Ð¼Ð½Ð¾Ð³Ð¾Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ-ÐºÑ€Ð¸Ñ‚Ð¸Ðº, ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ð¾ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ð°Ñ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#benchmark"], "emoji": "ðŸ§ ", "ru": {"title": "ÐžÑ†ÐµÐ½ÐºÐ° Ñ„Ð»ÑŽÐ¸Ð´Ð½Ð¾Ð³Ð¾ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚Ð°: Ð¾Ñ‚ Ð¿Ð°Ð¼ÑÑ‚Ð¸ Ðº ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ð¾Ð¼Ñƒ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸ÑŽ", "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° Ð½Ð¾Ð²Ð°Ñ Ð¾Ñ†ÐµÐ½Ð¾Ñ‡Ð½Ð°Ñ Ð¼ÐµÑ‚Ð¾Ð´Ð¸ÐºÐ° GENIUS Ð´Ð»Ñ Ð¸Ð·Ð¼ÐµÑ€ÐµÐ½Ð¸Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ñ„Ð»ÑŽÐ¸Ð´Ð½Ð¾Ð³Ð¾ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚Ð° Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ‡ÐµÑ€ÐµÐ· Ð·Ð°Ð´
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#small_models", "#inference", "#training", "#agents"], "emoji": "ðŸŽ¯", "ru": {"title": "ÐÐ°Ð¿Ñ€Ð°Ð²Ð»ÑÑŽÑ‰Ð¸Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸Ð¹: Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð±ÐµÐ· Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð´Ð»Ñ Ð½Ð°Ð´Ñ‘Ð¶Ð½Ð¾Ð³Ð¾ Ð²Ñ‹Ð·Ð¾Ð²Ð° Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²", "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ Ð¼ÐµÑ‚Ð¾Ð´ Activation Steering Adapter (ASA) Ð´Ð»Ñ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ†Ð¸Ð¸ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ…
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#science", "#training", "#reasoning", "#agents", "#math", "#open_source"], "emoji": "ðŸ§®", "ru": {"title": "ÐžÑ‚ Ð¾Ð»Ð¸Ð¼Ð¿Ð¸Ð°Ð´ Ðº Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ð¼ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸ÑÐ¼: AI-Ð°Ð³ÐµÐ½Ñ‚ Ð´Ð»Ñ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ð¹", "desc": "Aletheia â€” ÑÑ‚Ð¾ Ð°Ð³ÐµÐ½Ñ‚ Ð´Ð»Ñ Ð¼Ð°Ñ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ð¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¿Ñ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚ÑƒÑŽ Ð²ÐµÑ€ÑÐ¸ÑŽ 
[13.02.2026 01:26] Using data from previous issue: {"categories": [], "emoji": "ðŸ§¬", "ru": {"title": "Ð­Ð²Ð¾Ð»ÑŽÑ†Ð¸Ð¾Ð½Ð½Ð¾Ðµ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¾ÐºÑ€ÐµÑÑ‚Ð½Ð¾ÑÑ‚ÐµÐ¹ Ñ‡ÐµÑ€ÐµÐ· ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸", "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ G-LNS, Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ ÑÐ²Ð¾Ð»ÑŽÑ†Ð¸Ð¾Ð½Ð½Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ€Ð°ÑÑˆÐ¸Ñ€ÑÐµÑ‚ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð¾Ð² Ð¿
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#training", "#architecture", "#benchmark", "#open_source", "#optimization"], "emoji": "ðŸ‘¤", "ru": {"title": "ÐœÑÐ³ÐºÐ¾Ðµ Ð¼Ð°ÑÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ñ… Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¸Ð¹ Ð² ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð·ÑƒÑ‡Ð°ÐµÑ‚ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¹ Ð¼Ð°ÑÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð½Ð° ÐºÐ°Ñ‡ÐµÑÑ‚
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#long_context"], "emoji": "ðŸ§ ", "ru": {"title": "Ð£Ð¼Ð½Ð°Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒ Ñ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼Ñ‹Ð¼Ð¸ Ð·Ð°Ñ‚Ð²Ð¾Ñ€Ð°Ð¼Ð¸ Ð´Ð»Ñ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… Ñ‚ÐµÐºÑÑ‚Ð¾Ð²", "desc": "GRU-Mem Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð½Ð° Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°Ñ… Ð² Ð±Ð¾Ð»ÑŒÑˆÑ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼Ñ‹Ðµ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼ Ð·Ð°Ñ‚Ð²Ð¾Ñ€Ñ‹, Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸Ñ‡Ð½Ñ‹
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#video", "#multimodal", "#training", "#reasoning", "#benchmark", "#audio", "#dataset", "#open_source"], "emoji": "ðŸŽ¬", "ru": {"title": "Ð’Ð¸Ð´ÐµÐ¾ Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð¼ÐµÑ‚ÐºÐ°Ð¼Ð¸: ÐºÐ°Ðº Ð½Ð°ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¾Ð¿Ð¸ÑÑ‹Ð²Ð°Ñ‚ÑŒ Ð²Ð¸Ð´ÐµÐ¾ ÐºÐ°Ðº ÐºÐ¸Ð½ÐµÐ¼Ð°Ñ‚Ð¾Ð³Ñ€Ð°Ñ„Ð¸ÑÑ‚", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Omni Dense Captioning â€” Ð½Ð¾Ð²ÑƒÑŽ Ð·Ð°Ð´Ð°
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#leakage", "#plp", "#open_source", "#benchmark", "#agents", "#optimization", "#dataset"], "emoji": "ðŸ—ï¸", "ru": {"title": "ÐžÑ‚ Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ñ… Ð±Ð°Ð³Ð¾Ð² Ðº Ð¿Ð¾Ð»Ð½Ð¾Ñ†ÐµÐ½Ð½Ð¾Ð¹ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ: Ð½Ð¾Ð²Ñ‹Ð¹ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚ Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÐºÐ¾Ð´Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²", "desc": "FeatureBench â€” ÑÑ‚Ð¾ Ð½Ð¾Ð²Ð°Ñ Ð³ÐµÐ¼Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð°Ð³ÐµÐ½
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#inference", "#training"], "emoji": "ðŸš€", "ru": {"title": "ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ ÑÐ¶Ð°Ñ‚Ð¸Ðµ Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÐµÐ¹ Ð±ÐµÐ· Ð¿ÐµÑ€ÐµÐ¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ‡ÐµÑ€ÐµÐ· Ñ€Ð°Ð·Ñ€ÐµÐ¶ÐµÐ½Ð½ÑƒÑŽ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸ÑŽ", "desc": "ROCKET â€” ÑÑ‚Ð¾ Ð¼ÐµÑ‚Ð¾Ð´ ÑÐ¶Ð°Ñ‚Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð±ÐµÐ· Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€ÑƒÐµÑ‚ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð¼Ð¿Ñ€ÐµÑÑÐ¸Ð¸ Ð¿Ð¾ ÑÐ»Ð¾ÑÐ¼ ÐºÐ°Ðº Ð·Ð°Ð´Ð°Ñ‡Ñƒ Ð¼Ð½Ð¾Ð³Ð¾Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð½Ð¾Ð³Ð¾ Ñ€ÑŽÐºÐ·Ð°Ðº
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rlhf", "#rl"], "emoji": "ðŸ§ ", "ru": {"title": "ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð° ÑÐ¾Ð±ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÑˆÐ¸Ð±ÐºÐ°Ñ…: Ð²ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð¾Ð¿Ñ‹Ñ‚Ð° Ð² Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸", "desc": "Meta-Experience Learning â€” ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÑŒ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ€ÐµÑˆÐ°Ñ‚ÑŒ ÑÐ»Ð¾Ð¶Ð½
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#synthetic", "#data", "#training", "#optimization", "#benchmark", "#rl"], "emoji": "ðŸ‘¨â€ðŸ³", "ru": {"title": "ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ ÐºÑƒÐ»Ð¸Ð½Ð°Ñ€Ð½Ñ‹Ñ… Ñ€ÐµÑ†ÐµÐ¿Ñ‚Ð¾Ð² Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ð¸ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ÑÑ DataChef-32B â€” ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ€ÐµÑ†ÐµÐ¿Ñ‚Ð¾
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#plp", "#multimodal", "#games", "#dataset", "#benchmark", "#agents", "#open_source"], "emoji": "ðŸŽ®", "ru": {"title": "ÐœÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð°Ð³ÐµÐ½Ñ‚Ñ‹ ÑƒÑ‡Ð°Ñ‚ÑÑ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ð¸Ð³Ñ€Ñ‹", "desc": "ÐŸÑ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° GameDevBench â€” Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð½Ð° Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ… Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¸Ð³Ñ€, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÑÐ¾Ñ‡ÐµÑ‚Ð°ÑŽÑ‚
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rlhf", "#rl"], "emoji": "ðŸŽ¯", "ru": {"title": "ÐšÐ°Ð»Ð¼Ð°Ð½Ð¾Ð²Ð° Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ ÑÑ‚Ð°Ð±Ð¸Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½ Ð¼ÐµÑ‚Ð¾Ð´ Online Causal Kalman Filtering Ð´Ð»Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ð´Ð¸ÑÐ¿ÐµÑ€ÑÐ¸Ð¸ Ð¿Ñ€Ð¸ importance samplin
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#optimization", "#small_models", "#reasoning", "#training", "#benchmark"], "emoji": "ðŸ”„", "ru": {"title": "ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€ÐµÐ½Ð¸Ðµ Ð²Ð¼ÐµÑÑ‚Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð°: ÐºÐ°Ðº Ð¼ÐµÐ½ÑŒÑˆÐµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´ÐµÐ»Ð°ÑŽÑ‚ ÑƒÐ¼Ð½ÐµÐµ", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ Ð¿Ñ€Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´Ð°ÑŽÑ‰Ð¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð¼ supervised fine-tuning Ð½Ð°
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#audio", "#3d", "#architecture", "#dataset"], "emoji": "ðŸŽ­", "ru": {"title": "Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð°Ñ Ñ€ÐµÑ‡ÑŒ Ð¸ Ð¼Ð¸Ð¼Ð¸ÐºÐ°: Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸ÐºÐ¸ Ð¾Ñ‚ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸", "desc": "Ex-Omni â€” ÑÑ‚Ð¾ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ€Ð°ÑÑˆÐ¸Ñ€ÑÐµÑ‚ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ð¾Ð¼Ð½Ð¸-Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, 
[13.02.2026 01:26] Using data from previous issue: {"categories": [], "emoji": "âš™ï¸", "ru": {"title": "Ð¡Ð¸Ð½Ñ‚ÐµÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð·Ð°Ð´Ð°Ñ‡ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ñ‡ÐµÑ€ÐµÐ· ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸ÑŽ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ CLI-Gym, ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ Ð´Ð»Ñ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼Ð¾Ð³Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡, Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‰Ð¸Ñ… Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð¹, Ð¿ÑƒÑ‚ÐµÐ¼ Ð¸Ð¼Ð¸Ñ‚Ð°Ñ†Ð¸Ð¸ Ð¸ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð¾
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#science", "#healthcare", "#dataset", "#benchmark", "#agents", "#multilingual", "#leakage", "#alignment"], "emoji": "ðŸ¥", "ru": {"title": "Ð–Ð¸Ð²Ð¾Ð¹ Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¸Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº: Ð¾Ð±ÑƒÐ·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð½Ñ‚Ð°Ð¼Ð¸Ð½Ð°Ñ†Ð¸Ð¸ Ð¸ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ñ€Ð°ÑÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð² Ð¾Ñ†ÐµÐ½ÐºÐµ ÐºÐ»Ð¸Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… LLM", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ LiveMedBe
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#agents", "#reasoning"], "emoji": "ðŸ’°", "ru": {"title": "Ð­ÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð´Ð¾Ð»Ð³Ð¾ÑÑ€Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ LLM-Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²", "desc": "EcoGym â€” ÑÑ‚Ð¾ Ð¾Ð±Ð¾Ð±Ñ‰Ñ‘Ð½Ð½Ñ‹Ð¹ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ LLM-based Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð¾Ð»Ð³Ð¾ÑÑ€Ð¾Ñ‡Ð½Ñ‹Ðµ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸ Ð² Ð¸Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#rag", "#video", "#benchmark", "#multimodal"], "emoji": "ðŸŽ¬", "ru": {"title": "Ð’Ð¸Ð´ÐµÐ¾Ð¿Ð¾Ð¸ÑÐº Ñ‡ÐµÑ€ÐµÐ· Ð¿Ñ€Ð¾Ð¼ÐµÐ¶ÑƒÑ‚Ð¾Ñ‡Ð½Ñ‹Ðµ ÑÐ»Ð¾Ð¸ Ð¸ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ð¾Ðµ Ð²Ñ‹Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð±ÐµÐ· Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ñ", "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ÑÑ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð²Ð¸
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#training", "#rlhf"], "emoji": "ðŸ§©", "ru": {"title": "Ð Ð°Ð·Ð´ÐµÐ»ÑÐ¹ Ð¸ Ð²Ð»Ð°ÑÑ‚Ð²ÑƒÐ¹: Ð¿Ñ€ÐµÐ¸Ð¼ÑƒÑ‰ÐµÑÑ‚Ð²Ð° Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð±Ð»Ð¾ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð°", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ Blockwise Advantage Estimation - Ð¼ÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð² Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ… ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€
[13.02.2026 01:26] Using data from previous issue: {"categories": [], "emoji": "ðŸŽ¨", "ru": {"title": "ÐžÑ‚ ÑˆÑ‚Ñ€Ð¸Ñ…Ð° Ðº Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸ÑŽ: Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ€Ð¸Ð³Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… 3D-Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Stroke3D â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ€Ð¸Ð³Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… 3D-Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¸Ð· Ð´Ð²ÑƒÐ¼ÐµÑ€Ð½Ñ‹Ñ… Ñ€Ð¸ÑÑƒÐ½ÐºÐ¾Ð² Ð¸ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ð¹ Ñ‡ÐµÑ€ÐµÐ· Ð´Ð²ÑƒÑ…ÑÑ‚Ð°Ð¿Ð½Ñ‹Ð¹ pipeline. ÐÐ° Ð¿ÐµÑ€Ð²Ð¾Ð¼ ÑÑ‚Ð°Ð¿Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð²Ð°Ñ€
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#security", "#ethics", "#benchmark"], "emoji": "ðŸŽ¨", "ru": {"title": "Ð’Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ ÑƒÑÐ·Ð²Ð¸Ð¼Ð¾ÑÑ‚Ð¸: Ð·Ð°Ñ‰Ð¸Ñ‚Ð° Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¾Ñ‚ Ð°Ñ‚Ð°Ðº Ñ‡ÐµÑ€ÐµÐ· Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ", "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ÑÑ Ð½Ð¾Ð²Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ Ð°Ñ‚Ð°Ðº Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹, Ð³Ð´Ðµ Ð²Ñ€ÐµÐ´Ð¾Ð½Ð¾ÑÐ½Ñ‹Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸
[13.02.2026 01:26] Using data from previous issue: {"categories": [], "emoji": "ðŸ”", "ru": {"title": "Ð•Ð´Ð¸Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð² ÑÐ¾Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐµÑ‚ÑÑ…", "desc": "Ð Ð°Ð±Ð¾Ñ‚Ð° Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ QP-OneModel â€” ÐµÐ´Ð¸Ð½ÑƒÑŽ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½ÑƒÑŽ Ð±Ð¾Ð»ÑŒÑˆÑƒÑŽ ÑÐ·Ñ‹ÐºÐ¾Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²Ñ‹Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð² ÑÐ¾Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐµÑ‚ÑÑ…. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿ÐµÑ€ÐµÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð»Ð¸ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¿Ð¾Ð´Ð·
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference", "#rl"], "emoji": "âš¡", "ru": {"title": "Ð£ÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ Ð¿Ð¾ÑÑ‚Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ LLM Ñ‡ÐµÑ€ÐµÐ· Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»Ñ‘Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "ECHO-2 â€” ÑÑ‚Ð¾ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»Ñ‘Ð½Ð½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#training", "#reasoning", "#architecture"], "emoji": "ðŸ§ ", "ru": {"title": "Ð Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð±ÐµÐ· ÑÐ»Ð¾Ð²: Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² ÑÐºÑ€Ñ‹Ñ‚Ð¾Ð¼ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ðµ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ Latent Thoughts Tuning (LT-Tuning) â€” Ð½Ð¾Ð²Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð² Ð½ÐµÐ¿Ñ€ÐµÑ€Ñ‹Ð²Ð½Ð¾Ð¼ ÑÐºÑ€Ñ‹Ñ‚Ð¾Ð¼ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ðµ, ÐºÐ¾Ñ‚Ð¾
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rl", "#optimization", "#transfer_learning"], "emoji": "ðŸ”„", "ru": {"title": "ÐšÑ€ÐµÐ¿ÐºÐ¸Ðµ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ñ‡ÐµÑ€ÐµÐ· Ñ‚Ñ€Ð°Ð½ÑÑ„ÐµÑ€Ð½Ð¾Ðµ Ð²Ð¾Ð·Ð½Ð°Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ðµ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ RLTR, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ ÑƒÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ð¾ÑÑ‚ÑŒ Ñ€Ð°ÑÑÑƒÐ¶Ð´Ðµ
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#training", "#long_context", "#reasoning", "#architecture", "#optimization"], "emoji": "ðŸ§ ", "ru": {"title": "ÐœÐ¾Ð´ÐµÐ»ÑŒ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ ÑƒÑ‡Ð¸Ñ‚ÑÑ Ð·Ð°Ð±Ñ‹Ð²Ð°Ñ‚ÑŒ Ð½ÐµÐ½ÑƒÐ¶Ð½Ð¾Ðµ Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ³Ð¾ Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ñ", "desc": "Free()LM Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ, Ð²Ð²Ð¾Ð´Ñ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼ ÑÐ°Ð¼Ð¾Ð·Ð°Ð±Ñ‹Ð²Ð°Ð½Ð¸Ñ Ñ‡ÐµÑ€ÐµÐ· Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€ Free-Module 
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#rag"], "emoji": "ðŸ”", "ru": {"title": "ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ„Ð°ÐºÑ‚Ð¾Ð² Ð² Ð³Ñ€Ð°Ñ„Ð°Ñ… Ð·Ð½Ð°Ð½Ð¸Ð¹: Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ LLM Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‚ Ð±Ð¾Ð»ÑŒÑˆÐµÐ¹ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½ Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº FactCheck Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÑ‚ÑŒ Ñ„Ð°ÐºÑ‚Ñ‹ Ð² Ð³Ñ€Ð°Ñ„Ð°Ñ… Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð¿Ð¾ Ñ‚Ñ€Ñ‘Ð¼ Ð½Ð°Ð¿Ñ€Ð°Ð²
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#open_source", "#low_resource", "#dataset", "#benchmark", "#ethics", "#small_models", "#multilingual"], "emoji": "ðŸ›¡ï¸", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¿Ð¾Ð»ÑŒÑÐºÐ¾ÑÐ·Ñ‹Ñ‡Ð½Ñ‹Ðµ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ñ‹ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ Ð´Ð»Ñ LLM Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹", "desc": "ÐŸÑ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° Bielik Guard â€” ÑÐµÐ¼ÐµÐ¹ÑÑ‚Ð²Ð¾ ÐºÐ¾Ð¼Ð¿Ð°ÐºÑ‚Ð½Ñ‹Ñ… ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#agents"], "emoji": "ðŸ¤", "ru": {"title": "LLM-Ð°Ð³ÐµÐ½Ñ‚Ñ‹ ÑƒÑ‡Ð°Ñ‚ÑÑ Ð²ÐµÑÑ‚Ð¸ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿ÐµÑ€ÐµÐ³Ð¾Ð²Ð¾Ñ€Ñ‹ Ñ‡ÐµÑ€ÐµÐ· ÑÐ·Ñ‹Ðº", "desc": "ÐŸÑ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° Ð±ÐµÐ½Ñ‡Ð¼Ð°Ñ€Ðº-ÑÐ¸ÑÑ‚ÐµÐ¼Ð° AgenticPay Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¼Ð½Ð¾Ð³Ð¾Ð°Ð³ÐµÐ½Ñ‚Ð½Ð¾Ð³Ð¾ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ° Ð² ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… ÑÑ†ÐµÐ½Ð°Ñ€Ð¸ÑÑ…. Ð¡Ð¸ÑÑ‚Ðµ
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#diffusion", "#inference"], "emoji": "âš¡", "ru": {"title": "ÐÐµÐ»Ð¸Ð½ÐµÐ¹Ð½Ñ‹Ðµ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¸ Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð¹ Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸Ð¸ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "ArcFlow â€” ÑÑ‚Ð¾ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð¸ÑÑ‚Ð¸Ð»Ð»ÑÑ†Ð¸Ð¸ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð½ÐµÐ»Ð¸Ð½ÐµÐ¹Ð½Ñ‹Ðµ Ñ‚Ñ€Ð°ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸ Ð¿Ð¾Ñ‚Ð¾
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#agents", "#plp", "#benchmark", "#reasoning"], "emoji": "ðŸ›", "ru": {"title": "Ð¢ÐµÑÑ‚Ñ‹ Ð² ÐºÐ¾Ð´Ðµ Ð½Ðµ Ð²ÑÐµÐ³Ð´Ð° Ñ€ÐµÑˆÐ°ÑŽÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ â€” Ð½Ð°Ð±Ð»ÑŽÐ´ÐµÐ½Ð¸Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð»ÑƒÑ‡ÑˆÐµ", "desc": "Ð’ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¿Ñ€Ð¸ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼ Ð² Ñ€ÐµÐ¿Ð¾Ð·Ð¸Ñ‚Ð¾Ñ€Ð¸ÑÑ… ÐºÐ¾Ð´Ð°, Ð³Ð´Ðµ Ð°Ð³ÐµÐ½Ñ‚Ñ‹ Ð¸Ñ‚ÐµÑ€Ð°Ñ‚Ð¸Ð²
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#training"], "emoji": "ðŸŽ¯", "ru": {"title": "Ð‘Ð¾Ð»ÑŒÑˆÐµ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ â€” Ð»ÑƒÑ‡ÑˆÐµ Ð´Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ", "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ÑÑ Ñ€Ð¾Ð»ÑŒ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð² Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð·Ñ€ÐµÐ½Ð¸Ñ Ð¿Ð»Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»Ð¸ â€” ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ðº Ð½Ð¾Ð²Ñ‹Ð¼ Ð·Ð°Ð´Ð°Ñ‡Ð°Ð¼ Ð¿Ñ€Ð¸ Ð´Ð¾Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸. 
[13.02.2026 01:26] Using data from previous issue: {"categories": [], "emoji": "ðŸ§ ", "ru": {"title": "Ð¡Ð°Ð¼Ð¾Ñ€Ð°ÑÐºÑ€Ñ‹Ð²Ð°ÑŽÑ‰Ð°ÑÑÑ Ð¿Ð°Ð¼ÑÑ‚ÑŒ: ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð½Ð°Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð¸ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð·Ð½Ð°Ð½Ð¸ÑÐ¼Ð¸ Ð² LLM-Ð°Ð³ÐµÐ½Ñ‚Ð°Ñ…", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ UMEM â€” ÐµÐ´Ð¸Ð½Ð°Ñ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð¸ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ Ð² Ð°Ð³ÐµÐ½Ñ‚Ð°Ñ… Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ñ€ÐµÑˆÐ°ÑŽ
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#agents", "#security", "#alignment", "#benchmark", "#dataset"], "emoji": "ðŸ›¡ï¸", "ru": {"title": "Ð—Ð°Ñ‰Ð¸Ñ‚Ð° Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ðµ Ð¸ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð½ÐµÐ¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ñ… Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹", "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° ÑÐ¸ÑÑ‚ÐµÐ¼Ð° DeAction Ð´Ð»Ñ Ð²Ñ‹ÑÐ²Ð»ÐµÐ½Ð¸Ñ Ð¸ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð½ÐµÐ¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ñ… Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ð² Ð°Ð³ÐµÐ½Ñ‚Ð°Ñ…, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð²
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#training", "#rl", "#small_models"], "emoji": "ðŸ”„", "ru": {"title": "Ð˜Ñ‚ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ ÑÐºÑÑ‚Ñ€Ð°Ð¿Ð¾Ð»ÑÑ†Ð¸Ð¸ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÐµÐ¹ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "RC â€” ÑÑ‚Ð¾ Ð¸Ñ‚ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ Ð´ÐµÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð¼ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ð¼ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼ ÑƒÐ»ÑƒÑ‡Ñˆ
[13.02.2026 01:26] Querying the API.
[13.02.2026 01:26] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Vision-language-action models for robotics are enhanced with a latency-aware framework that compensates for delayed semantic reasoning during real-time action generation through delayed semantic-control interfaces and latency-consistent training.  					AI-generated summary 				 Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/
[13.02.2026 01:26] Response: ```json
{
  "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Vision-Language-Action (VLA) Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð´Ð»Ñ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¾Ñ‚ÐµÑ…Ð½Ð¸ÐºÐ¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ ÑƒÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ¸ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð²Ð²Ð¾Ð´ÑÑ‚ Think-in-Control (TIC)-VLA Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑÐ²Ð½Ð¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸Ñ€ÑƒÐµÑ‚ Ð°ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð¾ÑÑ‚ÑŒ Ð¼ÐµÐ¶Ð´Ñƒ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¾Ð¼ Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÐµÐ¹ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ñ‡ÐµÑ€ÐµÐ· Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ Ð¾Ñ‚Ð»Ð¾Ð¶ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ. ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° Ð½Ð¾Ð²Ð°Ñ Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð»Ð¾Ð³Ð¸Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¸Ð½ÑŠÐµÐºÑ‚Ð¸Ñ€ÑƒÐµÑ‚ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ¸ Ð²Ñ‹Ð²Ð¾Ð´Ð° Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¿Ð¾ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°Ð¼, Ð¿Ñ€Ð¸Ð±Ð»Ð¸Ð¶Ð°Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ðº Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¼ ÑƒÑÐ»Ð¾Ð²Ð¸ÑÐ¼ Ñ€Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ñ. Ð­ÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÑŽÑ‚, Ñ‡Ñ‚Ð¾ TIC-VLA Ð¿Ñ€ÐµÐ²Ð¾ÑÑ…Ð¾Ð´Ð¸Ñ‚ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ðµ VLA Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ ÑƒÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ð¾Ðµ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð´Ð°Ð¶Ðµ Ð¿Ñ€Ð¸ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ°Ñ… Ð² Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÐµÐºÑƒÐ½Ð´.",
  "emoji": "ðŸ¤–",
  "title": "ÐšÐ¾Ð¼Ð¿ÐµÐ½ÑÐ°Ñ†Ð¸Ñ Ð·Ð°Ð´ÐµÑ€Ð¶ÐµÐº Ñ€Ð°ÑÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð² Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Ð·Ñ€ÐµÐ½Ð¸Ñ-ÑÐ·Ñ‹ÐºÐ°-Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ð´Ð»Ñ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¾Ñ‚ÐµÑ…Ð½Ð¸ÐºÐ¸"
}
```
[13.02.2026 01:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Vision-language-action models for robotics are enhanced with a latency-aware framework that compensates for delayed semantic reasoning during real-time action generation through delayed semantic-control interfaces and latency-consistent training.  					AI-generated summary 				 Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/"

[13.02.2026 01:26] Response: ```python
['ROBOTICS', 'MULTIMODAL', 'RL', 'TRAINING', 'DATASET']
```
[13.02.2026 01:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Vision-language-action models for robotics are enhanced with a latency-aware framework that compensates for delayed semantic reasoning during real-time action generation through delayed semantic-control interfaces and latency-consistent training.  					AI-generated summary 				 Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/"

[13.02.2026 01:26] Response: ```python
['OPTIMIZATION']
```
[13.02.2026 01:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new framework called Think-in-Control (TIC)-VLA, designed to improve how robots understand and act on language instructions in real-time. It addresses the challenge of delayed semantic reasoning, which occurs when robots process language information slower than they need to act. TIC-VLA introduces a delayed semantic-control interface that helps robots make decisions based on both current observations and past semantic states, allowing them to adapt to the timing of their reasoning. Additionally, the framework includes a training method that simulates these delays, ensuring that robots learn to operate effectively even when there is a lag in understanding language commands.","title":"Enhancing Robot Action with Latency-Aware Language Understanding"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new framework called Think-in-Control (TIC)-VLA, designed to improve how robots understand and act on language instructions in real-time. It addresses the challenge of delayed semantic reasoning, which occurs when robots process language information slower than they need to act. TIC-VLA introduces a delayed semantic-control interface that helps robots make decisions based on both current observations and past semantic states, allowing them to adapt to the timing of their reasoning. Additionally, the framework includes a training method that simulates these delays, ensuring that robots learn to operate effectively even when there is a lag in understanding language commands.', title='Enhancing Robot Action with Latency-Aware Language Understanding'))
[13.02.2026 01:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºThink-in-Control (TIC)-VLAçš„å»¶è¿Ÿæ„ŸçŸ¥æ¡†æž¶ï¼Œæ—¨åœ¨æ”¹å–„æœºå™¨äººåœ¨åŠ¨æ€çŽ¯å¢ƒä¸­æ‰§è¡Œè¯­è¨€æŒ‡ä»¤çš„èƒ½åŠ›ã€‚è¯¥æ¡†æž¶é€šè¿‡å»¶è¿Ÿè¯­ä¹‰æŽ§åˆ¶æŽ¥å£å’Œå»¶è¿Ÿä¸€è‡´æ€§è®­ç»ƒï¼Œè¡¥å¿äº†è¯­ä¹‰æŽ¨ç†çš„å»¶è¿Ÿï¼Œä»Žè€Œå®žçŽ°å®žæ—¶åŠ¨ä½œç”Ÿæˆã€‚TIC-VLAæ˜¾å¼å»ºæ¨¡äº†åœ¨åŠ¨ä½œç”Ÿæˆè¿‡ç¨‹ä¸­å»¶è¿Ÿçš„è¯­ä¹‰æŽ¨ç†ï¼Œå¹¶åˆ©ç”¨å»¶è¿Ÿçš„è§†è§‰-è¯­è¨€è¯­ä¹‰çŠ¶æ€å’Œæ˜¾å¼å»¶è¿Ÿå…ƒæ•°æ®æ¥æŒ‡å¯¼åŠ¨ä½œç”Ÿæˆã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒTIC-VLAåœ¨å¤šç§’æŽ¨ç†å»¶è¿Ÿä¸‹ï¼Œèƒ½å¤Ÿåœ¨çœŸå®žæœºå™¨äººä¸Šä¿æŒå¼ºå¤§çš„å®žæ—¶æŽ§åˆ¶èƒ½åŠ›ï¼Œä¸”æ€§èƒ½ä¼˜äºŽä¹‹å‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹ã€‚","title":"å»¶è¿Ÿæ„ŸçŸ¥æ¡†æž¶æå‡æœºå™¨äººå®žæ—¶æŽ§åˆ¶èƒ½åŠ›"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºThink-in-Control (TIC)-VLAçš„å»¶è¿Ÿæ„ŸçŸ¥æ¡†æž¶ï¼Œæ—¨åœ¨æ”¹å–„æœºå™¨äººåœ¨åŠ¨æ€çŽ¯å¢ƒä¸­æ‰§è¡Œè¯­è¨€æŒ‡ä»¤çš„èƒ½åŠ›ã€‚è¯¥æ¡†æž¶é€šè¿‡å»¶è¿Ÿè¯­ä¹‰æŽ§åˆ¶æŽ¥å£å’Œå»¶è¿Ÿä¸€è‡´æ€§è®­ç»ƒï¼Œè¡¥å¿äº†è¯­ä¹‰æŽ¨ç†çš„å»¶è¿Ÿï¼Œä»Žè€Œå®žçŽ°å®žæ—¶åŠ¨ä½œç”Ÿæˆã€‚TIC-VLAæ˜¾å¼å»ºæ¨¡äº†åœ¨åŠ¨ä½œç”Ÿæˆè¿‡ç¨‹ä¸­å»¶è¿Ÿçš„è¯­ä¹‰æŽ¨ç†ï¼Œå¹¶åˆ©ç”¨å»¶è¿Ÿçš„è§†è§‰-è¯­è¨€è¯­ä¹‰çŠ¶æ€å’Œæ˜¾å¼å»¶è¿Ÿå…ƒæ•°æ®æ¥æŒ‡å¯¼åŠ¨ä½œç”Ÿæˆã€‚å®žéªŒç»“æžœè¡¨æ˜Žï¼ŒTIC-VLAåœ¨å¤šç§’æŽ¨ç†å»¶è¿Ÿä¸‹ï¼Œèƒ½å¤Ÿåœ¨çœŸå®žæœºå™¨äººä¸Šä¿æŒå¼ºå¤§çš„å®žæ—¶æŽ§åˆ¶èƒ½åŠ›ï¼Œä¸”æ€§èƒ½ä¼˜äºŽä¹‹å‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡åž‹ã€‚', title='å»¶è¿Ÿæ„ŸçŸ¥æ¡†æž¶æå‡æœºå™¨äººå®žæ—¶æŽ§åˆ¶èƒ½åŠ›'))
[13.02.2026 01:26] Using data from previous issue: {"categories": [], "emoji": "ðŸ”", "ru": {"title": "ÐŸÑ€Ð¸Ð²Ð°Ñ‚Ð½Ð°Ñ Ð¸ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Ñ„ÐµÐ´ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð¼ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸", "desc": "FedPS â€” ÑÑ‚Ð¾ ÐµÐ´Ð¸Ð½Ð°Ñ Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð° Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Ñ„ÐµÐ´ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð¼ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð¸ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ ÑÑÐºÐ¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#optimization", "#inference", "#security", "#plp", "#interpretability", "#training"], "emoji": "ðŸ”’", "ru": {"title": "Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¹ ÐºÐ¾Ð´ Ñ‡ÐµÑ€ÐµÐ· Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ð¾-Ð¸Ð·Ð±Ð¸Ñ€Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ LLM", "desc": "GoodVibe â€” ÑÑ‚Ð¾ Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð¿Ð¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹, Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰Ð¸Ñ… Ð½Ð° ÐºÐ¾Ð´Ðµ, Ñ‡ÐµÑ€ÐµÐ· Ñ†
[13.02.2026 01:26] Using data from previous issue: {"categories": [], "emoji": "ðŸŒ³", "ru": {"title": "Ð”ÐµÑ€ÐµÐ²Ð¾ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐ¸Ñ… Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¹: Ð¿Ñ€ÐµÐ¾Ð´Ð¾Ð»ÐµÐ²Ð°ÐµÐ¼ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð½Ð¾-Ð½Ð°Ð³Ñ€Ð°Ð´Ð½Ð¾Ðµ Ð½ÐµÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ", "desc": "V-STAR â€” ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¼ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸ÑÐ¼, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ Ð½ÐµÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ñ Ð¼ÐµÐ¶Ð´Ñƒ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð¸ Ð½Ð°Ð³Ñ€Ð°Ð´Ð¾Ð¹ Ð² Ð¼Ð¾Ð´ÐµÐ»ÑÑ… Ñ ÑƒÑÐ¸Ð»ÐµÐ½Ð½Ñ‹Ð¼ Ð¾
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#training", "#security", "#architecture", "#alignment"], "emoji": "âš ï¸", "ru": {"title": "Ð­ÐºÑÐ¿Ð»ÑƒÐ°Ñ‚Ð°Ñ†Ð¸Ñ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð¾Ð±Ñ…Ð¾Ð´Ð° Ð·Ð°Ñ‰Ð¸Ñ‚Ñ‹ Ð² MoE Ð¼Ð¾Ð´ÐµÐ»ÑÑ…", "desc": "Ð’ ÑÑ‚Ð°Ñ‚ÑŒÐµ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ÑÑ ÑƒÑÐ·Ð²Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€ Mixture-of-Experts (MoE) Ð² Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÑÑ…, Ð³Ð´Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð±ÐµÐ·Ð¾Ð¿Ð°
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#graphs", "#optimization", "#architecture", "#benchmark", "#rlhf"], "emoji": "âš™ï¸", "ru": {"title": "Ð“Ð»ÑƒÐ±Ð¾ÐºÐ¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ Ð¼Ð½Ð¾Ð³Ð¾Ñ†ÐµÐ»ÐµÐ²Ð¾Ð³Ð¾ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´ÑÑ‚Ð²Ð°", "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð° ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¸
[13.02.2026 01:26] Using data from previous issue: {"categories": ["#security", "#open_source", "#training", "#benchmark", "#rl"], "emoji": "ðŸ›¡ï¸", "ru": {"title": "Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð°Ð´Ð²ÐµÑ€ÑÐ°Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð°Ñ‚Ð°ÐºÐ¸ Ð½Ð° Ð´ÐµÑ‚ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ð˜Ð˜-Ñ‚ÐµÐºÑÑ‚Ð° Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼", "desc": "Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÑ‚ÑÑ StealthRL â€” Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ Ð¿Ð¾Ð´ÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð´Ð»Ñ Ð³ÐµÐ½Ðµ
[13.02.2026 01:26] Querying the API.
[13.02.2026 01:26] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Static and agentic explainability approaches differ in their ability to interpret model behavior, with attribution methods effective for individual predictions but inadequate for diagnosing failures in multi-step decision processes, where trace-based diagnostics prove more reliable.  					AI-generated summary 				 Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman Ï= 0.86), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7times more prevalent in failed runs and reduces success probability by 49\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.   Resources:   https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework
[13.02.2026 01:26] Response: ```json
{
  "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð¸Ñ Ð¼ÐµÐ¶Ð´Ñƒ Ð¼ÐµÑ‚Ð¾Ð´Ð°Ð¼Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ð´Ð»Ñ ÑÑ‚Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð¸ Ð°Ð³ÐµÐ½Ñ‚Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð±Ð¾Ð»ÑŒÑˆÑ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚, Ñ‡Ñ‚Ð¾ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ†Ð¸Ð¸, ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹, Ð½ÐµÐ°Ð´ÐµÐºÐ²Ð°Ñ‚Ð½Ñ‹ Ð´Ð»Ñ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ¸ ÑÐ±Ð¾ÐµÐ² Ð² Ð¼Ð½Ð¾Ð³Ð¾ÑˆÐ°Ð³Ð¾Ð²Ñ‹Ñ… Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ñ… Ð¿Ñ€Ð¸Ð½ÑÑ‚Ð¸Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹. Ð’ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ðµ Ð°Ð»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ñ‹ Ð¾Ð½Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽÑ‚ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ñ‚Ñ€Ð°ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¹ (trace-based diagnostics), ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð½Ð°Ð´ÐµÐ¶Ð½ÐµÐµ Ð²Ñ‹ÑÐ²Ð»ÑÐµÑ‚ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Ð² Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ð¸ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð². Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚, Ñ‡Ñ‚Ð¾ Ð½ÐµÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚ÑŒ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ñ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð² 2.7 Ñ€Ð°Ð·Ð° Ð±Ð¾Ð»ÐµÐµ Ñ€Ð°ÑÐ¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÐµÐ½Ð° Ð² Ð½ÐµÑƒÐ´Ð°Ñ‡Ð½Ñ‹Ñ… Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ°Ñ… Ð¸ ÑÐ½Ð¸Ð¶Ð°ÐµÑ‚ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ ÑƒÑÐ¿ÐµÑ…Ð° Ð½Ð° 49%.",
  "emoji": "ðŸ”",
  "title": "ÐžÑ‚ ÑÑ‚Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ð¹ Ðº Ð¾Ð±ÑŠÑÑÐ½Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ñ‚Ñ€Ð°ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¹ Ð´Ð»Ñ Ð°Ð³ÐµÐ½Ñ‚Ð½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼"
}
```
[13.02.2026 01:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Static and agentic explainability approaches differ in their ability to interpret model behavior, with attribution methods effective for individual predictions but inadequate for diagnosing failures in multi-step decision processes, where trace-based diagnostics prove more reliable.  					AI-generated summary 				 Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman Ï= 0.86), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7times more prevalent in failed runs and reduces success probability by 49\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.   Resources:   https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework"

[13.02.2026 01:26] Response: ```python
["AGENTS", "BENCHMARK"]
```
[13.02.2026 01:26] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Static and agentic explainability approaches differ in their ability to interpret model behavior, with attribution methods effective for individual predictions but inadequate for diagnosing failures in multi-step decision processes, where trace-based diagnostics prove more reliable.  					AI-generated summary 				 Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman Ï= 0.86), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7times more prevalent in failed runs and reduces success probability by 49\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.   Resources:   https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework"

[13.02.2026 01:26] Response: ```python
["INTERPRETABILITY", "OPEN_SOURCE"]
```

**Justification:**

1. **INTERPRETABILITY**: The paper directly focuses on explainability and interpretability of AI models, comparing different explanation approaches (attribution-based vs. trace-based diagnostics) for understanding model behavior in both static and agentic settings.

2. **OPEN_SOURCE**: The paper includes a GitHub repository link (https://github.com/VectorInstitute/unified-xai-evaluation-framework) and mentions releasing resources publicly, indicating contribution to open-source projects.
[13.02.2026 01:26] Error. Failed to parse JSON from LLM. ["INTERPRETABILITY", "OPEN_SOURCE"]


**Justification:**

1. **INTERPRETABILITY**: The paper directly focuses on explainability and interpretability of AI models, comparing different explanation approaches (attribution-based vs. trace-based diagnostics) for understanding model behavior in both static and agentic settings.

2. **OPEN_SOURCE**: The paper includes a GitHub repository link (https://github.com/VectorInstitute/unified-xai-evaluation-framework) and mentions releasing resources publicly, indicating contribution to open-source projects.
[13.02.2026 01:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the differences between static and agentic explainability in AI systems. Static explainability focuses on understanding individual predictions, while agentic explainability deals with multi-step decision-making processes. The authors compare attribution methods, which work well for static predictions, with trace-based diagnostics that are more effective for diagnosing failures in agentic settings. Their findings suggest that to better evaluate and understand autonomous AI behavior, we need to adopt trajectory-level explainability for agentic systems.","title":"Bridging Static and Agentic Explainability in AI"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the differences between static and agentic explainability in AI systems. Static explainability focuses on understanding individual predictions, while agentic explainability deals with multi-step decision-making processes. The authors compare attribution methods, which work well for static predictions, with trace-based diagnostics that are more effective for diagnosing failures in agentic settings. Their findings suggest that to better evaluate and understand autonomous AI behavior, we need to adopt trajectory-level explainability for agentic systems.', title='Bridging Static and Agentic Explainability in AI'))
[13.02.2026 01:26] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬è®ºæ–‡æŽ¢è®¨äº†é™æ€å’Œä»£ç†å¯è§£é‡Šæ€§æ–¹æ³•åœ¨è§£é‡Šæ¨¡åž‹è¡Œä¸ºæ–¹é¢çš„å·®å¼‚ã€‚ä¼ ç»Ÿçš„å½’å› æ–¹æ³•é€‚ç”¨äºŽå•ä¸ªé¢„æµ‹ï¼Œä½†åœ¨å¤šæ­¥éª¤å†³ç­–è¿‡ç¨‹ä¸­æ— æ³•æœ‰æ•ˆè¯Šæ–­å¤±è´¥ã€‚é€šè¿‡æ¯”è¾ƒé™æ€åˆ†ç±»ä»»åŠ¡ä¸­çš„å½’å› è§£é‡Šå’Œä»£ç†åŸºå‡†ä¸­çš„è¿½è¸ªè¯Šæ–­ï¼Œç ”ç©¶å‘çŽ°å½’å› æ–¹æ³•åœ¨é™æ€è®¾ç½®ä¸­è¡¨çŽ°è‰¯å¥½ï¼Œä½†åœ¨ä»£ç†è®¾ç½®ä¸­æ— æ³•å¯é åœ°è¯Šæ–­æ‰§è¡Œçº§åˆ«çš„å¤±è´¥ã€‚ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼Œä»£ç†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§åº”è½¬å‘è½¨è¿¹çº§åˆ«ï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°å’Œè¯Šæ–­è‡ªä¸»AIè¡Œä¸ºã€‚","title":"ä»Žé™æ€åˆ°ä»£ç†ï¼šAIå¯è§£é‡Šæ€§çš„è½¬å˜"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬è®ºæ–‡æŽ¢è®¨äº†é™æ€å’Œä»£ç†å¯è§£é‡Šæ€§æ–¹æ³•åœ¨è§£é‡Šæ¨¡åž‹è¡Œä¸ºæ–¹é¢çš„å·®å¼‚ã€‚ä¼ ç»Ÿçš„å½’å› æ–¹æ³•é€‚ç”¨äºŽå•ä¸ªé¢„æµ‹ï¼Œä½†åœ¨å¤šæ­¥éª¤å†³ç­–è¿‡ç¨‹ä¸­æ— æ³•æœ‰æ•ˆè¯Šæ–­å¤±è´¥ã€‚é€šè¿‡æ¯”è¾ƒé™æ€åˆ†ç±»ä»»åŠ¡ä¸­çš„å½’å› è§£é‡Šå’Œä»£ç†åŸºå‡†ä¸­çš„è¿½è¸ªè¯Šæ–­ï¼Œç ”ç©¶å‘çŽ°å½’å› æ–¹æ³•åœ¨é™æ€è®¾ç½®ä¸­è¡¨çŽ°è‰¯å¥½ï¼Œä½†åœ¨ä»£ç†è®¾ç½®ä¸­æ— æ³•å¯é åœ°è¯Šæ–­æ‰§è¡Œçº§åˆ«çš„å¤±è´¥ã€‚ç ”ç©¶ç»“æžœè¡¨æ˜Žï¼Œä»£ç†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§åº”è½¬å‘è½¨è¿¹çº§åˆ«ï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°å’Œè¯Šæ–­è‡ªä¸»AIè¡Œä¸ºã€‚', title='ä»Žé™æ€åˆ°ä»£ç†ï¼šAIå¯è§£é‡Šæ€§çš„è½¬å˜'))
[13.02.2026 01:26] Renaming data file.
[13.02.2026 01:26] Renaming previous data. hf_papers.json to ./d/2026-02-13.json
[13.02.2026 01:26] Saving new data file.
[13.02.2026 01:26] Generating page.
[13.02.2026 01:26] Renaming previous page.
[13.02.2026 01:26] Renaming previous data. index.html to ./d/2026-02-13.html
[13.02.2026 01:26] Writing result.
[13.02.2026 01:26] Renaming log file.
[13.02.2026 01:26] Renaming previous data. log.txt to ./logs/2026-02-13_last_log.txt
