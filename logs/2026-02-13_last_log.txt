[13.02.2026 05:57] Read previous papers.
[13.02.2026 05:57] Generating top page (month).
[13.02.2026 05:57] Writing top page (month).
[13.02.2026 06:51] Read previous papers.
[13.02.2026 06:51] Get feed.
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09877
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12125
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10934
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12099
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12280
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12056
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12153
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.05548
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.09070
[13.02.2026 06:51] Extract page data from URL. URL: https://huggingface.co/papers/2602.12092
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11541
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11337
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08194
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12262
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12164
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11964
[13.02.2026 06:51] Extract page data from URL. URL: https://huggingface.co/papers/2602.11748
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11683
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11298
[13.02.2026 06:51] Extract page data from URL. URL: https://huggingface.co/papers/2602.12205
[13.02.2026 06:51] Extract page data from URL. URL: https://huggingface.co/papers/2602.12116
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11761
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11636
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11598
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11543
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.11509
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08277
[13.02.2026 06:51] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10585
[13.02.2026 06:51] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.02.2026 06:51] No deleted papers detected.
[13.02.2026 06:51] Downloading and parsing papers (pdf, html). Total: 28.
[13.02.2026 06:51] Downloading and parsing paper https://huggingface.co/papers/2602.09877.
[13.02.2026 06:51] Extra JSON file exists (./assets/json/2602.09877.json), skip PDF parsing.
[13.02.2026 06:51] Paper image links file exists (./assets/img_data/2602.09877.json), skip HTML parsing.
[13.02.2026 06:51] Success.
[13.02.2026 06:51] Downloading and parsing paper https://huggingface.co/papers/2602.12125.
[13.02.2026 06:51] Extra JSON file exists (./assets/json/2602.12125.json), skip PDF parsing.
[13.02.2026 06:51] Paper image links file exists (./assets/img_data/2602.12125.json), skip HTML parsing.
[13.02.2026 06:51] Success.
[13.02.2026 06:51] Downloading and parsing paper https://huggingface.co/papers/2602.10934.
[13.02.2026 06:51] Extra JSON file exists (./assets/json/2602.10934.json), skip PDF parsing.
[13.02.2026 06:51] Paper image links file exists (./assets/img_data/2602.10934.json), skip HTML parsing.
[13.02.2026 06:51] Success.
[13.02.2026 06:51] Downloading and parsing paper https://huggingface.co/papers/2602.12099.
[13.02.2026 06:51] Extra JSON file exists (./assets/json/2602.12099.json), skip PDF parsing.
[13.02.2026 06:51] Paper image links file exists (./assets/img_data/2602.12099.json), skip HTML parsing.
[13.02.2026 06:51] Success.
[13.02.2026 06:51] Downloading and parsing paper https://huggingface.co/papers/2602.12280.
[13.02.2026 06:51] Extra JSON file exists (./assets/json/2602.12280.json), skip PDF parsing.
[13.02.2026 06:51] Paper image links file exists (./assets/img_data/2602.12280.json), skip HTML parsing.
[13.02.2026 06:51] Success.
[13.02.2026 06:51] Downloading and parsing paper https://huggingface.co/papers/2602.12056.
[13.02.2026 06:51] Extra JSON file exists (./assets/json/2602.12056.json), skip PDF parsing.
[13.02.2026 06:51] Paper image links file exists (./assets/img_data/2602.12056.json), skip HTML parsing.
[13.02.2026 06:51] Success.
[13.02.2026 06:51] Downloading and parsing paper https://huggingface.co/papers/2602.12153.
[13.02.2026 06:51] Extra JSON file exists (./assets/json/2602.12153.json), skip PDF parsing.
[13.02.2026 06:51] Paper image links file exists (./assets/img_data/2602.12153.json), skip HTML parsing.
[13.02.2026 06:51] Success.
[13.02.2026 06:51] Downloading and parsing paper https://huggingface.co/papers/2602.05548.
[13.02.2026 06:51] Extra JSON file exists (./assets/json/2602.05548.json), skip PDF parsing.
[13.02.2026 06:51] Paper image links file exists (./assets/img_data/2602.05548.json), skip HTML parsing.
[13.02.2026 06:51] Success.
[13.02.2026 06:51] Downloading and parsing paper https://huggingface.co/papers/2602.09070.
[13.02.2026 06:51] Extra JSON file exists (./assets/json/2602.09070.json), skip PDF parsing.
[13.02.2026 06:51] Paper image links file exists (./assets/img_data/2602.09070.json), skip HTML parsing.
[13.02.2026 06:51] Success.
[13.02.2026 06:51] Downloading and parsing paper https://huggingface.co/papers/2602.12092.
[13.02.2026 06:51] Downloading paper 2602.12092 from https://arxiv.org/pdf/2602.12092v1...
[13.02.2026 06:51] Extracting affiliations from text.
[13.02.2026 06:51] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DeepSight: An All-in-One LM Safety Toolkit Bo Zhang, Jiaxuan Guo, Lijun Li, Dongrui Liu, Sujin Chen, Guanxu Chen, Zhijie Zheng, Qihao Lin, Lewen Yan, Chen Qian, Yijin Zhou, Yuyao Wu, Shaoxiong Guo, Tianyi Du, Jingyi Yang, Xuhao Hu, Ziqi Miao, Xiaoya Lu, Jing Shao(cid:66), Xia Hu (cid:135) github.com/AI45Lab/DeepSafe (cid:135) github.com/AI45Lab/DeepScan "
[13.02.2026 06:51] Response: ```python
[]
```
[13.02.2026 06:51] Extracting affiliations from text.
[13.02.2026 06:51] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DeepSight: An All-in-One LM Safety Toolkit Bo Zhang, Jiaxuan Guo, Lijun Li, Dongrui Liu, Sujin Chen, Guanxu Chen, Zhijie Zheng, Qihao Lin, Lewen Yan, Chen Qian, Yijin Zhou, Yuyao Wu, Shaoxiong Guo, Tianyi Du, Jingyi Yang, Xuhao Hu, Ziqi Miao, Xiaoya Lu, Jing Shao(cid:66), Xia Hu(cid:135) github.com/AI45Lab/DeepSafe (cid:135) github.com/AI45Lab/DeepScanAs the development of Large Models (LMs) progresses rapidly, their safety is also priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of evaluation toolkit DeepSafe and diagnosis toolkit DeepScan. By unifying task and data protocols, we build connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis. 6 2 0 2 2 1 ] . [ 1 2 9 0 2 1 . 2 0 6 2 : r * Co-leads; (cid:66) Corresponding author. DeepSight: An All-in-One LM Safety Toolkit2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 DeepSafe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 DeepScan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .3.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Content Risk Evaluation and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Frontier AI Risk Evaluation and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Joint Safety Evaluation and Diagnosis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Related Work 4.1 Evaluation Frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Diagnosis Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Conclusion and Discussion 3 3 4 6 9 9 17 21 23 23 24 2 DeepSight: An All-in-One LM Safety ToolkitThe rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has fundamentally transformed the artificial intelligence landscape, positioning safety as paramount concern alongside performance capability (Wei et al., 2023). As these models are increasingly integrated into critical applications (Chu et al., 2025; Zhang et al., 2023; Yang et al., 2026b), the industry faces an urgent need to ensure they operate within secure and ethical boundaries. However, the current landscape of safety evaluation remains fractured between black-box assessment and white-box insight. While the industry has established robust evaluation frameworks, ranging from OpenAI Evals (OpenAI, 2023) and Inspect (UK AI Security Institute, 2024) to comprehensive platforms like OpenCompass (Contributors, 2023) and HELM (Liang et al., 2022), these tools primarily focus on quantifying behavior capabilities instead of safety mechanisms. Conversely, diagnostic research has made significant strides in decoding internal mechanisms, including probing latent geometric boundaries, identifying safety-specific neurons, and analyzing reasoning dynamics via information flow (Qian et al., 2024a; 2025; Burns et al., 2022; Zou et al., 2023). Yet, these diagnostic methods often operate in isolation from standardized benchmarks, treating internal representation analysis as separate academic pursuit rather than debugging tool for deployment risks. To systematically address these issues, we propose DeepSight, an open-source project, that proposes and implements new safety evaluation-diagnosis integrated paradigm. By unifying task and data protocols, DeepSight connects evaluation and diagnosis. In this way, researchers and developers not only identify what went wrong but also understand how safety concepts are encoded internally, facilitating more reliable and interpretable repairs. DeepSight is composed of two core engines: DeepSafe: modular, configuration-driven framework for multimodal LLM/MLLM evaluation that integrates over 20 safety benchmarks. More importantly, DeepSafe also supports the evaluation of frontier AI risks, i.e., high-severity risks (Lab et al., 2025). It provides low-cost, reproducible, efficient, and highly scalable large-scale model security evaluation toolkit for researchers and professional developers. DeepScan: standardized and extensible framework equipped with suite of LLM diagnostic tools. It efficiently locates key layers and neurons to probe representation-level structures and objective-level conflicts without modifying model weights. To the best of our knowledge, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagn"
[13.02.2026 06:52] Mistral response. {"id": "675f351cdc96470785c260eb6f4c6427", "created": 1770965519, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1518, "total_tokens": 1524, "completion_tokens": 6, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[13.02.2026 06:52] Response: ```python
[]
```
[13.02.2026 06:52] Deleting PDF ./assets/pdf/2602.12092.pdf.
[13.02.2026 06:52] Success.
[13.02.2026 06:52] Downloading and parsing paper https://huggingface.co/papers/2602.11541.
[13.02.2026 06:52] Extra JSON file exists (./assets/json/2602.11541.json), skip PDF parsing.
[13.02.2026 06:52] Paper image links file exists (./assets/img_data/2602.11541.json), skip HTML parsing.
[13.02.2026 06:52] Success.
[13.02.2026 06:52] Downloading and parsing paper https://huggingface.co/papers/2602.11337.
[13.02.2026 06:52] Extra JSON file exists (./assets/json/2602.11337.json), skip PDF parsing.
[13.02.2026 06:52] Paper image links file exists (./assets/img_data/2602.11337.json), skip HTML parsing.
[13.02.2026 06:52] Success.
[13.02.2026 06:52] Downloading and parsing paper https://huggingface.co/papers/2602.08194.
[13.02.2026 06:52] Extra JSON file exists (./assets/json/2602.08194.json), skip PDF parsing.
[13.02.2026 06:52] Paper image links file exists (./assets/img_data/2602.08194.json), skip HTML parsing.
[13.02.2026 06:52] Success.
[13.02.2026 06:52] Downloading and parsing paper https://huggingface.co/papers/2602.12262.
[13.02.2026 06:52] Extra JSON file exists (./assets/json/2602.12262.json), skip PDF parsing.
[13.02.2026 06:52] Paper image links file exists (./assets/img_data/2602.12262.json), skip HTML parsing.
[13.02.2026 06:52] Success.
[13.02.2026 06:52] Downloading and parsing paper https://huggingface.co/papers/2602.12164.
[13.02.2026 06:52] Extra JSON file exists (./assets/json/2602.12164.json), skip PDF parsing.
[13.02.2026 06:52] Paper image links file exists (./assets/img_data/2602.12164.json), skip HTML parsing.
[13.02.2026 06:52] Success.
[13.02.2026 06:52] Downloading and parsing paper https://huggingface.co/papers/2602.11964.
[13.02.2026 06:52] Extra JSON file exists (./assets/json/2602.11964.json), skip PDF parsing.
[13.02.2026 06:52] Paper image links file exists (./assets/img_data/2602.11964.json), skip HTML parsing.
[13.02.2026 06:52] Success.
[13.02.2026 06:52] Downloading and parsing paper https://huggingface.co/papers/2602.11748.
[13.02.2026 06:52] Downloading paper 2602.11748 from https://arxiv.org/pdf/2602.11748v1...
[13.02.2026 06:54] Extracting affiliations from text.
[13.02.2026 06:54] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning Futing Wang * 1 2 3 Jianhao Yan * 1 2 3 Yun Luo 3 Ganqu Cui 3 Zhi Wang 4 3 Xiaoye Qu 3 Yue Zhang 2 5 Yu Cheng 6 Tao Lin 2 6 2 0 2 2 1 ] . [ 1 8 4 7 1 1 . 2 0 6 2 : r a "
[13.02.2026 06:54] Response: ```python
[]
```
[13.02.2026 06:54] Extracting affiliations from text.
[13.02.2026 06:54] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning Futing Wang * 1 2 3 Jianhao Yan * 1 2 3 Yun Luo 3 Ganqu Cui 3 Zhi Wang 4 3 Xiaoye Qu 3 Yue Zhang 2 5 Yu Cheng 6 Tao Lin 2 6 2 0 2 2 1 ] . [ 1 8 4 7 1 1 . 2 0 6 2 : r aAchieving effective test-time scaling requires models to engage in In-Context Exploration the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within single continuous context. Grounded in State Coverage theory, our analysis identifies critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, phenomenon we term the Shallow Exploration Trap. To bridge this gap, we propose LengthIncentivized Exploration(LIE). This simple yet effective recipe explicitly encourages models to explore more via length-based reward coupled with redundancy penalty, thereby maximizing state coverage in two-step manner. Comprehensive experiments across different models (Qwen3, Llama) demonstrate that LIE effectively incentivize in-context exploration. As result, our method achieves an average improvement of 4.4% on in-domain tasks and 2.7% gain on out-ofdomain benchmarks. Our code is publicly available in https://github.com/LINs-lab /LIE. 1. Introduction Scaling test-time computation, often conceptualized as enabling models to think harder before answering, has emerged as powerful paradigm for breaking the performance ceiling of Large Language Models (LLMs) (Snell et al., 2024; Wu et al., 2024; Liu et al., 2025a). Broadly, 1Zhejiang University 2Westlake University 3Shanghai AI Laboratory 4Nanjing University 5Institute of Advanced Technology, Westlake Institute for Advanced Study 6The Chinese University of Hong Kong. Correspondence to: Yun Luo <luoyun1@pjlab.org.cn>, Yu Cheng <chengyu@cse.cuhk.edu.hk>, Tao Lin <lintao@westlake.edu.cn>. Preprint. February 13, 2026. Figure 1. The difference between In-Context Exploration and Training Exploration. Our framework distinguishes between the exploration of the training process and in-context inference. In the training phase, reinforcement learning incentivizes the model to explore and learn from diverse state distributions. In contrast, during test-time inference, in-context exploration empowers the model to actively traverse and navigate states. Test-Time Scaling (TTS) strategies fall into two primary regimes: Parallel Scaling (Lightman et al., 2023; Snell et al., 2024; Liu et al., 2025a; Brown et al., 2024; Wang et al., 2022), which aggregates outputs from multiple independent samples, and Sequential Scaling (Guo et al., 2025; Jaech et al., 2024; Kumar et al., 2024), which prioritizes extended reasoning chains or iterative refinement. While Parallel Scaling structures the search space externally, Sequential Scaling via Long CoT represents an intrinsic dimension of TTS, as it directly harnesses the models intrinsic reasoning capabilities (Snell et al., 2024). One iconic capability among this intrinsic reasoning is incontext exploration (Setlur et al., 2025), which is achieved through the generation, verification, and refinement of multiple hypotheses within continuous context (Gandhi et al., 2024; Setlur et al., 2025). Consequently, fostering such in-context exploration serves as the critical catalyst for unlocking the full potential of sequential scaling. As illustrated in Figure 1, in-context exploration enables the model to sequentially traverse diverse reasoning states to find the correct answer. However, how to effectively train LLMs to Learn to Explore In-Context via Length-Incentivized Reinforcement Learning acquire in-context exploration remains underexplored. 2. Related Work To study this problem, we first analyze the bottleneck of in-context exploration through the lens of Count-Based Exploration theory in RL training (Auer, 2002), which establishes state coverage as fundamental proxy for exploration quality. We extend the theory into test-time in-context exploration and adopt state coverage, the in-context state diversity, as theoretical proxy for the exploration quality. Figure 1 demonstrates the difference between in-context exploration and the classic rl training exploration. Our theoretical analysis of state coverage exposes critical bottleneck (Remark 4.3): achieving broader state coverage requires longer reasoning sequences, yet such sequences face exponentially diminishing sampling probabilities (Shallow Exploration Trap) (Lemma 4.2). Empirically, we analyze the training dynamics of GSPO and GRPO baselines. Our observations validate our theoretical results and also reveal that these methods implicitly incentivize response length to some degree, while decreasing states density (Section 4.3). This observation prompts our primary research question: How to effectively incentivize in-context exploration in RL for test-time scaling? To address this, we propose Length-Incentivized Exploration (LIE), an RL recipe that maximizes in-context state coverage, in two-step manner. The length reward elevates the upper bound of states, and the redundancy penalty matches the number of states given length (as introduced in Section 5). Experiments demonstrate that LIE significantly improves performance compared to GRPO and GSPO baselines across diverse models (Qwen3 (Yang et al., 2025a) and LLaMA (Meta, 2024)). Specifically, our method achieves consistent performance boosts, yielding 4.4% average gain on in-domain reasoning tasks and 2.7% improvement on out-of-domain benchmarks, underscoring its superior generalization capabilities. Our key contributions are summarized as follows: Identifying the In-Context Exploration Bottleneck.. We identify Shallow Exploration Trap as the bottleneck of in-context exploration, substantiating its existence through both theoretical derivations (Proposition 4.1 & Lemma 4.2) and empirical validations (Figure 3). Length-Incentivized Exploration (LIE) Recipe. We propose simple yet effective training recipe that explicitly incentivizes the model to overcome the bottleneck, enabling the model to explore during both training and inference. Empirical Validation and Test-Time Scaling. Comprehensive experiments demonstrate that LIE significantly outperforms standard baselines, achieving effective test-time scaling and eliciting more cognitive behaviors. Scaling Test-Time Compute via Long"
[13.02.2026 06:54] Mistral response. {"id": "1c55699b3bc7472795423ec7e21cc387", "created": 1770965651, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1487, "total_tokens": 1543, "completion_tokens": 56, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Zhejiang University\",\n    \"Westlake University\",\n    \"Shanghai AI Laboratory\",\n    \"Nanjing University\",\n    \"Institute of Advanced Technology, Westlake Institute for Advanced Study\",\n    \"The Chinese University of Hong Kong\"\n]\n```"}}]}
[13.02.2026 06:54] Response: ```python
[
    "Zhejiang University",
    "Westlake University",
    "Shanghai AI Laboratory",
    "Nanjing University",
    "Institute of Advanced Technology, Westlake Institute for Advanced Study",
    "The Chinese University of Hong Kong"
]
```
[13.02.2026 06:54] Deleting PDF ./assets/pdf/2602.11748.pdf.
[13.02.2026 06:54] Success.
[13.02.2026 06:54] Downloading and parsing paper https://huggingface.co/papers/2602.11683.
[13.02.2026 06:54] Extra JSON file exists (./assets/json/2602.11683.json), skip PDF parsing.
[13.02.2026 06:54] Paper image links file exists (./assets/img_data/2602.11683.json), skip HTML parsing.
[13.02.2026 06:54] Success.
[13.02.2026 06:54] Downloading and parsing paper https://huggingface.co/papers/2602.11298.
[13.02.2026 06:54] Extra JSON file exists (./assets/json/2602.11298.json), skip PDF parsing.
[13.02.2026 06:54] Paper image links file exists (./assets/img_data/2602.11298.json), skip HTML parsing.
[13.02.2026 06:54] Success.
[13.02.2026 06:54] Downloading and parsing paper https://huggingface.co/papers/2602.12205.
[13.02.2026 06:54] Downloading paper 2602.12205 from https://arxiv.org/pdf/2602.12205v1...
[13.02.2026 06:54] Extracting affiliations from text.
[13.02.2026 06:54] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DeepGen 1.0: Lightweight Unified Multimodal Model for Advancing Image Generation and Editing Dianyi Wang1,2*, Ruihang Li1,3* Feng Han1,2*, Chaofan Ma4*, Wei Song1,5,6*, Siyuan Wang8*, Yibin Wang1,2*, Yi Xin1,7, Hongjian Liu3, Zhixiong Zhang1,4, Shengyuan Ding1,2, Tianhang Wang1,5, Zhenglin Cheng1,5,6, Tao Lin6, Cheng Jin2, Kaicheng Yu6, Jingjing Chen2, Wenjie Wang3, Zhongyu Wei1,2, Jiaqi Wang1 1Shanghai Innovation Institute, 2Fudan University, 3University of Science and Technology of China, 4Shanghai Jiao Tong University, 5Zhejiang University, 6Westlake University, 7Nanjing University, 8University of Southern California *Core Contributors, Project Leaders 6 2 0 2 2 1 ] . [ 1 5 0 2 2 1 . 2 0 6 2 : r Figure 1 Overview of DeepGen 1.0s visual generation and editing abilities, including reasoning-intensive scenarios. "
[13.02.2026 06:54] Response: ```python
[
    "Shanghai Innovation Institute",
    "Fudan University",
    "University of Science and Technology of China",
    "Shanghai Jiao Tong University",
    "Zhejiang University",
    "Westlake University",
    "Nanjing University",
    "University of Southern California"
]
```
[13.02.2026 06:54] Deleting PDF ./assets/pdf/2602.12205.pdf.
[13.02.2026 06:54] Success.
[13.02.2026 06:54] Downloading and parsing paper https://huggingface.co/papers/2602.12116.
[13.02.2026 06:54] Downloading paper 2602.12116 from https://arxiv.org/pdf/2602.12116v1...
[13.02.2026 06:54] Extracting affiliations from text.
[13.02.2026 06:54] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 1 ] . [ 1 6 1 1 2 1 . 2 0 6 2 : r P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling Pinyi Zhang, Ting-En Lin, Yuchuan Wu, Jingyang Chen, Zongqi Wang, Hua Yang, Ze Xu, Fei Huang, Kai Zhang, Yongbin Li Qwen-Character Team, Alibaba Group "
[13.02.2026 06:54] Response: ```python
["Alibaba Group"]
```
[13.02.2026 06:54] Deleting PDF ./assets/pdf/2602.12116.pdf.
[13.02.2026 06:54] Success.
[13.02.2026 06:54] Downloading and parsing paper https://huggingface.co/papers/2602.11761.
[13.02.2026 06:54] Extra JSON file exists (./assets/json/2602.11761.json), skip PDF parsing.
[13.02.2026 06:54] Paper image links file exists (./assets/img_data/2602.11761.json), skip HTML parsing.
[13.02.2026 06:54] Success.
[13.02.2026 06:54] Downloading and parsing paper https://huggingface.co/papers/2602.11636.
[13.02.2026 06:54] Extra JSON file exists (./assets/json/2602.11636.json), skip PDF parsing.
[13.02.2026 06:54] Paper image links file exists (./assets/img_data/2602.11636.json), skip HTML parsing.
[13.02.2026 06:54] Success.
[13.02.2026 06:54] Downloading and parsing paper https://huggingface.co/papers/2602.11598.
[13.02.2026 06:54] Extra JSON file exists (./assets/json/2602.11598.json), skip PDF parsing.
[13.02.2026 06:54] Paper image links file exists (./assets/img_data/2602.11598.json), skip HTML parsing.
[13.02.2026 06:54] Success.
[13.02.2026 06:54] Downloading and parsing paper https://huggingface.co/papers/2602.11543.
[13.02.2026 06:54] Extra JSON file exists (./assets/json/2602.11543.json), skip PDF parsing.
[13.02.2026 06:54] Paper image links file exists (./assets/img_data/2602.11543.json), skip HTML parsing.
[13.02.2026 06:54] Success.
[13.02.2026 06:54] Downloading and parsing paper https://huggingface.co/papers/2602.11509.
[13.02.2026 06:54] Extra JSON file exists (./assets/json/2602.11509.json), skip PDF parsing.
[13.02.2026 06:54] Paper image links file exists (./assets/img_data/2602.11509.json), skip HTML parsing.
[13.02.2026 06:54] Success.
[13.02.2026 06:54] Downloading and parsing paper https://huggingface.co/papers/2602.08277.
[13.02.2026 06:54] Extra JSON file exists (./assets/json/2602.08277.json), skip PDF parsing.
[13.02.2026 06:54] Paper image links file exists (./assets/img_data/2602.08277.json), skip HTML parsing.
[13.02.2026 06:54] Success.
[13.02.2026 06:54] Downloading and parsing paper https://huggingface.co/papers/2602.10585.
[13.02.2026 06:54] Extra JSON file exists (./assets/json/2602.10585.json), skip PDF parsing.
[13.02.2026 06:54] Paper image links file exists (./assets/img_data/2602.10585.json), skip HTML parsing.
[13.02.2026 06:54] Success.
[13.02.2026 06:54] Enriching papers with extra data.
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 0. Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution.  					AI-generated summary 				 The emergence of multi-agent systems built from large language models (LLMs)...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 1. On-policy distillation is extended through a generalized framework that introduces flexible reference models and reward scaling factors, demonstrating improved performance through reward extrapolation and reward correction techniques.  					AI-generated summary 				 On-policy distillation (OPD), whi...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 2. A fully end-to-end Transformer-based audio tokenizer architecture achieves high-fidelity reconstruction across diverse audio domains and enables superior text-to-speech and automatic speech recognition performance.  					AI-generated summary 				 Discrete audio tokenizers are fundamental to empoweri...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 3. A vision-language-action model enhanced with world model-based reinforcement learning demonstrates improved performance and long-horizon execution capabilities for robotic manipulation tasks.  					AI-generated summary 				 Vision-language-action (VLA) models that directly predict multi-step action ...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 4. Progressive Semantic Illusions use a generative framework with dual-branch Score Distillation Sampling to create vector sketches that transform semantically through sequential stroke additions, achieving superior recognizability and illusion strength.  					AI-generated summary 				 Visual illusions...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 5. LawThinker is an autonomous legal research agent that uses an Explore-Verify-Memorize strategy with a DeepVerifier module to ensure accurate and procedurally compliant legal reasoning through dynamic verification of intermediate steps.  					AI-generated summary 				 Legal reasoning requires not onl...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 6. Diffusion large language models enable parallel token generation and efficient reasoning enhancement through a voting technique that identifies and refines uncertain predictions across multiple samples.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) represent a new paradigm ...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 7. Asymmetric Group Relative Advantage Estimation addresses exploration and difficulty adaptation challenges in reinforcement learning with large language models by dynamically modulating exploration incentives and sample difficulty focus.  					AI-generated summary 				 Reinforcement Learning with Ver...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 8. NarraScore presents a hierarchical framework that uses frozen Vision-Language Models as affective sensors to generate coherent soundtracks for long-form videos by combining global semantic anchors with token-level adaptive modulation.  					AI-generated summary 				 Synthesizing coherent soundtracks...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 9. DeepSight is an open-source project that integrates safety evaluation and diagnosis for large language and multimodal models, enabling white-box insights through unified protocols and specialized toolkits.  					AI-generated summary 				 As the development of Large Models (LMs) progresses rapidly, t...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 10. Budget-constrained tool-augmented agents use a hierarchical world model and intent-aware planning to optimize multi-step task completion under monetary constraints.  					AI-generated summary 				 We study budget-constrained tool-augmented agents, where a large language model must solve multi-step t...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 11. MolmoSpaces presents an open ecosystem with diverse indoor environments and annotated objects for large-scale robot policy benchmarking across multiple tasks and simulators.  					AI-generated summary 				 Deploying robots at scale demands robustness to the long tail of everyday situations. The coun...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 12. Foundation models generate executable environment code to scaffold learning progress in open-ended worlds, enabling agents to acquire long-horizon skills through curriculum control.  					AI-generated summary 				 Open-ended learning frames intelligence as emerging from continual interaction with an...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 13. A trajectory self-distillation framework with direct discriminative optimization improves few-step decoding efficiency in diffusion large language models while maintaining generation quality.  					AI-generated summary 				 Diffusion large language models (DLLMs) have the potential to enable fast te...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 14. Sci-CoE is a two-stage scientific co-evolving framework that enables large language models to self-evolve as both solver and verifier through sparse-to-unsupervised learning transitions, improving scientific reasoning capabilities and evaluation system robustness.  					AI-generated summary 				 Lar...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 15. Gaia2 presents a benchmark for evaluating large language model agents in asynchronous, dynamic environments with temporal constraints and multi-agent collaboration, featuring a write-action verifier for reinforcement learning and revealing trade-offs between reasoning, efficiency, and robustness.  	...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 16. Models require in-context exploration capabilities to scale effectively at test time, but autoregressive generation faces exponential decay in sampling long sequences, which is addressed by a length-incentivized exploration method that improves performance on both in-domain and out-of-domain tasks. ...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 17. ThinkRouter is a confidence-aware routing mechanism that improves reasoning efficiency by switching between discrete token and latent spaces based on model confidence, achieving better accuracy and faster generation.  					AI-generated summary 				 Recent work explores latent reasoning to improve re...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 18. Voxtral Realtime is a streaming speech recognition model trained end-to-end for sub-second latency with performance matching offline systems.  					AI-generated summary 				 We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription qual...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 19. A lightweight 5B unified multimodal model achieves competitive performance through hierarchical feature extraction, learnable think tokens, and progressive training strategies including alignment pre-training, joint supervised fine-tuning, and reinforcement learning with MR-GRPO.  					AI-generated ...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 20. Personalized generative reward models address challenges in adapting language model responses to individual user preferences by using structured evaluation chains and dual-granularity scaling mechanisms for improved generalization and accuracy.  					AI-generated summary 				 Personalized alignment ...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 21. MiniCPM-SALA combines sparse and linear attention mechanisms in a hybrid architecture to enable efficient processing of ultra-long contexts while maintaining model performance and reducing training costs.  					AI-generated summary 				 The evolution of large language models (LLMs) towards applicati...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 22. ScalSelect is a scalable training-free method for selecting representative multimodal data that achieves near-full-dataset performance with significantly reduced computational requirements.  					AI-generated summary 				 Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for adva...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 23. A unified Vision-Language-Action model with a hierarchical architecture combining semantic reasoning and continuous trajectory generation achieves state-of-the-art performance across multiple embodied navigation tasks.  					AI-generated summary 				 Embodied navigation has long been fragmented by t...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 24. A memory-efficient decentralized framework for training mixture-of-experts language models using sparse expert synchronization and expert-merging warm-up strategies.  					AI-generated summary 				 Pretraining large language models (LLMs) typically requires centralized clusters with thousands of hig...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 25. MuRGAt is a benchmark for evaluating fact-level multimodal attribution in complex reasoning tasks, requiring models to provide precise citations for their answers across video, audio, and other modalities.  					AI-generated summary 				 Multimodal large language models (MLLMs) are increasingly used...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 26. Video diffusion model PISCO enables precise instance insertion with sparse keyframe control through variable-information guidance and distribution-preserving temporal masking.  					AI-generated summary 				 The landscape of AI video generation is undergoing a pivotal shift: moving beyond general ge...
[13.02.2026 06:54] ********************************************************************************
[13.02.2026 06:54] Abstract 27. Neural Additive Experts combines multiple specialized networks with a dynamic gating mechanism to balance predictive accuracy and feature interpretability in machine learning models.  					AI-generated summary 				 The trade-off between interpretability and accuracy remains a core challenge in machi...
[13.02.2026 06:54] Read previous papers.
[13.02.2026 06:54] Generating reviews via LLM API.
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#alignment", "#training", "#agents", "#security"], "emoji": "‚öñÔ∏è", "ru": {"title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–µ–π –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞, —á—Ç–æ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –Ω–µ –º–æ–≥—É—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –¥–æ—Å—Ç–∏—á—å 
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#alignment", "#training", "#rl", "#optimization", "#reasoning"], "emoji": "üéì", "ru": {"title": "–£–º–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è: –∫–∞–∫ —É—á–µ–Ω–∏–∫ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —É—á–∏—Ç–µ–ª—è —á–µ—Ä–µ–∑ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è", "desc": "–†–∞–±–æ—Ç–∞ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ ¬´–Ω–∞ –ø–æ–ª–∏—Ç–∏–∫–µ¬ª (on-policy distillation), –ø—Ä–µ–¥–ª–∞–≥–∞—è –æ–±–æ–±—â—ë–Ω
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#optimization", "#audio", "#training", "#architecture"], "emoji": "üéµ", "ru": {"title": "–°–∫–≤–æ–∑–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é –æ–¥–Ω–æ—Ä–æ–¥–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ CAT (Causal Audio Tokenizer with Transformer) ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–Ω–∞
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#robotics", "#rl", "#cv", "#benchmark", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ú–∏—Ä–æ–≤–∞—è –º–æ–¥–µ–ª—å –∫–∞–∫ –æ—Å–Ω–æ–≤–∞ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è GigaBrain-0.5M*, –º–æ–¥–µ–ª—å Vision-Language-Action, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–µ–Ω–∞ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#multimodal"], "emoji": "üé®", "ru": {"title": "–û—Ç —É—Ç–∫–∏ –∫ –æ–≤—Ü–µ: –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∏–ª–ª—é–∑–∏–∏ —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —à—Ç—Ä–∏—Ö–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∏–ª–ª—é–∑–∏–π ‚Äî –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç—Å–∫–∏–∑–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–µ–Ω—è—é—Ç —Å–≤–æ—ë –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ 
[13.02.2026 06:54] Using data from previous issue: {"categories": [], "emoji": "‚öñÔ∏è", "ru": {"title": "–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–≥–µ–Ω—Ç —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "LawThinker ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Explore-Verify-Memorize —Å –º–æ–¥—É–ª–µ–º DeepVerifier –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ –∏
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#reasoning", "#diffusion", "#open_source"], "emoji": "üó≥Ô∏è", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (dLLM), –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç—Ö–æ–¥—è—Ç –æ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –≥–µ–Ω–µ—Ä–∏
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#training", "#rl", "#rlhf", "#optimization", "#reasoning"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ê—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è exploration –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Asymmetric GRAE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#multimodal", "#long_context", "#audio", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–≠–º–æ—Ü–∏–∏ –∫–∞–∫ –∫–æ–º–ø—Ä–µ—Å—Å –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è: —É–º–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º—É–∑—ã–∫–∏ –¥–ª—è –≤–∏–¥–µ–æ", "desc": "NarraScore ‚Äî —ç—Ç–æ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –º—É–∑—ã–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä—Ç–∏—Ç—É—Ä –∫ –¥–ª–∏–Ω–Ω—ã–º –≤–∏–¥–µ–æ
[13.02.2026 06:54] Querying the API.
[13.02.2026 06:54] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DeepSight is an open-source project that integrates safety evaluation and diagnosis for large language and multimodal models, enabling white-box insights through unified protocols and specialized toolkits.  					AI-generated summary 				 As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.
[13.02.2026 06:54] Response: ```json
{
  "desc": "DeepSight ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π –ø—Ä–æ–µ–∫—Ç –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ü—Ä–æ–µ–∫—Ç –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–≤–µ —Å—Ç–∞–¥–∏–∏: –æ—Ü–µ–Ω–∫—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ toolkit DeepSafe –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É —á–µ—Ä–µ–∑ toolkit DeepScan, —Å–≤—è–∑—ã–≤–∞—è –∏—Ö –µ–¥–∏–Ω—ã–º–∏ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö. –í–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ —á—ë—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞, DeepSight –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–µ–ª—ã–π —è—â–∏–∫ –∞–Ω–∞–ª–∏–∑–∞, –ø–æ–∑–≤–æ–ª—è—è –ø–æ–Ω—è—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ –≤–Ω–µ—à–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ —Ä–∏—Å–∫–∏, –Ω–æ –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–∏—á–∏–Ω—ã –∏—Ö –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è. –ü—Ä–æ–µ–∫—Ç –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –Ω–∏–∑–∫–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é, –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å—é –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å—é, —Å—Ç–∞–≤ –ø–µ—Ä–≤—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π.",
  "emoji": "üîç",
  "title": "–û—Ç —á—ë—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞ –∫ –±–µ–ª–æ–º—É: –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π"
}
```
[13.02.2026 06:54] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DeepSight is an open-source project that integrates safety evaluation and diagnosis for large language and multimodal models, enabling white-box insights through unified protocols and specialized toolkits.  					AI-generated summary 				 As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis."

[13.02.2026 06:54] Response: ```python
["BENCHMARK", "MULTIMODAL", "DATASET"]
```
[13.02.2026 06:54] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DeepSight is an open-source project that integrates safety evaluation and diagnosis for large language and multimodal models, enabling white-box insights through unified protocols and specialized toolkits.  					AI-generated summary 				 As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis."

[13.02.2026 06:54] Response: ```python
['OPEN_SOURCE', 'ALIGNMENT', 'INTERPRETABILITY', 'SECURITY']
```
[13.02.2026 06:54] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DeepSight is an innovative open-source project designed to enhance the safety evaluation and diagnosis of large language models (LLMs) and multimodal models (MLLMs). It addresses the limitations of current safety workflows by integrating evaluation and diagnosis into a unified framework, allowing for better insights into both external risks and internal mechanisms. The project features two main toolkits: DeepSafe for safety evaluation and DeepScan for diagnosis, which work together to provide a comprehensive understanding of model safety. By transforming the safety evaluation process from a black-box approach to a white-box insight, DeepSight aims to improve the alignment and overall capabilities of large models.","title":"DeepSight: Unifying Safety Evaluation and Diagnosis for AI Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DeepSight is an innovative open-source project designed to enhance the safety evaluation and diagnosis of large language models (LLMs) and multimodal models (MLLMs). It addresses the limitations of current safety workflows by integrating evaluation and diagnosis into a unified framework, allowing for better insights into both external risks and internal mechanisms. The project features two main toolkits: DeepSafe for safety evaluation and DeepScan for diagnosis, which work together to provide a comprehensive understanding of model safety. By transforming the safety evaluation process from a black-box approach to a white-box insight, DeepSight aims to improve the alignment and overall capabilities of large models.', title='DeepSight: Unifying Safety Evaluation and Diagnosis for AI Models'))
[13.02.2026 06:54] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DeepSightÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÈ°πÁõÆÔºåÊó®Âú®‰∏∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåÂ§öÊ®°ÊÄÅÊ®°ÂûãÊèê‰æõÂÆâÂÖ®ËØÑ‰º∞ÂíåËØäÊñ≠ÁöÑÊï¥ÂêàËß£ÂÜ≥ÊñπÊ°à„ÄÇËØ•È°πÁõÆÈÄöËøáÁªü‰∏ÄÁöÑÂçèËÆÆÂíå‰∏ìÈó®ÁöÑÂ∑•ÂÖ∑ÂåÖÔºåÂÆûÁé∞‰∫Ü‰ªéÈªëÁÆ±Âà∞ÁôΩÁÆ±ÁöÑÂÆâÂÖ®ËØÑ‰º∞ËΩ¨Âèò„ÄÇDeepSightÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÂ∑•ÂÖ∑ÂåÖÔºöDeepSafeÁî®‰∫éÂÆâÂÖ®ËØÑ‰º∞ÔºåDeepScanÁî®‰∫éÂÆâÂÖ®ËØäÊñ≠ÔºåËÉΩÂ§üÈ´òÊïà„ÄÅ‰ΩéÊàêÊú¨Âú∞ËøõË°åÂ§ßËßÑÊ®°Ê®°ÂûãÁöÑÂÆâÂÖ®ËØÑ‰º∞„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåDeepSightËß£ÂÜ≥‰∫ÜÂΩìÂâçÂÆâÂÖ®ËØÑ‰º∞ÂíåËØäÊñ≠‰∏≠Â≠òÂú®ÁöÑÂàÜÁ¶ªÈóÆÈ¢òÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÂíåÂèØËß£ÈáäÊÄß„ÄÇ","title":"DeepSightÔºöÂÆâÂÖ®ËØÑ‰º∞‰∏éËØäÊñ≠ÁöÑÊï¥ÂêàÊñ∞ËåÉÂºè"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DeepSightÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÈ°πÁõÆÔºåÊó®Âú®‰∏∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåÂ§öÊ®°ÊÄÅÊ®°ÂûãÊèê‰æõÂÆâÂÖ®ËØÑ‰º∞ÂíåËØäÊñ≠ÁöÑÊï¥ÂêàËß£ÂÜ≥ÊñπÊ°à„ÄÇËØ•È°πÁõÆÈÄöËøáÁªü‰∏ÄÁöÑÂçèËÆÆÂíå‰∏ìÈó®ÁöÑÂ∑•ÂÖ∑ÂåÖÔºåÂÆûÁé∞‰∫Ü‰ªéÈªëÁÆ±Âà∞ÁôΩÁÆ±ÁöÑÂÆâÂÖ®ËØÑ‰º∞ËΩ¨Âèò„ÄÇDeepSightÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÂ∑•ÂÖ∑ÂåÖÔºöDeepSafeÁî®‰∫éÂÆâÂÖ®ËØÑ‰º∞ÔºåDeepScanÁî®‰∫éÂÆâÂÖ®ËØäÊñ≠ÔºåËÉΩÂ§üÈ´òÊïà„ÄÅ‰ΩéÊàêÊú¨Âú∞ËøõË°åÂ§ßËßÑÊ®°Ê®°ÂûãÁöÑÂÆâÂÖ®ËØÑ‰º∞„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåDeepSightËß£ÂÜ≥‰∫ÜÂΩìÂâçÂÆâÂÖ®ËØÑ‰º∞ÂíåËØäÊñ≠‰∏≠Â≠òÂú®ÁöÑÂàÜÁ¶ªÈóÆÈ¢òÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÂíåÂèØËß£ÈáäÊÄß„ÄÇ', title='DeepSightÔºöÂÆâÂÖ®ËØÑ‰º∞‰∏éËØäÊñ≠ÁöÑÊï¥ÂêàÊñ∞ËåÉÂºè'))
[13.02.2026 06:54] Using data from previous issue: {"categories": [], "emoji": "üí∞", "ru": {"title": "–≠–∫–æ–Ω–æ–º–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –Ω–∞–º–µ—Ä–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ INTENT –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ—à–∞—é—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–Ω–µ—à–Ω–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏ —Å—Ç—Ä–æ–≥–æ–º –±—é
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#open_source", "#benchmark", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–∏—Ç–∏–∫", "desc": "MolmoSpaces –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Ç–∫—Ä—ã—Ç—É—é —ç–∫–æ—Å–∏—Å—Ç–µ–º—É —Å –±–æ–ª–µ–µ —á–µ–º 230 —Ç—ã—Å—è—á–∞–º–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#training", "#agents"], "emoji": "üß†", "ru": {"title": "–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–∏—Ä–æ–≤ —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ–∑ –∫–æ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Dreaming in Code (DiCode) ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –≤ –∫–æ—Ç–æ—Ä–æ–º –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π 
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–°–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º–Ω–æ–≥–æ—Ç–æ–∫–µ–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ —Å–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#reasoning", "#math", "#science", "#open_source", "#benchmark", "#training"], "emoji": "üß¨", "ru": {"title": "–ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —Å–∞–º–∞ —Å–µ–±—è –ø—Ä–æ–≤–µ—Ä—è—Ç—å: –∫–æ—ç–≤–æ–ª—é—Ü–∏—è —Ä–µ—à–∞—Ç–µ–ª—è –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "Sci-CoE –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å–∏—Å—Ç–µ–º—É –Ω–∞—É—á–Ω–æ–≥–æ —Å–æ-—ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–≥
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#rl", "#benchmark", "#optimization", "#agents", "#reasoning"], "emoji": "‚è±Ô∏è", "ru": {"title": "–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –ø–æ–¥ –¥–∞–≤–ª–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏: –∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã –º–µ–∂–¥—É —Ä–∞–∑—É–º–æ–º, —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å—é", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Gaia2 ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ 
[13.02.2026 06:54] Querying the API.
[13.02.2026 06:54] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Models require in-context exploration capabilities to scale effectively at test time, but autoregressive generation faces exponential decay in sampling long sequences, which is addressed by a length-incentivized exploration method that improves performance on both in-domain and out-of-domain tasks.  					AI-generated summary 				 Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context.   Grounded in State Coverage theory, our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, a phenomenon we term the ``Shallow Exploration Trap''.   To bridge this gap, we propose Length-Incentivized Exploration(\method).   This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty, thereby maximizing state coverage in two-step manner.   Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \method effectively incentivize in-context exploration.   As a result, our method achieves an average improvement of 4.4\% on in-domain tasks and a 2.7\% gain on out-of-domain benchmarks.
[13.02.2026 06:54] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤–æ –≤—Ä–µ–º—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, —á—Ç–æ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Length-Incentivized Exploration, –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∏–º—É–ª–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –∫ –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–º—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é —á–µ—Ä–µ–∑ –Ω–∞–≥—Ä–∞–¥—É –∑–∞ –¥–ª–∏–Ω—É —Å —à—Ç—Ä–∞—Ñ–æ–º –∑–∞ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ç–µ–æ—Ä–∏–∏ –ø–æ–∫—Ä—ã—Ç–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—é –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏–∑—É—á–µ–Ω–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ in-context –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 4,4% –¥–ª—è –∑–∞–¥–∞—á –≤–Ω—É—Ç—Ä–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ –Ω–∞ 2,7% –¥–ª—è –∑–∞–¥–∞—á –≤–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.",
  "emoji": "üîç",
  "title": "–ü–æ–æ—â—Ä–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"
}
```
[13.02.2026 06:54] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Models require in-context exploration capabilities to scale effectively at test time, but autoregressive generation faces exponential decay in sampling long sequences, which is addressed by a length-incentivized exploration method that improves performance on both in-domain and out-of-domain tasks.  					AI-generated summary 				 Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context.   Grounded in State Coverage theory, our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, a phenomenon we term the ``Shallow Exploration Trap''.   To bridge this gap, we propose Length-Incentivized Exploration(\method).   This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty, thereby maximizing state coverage in two-step manner.   Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \method effectively incentivize in-context exploration.   As a result, our method achieves an average improvement of 4.4\% on in-domain tasks and a 2.7\% gain on out-of-domain benchmarks."

[13.02.2026 06:54] Response: ```python
["TRAINING", "RLHF"]
```

**Justification:**

- **TRAINING**: The paper proposes "Length-Incentivized Exploration," a method to improve model training and performance through a length-based reward mechanism and redundancy penalty. This directly addresses improving how models are trained and optimized.

- **RLHF**: The paper employs a reward-based approach (length-based reward coupled with redundancy penalty) to guide model behavior during generation, which is characteristic of reinforcement learning from human feedback techniques used to align and improve model outputs.
[13.02.2026 06:54] Error. Failed to parse JSON from LLM. ["TRAINING", "RLHF"]


**Justification:**

- **TRAINING**: The paper proposes "Length-Incentivized Exploration," a method to improve model training and performance through a length-based reward mechanism and redundancy penalty. This directly addresses improving how models are trained and optimized.

- **RLHF**: The paper employs a reward-based approach (length-based reward coupled with redundancy penalty) to guide model behavior during generation, which is characteristic of reinforcement learning from human feedback techniques used to align and improve model outputs.
[13.02.2026 06:54] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Models require in-context exploration capabilities to scale effectively at test time, but autoregressive generation faces exponential decay in sampling long sequences, which is addressed by a length-incentivized exploration method that improves performance on both in-domain and out-of-domain tasks.  					AI-generated summary 				 Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context.   Grounded in State Coverage theory, our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, a phenomenon we term the ``Shallow Exploration Trap''.   To bridge this gap, we propose Length-Incentivized Exploration(\method).   This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty, thereby maximizing state coverage in two-step manner.   Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \method effectively incentivize in-context exploration.   As a result, our method achieves an average improvement of 4.4\% on in-domain tasks and a 2.7\% gain on out-of-domain benchmarks."

[13.02.2026 06:54] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[13.02.2026 06:54] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenge of improving the performance of autoregressive models during test-time by introducing a method called Length-Incentivized Exploration. The authors identify a problem known as the \'Shallow Exploration Trap\', where the likelihood of generating longer sequences decreases exponentially, hindering effective in-context exploration. To overcome this, they propose a reward system that encourages longer reasoning paths while penalizing redundancy, thus enhancing state coverage. Experimental results show that this approach leads to significant performance gains on both in-domain and out-of-domain tasks across various models.","title":"Unlocking Long-Sequence Potential with Length-Incentivized Exploration"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper addresses the challenge of improving the performance of autoregressive models during test-time by introducing a method called Length-Incentivized Exploration. The authors identify a problem known as the 'Shallow Exploration Trap', where the likelihood of generating longer sequences decreases exponentially, hindering effective in-context exploration. To overcome this, they propose a reward system that encourages longer reasoning paths while penalizing redundancy, thus enhancing state coverage. Experimental results show that this approach leads to significant performance gains on both in-domain and out-of-domain tasks across various models.", title='Unlocking Long-Sequence Potential with Length-Incentivized Exploration'))
[13.02.2026 06:54] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊ®°ÂûãÂú®ÊµãËØïÊó∂ÈúÄË¶ÅÂÖ∑Â§á‰∏ä‰∏ãÊñáÊé¢Á¥¢ËÉΩÂäõÔºå‰ª•ÊúâÊïàÊâ©Â±ïÂÖ∂ÊÄßËÉΩ„ÄÇËá™ÂõûÂΩíÁîüÊàêÂú®ÈááÊ†∑ÈïøÂ∫èÂàóÊó∂Èù¢‰∏¥ÊåáÊï∞Ë°∞ÂáèÁöÑÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈïøÂ∫¶ÊøÄÂä±Êé¢Á¥¢ÊñπÊ≥ïÊù•Ëß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÈïøÂ∫¶Â•ñÂä±ÂíåÂÜó‰ΩôÊÉ©ÁΩöÔºåÈºìÂä±Ê®°ÂûãËøõË°åÊõ¥Â§öÊé¢Á¥¢Ôºå‰ªéËÄåÊúÄÂ§ßÂåñÁä∂ÊÄÅË¶ÜÁõñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®‰∏çÂêåÊ®°Âûã‰∏äÂùáËÉΩÊúâÊïàÊèêÂçáÂú®ÂüüÂÜÖÂíåÂüüÂ§ñ‰ªªÂä°ÁöÑË°®Áé∞„ÄÇ","title":"ÊøÄÂä±Êé¢Á¥¢ÔºåÊèêÂçáÊ®°ÂûãÊÄßËÉΩÔºÅ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊ®°ÂûãÂú®ÊµãËØïÊó∂ÈúÄË¶ÅÂÖ∑Â§á‰∏ä‰∏ãÊñáÊé¢Á¥¢ËÉΩÂäõÔºå‰ª•ÊúâÊïàÊâ©Â±ïÂÖ∂ÊÄßËÉΩ„ÄÇËá™ÂõûÂΩíÁîüÊàêÂú®ÈááÊ†∑ÈïøÂ∫èÂàóÊó∂Èù¢‰∏¥ÊåáÊï∞Ë°∞ÂáèÁöÑÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈïøÂ∫¶ÊøÄÂä±Êé¢Á¥¢ÊñπÊ≥ïÊù•Ëß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÈïøÂ∫¶Â•ñÂä±ÂíåÂÜó‰ΩôÊÉ©ÁΩöÔºåÈºìÂä±Ê®°ÂûãËøõË°åÊõ¥Â§öÊé¢Á¥¢Ôºå‰ªéËÄåÊúÄÂ§ßÂåñÁä∂ÊÄÅË¶ÜÁõñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®‰∏çÂêåÊ®°Âûã‰∏äÂùáËÉΩÊúâÊïàÊèêÂçáÂú®ÂüüÂÜÖÂíåÂüüÂ§ñ‰ªªÂä°ÁöÑË°®Áé∞„ÄÇ', title='ÊøÄÂä±Êé¢Á¥¢ÔºåÊèêÂçáÊ®°ÂûãÊÄßËÉΩÔºÅ'))
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "üõ£Ô∏è", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è: –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É —è–≤–Ω—ã–º –∏ —Å–∫—Ä—ã—Ç—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏", "desc": "ThinkRouter ‚Äî —ç—Ç–æ –º–µ—Ö–∞–Ω–∏–∑–º –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –º–µ–∂–¥—É –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã
[13.02.2026 06:54] Using data from previous issue: {"categories": ["#audio", "#open_source", "#low_resource", "#training", "#architecture", "#multilingual"], "emoji": "üéôÔ∏è", "ru": {"title": "–ü–æ—Ç–æ–∫–æ–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –≤ –ø–æ–ª—Å–µ–∫—É–Ω–¥—ã", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Voxtral Realtime ‚Äî –ø–æ—Ç–æ–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏, –æ–±—É—á–µ–Ω–Ω–∞—è end-to-en
[13.02.2026 06:54] Querying the API.
[13.02.2026 06:54] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A lightweight 5B unified multimodal model achieves competitive performance through hierarchical feature extraction, learnable think tokens, and progressive training strategies including alignment pre-training, joint supervised fine-tuning, and reinforcement learning with MR-GRPO.  					AI-generated summary 				 Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.
[13.02.2026 06:54] Response: ```json
{
  "desc": "DeepGen 1.0 ‚Äî —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è 5-–º–∏–ª–ª–∏–∞—Ä–¥–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –±–ª–∞–≥–æ–¥–∞—Ä—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–º—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —Ç–æ–∫–µ–Ω–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Stacked Channel Bridging –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–πËûçÂêà –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ—ë–≤ Vision Language Model —Å –æ–±—É—á–∞–µ–º—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è —Ä—É–∫–æ–≤–æ–¥—è—â–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤. –¢—Ä—ë—Ö—ç—Ç–∞–ø–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è –≤–∫–ª—é—á–∞–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –Ω–∞ –ø–∞—Ä–∞—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç, —Å–æ–≤–º–µ—Å—Ç–Ω—É—é —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –Ω–∞ –≤—ã—Å–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º MR-GRPO –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º —á–µ–ª–æ–≤–µ–∫–∞. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –≤—Å–µ–≥–æ –Ω–∞ 50 –º–∏–ª–ª–∏–æ–Ω–∞—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è, —á—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ data-centric –ø–æ–¥—Ö–æ–¥ —Å–ø–æ—Å–æ–±–Ω—ã –∑–∞–º–µ–Ω–∏—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º.",
  "emoji": "üé®",
  "title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–æ–π –º–æ—â—å—é: 5B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±–≥–æ–Ω—è—é—Ç 80B –≤ –∫–∞—á–µ—Å—Ç–≤–µ"
}
```
[13.02.2026 06:54] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A lightweight 5B unified multimodal model achieves competitive performance through hierarchical feature extraction, learnable think tokens, and progressive training strategies including alignment pre-training, joint supervised fine-tuning, and reinforcement learning with MR-GRPO.  					AI-generated summary 				 Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research."

[13.02.2026 06:54] Response: ```python
['SMALL_MODELS', 'MULTIMODAL', 'TRAINING', 'RLHF', 'DATASET', 'BENCHMARK']
```
[13.02.2026 06:54] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A lightweight 5B unified multimodal model achieves competitive performance through hierarchical feature extraction, learnable think tokens, and progressive training strategies including alignment pre-training, joint supervised fine-tuning, and reinforcement learning with MR-GRPO.  					AI-generated summary 				 Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research."

[13.02.2026 06:54] Response: ```python
['OPTIMIZATION', 'ALIGNMENT', 'OPEN_SOURCE']
```
[13.02.2026 06:54] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces DeepGen 1.0, a lightweight unified multimodal model with only 5 billion parameters that competes with larger models in image generation and editing. It employs hierarchical feature extraction through Stacked Channel Bridging (SCB) and utilizes learnable \'think tokens\' to enhance semantic understanding and control. The training strategy includes alignment pre-training, joint supervised fine-tuning, and reinforcement learning, which collectively improve generation quality and alignment with human preferences. Despite being trained on a smaller dataset, DeepGen 1.0 outperforms larger models on various benchmarks, making it a valuable resource for multimodal research.","title":"DeepGen 1.0: Lightweight Powerhouse in Multimodal AI"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces DeepGen 1.0, a lightweight unified multimodal model with only 5 billion parameters that competes with larger models in image generation and editing. It employs hierarchical feature extraction through Stacked Channel Bridging (SCB) and utilizes learnable 'think tokens' to enhance semantic understanding and control. The training strategy includes alignment pre-training, joint supervised fine-tuning, and reinforcement learning, which collectively improve generation quality and alignment with human preferences. Despite being trained on a smaller dataset, DeepGen 1.0 outperforms larger models on various benchmarks, making it a valuable resource for multimodal research.", title='DeepGen 1.0: Lightweight Powerhouse in Multimodal AI'))
[13.02.2026 06:54] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ßÁöÑ5BÁªü‰∏ÄÂ§öÊ®°ÊÄÅÊ®°ÂûãDeepGen 1.0ÔºåËØ•Ê®°ÂûãÈÄöËøáÂàÜÂ±ÇÁâπÂæÅÊèêÂèñ„ÄÅÂèØÂ≠¶‰π†ÁöÑÊÄùÁª¥Ê†áËÆ∞ÂíåÊ∏êËøõÂºèËÆ≠ÁªÉÁ≠ñÁï•ÂÆûÁé∞‰∫ÜÁ´û‰∫âÂäõÁöÑÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜÂÖãÊúçÁ¥ßÂáëÊ®°ÂûãÂú®ËØ≠‰πâÁêÜËß£ÂíåÁªÜÁ≤íÂ∫¶ÊéßÂà∂ÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄßÔºåÊèêÂá∫‰∫ÜÂ†ÜÂè†ÈÄöÈÅìÊ°•Êé•ÔºàSCBÔºâÊ°ÜÊû∂ÔºåËÉΩÂ§ü‰ªéÂ§ö‰∏™ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂ±ÇÊèêÂèñÁâπÂæÅÂπ∂‰∏éÊÄùÁª¥Ê†áËÆ∞ËûçÂêà„ÄÇËÆ≠ÁªÉÁ≠ñÁï•ÂåÖÊã¨‰∏â‰∏™Èò∂ÊÆµÔºöÂ§ßËßÑÊ®°ÂõæÂÉè-ÊñáÊú¨ÂØπÁöÑÂØπÈΩêÈ¢ÑËÆ≠ÁªÉ„ÄÅËÅîÂêàÁõëÁù£ÂæÆË∞É‰ª•ÂèäÂü∫‰∫éMR-GRPOÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàêË¥®ÈáèÂíå‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂØπÈΩê„ÄÇÂ∞ΩÁÆ°Âè™‰ΩøÁî®‰∫ÜÁ∫¶5000‰∏áÊ†∑Êú¨ÔºåDeepGen 1.0Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÊõ¥Â§ßÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ","title":"ËΩªÈáèÁ∫ßÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÊÄßËÉΩË∂ÖË∂äÊõ¥Â§ßÊ®°ÂûãÔºÅ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ßÁöÑ5BÁªü‰∏ÄÂ§öÊ®°ÊÄÅÊ®°ÂûãDeepGen 1.0ÔºåËØ•Ê®°ÂûãÈÄöËøáÂàÜÂ±ÇÁâπÂæÅÊèêÂèñ„ÄÅÂèØÂ≠¶‰π†ÁöÑÊÄùÁª¥Ê†áËÆ∞ÂíåÊ∏êËøõÂºèËÆ≠ÁªÉÁ≠ñÁï•ÂÆûÁé∞‰∫ÜÁ´û‰∫âÂäõÁöÑÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜÂÖãÊúçÁ¥ßÂáëÊ®°ÂûãÂú®ËØ≠‰πâÁêÜËß£ÂíåÁªÜÁ≤íÂ∫¶ÊéßÂà∂ÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄßÔºåÊèêÂá∫‰∫ÜÂ†ÜÂè†ÈÄöÈÅìÊ°•Êé•ÔºàSCBÔºâÊ°ÜÊû∂ÔºåËÉΩÂ§ü‰ªéÂ§ö‰∏™ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂ±ÇÊèêÂèñÁâπÂæÅÂπ∂‰∏éÊÄùÁª¥Ê†áËÆ∞ËûçÂêà„ÄÇËÆ≠ÁªÉÁ≠ñÁï•ÂåÖÊã¨‰∏â‰∏™Èò∂ÊÆµÔºöÂ§ßËßÑÊ®°ÂõæÂÉè-ÊñáÊú¨ÂØπÁöÑÂØπÈΩêÈ¢ÑËÆ≠ÁªÉ„ÄÅËÅîÂêàÁõëÁù£ÂæÆË∞É‰ª•ÂèäÂü∫‰∫éMR-GRPOÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàêË¥®ÈáèÂíå‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂØπÈΩê„ÄÇÂ∞ΩÁÆ°Âè™‰ΩøÁî®‰∫ÜÁ∫¶5000‰∏áÊ†∑Êú¨ÔºåDeepGen 1.0Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÊõ¥Â§ßÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ', title='ËΩªÈáèÁ∫ßÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÊÄßËÉΩË∂ÖË∂äÊõ¥Â§ßÊ®°ÂûãÔºÅ'))
[13.02.2026 06:54] Querying the API.
[13.02.2026 06:54] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Personalized generative reward models address challenges in adapting language model responses to individual user preferences by using structured evaluation chains and dual-granularity scaling mechanisms for improved generalization and accuracy.  					AI-generated summary 				 Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability.
[13.02.2026 06:54] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç P-GenRM ‚Äî –ø–µ—Ä–≤—É—é –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –ú–æ–¥–µ–ª—å –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–∏–≥–Ω–∞–ª—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ –æ—Ü–µ–Ω–∫–∏, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–µ—Ä—Å–æ–Ω—ã –∏ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤. –ö–ª—é—á–µ–≤–æ–π –∏–Ω–Ω–æ–≤–∞—Ü–∏–µ–π —è–≤–ª—è–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º –¥–≤–æ–π–Ω–æ–π –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –º–æ–¥–µ–ª—å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç —Å—Ö–µ–º—É –æ—Ü–µ–Ω–∫–∏, –∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ—Ö–æ–∂–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 2.31% –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ 3% –±–ª–∞–≥–æ–¥–∞—Ä—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –≤ –º–æ–º–µ–Ω—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.",
  "emoji": "üéØ",
  "title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ –æ—Ü–µ–Ω–∫–∏"
}
```
[13.02.2026 06:54] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Personalized generative reward models address challenges in adapting language model responses to individual user preferences by using structured evaluation chains and dual-granularity scaling mechanisms for improved generalization and accuracy.  					AI-generated summary 				 Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability."

[13.02.2026 06:55] Response: ```python
["RLHF", "TRAINING", "BENCHMARK"]
```

**Justification:**

1. **RLHF**: The paper explicitly discusses personalized reward models for aligning language models with user preferences through reinforcement learning, which is a core RLHF application.

2. **TRAINING**: The paper focuses on improving model training and fine-tuning methods through personalized reward modeling and scaling mechanisms to enhance alignment.

3. **BENCHMARK**: The paper evaluates P-GenRM on "widely-used personalized reward model benchmarks" and demonstrates results on benchmark datasets.
[13.02.2026 06:55] Error. Failed to parse JSON from LLM. ["RLHF", "TRAINING", "BENCHMARK"]


**Justification:**

1. **RLHF**: The paper explicitly discusses personalized reward models for aligning language models with user preferences through reinforcement learning, which is a core RLHF application.

2. **TRAINING**: The paper focuses on improving model training and fine-tuning methods through personalized reward modeling and scaling mechanisms to enhance alignment.

3. **BENCHMARK**: The paper evaluates P-GenRM on "widely-used personalized reward model benchmarks" and demonstrates results on benchmark datasets.
[13.02.2026 06:55] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Personalized generative reward models address challenges in adapting language model responses to individual user preferences by using structured evaluation chains and dual-granularity scaling mechanisms for improved generalization and accuracy.  					AI-generated summary 				 Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability."

[13.02.2026 06:55] Response: ```python
['ALIGNMENT', 'TRANSFER_LEARNING']
```
[13.02.2026 06:55] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces P-GenRM, a Personalized Generative Reward Model designed to enhance the adaptation of language model responses to individual user preferences. It addresses the limitations of existing models by using structured evaluation chains and dual-granularity scaling mechanisms, which help in accurately capturing diverse user preferences. By clustering users into prototypes and adapting scoring schemes at both individual and prototype levels, P-GenRM improves generalization to new users with limited feedback. Empirical results indicate that this model outperforms previous benchmarks, achieving significant improvements in personalized alignment and scalability.","title":"Personalized Responses Made Easy with P-GenRM!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces P-GenRM, a Personalized Generative Reward Model designed to enhance the adaptation of language model responses to individual user preferences. It addresses the limitations of existing models by using structured evaluation chains and dual-granularity scaling mechanisms, which help in accurately capturing diverse user preferences. By clustering users into prototypes and adapting scoring schemes at both individual and prototype levels, P-GenRM improves generalization to new users with limited feedback. Empirical results indicate that this model outperforms previous benchmarks, achieving significant improvements in personalized alignment and scalability.', title='Personalized Responses Made Easy with P-GenRM!'))
[13.02.2026 06:55] Response: ParsedChatCompletionMessage[Article](content='{"desc":"‰∏™ÊÄßÂåñÁîüÊàêÂ•ñÂä±Ê®°ÂûãÊó®Âú®Ëß£ÂÜ≥ËØ≠Ë®ÄÊ®°ÂûãÂìçÂ∫î‰∏™‰ΩìÁî®Êà∑ÂÅèÂ•ΩÁöÑÈÄÇÂ∫îÊÄßÈóÆÈ¢ò„ÄÇËØ•Ê®°ÂûãÈÄöËøáÁªìÊûÑÂåñËØÑ‰º∞ÈìæÂíåÂèåÁ≤íÂ∫¶Áº©ÊîæÊú∫Âà∂ÔºåÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂáÜÁ°ÆÊÄß„ÄÇP-GenRMÊòØÈ¶ñ‰∏™Âú®ÊµãËØïÊó∂Âü∫‰∫éÁî®Êà∑ÁöÑÁº©ÊîæÁöÑ‰∏™ÊÄßÂåñÁîüÊàêÂ•ñÂä±Ê®°ÂûãÔºåËÉΩÂ§üÂ∞ÜÂÅèÂ•Ω‰ø°Âè∑ËΩ¨Âåñ‰∏∫ÈÄÇÂ∫îÊÄßÁöÑ‰∫∫Áâ©ËßíËâ≤ÂíåËØÑÂàÜÊ†áÂáÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåP-GenRMÂú®‰∏™ÊÄßÂåñÂ•ñÂä±Ê®°ÂûãÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÂπ∂Âú®Êú™ËßÅÁî®Êà∑‰∏äË°®Áé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ","title":"‰∏™ÊÄßÂåñÁîüÊàêÂ•ñÂä±Ê®°ÂûãÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑÁî®Êà∑ÈÄÇÂ∫îÊÄß"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='‰∏™ÊÄßÂåñÁîüÊàêÂ•ñÂä±Ê®°ÂûãÊó®Âú®Ëß£ÂÜ≥ËØ≠Ë®ÄÊ®°ÂûãÂìçÂ∫î‰∏™‰ΩìÁî®Êà∑ÂÅèÂ•ΩÁöÑÈÄÇÂ∫îÊÄßÈóÆÈ¢ò„ÄÇËØ•Ê®°ÂûãÈÄöËøáÁªìÊûÑÂåñËØÑ‰º∞ÈìæÂíåÂèåÁ≤íÂ∫¶Áº©ÊîæÊú∫Âà∂ÔºåÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂáÜÁ°ÆÊÄß„ÄÇP-GenRMÊòØÈ¶ñ‰∏™Âú®ÊµãËØïÊó∂Âü∫‰∫éÁî®Êà∑ÁöÑÁº©ÊîæÁöÑ‰∏™ÊÄßÂåñÁîüÊàêÂ•ñÂä±Ê®°ÂûãÔºåËÉΩÂ§üÂ∞ÜÂÅèÂ•Ω‰ø°Âè∑ËΩ¨Âåñ‰∏∫ÈÄÇÂ∫îÊÄßÁöÑ‰∫∫Áâ©ËßíËâ≤ÂíåËØÑÂàÜÊ†áÂáÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåP-GenRMÂú®‰∏™ÊÄßÂåñÂ•ñÂä±Ê®°ÂûãÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÂπ∂Âú®Êú™ËßÅÁî®Êà∑‰∏äË°®Áé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ', title='‰∏™ÊÄßÂåñÁîüÊàêÂ•ñÂä±Ê®°ÂûãÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑÁî®Êà∑ÈÄÇÂ∫îÊÄß'))
[13.02.2026 06:55] Using data from previous issue: {"categories": ["#small_models", "#training", "#architecture", "#long_context", "#optimization", "#inference"], "emoji": "‚ö°", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —É–ª—å—Ç—Ä–∞-–¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "MiniCPM-SALA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ—Ç–æ—Ä–∞—è –æ
[13.02.2026 06:55] Using data from previous issue: {"categories": ["#multimodal", "#training", "#data"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "ScalSelect ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã—Ö –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ 
[13.02.2026 06:55] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#transfer_learning", "#3d", "#architecture", "#synthetic", "#benchmark", "#multimodal", "#agents", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –µ–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å Vision-Language-Action (VLA)
[13.02.2026 06:55] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Å —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –±–µ–∑ —Å—É–ø–µ—Ä–∫–ª–∞—Å—Ç–µ—Ä–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ SPES ‚Äî –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π mixture-of-experts, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –Ω–∞ GPU –ø—É—Ç—ë–º
[13.02.2026 06:55] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#long_context", "#hallucinations", "#audio", "#benchmark", "#video"], "emoji": "üé¨", "ru": {"title": "–í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º–æ—Å—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ç–æ—á–Ω–æ–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", "desc": "MuRGAt ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ
[13.02.2026 06:55] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–¢–æ—á–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å —Ä–µ–¥–∫–∏–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–ª—é—á–µ–≤—ã–º–∏ –∫–∞–¥—Ä–∞–º–∏", "desc": "PISCO ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—Å—Ç–∞–≤–ª—è—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –≤–∏–¥–µ–æ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã
[13.02.2026 06:55] Using data from previous issue: {"categories": ["#interpretability", "#open_source"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å—é —á–µ—Ä–µ–∑ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ —Å–º–µ—Å–∏", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Neural Additive Experts (NAE) ‚Äî –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫—É—é –¥–∏–ª–µ–º–º—É –º–µ–∂–¥—É –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç
[13.02.2026 06:55] Renaming data file.
[13.02.2026 06:55] Renaming previous data. hf_papers.json to ./d/2026-02-13.json
[13.02.2026 06:55] Saving new data file.
[13.02.2026 06:55] Generating page.
[13.02.2026 06:55] Renaming previous page.
[13.02.2026 06:55] Renaming previous data. index.html to ./d/2026-02-13.html
[13.02.2026 06:55] Writing result.
[13.02.2026 06:55] Renaming log file.
[13.02.2026 06:55] Renaming previous data. log.txt to ./logs/2026-02-13_last_log.txt
