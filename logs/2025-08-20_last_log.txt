[20.08.2025 04:15] Read previous papers.
[20.08.2025 04:15] Generating top page (month).
[20.08.2025 04:15] Writing top page (month).
[20.08.2025 05:12] Read previous papers.
[20.08.2025 05:12] Get feed.
[20.08.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13167
[20.08.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14041
[20.08.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13948
[20.08.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.09131
[20.08.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2508.06905
[20.08.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13632
[20.08.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.12669
[20.08.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13992
[20.08.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2508.13139
[20.08.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[20.08.2025 05:12] No deleted papers detected.
[20.08.2025 05:12] Downloading and parsing papers (pdf, html). Total: 9.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.13167.
[20.08.2025 05:12] Extra JSON file exists (./assets/json/2508.13167.json), skip PDF parsing.
[20.08.2025 05:12] Paper image links file exists (./assets/img_data/2508.13167.json), skip HTML parsing.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.14041.
[20.08.2025 05:12] Extra JSON file exists (./assets/json/2508.14041.json), skip PDF parsing.
[20.08.2025 05:12] Paper image links file exists (./assets/img_data/2508.14041.json), skip HTML parsing.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.13948.
[20.08.2025 05:12] Extra JSON file exists (./assets/json/2508.13948.json), skip PDF parsing.
[20.08.2025 05:12] Paper image links file exists (./assets/img_data/2508.13948.json), skip HTML parsing.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.09131.
[20.08.2025 05:12] Extra JSON file exists (./assets/json/2508.09131.json), skip PDF parsing.
[20.08.2025 05:12] Paper image links file exists (./assets/img_data/2508.09131.json), skip HTML parsing.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.06905.
[20.08.2025 05:12] Downloading paper 2508.06905 from http://arxiv.org/pdf/2508.06905v1...
[20.08.2025 05:12] Extracting affiliations from text.
[20.08.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MultiRef: Controllable Image Generation with Multiple Visual References Dongping Chen University of Washington Seattle, USA Ruoxi Chen Zhejiang Wanli University Ningbo, China Siyuan Wu Huazhong University of Science and Technology Wuhan, China 5 2 0 2 9 ] . [ 1 5 0 9 6 0 . 8 0 5 2 : r Sinan Wang Huazhong University of Science and Technology Wuhan, China Shiyun Lang Huazhong University of Science and Technology Wuhan, China Peter Sushko Allen Institute for AI Seattle, USA Gaoyang Jiang Huazhong University of Science and Technology Wuhan, China Yao Wan Huazhong University of Science and Technology Wuhan, China Ranjay Krishna* University of Washington Allen Institute for AI Seattle, USA Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs - either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like c"
[20.08.2025 05:12] Response: ```python
[
    "University of Washington Seattle, USA",
    "Zhejiang Wanli University Ningbo, China",
    "Huazhong University of Science and Technology Wuhan, China",
    "Allen Institute for AI Seattle, USA"
]
```
[20.08.2025 05:12] Deleting PDF ./assets/pdf/2508.06905.pdf.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.13632.
[20.08.2025 05:12] Extra JSON file exists (./assets/json/2508.13632.json), skip PDF parsing.
[20.08.2025 05:12] Paper image links file exists (./assets/img_data/2508.13632.json), skip HTML parsing.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.12669.
[20.08.2025 05:12] Extra JSON file exists (./assets/json/2508.12669.json), skip PDF parsing.
[20.08.2025 05:12] Paper image links file exists (./assets/img_data/2508.12669.json), skip HTML parsing.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.13992.
[20.08.2025 05:12] Extra JSON file exists (./assets/json/2508.13992.json), skip PDF parsing.
[20.08.2025 05:12] Paper image links file exists (./assets/img_data/2508.13992.json), skip HTML parsing.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.13139.
[20.08.2025 05:12] Downloading paper 2508.13139 from http://arxiv.org/pdf/2508.13139v1...
[20.08.2025 05:12] Extracting affiliations from text.
[20.08.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 9 3 1 3 1 . 8 0 5 2 : r Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence LING-HAO CHEN, Tsinghua University, International Digital Economy Academy, China YUHONG ZHANG, Tsinghua University, China ZIXIN YIN, The Hong Kong University of Science and Technology, China ZHIYANG DOU, The University of Hong Kong, China XIN CHEN, ByteDance, United States of America JINGBO WANG, Shanghai Artificial Intelligence Laboratory, China TAKU KOMURA, The University of Hong Kong, China LEI ZHANG, International Digital Economy Academy, China Fig. 1. We propose Motion2Motion, enabling motion transfer across characters with vastly different topologies. From left to right, we show motion transfer results across increasingly different target characters, anaconda king-kobra (in-species transfer) T-rex (cross-species transfer). This work studies the challenge of transfer animations between characters whose skeletal topologies differ substantially. While many techniques have advanced retargeting techniques in decades, transfer motions across diverse topologies remains less-explored. The primary obstacle lies in the inherent topological inconsistency between source and target skeletons, which restricts the establishment of straightforward one-to-one bone correspondences. Besides, the current lack of large-scale paired motion datasets spanning different topological structures severely constrains the development of data-driven approaches. To address these limitations, we introduce Motion2Motion, novel, training-free framework. Simply yet effectively, Motion2Motion works with only one or few example motions on the target skeleton, by accessing sparse set of bone correspondences between the source and target skeletons. Through comprehensive qualitative and quantitative evaluations, we demonstrate that Motion2Motion achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios. The practical utility of ou"
[20.08.2025 05:13] Response: ```python
[
    "Tsinghua University, International Digital Economy Academy, China",
    "Tsinghua University, China",
    "The Hong Kong University of Science and Technology, China",
    "The University of Hong Kong, China",
    "ByteDance, United States of America",
    "Shanghai Artificial Intelligence Laboratory, China",
    "The University of Hong Kong, China",
    "International Digital Economy Academy, China"
]
```
[20.08.2025 05:13] Deleting PDF ./assets/pdf/2508.13139.pdf.
[20.08.2025 05:13] Success.
[20.08.2025 05:13] Enriching papers with extra data.
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 0. Chain-of-Agents (CoA) paradigm enables end-to-end complex problem-solving in LLMs through dynamic agent activation, improving performance via multi-agent distillation and agentic reinforcement learning.  					AI-generated summary 				 Recent advances in large language models (LLMs) and multi-agent s...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 1. LongSplat improves novel view synthesis from long videos with irregular motion through joint optimization, robust pose estimation, and efficient anchor formation.  					AI-generated summary 				 LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos ...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 2. POML addresses challenges in prompting Large Language Models by providing a structured, data-integrated, and format-sensitive markup language with templating and developer tools.  					AI-generated summary 				 Large Language Models (LLMs) require sophisticated prompting, yet current practices face ...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 3. ColorCtrl, a training-free method using Multi-Modal Diffusion Transformers, achieves precise and consistent color editing in images and videos with word-level control and superior performance compared to existing approaches.  					AI-generated summary 				 Text-guided color editing in images and vid...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 4. Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.  					AI-generated summary 				 Visual designers naturally draw inspirat...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 5. OmniTry extends Virtual Try-ON to various wearable objects using a two-stage pipeline that combines unpaired and paired image training to improve object localization and appearance consistency.  					AI-generated summary 				 Virtual Try-ON (VTON) is a practical and widely-applied task, for which mo...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 6. LLMs predict misery scores from text using various prompting strategies, with few-shot approaches outperforming zero-shot, and a gamified framework assessing their adaptability in emotional reasoning tasks.  					AI-generated summary 				 This study investigates the use of Large Language Models (LLM...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 7. MMAU-Pro is a comprehensive benchmark for evaluating audio intelligence in AI systems, assessing 49 unique skills across speech, sound, music, and their combinations, revealing significant limitations in current models.  					AI-generated summary 				 Audio comprehension-including speech, non-speech...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 8. Motion2Motion is a training-free framework that efficiently transfers animations between characters with different skeletal topologies using sparse bone correspondences.  					AI-generated summary 				 This work studies the challenge of transfer animations between characters whose skeletal topologie...
[20.08.2025 05:13] Read previous papers.
[20.08.2025 05:13] Generating reviews via LLM API.
[20.08.2025 05:13] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#agi", "#architecture", "#open_source", "#rl", "#agents", "#training", "#rlhf"], "emoji": "🤖", "ru": {"title": "Chain-of-Agents: Новый подход к решению сложных задач в языковых моделях", "desc": "Статья представляет новую парадигму рассуждений для больших
[20.08.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#cv", "#architecture", "#3d"], "emoji": "🎥", "ru": {"title": "Улучшение синтеза новых ракурсов для длинных видео с помощью LongSplat", "desc": "LongSplat - это новый метод синтеза новых ракурсов из длинных видео с нерегулярным движением камеры. Он испо
[20.08.2025 05:13] Using data from previous issue: {"categories": ["#data", "#multimodal"], "emoji": "🗂️", "ru": {"title": "POML: структурированный подход к промптингу языковых моделей", "desc": "Статья представляет POML - язык разметки для структурированного промптинга больших языковых моделей. POML решает проблемы интеграции данных, чувствительнос
[20.08.2025 05:13] Using data from previous issue: {"categories": ["#diffusion", "#video", "#cv", "#multimodal"], "emoji": "🎨", "ru": {"title": "Точное редактирование цвета без обучения с помощью диффузионных трансформеров", "desc": "ColorCtrl - это метод редактирования цвета в изображениях и видео без дополнительного обучения, использующий мультимо
[20.08.2025 05:13] Querying the API.
[20.08.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.  					AI-generated summary 				 Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/.
[20.08.2025 05:13] Response: {
  "desc": "Исследование показывает, что современные системы генерации изображений испытывают трудности при работе с несколькими визуальными ссылками. Авторы представляют MultiRef-bench - набор данных для оценки способности моделей комбинировать элементы из нескольких изображений. Эксперименты с различными моделями, включая OmniGen, ACE и Show-o, демонстрируют ограниченную эффективность даже передовых систем в этой задаче. Результаты указывают на необходимость разработки более гибких инструментов генеративного ИИ, способных интегрировать множественные источники визуального вдохновения.",
  "emoji": "🎨",
  "title": "Генеративные модели нуждаются в улучшении работы с множественными визуальными ссылками"
}
[20.08.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.  					AI-generated summary 				 Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/."

[20.08.2025 05:13] Response: ```python
['DATASET', 'BENCHMARK', 'CV', 'MULTIMODAL']
```
[20.08.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.  					AI-generated summary 				 Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/."

[20.08.2025 05:13] Response: ```python
['SYNTHETIC', 'OPEN_SOURCE']
```
[20.08.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenges faced by current image generation models when tasked with creating images from multiple visual references. It introduces MultiRef-bench, a new evaluation framework designed to assess the performance of these models using a diverse set of synthetic and real-world samples. The study reveals that even advanced models struggle with multi-reference conditioning, achieving only moderate success rates. The findings emphasize the need for improved generative tools that can better mimic human creativity by integrating various visual inspirations.","title":"Enhancing Image Generation with Multiple Visual References"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenges faced by current image generation models when tasked with creating images from multiple visual references. It introduces MultiRef-bench, a new evaluation framework designed to assess the performance of these models using a diverse set of synthetic and real-world samples. The study reveals that even advanced models struggle with multi-reference conditioning, achieving only moderate success rates. The findings emphasize the need for improved generative tools that can better mimic human creativity by integrating various visual inspirations.', title='Enhancing Image Generation with Multiple Visual References'))
[20.08.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文探讨了使用多个视觉参考进行可控图像生成的任务。我们提出了MultiRef-bench，这是一个包含990个合成样本和1000个真实样本的评估框架，旨在测试多参考图像的生成能力。实验结果显示，即使是最先进的图像生成模型在多参考条件下也面临挑战，最佳模型OmniGen在合成样本上的表现仅为66.6%。这些发现为开发更灵活的人性化创作工具提供了重要方向。","title":"多参考图像生成的挑战与机遇"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文探讨了使用多个视觉参考进行可控图像生成的任务。我们提出了MultiRef-bench，这是一个包含990个合成样本和1000个真实样本的评估框架，旨在测试多参考图像的生成能力。实验结果显示，即使是最先进的图像生成模型在多参考条件下也面临挑战，最佳模型OmniGen在合成样本上的表现仅为66.6%。这些发现为开发更灵活的人性化创作工具提供了重要方向。', title='多参考图像生成的挑战与机遇'))
[20.08.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#data", "#benchmark", "#cv", "#open_source", "#dataset", "#training"], "emoji": "🕶️", "ru": {"title": "Виртуальная примерка любых носимых объектов без масок", "desc": "OmniTry - это унифицированная система, расширяющая возможности виртуальной примерки (VTON) на разл
[20.08.2025 05:13] Using data from previous issue: {"categories": ["#data", "#multimodal", "#reasoning", "#games", "#training"], "emoji": "😢", "ru": {"title": "LLM как эмпаты: оценка способности AI понимать человеческие страдания", "desc": "Это исследование оценивает способность больших языковых моделей (LLM) предсказывать уровень несчастья человека
[20.08.2025 05:13] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#benchmark", "#reasoning", "#agi", "#open_source"], "emoji": "🎧", "ru": {"title": "MMAU-Pro: новый стандарт оценки аудиоинтеллекта ИИ", "desc": "MMAU-Pro - это комплексный бенчмарк для оценки аудиоинтеллекта в системах искусственного интеллекта. Он оценивает
[20.08.2025 05:13] Querying the API.
[20.08.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Motion2Motion is a training-free framework that efficiently transfers animations between characters with different skeletal topologies using sparse bone correspondences.  					AI-generated summary 				 This work studies the challenge of transfer animations between characters whose skeletal topologies differ substantially. While many techniques have advanced retargeting techniques in decades, transfer motions across diverse topologies remains less-explored. The primary obstacle lies in the inherent topological inconsistency between source and target skeletons, which restricts the establishment of straightforward one-to-one bone correspondences. Besides, the current lack of large-scale paired motion datasets spanning different topological structures severely constrains the development of data-driven approaches. To address these limitations, we introduce Motion2Motion, a novel, training-free framework. Simply yet effectively, Motion2Motion works with only one or a few example motions on the target skeleton, by accessing a sparse set of bone correspondences between the source and target skeletons. Through comprehensive qualitative and quantitative evaluations, we demonstrate that Motion2Motion achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios. The practical utility of our approach is further evidenced by its successful integration in downstream applications and user interfaces, highlighting its potential for industrial applications. Code and data are available at https://lhchen.top/Motion2Motion.
[20.08.2025 05:13] Response: {
  "desc": "Motion2Motion - это система для переноса анимации между персонажами с разной скелетной топологией, не требующая обучения. Она использует разреженные соответствия между костями для эффективного переноса движений даже между сильно различающимися скелетами. Система работает с небольшим количеством примеров движений целевого скелета. Эксперименты показали надежную работу как для похожих скелетов, так и для переноса между разными видами.",
  "emoji": "🦴",
  "title": "Универсальный перенос анимации без обучения"
}
[20.08.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Motion2Motion is a training-free framework that efficiently transfers animations between characters with different skeletal topologies using sparse bone correspondences.  					AI-generated summary 				 This work studies the challenge of transfer animations between characters whose skeletal topologies differ substantially. While many techniques have advanced retargeting techniques in decades, transfer motions across diverse topologies remains less-explored. The primary obstacle lies in the inherent topological inconsistency between source and target skeletons, which restricts the establishment of straightforward one-to-one bone correspondences. Besides, the current lack of large-scale paired motion datasets spanning different topological structures severely constrains the development of data-driven approaches. To address these limitations, we introduce Motion2Motion, a novel, training-free framework. Simply yet effectively, Motion2Motion works with only one or a few example motions on the target skeleton, by accessing a sparse set of bone correspondences between the source and target skeletons. Through comprehensive qualitative and quantitative evaluations, we demonstrate that Motion2Motion achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios. The practical utility of our approach is further evidenced by its successful integration in downstream applications and user interfaces, highlighting its potential for industrial applications. Code and data are available at https://lhchen.top/Motion2Motion."

[20.08.2025 05:13] Response: ```python
['3D', 'DATASET']
```
[20.08.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Motion2Motion is a training-free framework that efficiently transfers animations between characters with different skeletal topologies using sparse bone correspondences.  					AI-generated summary 				 This work studies the challenge of transfer animations between characters whose skeletal topologies differ substantially. While many techniques have advanced retargeting techniques in decades, transfer motions across diverse topologies remains less-explored. The primary obstacle lies in the inherent topological inconsistency between source and target skeletons, which restricts the establishment of straightforward one-to-one bone correspondences. Besides, the current lack of large-scale paired motion datasets spanning different topological structures severely constrains the development of data-driven approaches. To address these limitations, we introduce Motion2Motion, a novel, training-free framework. Simply yet effectively, Motion2Motion works with only one or a few example motions on the target skeleton, by accessing a sparse set of bone correspondences between the source and target skeletons. Through comprehensive qualitative and quantitative evaluations, we demonstrate that Motion2Motion achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios. The practical utility of our approach is further evidenced by its successful integration in downstream applications and user interfaces, highlighting its potential for industrial applications. Code and data are available at https://lhchen.top/Motion2Motion."

[20.08.2025 05:13] Response: ```python
["TRANSFER_LEARNING"]
```
[20.08.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Motion2Motion is a novel framework designed to transfer animations between characters with different skeletal structures without the need for extensive training. It addresses the challenge of establishing bone correspondences when the source and target skeletons have significant topological differences. By utilizing a sparse set of correspondences and requiring only a few example motions, Motion2Motion efficiently facilitates motion transfer. The framework has been evaluated rigorously, showing strong performance in both similar and diverse skeletal scenarios, making it suitable for practical applications in the industry.","title":"Effortless Animation Transfer Across Diverse Skeletons"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Motion2Motion is a novel framework designed to transfer animations between characters with different skeletal structures without the need for extensive training. It addresses the challenge of establishing bone correspondences when the source and target skeletons have significant topological differences. By utilizing a sparse set of correspondences and requiring only a few example motions, Motion2Motion efficiently facilitates motion transfer. The framework has been evaluated rigorously, showing strong performance in both similar and diverse skeletal scenarios, making it suitable for practical applications in the industry.', title='Effortless Animation Transfer Across Diverse Skeletons'))
[20.08.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Motion2Motion是一个无需训练的框架，能够高效地在不同骨骼拓扑的角色之间转移动画。该方法解决了源骨骼和目标骨骼之间的拓扑不一致性问题，利用稀疏的骨骼对应关系进行动画转移。尽管现有的重定向技术已经取得了一定进展，但在多样化拓扑结构之间的运动转移仍然较少被探索。通过定性和定量评估，我们证明了Motion2Motion在相似骨骼和跨物种骨骼转移场景中都能实现高效可靠的性能。","title":"高效动画转移，跨越骨骼拓扑的界限"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Motion2Motion是一个无需训练的框架，能够高效地在不同骨骼拓扑的角色之间转移动画。该方法解决了源骨骼和目标骨骼之间的拓扑不一致性问题，利用稀疏的骨骼对应关系进行动画转移。尽管现有的重定向技术已经取得了一定进展，但在多样化拓扑结构之间的运动转移仍然较少被探索。通过定性和定量评估，我们证明了Motion2Motion在相似骨骼和跨物种骨骼转移场景中都能实现高效可靠的性能。', title='高效动画转移，跨越骨骼拓扑的界限'))
[20.08.2025 05:13] Renaming data file.
[20.08.2025 05:13] Renaming previous data. hf_papers.json to ./d/2025-08-20.json
[20.08.2025 05:13] Saving new data file.
[20.08.2025 05:13] Generating page.
[20.08.2025 05:13] Renaming previous page.
[20.08.2025 05:13] Renaming previous data. index.html to ./d/2025-08-20.html
[20.08.2025 05:13] Writing result.
[20.08.2025 05:13] Renaming log file.
[20.08.2025 05:13] Renaming previous data. log.txt to ./logs/2025-08-20_last_log.txt
