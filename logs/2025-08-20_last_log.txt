[20.08.2025 03:39] Read previous papers.
[20.08.2025 03:39] Generating top page (month).
[20.08.2025 03:39] Writing top page (month).
[20.08.2025 04:15] Read previous papers.
[20.08.2025 04:15] Get feed.
[20.08.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13167
[20.08.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14041
[20.08.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13948
[20.08.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.09131
[20.08.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13632
[20.08.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13992
[20.08.2025 04:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.12669
[20.08.2025 04:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[20.08.2025 04:15] No deleted papers detected.
[20.08.2025 04:15] Downloading and parsing papers (pdf, html). Total: 7.
[20.08.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2508.13167.
[20.08.2025 04:15] Downloading paper 2508.13167 from http://arxiv.org/pdf/2508.13167v1...
[20.08.2025 04:15] Extracting affiliations from text.
[20.08.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL "
[20.08.2025 04:15] Response: []
[20.08.2025 04:15] Extracting affiliations from text.
[20.08.2025 04:15] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RLRecent advances in large language models (LLMs) and multi-agent systems have demonstrated remarkable capabilities in complex problem-solving tasks such as deep research, vibe coding, and mathematical reasoning. However, most existing multi-agent systems are built upon manual prompt/workflow engineering with sophisticated agent frameworks, making them computationally inefficient, less capable, and can not benefit from data-centric learning. In this work, we introduce Chain-of-Agents (CoA), novel paradigm of LLM reasoning that enables native end-to-end complex problem-solving in the same way as multi-agent system (i.e., multi-turn problem solving with multiple tools and multiple agents) within one model. In chain-of-agents problem-solving, the model dynamically activates different tool agents and role-playing agents to simulate multi-agent collaboration in an end-to-end fashion. To elicit end-to-end chain-of-agents problem-solving abilities in LLMs, we introduce multi-agent distillation framework to distill state-ofthe-art multi-agent systems into chain-of-agents trajectories for agentic supervised fine-tuning. We then use agentic reinforcement learning on verifiable agentic tasks to further improve the models capabilities on chain-of-agents problem solving. We call the resulting models Agent Foundation Models (AFMs). Our empirical studies demonstrate that AFM establishes new state-of-the-art performance across diverse benchmarks in both web agent and code agent settings. We make the entire research, including the model weights, code for training and evaluation, and the training data, fully open-sourced, which offers solid starting point for future research on agent models and agentic RL. Date: August 20, 2025 Open Source: (cid:209) Project Code Correspondence: Wangchunshu Zhou at zhouwangchunshu@oppo.com Models ƒ± Datasets 5 2 0 2 6 ] A . [ 1 7 6 1 3 1 . 8 0 5 2 : r Figure 1 Performance comparison of AFM with the proposed Chain-of-Action paradigm against state-of-the-art tool-integrated reasoning (TIR) methods on GAIA, BrowseComp, HLE, and AIME25 benchmarks. AFM demonstrates consistent effectiveness across web agent and code agent benchmarks.3.2.3 4 4 4 5 5 8 8 8 9 914 14 16 5 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 5.1 Computational Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 5.2 Generalization on Unseen Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Agentic Test-Time Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 6 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 20 6.1 Multi-Agent Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Tool-Integrated Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 Reinforcement Learning for Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 8 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 18"
[20.08.2025 04:15] Mistral response. {"id": "af9e73ed9ca642be87c27889dea58931", "created": 1755663314, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1211, "total_tokens": 1213, "completion_tokens": 2}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "[]"}}]}
[20.08.2025 04:15] Response: []
[20.08.2025 04:15] Deleting PDF ./assets/pdf/2508.13167.pdf.
[20.08.2025 04:15] Success.
[20.08.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2508.14041.
[20.08.2025 04:15] Extra JSON file exists (./assets/json/2508.14041.json), skip PDF parsing.
[20.08.2025 04:15] Paper image links file exists (./assets/img_data/2508.14041.json), skip HTML parsing.
[20.08.2025 04:15] Success.
[20.08.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2508.13948.
[20.08.2025 04:15] Extra JSON file exists (./assets/json/2508.13948.json), skip PDF parsing.
[20.08.2025 04:15] Paper image links file exists (./assets/img_data/2508.13948.json), skip HTML parsing.
[20.08.2025 04:15] Success.
[20.08.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2508.09131.
[20.08.2025 04:15] Extra JSON file exists (./assets/json/2508.09131.json), skip PDF parsing.
[20.08.2025 04:15] Paper image links file exists (./assets/img_data/2508.09131.json), skip HTML parsing.
[20.08.2025 04:15] Success.
[20.08.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2508.13632.
[20.08.2025 04:15] Extra JSON file exists (./assets/json/2508.13632.json), skip PDF parsing.
[20.08.2025 04:15] Paper image links file exists (./assets/img_data/2508.13632.json), skip HTML parsing.
[20.08.2025 04:15] Success.
[20.08.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2508.13992.
[20.08.2025 04:15] Extra JSON file exists (./assets/json/2508.13992.json), skip PDF parsing.
[20.08.2025 04:15] Paper image links file exists (./assets/img_data/2508.13992.json), skip HTML parsing.
[20.08.2025 04:15] Success.
[20.08.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2508.12669.
[20.08.2025 04:15] Extra JSON file exists (./assets/json/2508.12669.json), skip PDF parsing.
[20.08.2025 04:15] Paper image links file exists (./assets/img_data/2508.12669.json), skip HTML parsing.
[20.08.2025 04:15] Success.
[20.08.2025 04:15] Enriching papers with extra data.
[20.08.2025 04:15] ********************************************************************************
[20.08.2025 04:15] Abstract 0. Chain-of-Agents (CoA) paradigm enables end-to-end complex problem-solving in LLMs through dynamic agent activation, improving performance via multi-agent distillation and agentic reinforcement learning.  					AI-generated summary 				 Recent advances in large language models (LLMs) and multi-agent s...
[20.08.2025 04:15] ********************************************************************************
[20.08.2025 04:15] Abstract 1. LongSplat improves novel view synthesis from long videos with irregular motion through joint optimization, robust pose estimation, and efficient anchor formation.  					AI-generated summary 				 LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos ...
[20.08.2025 04:15] ********************************************************************************
[20.08.2025 04:15] Abstract 2. POML addresses challenges in prompting Large Language Models by providing a structured, data-integrated, and format-sensitive markup language with templating and developer tools.  					AI-generated summary 				 Large Language Models (LLMs) require sophisticated prompting, yet current practices face ...
[20.08.2025 04:15] ********************************************************************************
[20.08.2025 04:15] Abstract 3. ColorCtrl, a training-free method using Multi-Modal Diffusion Transformers, achieves precise and consistent color editing in images and videos with word-level control and superior performance compared to existing approaches.  					AI-generated summary 				 Text-guided color editing in images and vid...
[20.08.2025 04:15] ********************************************************************************
[20.08.2025 04:15] Abstract 4. OmniTry extends Virtual Try-ON to various wearable objects using a two-stage pipeline that combines unpaired and paired image training to improve object localization and appearance consistency.  					AI-generated summary 				 Virtual Try-ON (VTON) is a practical and widely-applied task, for which mo...
[20.08.2025 04:15] ********************************************************************************
[20.08.2025 04:15] Abstract 5. MMAU-Pro is a comprehensive benchmark for evaluating audio intelligence in AI systems, assessing 49 unique skills across speech, sound, music, and their combinations, revealing significant limitations in current models.  					AI-generated summary 				 Audio comprehension-including speech, non-speech...
[20.08.2025 04:15] ********************************************************************************
[20.08.2025 04:15] Abstract 6. LLMs predict misery scores from text using various prompting strategies, with few-shot approaches outperforming zero-shot, and a gamified framework assessing their adaptability in emotional reasoning tasks.  					AI-generated summary 				 This study investigates the use of Large Language Models (LLM...
[20.08.2025 04:15] Read previous papers.
[20.08.2025 04:15] Generating reviews via LLM API.
[20.08.2025 04:15] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#agi", "#architecture", "#open_source", "#rl", "#agents", "#training", "#rlhf"], "emoji": "ü§ñ", "ru": {"title": "Chain-of-Agents: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö
[20.08.2025 04:15] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#cv", "#architecture", "#3d"], "emoji": "üé•", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é LongSplat", "desc": "LongSplat - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –∏–∑ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –Ω–µ—Ä–µ–≥—É–ª—è—Ä–Ω—ã–º –¥–≤–∏–∂–µ–Ω–∏–µ–º –∫–∞–º–µ—Ä—ã. –û–Ω –∏—Å–ø–æ
[20.08.2025 04:15] Using data from previous issue: {"categories": ["#data", "#multimodal"], "emoji": "üóÇÔ∏è", "ru": {"title": "POML: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥—É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç POML - —è–∑—ã–∫ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. POML —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö, —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å
[20.08.2025 04:15] Using data from previous issue: {"categories": ["#diffusion", "#video", "#cv", "#multimodal"], "emoji": "üé®", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–≤–µ—Ç–∞ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "ColorCtrl - —ç—Ç–æ –º–µ—Ç–æ–¥ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–≤–µ—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –∏ –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º—É–ª—å—Ç–∏–º–æ
[20.08.2025 04:15] Using data from previous issue: {"categories": ["#optimization", "#data", "#benchmark", "#cv", "#open_source", "#dataset", "#training"], "emoji": "üï∂Ô∏è", "ru": {"title": "–í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–∏–º–µ—Ä–∫–∞ –ª—é–±—ã—Ö –Ω–æ—Å–∏–º—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –±–µ–∑ –º–∞—Å–æ–∫", "desc": "OmniTry - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, —Ä–∞—Å—à–∏—Ä—è—é—â–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–º–µ—Ä–∫–∏ (VTON) –Ω–∞ —Ä–∞–∑–ª
[20.08.2025 04:15] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#benchmark", "#reasoning", "#agi", "#open_source"], "emoji": "üéß", "ru": {"title": "MMAU-Pro: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –∞—É–¥–∏–æ–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –ò–ò", "desc": "MMAU-Pro - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞—É–¥–∏–æ–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –û–Ω –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç
[20.08.2025 04:15] Using data from previous issue: {"categories": ["#data", "#multimodal", "#reasoning", "#games", "#training"], "emoji": "üò¢", "ru": {"title": "LLM –∫–∞–∫ —ç–º–ø–∞—Ç—ã: –æ—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ AI –ø–æ–Ω–∏–º–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ —Å—Ç—Ä–∞–¥–∞–Ω–∏—è", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —É—Ä–æ–≤–µ–Ω—å –Ω–µ—Å—á–∞—Å—Ç—å—è —á–µ–ª–æ–≤–µ–∫–∞
[20.08.2025 04:15] Renaming data file.
[20.08.2025 04:15] Renaming previous data. hf_papers.json to ./d/2025-08-20.json
[20.08.2025 04:15] Saving new data file.
[20.08.2025 04:15] Generating page.
[20.08.2025 04:15] Renaming previous page.
[20.08.2025 04:15] Renaming previous data. index.html to ./d/2025-08-20.html
[20.08.2025 04:15] Writing result.
[20.08.2025 04:15] Renaming log file.
[20.08.2025 04:15] Renaming previous data. log.txt to ./logs/2025-08-20_last_log.txt
