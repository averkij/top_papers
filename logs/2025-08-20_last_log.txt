[20.08.2025 04:15] Read previous papers.
[20.08.2025 04:15] Generating top page (month).
[20.08.2025 04:15] Writing top page (month).
[20.08.2025 05:12] Read previous papers.
[20.08.2025 05:12] Get feed.
[20.08.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13167
[20.08.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14041
[20.08.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13948
[20.08.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.09131
[20.08.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2508.06905
[20.08.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13632
[20.08.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.12669
[20.08.2025 05:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13992
[20.08.2025 05:12] Extract page data from URL. URL: https://huggingface.co/papers/2508.13139
[20.08.2025 05:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[20.08.2025 05:12] No deleted papers detected.
[20.08.2025 05:12] Downloading and parsing papers (pdf, html). Total: 9.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.13167.
[20.08.2025 05:12] Extra JSON file exists (./assets/json/2508.13167.json), skip PDF parsing.
[20.08.2025 05:12] Paper image links file exists (./assets/img_data/2508.13167.json), skip HTML parsing.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.14041.
[20.08.2025 05:12] Extra JSON file exists (./assets/json/2508.14041.json), skip PDF parsing.
[20.08.2025 05:12] Paper image links file exists (./assets/img_data/2508.14041.json), skip HTML parsing.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.13948.
[20.08.2025 05:12] Extra JSON file exists (./assets/json/2508.13948.json), skip PDF parsing.
[20.08.2025 05:12] Paper image links file exists (./assets/img_data/2508.13948.json), skip HTML parsing.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.09131.
[20.08.2025 05:12] Extra JSON file exists (./assets/json/2508.09131.json), skip PDF parsing.
[20.08.2025 05:12] Paper image links file exists (./assets/img_data/2508.09131.json), skip HTML parsing.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.06905.
[20.08.2025 05:12] Downloading paper 2508.06905 from http://arxiv.org/pdf/2508.06905v1...
[20.08.2025 05:12] Extracting affiliations from text.
[20.08.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MultiRef: Controllable Image Generation with Multiple Visual References Dongping Chen University of Washington Seattle, USA Ruoxi Chen Zhejiang Wanli University Ningbo, China Siyuan Wu Huazhong University of Science and Technology Wuhan, China 5 2 0 2 9 ] . [ 1 5 0 9 6 0 . 8 0 5 2 : r Sinan Wang Huazhong University of Science and Technology Wuhan, China Shiyun Lang Huazhong University of Science and Technology Wuhan, China Peter Sushko Allen Institute for AI Seattle, USA Gaoyang Jiang Huazhong University of Science and Technology Wuhan, China Yao Wan Huazhong University of Science and Technology Wuhan, China Ranjay Krishna* University of Washington Allen Institute for AI Seattle, USA Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs - either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like c"
[20.08.2025 05:12] Response: ```python
[
    "University of Washington Seattle, USA",
    "Zhejiang Wanli University Ningbo, China",
    "Huazhong University of Science and Technology Wuhan, China",
    "Allen Institute for AI Seattle, USA"
]
```
[20.08.2025 05:12] Deleting PDF ./assets/pdf/2508.06905.pdf.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.13632.
[20.08.2025 05:12] Extra JSON file exists (./assets/json/2508.13632.json), skip PDF parsing.
[20.08.2025 05:12] Paper image links file exists (./assets/img_data/2508.13632.json), skip HTML parsing.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.12669.
[20.08.2025 05:12] Extra JSON file exists (./assets/json/2508.12669.json), skip PDF parsing.
[20.08.2025 05:12] Paper image links file exists (./assets/img_data/2508.12669.json), skip HTML parsing.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.13992.
[20.08.2025 05:12] Extra JSON file exists (./assets/json/2508.13992.json), skip PDF parsing.
[20.08.2025 05:12] Paper image links file exists (./assets/img_data/2508.13992.json), skip HTML parsing.
[20.08.2025 05:12] Success.
[20.08.2025 05:12] Downloading and parsing paper https://huggingface.co/papers/2508.13139.
[20.08.2025 05:12] Downloading paper 2508.13139 from http://arxiv.org/pdf/2508.13139v1...
[20.08.2025 05:12] Extracting affiliations from text.
[20.08.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 9 3 1 3 1 . 8 0 5 2 : r Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence LING-HAO CHEN, Tsinghua University, International Digital Economy Academy, China YUHONG ZHANG, Tsinghua University, China ZIXIN YIN, The Hong Kong University of Science and Technology, China ZHIYANG DOU, The University of Hong Kong, China XIN CHEN, ByteDance, United States of America JINGBO WANG, Shanghai Artificial Intelligence Laboratory, China TAKU KOMURA, The University of Hong Kong, China LEI ZHANG, International Digital Economy Academy, China Fig. 1. We propose Motion2Motion, enabling motion transfer across characters with vastly different topologies. From left to right, we show motion transfer results across increasingly different target characters, anaconda king-kobra (in-species transfer) T-rex (cross-species transfer). This work studies the challenge of transfer animations between characters whose skeletal topologies differ substantially. While many techniques have advanced retargeting techniques in decades, transfer motions across diverse topologies remains less-explored. The primary obstacle lies in the inherent topological inconsistency between source and target skeletons, which restricts the establishment of straightforward one-to-one bone correspondences. Besides, the current lack of large-scale paired motion datasets spanning different topological structures severely constrains the development of data-driven approaches. To address these limitations, we introduce Motion2Motion, novel, training-free framework. Simply yet effectively, Motion2Motion works with only one or few example motions on the target skeleton, by accessing sparse set of bone correspondences between the source and target skeletons. Through comprehensive qualitative and quantitative evaluations, we demonstrate that Motion2Motion achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios. The practical utility of ou"
[20.08.2025 05:13] Response: ```python
[
    "Tsinghua University, International Digital Economy Academy, China",
    "Tsinghua University, China",
    "The Hong Kong University of Science and Technology, China",
    "The University of Hong Kong, China",
    "ByteDance, United States of America",
    "Shanghai Artificial Intelligence Laboratory, China",
    "The University of Hong Kong, China",
    "International Digital Economy Academy, China"
]
```
[20.08.2025 05:13] Deleting PDF ./assets/pdf/2508.13139.pdf.
[20.08.2025 05:13] Success.
[20.08.2025 05:13] Enriching papers with extra data.
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 0. Chain-of-Agents (CoA) paradigm enables end-to-end complex problem-solving in LLMs through dynamic agent activation, improving performance via multi-agent distillation and agentic reinforcement learning.  					AI-generated summary 				 Recent advances in large language models (LLMs) and multi-agent s...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 1. LongSplat improves novel view synthesis from long videos with irregular motion through joint optimization, robust pose estimation, and efficient anchor formation.  					AI-generated summary 				 LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos ...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 2. POML addresses challenges in prompting Large Language Models by providing a structured, data-integrated, and format-sensitive markup language with templating and developer tools.  					AI-generated summary 				 Large Language Models (LLMs) require sophisticated prompting, yet current practices face ...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 3. ColorCtrl, a training-free method using Multi-Modal Diffusion Transformers, achieves precise and consistent color editing in images and videos with word-level control and superior performance compared to existing approaches.  					AI-generated summary 				 Text-guided color editing in images and vid...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 4. Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.  					AI-generated summary 				 Visual designers naturally draw inspirat...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 5. OmniTry extends Virtual Try-ON to various wearable objects using a two-stage pipeline that combines unpaired and paired image training to improve object localization and appearance consistency.  					AI-generated summary 				 Virtual Try-ON (VTON) is a practical and widely-applied task, for which mo...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 6. LLMs predict misery scores from text using various prompting strategies, with few-shot approaches outperforming zero-shot, and a gamified framework assessing their adaptability in emotional reasoning tasks.  					AI-generated summary 				 This study investigates the use of Large Language Models (LLM...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 7. MMAU-Pro is a comprehensive benchmark for evaluating audio intelligence in AI systems, assessing 49 unique skills across speech, sound, music, and their combinations, revealing significant limitations in current models.  					AI-generated summary 				 Audio comprehension-including speech, non-speech...
[20.08.2025 05:13] ********************************************************************************
[20.08.2025 05:13] Abstract 8. Motion2Motion is a training-free framework that efficiently transfers animations between characters with different skeletal topologies using sparse bone correspondences.  					AI-generated summary 				 This work studies the challenge of transfer animations between characters whose skeletal topologie...
[20.08.2025 05:13] Read previous papers.
[20.08.2025 05:13] Generating reviews via LLM API.
[20.08.2025 05:13] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#agi", "#architecture", "#open_source", "#rl", "#agents", "#training", "#rlhf"], "emoji": "ğŸ¤–", "ru": {"title": "Chain-of-Agents: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ…
[20.08.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#cv", "#architecture", "#3d"], "emoji": "ğŸ¥", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LongSplat", "desc": "LongSplat - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸Ğ· Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾
[20.08.2025 05:13] Using data from previous issue: {"categories": ["#data", "#multimodal"], "emoji": "ğŸ—‚ï¸", "ru": {"title": "POML: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ POML - ÑĞ·Ñ‹Ğº Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. POML Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ñ
[20.08.2025 05:13] Using data from previous issue: {"categories": ["#diffusion", "#video", "#cv", "#multimodal"], "emoji": "ğŸ¨", "ru": {"title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†Ğ²ĞµÑ‚Ğ° Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²", "desc": "ColorCtrl - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ†Ğ²ĞµÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾
[20.08.2025 05:13] Querying the API.
[20.08.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.  					AI-generated summary 				 Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/.
[20.08.2025 05:13] Response: {
  "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ MultiRef-bench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ OmniGen, ACE Ğ¸ Show-o, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ.",
  "emoji": "ğŸ¨",
  "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼Ğ¸"
}
[20.08.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.  					AI-generated summary 				 Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/."

[20.08.2025 05:13] Response: ```python
['DATASET', 'BENCHMARK', 'CV', 'MULTIMODAL']
```
[20.08.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.  					AI-generated summary 				 Visual designers naturally draw inspiration from multiple visual references, combining diverse elements and aesthetic principles to create artwork. However, current image generative frameworks predominantly rely on single-source inputs -- either text prompts or individual reference images. In this paper, we focus on the task of controllable image generation using multiple visual references. We introduce MultiRef-bench, a rigorous evaluation framework comprising 990 synthetic and 1,000 real-world samples that require incorporating visual content from multiple reference images. The synthetic samples are synthetically generated through our data engine RefBlend, with 10 reference types and 33 reference combinations. Based on RefBlend, we further construct a dataset MultiRef containing 38k high-quality images to facilitate further research. Our experiments across three interleaved image-text models (i.e., OmniGen, ACE, and Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that even state-of-the-art systems struggle with multi-reference conditioning, with the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in real-world cases on average compared to the golden answer. These findings provide valuable directions for developing more flexible and human-like creative tools that can effectively integrate multiple sources of visual inspiration. The dataset is publicly available at: https://multiref.github.io/."

[20.08.2025 05:13] Response: ```python
['SYNTHETIC', 'OPEN_SOURCE']
```
[20.08.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenges faced by current image generation models when tasked with creating images from multiple visual references. It introduces MultiRef-bench, a new evaluation framework designed to assess the performance of these models using a diverse set of synthetic and real-world samples. The study reveals that even advanced models struggle with multi-reference conditioning, achieving only moderate success rates. The findings emphasize the need for improved generative tools that can better mimic human creativity by integrating various visual inspirations.","title":"Enhancing Image Generation with Multiple Visual References"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the challenges faced by current image generation models when tasked with creating images from multiple visual references. It introduces MultiRef-bench, a new evaluation framework designed to assess the performance of these models using a diverse set of synthetic and real-world samples. The study reveals that even advanced models struggle with multi-reference conditioning, achieving only moderate success rates. The findings emphasize the need for improved generative tools that can better mimic human creativity by integrating various visual inspirations.', title='Enhancing Image Generation with Multiple Visual References'))
[20.08.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å¤šä¸ªè§†è§‰å‚è€ƒè¿›è¡Œå¯æ§å›¾åƒç”Ÿæˆçš„ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†MultiRef-benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«990ä¸ªåˆæˆæ ·æœ¬å’Œ1000ä¸ªçœŸå®æ ·æœ¬çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æµ‹è¯•å¤šå‚è€ƒå›¾åƒçš„ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤šå‚è€ƒæ¡ä»¶ä¸‹ä¹Ÿé¢ä¸´æŒ‘æˆ˜ï¼Œæœ€ä½³æ¨¡å‹OmniGenåœ¨åˆæˆæ ·æœ¬ä¸Šçš„è¡¨ç°ä»…ä¸º66.6%ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘æ›´çµæ´»çš„äººæ€§åŒ–åˆ›ä½œå·¥å…·æä¾›äº†é‡è¦æ–¹å‘ã€‚","title":"å¤šå‚è€ƒå›¾åƒç”Ÿæˆçš„æŒ‘æˆ˜ä¸æœºé‡"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å¤šä¸ªè§†è§‰å‚è€ƒè¿›è¡Œå¯æ§å›¾åƒç”Ÿæˆçš„ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†MultiRef-benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«990ä¸ªåˆæˆæ ·æœ¬å’Œ1000ä¸ªçœŸå®æ ·æœ¬çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æµ‹è¯•å¤šå‚è€ƒå›¾åƒçš„ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤šå‚è€ƒæ¡ä»¶ä¸‹ä¹Ÿé¢ä¸´æŒ‘æˆ˜ï¼Œæœ€ä½³æ¨¡å‹OmniGenåœ¨åˆæˆæ ·æœ¬ä¸Šçš„è¡¨ç°ä»…ä¸º66.6%ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘æ›´çµæ´»çš„äººæ€§åŒ–åˆ›ä½œå·¥å…·æä¾›äº†é‡è¦æ–¹å‘ã€‚', title='å¤šå‚è€ƒå›¾åƒç”Ÿæˆçš„æŒ‘æˆ˜ä¸æœºé‡'))
[20.08.2025 05:13] Using data from previous issue: {"categories": ["#optimization", "#data", "#benchmark", "#cv", "#open_source", "#dataset", "#training"], "emoji": "ğŸ•¶ï¸", "ru": {"title": "Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ğ»ÑĞ±Ñ‹Ñ… Ğ½Ğ¾ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¼Ğ°ÑĞ¾Ğº", "desc": "OmniTry - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ°Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ (VTON) Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»
[20.08.2025 05:13] Using data from previous issue: {"categories": ["#data", "#multimodal", "#reasoning", "#games", "#training"], "emoji": "ğŸ˜¢", "ru": {"title": "LLM ĞºĞ°Ğº ÑĞ¼Ğ¿Ğ°Ñ‚Ñ‹: Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ½ĞµÑÑ‡Ğ°ÑÑ‚ÑŒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°
[20.08.2025 05:13] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#benchmark", "#reasoning", "#agi", "#open_source"], "emoji": "ğŸ§", "ru": {"title": "MMAU-Pro: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ˜Ğ˜", "desc": "MMAU-Pro - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚
[20.08.2025 05:13] Querying the API.
[20.08.2025 05:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Motion2Motion is a training-free framework that efficiently transfers animations between characters with different skeletal topologies using sparse bone correspondences.  					AI-generated summary 				 This work studies the challenge of transfer animations between characters whose skeletal topologies differ substantially. While many techniques have advanced retargeting techniques in decades, transfer motions across diverse topologies remains less-explored. The primary obstacle lies in the inherent topological inconsistency between source and target skeletons, which restricts the establishment of straightforward one-to-one bone correspondences. Besides, the current lack of large-scale paired motion datasets spanning different topological structures severely constrains the development of data-driven approaches. To address these limitations, we introduce Motion2Motion, a novel, training-free framework. Simply yet effectively, Motion2Motion works with only one or a few example motions on the target skeleton, by accessing a sparse set of bone correspondences between the source and target skeletons. Through comprehensive qualitative and quantitative evaluations, we demonstrate that Motion2Motion achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios. The practical utility of our approach is further evidenced by its successful integration in downstream applications and user interfaces, highlighting its potential for industrial applications. Code and data are available at https://lhchen.top/Motion2Motion.
[20.08.2025 05:13] Response: {
  "desc": "Motion2Motion - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ°Ğ¶Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ ÑĞºĞµĞ»ĞµÑ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞºĞµĞ»ĞµÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… ÑĞºĞµĞ»ĞµÑ‚Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸.",
  "emoji": "ğŸ¦´",
  "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ"
}
[20.08.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Motion2Motion is a training-free framework that efficiently transfers animations between characters with different skeletal topologies using sparse bone correspondences.  					AI-generated summary 				 This work studies the challenge of transfer animations between characters whose skeletal topologies differ substantially. While many techniques have advanced retargeting techniques in decades, transfer motions across diverse topologies remains less-explored. The primary obstacle lies in the inherent topological inconsistency between source and target skeletons, which restricts the establishment of straightforward one-to-one bone correspondences. Besides, the current lack of large-scale paired motion datasets spanning different topological structures severely constrains the development of data-driven approaches. To address these limitations, we introduce Motion2Motion, a novel, training-free framework. Simply yet effectively, Motion2Motion works with only one or a few example motions on the target skeleton, by accessing a sparse set of bone correspondences between the source and target skeletons. Through comprehensive qualitative and quantitative evaluations, we demonstrate that Motion2Motion achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios. The practical utility of our approach is further evidenced by its successful integration in downstream applications and user interfaces, highlighting its potential for industrial applications. Code and data are available at https://lhchen.top/Motion2Motion."

[20.08.2025 05:13] Response: ```python
['3D', 'DATASET']
```
[20.08.2025 05:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Motion2Motion is a training-free framework that efficiently transfers animations between characters with different skeletal topologies using sparse bone correspondences.  					AI-generated summary 				 This work studies the challenge of transfer animations between characters whose skeletal topologies differ substantially. While many techniques have advanced retargeting techniques in decades, transfer motions across diverse topologies remains less-explored. The primary obstacle lies in the inherent topological inconsistency between source and target skeletons, which restricts the establishment of straightforward one-to-one bone correspondences. Besides, the current lack of large-scale paired motion datasets spanning different topological structures severely constrains the development of data-driven approaches. To address these limitations, we introduce Motion2Motion, a novel, training-free framework. Simply yet effectively, Motion2Motion works with only one or a few example motions on the target skeleton, by accessing a sparse set of bone correspondences between the source and target skeletons. Through comprehensive qualitative and quantitative evaluations, we demonstrate that Motion2Motion achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios. The practical utility of our approach is further evidenced by its successful integration in downstream applications and user interfaces, highlighting its potential for industrial applications. Code and data are available at https://lhchen.top/Motion2Motion."

[20.08.2025 05:13] Response: ```python
["TRANSFER_LEARNING"]
```
[20.08.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Motion2Motion is a novel framework designed to transfer animations between characters with different skeletal structures without the need for extensive training. It addresses the challenge of establishing bone correspondences when the source and target skeletons have significant topological differences. By utilizing a sparse set of correspondences and requiring only a few example motions, Motion2Motion efficiently facilitates motion transfer. The framework has been evaluated rigorously, showing strong performance in both similar and diverse skeletal scenarios, making it suitable for practical applications in the industry.","title":"Effortless Animation Transfer Across Diverse Skeletons"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Motion2Motion is a novel framework designed to transfer animations between characters with different skeletal structures without the need for extensive training. It addresses the challenge of establishing bone correspondences when the source and target skeletons have significant topological differences. By utilizing a sparse set of correspondences and requiring only a few example motions, Motion2Motion efficiently facilitates motion transfer. The framework has been evaluated rigorously, showing strong performance in both similar and diverse skeletal scenarios, making it suitable for practical applications in the industry.', title='Effortless Animation Transfer Across Diverse Skeletons'))
[20.08.2025 05:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Motion2Motionæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°åœ¨ä¸åŒéª¨éª¼æ‹“æ‰‘çš„è§’è‰²ä¹‹é—´è½¬ç§»åŠ¨ç”»ã€‚è¯¥æ–¹æ³•è§£å†³äº†æºéª¨éª¼å’Œç›®æ ‡éª¨éª¼ä¹‹é—´çš„æ‹“æ‰‘ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œåˆ©ç”¨ç¨€ç–çš„éª¨éª¼å¯¹åº”å…³ç³»è¿›è¡ŒåŠ¨ç”»è½¬ç§»ã€‚å°½ç®¡ç°æœ‰çš„é‡å®šå‘æŠ€æœ¯å·²ç»å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†åœ¨å¤šæ ·åŒ–æ‹“æ‰‘ç»“æ„ä¹‹é—´çš„è¿åŠ¨è½¬ç§»ä»ç„¶è¾ƒå°‘è¢«æ¢ç´¢ã€‚é€šè¿‡å®šæ€§å’Œå®šé‡è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†Motion2Motionåœ¨ç›¸ä¼¼éª¨éª¼å’Œè·¨ç‰©ç§éª¨éª¼è½¬ç§»åœºæ™¯ä¸­éƒ½èƒ½å®ç°é«˜æ•ˆå¯é çš„æ€§èƒ½ã€‚","title":"é«˜æ•ˆåŠ¨ç”»è½¬ç§»ï¼Œè·¨è¶Šéª¨éª¼æ‹“æ‰‘çš„ç•Œé™"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Motion2Motionæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°åœ¨ä¸åŒéª¨éª¼æ‹“æ‰‘çš„è§’è‰²ä¹‹é—´è½¬ç§»åŠ¨ç”»ã€‚è¯¥æ–¹æ³•è§£å†³äº†æºéª¨éª¼å’Œç›®æ ‡éª¨éª¼ä¹‹é—´çš„æ‹“æ‰‘ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œåˆ©ç”¨ç¨€ç–çš„éª¨éª¼å¯¹åº”å…³ç³»è¿›è¡ŒåŠ¨ç”»è½¬ç§»ã€‚å°½ç®¡ç°æœ‰çš„é‡å®šå‘æŠ€æœ¯å·²ç»å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†åœ¨å¤šæ ·åŒ–æ‹“æ‰‘ç»“æ„ä¹‹é—´çš„è¿åŠ¨è½¬ç§»ä»ç„¶è¾ƒå°‘è¢«æ¢ç´¢ã€‚é€šè¿‡å®šæ€§å’Œå®šé‡è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†Motion2Motionåœ¨ç›¸ä¼¼éª¨éª¼å’Œè·¨ç‰©ç§éª¨éª¼è½¬ç§»åœºæ™¯ä¸­éƒ½èƒ½å®ç°é«˜æ•ˆå¯é çš„æ€§èƒ½ã€‚', title='é«˜æ•ˆåŠ¨ç”»è½¬ç§»ï¼Œè·¨è¶Šéª¨éª¼æ‹“æ‰‘çš„ç•Œé™'))
[20.08.2025 05:13] Renaming data file.
[20.08.2025 05:13] Renaming previous data. hf_papers.json to ./d/2025-08-20.json
[20.08.2025 05:13] Saving new data file.
[20.08.2025 05:13] Generating page.
[20.08.2025 05:13] Renaming previous page.
[20.08.2025 05:13] Renaming previous data. index.html to ./d/2025-08-20.html
[20.08.2025 05:13] Writing result.
[20.08.2025 05:13] Renaming log file.
[20.08.2025 05:13] Renaming previous data. log.txt to ./logs/2025-08-20_last_log.txt
