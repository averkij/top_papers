[20.08.2025 07:12] Read previous papers.
[20.08.2025 07:12] Generating top page (month).
[20.08.2025 07:12] Writing top page (month).
[20.08.2025 08:15] Read previous papers.
[20.08.2025 08:15] Get feed.
[20.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13167
[20.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14041
[20.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13948
[20.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.06905
[20.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.09131
[20.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13632
[20.08.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2508.10830
[20.08.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2508.11548
[20.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.12669
[20.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.04324
[20.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13139
[20.08.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2508.12535
[20.08.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2508.04326
[20.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13992
[20.08.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2508.10478
[20.08.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2508.09789
[20.08.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2508.08777
[20.08.2025 08:15] Extract page data from URL. URL: https://huggingface.co/papers/2508.04038
[20.08.2025 08:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[20.08.2025 08:15] No deleted papers detected.
[20.08.2025 08:15] Downloading and parsing papers (pdf, html). Total: 18.
[20.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.13167.
[20.08.2025 08:15] Extra JSON file exists (./assets/json/2508.13167.json), skip PDF parsing.
[20.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.13167.json), skip HTML parsing.
[20.08.2025 08:15] Success.
[20.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.14041.
[20.08.2025 08:15] Extra JSON file exists (./assets/json/2508.14041.json), skip PDF parsing.
[20.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.14041.json), skip HTML parsing.
[20.08.2025 08:15] Success.
[20.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.13948.
[20.08.2025 08:15] Extra JSON file exists (./assets/json/2508.13948.json), skip PDF parsing.
[20.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.13948.json), skip HTML parsing.
[20.08.2025 08:15] Success.
[20.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.06905.
[20.08.2025 08:15] Extra JSON file exists (./assets/json/2508.06905.json), skip PDF parsing.
[20.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.06905.json), skip HTML parsing.
[20.08.2025 08:15] Success.
[20.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.09131.
[20.08.2025 08:15] Extra JSON file exists (./assets/json/2508.09131.json), skip PDF parsing.
[20.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.09131.json), skip HTML parsing.
[20.08.2025 08:15] Success.
[20.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.13632.
[20.08.2025 08:15] Extra JSON file exists (./assets/json/2508.13632.json), skip PDF parsing.
[20.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.13632.json), skip HTML parsing.
[20.08.2025 08:15] Success.
[20.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.10830.
[20.08.2025 08:15] Downloading paper 2508.10830 from http://arxiv.org/pdf/2508.10830v1...
[20.08.2025 08:16] Extracting affiliations from text.
[20.08.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 Advances in Speech Separation: Techniques, Challenges, and Future Kai Li1, , , Guo Chen1, , Wendi Sang2, Yi Luo3, Zhuo Chen4, Shuai Wang5, Shulin He6, Zhong-Qiu Wang6, Andong Li7, Zhiyong Wu8, and Xiaolin Hu1, * 5 2 0 2 4 1 ] . [ 1 0 3 8 0 1 . 8 0 5 2 : r AbstractThe field of speech separation, targeting the challenging cocktail party problem, has witnessed revolutionary advances along with the development of deep neural networks (DNNs). Speech separation can be employed in standalone applications, enhancing speech clarity in complex acoustic enit can function as crucial previronments. Additionally, processing method for other speech processing tasks, such as speech recognition and speaker recognition. Despite numerous publications addressing this challenge, current literature surveys and reviews tend to focus narrowly on specific architectural designs or isolated learning approaches, creating fragmented understanding of this rapidly evolving domain. This fragmentation underscores the urgent need for unified, comprehensive survey that captures the fields breadth and recent innovations. This survey addresses this critical gap by providing systematic and holistic examination of DNN-based speech separation techniques. Our work differentiates itself from existing surveys and reviews: (I) Comprehensive perspective: Instead of only cataloging architectural variations, we systematically investigated higher-level learning paradigms, comprehensively covering separation scenarios with known or unknown number of speakers, comparative analysis of supervised, self-supervised, and unsupervised learning frameworks, and detailed examination of various architectural components from front-end encoders to back-end estimation strategies. (II) Rigorous timeliness: By covering the cutting-edge developments from the most recent years, we provide up-to-date coverage to ensure researchers have access to the most current methodologi"
[20.08.2025 08:16] Response: ```python
[]
```
[20.08.2025 08:16] Extracting affiliations from text.
[20.08.2025 08:16] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 Advances in Speech Separation: Techniques, Challenges, and Future Kai Li1, , , Guo Chen1, , Wendi Sang2, Yi Luo3, Zhuo Chen4, Shuai Wang5, Shulin He6, Zhong-Qiu Wang6, Andong Li7, Zhiyong Wu8, and Xiaolin Hu1, *5 2 0 2 4 1 ] . [ 1 0 3 8 0 1 . 8 0 5 2 : r AbstractThe field of speech separation, targeting the challenging cocktail party problem, has witnessed revolutionary advances along with the development of deep neural networks (DNNs). Speech separation can be employed in standalone applications, enhancing speech clarity in complex acoustic enit can function as crucial previronments. Additionally, processing method for other speech processing tasks, such as speech recognition and speaker recognition. Despite numerous publications addressing this challenge, current literature surveys and reviews tend to focus narrowly on specific architectural designs or isolated learning approaches, creating fragmented understanding of this rapidly evolving domain. This fragmentation underscores the urgent need for unified, comprehensive survey that captures the fields breadth and recent innovations. This survey addresses this critical gap by providing systematic and holistic examination of DNN-based speech separation techniques. Our work differentiates itself from existing surveys and reviews: (I) Comprehensive perspective: Instead of only cataloging architectural variations, we systematically investigated higher-level learning paradigms, comprehensively covering separation scenarios with known or unknown number of speakers, comparative analysis of supervised, self-supervised, and unsupervised learning frameworks, and detailed examination of various architectural components from front-end encoders to back-end estimation strategies. (II) Rigorous timeliness: By covering the cutting-edge developments from the most recent years, we provide up-to-date coverage to ensure researchers have access to the most current methodological innovations and performance benchmarks. (III) Unique insights: Beyond mere summarization, we critically evaluate technological trajectories, identify emerging research patterns, and highlight promising directions including domain-robust frameworks, computationally efficient architectures, multimodal integration approaches, and novel self-supervised paradigms. (IV) Fair quantitative evaluation: We provide fair quantitative evaluations on standard datasets, aiming to provide clear and reliable performance benchmarks for Manuscript received April 19, 2021; revised August 16, 2021. This work was supported by the National Key Research and Development Program of China under Grant 2021ZD0200301 and the National Natural Science Foundation of China under Grant U2341228. 1Kai Li, Guo Chen, and Xiaolin Hu are with the Department of Computer Science and Technology, Tsinghua University, Beijing, China. 2Wendi Sang is with the School of Computer Technology and Application, Qinghai University, Xining, China. independent 3Yi Luo an y.luo@columbia.edu). is author, Shenzhen, China (e-mail: 4Zhuo Chen is with ByteDance. 5Shuai Wang is with Nanjing University, Suzhou, China. 6Shulin He and Zhong-Qiu Wang are with the Southern University of Science and Technology, Shenzhen, China. 7Andong Li is with the Institute of Acoustics, Chinese Academy of Sciences, Beijing, China. 8Zhiyong Wu is with the Shenzhen International Graduate School, Tsinghua University, Shenzhen, China. Project leader. Equal contribution. (Kai Li and Guo Chen contributed equally to this work.) *Corresponding author: Xiaolin Hu (e-mail: xlhu@tsinghua.edu.cn). the research community, thereby revealing the true capabilities and limitations of different methods. This comprehensive survey aims to serve as an accessible reference for both experienced researchers and newcomers navigating the complex landscape of speech separation. Index Termscocktail party problem, speech separation, deep neural networks, large-scale datasets, toolkit. I. INTRODUCTION the human auditory system, the cocktail party effect exemplifies our remarkable ability to selectively attend to target speaker while suppressing interfering sources in noisy environments [1][3]. However, endowing machines with comparable level of separation capability remains one of the most challenging topics in the field of speech processing. Over the past decades, conventional approaches have primarily relied on statistical models and signal processing techniques, such as Independent Component Analysis (ICA) [4], Non-negative Matrix Factorization (NMF) [5], and heuristic spatial filtering methods [6], [7]. Although these methods perform well in certain scenarios, their separation performance and generalization ability are severely limited in real-world settings characterized by high nonlinearity, complex environments, and dynamically varying numbers of speech sources [8]. In recent years, data-driven paradigmspearheaded by deep neural networks (DNNs)has triggered significant shift, greatly advancing the state-of-the-art in speech separation systems. host of architectures and methodologies have been proposed, encompassing Convolutional Neural Networks (CNNs) [9][11], Recurrent Neural Networks (RNNs) [12] [15], and more recently, variants based on self-attention mechanisms such as Transformers [16][18], as well as generative models like Generative Adversarial Networks (GANs) [19], [20] and diffusion models [21][23]. These modern DNNbased approaches automatically learn deep hierarchical features and, through direct exposure to large-scale mixtures, substantially boost the performance of speech separation tasks. As visually illustrated in Figure 1, performance curves on the canonical WSJ0-2mix dataset highlight the remarkable progress driven by the evolution of architectures and algorithms [24]. Nevertheless, alongside rapid technological advances, application demands for speech separation have become increasingly diverse and complex. For example, meeting transcription and real-time communication require extremely low latency and online output; assistive hearing devices and mobile applications demand lightweight, efficient models with constrained resources. Meanwhile, real-world scenarios present persistent JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Fig. 1. Speech Separation Model Performance on WSJ0-2mix over Time. The size of the points represents the number of parameters of the model. TABLE COMPARATIVE ANALYS"
[20.08.2025 08:16] Mistral response. {"id": "6d6060b98597450db6c0d8bb06a19aa0", "created": 1755677764, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1406, "total_tokens": 1524, "completion_tokens": 118}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Department of Computer Science and Technology, Tsinghua University, Beijing, China\",\n    \"School of Computer Technology and Application, Qinghai University, Xining, China\",\n    \"Shenzhen, China\",\n    \"ByteDance\",\n    \"Nanjing University, Suzhou, China\",\n    \"Southern University of Science and Technology, Shenzhen, China\",\n    \"Institute of Acoustics, Chinese Academy of Sciences, Beijing, China\",\n    \"Shenzhen International Graduate School, Tsinghua University, Shenzhen, China\"\n]\n```"}}]}
[20.08.2025 08:16] Response: ```python
[
    "Department of Computer Science and Technology, Tsinghua University, Beijing, China",
    "School of Computer Technology and Application, Qinghai University, Xining, China",
    "Shenzhen, China",
    "ByteDance",
    "Nanjing University, Suzhou, China",
    "Southern University of Science and Technology, Shenzhen, China",
    "Institute of Acoustics, Chinese Academy of Sciences, Beijing, China",
    "Shenzhen International Graduate School, Tsinghua University, Shenzhen, China"
]
```
[20.08.2025 08:16] Deleting PDF ./assets/pdf/2508.10830.pdf.
[20.08.2025 08:16] Success.
[20.08.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2508.11548.
[20.08.2025 08:16] Downloading paper 2508.11548 from http://arxiv.org/pdf/2508.11548v1...
[20.08.2025 08:16] Extracting affiliations from text.
[20.08.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 8 4 5 1 1 . 8 0 5 2 : r Copyright Protection for Large Language Models: Survey of Methods, Challenges, and Trends Zhenhua Xu1, Xubin Yue1, Zhebo Wang1, Qichen Liu2, Xixiang Zhao2, Jingxuan Zhang2, Wenjun Zeng2, Wenpeng Xing1, 2, Dezhang Kong1, 2, Changting Lin1, 2, Meng Han1, 2, 1Zhejiang University 2GenTel.io Abstract: Copyright protection for large language models is of critical importance, given their substantial development costs, proprietary value, and potential for misuse. Existing surveys have predominantly focused on techniques for tracing LLM-generated contentnamely, text watermarkingwhile systematic exploration of methods for protecting the models themselves (i.e., model watermarking and model fingerprinting) remains absent. Moreover, the relationships and distinctions among text watermarking, model watermarking, and model fingerprinting have not been comprehensively clarified. This work presents comprehensive survey of the current state of LLM copyright protection technologies, with focus on model fingerprinting, covering the following aspects: (1) clarifying the conceptual connection from text watermarking to model watermarking and fingerprinting, and adopting unified terminology that incorporates model watermarking into the broader fingerprinting framework; (2) providing an overview and comparison of diverse text watermarking techniques, highlighting cases where such methods can function as model fingerprinting; (3) systematically categorizing and comparing existing model fingerprinting approaches for LLM copyright protection; (4) presenting, for the first time, techniques for fingerprint transfer and fingerprint removal; (5) summarizing evaluation metrics for model fingerprints, including effectiveness, harmlessness, robustness, stealthiness, and reliability; and (6) discussing open challenges and future research directions. This survey aims to offer researchers thorough understanding of both text watermarking and model fingerprinti"
[20.08.2025 08:16] Response: ```python
["Zhejiang University", "GenTel.io"]
```
[20.08.2025 08:16] Deleting PDF ./assets/pdf/2508.11548.pdf.
[20.08.2025 08:16] Success.
[20.08.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2508.12669.
[20.08.2025 08:16] Extra JSON file exists (./assets/json/2508.12669.json), skip PDF parsing.
[20.08.2025 08:16] Paper image links file exists (./assets/img_data/2508.12669.json), skip HTML parsing.
[20.08.2025 08:16] Success.
[20.08.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2508.04324.
[20.08.2025 08:16] Extra JSON file exists (./assets/json/2508.04324.json), skip PDF parsing.
[20.08.2025 08:16] Paper image links file exists (./assets/img_data/2508.04324.json), skip HTML parsing.
[20.08.2025 08:16] Success.
[20.08.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2508.13139.
[20.08.2025 08:16] Extra JSON file exists (./assets/json/2508.13139.json), skip PDF parsing.
[20.08.2025 08:16] Paper image links file exists (./assets/img_data/2508.13139.json), skip HTML parsing.
[20.08.2025 08:16] Success.
[20.08.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2508.12535.
[20.08.2025 08:16] Downloading paper 2508.12535 from http://arxiv.org/pdf/2508.12535v1...
[20.08.2025 08:16] Extracting affiliations from text.
[20.08.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection Seonglae Cho1,2 Zekun Wu1,2 Adriano Koshiyama1,2 1Holistic AI 2University College London 5 2 0 2 8 ] . [ 1 5 3 5 2 1 . 8 0 5 2 : r a "
[20.08.2025 08:16] Response: ```python
["Holistic AI", "University College London"]
```
[20.08.2025 08:16] Deleting PDF ./assets/pdf/2508.12535.pdf.
[20.08.2025 08:16] Success.
[20.08.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2508.04326.
[20.08.2025 08:16] Downloading paper 2508.04326 from http://arxiv.org/pdf/2508.04326v2...
[20.08.2025 08:16] Extracting affiliations from text.
[20.08.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Radiance Fields in XR: Survey on How Radiance Fields are Envisioned and Addressed for XR Research * , Mana Masuda , Susanne Schmidt , Shohei Mori 5 2 0 2 7 ] . [ 2 6 2 3 4 0 . 8 0 5 2 : r Fig. 1: We observe gap between the anticipated interests in RF-related technologies expected to advance new XR applications (left) and the actual interests that are considered the main factors in XR (right). This survey paper investigates and discusses how RF technologies, including NeRF and 3DGS, are envisioned for XR (i.e., (cid:17) XR-Envisioned) and how they are actually implemented for XR (i.e., (cid:192) XR-Addressed) in scientific papers from XR, HCI, CV, CG, Robotics, and MM communities via systematic review. Abstract The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF), has revolutionized interactive photorealistic view synthesis and presents enormous opportunities for XR research and applications. However, despite the exponential growth of RF research, RF-related contributions to the XR community remain sparse. To better understand this research gap, we performed systematic survey of current RF literature to analyze (i) how RF is envisioned for XR applications, (ii) how they have already been implemented, and (iii) the remaining research gaps. We collected 365 RF contributions related to XR from computer vision, computer graphics, robotics, multimedia, human-computer interaction, and XR communities, seeking to answer the above research questions. Among the 365 papers, we performed an analysis of 66 papers that already addressed detailed aspect of RF research for XR. With this survey, we extended and positioned XR-specific RF research topics in the broader RF research field and provide helpful resource for the XR community to navigate within the rapid development of RF research. Index TermsXR, Neural Radiance Fields, 3D Gaussian Splatting, Survey, Systematic Review. Machine learning revolutionized how we optimize s"
[20.08.2025 08:16] Response: ```python
[]
```
[20.08.2025 08:16] Extracting affiliations from text.
[20.08.2025 08:16] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Radiance Fields in XR: Survey on How Radiance Fields are Envisioned and Addressed for XR Research *, Mana Masuda , Susanne Schmidt , Shohei Mori 5 2 0 2 7 ] . [ 2 6 2 3 4 0 . 8 0 5 2 : r Fig. 1: We observe gap between the anticipated interests in RF-related technologies expected to advance new XR applications (left) and the actual interests that are considered the main factors in XR (right). This survey paper investigates and discusses how RF technologies, including NeRF and 3DGS, are envisioned for XR (i.e., (cid:17) XR-Envisioned) and how they are actually implemented for XR (i.e., (cid:192) XR-Addressed) in scientific papers from XR, HCI, CV, CG, Robotics, and MM communities via systematic review. Abstract The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF), has revolutionized interactive photorealistic view synthesis and presents enormous opportunities for XR research and applications. However, despite the exponential growth of RF research, RF-related contributions to the XR community remain sparse. To better understand this research gap, we performed systematic survey of current RF literature to analyze (i) how RF is envisioned for XR applications, (ii) how they have already been implemented, and (iii) the remaining research gaps. We collected 365 RF contributions related to XR from computer vision, computer graphics, robotics, multimedia, human-computer interaction, and XR communities, seeking to answer the above research questions. Among the 365 papers, we performed an analysis of 66 papers that already addressed detailed aspect of RF research for XR. With this survey, we extended and positioned XR-specific RF research topics in the broader RF research field and provide helpful resource for the XR community to navigate within the rapid development of RF research. Index TermsXR, Neural Radiance Fields, 3D Gaussian Splatting, Survey, Systematic Review.Machine learning revolutionized how we optimize scene representations for given images. Radiance fields (RF) [20, 38] are noticeable examples that have enabled efficient encoding and rendering of complex scene appearance since 2020. This innovation introduced significant potential for XR applications such as remote collaboration [52], teleoperation [26], and telepresence [50] and could potentially replace the traditional meshes, point clouds, and voxels. Advances in efficient training and rendering of RF [20,41] further expanded the opportunities in XR applications that require high-resolution and real-time rendering. RF is considered to provide novel technical foundation for enhancing immersive video see-through head-mounted displays [79], creating * 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. Ke Li is with University of Hamburg, Germany. E-mail: ke.li@uni-hamburg.de Mana Masuda is with Keio University, Japan. E-mail: mana.smile@keio.jp Susanne Schmidt is with University of Canterbury, New Zealand. E-mail: susanne.schmidt@canterbury.ac.nz Shohei Mori is with University of Stuttgart, Germany. E-mail: s.mori.jp@ieee.org Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx dynamic photorealistic avatars for the metaverse [50], and low-cost generation of digital twins for real-world XR applications [26]. Due to its significance, there has been an exponential growth of RF research in computer vision (CV), computer graphics (CG), robotics, and multimedia (MM) communities since the first introduction of the neural radiance field (NeRF) paper [38]. While many of these technical papers envisioned enormous impacts of RF technology for XR, the actual implementation of RF for XR-focused research and applications remains comparably sparse. For instance, Fig. 1 shows that 203 RFrelated papers were published at the Conference on Computer Vision and Pattern Recognition (CVPR) in 2024, with 68 explicitly mentioning XR as target application domain. However, in the proceedings of leading XR conferences IEEE VR 2024 and IEEE ISMAR 2024 only 11 contributions (including 4 conference papers and 7 extended abstracts) were related to RF. Among these, just 5 directly addressed an XR-related RF research question through technical benchmarks or user studies conducted in actual XR settings. This indicates significant research gap between the communitys initial vision for RF in XR and its current integration into XR research and applications. To better understand this research gap, this paper presents systematic survey on how radiance fields are envisioned for XR and how they have been implemented. While numerous surveys on radiance fields exist across various domains including robotics [74] and 3D computer vision [10] to the best of our knowledge, no dedicated survey has been conducted on radiance fields for XR. Moreover, other existing general radiance field surveys typically only provide broad overview of current research [8], offering limited insights into crucial XR-specific topics such as foveated rendering, haptic rendering, telepresence, 3D interaction, and perception. As result, the XR community currently has limited resources in evaluating the advancement of radiance field technology for XR and assessing its applicability in real-world immerreceiving the results from the renderer in XR middleware (e.g., Unity). 3DGS, as an explicit data representation, offers greater potential for tighter integration by leveraging shader codes for optimal performance. When depth rendering is available, on-screen composition with other frame buffers, such as those used for meshes, is possible. For detailed comparison of 3DGS, NeRF, and textured meshes for XR applications, we refer readers to the supplementary materials. Neural Radiance Fields: NeRF represents continuous 3D scene as parameters of multi-layer perceptron (MLP). The MLP is trained using set of multi-viewpoint images and their camera poses typically obtained from structured from motion (S"
[20.08.2025 08:16] Mistral response. {"id": "c27d043134f943119778708e5baedcd8", "created": 1755677790, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1512, "total_tokens": 1552, "completion_tokens": 40}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"University of Hamburg, Germany\",\n    \"Keio University, Japan\",\n    \"University of Canterbury, New Zealand\",\n    \"University of Stuttgart, Germany\"\n]\n```"}}]}
[20.08.2025 08:16] Response: ```python
[
    "University of Hamburg, Germany",
    "Keio University, Japan",
    "University of Canterbury, New Zealand",
    "University of Stuttgart, Germany"
]
```
[20.08.2025 08:16] Deleting PDF ./assets/pdf/2508.04326.pdf.
[20.08.2025 08:16] Success.
[20.08.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2508.13992.
[20.08.2025 08:16] Extra JSON file exists (./assets/json/2508.13992.json), skip PDF parsing.
[20.08.2025 08:16] Paper image links file exists (./assets/img_data/2508.13992.json), skip HTML parsing.
[20.08.2025 08:16] Success.
[20.08.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2508.10478.
[20.08.2025 08:16] Downloading paper 2508.10478 from http://arxiv.org/pdf/2508.10478v1...
[20.08.2025 08:16] Extracting affiliations from text.
[20.08.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 1 ] . [ 1 8 7 4 0 1 . 8 0 5 2 : r a Gustavo Penha gustavop@spotify.com Spotify Delft, Netherlands Enrico Palumbo enricop@spotify.com Spotify Turin, Italy Max Lefarov mlefarov@spotify.com Spotify Munich, Germany Edoardo DAmico edoardod@spotify.com Spotify Madrid, Spain Alexandre Tamborrino alexandret@spotify.com Spotify Paris, France Shawn Lin weihsiangl@spotify.com Spotify New York, United States Marco De Nadai mdenadai@spotify.com Spotify Copenhagen, Denmark Ali Vardasbi aliv@spotify.com Spotify Amsterdam, Netherlands Timothy Heath theath@spotify.com Spotify New York, United States Francesco Fabbri francescof@spotify.com Spotify Barcelona, Spain Hugues Bouchard hb@spotify.com Spotify Barcelona, Spain Abstract Generative models powered by Large Language Models (LLMs) are emerging as unified solution for powering both recommendation and search tasks. key design choice in these models is how to represent items, traditionally through unique identifiers (IDs) and more recently with Semantic IDs composed of discrete codes, obtained from embeddings. While task-specific embedding models can improve performance for individual tasks, they may not generalize well in joint setting. In this paper, we explore how to construct Semantic IDs that perform well both in search and recommendation when using unified model. We compare range of strategies to construct Semantic IDs, looking into task-specific and cross-tasks approaches, and also whether each task should have its own semantic ID tokens in joint search and recommendation generative model. Our results show that using bi-encoder model fine-tuned on both search and recommendation tasks to obtain item embeddings, followed by the construction of unified Semantic ID space provides an effective trade-off, enabling strong performance in both tasks. We hope these findings spark follow-up work on generalisable, semantically grounded ID schemes and inform the next wave of unified generative recommender architectures. ACM Refe"
[20.08.2025 08:16] Response: ```python
["Spotify Delft, Netherlands", "Spotify Turin, Italy", "Spotify Munich, Germany", "Spotify Madrid, Spain", "Spotify Paris, France", "Spotify New York, United States", "Spotify Copenhagen, Denmark", "Spotify Amsterdam, Netherlands", "Spotify New York, United States", "Spotify Barcelona, Spain", "Spotify Barcelona, Spain"]
```
[20.08.2025 08:16] Deleting PDF ./assets/pdf/2508.10478.pdf.
[20.08.2025 08:16] Success.
[20.08.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2508.09789.
[20.08.2025 08:16] Downloading paper 2508.09789 from http://arxiv.org/pdf/2508.09789v1...
[20.08.2025 08:16] Extracting affiliations from text.
[20.08.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations Andreas Damianou andreasd@spotify.com Spotify United Kingdom Marco De Nadai mdenadai@spotify.com Spotify Denmark Mounia Lalmas mounial@spotify.com Spotify United Kingdom 5 2 0 2 3 1 ] . [ 1 9 8 7 9 0 . 8 0 5 2 : r Abstract Existing video recommender systems rely primarily on user-defined metadata or on low-level visual and acoustic signals extracted by specialised encoders. These low-level features describe what appears on the screen but miss deeper semantics such as intent, humour, and world knowledge that make clips resonate with viewers. For example, is 30-second clip simply singer on rooftop, or an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such distinctions are critical to personalised recommendations yet remain invisible to traditional encoding pipelines. In this paper, we introduce simple, recommendation system-agnostic zero-finetuning framework that injects high-level semantics into the recommendation pipeline by prompting an off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip into rich natural-language description (e.g. superhero parody with slapstick fights and orchestral stabs), bridging the gap between raw content and user intent. We use MLLM output with state-of-the-art text encoder and feed it into standard collaborative, content-based, and generative recommenders. On the MicroLens-100K dataset, which emulates user interactions with TikTok-style videos, our framework consistently surpasses conventional video, audio, and metadata features in five representative models. Our findings highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to build more intent-aware video recommenders. Keywords Multimodal Large Language Models, LLMs, video recommendation systems, video ACM Reference Format: Marco De Nadai, Andreas Damianou, and Mounia Lalmas. 2025. Describe What You See with Multimodal Large Lang"
[20.08.2025 08:16] Response: ```python
["Spotify United Kingdom", "Spotify Denmark", "Spotify United Kingdom"]
```
[20.08.2025 08:16] Deleting PDF ./assets/pdf/2508.09789.pdf.
[20.08.2025 08:16] Success.
[20.08.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2508.08777.
[20.08.2025 08:16] Downloading paper 2508.08777 from http://arxiv.org/pdf/2508.08777v1...
[20.08.2025 08:16] Extracting affiliations from text.
[20.08.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge Gustavo Penha Spotify Netherlands Edoardo DAmico Spotify Spain 5 2 0 2 2 1 ] . [ 1 7 7 7 8 0 . 8 0 5 2 : r a Oskar Stål Spotify Sweden Abstract Evaluating personalized recommendations remains central challenge, especially in long-form audio domains like podcasts, where traditional offline metrics suffer from exposure bias and online methods such as A/B testing are costly and operationally constrained. In this paper, we propose novel framework that leverages Large Language Models (LLMs) as offline judges to assess the quality of podcast recommendations in scalable and interpretable manner. Our two-stage profile-aware approach first constructs natural-language user profiles distilled from 90 days of listening history. These profiles summarize both topical interests and behavioral patterns, serving as compact, interpretable representations of user preferences. Rather than prompting the LLM with raw data, we use these profiles to provide high-level, semantically rich contextenabling the LLM to reason more effectively about alignment between users interests and recommended episodes. This reduces input complexity and improves interpretability. The LLM is then prompted to deliver fine-grained pointwise and pairwise judgments based on the profile-episode match. In controlled study with 47 participants, our profile-aware judge matched human judgments with high fidelity and outperformed or matched variant using raw listening histories. The framework enables efficient, profile-aware evaluation for iterative testing and model selection in recommender systems. CCS Concepts Information systems Language models; Personalization. Corresponding author: francescof@spotify.com. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and "
[20.08.2025 08:16] Response: ```python
["Spotify Netherlands", "Spotify Spain", "Spotify Sweden"]
```
[20.08.2025 08:16] Deleting PDF ./assets/pdf/2508.08777.pdf.
[20.08.2025 08:16] Success.
[20.08.2025 08:16] Downloading and parsing paper https://huggingface.co/papers/2508.04038.
[20.08.2025 08:16] Downloading paper 2508.04038 from http://arxiv.org/pdf/2508.04038v1...
[20.08.2025 08:16] Extracting affiliations from text.
[20.08.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents Zechen Li, Baiyu Chen, Hao Xue, Flora D. Salim University of New South Wales, Sydney {zechen.li, hao.xue1, flora.salim}@unsw.edu.au, breeze.chen@student.unsw.edu.au 5 2 0 2 6 ] . [ 1 8 3 0 4 0 . 8 0 5 2 : r Abstract Motion sensor time-series are central to human activity recognition (HAR), with applications in health, sports, and smart devices. However, existing methods are trained for fixed activity sets and require costly retraining when new behaviours or sensor setups appear. Recent attempts to use large language models (LLMs) for HAR, typically by converting signals into text or images, suffer from limited accuracy and lack verifiable interpretability. We propose ZARA, the first agent-based framework for zero-shot, explainable HAR directly from raw motion time-series. ZARA integrates an automatically derived pair-wise feature knowledge base that captures discriminative statistics for every activity pair, multisensor retrieval module that surfaces relevant evidence, and hierarchical agent pipeline that guides the LLM to iteratively select features, draw on this evidence, and produce both activity predictions and natural-language explanations. ZARA enables flexible and interpretable HAR without any fine-tuning or task-specific classifiers. Extensive experiments on 8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering clear reasoning while exceeding the strongest baselines by 2.53 in macro F1. Ablation studies further confirm the necessity of each module, marking ZARA as promising step toward trustworthy, plugand-play motion time-series analysis. Our codes are available at https://github.com/zechenli03/ZARA. Introduction Human activity recognition (HAR) from on-body motion sensors underpins wide range of ubiquitous computing applications, from digital health and sports analytics to adaptive user interfaces. However, most existing HAR systems remain he"
[20.08.2025 08:16] Response: ```python
["University of New South Wales, Sydney"]
```
[20.08.2025 08:16] Deleting PDF ./assets/pdf/2508.04038.pdf.
[20.08.2025 08:16] Success.
[20.08.2025 08:16] Enriching papers with extra data.
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 0. Chain-of-Agents (CoA) paradigm enables end-to-end complex problem-solving in LLMs through dynamic agent activation, improving performance via multi-agent distillation and agentic reinforcement learning.  					AI-generated summary 				 Recent advances in large language models (LLMs) and multi-agent s...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 1. LongSplat improves novel view synthesis from long videos with irregular motion through joint optimization, robust pose estimation, and efficient anchor formation.  					AI-generated summary 				 LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos ...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 2. POML addresses challenges in prompting Large Language Models by providing a structured, data-integrated, and format-sensitive markup language with templating and developer tools.  					AI-generated summary 				 Large Language Models (LLMs) require sophisticated prompting, yet current practices face ...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 3. Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.  					AI-generated summary 				 Visual designers naturally draw inspirat...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 4. ColorCtrl, a training-free method using Multi-Modal Diffusion Transformers, achieves precise and consistent color editing in images and videos with word-level control and superior performance compared to existing approaches.  					AI-generated summary 				 Text-guided color editing in images and vid...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 5. OmniTry extends Virtual Try-ON to various wearable objects using a two-stage pipeline that combines unpaired and paired image training to improve object localization and appearance consistency.  					AI-generated summary 				 Virtual Try-ON (VTON) is a practical and widely-applied task, for which mo...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 6. A survey of DNN-based speech separation techniques, covering learning paradigms, separation scenarios, and architectural components, with a focus on current advancements and promising future directions.  					AI-generated summary 				 The field of speech separation, addressing the "cocktail party pr...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 7. A survey of LLM copyright protection technologies, focusing on model fingerprinting, clarifies its relationship with text watermarking and provides a comprehensive overview of fingerprinting techniques, evaluation metrics, and open challenges.  					AI-generated summary 				 Copyright protection for...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 8. LLMs predict misery scores from text using various prompting strategies, with few-shot approaches outperforming zero-shot, and a gamified framework assessing their adaptability in emotional reasoning tasks.  					AI-generated summary 				 This study investigates the use of Large Language Models (LLM...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 9. TempFlow-GRPO enhances text-to-image generation by addressing temporal credit assignment and noise-aware optimization in flow models, improving human preference alignment and benchmark performance.  					AI-generated summary 				 Recent flow matching models for text-to-image generation have achieved...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 10. Motion2Motion is a training-free framework that efficiently transfers animations between characters with different skeletal topologies using sparse bone correspondences.  					AI-generated summary 				 This work studies the challenge of transfer animations between characters whose skeletal topologie...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 11. CorrSteer improves language model performance by selecting relevant features through correlation with SAE activations at inference time, enhancing tasks like QA, bias mitigation, and reasoning.  					AI-generated summary 				 Sparse Autoencoders (SAEs) can extract interpretable features from large l...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 12. A systematic survey of radiance fields in XR applications identifies implementation details, research gaps, and future directions within the broader RF research field.  					AI-generated summary 				 The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS) and Neural Radiance Fi...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 13. MMAU-Pro is a comprehensive benchmark for evaluating audio intelligence in AI systems, assessing 49 unique skills across speech, sound, music, and their combinations, revealing significant limitations in current models.  					AI-generated summary 				 Audio comprehension-including speech, non-speech...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 14. A bi-encoder model fine-tuned on both search and recommendation tasks provides effective Semantic IDs for a unified generative recommender system.  					AI-generated summary 				 Generative models powered by Large Language Models (LLMs) are emerging as a unified solution for powering both recommenda...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 15. A zero-finetuning framework uses Multimodal Large Language Models to inject high-level semantics into video recommendations, improving intent-awareness over traditional methods.  					AI-generated summary 				 Existing video recommender systems rely primarily on user-defined metadata or on low-level...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 16. A novel framework uses Large Language Models to evaluate podcast recommendations by constructing user profiles and providing context for the LLM to make judgments, improving efficiency and interpretability.  					AI-generated summary 				 Evaluating personalized recommendations remains a central cha...
[20.08.2025 08:16] ********************************************************************************
[20.08.2025 08:16] Abstract 17. ZARA is an agent-based framework for zero-shot, explainable human activity recognition from raw motion time-series, achieving state-of-the-art performance without fine-tuning.  					AI-generated summary 				 Motion sensor time-series are central to human activity recognition (HAR), with applications...
[20.08.2025 08:16] Read previous papers.
[20.08.2025 08:16] Generating reviews via LLM API.
[20.08.2025 08:16] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#agi", "#architecture", "#open_source", "#rl", "#agents", "#training", "#rlhf"], "emoji": "🤖", "ru": {"title": "Chain-of-Agents: Новый подход к решению сложных задач в языковых моделях", "desc": "Статья представляет новую парадигму рассуждений для больших
[20.08.2025 08:16] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#cv", "#architecture", "#3d"], "emoji": "🎥", "ru": {"title": "Улучшение синтеза новых ракурсов для длинных видео с помощью LongSplat", "desc": "LongSplat - это новый метод синтеза новых ракурсов из длинных видео с нерегулярным движением камеры. Он испо
[20.08.2025 08:16] Using data from previous issue: {"categories": ["#data", "#multimodal"], "emoji": "🗂️", "ru": {"title": "POML: структурированный подход к промптингу языковых моделей", "desc": "Статья представляет POML - язык разметки для структурированного промптинга больших языковых моделей. POML решает проблемы интеграции данных, чувствительнос
[20.08.2025 08:16] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#synthetic", "#cv", "#dataset", "#benchmark"], "emoji": "🎨", "ru": {"title": "Генеративные модели нуждаются в улучшении работы с множественными визуальными ссылками", "desc": "Исследование показывает, что современные системы генерации изображений испыт
[20.08.2025 08:16] Using data from previous issue: {"categories": ["#diffusion", "#video", "#cv", "#multimodal"], "emoji": "🎨", "ru": {"title": "Точное редактирование цвета без обучения с помощью диффузионных трансформеров", "desc": "ColorCtrl - это метод редактирования цвета в изображениях и видео без дополнительного обучения, использующий мультимо
[20.08.2025 08:16] Using data from previous issue: {"categories": ["#optimization", "#data", "#benchmark", "#cv", "#open_source", "#dataset", "#training"], "emoji": "🕶️", "ru": {"title": "Виртуальная примерка любых носимых объектов без масок", "desc": "OmniTry - это унифицированная система, расширяющая возможности виртуальной примерки (VTON) на разл
[20.08.2025 08:16] Querying the API.
[20.08.2025 08:16] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A survey of DNN-based speech separation techniques, covering learning paradigms, separation scenarios, and architectural components, with a focus on current advancements and promising future directions.  					AI-generated summary 				 The field of speech separation, addressing the "cocktail party problem", has seen revolutionary advances with DNNs. Speech separation enhances clarity in complex acoustic environments and serves as crucial pre-processing for speech recognition and speaker recognition. However, current literature focuses narrowly on specific architectures or isolated approaches, creating fragmented understanding. This survey addresses this gap by providing systematic examination of DNN-based speech separation techniques. Our work differentiates itself through: (I) Comprehensive perspective: We systematically investigate learning paradigms, separation scenarios with known/unknown speakers, comparative analysis of supervised/self-supervised/unsupervised frameworks, and architectural components from encoders to estimation strategies. (II) Timeliness: Coverage of cutting-edge developments ensures access to current innovations and benchmarks. (III) Unique insights: Beyond summarization, we evaluate technological trajectories, identify emerging patterns, and highlight promising directions including domain-robust frameworks, efficient architectures, multimodal integration, and novel self-supervised paradigms. (IV) Fair evaluation: We provide quantitative evaluations on standard datasets, revealing true capabilities and limitations of different methods. This comprehensive survey serves as an accessible reference for experienced researchers and newcomers navigating speech separation's complex landscape.
[20.08.2025 08:16] Response: {
  "desc": "Это обзор методов разделения речи на основе глубоких нейронных сетей (DNN). В статье рассматриваются парадигмы обучения, сценарии разделения и архитектурные компоненты, с акцентом на текущие достижения и перспективные направления. Авторы предоставляют систематический анализ методов, включая сравнение контролируемых, самоконтролируемых и неконтролируемых подходов. Особое внимание уделяется новейшим разработкам, оценке технологических траекторий и выявлению перспективных направлений, таких как мультимодальная интеграция и новые парадигмы самоконтролируемого обучения.",
  "emoji": "🎙️",
  "title": "Комплексный анализ DNN-методов разделения речи: от основ до инноваций"
}
[20.08.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A survey of DNN-based speech separation techniques, covering learning paradigms, separation scenarios, and architectural components, with a focus on current advancements and promising future directions.  					AI-generated summary 				 The field of speech separation, addressing the "cocktail party problem", has seen revolutionary advances with DNNs. Speech separation enhances clarity in complex acoustic environments and serves as crucial pre-processing for speech recognition and speaker recognition. However, current literature focuses narrowly on specific architectures or isolated approaches, creating fragmented understanding. This survey addresses this gap by providing systematic examination of DNN-based speech separation techniques. Our work differentiates itself through: (I) Comprehensive perspective: We systematically investigate learning paradigms, separation scenarios with known/unknown speakers, comparative analysis of supervised/self-supervised/unsupervised frameworks, and architectural components from encoders to estimation strategies. (II) Timeliness: Coverage of cutting-edge developments ensures access to current innovations and benchmarks. (III) Unique insights: Beyond summarization, we evaluate technological trajectories, identify emerging patterns, and highlight promising directions including domain-robust frameworks, efficient architectures, multimodal integration, and novel self-supervised paradigms. (IV) Fair evaluation: We provide quantitative evaluations on standard datasets, revealing true capabilities and limitations of different methods. This comprehensive survey serves as an accessible reference for experienced researchers and newcomers navigating speech separation's complex landscape."

[20.08.2025 08:16] Response: ```python
['AUDIO', 'BENCHMARK', 'MULTIMODAL']
```
[20.08.2025 08:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A survey of DNN-based speech separation techniques, covering learning paradigms, separation scenarios, and architectural components, with a focus on current advancements and promising future directions.  					AI-generated summary 				 The field of speech separation, addressing the "cocktail party problem", has seen revolutionary advances with DNNs. Speech separation enhances clarity in complex acoustic environments and serves as crucial pre-processing for speech recognition and speaker recognition. However, current literature focuses narrowly on specific architectures or isolated approaches, creating fragmented understanding. This survey addresses this gap by providing systematic examination of DNN-based speech separation techniques. Our work differentiates itself through: (I) Comprehensive perspective: We systematically investigate learning paradigms, separation scenarios with known/unknown speakers, comparative analysis of supervised/self-supervised/unsupervised frameworks, and architectural components from encoders to estimation strategies. (II) Timeliness: Coverage of cutting-edge developments ensures access to current innovations and benchmarks. (III) Unique insights: Beyond summarization, we evaluate technological trajectories, identify emerging patterns, and highlight promising directions including domain-robust frameworks, efficient architectures, multimodal integration, and novel self-supervised paradigms. (IV) Fair evaluation: We provide quantitative evaluations on standard datasets, revealing true capabilities and limitations of different methods. This comprehensive survey serves as an accessible reference for experienced researchers and newcomers navigating speech separation's complex landscape."

[20.08.2025 08:16] Response: ```python
["SURVEY"]
```
[20.08.2025 08:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper surveys deep neural network (DNN)-based techniques for speech separation, which is essential for improving speech clarity in noisy environments. It addresses the \'cocktail party problem\' by systematically examining various learning paradigms, separation scenarios, and architectural components. The authors highlight advancements in supervised, self-supervised, and unsupervised frameworks, providing a comprehensive analysis of current methods and their effectiveness. Additionally, the survey identifies future directions for research, including domain robustness and multimodal integration, making it a valuable resource for both new and experienced researchers in the field.","title":"Revolutionizing Speech Clarity with DNNs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper surveys deep neural network (DNN)-based techniques for speech separation, which is essential for improving speech clarity in noisy environments. It addresses the 'cocktail party problem' by systematically examining various learning paradigms, separation scenarios, and architectural components. The authors highlight advancements in supervised, self-supervised, and unsupervised frameworks, providing a comprehensive analysis of current methods and their effectiveness. Additionally, the survey identifies future directions for research, including domain robustness and multimodal integration, making it a valuable resource for both new and experienced researchers in the field.", title='Revolutionizing Speech Clarity with DNNs'))
[20.08.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文对基于深度神经网络（DNN）的语音分离技术进行了全面调查，涵盖了学习范式、分离场景和架构组件。语音分离技术旨在解决“鸡尾酒会问题”，提高复杂声学环境中的语音清晰度，并为语音识别和说话人识别提供重要的预处理。当前文献主要集中在特定架构或孤立方法上，导致理解的碎片化。我们的研究通过系统性分析和评估不同方法的能力与局限性，填补了这一空白，为研究人员提供了一个易于访问的参考。","title":"深度学习助力语音分离技术的未来"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文对基于深度神经网络（DNN）的语音分离技术进行了全面调查，涵盖了学习范式、分离场景和架构组件。语音分离技术旨在解决“鸡尾酒会问题”，提高复杂声学环境中的语音清晰度，并为语音识别和说话人识别提供重要的预处理。当前文献主要集中在特定架构或孤立方法上，导致理解的碎片化。我们的研究通过系统性分析和评估不同方法的能力与局限性，填补了这一空白，为研究人员提供了一个易于访问的参考。', title='深度学习助力语音分离技术的未来'))
[20.08.2025 08:17] Querying the API.
[20.08.2025 08:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A survey of LLM copyright protection technologies, focusing on model fingerprinting, clarifies its relationship with text watermarking and provides a comprehensive overview of fingerprinting techniques, evaluation metrics, and open challenges.  					AI-generated summary 				 Copyright protection for large language models is of critical importance, given their substantial development costs, proprietary value, and potential for misuse. Existing surveys have predominantly focused on techniques for tracing LLM-generated content-namely, text watermarking-while a systematic exploration of methods for protecting the models themselves (i.e., model watermarking and model fingerprinting) remains absent. Moreover, the relationships and distinctions among text watermarking, model watermarking, and model fingerprinting have not been comprehensively clarified. This work presents a comprehensive survey of the current state of LLM copyright protection technologies, with a focus on model fingerprinting, covering the following aspects: (1) clarifying the conceptual connection from text watermarking to model watermarking and fingerprinting, and adopting a unified terminology that incorporates model watermarking into the broader fingerprinting framework; (2) providing an overview and comparison of diverse text watermarking techniques, highlighting cases where such methods can function as model fingerprinting; (3) systematically categorizing and comparing existing model fingerprinting approaches for LLM copyright protection; (4) presenting, for the first time, techniques for fingerprint transfer and fingerprint removal; (5) summarizing evaluation metrics for model fingerprints, including effectiveness, harmlessness, robustness, stealthiness, and reliability; and (6) discussing open challenges and future research directions. This survey aims to offer researchers a thorough understanding of both text watermarking and model fingerprinting technologies in the era of LLMs, thereby fostering further advances in protecting their intellectual property.
[20.08.2025 08:17] Response: {
  "desc": "Статья представляет собой обзор технологий защиты авторских прав для больших языковых моделей (LLM), фокусируясь на методе отпечатков моделей. Авторы проясняют связь между водяными знаками текста и отпечатками моделей, предоставляя комплексный обзор техник, метрик оценки и открытых проблем в этой области. Исследование систематизирует различные подходы к созданию отпечатков моделей для защиты авторских прав LLM, включая методы переноса и удаления отпечатков. Статья также обсуждает критерии оценки отпечатков моделей, такие как эффективность, безвредность, устойчивость, скрытность и надежность.",
  "emoji": "🔐",
  "title": "Защита интеллектуальной собственности в эпоху больших языковых моделей"
}
[20.08.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A survey of LLM copyright protection technologies, focusing on model fingerprinting, clarifies its relationship with text watermarking and provides a comprehensive overview of fingerprinting techniques, evaluation metrics, and open challenges.  					AI-generated summary 				 Copyright protection for large language models is of critical importance, given their substantial development costs, proprietary value, and potential for misuse. Existing surveys have predominantly focused on techniques for tracing LLM-generated content-namely, text watermarking-while a systematic exploration of methods for protecting the models themselves (i.e., model watermarking and model fingerprinting) remains absent. Moreover, the relationships and distinctions among text watermarking, model watermarking, and model fingerprinting have not been comprehensively clarified. This work presents a comprehensive survey of the current state of LLM copyright protection technologies, with a focus on model fingerprinting, covering the following aspects: (1) clarifying the conceptual connection from text watermarking to model watermarking and fingerprinting, and adopting a unified terminology that incorporates model watermarking into the broader fingerprinting framework; (2) providing an overview and comparison of diverse text watermarking techniques, highlighting cases where such methods can function as model fingerprinting; (3) systematically categorizing and comparing existing model fingerprinting approaches for LLM copyright protection; (4) presenting, for the first time, techniques for fingerprint transfer and fingerprint removal; (5) summarizing evaluation metrics for model fingerprints, including effectiveness, harmlessness, robustness, stealthiness, and reliability; and (6) discussing open challenges and future research directions. This survey aims to offer researchers a thorough understanding of both text watermarking and model fingerprinting technologies in the era of LLMs, thereby fostering further advances in protecting their intellectual property."

[20.08.2025 08:17] Response: ```python
["DATASET", "DATA", "BENCHMARK", "MULTIMODAL"]
```
[20.08.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A survey of LLM copyright protection technologies, focusing on model fingerprinting, clarifies its relationship with text watermarking and provides a comprehensive overview of fingerprinting techniques, evaluation metrics, and open challenges.  					AI-generated summary 				 Copyright protection for large language models is of critical importance, given their substantial development costs, proprietary value, and potential for misuse. Existing surveys have predominantly focused on techniques for tracing LLM-generated content-namely, text watermarking-while a systematic exploration of methods for protecting the models themselves (i.e., model watermarking and model fingerprinting) remains absent. Moreover, the relationships and distinctions among text watermarking, model watermarking, and model fingerprinting have not been comprehensively clarified. This work presents a comprehensive survey of the current state of LLM copyright protection technologies, with a focus on model fingerprinting, covering the following aspects: (1) clarifying the conceptual connection from text watermarking to model watermarking and fingerprinting, and adopting a unified terminology that incorporates model watermarking into the broader fingerprinting framework; (2) providing an overview and comparison of diverse text watermarking techniques, highlighting cases where such methods can function as model fingerprinting; (3) systematically categorizing and comparing existing model fingerprinting approaches for LLM copyright protection; (4) presenting, for the first time, techniques for fingerprint transfer and fingerprint removal; (5) summarizing evaluation metrics for model fingerprints, including effectiveness, harmlessness, robustness, stealthiness, and reliability; and (6) discussing open challenges and future research directions. This survey aims to offer researchers a thorough understanding of both text watermarking and model fingerprinting technologies in the era of LLMs, thereby fostering further advances in protecting their intellectual property."

[20.08.2025 08:17] Response: ```python
['SURVEY', 'ETHICS']
```
[20.08.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper surveys copyright protection technologies for large language models (LLMs), emphasizing model fingerprinting. It clarifies the connections between text watermarking, model watermarking, and model fingerprinting, proposing a unified terminology. The study categorizes various fingerprinting techniques and evaluation metrics, addressing their effectiveness and robustness. Additionally, it highlights open challenges and future research directions in the field of LLM copyright protection.","title":"Protecting LLMs: Unifying Fingerprinting and Watermarking Techniques"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper surveys copyright protection technologies for large language models (LLMs), emphasizing model fingerprinting. It clarifies the connections between text watermarking, model watermarking, and model fingerprinting, proposing a unified terminology. The study categorizes various fingerprinting techniques and evaluation metrics, addressing their effectiveness and robustness. Additionally, it highlights open challenges and future research directions in the field of LLM copyright protection.', title='Protecting LLMs: Unifying Fingerprinting and Watermarking Techniques'))
[20.08.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"这篇论文调查了大型语言模型（LLM）的版权保护技术，特别关注模型指纹技术。它阐明了文本水印与模型水印和模型指纹之间的关系，并提供了指纹技术的全面概述。论文还系统分类和比较了现有的模型指纹方法，并首次介绍了指纹转移和指纹去除技术。最后，讨论了评估模型指纹的指标以及未来研究的挑战和方向。","title":"保护大型语言模型的版权技术探索"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='这篇论文调查了大型语言模型（LLM）的版权保护技术，特别关注模型指纹技术。它阐明了文本水印与模型水印和模型指纹之间的关系，并提供了指纹技术的全面概述。论文还系统分类和比较了现有的模型指纹方法，并首次介绍了指纹转移和指纹去除技术。最后，讨论了评估模型指纹的指标以及未来研究的挑战和方向。', title='保护大型语言模型的版权技术探索'))
[20.08.2025 08:17] Using data from previous issue: {"categories": ["#data", "#multimodal", "#reasoning", "#games", "#training"], "emoji": "😢", "ru": {"title": "LLM как эмпаты: оценка способности AI понимать человеческие страдания", "desc": "Это исследование оценивает способность больших языковых моделей (LLM) предсказывать уровень несчастья человека
[20.08.2025 08:17] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#cv", "#benchmark", "#alignment", "#optimization"], "emoji": "🖼️", "ru": {"title": "Темпоральная оптимизация для улучшения генерации изображений", "desc": "Статья представляет TempFlow-GRPO - новый метод для улучшения генерации изображений по текстовому описанию. Дан
[20.08.2025 08:17] Using data from previous issue: {"categories": ["#dataset", "#3d", "#transfer_learning"], "emoji": "🦴", "ru": {"title": "Универсальный перенос анимации без обучения", "desc": "Motion2Motion - это система для переноса анимации между персонажами с разной скелетной топологией, не требующая обучения. Она использует разреженные соответ
[20.08.2025 08:17] Querying the API.
[20.08.2025 08:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

CorrSteer improves language model performance by selecting relevant features through correlation with SAE activations at inference time, enhancing tasks like QA, bias mitigation, and reasoning.  					AI-generated summary 				 Sparse Autoencoders (SAEs) can extract interpretable features from large language models (LLMs) without supervision. However, their effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation storage. To address these limitations, we propose CorrSteer, which selects features by correlating sample correctness with SAE activations from generated tokens at inference time. This approach uses only inference-time activations to extract more relevant features, thereby avoiding spurious correlations. It also obtains steering coefficients from average activations, automating the entire pipeline. Our method shows improved task performance on QA, bias mitigation, jailbreaking prevention, and reasoning benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1% improvement in MMLU performance and a +22.9% improvement in HarmBench with only 4000 samples. Selected features demonstrate semantically meaningful patterns aligned with each task's requirements, revealing the underlying capabilities that drive performance. Our work establishes correlationbased selection as an effective and scalable approach for automated SAE steering across language model applications.
[20.08.2025 08:17] Response: {
  "desc": "CorrSteer - это новый метод улучшения работы языковых моделей, использующий корреляцию между правильностью ответов и активациями разреженных автоэнкодеров (SAE) во время вывода. Этот подход позволяет автоматически выбирать релевантные признаки без необходимости в контрастных наборах данных или хранении больших объемов активаций. CorrSteer показывает улучшение производительности в задачах вопросно-ответных систем, снижения предвзятости, предотвращения обхода ограничений и рассуждений на моделях Gemma 2 2B и LLaMA 3.1 8B. Выбранные признаки демонстрируют семантически значимые паттерны, соответствующие требованиям каждой задачи.",
  "emoji": "🧠",
  "title": "CorrSteer: Автоматическое улучшение ИИ через корреляционный анализ"
}
[20.08.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CorrSteer improves language model performance by selecting relevant features through correlation with SAE activations at inference time, enhancing tasks like QA, bias mitigation, and reasoning.  					AI-generated summary 				 Sparse Autoencoders (SAEs) can extract interpretable features from large language models (LLMs) without supervision. However, their effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation storage. To address these limitations, we propose CorrSteer, which selects features by correlating sample correctness with SAE activations from generated tokens at inference time. This approach uses only inference-time activations to extract more relevant features, thereby avoiding spurious correlations. It also obtains steering coefficients from average activations, automating the entire pipeline. Our method shows improved task performance on QA, bias mitigation, jailbreaking prevention, and reasoning benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1% improvement in MMLU performance and a +22.9% improvement in HarmBench with only 4000 samples. Selected features demonstrate semantically meaningful patterns aligned with each task's requirements, revealing the underlying capabilities that drive performance. Our work establishes correlationbased selection as an effective and scalable approach for automated SAE steering across language model applications."

[20.08.2025 08:17] Response: ```python
['DATA', 'BENCHMARK', 'ARCHITECTURE', 'TRAINING']
```
[20.08.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CorrSteer improves language model performance by selecting relevant features through correlation with SAE activations at inference time, enhancing tasks like QA, bias mitigation, and reasoning.  					AI-generated summary 				 Sparse Autoencoders (SAEs) can extract interpretable features from large language models (LLMs) without supervision. However, their effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation storage. To address these limitations, we propose CorrSteer, which selects features by correlating sample correctness with SAE activations from generated tokens at inference time. This approach uses only inference-time activations to extract more relevant features, thereby avoiding spurious correlations. It also obtains steering coefficients from average activations, automating the entire pipeline. Our method shows improved task performance on QA, bias mitigation, jailbreaking prevention, and reasoning benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1% improvement in MMLU performance and a +22.9% improvement in HarmBench with only 4000 samples. Selected features demonstrate semantically meaningful patterns aligned with each task's requirements, revealing the underlying capabilities that drive performance. Our work establishes correlationbased selection as an effective and scalable approach for automated SAE steering across language model applications."

[20.08.2025 08:17] Response: ```python
['INTERPRETABILITY', 'REASONING', 'ETHICS', 'OPTIMIZATION']
```
[20.08.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CorrSteer is a method that enhances the performance of language models by selecting relevant features based on their correlation with Sparse Autoencoder (SAE) activations during inference. This technique allows for the extraction of interpretable features without needing large datasets or extensive storage of activations. By focusing on the relationship between sample correctness and SAE activations, CorrSteer avoids misleading correlations and automates the feature selection process. The results show significant improvements in various tasks, including question answering and bias mitigation, demonstrating the effectiveness of correlation-based feature selection in language model applications.","title":"Enhancing Language Models with Correlation-Based Feature Selection"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CorrSteer is a method that enhances the performance of language models by selecting relevant features based on their correlation with Sparse Autoencoder (SAE) activations during inference. This technique allows for the extraction of interpretable features without needing large datasets or extensive storage of activations. By focusing on the relationship between sample correctness and SAE activations, CorrSteer avoids misleading correlations and automates the feature selection process. The results show significant improvements in various tasks, including question answering and bias mitigation, demonstrating the effectiveness of correlation-based feature selection in language model applications.', title='Enhancing Language Models with Correlation-Based Feature Selection'))
[20.08.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CorrSteer是一种通过与稀疏自编码器(SAE)激活的相关性来选择特征的方法，从而提高语言模型的性能。该方法在推理时仅使用激活信息，避免了虚假相关性，并自动化了整个特征选择过程。通过这种方式，CorrSteer在问答、偏见缓解和推理等任务上表现出显著的性能提升。我们的研究表明，基于相关性的特征选择是一种有效且可扩展的自动化SAE引导方法。","title":"CorrSteer：通过相关性选择特征，提升语言模型性能"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CorrSteer是一种通过与稀疏自编码器(SAE)激活的相关性来选择特征的方法，从而提高语言模型的性能。该方法在推理时仅使用激活信息，避免了虚假相关性，并自动化了整个特征选择过程。通过这种方式，CorrSteer在问答、偏见缓解和推理等任务上表现出显著的性能提升。我们的研究表明，基于相关性的特征选择是一种有效且可扩展的自动化SAE引导方法。', title='CorrSteer：通过相关性选择特征，提升语言模型性能'))
[20.08.2025 08:17] Querying the API.
[20.08.2025 08:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A systematic survey of radiance fields in XR applications identifies implementation details, research gaps, and future directions within the broader RF research field.  					AI-generated summary 				 The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF), has revolutionized interactive photorealistic view synthesis and presents enormous opportunities for XR research and applications. However, despite the exponential growth of RF research, RF-related contributions to the XR community remain sparse. To better understand this research gap, we performed a systematic survey of current RF literature to analyze (i) how RF is envisioned for XR applications, (ii) how they have already been implemented, and (iii) the remaining research gaps. We collected 365 RF contributions related to XR from computer vision, computer graphics, robotics, multimedia, human-computer interaction, and XR communities, seeking to answer the above research questions. Among the 365 papers, we performed an analysis of 66 papers that already addressed a detailed aspect of RF research for XR. With this survey, we extended and positioned XR-specific RF research topics in the broader RF research field and provide a helpful resource for the XR community to navigate within the rapid development of RF research.
[20.08.2025 08:17] Response: {
  "desc": "Это систематический обзор применения радиационных полей (RF) в приложениях расширенной реальности (XR). Исследование анализирует 365 работ, связанных с RF и XR, из различных областей, включая компьютерное зрение и графику. Особое внимание уделяется 66 статьям, детально рассматривающим RF для XR. Обзор выявляет текущее состояние, пробелы в исследованиях и будущие направления развития RF в контексте XR.",
  "emoji": "🔍",
  "title": "Радиационные поля в XR: от визии к реальности"
}
[20.08.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A systematic survey of radiance fields in XR applications identifies implementation details, research gaps, and future directions within the broader RF research field.  					AI-generated summary 				 The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF), has revolutionized interactive photorealistic view synthesis and presents enormous opportunities for XR research and applications. However, despite the exponential growth of RF research, RF-related contributions to the XR community remain sparse. To better understand this research gap, we performed a systematic survey of current RF literature to analyze (i) how RF is envisioned for XR applications, (ii) how they have already been implemented, and (iii) the remaining research gaps. We collected 365 RF contributions related to XR from computer vision, computer graphics, robotics, multimedia, human-computer interaction, and XR communities, seeking to answer the above research questions. Among the 365 papers, we performed an analysis of 66 papers that already addressed a detailed aspect of RF research for XR. With this survey, we extended and positioned XR-specific RF research topics in the broader RF research field and provide a helpful resource for the XR community to navigate within the rapid development of RF research."

[20.08.2025 08:17] Response: ```python
['3D', 'CV', 'ROBOTICS', 'MULTIMODAL']
```
[20.08.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A systematic survey of radiance fields in XR applications identifies implementation details, research gaps, and future directions within the broader RF research field.  					AI-generated summary 				 The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF), has revolutionized interactive photorealistic view synthesis and presents enormous opportunities for XR research and applications. However, despite the exponential growth of RF research, RF-related contributions to the XR community remain sparse. To better understand this research gap, we performed a systematic survey of current RF literature to analyze (i) how RF is envisioned for XR applications, (ii) how they have already been implemented, and (iii) the remaining research gaps. We collected 365 RF contributions related to XR from computer vision, computer graphics, robotics, multimedia, human-computer interaction, and XR communities, seeking to answer the above research questions. Among the 365 papers, we performed an analysis of 66 papers that already addressed a detailed aspect of RF research for XR. With this survey, we extended and positioned XR-specific RF research topics in the broader RF research field and provide a helpful resource for the XR community to navigate within the rapid development of RF research."

[20.08.2025 08:17] Response: ```python
["SURVEY"]
```
[20.08.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a comprehensive survey of radiance fields (RF) in the context of extended reality (XR) applications, highlighting key implementation details and identifying significant research gaps. It discusses the impact of advanced RF techniques like 3D Gaussian Splatting and Neural Radiance Fields on photorealistic view synthesis, which is crucial for XR experiences. The authors analyzed 365 contributions from various fields, focusing on 66 papers that specifically address RF applications in XR. This work aims to guide future research directions and enhance the integration of RF technologies within the XR community.","title":"Bridging Radiance Fields and XR: A Path Forward"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a comprehensive survey of radiance fields (RF) in the context of extended reality (XR) applications, highlighting key implementation details and identifying significant research gaps. It discusses the impact of advanced RF techniques like 3D Gaussian Splatting and Neural Radiance Fields on photorealistic view synthesis, which is crucial for XR experiences. The authors analyzed 365 contributions from various fields, focusing on 66 papers that specifically address RF applications in XR. This work aims to guide future research directions and enhance the integration of RF technologies within the XR community.', title='Bridging Radiance Fields and XR: A Path Forward'))
[20.08.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文系统性地调查了辐射场（RF）在扩展现实（XR）应用中的实现细节、研究空白和未来方向。研究表明，尽管辐射场技术（如3D高斯点云和神经辐射场）在交互式真实感视图合成中取得了重大进展，但与XR社区相关的RF贡献仍然稀少。我们分析了365篇与XR相关的RF文献，并深入研究了66篇已详细探讨RF在XR中的应用的论文。通过这项调查，我们为XR社区提供了一个有用的资源，以便在RF研究的快速发展中进行导航。","title":"辐射场技术：扩展现实的未来之路"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文系统性地调查了辐射场（RF）在扩展现实（XR）应用中的实现细节、研究空白和未来方向。研究表明，尽管辐射场技术（如3D高斯点云和神经辐射场）在交互式真实感视图合成中取得了重大进展，但与XR社区相关的RF贡献仍然稀少。我们分析了365篇与XR相关的RF文献，并深入研究了66篇已详细探讨RF在XR中的应用的论文。通过这项调查，我们为XR社区提供了一个有用的资源，以便在RF研究的快速发展中进行导航。', title='辐射场技术：扩展现实的未来之路'))
[20.08.2025 08:17] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#benchmark", "#reasoning", "#agi", "#open_source"], "emoji": "🎧", "ru": {"title": "MMAU-Pro: новый стандарт оценки аудиоинтеллекта ИИ", "desc": "MMAU-Pro - это комплексный бенчмарк для оценки аудиоинтеллекта в системах искусственного интеллекта. Он оценивает
[20.08.2025 08:17] Querying the API.
[20.08.2025 08:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A bi-encoder model fine-tuned on both search and recommendation tasks provides effective Semantic IDs for a unified generative recommender system.  					AI-generated summary 				 Generative models powered by Large Language Models (LLMs) are emerging as a unified solution for powering both recommendation and search tasks. A key design choice in these models is how to represent items, traditionally through unique identifiers (IDs) and more recently with Semantic IDs composed of discrete codes, obtained from embeddings. While task-specific embedding models can improve performance for individual tasks, they may not generalize well in a joint setting. In this paper, we explore how to construct Semantic IDs that perform well both in search and recommendation when using a unified model. We compare a range of strategies to construct Semantic IDs, looking into task-specific and cross-tasks approaches, and also whether each task should have its own semantic ID tokens in a joint search and recommendation generative model. Our results show that using a bi-encoder model fine-tuned on both search and recommendation tasks to obtain item embeddings, followed by the construction of a unified Semantic ID space provides an effective trade-off, enabling strong performance in both tasks. We hope these findings spark follow-up work on generalisable, semantically grounded ID schemes and inform the next wave of unified generative recommender architectures.
[20.08.2025 08:17] Response: {
  "desc": "Статья исследует эффективные способы создания семантических идентификаторов для единой генеративной рекомендательной системы. Авторы сравнивают различные стратегии построения семантических ID, включая подходы для отдельных задач и кросс-задачные методы. Результаты показывают, что использование би-энкодерной модели, дообученной на задачах поиска и рекомендаций, обеспечивает оптимальный баланс производительности. Исследование направлено на развитие обобщаемых, семантически обоснованных схем идентификации для унифицированных генеративных рекомендательных архитектур.",
  "emoji": "🔍",
  "title": "Единые семантические ID для поиска и рекомендаций"
}
[20.08.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A bi-encoder model fine-tuned on both search and recommendation tasks provides effective Semantic IDs for a unified generative recommender system.  					AI-generated summary 				 Generative models powered by Large Language Models (LLMs) are emerging as a unified solution for powering both recommendation and search tasks. A key design choice in these models is how to represent items, traditionally through unique identifiers (IDs) and more recently with Semantic IDs composed of discrete codes, obtained from embeddings. While task-specific embedding models can improve performance for individual tasks, they may not generalize well in a joint setting. In this paper, we explore how to construct Semantic IDs that perform well both in search and recommendation when using a unified model. We compare a range of strategies to construct Semantic IDs, looking into task-specific and cross-tasks approaches, and also whether each task should have its own semantic ID tokens in a joint search and recommendation generative model. Our results show that using a bi-encoder model fine-tuned on both search and recommendation tasks to obtain item embeddings, followed by the construction of a unified Semantic ID space provides an effective trade-off, enabling strong performance in both tasks. We hope these findings spark follow-up work on generalisable, semantically grounded ID schemes and inform the next wave of unified generative recommender architectures."

[20.08.2025 08:17] Response: ```python
['DATA', 'TRAINING', 'ARCHITECTURE']
```
[20.08.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A bi-encoder model fine-tuned on both search and recommendation tasks provides effective Semantic IDs for a unified generative recommender system.  					AI-generated summary 				 Generative models powered by Large Language Models (LLMs) are emerging as a unified solution for powering both recommendation and search tasks. A key design choice in these models is how to represent items, traditionally through unique identifiers (IDs) and more recently with Semantic IDs composed of discrete codes, obtained from embeddings. While task-specific embedding models can improve performance for individual tasks, they may not generalize well in a joint setting. In this paper, we explore how to construct Semantic IDs that perform well both in search and recommendation when using a unified model. We compare a range of strategies to construct Semantic IDs, looking into task-specific and cross-tasks approaches, and also whether each task should have its own semantic ID tokens in a joint search and recommendation generative model. Our results show that using a bi-encoder model fine-tuned on both search and recommendation tasks to obtain item embeddings, followed by the construction of a unified Semantic ID space provides an effective trade-off, enabling strong performance in both tasks. We hope these findings spark follow-up work on generalisable, semantically grounded ID schemes and inform the next wave of unified generative recommender architectures."

[20.08.2025 08:17] Response: ```python
["OPTIMIZATION", "TRANSFER_LEARNING"]
```
[20.08.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses a bi-encoder model that is fine-tuned for both search and recommendation tasks, which helps create effective Semantic IDs for a unified generative recommender system. The authors investigate different strategies for constructing Semantic IDs, focusing on how these IDs can be optimized for both tasks simultaneously. They find that using a shared Semantic ID space derived from item embeddings leads to better performance compared to task-specific approaches. The study aims to inspire further research into generalizable ID schemes that can enhance future generative recommender systems.","title":"Unified Semantic IDs for Enhanced Recommendations and Search"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses a bi-encoder model that is fine-tuned for both search and recommendation tasks, which helps create effective Semantic IDs for a unified generative recommender system. The authors investigate different strategies for constructing Semantic IDs, focusing on how these IDs can be optimized for both tasks simultaneously. They find that using a shared Semantic ID space derived from item embeddings leads to better performance compared to task-specific approaches. The study aims to inspire further research into generalizable ID schemes that can enhance future generative recommender systems.', title='Unified Semantic IDs for Enhanced Recommendations and Search'))
[20.08.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文探讨了一种双编码器模型，该模型在搜索和推荐任务上进行微调，以生成有效的语义ID，从而支持统一的生成推荐系统。传统上，项目通过唯一标识符表示，而最近则使用由嵌入生成的离散代码构成的语义ID。我们比较了多种构建语义ID的策略，发现双编码器模型在搜索和推荐任务上表现良好，能够有效地生成统一的语义ID空间。我们的研究结果为未来的通用、语义基础的ID方案提供了启示，并推动了统一生成推荐架构的发展。","title":"统一生成推荐系统的语义ID构建"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文探讨了一种双编码器模型，该模型在搜索和推荐任务上进行微调，以生成有效的语义ID，从而支持统一的生成推荐系统。传统上，项目通过唯一标识符表示，而最近则使用由嵌入生成的离散代码构成的语义ID。我们比较了多种构建语义ID的策略，发现双编码器模型在搜索和推荐任务上表现良好，能够有效地生成统一的语义ID空间。我们的研究结果为未来的通用、语义基础的ID方案提供了启示，并推动了统一生成推荐架构的发展。', title='统一生成推荐系统的语义ID构建'))
[20.08.2025 08:17] Querying the API.
[20.08.2025 08:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A zero-finetuning framework uses Multimodal Large Language Models to inject high-level semantics into video recommendations, improving intent-awareness over traditional methods.  					AI-generated summary 				 Existing video recommender systems rely primarily on user-defined metadata or on low-level visual and acoustic signals extracted by specialised encoders. These low-level features describe what appears on the screen but miss deeper semantics such as intent, humour, and world knowledge that make clips resonate with viewers. For example, is a 30-second clip simply a singer on a rooftop, or an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such distinctions are critical to personalised recommendations yet remain invisible to traditional encoding pipelines. In this paper, we introduce a simple, recommendation system-agnostic zero-finetuning framework that injects high-level semantics into the recommendation pipeline by prompting an off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip into a rich natural-language description (e.g. "a superhero parody with slapstick fights and orchestral stabs"), bridging the gap between raw content and user intent. We use MLLM output with a state-of-the-art text encoder and feed it into standard collaborative, content-based, and generative recommenders. On the MicroLens-100K dataset, which emulates user interactions with TikTok-style videos, our framework consistently surpasses conventional video, audio, and metadata features in five representative models. Our findings highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to build more intent-aware video recommenders.
[20.08.2025 08:17] Response: {
  "desc": "В статье представлен новый подход к рекомендательным системам для видео, использующий мультимодальные языковые модели (MLLM) для извлечения высокоуровневой семантики из клипов. Это позволяет учитывать такие аспекты, как намерение, юмор и общие знания, которые обычно недоступны традиционным методам. Предложенная система не требует дополнительного обучения и может использоваться с существующими рекомендательными алгоритмами. Эксперименты на датасете MicroLens-100K показали превосходство этого подхода над стандартными методами, основанными на метаданных и низкоуровневых признаках.",
  "emoji": "🎥",
  "title": "MLLM для интеллектуальных видеорекомендаций без дообучения"
}
[20.08.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A zero-finetuning framework uses Multimodal Large Language Models to inject high-level semantics into video recommendations, improving intent-awareness over traditional methods.  					AI-generated summary 				 Existing video recommender systems rely primarily on user-defined metadata or on low-level visual and acoustic signals extracted by specialised encoders. These low-level features describe what appears on the screen but miss deeper semantics such as intent, humour, and world knowledge that make clips resonate with viewers. For example, is a 30-second clip simply a singer on a rooftop, or an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such distinctions are critical to personalised recommendations yet remain invisible to traditional encoding pipelines. In this paper, we introduce a simple, recommendation system-agnostic zero-finetuning framework that injects high-level semantics into the recommendation pipeline by prompting an off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip into a rich natural-language description (e.g. "a superhero parody with slapstick fights and orchestral stabs"), bridging the gap between raw content and user intent. We use MLLM output with a state-of-the-art text encoder and feed it into standard collaborative, content-based, and generative recommenders. On the MicroLens-100K dataset, which emulates user interactions with TikTok-style videos, our framework consistently surpasses conventional video, audio, and metadata features in five representative models. Our findings highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to build more intent-aware video recommenders."

[20.08.2025 08:17] Response: ```python
['MULTIMODAL', 'VIDEO', 'DATASET']
```
[20.08.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A zero-finetuning framework uses Multimodal Large Language Models to inject high-level semantics into video recommendations, improving intent-awareness over traditional methods.  					AI-generated summary 				 Existing video recommender systems rely primarily on user-defined metadata or on low-level visual and acoustic signals extracted by specialised encoders. These low-level features describe what appears on the screen but miss deeper semantics such as intent, humour, and world knowledge that make clips resonate with viewers. For example, is a 30-second clip simply a singer on a rooftop, or an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such distinctions are critical to personalised recommendations yet remain invisible to traditional encoding pipelines. In this paper, we introduce a simple, recommendation system-agnostic zero-finetuning framework that injects high-level semantics into the recommendation pipeline by prompting an off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip into a rich natural-language description (e.g. "a superhero parody with slapstick fights and orchestral stabs"), bridging the gap between raw content and user intent. We use MLLM output with a state-of-the-art text encoder and feed it into standard collaborative, content-based, and generative recommenders. On the MicroLens-100K dataset, which emulates user interactions with TikTok-style videos, our framework consistently surpasses conventional video, audio, and metadata features in five representative models. Our findings highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to build more intent-aware video recommenders."

[20.08.2025 08:17] Response: ```python
['GAMES', 'OPTIMIZATION']
```
[20.08.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a novel framework that enhances video recommendation systems by incorporating high-level semantics using Multimodal Large Language Models (MLLMs). Traditional methods often rely on basic visual and audio features, which fail to capture deeper meanings like intent and humor. The proposed framework generates rich natural-language descriptions of video clips, allowing for a better understanding of user intent. By integrating these descriptions into existing recommendation models, the system significantly improves performance on user interaction datasets, demonstrating the effectiveness of MLLMs in creating more personalized recommendations.","title":"Enhancing Video Recommendations with High-Level Semantics"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a novel framework that enhances video recommendation systems by incorporating high-level semantics using Multimodal Large Language Models (MLLMs). Traditional methods often rely on basic visual and audio features, which fail to capture deeper meanings like intent and humor. The proposed framework generates rich natural-language descriptions of video clips, allowing for a better understanding of user intent. By integrating these descriptions into existing recommendation models, the system significantly improves performance on user interaction datasets, demonstrating the effectiveness of MLLMs in creating more personalized recommendations.', title='Enhancing Video Recommendations with High-Level Semantics'))
[20.08.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种零微调框架，利用多模态大型语言模型（MLLM）为视频推荐系统注入高层语义，从而提高对用户意图的理解。传统的视频推荐系统主要依赖用户定义的元数据或低层次的视觉和音频信号，无法捕捉到意图、幽默和世界知识等深层语义。通过让MLLM对每个视频片段进行自然语言描述，本文架起了原始内容与用户意图之间的桥梁。实验结果表明，该框架在多个推荐模型中均优于传统的视频、音频和元数据特征，展示了MLLM作为知识提取工具的潜力。","title":"利用多模态语言模型提升视频推荐的意图感知"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种零微调框架，利用多模态大型语言模型（MLLM）为视频推荐系统注入高层语义，从而提高对用户意图的理解。传统的视频推荐系统主要依赖用户定义的元数据或低层次的视觉和音频信号，无法捕捉到意图、幽默和世界知识等深层语义。通过让MLLM对每个视频片段进行自然语言描述，本文架起了原始内容与用户意图之间的桥梁。实验结果表明，该框架在多个推荐模型中均优于传统的视频、音频和元数据特征，展示了MLLM作为知识提取工具的潜力。', title='利用多模态语言模型提升视频推荐的意图感知'))
[20.08.2025 08:17] Querying the API.
[20.08.2025 08:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel framework uses Large Language Models to evaluate podcast recommendations by constructing user profiles and providing context for the LLM to make judgments, improving efficiency and interpretability.  					AI-generated summary 				 Evaluating personalized recommendations remains a central challenge, especially in long-form audio domains like podcasts, where traditional offline metrics suffer from exposure bias and online methods such as A/B testing are costly and operationally constrained. In this paper, we propose a novel framework that leverages Large Language Models (LLMs) as offline judges to assess the quality of podcast recommendations in a scalable and interpretable manner. Our two-stage profile-aware approach first constructs natural-language user profiles distilled from 90 days of listening history. These profiles summarize both topical interests and behavioral patterns, serving as compact, interpretable representations of user preferences. Rather than prompting the LLM with raw data, we use these profiles to provide high-level, semantically rich context-enabling the LLM to reason more effectively about alignment between a user's interests and recommended episodes. This reduces input complexity and improves interpretability. The LLM is then prompted to deliver fine-grained pointwise and pairwise judgments based on the profile-episode match. In a controlled study with 47 participants, our profile-aware judge matched human judgments with high fidelity and outperformed or matched a variant using raw listening histories. The framework enables efficient, profile-aware evaluation for iterative testing and model selection in recommender systems.
[20.08.2025 08:17] Response: {
  "desc": "Предложена новая система оценки рекомендаций подкастов с использованием больших языковых моделей (LLM). Система создает профили пользователей на основе истории прослушивания и использует их для предоставления контекста LLM. LLM затем оценивает соответствие рекомендаций интересам пользователя. В исследовании с участием 47 человек система показала высокую точность по сравнению с оценками людей.",
  "emoji": "🎧",
  "title": "LLM как эксперт по рекомендациям подкастов"
}
[20.08.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel framework uses Large Language Models to evaluate podcast recommendations by constructing user profiles and providing context for the LLM to make judgments, improving efficiency and interpretability.  					AI-generated summary 				 Evaluating personalized recommendations remains a central challenge, especially in long-form audio domains like podcasts, where traditional offline metrics suffer from exposure bias and online methods such as A/B testing are costly and operationally constrained. In this paper, we propose a novel framework that leverages Large Language Models (LLMs) as offline judges to assess the quality of podcast recommendations in a scalable and interpretable manner. Our two-stage profile-aware approach first constructs natural-language user profiles distilled from 90 days of listening history. These profiles summarize both topical interests and behavioral patterns, serving as compact, interpretable representations of user preferences. Rather than prompting the LLM with raw data, we use these profiles to provide high-level, semantically rich context-enabling the LLM to reason more effectively about alignment between a user's interests and recommended episodes. This reduces input complexity and improves interpretability. The LLM is then prompted to deliver fine-grained pointwise and pairwise judgments based on the profile-episode match. In a controlled study with 47 participants, our profile-aware judge matched human judgments with high fidelity and outperformed or matched a variant using raw listening histories. The framework enables efficient, profile-aware evaluation for iterative testing and model selection in recommender systems."

[20.08.2025 08:17] Response: ```python
['MULTIMODAL', 'AUDIO', 'TRAINING']
```
[20.08.2025 08:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel framework uses Large Language Models to evaluate podcast recommendations by constructing user profiles and providing context for the LLM to make judgments, improving efficiency and interpretability.  					AI-generated summary 				 Evaluating personalized recommendations remains a central challenge, especially in long-form audio domains like podcasts, where traditional offline metrics suffer from exposure bias and online methods such as A/B testing are costly and operationally constrained. In this paper, we propose a novel framework that leverages Large Language Models (LLMs) as offline judges to assess the quality of podcast recommendations in a scalable and interpretable manner. Our two-stage profile-aware approach first constructs natural-language user profiles distilled from 90 days of listening history. These profiles summarize both topical interests and behavioral patterns, serving as compact, interpretable representations of user preferences. Rather than prompting the LLM with raw data, we use these profiles to provide high-level, semantically rich context-enabling the LLM to reason more effectively about alignment between a user's interests and recommended episodes. This reduces input complexity and improves interpretability. The LLM is then prompted to deliver fine-grained pointwise and pairwise judgments based on the profile-episode match. In a controlled study with 47 participants, our profile-aware judge matched human judgments with high fidelity and outperformed or matched a variant using raw listening histories. The framework enables efficient, profile-aware evaluation for iterative testing and model selection in recommender systems."

[20.08.2025 08:17] Response: ```python
['INTERPRETABILITY', 'ALIGNMENT']
```
[20.08.2025 08:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a new framework that utilizes Large Language Models (LLMs) to enhance the evaluation of podcast recommendations. By creating user profiles from 90 days of listening history, the framework captures user interests and behaviors in a clear and interpretable way. Instead of using raw data, the LLM is provided with these profiles to make more informed judgments about how well podcast episodes align with user preferences. The results show that this method not only matches human evaluations closely but also improves the efficiency of testing and selecting recommendation models.","title":"Enhancing Podcast Recommendations with Profile-Aware LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a new framework that utilizes Large Language Models (LLMs) to enhance the evaluation of podcast recommendations. By creating user profiles from 90 days of listening history, the framework captures user interests and behaviors in a clear and interpretable way. Instead of using raw data, the LLM is provided with these profiles to make more informed judgments about how well podcast episodes align with user preferences. The results show that this method not only matches human evaluations closely but also improves the efficiency of testing and selecting recommendation models.', title='Enhancing Podcast Recommendations with Profile-Aware LLMs'))
[20.08.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新颖的框架，利用大型语言模型（LLM）来评估播客推荐的质量。该框架通过构建用户个人资料，提取90天的收听历史，形成自然语言的用户偏好表示。通过这种方式，LLM能够在更高层次上理解用户的兴趣与推荐内容之间的关系，从而提高评估的效率和可解释性。实验结果表明，该方法在与人类判断的匹配度上表现优异，能够有效支持推荐系统的迭代测试和模型选择。","title":"利用大型语言模型提升播客推荐评估的效率与可解释性"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新颖的框架，利用大型语言模型（LLM）来评估播客推荐的质量。该框架通过构建用户个人资料，提取90天的收听历史，形成自然语言的用户偏好表示。通过这种方式，LLM能够在更高层次上理解用户的兴趣与推荐内容之间的关系，从而提高评估的效率和可解释性。实验结果表明，该方法在与人类判断的匹配度上表现优异，能够有效支持推荐系统的迭代测试和模型选择。', title='利用大型语言模型提升播客推荐评估的效率与可解释性'))
[20.08.2025 08:18] Querying the API.
[20.08.2025 08:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ZARA is an agent-based framework for zero-shot, explainable human activity recognition from raw motion time-series, achieving state-of-the-art performance without fine-tuning.  					AI-generated summary 				 Motion sensor time-series are central to human activity recognition (HAR), with applications in health, sports, and smart devices. However, existing methods are trained for fixed activity sets and require costly retraining when new behaviours or sensor setups appear. Recent attempts to use large language models (LLMs) for HAR, typically by converting signals into text or images, suffer from limited accuracy and lack verifiable interpretability. We propose ZARA, the first agent-based framework for zero-shot, explainable HAR directly from raw motion time-series. ZARA integrates an automatically derived pair-wise feature knowledge base that captures discriminative statistics for every activity pair, a multi-sensor retrieval module that surfaces relevant evidence, and a hierarchical agent pipeline that guides the LLM to iteratively select features, draw on this evidence, and produce both activity predictions and natural-language explanations. ZARA enables flexible and interpretable HAR without any fine-tuning or task-specific classifiers. Extensive experiments on 8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering clear reasoning while exceeding the strongest baselines by 2.53x in macro F1. Ablation studies further confirm the necessity of each module, marking ZARA as a promising step toward trustworthy, plug-and-play motion time-series analysis. Our codes are available at https://github.com/zechenli03/ZARA.
[20.08.2025 08:18] Response: {
  "desc": "ZARA - это агентная система для распознавания человеческой активности по временным рядам движения без предварительного обучения. Она использует автоматически созданную базу знаний о парных признаках, модуль извлечения данных с нескольких датчиков и иерархический конвейер агентов для управления большой языковой моделью. ZARA достигает лучших результатов в задаче распознавания активности с нулевым обучением на 8 эталонных наборах данных. Система обеспечивает гибкое и интерпретируемое распознавание активности без необходимости дообучения или специфических классификаторов.",
  "emoji": "🏃",
  "title": "ZARA: Универсальное распознавание активности без обучения"
}
[20.08.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ZARA is an agent-based framework for zero-shot, explainable human activity recognition from raw motion time-series, achieving state-of-the-art performance without fine-tuning.  					AI-generated summary 				 Motion sensor time-series are central to human activity recognition (HAR), with applications in health, sports, and smart devices. However, existing methods are trained for fixed activity sets and require costly retraining when new behaviours or sensor setups appear. Recent attempts to use large language models (LLMs) for HAR, typically by converting signals into text or images, suffer from limited accuracy and lack verifiable interpretability. We propose ZARA, the first agent-based framework for zero-shot, explainable HAR directly from raw motion time-series. ZARA integrates an automatically derived pair-wise feature knowledge base that captures discriminative statistics for every activity pair, a multi-sensor retrieval module that surfaces relevant evidence, and a hierarchical agent pipeline that guides the LLM to iteratively select features, draw on this evidence, and produce both activity predictions and natural-language explanations. ZARA enables flexible and interpretable HAR without any fine-tuning or task-specific classifiers. Extensive experiments on 8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering clear reasoning while exceeding the strongest baselines by 2.53x in macro F1. Ablation studies further confirm the necessity of each module, marking ZARA as a promising step toward trustworthy, plug-and-play motion time-series analysis. Our codes are available at https://github.com/zechenli03/ZARA."

[20.08.2025 08:18] Response: ```python
['AGENTS', 'MULTIMODAL', 'HEALTHCARE']
```
[20.08.2025 08:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ZARA is an agent-based framework for zero-shot, explainable human activity recognition from raw motion time-series, achieving state-of-the-art performance without fine-tuning.  					AI-generated summary 				 Motion sensor time-series are central to human activity recognition (HAR), with applications in health, sports, and smart devices. However, existing methods are trained for fixed activity sets and require costly retraining when new behaviours or sensor setups appear. Recent attempts to use large language models (LLMs) for HAR, typically by converting signals into text or images, suffer from limited accuracy and lack verifiable interpretability. We propose ZARA, the first agent-based framework for zero-shot, explainable HAR directly from raw motion time-series. ZARA integrates an automatically derived pair-wise feature knowledge base that captures discriminative statistics for every activity pair, a multi-sensor retrieval module that surfaces relevant evidence, and a hierarchical agent pipeline that guides the LLM to iteratively select features, draw on this evidence, and produce both activity predictions and natural-language explanations. ZARA enables flexible and interpretable HAR without any fine-tuning or task-specific classifiers. Extensive experiments on 8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering clear reasoning while exceeding the strongest baselines by 2.53x in macro F1. Ablation studies further confirm the necessity of each module, marking ZARA as a promising step toward trustworthy, plug-and-play motion time-series analysis. Our codes are available at https://github.com/zechenli03/ZARA."

[20.08.2025 08:18] Response: ```python
['INTERPRETABILITY', 'REASONING']
```
[20.08.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ZARA is a novel framework designed for zero-shot human activity recognition (HAR) using raw motion time-series data. It eliminates the need for retraining by leveraging an agent-based approach that integrates a feature knowledge base and a multi-sensor retrieval module. This allows ZARA to provide accurate activity predictions along with natural-language explanations, enhancing interpretability. Extensive testing shows that ZARA outperforms existing methods significantly, making it a promising solution for flexible and explainable HAR applications.","title":"ZARA: Zero-Shot HAR with Explainable Insights"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ZARA is a novel framework designed for zero-shot human activity recognition (HAR) using raw motion time-series data. It eliminates the need for retraining by leveraging an agent-based approach that integrates a feature knowledge base and a multi-sensor retrieval module. This allows ZARA to provide accurate activity predictions along with natural-language explanations, enhancing interpretability. Extensive testing shows that ZARA outperforms existing methods significantly, making it a promising solution for flexible and explainable HAR applications.', title='ZARA: Zero-Shot HAR with Explainable Insights'))
[20.08.2025 08:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ZARA是一个基于代理的框架，旨在从原始运动时间序列中进行零样本、可解释的人类活动识别。与传统方法不同，ZARA无需微调，能够灵活应对新行为和传感器设置。该框架结合了自动生成的特征知识库和多传感器检索模块，能够提供相关证据并生成自然语言解释。实验结果表明，ZARA在八个基准测试中实现了最先进的零样本性能，显著超越了现有的最佳基线。","title":"ZARA：无微调的可解释人类活动识别新框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ZARA是一个基于代理的框架，旨在从原始运动时间序列中进行零样本、可解释的人类活动识别。与传统方法不同，ZARA无需微调，能够灵活应对新行为和传感器设置。该框架结合了自动生成的特征知识库和多传感器检索模块，能够提供相关证据并生成自然语言解释。实验结果表明，ZARA在八个基准测试中实现了最先进的零样本性能，显著超越了现有的最佳基线。', title='ZARA：无微调的可解释人类活动识别新框架'))
[20.08.2025 08:18] Renaming data file.
[20.08.2025 08:18] Renaming previous data. hf_papers.json to ./d/2025-08-20.json
[20.08.2025 08:18] Saving new data file.
[20.08.2025 08:18] Generating page.
[20.08.2025 08:18] Renaming previous page.
[20.08.2025 08:18] Renaming previous data. index.html to ./d/2025-08-20.html
[20.08.2025 08:18] Writing result.
[20.08.2025 08:18] Renaming log file.
[20.08.2025 08:18] Renaming previous data. log.txt to ./logs/2025-08-20_last_log.txt
