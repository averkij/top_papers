[23.09.2025 20:12] Read previous papers.
[23.09.2025 20:12] Generating top page (month).
[23.09.2025 20:12] Writing top page (month).
[23.09.2025 21:12] Read previous papers.
[23.09.2025 21:12] Get feed.
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17567
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17765
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17627
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18091
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18056
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17437
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.16117
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17396
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.16941
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18084
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17985
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17177
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17158
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.16596
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17671
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17428
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18058
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.15709
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18010
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17818
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.15248
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18095
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18094
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18083
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17641
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17336
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.18053
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17786
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17998
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17938
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17399
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17277
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.17191
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.16633
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.16591
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.16415
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.14856
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.09873
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04441
[23.09.2025 21:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.16548
[23.09.2025 21:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[23.09.2025 21:12] No deleted papers detected.
[23.09.2025 21:12] Downloading and parsing papers (pdf, html). Total: 40.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17567.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17567.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17567.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17765.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17765.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17765.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17627.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17627.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17627.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.18091.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.18091.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.18091.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.18056.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.18056.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.18056.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17437.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17437.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17437.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.16117.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.16117.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.16117.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17396.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17396.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17396.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.16941.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.16941.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.16941.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.18084.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.18084.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.18084.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17985.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17985.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17985.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17177.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17177.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17177.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17158.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17158.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17158.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.16596.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.16596.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.16596.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17671.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17671.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17671.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17428.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17428.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17428.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.18058.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.18058.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.18058.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.15709.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.15709.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.15709.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.18010.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.18010.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.18010.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17818.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17818.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17818.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.15248.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.15248.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.15248.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.18095.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.18095.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.18095.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.18094.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.18094.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.18094.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.18083.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.18083.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.18083.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17641.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17641.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17641.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17336.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17336.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17336.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.18053.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.18053.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.18053.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17786.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17786.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17786.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17998.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17998.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17998.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17938.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17938.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17938.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17399.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17399.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17399.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17277.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17277.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17277.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.17191.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.17191.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.17191.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.16633.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.16633.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.16633.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.16591.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.16591.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.16591.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.16415.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.16415.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.16415.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.14856.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.14856.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.14856.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.09873.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.09873.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.09873.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.04441.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.04441.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.04441.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Downloading and parsing paper https://huggingface.co/papers/2509.16548.
[23.09.2025 21:12] Extra JSON file exists (./assets/json/2509.16548.json), skip PDF parsing.
[23.09.2025 21:12] Paper image links file exists (./assets/img_data/2509.16548.json), skip HTML parsing.
[23.09.2025 21:12] Success.
[23.09.2025 21:12] Enriching papers with extra data.
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 0. LIMI demonstrates that sophisticated agentic intelligence can emerge from minimal, strategically curated demonstrations, outperforming data-intensive models on agency benchmarks.  					AI-generated summary 				 We define Agency as the emergent capacity of AI systems to function as autonomous agents ...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 1. Qwen3-Omni, a multimodal model, achieves state-of-the-art performance across text, image, audio, and video, using a Thinker-Talker MoE architecture and a lightweight causal ConvNet for efficient streaming synthesis.  					AI-generated summary 				 We present Qwen3-Omni, a single multimodal model tha...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 2. OmniInsert addresses challenges in mask-free video insertion using a novel data pipeline, feature injection, progressive training, and context-aware rephrasing, outperforming commercial solutions.  					AI-generated summary 				 Recent advances in video insertion based on diffusion models are impres...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 3. OnePiece integrates LLM-style context engineering and reasoning into industrial search and recommendation systems, achieving significant improvements in key business metrics.  					AI-generated summary 				 Despite the growing interest in replicating the scaled success of large language models (LLMs...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 4. TempSamp-R1, a reinforcement fine-tuning framework, enhances multimodal large language models for video temporal grounding by using off-policy supervision and a hybrid Chain-of-Thought training paradigm, achieving state-of-the-art performance on benchmark datasets.  					AI-generated summary 				 Th...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 5. A two-stage reinforcement learning framework improves geometric reasoning and problem-solving in multimodal language models by first enhancing visual perception.  					AI-generated summary 				 Recent advancements in reinforcement learning (RL) have enhanced the reasoning abilities of large language...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 6. Diffusion Negative-aware FineTuning (DiffusionNFT) optimizes diffusion models directly on the forward process via flow matching, improving efficiency and performance compared to existing methods.  					AI-generated summary 				 Online reinforcement learning (RL) has been central to post-training lan...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 7. EpiCache is a KV cache management framework for long conversational question answering that reduces memory usage and improves accuracy through block-wise prefill, episodic KV compression, and adaptive layer-wise budget allocation.  					AI-generated summary 				 Recent advances in large language mod...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 8. SWE-Bench Pro is a challenging benchmark for coding models, featuring complex, enterprise-level problems that require substantial code modifications, with performance evaluations showing significant limitations in current models.  					AI-generated summary 				 We introduce SWE-Bench Pro, a substant...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 9. This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic parallel wrist for robotic manipulation. ByteWrist addresses the critical limitations of existing serial and parallel wrists in narrow-space operations through a compact three-stage parallel drive mechanism integrated with ...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 10. VideoFrom3D synthesizes high-quality 3D scene videos using a combination of image and video diffusion models, achieving style consistency without requiring paired datasets.  					AI-generated summary 				 In this paper, we propose VideoFrom3D, a novel framework for synthesizing high-quality 3D scene...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 11. A contamination-free evaluation of large reasoning models is conducted using the ROME benchmark, which tests reasoning from visual clues in vision language models.  					AI-generated summary 				 We conduct a moderate-scale contamination-free (to some extent) evaluation of current large reasoning mo...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 12. Meta Agents Research Environments (ARE) facilitate the creation and execution of complex environments for agent research, and Gaia2, a benchmark built on ARE, evaluates general agent capabilities in dynamic, asynchronous settings.  					AI-generated summary 				 We introduce Meta Agents Research Env...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 13. Supervised fine-tuning of large language models can negatively impact closed-book question answering performance, with up to 90% of parameter updates not contributing to knowledge enhancement.  					AI-generated summary 				 Large language models (LLMs) acquire substantial world knowledge during pre...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 14. Turk-LettuceDetect, a suite of hallucination detection models for Turkish RAG applications, achieves high performance using fine-tuned encoder architectures on a machine-translated RAGTruth dataset.  					AI-generated summary 				 The widespread adoption of Large Language Models (LLMs) has been hind...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 15. QWHA integrates Walsh-Hadamard Transform-based adapters into quantized models to reduce quantization errors and computational overhead, improving low-bit quantization accuracy and training speed.  					AI-generated summary 				 The demand for efficient deployment of large language models (LLMs) has ...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 16. Frontier large language models can develop a preference for strategic dishonesty in response to harmful requests, impacting safety evaluations and acting as a honeypot against malicious users, while internal activation probes can detect this behavior.  					AI-generated summary 				 Large language m...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 17. Large-scale experiments reveal double-peak and logarithmic performance patterns in collaborative filtering models as embedding dimensions scale, and provide theoretical insights into their causes.  					AI-generated summary 				 Scaling recommendation models into large recommendation models has beco...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 18. Cross-attention in speech-to-text models aligns moderately with saliency-based explanations but captures only a portion of input relevance and decoder attention.  					AI-generated summary 				 Cross-attention is a core mechanism in encoder-decoder architectures, widespread in many fields, including...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 19. ContextFlow, a training-free framework for Diffusion Transformers, enhances video object editing by using a high-order Rectified Flow solver and Adaptive Context Enrichment to achieve precise, temporally consistent, and high-fidelity object manipulation.  					AI-generated summary 				 Training-free...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 20. Synthetic Bootstrapped Pretraining (SBP) enhances language model performance by learning inter-document correlations and synthesizing new training data, leading to significant improvements over standard pretraining methods.  					AI-generated summary 				 We introduce Synthetic Bootstrapped Pretrain...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 21. MetaEmbed, a new framework for multimodal retrieval, uses learnable Meta Tokens to provide compact yet expressive multi-vector embeddings, enabling scalable and efficient retrieval performance.  					AI-generated summary 				 Universal multimodal embedding models have achieved great success in captu...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 22. UniPixel, a large multi-modal model, integrates pixel-level perception with general visual understanding, enabling fine-grained reasoning across various tasks including pixel-level referring, segmentation, and question answering.  					AI-generated summary 				 Recent advances in Large Multi-modal M...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 23. Reasoning Core is a scalable RLVR environment that generates diverse symbolic reasoning problems to enhance LLM capabilities.  					AI-generated summary 				 We introduce Reasoning Core, a new scalable environment for Reinforcement Learning with Verifiable Rewards (RLVR), designed to advance foundat...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 24. AuditoryBench++ and AIR-CoT enhance text-only models' auditory reasoning and knowledge integration, outperforming existing models in multimodal interactions.  					AI-generated summary 				 Even without directly hearing sounds, humans can effortlessly reason about auditory properties, such as pitch,...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 25. A robust GUI agent, Mano, integrates reinforcement learning with vision-language models for high-fidelity data generation and improved performance on GUI benchmarks.  					AI-generated summary 				 Graphical user interfaces (GUIs) are the primary medium for human-computer interaction, yet automating...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 26. A graph-of-thoughts framework incorporating occlusion-aware perception and planning-aware prediction enhances cooperative autonomous driving using a Multimodal Large Language Model.  					AI-generated summary 				 Current state-of-the-art autonomous vehicles could face safety-critical situations whe...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 27. Core Space merging framework improves the accuracy and efficiency of merging low-rank adapted models across tasks without significant computational cost.  					AI-generated summary 				 In this paper, we address the challenges associated with merging low-rank adaptations of large neural networks. Wi...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 28. Context-Aware Kernel Evolution (CAKE) enhances Bayesian optimization by using large language models to adaptively generate and refine Gaussian process kernels, outperforming traditional methods across various tasks.  					AI-generated summary 				 The efficiency of Bayesian optimization (BO) relies ...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 29. The Deceptive Reasoning Exposure Suite (D-REX) evaluates the internal reasoning of Large Language Models to detect deceptive behaviors that bypass safety filters.  					AI-generated summary 				 The safety and alignment of Large Language Models (LLMs) are critical for their responsible deployment. C...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 30. A new dataset for Indian culture is introduced to evaluate the cultural competence of large language models, focusing on sub-regional cultural facets and providing a framework for human and model-based evaluations.  					AI-generated summary 				 Large language models (LLMs) are widely used in vario...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 31. BeepBank-500 is a synthetic earcon/alert dataset for audio machine learning, featuring parametrically generated clips with various waveform families and reverberation settings.  					AI-generated summary 				 We introduce BeepBank-500, a compact, fully synthetic earcon/alert dataset (300-500 clips) ...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 32. VaseVL, an SFT-then-RL system, enhances MLLMs for ancient Greek pottery analysis by addressing performance gaps through taxonomy-conditioned rewards, achieving state-of-the-art results in style classification and historical attribution.  					AI-generated summary 				 Analyzing cultural-heritage art...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 33. The Model Parity Aligner (MPA) framework improves Small Vision-Language Models (S-VLMs) by leveraging unlabeled images and knowledge transfer from Large Vision-Language Models (L-VLMs) to reduce performance gaps in vision and language tasks.  					AI-generated summary 				 Large Vision-Language Mode...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 34. Heterogeneous Adaptive Policy Optimization (HAPO) enhances reinforcement learning in LLMs by dynamically adapting token optimization based on entropy, improving performance across various model scales.  					AI-generated summary 				 Reinforcement Learning has emerged as the fundamental technique fo...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 35. StereoAdapter is a parameter-efficient self-supervised framework that integrates a LoRA-adapted monocular encoder with a recurrent stereo refinement module for underwater stereo depth estimation, improving accuracy and robustness.  					AI-generated summary 				 Underwater stereo depth estimation pr...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 36. A new benchmark, CodeFuse-CR-Bench, evaluates LLMs in repository-level code review with comprehensive, context-rich data and a novel evaluation framework.  					AI-generated summary 				 Automated code review (CR) is a key application for Large Language Models (LLMs), but progress is hampered by a "...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 37. The study audits licenses in the Hugging Face ecosystem, revealing systemic non-compliance and proposing a rule engine to detect and resolve license conflicts in open-source AI.  					AI-generated summary 				 Hidden license conflicts in the open-source AI ecosystem pose serious legal and ethical ri...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 38. DEXOP, a passive hand exoskeleton, enhances robotic data collection by sensorizing human manipulation, improving data transferability and task performance.  					AI-generated summary 				 We introduce perioperation, a paradigm for robotic data collection that sensorizes and records human manipulatio...
[23.09.2025 21:12] ********************************************************************************
[23.09.2025 21:12] Abstract 39. SCAN, a self-denoising Monte Carlo framework, improves PRM performance with synthetic data, achieving high F1 scores and surpassing human-annotated baselines.  					AI-generated summary 				 Process reward models (PRMs) offer fine-grained, step-level evaluations that facilitate deeper reasoning proc...
[23.09.2025 21:12] Read previous papers.
[23.09.2025 21:12] Generating reviews via LLM API.
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#optimization", "#agents", "#agi"], "emoji": "ü§ñ", "ru": {"title": "–ú–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö, –±–æ–ª—å—à–µ –∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç–∏: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º LIMI (Less Is More for Intelligent Agenc
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#hallucinations", "#audio", "#multimodal", "#architecture", "#open_source", "#long_context", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "Qwen3-Omni: –ï–¥–∏–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ò–ò –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "Qwen3-Omni - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –¥–æ—Å—Ç–∏–≥–∞—é—â–∞—è –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#data", "#dataset", "#benchmark", "#optimization", "#video", "#open_source", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–£–º–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ –≤ –≤–∏–¥–µ–æ –±–µ–∑ –º–∞—Å–æ–∫", "desc": "OmniInsert - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–∞—Å–æ–∫. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Ö–≤–∞—Ç–∫–∏ –¥–∞–Ω
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#optimization", "#reasoning", "#training"], "emoji": "üß©", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω—è—è –º–æ—â—å LLM –∏ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "OnePiece - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—â–∞—è –º–µ—Ç–æ–¥—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#rl", "#optimization", "#training", "#rag", "#multimodal", "#benchmark", "#reasoning"], "emoji": "üé•", "ru": {"title": "TempSamp-R1: –ü—Ä–æ—Ä—ã–≤ –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "TempSamp-R1 - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#rl", "#hallucinations", "#training", "#multimodal", "#benchmark", "#reasoning"], "emoji": "üìê", "ru": {"title": "–î–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —É–ª—É—á—à–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#optimization", "#diffusion", "#training"], "emoji": "üîÑ", "ru": {"title": "DiffusionNFT: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–π –ø—Ä–æ—Ü–µ—Å—Å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º DiffusionNFT. –≠—Ç–æ
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#data", "#inference", "#optimization", "#long_context", "#training"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ —Å –ò–ò", "desc": "EpiCache - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è KV-–∫—ç—à–µ–º –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–ª–æ—á–Ω–æ
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#optimization", "#interpretability", "#agents", "#benchmark"], "emoji": "üßë‚Äçüíª", "ru": {"title": "SWE-Bench Pro: –í—ã–∑–æ–≤ –¥–ª—è AI –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ü–û", "desc": "SWE-Bench Pro - —ç—Ç–æ —Å–ª–æ–∂–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º–æ–¥–µ–ª–µ–π –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è. –û–Ω –≤–∫–ª—é—á
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#robotics"], "emoji": "ü¶æ", "ru": {"title": "ByteWrist: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—è—Å—Ç—å—è—Ö –¥–ª—è —É–∑–∫–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ByteWrist - –Ω–æ–≤–æ–µ –≤—ã—Å–æ–∫–æ–≥–∏–±–∫–æ–µ –∏ –∞–Ω—Ç—Ä–æ–ø–æ–º–æ—Ä—Ñ–Ω–æ–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –∑–∞–ø—è—Å—Ç—å–µ –¥–ª—è —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π. ByteWrist —Ä–µ—à–∞–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#synthetic", "#3d", "#diffusion", "#video"], "emoji": "üé¨", "ru": {"title": "VideoFrom3D: –°–∏–Ω—Ç–µ–∑ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ –∏–∑ –≥—Ä—É–±–æ–π 3D-–≥–µ–æ–º–µ—Ç—Ä–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VideoFrom3D - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ 3D-—Å—Ü–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∏–∑–æ–±—Ä
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#reasoning", "#cv", "#benchmark"], "emoji": "üß†", "ru": {"title": "–ß–∏—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏", "desc": "–ü—Ä–æ–≤–µ–¥–µ–Ω–∞ –æ—Ü–µ–Ω–∫–∞ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –±–µ–∑ –∫–æ–Ω—Ç–∞–º–∏–Ω–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–µ–Ω—á–º–∞—Ä–∫–∞ ROME. ROME –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#agi", "#optimization", "#transfer_learning", "#games", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "ARE –∏ Gaia2: –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Meta Agents Research Environments (ARE) - –ø–ª–∞—Ç—Ñ
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#interpretability", "#optimization", "#training"], "emoji": "üß†", "ru": {"title": "–û—Å—Ç–æ—Ä–æ–∂–Ω–æ —Å –¥–æ–æ–±—É—á–µ–Ω–∏–µ–º: –±–æ–ª—å—à–µ –Ω–µ –≤—Å–µ–≥–¥–∞ –ª—É—á—à–µ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ –≤–ª–∏—è—Ç—å –Ω–∞ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å 
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#multilingual", "#open_source", "#architecture", "#long_context", "#hallucinations", "#low_resource", "#rag", "#dataset"], "emoji": "ü¶É", "ru": {"title": "–ë–æ—Ä—å–±–∞ —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –≤ —Ç—É—Ä–µ—Ü–∫–æ–º —è–∑—ã–∫–µ: —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å Turk-LettuceDetect", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Turk-Let
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#training", "#inference", "#optimization"], "emoji": "üßÆ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é QWHA", "desc": "QWHA - —ç—Ç–æ –º–µ—Ç–æ–¥, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—â–∏–π –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –£–æ–ª—à–∞-–ê–¥–∞–º–∞—Ä–∞ –≤ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –∫–≤–∞–Ω—Ç–æ
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#benchmark", "#ethics", "#rlhf", "#alignment", "#dataset", "#security"], "emoji": "üé≠", "ru": {"title": "–°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∞—è –Ω–µ—á–µ—Å—Ç–Ω–æ—Å—Ç—å –ò–ò: —Å–∫—Ä—ã—Ç–∞—è —É–≥—Ä–æ–∑–∞ –∏–ª–∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –∑–∞—â–∏—Ç–Ω–∏–∫?", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø–µ—Ä–µ–¥–æ–≤—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –º–æ–≥—É—Ç —Ä–∞–∑–≤–∏—Ç—å —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ —Å—Ç—Ä–∞—Ç–µ–≥–∏
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#architecture", "#benchmark"], "emoji": "üìä", "ru": {"title": "–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–≤–µ–ª–∏ –º–∞—Å—à—Ç–∞–±–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏, –≤–∞—Ä—å–∏—Ä—É—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#interpretability", "#audio", "#multilingual"], "emoji": "üéôÔ∏è", "ru": {"title": "–ö—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏–µ –≤ —Ä–µ—á–µ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ, –Ω–æ –Ω–µ–ø–æ–ª–Ω–æ", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –æ–±—ä—è—Å–Ω–∏—Ç–µ–ª—å–Ω—É—é —Å–∏–ª—É –º–µ—Ö–∞–Ω–∏–∑–º–∞ –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏—è –≤ –º–æ–¥–µ–ª—è—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ä–µ—á–∏ –≤ —Ç–µ–∫—Å—Ç. –ê–≤—Ç–æ—Ä—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç –æ—Ü–µ–Ω–∫–∏ –∫—Ä–æ—Å—Å-–≤
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#architecture", "#video", "#diffusion", "#optimization"], "emoji": "üé¨", "ru": {"title": "ContextFlow: –ü—Ä–æ—Ä—ã–≤ –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤–∏–¥–µ–æ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "ContextFlow - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#data", "#dataset", "#synthetic", "#architecture", "#optimization", "#training"], "emoji": "üîÑ", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Synthetic Boo
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#optimization", "#rag", "#games"], "emoji": "üîç", "ru": {"title": "MetaEmbed: –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –≤–ª–æ–∂–µ–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞", "desc": "MetaEmbed - –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –æ–±—É—á–∞–µ–º—ã–µ –ú–µ—Ç–∞-–¢–æ–∫–µ–Ω—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#games", "#cv", "#multimodal"], "emoji": "üñºÔ∏è", "ru": {"title": "UniPixel: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–∏–∫—Å–µ–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è", "desc": "UniPixel - —ç—Ç–æ –∫—Ä—É–ø–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∏–∫—Å–µ–ª–µ–π —Å –æ–±—â–∏–º –≤–∏–∑—É–∞–ª—å–Ω—ã–º –ø–æ
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#benchmark", "#training"], "emoji": "üß†", "ru": {"title": "Reasoning Core: –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∑–∞–¥–∞—á –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ò–ò", "desc": "Reasoning Core - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å—Ä–µ–¥–∞ RLVR, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ —Å–∏–º–≤–æ–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –¥–ª—è —É
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#benchmark", "#audio", "#reasoning"], "emoji": "üéß", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Å–ª—É—Ö–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ò–ò-–º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AuditoryBench++, –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ª—É—Ö–æ–≤–æ–≥–æ –∑–Ω–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö 
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#cv", "#agents", "#rl", "#games", "#multimodal", "#rlhf", "#benchmark", "#optimization", "#training"], "emoji": "üñ•Ô∏è", "ru": {"title": "Mano: –ò–ò-–∞–≥–µ–Ω—Ç –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Mano - –Ω–∞–¥–µ–∂–Ω–æ–≥–æ GUI-–∞–≥–µ–Ω—Ç–∞, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—â–µ–≥–æ
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#training", "#agents", "#graphs", "#multimodal"], "emoji": "üöó", "ru": {"title": "–ì—Ä–∞—Ñ –º—ã—Å–ª–µ–π —É–ª—É—á—à–∞–µ—Ç –∫–æ–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –≤–æ–∂–¥–µ–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≥—Ä–∞—Ñ–∞ –º—ã—Å–ª–µ–π –¥–ª—è –∫–æ–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#training", "#architecture"], "emoji": "üîÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤—ã—Ö –∞–¥–∞–ø—Ç–∞—Ü–∏–π –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é Low-Rank Adaptation (LoRA). –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture", "#open_source", "#data"], "emoji": "üç∞", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —è–¥—Ä–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ë–∞–π–µ—Å–∞: —Å–≤–µ–∂–µ–∏—Å–ø–µ—á–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ë–∞–π–µ—Å–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CAKE (Context-Aware Kernel Evolution). C
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#alignment", "#hallucinations", "#reasoning", "#benchmark", "#dataset", "#security"], "emoji": "üïµÔ∏è", "ru": {"title": "–†–∞–∑–æ–±–ª–∞—á–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö –Ω–∞–º–µ—Ä–µ–Ω–∏–π –ò–ò", "desc": "D-REX - —ç—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å —Ü–µ–ª—å—é –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±–º–∞–Ω—á–∏–≤–æ–≥
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#alignment", "#ethics", "#dataset", "#data"], "emoji": "üáÆüá≥", "ru": {"title": "–ö—É–ª—å—Ç—É—Ä–Ω–∞—è –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –ò–ò: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –æ—Ü–µ–Ω–∫—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫—É–ª—å—Ç—É—Ä–Ω–æ–π –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏–Ω–¥–∏–π—Å–∫–æ–π –∫
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#dataset", "#audio", "#open_source", "#synthetic"], "emoji": "üîä", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–≤—É–∫–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "BeepBank-500 - —ç—Ç–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ –∞—É–¥–∏–æ, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ 300-50
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#rl", "#optimization", "#training", "#benchmark", "#reasoning", "#dataset", "#science"], "emoji": "üè∫", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Ç–∞–π–Ω—ã –¥—Ä–µ–≤–Ω–µ–≥—Ä–µ—á–µ—Å–∫–∏—Ö –≤–∞–∑", "desc": "VaseVL - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–∞—è –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#benchmark", "#small_models", "#transfer_learning", "#optimization", "#training"], "emoji": "üîç", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–∞–ª—ã—Ö –∏ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è", "desc": "–ú–æ–¥–µ–ª—å Model Parity Aligner (MPA) –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#rl", "#rlhf", "#optimization", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - Heterogeneous Adaptive Poli
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#architecture", "#dataset", "#robotics", "#benchmark", "#optimization", "#synthetic", "#3d"], "emoji": "üê†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –¥–ª—è –ø–æ–¥–≤–æ–¥–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã", "desc": "StereoAdapter - —ç—Ç–æ —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã –ø
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#optimization", "#survey", "#benchmark"], "emoji": "üßë‚Äçüíª", "ru": {"title": "CodeFuse-CR-Bench: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ LLM –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–¥–∞", "desc": "CodeFuse-CR-Bench - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è. –û–Ω 
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#data", "#open_source", "#dataset", "#ethics"], "emoji": "‚öñÔ∏è", "ru": {"title": "–°–∫—Ä—ã—Ç—ã–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –ª–∏—Ü–µ–Ω–∑–∏–π –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –ò–ò: –≤—ã—è–≤–ª–µ–Ω–∏–µ –∏ —Ä–µ—à–µ–Ω–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç –∞—É–¥–∏—Ç –ª–∏—Ü–µ–Ω–∑–∏–π –≤ —ç–∫–æ—Å–∏—Å—Ç–µ–º–µ Hugging Face, –≤—ã—è–≤–ª—è—è —Å–∏—Å—Ç–µ–º–Ω–æ–µ –Ω–µ—Å–æ–±–ª—é–¥–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª. –ê–Ω–∞–ª–∏–∑ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 364 —Ç—ã—Å—è—á
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#robotics", "#transfer_learning", "#optimization", "#agents", "#dataset"], "emoji": "ü¶æ", "ru": {"title": "DEXOP: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏ —á–µ—Ä–µ–∑ —ç–∫–∑–æ—Å–∫–µ–ª–µ—Ç —Ä—É–∫–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω DEXOP - –ø–∞—Å—Å–∏–≤–Ω—ã–π —ç–∫–∑–æ—Å–∫–µ–ª–µ—Ç —Ä—É–∫–∏, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–±–æ—Ä —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö
[23.09.2025 21:12] Using data from previous issue: {"categories": ["#optimization", "#synthetic", "#training", "#data", "#reasoning", "#dataset"], "emoji": "üé≤", "ru": {"title": "SCAN: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ PRM –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SCAN - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∞–º–æ–æ—á–∏—â–∞—é—â–µ–π—Å—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –º–µ—Ç–æ–¥–æ–º –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞
[23.09.2025 21:12] Renaming data file.
[23.09.2025 21:12] Renaming previous data. hf_papers.json to ./d/2025-09-23.json
[23.09.2025 21:12] Saving new data file.
[23.09.2025 21:12] Generating page.
[23.09.2025 21:12] Renaming previous page.
[23.09.2025 21:12] Renaming previous data. index.html to ./d/2025-09-23.html
[23.09.2025 21:12] Writing result.
[23.09.2025 21:12] Renaming log file.
[23.09.2025 21:12] Renaming previous data. log.txt to ./logs/2025-09-23_last_log.txt
