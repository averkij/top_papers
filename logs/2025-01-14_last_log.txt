[14.01.2025 08:13] Read previous papers.
[14.01.2025 08:13] Generating top page (month).
[14.01.2025 08:13] Writing top page (month).
[14.01.2025 09:10] Read previous papers.
[14.01.2025 09:10] Get feed.
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.07301
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06425
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06173
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06252
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.07572
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06458
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06282
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06590
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.07574
[14.01.2025 09:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.01.2025 09:10] No deleted papers detected.
[14.01.2025 09:10] Downloading and parsing papers (pdf, html). Total: 9.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.07301.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.07301.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.07301.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.06425.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.06425.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.06425.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.06173.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.06173.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.06173.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.06252.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.06252.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.06252.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.07572.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.07572.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.07572.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.06458.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.06458.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.06458.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.06282.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.06282.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.06282.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.06590.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.06590.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.06590.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.07574.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.07574.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.07574.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Enriching papers with extra data.
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 0. Process Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), which aim to identify and mitigate intermediate errors in the reasoning processes. However, the development of effective PRMs faces significant challenges, p...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 1. Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent q...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 2. Recent video generation models have shown promising results in producing high-quality video clips lasting several seconds. However, these models face challenges in generating long sequences that convey clear and informative events, limiting their ability to support coherent narrations. In this paper...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 3. Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce \implname, a novel self-adaptation framework that adapts LLMs for unseen tasks ...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 4. Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address it, we introduce WebWalkerQ...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 5. Building upon our previous investigations of O1 replication (Part 1: Journey Learning [Qin et al., 2024] and Part 2: Distillation [Huang et al., 2024]), this work explores the potential of inference-time scaling in large language models (LLMs) for medical reasoning tasks, ranging from diagnostic dec...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 6. Recent advancements in large language models (LLMs) and multimodal speech-text models have laid the groundwork for seamless voice interactions, enabling real-time, natural, and human-like conversations. Previous models for voice interactions are categorized as native and aligned. Native models integ...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 7. Chemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and in...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 8. We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for 3D deep learning and 3D generative AI. uCO3D is the largest publicly-available collection of high-resolution videos of objects with 3D annotations that ensures full-360^{circ} coverage. uCO3D is significantly more diverse ...
[14.01.2025 09:10] Read previous papers.
[14.01.2025 09:10] Generating reviews via LLM API.
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#math", "#data", "#reasoning", "#benchmark", "#optimization", "#open_source", "#training"], "emoji": "üßÆ", "ru": {"title": "–£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–µ Process Reward Models –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ Process Reward Models (PRM) –¥–ª—è –∫–æ–Ω—Ç—Ä
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#benchmark", "#long_context", "#optimization", "#inference", "#architecture"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è - Tensor Product Attention (TPA), –∏—Å–ø–æ–ª
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#video", "#story_generation", "#dataset", "#long_context", "#training", "#multimodal", "#alignment"], "emoji": "üç≥", "ru": {"title": "–ì–æ—Ç–æ–≤–∏–º –¥–ª–∏–Ω–Ω—ã–µ –≤–∏–¥–µ–æ: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞—Ä—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫–æ–≤ –æ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–∏ –ø–∏—â–∏ –¥
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#multimodal", "#agi", "#rl", "#optimization", "#training", "#architecture"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–∞–¥–∞–ø—Ç–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º–æ–∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#rag", "#reasoning", "#benchmark", "#agi", "#optimization", "#games", "#interpretability", "#agents", "#survey"], "emoji": "üï∏Ô∏è", "ru": {"title": "WebWalker: —É–º–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#science", "#inference", "#healthcare", "#reasoning"], "emoji": "ü©∫", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–≤–æ–¥–∞ LLM —É–ª—É—á—à–∞–µ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–î–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–≤–æ–¥–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –¥–ª—è –∑–∞–¥–∞—á –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#training"], "emoji": "üó£Ô∏è", "ru": {"title": "MinMo: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–æ–ª–æ—Å–æ–≤–æ–º –ò–ò-–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MinMo - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –±–µ—Å–ø—Ä–µ–ø—è—Ç—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ 1,4 –º–∏–ª–ª–∏–æ–Ω–∞—Ö —á–∞—Å–æ–≤
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#science", "#reasoning", "#multimodal", "#agents", "#dataset"], "emoji": "üß™", "ru": {"title": "ChemAgent: –£–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è LLM –≤ —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "ChemAgent - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, —É–ª—É—á—à–∞—é—â–∞—è —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∑–∞–¥–∞—á–∞—Ö —Ö–∏–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –û–Ω–∞ –∏—Å–ø
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#synthetic", "#3d"], "emoji": "üîç", "ru": {"title": "uCO3D: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è 3D-–¥–∞–Ω–Ω—ã—Ö –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö uCO3D –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò –≤ 3D. –≠—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∏
[14.01.2025 09:10] Trying to get texts in Chinese.
[14.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Process Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), which aim to identify and mitigate intermediate errors in the reasoning processes. However, the development of effective PRMs faces significant challenges, particularly in data annotation and evaluation methodologies. In this paper, through extensive experiments, we demonstrate that commonly used Monte Carlo (MC) estimation-based data synthesis for PRMs typically yields inferior performance and generalization compared to LLM-as-a-judge and human annotation methods. MC estimation relies on completion models to evaluate current-step correctness, leading to inaccurate step verification. Furthermore, we identify potential biases in conventional Best-of-N (BoN) evaluation strategies for PRMs: (1) The unreliable policy models generate responses with correct answers but flawed processes, leading to a misalignment between the evaluation criteria of BoN and the PRM objectives of process verification. (2) The tolerance of PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a significant proportion of minimum scores concentrated on the final answer steps, revealing the shift from process to outcome-based assessment in BoN Optimized PRMs. To address these challenges, we develop a consensus filtering mechanism that effectively integrates MC estimation with LLM-as-a-judge and advocates a more comprehensive evaluation framework that combines response-level and step-level metrics. Based on the mechanisms, we significantly improve both model performance and data efficiency in the BoN evaluation and the step-wise error identification task. Finally, we release a new state-of-the-art PRM that outperforms existing open-source alternatives and provides practical guidelines for future research in building process supervision models.
[14.01.2025 09:10] Mistral response. {"id": "37b4761259d0491aa6079b79be146fb7", "object": "chat.completion", "created": 1736845843, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u53eb\u505a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8fc7\u7a0b\u76d1\u7763\u3002\u76ee\u6807\u662f\u8bc6\u522b\u548c\u51cf\u5c11\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5e38\u7528\u7684\u8499\u7279\u5361\u7f57\uff08MC\uff09\u4f30\u8ba1\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4f9d\u8d56\u5b8c\u6210\u6a21\u578b\u8bc4\u4f30\u5f53\u524d\u6b65\u9aa4\u7684\u6b63\u786e\u6027\uff0c\u5bfc\u81f4\u6b65\u9aa4\u9a8c\u8bc1\u4e0d\u51c6\u786e\u3002\u6587\u7ae0\u8fd8\u6307\u51fa\u4e86\u4f20\u7edfBest-of-N\uff08BoN\uff09\u8bc4\u4f30\u7b56\u7565\u7684\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5171\u8bc6\u8fc7\u6ee4\u673a\u5236\uff0c\u7ed3\u5408MC\u4f30\u8ba1\u548cLLM-as-a-judge\uff0c\u6539\u8fdb\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002\u6700\u540e\uff0c\u6587\u7ae0\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u6700\u5148\u8fdb\u7684PRM\uff0c\u5e76\u63d0\u4f9b\u4e86\u672a\u6765\u7814\u7a76\u7684\u5b9e\u7528\u6307\u5357\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 405, "total_tokens": 642, "completion_tokens": 237}}
[14.01.2025 09:10] Response: ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÂè´ÂÅöËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÔºåÁî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êï∞Â≠¶Êé®ÁêÜ‰∏≠ÁöÑËøáÁ®ãÁõëÁù£„ÄÇÁõÆÊ†áÊòØËØÜÂà´ÂíåÂáèÂ∞ëÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÈîôËØØ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∏∏Áî®ÁöÑËíôÁâπÂç°ÁΩóÔºàMCÔºâ‰º∞ËÆ°ÊñπÊ≥ïÊïàÊûú‰∏ç‰Ω≥ÔºåÂõ†‰∏∫ÂÆÉ‰æùËµñÂÆåÊàêÊ®°ÂûãËØÑ‰º∞ÂΩìÂâçÊ≠•È™§ÁöÑÊ≠£Á°ÆÊÄßÔºåÂØºËá¥Ê≠•È™§È™åËØÅ‰∏çÂáÜÁ°Æ„ÄÇÊñáÁ´†ËøòÊåáÂá∫‰∫Ü‰º†ÁªüBest-of-NÔºàBoNÔºâËØÑ‰º∞Á≠ñÁï•ÁöÑÂÅèÂ∑ÆÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÖ±ËØÜËøáÊª§Êú∫Âà∂ÔºåÁªìÂêàMC‰º∞ËÆ°ÂíåLLM-as-a-judgeÔºåÊîπËøõ‰∫ÜÊ®°ÂûãÊÄßËÉΩÂíåÊï∞ÊçÆÊïàÁéá„ÄÇÊúÄÂêéÔºåÊñáÁ´†ÂèëÂ∏É‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÊúÄÂÖàËøõÁöÑPRMÔºåÂπ∂Êèê‰æõ‰∫ÜÊú™Êù•Á†îÁ©∂ÁöÑÂÆûÁî®ÊåáÂçó„ÄÇ
[14.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÂè´ÂÅöËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÔºåÁî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êï∞Â≠¶Êé®ÁêÜ‰∏≠ÁöÑËøáÁ®ãÁõëÁù£„ÄÇÁõÆÊ†áÊòØËØÜÂà´ÂíåÂáèÂ∞ëÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÈîôËØØ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∏∏Áî®ÁöÑËíôÁâπÂç°ÁΩóÔºàMCÔºâ‰º∞ËÆ°ÊñπÊ≥ïÊïàÊûú‰∏ç‰Ω≥ÔºåÂõ†‰∏∫ÂÆÉ‰æùËµñÂÆåÊàêÊ®°ÂûãËØÑ‰º∞ÂΩìÂâçÊ≠•È™§ÁöÑÊ≠£Á°ÆÊÄßÔºåÂØºËá¥Ê≠•È™§È™åËØÅ‰∏çÂáÜÁ°Æ„ÄÇÊñáÁ´†ËøòÊåáÂá∫‰∫Ü‰º†ÁªüBest-of-NÔºàBoNÔºâËØÑ‰º∞Á≠ñÁï•ÁöÑÂÅèÂ∑ÆÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÖ±ËØÜËøáÊª§Êú∫Âà∂ÔºåÁªìÂêàMC‰º∞ËÆ°ÂíåLLM-as-a-judgeÔºåÊîπËøõ‰∫ÜÊ®°ÂûãÊÄßËÉΩÂíåÊï∞ÊçÆÊïàÁéá„ÄÇÊúÄÂêéÔºåÊñáÁ´†ÂèëÂ∏É‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÊúÄÂÖàËøõÁöÑPRMÔºåÂπ∂Êèê‰æõ‰∫ÜÊú™Êù•Á†îÁ©∂ÁöÑÂÆûÁî®ÊåáÂçó„ÄÇ
[14.01.2025 09:11] Mistral response. {"id": "a3f4ce8b79ce46cc9438dd060912250c", "object": "chat.completion", "created": 1736845848, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u53eb\u505a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8fc7\u7a0b\u76d1\u7763\u3002\u76ee\u6807\u662f\u8bc6\u522b\u548c\u51cf\u5c11\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5e38\u7528\u7684\u8499\u7279\u5361\u7f57\uff08MC\uff09\u4f30\u8ba1\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4f9d\u8d56\u5b8c\u6210\u6a21\u578b\u8bc4\u4f30\u5f53\u524d\u6b65\u9aa4\u7684\u6b63\u786e\u6027\uff0c\u5bfc\u81f4\u6b65\u9aa4\u9a8c\u8bc1\u4e0d\u51c6\u786e\u3002\u6587\u7ae0\u8fd8\u6307\u51fa\u4e86\u4f20\u7edfBest-of-N\uff08BoN\uff09\u8bc4\u4f30\u7b56\u7565\u7684\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5171\u8bc6\u8fc7\u6ee4\u673a\u5236\uff0c\u7ed3\u5408MC\u4f30\u8ba1\u548cLLM-as-a-judge\uff0c\u6539\u8fdb\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002\u6700\u540e\uff0c\u6587\u7ae0\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u6700\u5148\u8fdb\u7684PRM\uff0c\u5e76\u63d0\u4f9b\u4e86\u672a\u6765\u7814\u7a76\u7684\u5b9e\u7528\u6307\u5357\u3002\n\nzh\u00e8 pi\u0101n w\u00e9nzh\u0101ng ji\u00e8sh\u00e0o le y\u012b zh\u01d2ng x\u012bn de f\u0101ngf\u01ce, ji\u00e0ozu\u00f2 gu\u00f2ch\u00e9ng ji\u01cengl\u00ec m\u00f3x\u00edng (PRMs), y\u00f2ngy\u00fa d\u00e0x\u00edng y\u01d4y\u00e1n m\u00f3x\u00edng (LLMs) z\u00e0i sh\u00f9xu\u00e9 tu\u012bl\u01d0 zh\u014dng de gu\u00f2ch\u00e9ng ji\u00e0nd\u016b. M\u00f9bi\u0101o sh\u00ec sh\u00edbi\u00e9 h\u00e9 ji\u01censh\u01ceo tu\u012bl\u01d0 gu\u00f2ch\u00e9ng zh\u014dng de cu\u00f2w\u00f9. Y\u00e1nji\u016b f\u0101xi\u00e0n, ch\u00e1ngy\u00f2ng de m\u00e9ngt\u00e8k\u01celu\u00f3 (MC) g\u016bj\u00ec f\u0101ngf\u01ce xi\u00e0oji\u00e0, y\u012bnw\u00e8i t\u0101 y\u012bl\u00e0i w\u00e1nch\u00e9ng m\u00f3x\u00edng p\u00edngji\u00e0 d\u0101ngqi\u00e1n b\u00f9zh\u00f2u de zh\u00e8ngqu\u00e8x\u00ecng, d\u01ceozh\u00ec b\u00f9zh\u00f2u y\u00e0nzh\u00e8ng b\u00f9 zh\u01d4nqu\u00e8. W\u00e9nzh\u0101ng h\u00e1i zh\u01d0ch\u016b le chu\u00e1nt\u01d2ng Best-of-N (BoN) p\u00edngji\u00e0 c\u00e8l\u00fc\u00e8 de pi\u0101nch\u0101, b\u00ecng t\u00edch\u016b le y\u012b zh\u01d2ng g\u00f2ngsh\u00ec gu\u00f2l\u01dc j\u012bzh\u00ec, ji\u00e9h\u00e9 MC g\u016bj\u00ec h\u00e9 LLM-as-a-judge, g\u01ceij\u00ecn le m\u00f3x\u00edng x\u00ecngn\u00e9ng h\u00e9 sh\u00f9j\u00f9 xi\u00e0oy\u00f2ng. Zu\u00ech\u00f2u, w\u00e9nzh\u0101ng f\u0101b\u00f9 le y\u012bg\u00e8 x\u012bn de zu\u00ec xi\u0101nj\u00ecn de PRM, b\u00ecng t\u00edg\u014dng le w\u00e8il\u00e1i y\u00e1nji\u016b de sh\u00edy\u00f2ng zh\u01d0n\u00e1n."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 252, "total_tokens": 940, "completion_tokens": 688}}
[14.01.2025 09:11] Response: ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÂè´ÂÅöËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÔºåÁî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êï∞Â≠¶Êé®ÁêÜ‰∏≠ÁöÑËøáÁ®ãÁõëÁù£„ÄÇÁõÆÊ†áÊòØËØÜÂà´ÂíåÂáèÂ∞ëÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÈîôËØØ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∏∏Áî®ÁöÑËíôÁâπÂç°ÁΩóÔºàMCÔºâ‰º∞ËÆ°ÊñπÊ≥ïÊïàÊûú‰∏ç‰Ω≥ÔºåÂõ†‰∏∫ÂÆÉ‰æùËµñÂÆåÊàêÊ®°ÂûãËØÑ‰º∞ÂΩìÂâçÊ≠•È™§ÁöÑÊ≠£Á°ÆÊÄßÔºåÂØºËá¥Ê≠•È™§È™åËØÅ‰∏çÂáÜÁ°Æ„ÄÇÊñáÁ´†ËøòÊåáÂá∫‰∫Ü‰º†ÁªüBest-of-NÔºàBoNÔºâËØÑ‰º∞Á≠ñÁï•ÁöÑÂÅèÂ∑ÆÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÖ±ËØÜËøáÊª§Êú∫Âà∂ÔºåÁªìÂêàMC‰º∞ËÆ°ÂíåLLM-as-a-judgeÔºåÊîπËøõ‰∫ÜÊ®°ÂûãÊÄßËÉΩÂíåÊï∞ÊçÆÊïàÁéá„ÄÇÊúÄÂêéÔºåÊñáÁ´†ÂèëÂ∏É‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÊúÄÂÖàËøõÁöÑPRMÔºåÂπ∂Êèê‰æõ‰∫ÜÊú™Êù•Á†îÁ©∂ÁöÑÂÆûÁî®ÊåáÂçó„ÄÇ

zh√® piƒÅn w√©nzhƒÅng ji√®sh√†o le yƒ´ zh«íng xƒ´n de fƒÅngf«é, ji√†ozu√≤ gu√≤ch√©ng ji«éngl√¨ m√≥x√≠ng (PRMs), y√≤ngy√∫ d√†x√≠ng y«îy√°n m√≥x√≠ng (LLMs) z√†i sh√πxu√© tuƒ´l«ê zh≈çng de gu√≤ch√©ng ji√†nd≈´. M√πbiƒÅo sh√¨ sh√≠bi√© h√© ji«énsh«éo tuƒ´l«ê gu√≤ch√©ng zh≈çng de cu√≤w√π. Y√°nji≈´ fƒÅxi√†n, ch√°ngy√≤ng de m√©ngt√®k«élu√≥ (MC) g≈´j√¨ fƒÅngf«é xi√†oji√†, yƒ´nw√®i tƒÅ yƒ´l√†i w√°nch√©ng m√≥x√≠ng p√≠ngji√† dƒÅngqi√°n b√πzh√≤u de zh√®ngqu√®x√¨ng, d«éozh√¨ b√πzh√≤u y√†nzh√®ng b√π zh«înqu√®. W√©nzhƒÅng h√°i zh«êch≈´ le chu√°nt«íng Best-of-N (BoN) p√≠ngji√† c√®l√º√® de piƒÅnchƒÅ, b√¨ng t√≠ch≈´ le yƒ´ zh«íng g√≤ngsh√¨ gu√≤l«ú jƒ´zh√¨, ji√©h√© MC g≈´j√¨ h√© LLM-as-a-judge, g«éij√¨n le m√≥x√≠ng x√¨ngn√©ng h√© sh√πj√π xi√†oy√≤ng. Zu√¨h√≤u, w√©nzhƒÅng fƒÅb√π le yƒ´g√® xƒ´n de zu√¨ xiƒÅnj√¨n de PRM, b√¨ng t√≠g≈çng le w√®il√°i y√°nji≈´ de sh√≠y√≤ng zh«ên√°n.
[14.01.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÂè´ÂÅöËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÔºåÁî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êï∞Â≠¶Êé®ÁêÜ‰∏≠ÁöÑËøáÁ®ãÁõëÁù£„ÄÇÁõÆÊ†áÊòØËØÜÂà´ÂíåÂáèÂ∞ëÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÈîôËØØ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∏∏Áî®ÁöÑËíôÁâπÂç°ÁΩóÔºàMCÔºâ‰º∞ËÆ°ÊñπÊ≥ïÊïàÊûú‰∏ç‰Ω≥ÔºåÂõ†‰∏∫ÂÆÉ‰æùËµñÂÆåÊàêÊ®°ÂûãËØÑ‰º∞ÂΩìÂâçÊ≠•È™§ÁöÑÊ≠£Á°ÆÊÄßÔºåÂØºËá¥Ê≠•È™§È™åËØÅ‰∏çÂáÜÁ°Æ„ÄÇÊñáÁ´†ËøòÊåáÂá∫‰∫Ü‰º†ÁªüBest-of-NÔºàBoNÔºâËØÑ‰º∞Á≠ñÁï•ÁöÑÂÅèÂ∑ÆÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÖ±ËØÜËøáÊª§Êú∫Âà∂ÔºåÁªìÂêàMC‰º∞ËÆ°ÂíåLLM-as-a-judgeÔºåÊîπËøõ‰∫ÜÊ®°ÂûãÊÄßËÉΩÂíåÊï∞ÊçÆÊïàÁéá„ÄÇÊúÄÂêéÔºåÊñáÁ´†ÂèëÂ∏É‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÊúÄÂÖàËøõÁöÑPRMÔºåÂπ∂Êèê‰æõ‰∫ÜÊú™Êù•Á†îÁ©∂ÁöÑÂÆûÁî®ÊåáÂçó„ÄÇ
[14.01.2025 09:11] Mistral response. {"id": "b8d627f36e6e49b2bc8f05fa588d1e9a", "object": "chat.completion", "created": 1736845862, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\", \"pinyin\": \"gu\u00f2ch\u00e9ng ji\u01cengl\u00ec m\u00f3x\u00edng\", \"trans\": \"Process Reward Model\"},\n    {\"word\": \"\u5927\u578b\u8bed\u8a00\u6a21\u578b\", \"pinyin\": \"d\u00e0x\u00edng y\u01d4y\u00e1n m\u00f3x\u00edng\", \"trans\": \"Large Language Model\"},\n    {\"word\": \"\u6570\u5b66\u63a8\u7406\", \"pinyin\": \"sh\u00f9xu\u00e9 tu\u012bl\u01d0\", \"trans\": \"Mathematical Reasoning\"},\n    {\"word\": \"\u8fc7\u7a0b\u76d1\u7763\", \"pinyin\": \"gu\u00f2ch\u00e9ng ji\u00e0nd\u016b\", \"trans\": \"Process Supervision\"},\n    {\"word\": \"\u8499\u7279\u5361\u7f57\", \"pinyin\": \"m\u00e9ngt\u00e8k\u01celu\u00f3\", \"trans\": \"Monte Carlo\"},\n    {\"word\": \"\u4f30\u8ba1\", \"pinyin\": \"g\u016bj\u00ec\", \"trans\": \"Estimation\"},\n    {\"word\": \"\u4f9d\u8d56\", \"pinyin\": \"y\u012bl\u00e0i\", \"trans\": \"Depend\"},\n    {\"word\": \"\u8bc4\u4f30\", \"pinyin\": \"p\u00edngg\u016b\", \"trans\": \"Evaluate\"},\n    {\"word\": \"\u6b65\u9aa4\u9a8c\u8bc1\", \"pinyin\": \"b\u00f9zh\u00f2u y\u00e0nzh\u00e8ng\", \"trans\": \"Step Verification\"},\n    {\"word\": \"\u504f\u5dee\", \"pinyin\": \"pi\u0101nch\u0101\", \"trans\": \"Bias\"},\n    {\"word\": \"\u5171\u8bc6\u8fc7\u6ee4\u673a\u5236\", \"pinyin\": \"g\u00f2ngsh\u00ed gu\u00f2l\u01dc j\u012bzh\u00ec\", \"trans\": \"Consensus Filtering Mechanism\"},\n    {\"word\": \"LLM-as-a-judge\", \"pinyin\": \"LLM-as-a-judge\", \"trans\": \"LLM-as-a-judge\"},\n    {\"word\": \"\u6700\u5148\u8fdb\", \"pinyin\": \"zu\u00ecxi\u0101nj\u00ecn\", \"trans\": \"State-of-the-art\"},\n    {\"word\": \"\u5b9e\u7528\u6307\u5357\", \"pinyin\": \"sh\u00edy\u00f2ng zh\u01d0n\u00e1n\", \"trans\": \"Practical Guide\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 284, "total_tokens": 798, "completion_tokens": 514}}
[14.01.2025 09:11] Response: [
    {"word": "ËøáÁ®ãÂ•ñÂä±Ê®°Âûã", "pinyin": "gu√≤ch√©ng ji«éngl√¨ m√≥x√≠ng", "trans": "Process Reward Model"},
    {"word": "Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã", "pinyin": "d√†x√≠ng y«îy√°n m√≥x√≠ng", "trans": "Large Language Model"},
    {"word": "Êï∞Â≠¶Êé®ÁêÜ", "pinyin": "sh√πxu√© tuƒ´l«ê", "trans": "Mathematical Reasoning"},
    {"word": "ËøáÁ®ãÁõëÁù£", "pinyin": "gu√≤ch√©ng ji√†nd≈´", "trans": "Process Supervision"},
    {"word": "ËíôÁâπÂç°ÁΩó", "pinyin": "m√©ngt√®k«élu√≥", "trans": "Monte Carlo"},
    {"word": "‰º∞ËÆ°", "pinyin": "g≈´j√¨", "trans": "Estimation"},
    {"word": "‰æùËµñ", "pinyin": "yƒ´l√†i", "trans": "Depend"},
    {"word": "ËØÑ‰º∞", "pinyin": "p√≠ngg≈´", "trans": "Evaluate"},
    {"word": "Ê≠•È™§È™åËØÅ", "pinyin": "b√πzh√≤u y√†nzh√®ng", "trans": "Step Verification"},
    {"word": "ÂÅèÂ∑Æ", "pinyin": "piƒÅnchƒÅ", "trans": "Bias"},
    {"word": "ÂÖ±ËØÜËøáÊª§Êú∫Âà∂", "pinyin": "g√≤ngsh√≠ gu√≤l«ú jƒ´zh√¨", "trans": "Consensus Filtering Mechanism"},
    {"word": "LLM-as-a-judge", "pinyin": "LLM-as-a-judge", "trans": "LLM-as-a-judge"},
    {"word": "ÊúÄÂÖàËøõ", "pinyin": "zu√¨xiƒÅnj√¨n", "trans": "State-of-the-art"},
    {"word": "ÂÆûÁî®ÊåáÂçó", "pinyin": "sh√≠y√≤ng zh«ên√°n", "trans": "Practical Guide"}
]
[14.01.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÂè´ÂÅöËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÔºåÁî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Êï∞Â≠¶Êé®ÁêÜ‰∏≠ÁöÑËøáÁ®ãÁõëÁù£„ÄÇÁõÆÊ†áÊòØËØÜÂà´ÂíåÂáèÂ∞ëÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑÈîôËØØ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∏∏Áî®ÁöÑËíôÁâπÂç°ÁΩóÔºàMCÔºâ‰º∞ËÆ°ÊñπÊ≥ïÊïàÊûú‰∏ç‰Ω≥ÔºåÂõ†‰∏∫ÂÆÉ‰æùËµñÂÆåÊàêÊ®°ÂûãËØÑ‰º∞ÂΩìÂâçÊ≠•È™§ÁöÑÊ≠£Á°ÆÊÄßÔºåÂØºËá¥Ê≠•È™§È™åËØÅ‰∏çÂáÜÁ°Æ„ÄÇÊñáÁ´†ËøòÊåáÂá∫‰∫Ü‰º†ÁªüBest-of-NÔºàBoNÔºâËØÑ‰º∞Á≠ñÁï•ÁöÑÂÅèÂ∑ÆÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÖ±ËØÜËøáÊª§Êú∫Âà∂ÔºåÁªìÂêàMC‰º∞ËÆ°ÂíåLLM-as-a-judgeÔºåÊîπËøõ‰∫ÜÊ®°ÂûãÊÄßËÉΩÂíåÊï∞ÊçÆÊïàÁéá„ÄÇÊúÄÂêéÔºåÊñáÁ´†ÂèëÂ∏É‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÊúÄÂÖàËøõÁöÑPRMÔºåÂπ∂Êèê‰æõ‰∫ÜÊú™Êù•Á†îÁ©∂ÁöÑÂÆûÁî®ÊåáÂçó„ÄÇ
[14.01.2025 09:11] Mistral response. {"id": "b3d272c7ba2d4378b68a4a448c7006c6", "object": "chat.completion", "created": 1736845873, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article introduces a new method called Process Reward Models (PRMs) for process supervision of large language models (LLMs) in mathematical reasoning. The goal is to identify and reduce errors in the reasoning process. The research found that the commonly used Monte Carlo (MC) estimation method performs poorly because it relies on the completion model to evaluate the correctness of the current step, leading to inaccurate step verification. The article also points out the bias in traditional Best-of-N (BoN) evaluation strategies and proposes a consensus filtering mechanism that combines MC estimation and LLM-as-a-judge to improve model performance and data efficiency. Finally, the article releases a new state-of-the-art PRM and provides practical guidelines for future research."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 250, "total_tokens": 413, "completion_tokens": 163}}
[14.01.2025 09:11] Response: This article introduces a new method called Process Reward Models (PRMs) for process supervision of large language models (LLMs) in mathematical reasoning. The goal is to identify and reduce errors in the reasoning process. The research found that the commonly used Monte Carlo (MC) estimation method performs poorly because it relies on the completion model to evaluate the correctness of the current step, leading to inaccurate step verification. The article also points out the bias in traditional Best-of-N (BoN) evaluation strategies and proposes a consensus filtering mechanism that combines MC estimation and LLM-as-a-judge to improve model performance and data efficiency. Finally, the article releases a new state-of-the-art PRM and provides practical guidelines for future research.
[14.01.2025 09:11] Renaming data file.
[14.01.2025 09:11] Renaming previous data. hf_papers.json to ./d/2025-01-14.json
[14.01.2025 09:11] Saving new data file.
[14.01.2025 09:11] Generating page.
[14.01.2025 09:11] Renaming previous page.
[14.01.2025 09:11] Renaming previous data. index.html to ./d/2025-01-14.html
[14.01.2025 09:11] [Experimental] Generating Chinese page for reading.
[14.01.2025 09:11] Chinese vocab [{'word': 'ËøáÁ®ãÂ•ñÂä±Ê®°Âûã', 'pinyin': 'gu√≤ch√©ng ji«éngl√¨ m√≥x√≠ng', 'trans': 'Process Reward Model'}, {'word': 'Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√†x√≠ng y«îy√°n m√≥x√≠ng', 'trans': 'Large Language Model'}, {'word': 'Êï∞Â≠¶Êé®ÁêÜ', 'pinyin': 'sh√πxu√© tuƒ´l«ê', 'trans': 'Mathematical Reasoning'}, {'word': 'ËøáÁ®ãÁõëÁù£', 'pinyin': 'gu√≤ch√©ng ji√†nd≈´', 'trans': 'Process Supervision'}, {'word': 'ËíôÁâπÂç°ÁΩó', 'pinyin': 'm√©ngt√®k«élu√≥', 'trans': 'Monte Carlo'}, {'word': '‰º∞ËÆ°', 'pinyin': 'g≈´j√¨', 'trans': 'Estimation'}, {'word': '‰æùËµñ', 'pinyin': 'yƒ´l√†i', 'trans': 'Depend'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ngg≈´', 'trans': 'Evaluate'}, {'word': 'Ê≠•È™§È™åËØÅ', 'pinyin': 'b√πzh√≤u y√†nzh√®ng', 'trans': 'Step Verification'}, {'word': 'ÂÅèÂ∑Æ', 'pinyin': 'piƒÅnchƒÅ', 'trans': 'Bias'}, {'word': 'ÂÖ±ËØÜËøáÊª§Êú∫Âà∂', 'pinyin': 'g√≤ngsh√≠ gu√≤l«ú jƒ´zh√¨', 'trans': 'Consensus Filtering Mechanism'}, {'word': 'LLM-as-a-judge', 'pinyin': 'LLM-as-a-judge', 'trans': 'LLM-as-a-judge'}, {'word': 'ÊúÄÂÖàËøõ', 'pinyin': 'zu√¨xiƒÅnj√¨n', 'trans': 'State-of-the-art'}, {'word': 'ÂÆûÁî®ÊåáÂçó', 'pinyin': 'sh√≠y√≤ng zh«ên√°n', 'trans': 'Practical Guide'}]
[14.01.2025 09:11] Renaming previous Chinese page.
[14.01.2025 09:11] Renaming previous data. zh.html to ./d/2025-01-13_zh_reading_task.html
[14.01.2025 09:11] Writing Chinese reading task.
[14.01.2025 09:11] Writing result.
[14.01.2025 09:11] Renaming log file.
[14.01.2025 09:11] Renaming previous data. log.txt to ./logs/2025-01-14_last_log.txt
