[14.01.2025 02:07] Read previous papers.
[14.01.2025 02:07] Generating top page (month).
[14.01.2025 02:07] Writing top page (month).
[14.01.2025 03:10] Read previous papers.
[14.01.2025 03:10] Get feed.
[14.01.2025 03:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.05874
[14.01.2025 03:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.03841
[14.01.2025 03:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06186
[14.01.2025 03:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.05510
[14.01.2025 03:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.05727
[14.01.2025 03:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.05767
[14.01.2025 03:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.05452
[14.01.2025 03:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04698
[14.01.2025 03:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.05542
[14.01.2025 03:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.05707
[14.01.2025 03:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.04961
[14.01.2025 03:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06187
[14.01.2025 03:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06250
[14.01.2025 03:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.01.2025 03:10] No deleted papers detected.
[14.01.2025 03:10] Downloading and parsing papers (pdf, html). Total: 13.
[14.01.2025 03:10] Downloading and parsing paper https://huggingface.co/papers/2501.05874.
[14.01.2025 03:10] Extra JSON file exists (./assets/json/2501.05874.json), skip PDF parsing.
[14.01.2025 03:10] Paper image links file exists (./assets/img_data/2501.05874.json), skip HTML parsing.
[14.01.2025 03:10] Success.
[14.01.2025 03:10] Downloading and parsing paper https://huggingface.co/papers/2501.03841.
[14.01.2025 03:10] Extra JSON file exists (./assets/json/2501.03841.json), skip PDF parsing.
[14.01.2025 03:10] Paper image links file exists (./assets/img_data/2501.03841.json), skip HTML parsing.
[14.01.2025 03:10] Success.
[14.01.2025 03:10] Downloading and parsing paper https://huggingface.co/papers/2501.06186.
[14.01.2025 03:10] Extra JSON file exists (./assets/json/2501.06186.json), skip PDF parsing.
[14.01.2025 03:10] Paper image links file exists (./assets/img_data/2501.06186.json), skip HTML parsing.
[14.01.2025 03:10] Success.
[14.01.2025 03:10] Downloading and parsing paper https://huggingface.co/papers/2501.05510.
[14.01.2025 03:10] Extra JSON file exists (./assets/json/2501.05510.json), skip PDF parsing.
[14.01.2025 03:10] Paper image links file exists (./assets/img_data/2501.05510.json), skip HTML parsing.
[14.01.2025 03:10] Success.
[14.01.2025 03:10] Downloading and parsing paper https://huggingface.co/papers/2501.05727.
[14.01.2025 03:10] Extra JSON file exists (./assets/json/2501.05727.json), skip PDF parsing.
[14.01.2025 03:10] Paper image links file exists (./assets/img_data/2501.05727.json), skip HTML parsing.
[14.01.2025 03:10] Success.
[14.01.2025 03:10] Downloading and parsing paper https://huggingface.co/papers/2501.05767.
[14.01.2025 03:10] Extra JSON file exists (./assets/json/2501.05767.json), skip PDF parsing.
[14.01.2025 03:10] Paper image links file exists (./assets/img_data/2501.05767.json), skip HTML parsing.
[14.01.2025 03:10] Success.
[14.01.2025 03:10] Downloading and parsing paper https://huggingface.co/papers/2501.05452.
[14.01.2025 03:10] Extra JSON file exists (./assets/json/2501.05452.json), skip PDF parsing.
[14.01.2025 03:10] Paper image links file exists (./assets/img_data/2501.05452.json), skip HTML parsing.
[14.01.2025 03:10] Success.
[14.01.2025 03:10] Downloading and parsing paper https://huggingface.co/papers/2501.04698.
[14.01.2025 03:10] Extra JSON file exists (./assets/json/2501.04698.json), skip PDF parsing.
[14.01.2025 03:10] Paper image links file exists (./assets/img_data/2501.04698.json), skip HTML parsing.
[14.01.2025 03:10] Success.
[14.01.2025 03:10] Downloading and parsing paper https://huggingface.co/papers/2501.05542.
[14.01.2025 03:10] Extra JSON file exists (./assets/json/2501.05542.json), skip PDF parsing.
[14.01.2025 03:10] Paper image links file exists (./assets/img_data/2501.05542.json), skip HTML parsing.
[14.01.2025 03:10] Success.
[14.01.2025 03:10] Downloading and parsing paper https://huggingface.co/papers/2501.05707.
[14.01.2025 03:10] Extra JSON file exists (./assets/json/2501.05707.json), skip PDF parsing.
[14.01.2025 03:10] Paper image links file exists (./assets/img_data/2501.05707.json), skip HTML parsing.
[14.01.2025 03:10] Success.
[14.01.2025 03:10] Downloading and parsing paper https://huggingface.co/papers/2501.04961.
[14.01.2025 03:10] Extra JSON file exists (./assets/json/2501.04961.json), skip PDF parsing.
[14.01.2025 03:10] Paper image links file exists (./assets/img_data/2501.04961.json), skip HTML parsing.
[14.01.2025 03:10] Success.
[14.01.2025 03:10] Downloading and parsing paper https://huggingface.co/papers/2501.06187.
[14.01.2025 03:10] Extra JSON file exists (./assets/json/2501.06187.json), skip PDF parsing.
[14.01.2025 03:10] Paper image links file exists (./assets/img_data/2501.06187.json), skip HTML parsing.
[14.01.2025 03:10] Success.
[14.01.2025 03:10] Downloading and parsing paper https://huggingface.co/papers/2501.06250.
[14.01.2025 03:10] Extra JSON file exists (./assets/json/2501.06250.json), skip PDF parsing.
[14.01.2025 03:10] Paper image links file exists (./assets/img_data/2501.06250.json), skip HTML parsing.
[14.01.2025 03:10] Success.
[14.01.2025 03:10] Enriching papers with extra data.
[14.01.2025 03:10] ********************************************************************************
[14.01.2025 03:10] Abstract 0. Retrieval-Augmented Generation (RAG) is a powerful strategy to address the issue of generating factually incorrect outputs in foundation models by retrieving external knowledge relevant to queries and incorporating it into their generation process. However, existing RAG approaches have primarily foc...
[14.01.2025 03:10] ********************************************************************************
[14.01.2025 03:10] Abstract 1. The development of general robotic systems capable of manipulating in unstructured environments is a significant challenge. While Vision-Language Models(VLM) excel in high-level commonsense reasoning, they lack the fine-grained 3D spatial understanding required for precise manipulation tasks. Fine-t...
[14.01.2025 03:10] ********************************************************************************
[14.01.2025 03:10] Abstract 2. Reasoning is a fundamental capability for solving complex multi-step problems, particularly in visual contexts where sequential step-wise understanding is essential. Existing approaches lack a comprehensive framework for evaluating visual reasoning and do not emphasize step-wise problem-solving. To ...
[14.01.2025 03:10] ********************************************************************************
[14.01.2025 03:10] Abstract 3. Temporal Awareness, the ability to reason dynamically based on the timestamp when a question is raised, is the key distinction between offline and online video LLMs. Unlike offline models, which rely on complete videos for static, post hoc analysis, online models process video streams incrementally ...
[14.01.2025 03:10] ********************************************************************************
[14.01.2025 03:10] Abstract 4. Despite their remarkable performance, the development of Large Language Models (LLMs) faces a critical challenge in scalable oversight: providing effective feedback for tasks where human evaluation is difficult or where LLMs outperform humans. While there is growing interest in using LLMs for critiq...
[14.01.2025 03:10] ********************************************************************************
[14.01.2025 03:10] Abstract 5. The recent advancement of Multimodal Large Language Models (MLLMs) has significantly improved their fine-grained perception of single images and general comprehension across multiple images. However, existing MLLMs still face challenges in achieving precise grounding in complex multi-image scenarios...
[14.01.2025 03:10] ********************************************************************************
[14.01.2025 03:10] Abstract 6. Structured image understanding, such as interpreting tables and charts, requires strategically refocusing across various structures and texts within an image, forming a reasoning sequence to arrive at the final answer. However, current multimodal large language models (LLMs) lack this multihop selec...
[14.01.2025 03:10] ********************************************************************************
[14.01.2025 03:10] Abstract 7. Text-to-video generation has made remarkable advancements through diffusion models. However, Multi-Concept Video Customization (MCVC) remains a significant challenge. We identify two key challenges in this task: 1) the identity decoupling problem, where directly adopting existing customization metho...
[14.01.2025 03:10] ********************************************************************************
[14.01.2025 03:10] Abstract 8. This study demonstrates a novel approach to testing the security boundaries of Vision-Large Language Model (VLM/ LLM) using the EICAR test file embedded within JPEG images. We successfully executed four distinct protocols across multiple LLM platforms, including OpenAI GPT-4o, Microsoft Copilot, Goo...
[14.01.2025 03:10] ********************************************************************************
[14.01.2025 03:10] Abstract 9. Large language models (LLMs) have achieved remarkable performance in recent years but are fundamentally limited by the underlying training data. To improve models beyond the training data, recent works have explored how LLMs can be used to generate synthetic data for autonomous self-improvement. How...
[14.01.2025 03:10] ********************************************************************************
[14.01.2025 03:10] Abstract 10. Domain-adaptive post-training of large language models (LLMs) has emerged as a promising approach for specialized domains such as medicine and finance. However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configuratio...
[14.01.2025 03:10] ********************************************************************************
[14.01.2025 03:10] Abstract 11. Video personalization methods allow us to synthesize videos with specific concepts such as people, pets, and places. However, existing methods often focus on limited domains, require time-consuming optimization per subject, or support only a single subject. We present Video Alchemist - a video model...
[14.01.2025 03:10] ********************************************************************************
[14.01.2025 03:10] Abstract 12. Traditional Celluloid (Cel) Animation production pipeline encompasses multiple essential steps, including storyboarding, layout design, keyframe animation, inbetweening, and colorization, which demand substantial manual effort, technical expertise, and significant time investment. These challenges h...
[14.01.2025 03:10] Read previous papers.
[14.01.2025 03:10] Generating reviews via LLM API.
[14.01.2025 03:10] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#interpretability", "#hallucinations", "#video"], "emoji": "üé•", "ru": {"title": "VideoRAG: –û–±–æ–≥–∞—â–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "VideoRAG - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç
[14.01.2025 03:10] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#robotics", "#3d", "#transfer_learning", "#agi"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–∫—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º VLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
[14.01.2025 03:10] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#training", "#multimodal", "#open_source", "#reasoning"], "emoji": "üß†", "ru": {"title": "–®–∞–≥ –∑–∞ —à–∞–≥–æ–º –∫ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–º—É –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[14.01.2025 03:10] Using data from previous issue: {"categories": ["#benchmark", "#survey", "#video", "#reasoning"], "emoji": "‚è±Ô∏è", "ru": {"title": "–í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç—å –∫–∞–∫ –∫–ª—é—á –∫ –æ–Ω–ª–∞–π–Ω-–∞–Ω–∞–ª–∏–∑—É –≤–∏–¥–µ–æ –¥–ª—è LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ OVO-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ-LLM –º–æ–¥–µ–ª–µ–π –∫ –æ–Ω–ª–∞–π–Ω-–∞–Ω–∞–ª–∏–∑—É –≤–∏–¥–µ–æ —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–Ω
[14.01.2025 03:10] Using data from previous issue: {"categories": ["#training", "#benchmark", "#optimization", "#rlhf", "#synthetic"], "emoji": "üî¨", "ru": {"title": "SCRIT: –°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É—é—â–∏–π—Å—è –∫—Ä–∏—Ç–∏–∫ –¥–ª—è LLM", "desc": "SCRIT - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫–µ –±–µ–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –Ω–∞–¥–∑–æ—Ä–∞. –û–Ω–∞ –∏—Å–ø–æ–ª
[14.01.2025 03:10] Using data from previous issue: {"categories": ["#training", "#dataset", "#multimodal", "#open_source", "#benchmark"], "emoji": "üîç", "ru": {"title": "Migician: —Ç–æ—á–Ω–∞—è –ø—Ä–∏–≤—è–∑–∫–∞ –∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Migician - –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∏–∑–æ–±—Ä–∞
[14.01.2025 03:10] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#dataset", "#reasoning", "#training", "#cv"], "emoji": "üîç", "ru": {"title": "ReFocus: –£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è LLM —á–µ—Ä–µ–∑ —É–ø—Ä–∞–≤–ª—è–µ–º–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ReFocus - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–¥–µ–ª—è–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª
[14.01.2025 03:10] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#data", "#video", "#dataset"], "emoji": "üé¨", "ru": {"title": "ConceptMaster: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ConceptMaster - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ü–µ–ø—Ç–∞–º–∏. 
[14.01.2025 03:10] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#data", "#security"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ù–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ VLM/LLM —Å –ø–æ–º–æ—â—å—é EICAR", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –≥—Ä–∞–Ω–∏—Ü –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Ç–∏–ø–∞ Vision-Large Language Model (VLM/LLM)
[14.01.2025 03:10] Using data from previous issue: {"categories": ["#synthetic", "#reasoning", "#training", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø
[14.01.2025 03:10] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#healthcare", "#transfer_learning", "#training"], "emoji": "üíπ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è LLM –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤: –æ—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–æ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FINDAP - —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–æ–º–µ–Ω–Ω–æ-–∞–¥–∞–ø—Ç–∏–≤–Ω–æ–º—É –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ
[14.01.2025 03:10] Using data from previous issue: {"categories": ["#diffusion", "#synthetic", "#benchmark", "#data", "#optimization", "#video", "#dataset"], "emoji": "üé≠", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –≤–∏–¥–µ–æ –±–µ–∑ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Video Alchemist - –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å
[14.01.2025 03:10] Using data from previous issue: {"categories": ["#ethics", "#survey", "#video", "#multimodal"], "emoji": "üé®", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–Ω–∏–º–∞—Ü–∏–∏: –∫–∞–∫ –ò–ò –º–µ–Ω—è–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Å–æ–∑–¥–∞–Ω–∏—è –º—É–ª—å—Ç—Ñ–∏–ª—å–º–æ–≤", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ (GenAI) –≤ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–π —Ü–µ–ª–ª—É–ª–æ–∏–¥–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏
[14.01.2025 03:10] Loading Chinese text from previous data.
[14.01.2025 03:10] Renaming data file.
[14.01.2025 03:10] Renaming previous data. hf_papers.json to ./d/2025-01-14.json
[14.01.2025 03:10] Saving new data file.
[14.01.2025 03:10] Generating page.
[14.01.2025 03:10] Renaming previous page.
[14.01.2025 03:10] Renaming previous data. index.html to ./d/2025-01-14.html
[14.01.2025 03:10] [Experimental] Generating Chinese page for reading.
[14.01.2025 03:10] Chinese vocab [{'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}, {'word': 'ÈîôËØØ', 'pinyin': 'cu√≤w√π', 'trans': 'error'}, {'word': '‰ø°ÊÅØ', 'pinyin': 'x√¨nxƒ´', 'trans': 'information'}, {'word': 'Ê£ÄÁ¥¢', 'pinyin': 'ji«énsu«í', 'trans': 'retrieve'}, {'word': 'Êü•ËØ¢', 'pinyin': 'ch√°x√∫n', 'trans': 'query'}, {'word': 'Áõ∏ÂÖ≥', 'pinyin': 'xiƒÅngguƒÅn', 'trans': 'related'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ju√©', 'trans': 'visual'}, {'word': 'Âà©Áî®', 'pinyin': 'l√¨y√≤ng', 'trans': 'utilize'}, {'word': 'ËßÜÈ¢ë', 'pinyin': 'sh√¨p√≠n', 'trans': 'video'}, {'word': 'ÊñáÊú¨', 'pinyin': 'w√©nbƒõn', 'trans': 'text'}, {'word': 'ËæìÂá∫', 'pinyin': 'sh≈´ch≈´', 'trans': 'output'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´y√∫', 'trans': 'based on'}, {'word': 'Â§ßÂûã', 'pinyin': 'd√†x√≠ng', 'trans': 'large-scale'}, {'word': 'ËØ≠Ë®Ä', 'pinyin': 'y«îy√°n', 'trans': 'language'}, {'word': 'Â§ÑÁêÜ', 'pinyin': 'ch«îl«ê', 'trans': 'process'}, {'word': 'Ë°®Á§∫', 'pinyin': 'bi«éosh√¨', 'trans': 'represent'}, {'word': 'ÂÜÖÂÆπ', 'pinyin': 'n√®ir√≥ng', 'trans': 'content'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'}, {'word': 'ËØÅÊòé', 'pinyin': 'zh√®ngm√≠ng', 'trans': 'prove'}, {'word': 'ÊúâÊïà', 'pinyin': 'y«íuxi√†o', 'trans': 'effective'}]
[14.01.2025 03:10] Renaming previous Chinese page.
[14.01.2025 03:10] Renaming previous data. zh.html to ./d/2025-01-13_zh_reading_task.html
[14.01.2025 03:10] Writing Chinese reading task.
[14.01.2025 03:10] Writing result.
[14.01.2025 03:10] Renaming log file.
[14.01.2025 03:10] Renaming previous data. log.txt to ./logs/2025-01-14_last_log.txt
