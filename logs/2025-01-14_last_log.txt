[14.01.2025 08:13] Read previous papers.
[14.01.2025 08:13] Generating top page (month).
[14.01.2025 08:13] Writing top page (month).
[14.01.2025 09:10] Read previous papers.
[14.01.2025 09:10] Get feed.
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.07301
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06425
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06173
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06252
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.07572
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06458
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06282
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.06590
[14.01.2025 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.07574
[14.01.2025 09:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.01.2025 09:10] No deleted papers detected.
[14.01.2025 09:10] Downloading and parsing papers (pdf, html). Total: 9.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.07301.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.07301.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.07301.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.06425.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.06425.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.06425.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.06173.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.06173.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.06173.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.06252.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.06252.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.06252.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.07572.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.07572.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.07572.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.06458.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.06458.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.06458.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.06282.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.06282.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.06282.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.06590.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.06590.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.06590.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Downloading and parsing paper https://huggingface.co/papers/2501.07574.
[14.01.2025 09:10] Extra JSON file exists (./assets/json/2501.07574.json), skip PDF parsing.
[14.01.2025 09:10] Paper image links file exists (./assets/img_data/2501.07574.json), skip HTML parsing.
[14.01.2025 09:10] Success.
[14.01.2025 09:10] Enriching papers with extra data.
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 0. Process Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), which aim to identify and mitigate intermediate errors in the reasoning processes. However, the development of effective PRMs faces significant challenges, p...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 1. Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent q...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 2. Recent video generation models have shown promising results in producing high-quality video clips lasting several seconds. However, these models face challenges in generating long sequences that convey clear and informative events, limiting their ability to support coherent narrations. In this paper...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 3. Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce \implname, a novel self-adaptation framework that adapts LLMs for unseen tasks ...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 4. Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address it, we introduce WebWalkerQ...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 5. Building upon our previous investigations of O1 replication (Part 1: Journey Learning [Qin et al., 2024] and Part 2: Distillation [Huang et al., 2024]), this work explores the potential of inference-time scaling in large language models (LLMs) for medical reasoning tasks, ranging from diagnostic dec...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 6. Recent advancements in large language models (LLMs) and multimodal speech-text models have laid the groundwork for seamless voice interactions, enabling real-time, natural, and human-like conversations. Previous models for voice interactions are categorized as native and aligned. Native models integ...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 7. Chemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and in...
[14.01.2025 09:10] ********************************************************************************
[14.01.2025 09:10] Abstract 8. We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for 3D deep learning and 3D generative AI. uCO3D is the largest publicly-available collection of high-resolution videos of objects with 3D annotations that ensures full-360^{circ} coverage. uCO3D is significantly more diverse ...
[14.01.2025 09:10] Read previous papers.
[14.01.2025 09:10] Generating reviews via LLM API.
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#math", "#data", "#reasoning", "#benchmark", "#optimization", "#open_source", "#training"], "emoji": "ğŸ§®", "ru": {"title": "Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Process Reward Models Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Process Reward Models (PRM) Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#benchmark", "#long_context", "#optimization", "#inference", "#architecture"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ - Tensor Product Attention (TPA), Ğ¸ÑĞ¿Ğ¾Ğ»
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#video", "#story_generation", "#dataset", "#long_context", "#training", "#multimodal", "#alignment"], "emoji": "ğŸ³", "ru": {"title": "Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ğ¼ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ² Ğ¾ Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¸Ñ‰Ğ¸ Ğ´
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#multimodal", "#agi", "#rl", "#optimization", "#training", "#architecture"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#rag", "#reasoning", "#benchmark", "#agi", "#optimization", "#games", "#interpretability", "#agents", "#survey"], "emoji": "ğŸ•¸ï¸", "ru": {"title": "WebWalker: ÑƒĞ¼Ğ½Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#science", "#inference", "#healthcare", "#reasoning"], "emoji": "ğŸ©º", "ru": {"title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LLM ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ", "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#training"], "emoji": "ğŸ—£ï¸", "ru": {"title": "MinMo: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ¼ Ğ˜Ğ˜-Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MinMo - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ±ĞµÑĞ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 1,4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‡Ğ°ÑĞ¾Ğ²
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#science", "#reasoning", "#multimodal", "#agents", "#dataset"], "emoji": "ğŸ§ª", "ru": {"title": "ChemAgent: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ LLM Ğ² Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…", "desc": "ChemAgent - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿
[14.01.2025 09:10] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#synthetic", "#3d"], "emoji": "ğŸ”", "ru": {"title": "uCO3D: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸", "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… uCO3D Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ² 3D. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸
[14.01.2025 09:10] Trying to get texts in Chinese.
[14.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Process Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), which aim to identify and mitigate intermediate errors in the reasoning processes. However, the development of effective PRMs faces significant challenges, particularly in data annotation and evaluation methodologies. In this paper, through extensive experiments, we demonstrate that commonly used Monte Carlo (MC) estimation-based data synthesis for PRMs typically yields inferior performance and generalization compared to LLM-as-a-judge and human annotation methods. MC estimation relies on completion models to evaluate current-step correctness, leading to inaccurate step verification. Furthermore, we identify potential biases in conventional Best-of-N (BoN) evaluation strategies for PRMs: (1) The unreliable policy models generate responses with correct answers but flawed processes, leading to a misalignment between the evaluation criteria of BoN and the PRM objectives of process verification. (2) The tolerance of PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a significant proportion of minimum scores concentrated on the final answer steps, revealing the shift from process to outcome-based assessment in BoN Optimized PRMs. To address these challenges, we develop a consensus filtering mechanism that effectively integrates MC estimation with LLM-as-a-judge and advocates a more comprehensive evaluation framework that combines response-level and step-level metrics. Based on the mechanisms, we significantly improve both model performance and data efficiency in the BoN evaluation and the step-wise error identification task. Finally, we release a new state-of-the-art PRM that outperforms existing open-source alternatives and provides practical guidelines for future research in building process supervision models.
[14.01.2025 09:10] Mistral response. {"id": "37b4761259d0491aa6079b79be146fb7", "object": "chat.completion", "created": 1736845843, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u53eb\u505a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8fc7\u7a0b\u76d1\u7763\u3002\u76ee\u6807\u662f\u8bc6\u522b\u548c\u51cf\u5c11\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5e38\u7528\u7684\u8499\u7279\u5361\u7f57\uff08MC\uff09\u4f30\u8ba1\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4f9d\u8d56\u5b8c\u6210\u6a21\u578b\u8bc4\u4f30\u5f53\u524d\u6b65\u9aa4\u7684\u6b63\u786e\u6027\uff0c\u5bfc\u81f4\u6b65\u9aa4\u9a8c\u8bc1\u4e0d\u51c6\u786e\u3002\u6587\u7ae0\u8fd8\u6307\u51fa\u4e86\u4f20\u7edfBest-of-N\uff08BoN\uff09\u8bc4\u4f30\u7b56\u7565\u7684\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5171\u8bc6\u8fc7\u6ee4\u673a\u5236\uff0c\u7ed3\u5408MC\u4f30\u8ba1\u548cLLM-as-a-judge\uff0c\u6539\u8fdb\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002\u6700\u540e\uff0c\u6587\u7ae0\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u6700\u5148\u8fdb\u7684PRM\uff0c\u5e76\u63d0\u4f9b\u4e86\u672a\u6765\u7814\u7a76\u7684\u5b9e\u7528\u6307\u5357\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 405, "total_tokens": 642, "completion_tokens": 237}}
[14.01.2025 09:10] Response: è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå«åšè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¿‡ç¨‹ç›‘ç£ã€‚ç›®æ ‡æ˜¯è¯†åˆ«å’Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¸¸ç”¨çš„è’™ç‰¹å¡ç½—ï¼ˆMCï¼‰ä¼°è®¡æ–¹æ³•æ•ˆæœä¸ä½³ï¼Œå› ä¸ºå®ƒä¾èµ–å®Œæˆæ¨¡å‹è¯„ä¼°å½“å‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¯¼è‡´æ­¥éª¤éªŒè¯ä¸å‡†ç¡®ã€‚æ–‡ç« è¿˜æŒ‡å‡ºäº†ä¼ ç»ŸBest-of-Nï¼ˆBoNï¼‰è¯„ä¼°ç­–ç•¥çš„åå·®ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…±è¯†è¿‡æ»¤æœºåˆ¶ï¼Œç»“åˆMCä¼°è®¡å’ŒLLM-as-a-judgeï¼Œæ”¹è¿›äº†æ¨¡å‹æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚æœ€åï¼Œæ–‡ç« å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„PRMï¼Œå¹¶æä¾›äº†æœªæ¥ç ”ç©¶çš„å®ç”¨æŒ‡å—ã€‚
[14.01.2025 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå«åšè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¿‡ç¨‹ç›‘ç£ã€‚ç›®æ ‡æ˜¯è¯†åˆ«å’Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¸¸ç”¨çš„è’™ç‰¹å¡ç½—ï¼ˆMCï¼‰ä¼°è®¡æ–¹æ³•æ•ˆæœä¸ä½³ï¼Œå› ä¸ºå®ƒä¾èµ–å®Œæˆæ¨¡å‹è¯„ä¼°å½“å‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¯¼è‡´æ­¥éª¤éªŒè¯ä¸å‡†ç¡®ã€‚æ–‡ç« è¿˜æŒ‡å‡ºäº†ä¼ ç»ŸBest-of-Nï¼ˆBoNï¼‰è¯„ä¼°ç­–ç•¥çš„åå·®ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…±è¯†è¿‡æ»¤æœºåˆ¶ï¼Œç»“åˆMCä¼°è®¡å’ŒLLM-as-a-judgeï¼Œæ”¹è¿›äº†æ¨¡å‹æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚æœ€åï¼Œæ–‡ç« å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„PRMï¼Œå¹¶æä¾›äº†æœªæ¥ç ”ç©¶çš„å®ç”¨æŒ‡å—ã€‚
[14.01.2025 09:11] Mistral response. {"id": "a3f4ce8b79ce46cc9438dd060912250c", "object": "chat.completion", "created": 1736845848, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u53eb\u505a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8fc7\u7a0b\u76d1\u7763\u3002\u76ee\u6807\u662f\u8bc6\u522b\u548c\u51cf\u5c11\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5e38\u7528\u7684\u8499\u7279\u5361\u7f57\uff08MC\uff09\u4f30\u8ba1\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4f9d\u8d56\u5b8c\u6210\u6a21\u578b\u8bc4\u4f30\u5f53\u524d\u6b65\u9aa4\u7684\u6b63\u786e\u6027\uff0c\u5bfc\u81f4\u6b65\u9aa4\u9a8c\u8bc1\u4e0d\u51c6\u786e\u3002\u6587\u7ae0\u8fd8\u6307\u51fa\u4e86\u4f20\u7edfBest-of-N\uff08BoN\uff09\u8bc4\u4f30\u7b56\u7565\u7684\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5171\u8bc6\u8fc7\u6ee4\u673a\u5236\uff0c\u7ed3\u5408MC\u4f30\u8ba1\u548cLLM-as-a-judge\uff0c\u6539\u8fdb\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002\u6700\u540e\uff0c\u6587\u7ae0\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u6700\u5148\u8fdb\u7684PRM\uff0c\u5e76\u63d0\u4f9b\u4e86\u672a\u6765\u7814\u7a76\u7684\u5b9e\u7528\u6307\u5357\u3002\n\nzh\u00e8 pi\u0101n w\u00e9nzh\u0101ng ji\u00e8sh\u00e0o le y\u012b zh\u01d2ng x\u012bn de f\u0101ngf\u01ce, ji\u00e0ozu\u00f2 gu\u00f2ch\u00e9ng ji\u01cengl\u00ec m\u00f3x\u00edng (PRMs), y\u00f2ngy\u00fa d\u00e0x\u00edng y\u01d4y\u00e1n m\u00f3x\u00edng (LLMs) z\u00e0i sh\u00f9xu\u00e9 tu\u012bl\u01d0 zh\u014dng de gu\u00f2ch\u00e9ng ji\u00e0nd\u016b. M\u00f9bi\u0101o sh\u00ec sh\u00edbi\u00e9 h\u00e9 ji\u01censh\u01ceo tu\u012bl\u01d0 gu\u00f2ch\u00e9ng zh\u014dng de cu\u00f2w\u00f9. Y\u00e1nji\u016b f\u0101xi\u00e0n, ch\u00e1ngy\u00f2ng de m\u00e9ngt\u00e8k\u01celu\u00f3 (MC) g\u016bj\u00ec f\u0101ngf\u01ce xi\u00e0oji\u00e0, y\u012bnw\u00e8i t\u0101 y\u012bl\u00e0i w\u00e1nch\u00e9ng m\u00f3x\u00edng p\u00edngji\u00e0 d\u0101ngqi\u00e1n b\u00f9zh\u00f2u de zh\u00e8ngqu\u00e8x\u00ecng, d\u01ceozh\u00ec b\u00f9zh\u00f2u y\u00e0nzh\u00e8ng b\u00f9 zh\u01d4nqu\u00e8. W\u00e9nzh\u0101ng h\u00e1i zh\u01d0ch\u016b le chu\u00e1nt\u01d2ng Best-of-N (BoN) p\u00edngji\u00e0 c\u00e8l\u00fc\u00e8 de pi\u0101nch\u0101, b\u00ecng t\u00edch\u016b le y\u012b zh\u01d2ng g\u00f2ngsh\u00ec gu\u00f2l\u01dc j\u012bzh\u00ec, ji\u00e9h\u00e9 MC g\u016bj\u00ec h\u00e9 LLM-as-a-judge, g\u01ceij\u00ecn le m\u00f3x\u00edng x\u00ecngn\u00e9ng h\u00e9 sh\u00f9j\u00f9 xi\u00e0oy\u00f2ng. Zu\u00ech\u00f2u, w\u00e9nzh\u0101ng f\u0101b\u00f9 le y\u012bg\u00e8 x\u012bn de zu\u00ec xi\u0101nj\u00ecn de PRM, b\u00ecng t\u00edg\u014dng le w\u00e8il\u00e1i y\u00e1nji\u016b de sh\u00edy\u00f2ng zh\u01d0n\u00e1n."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 252, "total_tokens": 940, "completion_tokens": 688}}
[14.01.2025 09:11] Response: è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå«åšè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¿‡ç¨‹ç›‘ç£ã€‚ç›®æ ‡æ˜¯è¯†åˆ«å’Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¸¸ç”¨çš„è’™ç‰¹å¡ç½—ï¼ˆMCï¼‰ä¼°è®¡æ–¹æ³•æ•ˆæœä¸ä½³ï¼Œå› ä¸ºå®ƒä¾èµ–å®Œæˆæ¨¡å‹è¯„ä¼°å½“å‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¯¼è‡´æ­¥éª¤éªŒè¯ä¸å‡†ç¡®ã€‚æ–‡ç« è¿˜æŒ‡å‡ºäº†ä¼ ç»ŸBest-of-Nï¼ˆBoNï¼‰è¯„ä¼°ç­–ç•¥çš„åå·®ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…±è¯†è¿‡æ»¤æœºåˆ¶ï¼Œç»“åˆMCä¼°è®¡å’ŒLLM-as-a-judgeï¼Œæ”¹è¿›äº†æ¨¡å‹æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚æœ€åï¼Œæ–‡ç« å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„PRMï¼Œå¹¶æä¾›äº†æœªæ¥ç ”ç©¶çš„å®ç”¨æŒ‡å—ã€‚

zhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng xÄ«n de fÄngfÇ, jiÃ ozuÃ² guÃ²chÃ©ng jiÇnglÃ¬ mÃ³xÃ­ng (PRMs), yÃ²ngyÃº dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i shÃ¹xuÃ© tuÄ«lÇ zhÅng de guÃ²chÃ©ng jiÃ ndÅ«. MÃ¹biÄo shÃ¬ shÃ­biÃ© hÃ© jiÇnshÇo tuÄ«lÇ guÃ²chÃ©ng zhÅng de cuÃ²wÃ¹. YÃ¡njiÅ« fÄxiÃ n, chÃ¡ngyÃ²ng de mÃ©ngtÃ¨kÇluÃ³ (MC) gÅ«jÃ¬ fÄngfÇ xiÃ ojiÃ , yÄ«nwÃ¨i tÄ yÄ«lÃ i wÃ¡nchÃ©ng mÃ³xÃ­ng pÃ­ngjiÃ  dÄngqiÃ¡n bÃ¹zhÃ²u de zhÃ¨ngquÃ¨xÃ¬ng, dÇozhÃ¬ bÃ¹zhÃ²u yÃ nzhÃ¨ng bÃ¹ zhÇ”nquÃ¨. WÃ©nzhÄng hÃ¡i zhÇchÅ« le chuÃ¡ntÇ’ng Best-of-N (BoN) pÃ­ngjiÃ  cÃ¨lÃ¼Ã¨ de piÄnchÄ, bÃ¬ng tÃ­chÅ« le yÄ« zhÇ’ng gÃ²ngshÃ¬ guÃ²lÇœ jÄ«zhÃ¬, jiÃ©hÃ© MC gÅ«jÃ¬ hÃ© LLM-as-a-judge, gÇijÃ¬n le mÃ³xÃ­ng xÃ¬ngnÃ©ng hÃ© shÃ¹jÃ¹ xiÃ oyÃ²ng. ZuÃ¬hÃ²u, wÃ©nzhÄng fÄbÃ¹ le yÄ«gÃ¨ xÄ«n de zuÃ¬ xiÄnjÃ¬n de PRM, bÃ¬ng tÃ­gÅng le wÃ¨ilÃ¡i yÃ¡njiÅ« de shÃ­yÃ²ng zhÇnÃ¡n.
[14.01.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå«åšè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¿‡ç¨‹ç›‘ç£ã€‚ç›®æ ‡æ˜¯è¯†åˆ«å’Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¸¸ç”¨çš„è’™ç‰¹å¡ç½—ï¼ˆMCï¼‰ä¼°è®¡æ–¹æ³•æ•ˆæœä¸ä½³ï¼Œå› ä¸ºå®ƒä¾èµ–å®Œæˆæ¨¡å‹è¯„ä¼°å½“å‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¯¼è‡´æ­¥éª¤éªŒè¯ä¸å‡†ç¡®ã€‚æ–‡ç« è¿˜æŒ‡å‡ºäº†ä¼ ç»ŸBest-of-Nï¼ˆBoNï¼‰è¯„ä¼°ç­–ç•¥çš„åå·®ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…±è¯†è¿‡æ»¤æœºåˆ¶ï¼Œç»“åˆMCä¼°è®¡å’ŒLLM-as-a-judgeï¼Œæ”¹è¿›äº†æ¨¡å‹æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚æœ€åï¼Œæ–‡ç« å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„PRMï¼Œå¹¶æä¾›äº†æœªæ¥ç ”ç©¶çš„å®ç”¨æŒ‡å—ã€‚
[14.01.2025 09:11] Mistral response. {"id": "b8d627f36e6e49b2bc8f05fa588d1e9a", "object": "chat.completion", "created": 1736845862, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\", \"pinyin\": \"gu\u00f2ch\u00e9ng ji\u01cengl\u00ec m\u00f3x\u00edng\", \"trans\": \"Process Reward Model\"},\n    {\"word\": \"\u5927\u578b\u8bed\u8a00\u6a21\u578b\", \"pinyin\": \"d\u00e0x\u00edng y\u01d4y\u00e1n m\u00f3x\u00edng\", \"trans\": \"Large Language Model\"},\n    {\"word\": \"\u6570\u5b66\u63a8\u7406\", \"pinyin\": \"sh\u00f9xu\u00e9 tu\u012bl\u01d0\", \"trans\": \"Mathematical Reasoning\"},\n    {\"word\": \"\u8fc7\u7a0b\u76d1\u7763\", \"pinyin\": \"gu\u00f2ch\u00e9ng ji\u00e0nd\u016b\", \"trans\": \"Process Supervision\"},\n    {\"word\": \"\u8499\u7279\u5361\u7f57\", \"pinyin\": \"m\u00e9ngt\u00e8k\u01celu\u00f3\", \"trans\": \"Monte Carlo\"},\n    {\"word\": \"\u4f30\u8ba1\", \"pinyin\": \"g\u016bj\u00ec\", \"trans\": \"Estimation\"},\n    {\"word\": \"\u4f9d\u8d56\", \"pinyin\": \"y\u012bl\u00e0i\", \"trans\": \"Depend\"},\n    {\"word\": \"\u8bc4\u4f30\", \"pinyin\": \"p\u00edngg\u016b\", \"trans\": \"Evaluate\"},\n    {\"word\": \"\u6b65\u9aa4\u9a8c\u8bc1\", \"pinyin\": \"b\u00f9zh\u00f2u y\u00e0nzh\u00e8ng\", \"trans\": \"Step Verification\"},\n    {\"word\": \"\u504f\u5dee\", \"pinyin\": \"pi\u0101nch\u0101\", \"trans\": \"Bias\"},\n    {\"word\": \"\u5171\u8bc6\u8fc7\u6ee4\u673a\u5236\", \"pinyin\": \"g\u00f2ngsh\u00ed gu\u00f2l\u01dc j\u012bzh\u00ec\", \"trans\": \"Consensus Filtering Mechanism\"},\n    {\"word\": \"LLM-as-a-judge\", \"pinyin\": \"LLM-as-a-judge\", \"trans\": \"LLM-as-a-judge\"},\n    {\"word\": \"\u6700\u5148\u8fdb\", \"pinyin\": \"zu\u00ecxi\u0101nj\u00ecn\", \"trans\": \"State-of-the-art\"},\n    {\"word\": \"\u5b9e\u7528\u6307\u5357\", \"pinyin\": \"sh\u00edy\u00f2ng zh\u01d0n\u00e1n\", \"trans\": \"Practical Guide\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 284, "total_tokens": 798, "completion_tokens": 514}}
[14.01.2025 09:11] Response: [
    {"word": "è¿‡ç¨‹å¥–åŠ±æ¨¡å‹", "pinyin": "guÃ²chÃ©ng jiÇnglÃ¬ mÃ³xÃ­ng", "trans": "Process Reward Model"},
    {"word": "å¤§å‹è¯­è¨€æ¨¡å‹", "pinyin": "dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng", "trans": "Large Language Model"},
    {"word": "æ•°å­¦æ¨ç†", "pinyin": "shÃ¹xuÃ© tuÄ«lÇ", "trans": "Mathematical Reasoning"},
    {"word": "è¿‡ç¨‹ç›‘ç£", "pinyin": "guÃ²chÃ©ng jiÃ ndÅ«", "trans": "Process Supervision"},
    {"word": "è’™ç‰¹å¡ç½—", "pinyin": "mÃ©ngtÃ¨kÇluÃ³", "trans": "Monte Carlo"},
    {"word": "ä¼°è®¡", "pinyin": "gÅ«jÃ¬", "trans": "Estimation"},
    {"word": "ä¾èµ–", "pinyin": "yÄ«lÃ i", "trans": "Depend"},
    {"word": "è¯„ä¼°", "pinyin": "pÃ­nggÅ«", "trans": "Evaluate"},
    {"word": "æ­¥éª¤éªŒè¯", "pinyin": "bÃ¹zhÃ²u yÃ nzhÃ¨ng", "trans": "Step Verification"},
    {"word": "åå·®", "pinyin": "piÄnchÄ", "trans": "Bias"},
    {"word": "å…±è¯†è¿‡æ»¤æœºåˆ¶", "pinyin": "gÃ²ngshÃ­ guÃ²lÇœ jÄ«zhÃ¬", "trans": "Consensus Filtering Mechanism"},
    {"word": "LLM-as-a-judge", "pinyin": "LLM-as-a-judge", "trans": "LLM-as-a-judge"},
    {"word": "æœ€å…ˆè¿›", "pinyin": "zuÃ¬xiÄnjÃ¬n", "trans": "State-of-the-art"},
    {"word": "å®ç”¨æŒ‡å—", "pinyin": "shÃ­yÃ²ng zhÇnÃ¡n", "trans": "Practical Guide"}
]
[14.01.2025 09:11] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå«åšè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¿‡ç¨‹ç›‘ç£ã€‚ç›®æ ‡æ˜¯è¯†åˆ«å’Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¸¸ç”¨çš„è’™ç‰¹å¡ç½—ï¼ˆMCï¼‰ä¼°è®¡æ–¹æ³•æ•ˆæœä¸ä½³ï¼Œå› ä¸ºå®ƒä¾èµ–å®Œæˆæ¨¡å‹è¯„ä¼°å½“å‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¯¼è‡´æ­¥éª¤éªŒè¯ä¸å‡†ç¡®ã€‚æ–‡ç« è¿˜æŒ‡å‡ºäº†ä¼ ç»ŸBest-of-Nï¼ˆBoNï¼‰è¯„ä¼°ç­–ç•¥çš„åå·®ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…±è¯†è¿‡æ»¤æœºåˆ¶ï¼Œç»“åˆMCä¼°è®¡å’ŒLLM-as-a-judgeï¼Œæ”¹è¿›äº†æ¨¡å‹æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚æœ€åï¼Œæ–‡ç« å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„PRMï¼Œå¹¶æä¾›äº†æœªæ¥ç ”ç©¶çš„å®ç”¨æŒ‡å—ã€‚
[14.01.2025 09:11] Mistral response. {"id": "b3d272c7ba2d4378b68a4a448c7006c6", "object": "chat.completion", "created": 1736845873, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article introduces a new method called Process Reward Models (PRMs) for process supervision of large language models (LLMs) in mathematical reasoning. The goal is to identify and reduce errors in the reasoning process. The research found that the commonly used Monte Carlo (MC) estimation method performs poorly because it relies on the completion model to evaluate the correctness of the current step, leading to inaccurate step verification. The article also points out the bias in traditional Best-of-N (BoN) evaluation strategies and proposes a consensus filtering mechanism that combines MC estimation and LLM-as-a-judge to improve model performance and data efficiency. Finally, the article releases a new state-of-the-art PRM and provides practical guidelines for future research."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 250, "total_tokens": 413, "completion_tokens": 163}}
[14.01.2025 09:11] Response: This article introduces a new method called Process Reward Models (PRMs) for process supervision of large language models (LLMs) in mathematical reasoning. The goal is to identify and reduce errors in the reasoning process. The research found that the commonly used Monte Carlo (MC) estimation method performs poorly because it relies on the completion model to evaluate the correctness of the current step, leading to inaccurate step verification. The article also points out the bias in traditional Best-of-N (BoN) evaluation strategies and proposes a consensus filtering mechanism that combines MC estimation and LLM-as-a-judge to improve model performance and data efficiency. Finally, the article releases a new state-of-the-art PRM and provides practical guidelines for future research.
[14.01.2025 09:11] Renaming data file.
[14.01.2025 09:11] Renaming previous data. hf_papers.json to ./d/2025-01-14.json
[14.01.2025 09:11] Saving new data file.
[14.01.2025 09:11] Generating page.
[14.01.2025 09:11] Renaming previous page.
[14.01.2025 09:11] Renaming previous data. index.html to ./d/2025-01-14.html
[14.01.2025 09:11] [Experimental] Generating Chinese page for reading.
[14.01.2025 09:11] Chinese vocab [{'word': 'è¿‡ç¨‹å¥–åŠ±æ¨¡å‹', 'pinyin': 'guÃ²chÃ©ng jiÇnglÃ¬ mÃ³xÃ­ng', 'trans': 'Process Reward Model'}, {'word': 'å¤§å‹è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng', 'trans': 'Large Language Model'}, {'word': 'æ•°å­¦æ¨ç†', 'pinyin': 'shÃ¹xuÃ© tuÄ«lÇ', 'trans': 'Mathematical Reasoning'}, {'word': 'è¿‡ç¨‹ç›‘ç£', 'pinyin': 'guÃ²chÃ©ng jiÃ ndÅ«', 'trans': 'Process Supervision'}, {'word': 'è’™ç‰¹å¡ç½—', 'pinyin': 'mÃ©ngtÃ¨kÇluÃ³', 'trans': 'Monte Carlo'}, {'word': 'ä¼°è®¡', 'pinyin': 'gÅ«jÃ¬', 'trans': 'Estimation'}, {'word': 'ä¾èµ–', 'pinyin': 'yÄ«lÃ i', 'trans': 'Depend'}, {'word': 'è¯„ä¼°', 'pinyin': 'pÃ­nggÅ«', 'trans': 'Evaluate'}, {'word': 'æ­¥éª¤éªŒè¯', 'pinyin': 'bÃ¹zhÃ²u yÃ nzhÃ¨ng', 'trans': 'Step Verification'}, {'word': 'åå·®', 'pinyin': 'piÄnchÄ', 'trans': 'Bias'}, {'word': 'å…±è¯†è¿‡æ»¤æœºåˆ¶', 'pinyin': 'gÃ²ngshÃ­ guÃ²lÇœ jÄ«zhÃ¬', 'trans': 'Consensus Filtering Mechanism'}, {'word': 'LLM-as-a-judge', 'pinyin': 'LLM-as-a-judge', 'trans': 'LLM-as-a-judge'}, {'word': 'æœ€å…ˆè¿›', 'pinyin': 'zuÃ¬xiÄnjÃ¬n', 'trans': 'State-of-the-art'}, {'word': 'å®ç”¨æŒ‡å—', 'pinyin': 'shÃ­yÃ²ng zhÇnÃ¡n', 'trans': 'Practical Guide'}]
[14.01.2025 09:11] Renaming previous Chinese page.
[14.01.2025 09:11] Renaming previous data. zh.html to ./d/2025-01-13_zh_reading_task.html
[14.01.2025 09:11] Writing Chinese reading task.
[14.01.2025 09:11] Writing result.
[14.01.2025 09:11] Renaming log file.
[14.01.2025 09:11] Renaming previous data. log.txt to ./logs/2025-01-14_last_log.txt
