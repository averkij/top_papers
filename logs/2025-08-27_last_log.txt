[27.08.2025 05:12] Read previous papers.
[27.08.2025 05:12] Generating top page (month).
[27.08.2025 05:12] Writing top page (month).
[27.08.2025 06:17] Read previous papers.
[27.08.2025 06:17] Get feed.
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19205
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18124
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17661
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18756
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19209
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17445
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19247
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17437
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15774
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19242
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18773
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18621
[27.08.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2508.15804
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18370
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.16697
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19188
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19026
[27.08.2025 06:17] Extract page data from URL. URL: https://huggingface.co/papers/2508.18672
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18271
[27.08.2025 06:17] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18192
[27.08.2025 06:17] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.08.2025 06:17] No deleted papers detected.
[27.08.2025 06:17] Downloading and parsing papers (pdf, html). Total: 20.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.19205.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.19205.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.19205.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.18124.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.18124.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.18124.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.17661.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.17661.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.17661.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.18756.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.18756.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.18756.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.19209.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.19209.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.19209.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.17445.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.17445.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.17445.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.19247.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.19247.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.19247.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.17437.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.17437.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.17437.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.15774.
[27.08.2025 06:17] Downloading paper 2508.15774 from http://arxiv.org/pdf/2508.15774v1...
[27.08.2025 06:17] Extracting affiliations from text.
[27.08.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 CineScale: Free Lunch in High-Resolution Cinematic Visual Generation Haonan Qiu*, Ning Yu(cid:66), Ziqi Huang, Paul Debevec, Ziwei Liu(cid:66) 5 2 0 2 1 ] . [ 1 4 7 7 5 1 . 8 0 5 2 : r AbstractVisual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of highresolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained these methods are still prone to producing lowmodels. However, quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/. Index TermsDiffusion Models, Image Generation, Video Generation, High Resolution Diffusi"
[27.08.2025 06:17] Response: ```python
[]
```
[27.08.2025 06:17] Extracting affiliations from text.
[27.08.2025 06:17] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 CineScale: Free Lunch in High-Resolution Cinematic Visual Generation Haonan Qiu*, Ning Yu(cid:66), Ziqi Huang, Paul Debevec, Ziwei Liu(cid:66) 5 2 0 2 1 ] . [ 1 4 7 7 5 1 . 8 0 5 2 : r AbstractVisual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of highresolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained these methods are still prone to producing lowmodels. However, quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/. Index TermsDiffusion Models, Image Generation, Video Generation, High ResolutionDiffusion models have revolutionized visual generation [1] [6], empowering individuals without any artistic expertise to effortlessly create distinctive and personalized designs, graphics, and short films using specific textual descriptions. Nonetheless, current visual diffusion models are generally trained on data with limited resolution, such as 5122 for SD 1.5 [7], 10242 for SDXL [1], and 320 512 for VideoCrafter2 [4], hampering their ability to generate highfidelity images or videos at higher resolutions. Given the scarcity of high-resolution visual data and the substantially greater model capacity required for modeling such *This work was done during an internship at Netflix Eyeline Studios, (cid:66)corresponding authors H. Qiu, Z. Huang, and Z. Liu are with Nanyang Technological University. Email: {HAONAN002, ZIQI002}@e.ntu.edu.sg, ziwei.liu@ntu.edu.sg H. Qiu, N. Yu, and P. Debevec are with Netflix Eyeline Studios. Email: {ning.yu, debevec}@scanlinevfx.com data, recent efforts have focused on employing tuning-free strategies for high-resolution visual generation to inherit the strong generation capacities of existing pre-trained diffusion models. Despite the advances achieved by existing methods, they are still prone to producing low-quality images or videos, particularly manifesting as repetitive object occurrences and unreasonable object structures. ScaleCrafter [8] puts forward that the primary cause of the object repetition issue is the limited convolutional receptive field and uses dilated convolutional layers to achieve tuning-free higher-resolution sampling. But the generated results of ScaleCrafter still suffer from the problem of local repetition. Inspired by MultiDiffusion [9] fusing the local patches of the whole images, DemoFusion [10] designed mechanism by fusing the local patches and global patches, almost eliminating the local repetition. Essentially, this solution just transfers the extra signal of the object to the background, leading to small object repetition generation. FouriScale [11] reduces those extra signals by removing the high-frequency signals of the latent before the convolution operation. Although FouriScale completely eliminates all types of repetition, the generated results always have weird colors and textures due to its violent editing on the frequency domain. To generate satisfactory visual contents without any unexpected repetition, we propose FreeScale, tuning-free inference paradigm that enables pre-trained image and video diffusion models to generate vivid higher-resolution results. Building on past effective modules [8], [12], we first propose tailored self-cascade upscaling and restrained dilated convolution to gain the basic visual structure and maintain the quality in higher-resolution generation. To further eliminate all kinds of unexpected object repetitions, FreeScale processes information from different receptive scales and then fuses it by extracting desired frequency components, ensuring both the structures overall rationality and the objects local quality. This fusion is smoothly integrated into the original self-attention layers, thereby bringing only minimal additional time overhead. Finally, we demonstrate the effectiveness of our model on both the text-to-image model and the text-to-video model, pushing the boundaries of image generation even up to an 8k resolution. Benefiting from the exceptional scalability, DiT has become the dominant architecture in the development of recent foundational diffusion models. Nevertheless, FreeScale and the majority of existing works are built upon the UNet 00000000/00$00.00 2021 IEEE JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST architecture. Due to the architectural gap, these methods exhibit limited effectiveness on DiT-based models. Specifically, the major challenge faced by DiT-based models in highresolution generation is the substantial increase in token count, which results in untrained positional embeddings and overly diluted attention, ultimately hindering generation quality. Indeed, these challenges have been thoroughly explored in large language models for long-text generation [13], [14], providing valuable empirical knowledge like NTK-aware interpolation and attention reweighting. Combining those technologies, we extend the original FreeScale framework by tailoring it to the architectural properties of DiT, yielding new variant that supports high-resolution generation on DiT-based models. Although tuning-free stra"
[27.08.2025 06:17] Mistral response. {"object": "error", "message": "Service tier capacity exceeded for this model.", "type": "service_tier_capacity_exceeded", "param": null, "code": "3505"}
[27.08.2025 06:17] Failed to download and parse paper https://huggingface.co/papers/2508.15774: 'choices'
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.19242.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.19242.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.19242.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.18773.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.18773.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.18773.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.18621.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.18621.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.18621.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.15804.
[27.08.2025 06:17] Downloading paper 2508.15804 from http://arxiv.org/pdf/2508.15804v1...
[27.08.2025 06:17] Extracting affiliations from text.
[27.08.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 1 ] . [ 1 4 0 8 5 1 . 8 0 5 2 : r ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks Minghao Li, Ying Zeng, Zhihao Cheng, Cong Ma, Kai Jia ByteDance BandAI {liminghao.bd,zengying.ss,zhihao.cheng,macong.13,jiakai}@bytedance.com Abstract The advent of Deep Research agents has substantially reduced the time required for conducting extensive research tasks. However, these tasks inherently demand rigorous standards of factual accuracy and comprehensiveness, necessitating thorough evaluation before widespread adoption. In this paper, we propose ReportBench, systematic benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). Our evaluation focuses on two critical dimensions: (1) the quality and relevance of cited literature, and (2) the faithfulness and veracity of the statements within the generated reports. ReportBench leverages high-quality published survey papers available on arXiv as gold-standard references, from which we apply reverse prompt engineering to derive domainspecific prompts and establish comprehensive evaluation corpus. Furthermore, we develop an agent-based automated framework within ReportBench that systematically analyzes generated reports by extracting citations and statements, checking the faithfulness of cited content against original sources, and validating non-cited claims using web-based resources. Empirical evaluations demonstrate that commercial Deep Research agents such as those developed by OpenAI and Google consistently generate more comprehensive and reliable reports than standalone LLMs augmented with search or browsing tools. However, there remains substantial room for improvement in terms of the breadth and depth of research coverage, as well as factual consistency. The complete code and data will be released at the following link: https://github.com/ByteDance-BandAI/ReportBench. The rapid development of LLM-powered Deep Research agents has revolution"
[27.08.2025 06:17] Response: ```python
["ByteDance"]
```
[27.08.2025 06:17] Deleting PDF ./assets/pdf/2508.15804.pdf.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.18370.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.18370.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.18370.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.16697.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.16697.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.16697.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.19188.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.19188.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.19188.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.19026.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.19026.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.19026.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.18672.
[27.08.2025 06:17] Downloading paper 2508.18672 from http://arxiv.org/pdf/2508.18672v1...
[27.08.2025 06:17] Extracting affiliations from text.
[27.08.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks Taishi Nakamura 1 2 Satoki Ishikawa 1 Masaki Kawamura 1 Takumi Okamoto 1 2 Daisuke Nohara 1 Jun Suzuki 3 2 4 Rio Yokota "
[27.08.2025 06:17] Response: []
[27.08.2025 06:17] Extracting affiliations from text.
[27.08.2025 06:17] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks Taishi Nakamura 1 2 Satoki Ishikawa 1 Masaki Kawamura 1 Takumi Okamoto 1 2 Daisuke Nohara 1 Jun Suzuki 3 2 4 Rio Yokota1. Introduction 5 2 0 2 6 2 ] . [ 1 2 7 6 8 1 . 8 0 5 2 : r Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization and reasoning. We train families of MoE Transformers that systematically vary total parameters, active parameters, and top-k routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap. Memorization benchmarks improve monotonically with total parameters, mirroring training loss. By contrast, reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top-k alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as sparsity. Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly sparse models. Our model checkpoints, code and logs are open-source at https://github.com/ rioyokotalab/optimal-sparsity. 1Institute of Science Tokyo, Tokyo, Japan 2Research and Development Center for Large Language Models, National Institute of Informatics, Tokyo, Japan 3Tohoku University, Sendai, Japan 4RIKEN, Tokyo, Japan. Correspondence to: Taishi Nakamura <taishi@rio.scrc.iir.isct.ac.jp>, Rio Yokota <rioyokota@rio.scrc.iir.isct.ac.jp>. 1 The recent evolution of large language models (LLMs) has been driven by empirical scaling laws (Hestness et al., 2017) that link training loss to model size, dataset size, and compute budget. Kaplan et al. showed that these laws hold across seven orders of magnitude, establishing them as reliable extrapolation tool for dense Transformers (Kaplan et al., 2020). Subsequent work by Hoffmann et al. demonstrated that scaling curves can be inverted to choose the compute-optimal combination of parameters and tokens for fixed budget (Hoffmann et al., 2022). Together, these results have made scaling analysis cornerstone of model planning at both academic and industrial labs. Yet the coefficients of the scaling laws are not universal. Highly expressive models trained under different optimizers or architectures often follow the same loss trajectory but diverge substantially on downstream reasoning benchmarks (Liu et al., 2023). Brandfonbrener et al. extend the classic laws with loss-to-loss prediction, showing that the mapping between training and test distributions admits its own power law when the distributions differ substantially (Brandfonbrener et al., 2025). These observations imply that optimal budgets must be re-estimated whenever we modify the model or the data pipeline. particularly compelling architectural modification is the Mixture-of-Experts (MoE) paradigm, offering high capacity at fixed FLOPs by routing each token through sparse subset of experts (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2021). Modern flagship models, e.g., Gemini 2.5 Pro (Gemini Team, 2025), DeepSeek-V3 (DeepSeek-AI, 2025b), and Qwen3 (Qwen Team, 2025) now rely on MoE as de-facto standard for economical scaling. Abnar et al. derive parameters-vs-FLOPs frontier and locate an optimal sparsity for given compute budget (Abnar et al., 2025). These findings emphasize that the classical dense-model frontier is an incomplete picture, and one must account for architectural knobs such as MoE sparsity and top-k routing. Furthermore, loss-based scaling curves do not always predict the performance on downstream tasks. Jelassi et al. report that increasing MoE sparsity improves memorization benchmarks, but saturates for reasoning performance (Jelassi et al., 2025). However, the Mixture of Parrots paper (JeOptimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks lassi et al., 2025) only explores the number of active vs. total parameters, ignoring the effect of routing strategies beyond standard top-2 routing. They also do not consider the effect of reinforcement learning and test-time compute on their reasoning benchmarks. Evaluating reasoning performance immediately after pre-training overlooks both the benefits of post-training adaptation and the leverage of additional testtime compute. Post-training methods such as GRPO, which use reinforcement signals to encourage coherent chain-ofthought generation, sharpen models reasoning on complex tasks (OpenAI, 2024b; DeepSeek-AI, 2025a). Beyond these refinements, models can further improve outputs at test time by adopting calibrated decoding strategies that mirror how humans pause to reconsider difficult problems. These test-time approaches not only boost routine benchmark performance but, when properly tuned, substantially enhance multi-step mathematical reasoning, demonstrating that adaptive computing at test time is powerful complement to both model scale and post-training adaptation. In this paper, we aim to identify how the optimal sparsity of MoE changes between memorization (TriviaQA, HellaSwag) and reasoning (GSM8K, GSM-Plus) tasks. In this work, we use the term dense models to refer to standard Transformers with single feed-forward network per layer. For MoE models, we define sparsity as sparsity = 1 Top-k Experts following the convention that sparsity measures the fraction of inactive parameters. We train families of MoEs varying not only the total vs. active parameters, but also the number of top-k experts. For each model, we measure the loss on the pre-training data, the task loss on the downstream benchmarks, and the accuracy on those benchmarks. This allows us to disentangle the generalization gap between the train vs. test loss, and the gap between loss vs. accuracy. For both memorization and reasoning benchmarks, the train loss decreases monotonically with the total parameters. The task loss and accuracy follow the same monotonic trend as the trai"
[27.08.2025 06:17] Mistral response. {"object": "error", "message": "Service tier capacity exceeded for this model.", "type": "service_tier_capacity_exceeded", "param": null, "code": "3505"}
[27.08.2025 06:17] Failed to download and parse paper https://huggingface.co/papers/2508.18672: 'choices'
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.18271.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.18271.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.18271.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Downloading and parsing paper https://huggingface.co/papers/2508.18192.
[27.08.2025 06:17] Extra JSON file exists (./assets/json/2508.18192.json), skip PDF parsing.
[27.08.2025 06:17] Paper image links file exists (./assets/img_data/2508.18192.json), skip HTML parsing.
[27.08.2025 06:17] Success.
[27.08.2025 06:17] Enriching papers with extra data.
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 0. VibeVoice synthesizes long-form multi-speaker speech using next-token diffusion and a highly efficient continuous speech tokenizer, achieving superior performance and fidelity.  					AI-generated summary 				 This report presents VibeVoice, a novel model designed to synthesize long-form speech with ...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 1. CMPhysBench evaluates LLMs in condensed matter physics using calculation problems and a new SEED score for partial credit assessment, revealing significant capability gaps.  					AI-generated summary 				 We introduce CMPhysBench, designed to assess the proficiency of Large Language Models (LLMs) in...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 2. Spacer, a scientific discovery system, uses deliberate decontextualization to generate creative and factually grounded scientific concepts from keyword sets, achieving high accuracy and similarity to leading publications.  					AI-generated summary 				 Recent advances in LLMs have made automated sc...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 3. UltraMemV2, a redesigned memory-layer architecture, achieves performance parity with 8-expert MoE models while significantly reducing memory access costs.  					AI-generated summary 				 While Mixture of Experts (MoE) models achieve remarkable efficiency by activating only subsets of parameters, the...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 4. A framework using Multimodal Large Language Models and a specialized Multimodal DiT architecture generates semantically coherent and expressive character animations from multimodal inputs.  					AI-generated summary 				 Existing video avatar models can produce fluid human animations, yet they strug...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 5. TreePO, a self-guided rollout algorithm for sequence generation, reduces computational cost and enhances exploration diversity in reinforcement learning for large language models.  					AI-generated summary 				 Recent advancements in aligning large language models via reinforcement learning have ac...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 6. VoxHammer is a training-free method that performs precise and coherent 3D editing in latent space, ensuring consistency in preserved regions and high-quality overall results.  					AI-generated summary 				 3D local editing of specified regions is crucial for game industry and robot interaction. Rec...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 7. PIXIE, a neural network method, predicts physical properties of 3D scenes from visual features, enabling fast and realistic physics simulation using supervised learning and pretrained visual features.  					AI-generated summary 				 Inferring the physical properties of 3D scenes from visual informat...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 8. CineScale is a novel inference paradigm that enables high-resolution visual generation for both images and videos without extensive fine-tuning, addressing issues of repetitive patterns and high-frequency information accumulation.  					AI-generated summary 				 Visual diffusion models achieve remar...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 9. AUSM, an autoregressive universal segmentation model, unifies prompted and unprompted video segmentation by treating it as sequential mask prediction, achieving superior performance and faster training on standard benchmarks.  					AI-generated summary 				 Recent video foundation models such as SAM...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 10. ThinkDial is an open-source framework that implements controllable reasoning in large language models through discrete operational modes, achieving performance while reducing computational effort.  					AI-generated summary 				 Large language models (LLMs) with chain-of-thought reasoning have demon...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 11. Wan-S2V, an audio-driven model built on Wan, enhances expressiveness and fidelity in cinematic character animation compared to existing methods.  					AI-generated summary 				 Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenario...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 12. ReportBench evaluates the content quality of research reports generated by large language models, focusing on cited literature quality and statement faithfulness, demonstrating that commercial Deep Research agents produce more comprehensive and reliable reports than standalone LLMs.  					AI-generat...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 13. CTF-Dojo, a large-scale executable runtime with 658 CTF challenges, enables rapid training of LLM-based agents with verifiable feedback, achieving state-of-the-art performance in competitive benchmarks.  					AI-generated summary 				 Large language models (LLMs) have demonstrated exceptional capabi...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 14. QueryBandits, a bandit framework, effectively mitigates hallucinations in LLMs by proactively rewriting queries based on linguistic features, outperforming static prompting strategies.  					AI-generated summary 				 Advanced reasoning capabilities in Large Language Models (LLMs) have caused higher ...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 15. A framework for efficient artistic mesh generation reduces redundancy by separating vertex and face generation, using an autoregressive model for vertices and a bidirectional transformer for faces, and includes a fidelity enhancer and post-processing to improve quality and speed.  					AI-generated ...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 16. MovieCORE is a video question answering dataset that uses multiple large language models to generate deep cognitive questions, and introduces an agentic enhancement module to improve VQA model performance.  					AI-generated summary 				 This paper introduces MovieCORE, a novel video question answer...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 17. MoE models introduce sparsity that affects memorization and reasoning capabilities differently in large language models, with reasoning performance potentially regressing despite increased parameters.  					AI-generated summary 				 Empirical scaling laws have driven the evolution of large language ...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 18. ObjFiller-3D uses video editing models to achieve high-quality and consistent 3D object completion, outperforming previous methods in terms of reconstruction fidelity and practical deployment.  					AI-generated summary 				 3D inpainting often relies on multi-view 2D image inpainting, where the inh...
[27.08.2025 06:17] ********************************************************************************
[27.08.2025 06:17] Abstract 19. A network-based framework links cognitive skills, LLM architectures, and datasets, revealing unique emergent skill patterns in LLMs that benefit from dynamic, cross-regional interactions.  					AI-generated summary 				 Large Language Models (LLMs) have reshaped our world with significant advancemen...
[27.08.2025 06:17] Read previous papers.
[27.08.2025 06:17] Generating reviews via LLM API.
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#open_source", "#long_context", "#diffusion", "#audio"], "emoji": "🗣️", "ru": {"title": "VibeVoice: революция в синтезе длительной многоголосой речи", "desc": "VibeVoice - это новая модель для синтеза длительной многоголосой речи, использующая диффузию следующего токена и эффективны
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#benchmark", "#science", "#dataset"], "emoji": "🔬", "ru": {"title": "Новый вызов для ИИ: решение задач по физике конденсированного состояния", "desc": "CMPhysBench - это новый бенчмарк для оценки способностей больших языковых моделей (LLM) в области физики конденсированного состояни
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#science", "#data", "#multimodal", "#dataset"], "emoji": "🧬", "ru": {"title": "Spacer: Революция в автоматизированных научных открытиях", "desc": "Spacer - это система научных открытий, использующая намеренную деконтекстуализацию для генерации креативных и фактически обоснованных на
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#architecture", "#long_context", "#optimization", "#training"], "emoji": "🧠", "ru": {"title": "UltraMemV2: Эффективность MoE без высоких затрат на память", "desc": "UltraMemV2 - это новая архитектура слоя памяти для нейронных сетей, которая достигает производительности 8-экспертных 
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#architecture", "#interpretability", "#optimization", "#games", "#multimodal", "#video"], "emoji": "🎭", "ru": {"title": "Семантически осмысленная анимация персонажей с помощью мультимодального ИИ", "desc": "Эта статья представляет новую модель OmniHuman-1.5 для генерации анимации пе
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#training", "#rl", "#rlhf"], "emoji": "🌳", "ru": {"title": "TreePO: Эффективное обучение с подкреплением для языковых моделей", "desc": "Статья представляет TreePO - алгоритм самонаправляемой генерации последовательностей для обучения с подкреплением б
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#games", "#dataset", "#synthetic", "#3d"], "emoji": "🔨", "ru": {"title": "Точное 3D-редактирование без компромиссов", "desc": "VoxHammer - это метод редактирования 3D-моделей в латентном пространстве без дополнительного обучения. Он обеспечивает точное и согласованное редактирование
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#inference", "#synthetic", "#3d"], "emoji": "🧠", "ru": {"title": "Быстрое предсказание физических свойств 3D-сцен с помощью нейронных сетей", "desc": "PIXIE - это нейросетевой метод, который предсказывает физические свойства 3D-сцен на основе визуальных 
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#inference", "#cv", "#video"], "emoji": "🎬", "ru": {"title": "CineScale: Прорыв в генерации высококачественного визуального контента", "desc": "CineScale - это новая парадигма вывода, позволяющая генерировать изображения и видео высокого разрешения без 
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#architecture", "#training", "#video", "#optimization"], "emoji": "🎬", "ru": {"title": "Универсальная сегментация видео как последовательное предсказание масок", "desc": "AUSM - это модель универсальной сегментации видео, объединяющая подходы с подсказками и без них. Она рассматрива
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#architecture", "#open_source", "#training", "#optimization", "#reasoning", "#agi", "#rl"], "emoji": "🧠", "ru": {"title": "ThinkDial: контролируемое рассуждение в больших языковых моделях", "desc": "ThinkDial - это фреймворк с открытым исходным кодом, который реализует контролируемо
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#story_generation", "#audio", "#games", "#benchmark", "#video"], "emoji": "🎭", "ru": {"title": "Wan-S2V: Революция в анимации кинематографических персонажей на основе аудио", "desc": "Модель Wan-S2V, основанная на аудио, улучшает выразительность и точность анимации кинематографическ
[27.08.2025 06:17] Querying the API.
[27.08.2025 06:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ReportBench evaluates the content quality of research reports generated by large language models, focusing on cited literature quality and statement faithfulness, demonstrating that commercial Deep Research agents produce more comprehensive and reliable reports than standalone LLMs.  					AI-generated summary 				 The advent of Deep Research agents has substantially reduced the time required for conducting extensive research tasks. However, these tasks inherently demand rigorous standards of factual accuracy and comprehensiveness, necessitating thorough evaluation before widespread adoption. In this paper, we propose ReportBench, a systematic benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). Our evaluation focuses on two critical dimensions: (1) the quality and relevance of cited literature, and (2) the faithfulness and veracity of the statements within the generated reports. ReportBench leverages high-quality published survey papers available on arXiv as gold-standard references, from which we apply reverse prompt engineering to derive domain-specific prompts and establish a comprehensive evaluation corpus. Furthermore, we develop an agent-based automated framework within ReportBench that systematically analyzes generated reports by extracting citations and statements, checking the faithfulness of cited content against original sources, and validating non-cited claims using web-based resources. Empirical evaluations demonstrate that commercial Deep Research agents such as those developed by OpenAI and Google consistently generate more comprehensive and reliable reports than standalone LLMs augmented with search or browsing tools. However, there remains substantial room for improvement in terms of the breadth and depth of research coverage, as well as factual consistency. The complete code and data will be released at the following link: https://github.com/ByteDance-BandAI/ReportBench
[27.08.2025 06:17] Response: {
  "desc": "ReportBench - это новый метод оценки качества исследовательских отчетов, сгенерированных большими языковыми моделями (LLM). Он фокусируется на качестве цитируемой литературы и достоверности утверждений в отчетах. ReportBench использует высококачественные обзорные статьи с arXiv в качестве эталона и применяет обратную инженерию промптов для создания корпуса оценки. Результаты показывают, что коммерческие агенты Deep Research генерируют более надежные отчеты, чем standalone LLM, хотя все еще есть возможности для улучшения.",
  "emoji": "🔬",
  "title": "ReportBench: новый стандарт оценки AI-исследований"
}
[27.08.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ReportBench evaluates the content quality of research reports generated by large language models, focusing on cited literature quality and statement faithfulness, demonstrating that commercial Deep Research agents produce more comprehensive and reliable reports than standalone LLMs.  					AI-generated summary 				 The advent of Deep Research agents has substantially reduced the time required for conducting extensive research tasks. However, these tasks inherently demand rigorous standards of factual accuracy and comprehensiveness, necessitating thorough evaluation before widespread adoption. In this paper, we propose ReportBench, a systematic benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). Our evaluation focuses on two critical dimensions: (1) the quality and relevance of cited literature, and (2) the faithfulness and veracity of the statements within the generated reports. ReportBench leverages high-quality published survey papers available on arXiv as gold-standard references, from which we apply reverse prompt engineering to derive domain-specific prompts and establish a comprehensive evaluation corpus. Furthermore, we develop an agent-based automated framework within ReportBench that systematically analyzes generated reports by extracting citations and statements, checking the faithfulness of cited content against original sources, and validating non-cited claims using web-based resources. Empirical evaluations demonstrate that commercial Deep Research agents such as those developed by OpenAI and Google consistently generate more comprehensive and reliable reports than standalone LLMs augmented with search or browsing tools. However, there remains substantial room for improvement in terms of the breadth and depth of research coverage, as well as factual consistency. The complete code and data will be released at the following link: https://github.com/ByteDance-BandAI/ReportBench"

[27.08.2025 06:17] Response: ```python
['BENCHMARK', 'AGENTS']
```
[27.08.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ReportBench evaluates the content quality of research reports generated by large language models, focusing on cited literature quality and statement faithfulness, demonstrating that commercial Deep Research agents produce more comprehensive and reliable reports than standalone LLMs.  					AI-generated summary 				 The advent of Deep Research agents has substantially reduced the time required for conducting extensive research tasks. However, these tasks inherently demand rigorous standards of factual accuracy and comprehensiveness, necessitating thorough evaluation before widespread adoption. In this paper, we propose ReportBench, a systematic benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). Our evaluation focuses on two critical dimensions: (1) the quality and relevance of cited literature, and (2) the faithfulness and veracity of the statements within the generated reports. ReportBench leverages high-quality published survey papers available on arXiv as gold-standard references, from which we apply reverse prompt engineering to derive domain-specific prompts and establish a comprehensive evaluation corpus. Furthermore, we develop an agent-based automated framework within ReportBench that systematically analyzes generated reports by extracting citations and statements, checking the faithfulness of cited content against original sources, and validating non-cited claims using web-based resources. Empirical evaluations demonstrate that commercial Deep Research agents such as those developed by OpenAI and Google consistently generate more comprehensive and reliable reports than standalone LLMs augmented with search or browsing tools. However, there remains substantial room for improvement in terms of the breadth and depth of research coverage, as well as factual consistency. The complete code and data will be released at the following link: https://github.com/ByteDance-BandAI/ReportBench"

[27.08.2025 06:17] Response: ```python
['INTERPRETABILITY', 'SURVEY', 'OPEN_SOURCE']
```
[27.08.2025 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ReportBench is a new tool that assesses the quality of research reports created by large language models (LLMs). It focuses on two main aspects: the quality of the literature cited and the accuracy of the statements made in the reports. By using high-quality survey papers as references, ReportBench checks if the generated content is both comprehensive and trustworthy. The findings show that advanced Deep Research agents outperform standalone LLMs in producing reliable reports, but there is still a need for improvement in research coverage and factual accuracy.","title":"Evaluating Research Quality in AI-Generated Reports"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ReportBench is a new tool that assesses the quality of research reports created by large language models (LLMs). It focuses on two main aspects: the quality of the literature cited and the accuracy of the statements made in the reports. By using high-quality survey papers as references, ReportBench checks if the generated content is both comprehensive and trustworthy. The findings show that advanced Deep Research agents outperform standalone LLMs in producing reliable reports, but there is still a need for improvement in research coverage and factual accuracy.', title='Evaluating Research Quality in AI-Generated Reports'))
[27.08.2025 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文提出了ReportBench，这是一个系统化的基准，用于评估大型语言模型生成的研究报告的内容质量。评估主要集中在两个方面：引用文献的质量和相关性，以及生成报告中陈述的真实性和可靠性。研究表明，商业深度研究代理（如OpenAI和Google开发的）生成的报告比独立的语言模型更全面和可靠。尽管如此，研究覆盖的广度和深度以及事实一致性仍有很大的改进空间。","title":"评估AI生成研究报告的质量"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文提出了ReportBench，这是一个系统化的基准，用于评估大型语言模型生成的研究报告的内容质量。评估主要集中在两个方面：引用文献的质量和相关性，以及生成报告中陈述的真实性和可靠性。研究表明，商业深度研究代理（如OpenAI和Google开发的）生成的报告比独立的语言模型更全面和可靠。尽管如此，研究覆盖的广度和深度以及事实一致性仍有很大的改进空间。', title='评估AI生成研究报告的质量'))
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#open_source", "#training", "#dataset", "#games", "#benchmark", "#agents"], "emoji": "🏆", "ru": {"title": "CTF-Dojo: революция в обучении ИИ-агентов через исполняемую среду", "desc": "CTF-Dojo - это крупномасштабная исполняемая среда с 658 задачами CTF, позволяющая быстро обучать аг
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#reasoning", "#training", "#hallucinations", "#multimodal", "#rl"], "emoji": "🎯", "ru": {"title": "Умное переписывание запросов побеждает галлюцинации ИИ", "desc": "QueryBandits - это фреймворк, использующий алгоритмы многоруких бандитов для снижения галлюцинаций в больших языковых 
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#games", "#optimization", "#3d"], "emoji": "🎨", "ru": {"title": "Эффективная генерация 3D-моделей: разделяй и властвуй", "desc": "Предложена эффективная система генерации художественных 3D-моделей, разделяющая процессы создания вершин и граней. Для вершин используется авторегрессион
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#video", "#reasoning", "#alignment", "#agents", "#benchmark", "#dataset"], "emoji": "🎬", "ru": {"title": "MovieCORE: Глубокое понимание фильмов с помощью ИИ", "desc": "MovieCORE - это новый набор данных для ответов на вопросы по видео, который использует несколько больших языковых м
[27.08.2025 06:17] Querying the API.
[27.08.2025 06:17] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MoE models introduce sparsity that affects memorization and reasoning capabilities differently in large language models, with reasoning performance potentially regressing despite increased parameters.  					AI-generated summary 				 Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization and reasoning. We train families of MoE Transformers that systematically vary total parameters, active parameters, and top-k routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap. Memorization benchmarks improve monotonically with total parameters, mirroring training loss. By contrast, reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top-k alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as sparsity. Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly sparse models. Our model checkpoints, code and logs are open-source at https://github.com/rioyokotalab/optimal-sparsity.
[27.08.2025 06:17] Response: {
  "desc": "Статья исследует влияние разреженности в моделях Mixture-of-Experts (MoE) на способности больших языковых моделей к запоминанию и рассуждению. Авторы обнаружили, что увеличение общего числа параметров улучшает запоминание, но может ухудшать способность к рассуждениям. Изменение параметра top-k маршрутизации мало влияет при постоянном числе активных параметров. Исследование показывает, что чрезмерная разреженность может негативно сказываться на некоторых аспектах производительности языковых моделей.",

  "emoji": "🧠",

  "title": "Разреженность в MoE: компромисс между запоминанием и рассуждением"
}
[27.08.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MoE models introduce sparsity that affects memorization and reasoning capabilities differently in large language models, with reasoning performance potentially regressing despite increased parameters.  					AI-generated summary 				 Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization and reasoning. We train families of MoE Transformers that systematically vary total parameters, active parameters, and top-k routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap. Memorization benchmarks improve monotonically with total parameters, mirroring training loss. By contrast, reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top-k alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as sparsity. Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly sparse models. Our model checkpoints, code and logs are open-source at https://github.com/rioyokotalab/optimal-sparsity."

[27.08.2025 06:17] Response: ```python
['ARCHITECTURE', 'TRAINING']
```
[27.08.2025 06:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MoE models introduce sparsity that affects memorization and reasoning capabilities differently in large language models, with reasoning performance potentially regressing despite increased parameters.  					AI-generated summary 				 Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization and reasoning. We train families of MoE Transformers that systematically vary total parameters, active parameters, and top-k routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap. Memorization benchmarks improve monotonically with total parameters, mirroring training loss. By contrast, reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top-k alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as sparsity. Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly sparse models. Our model checkpoints, code and logs are open-source at https://github.com/rioyokotalab/optimal-sparsity."

[27.08.2025 06:17] Response: ```python
["REASONING", "OPTIMIZATION", "OPEN_SOURCE"]
```
[27.08.2025 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how Mixture-of-Experts (MoE) models, which introduce sparsity in large language models (LLMs), impact their ability to memorize and reason. While increasing the number of parameters improves memorization performance, reasoning capabilities can actually decline despite these gains. The authors systematically analyze various configurations of MoE Transformers, focusing on how active parameters and routing strategies affect model performance. Their findings indicate that traditional hyperparameters and post-training techniques do not effectively address the reasoning deficits caused by excessive sparsity in these models.","title":"Balancing Memorization and Reasoning in Sparse MoE Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores how Mixture-of-Experts (MoE) models, which introduce sparsity in large language models (LLMs), impact their ability to memorize and reason. While increasing the number of parameters improves memorization performance, reasoning capabilities can actually decline despite these gains. The authors systematically analyze various configurations of MoE Transformers, focusing on how active parameters and routing strategies affect model performance. Their findings indicate that traditional hyperparameters and post-training techniques do not effectively address the reasoning deficits caused by excessive sparsity in these models.', title='Balancing Memorization and Reasoning in Sparse MoE Models'))
[27.08.2025 06:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MoE模型引入了稀疏性，这对大型语言模型的记忆和推理能力产生了不同的影响。尽管参数增加，推理性能可能会下降，而记忆能力则随着总参数的增加而持续改善。我们研究了MoE稀疏性如何影响记忆和推理这两种能力模式，并通过训练不同参数配置的MoE Transformer来分析其效果。结果表明，经典超参数如学习率和初始化对泛化差距的影响与稀疏性方向一致，但过于稀疏的模型在推理能力上存在明显不足。","title":"MoE模型：稀疏性对记忆与推理的双重影响"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MoE模型引入了稀疏性，这对大型语言模型的记忆和推理能力产生了不同的影响。尽管参数增加，推理性能可能会下降，而记忆能力则随着总参数的增加而持续改善。我们研究了MoE稀疏性如何影响记忆和推理这两种能力模式，并通过训练不同参数配置的MoE Transformer来分析其效果。结果表明，经典超参数如学习率和初始化对泛化差距的影响与稀疏性方向一致，但过于稀疏的模型在推理能力上存在明显不足。', title='MoE模型：稀疏性对记忆与推理的双重影响'))
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#video", "#optimization", "#3d"], "emoji": "🎨", "ru": {"title": "Революция в 3D-инпейнтинге: от видео к реалистичным объектам", "desc": "ObjFiller-3D - это новый метод для заполнения и редактирования трехмерных объектов высокого качества. Он использует модели редактирования видео дл
[27.08.2025 06:17] Using data from previous issue: {"categories": ["#architecture", "#science", "#training", "#interpretability", "#dataset"], "emoji": "🧠", "ru": {"title": "Сетевой анализ раскрывает когнитивные паттерны в языковых моделях", "desc": "Данная статья представляет сетевую модель, связывающую когнитивные навыки, архитектуры языковых моде
[27.08.2025 06:17] Renaming data file.
[27.08.2025 06:17] Renaming previous data. hf_papers.json to ./d/2025-08-27.json
[27.08.2025 06:17] Saving new data file.
[27.08.2025 06:17] Generating page.
[27.08.2025 06:17] Renaming previous page.
[27.08.2025 06:17] Renaming previous data. index.html to ./d/2025-08-27.html
[27.08.2025 06:17] Writing result.
[27.08.2025 06:17] Renaming log file.
[27.08.2025 06:17] Renaming previous data. log.txt to ./logs/2025-08-27_last_log.txt
