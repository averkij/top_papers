[27.08.2025 11:10] Read previous papers.
[27.08.2025 11:10] Generating top page (month).
[27.08.2025 11:10] Writing top page (month).
[27.08.2025 12:21] Read previous papers.
[27.08.2025 12:21] Get feed.
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18124
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17445
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19205
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19247
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17661
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19209
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18756
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17437
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19242
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18621
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15774
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19188
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15804
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18773
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19026
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18672
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18370
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18271
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.16697
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19202
[27.08.2025 12:21] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18192
[27.08.2025 12:21] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.08.2025 12:21] No deleted papers detected.
[27.08.2025 12:21] Downloading and parsing papers (pdf, html). Total: 21.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.18124.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.18124.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.18124.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.17445.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.17445.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.17445.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.19205.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.19205.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.19205.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.19247.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.19247.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.19247.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.17661.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.17661.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.17661.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.19209.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.19209.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.19209.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.18756.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.18756.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.18756.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.17437.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.17437.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.17437.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.19242.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.19242.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.19242.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.18621.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.18621.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.18621.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.15774.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.15774.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.15774.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.19188.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.19188.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.19188.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.15804.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.15804.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.15804.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.18773.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.18773.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.18773.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.19026.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.19026.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.19026.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.18672.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.18672.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.18672.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.18370.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.18370.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.18370.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.18271.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.18271.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.18271.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.16697.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.16697.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.16697.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.19202.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.19202.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.19202.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Downloading and parsing paper https://huggingface.co/papers/2508.18192.
[27.08.2025 12:21] Extra JSON file exists (./assets/json/2508.18192.json), skip PDF parsing.
[27.08.2025 12:21] Paper image links file exists (./assets/img_data/2508.18192.json), skip HTML parsing.
[27.08.2025 12:21] Success.
[27.08.2025 12:21] Enriching papers with extra data.
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 0. CMPhysBench evaluates LLMs in condensed matter physics using calculation problems and a new SEED score for partial credit assessment, revealing significant capability gaps.  					AI-generated summary 				 We introduce CMPhysBench, designed to assess the proficiency of Large Language Models (LLMs) in...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 1. TreePO, a self-guided rollout algorithm for sequence generation, reduces computational cost and enhances exploration diversity in reinforcement learning for large language models.  					AI-generated summary 				 Recent advancements in aligning large language models via reinforcement learning have ac...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 2. VibeVoice synthesizes long-form multi-speaker speech using next-token diffusion and a highly efficient continuous speech tokenizer, achieving superior performance and fidelity.  					AI-generated summary 				 This report presents VibeVoice, a novel model designed to synthesize long-form speech with ...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 3. VoxHammer is a training-free method that performs precise and coherent 3D editing in latent space, ensuring consistency in preserved regions and high-quality overall results.  					AI-generated summary 				 3D local editing of specified regions is crucial for game industry and robot interaction. Rec...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 4. Spacer, a scientific discovery system, uses deliberate decontextualization to generate creative and factually grounded scientific concepts from keyword sets, achieving high accuracy and similarity to leading publications.  					AI-generated summary 				 Recent advances in LLMs have made automated sc...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 5. A framework using Multimodal Large Language Models and a specialized Multimodal DiT architecture generates semantically coherent and expressive character animations from multimodal inputs.  					AI-generated summary 				 Existing video avatar models can produce fluid human animations, yet they strug...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 6. UltraMemV2, a redesigned memory-layer architecture, achieves performance parity with 8-expert MoE models while significantly reducing memory access costs.  					AI-generated summary 				 While Mixture of Experts (MoE) models achieve remarkable efficiency by activating only subsets of parameters, the...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 7. PIXIE, a neural network method, predicts physical properties of 3D scenes from visual features, enabling fast and realistic physics simulation using supervised learning and pretrained visual features.  					AI-generated summary 				 Inferring the physical properties of 3D scenes from visual informat...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 8. AUSM, an autoregressive universal segmentation model, unifies prompted and unprompted video segmentation by treating it as sequential mask prediction, achieving superior performance and faster training on standard benchmarks.  					AI-generated summary 				 Recent video foundation models such as SAM...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 9. Wan-S2V, an audio-driven model built on Wan, enhances expressiveness and fidelity in cinematic character animation compared to existing methods.  					AI-generated summary 				 Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenario...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 10. CineScale is a novel inference paradigm that enables high-resolution visual generation for both images and videos without extensive fine-tuning, addressing issues of repetitive patterns and high-frequency information accumulation.  					AI-generated summary 				 Visual diffusion models achieve remar...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 11. A framework for efficient artistic mesh generation reduces redundancy by separating vertex and face generation, using an autoregressive model for vertices and a bidirectional transformer for faces, and includes a fidelity enhancer and post-processing to improve quality and speed.  					AI-generated ...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 12. ReportBench evaluates the content quality of research reports generated by large language models, focusing on cited literature quality and statement faithfulness, demonstrating that commercial Deep Research agents produce more comprehensive and reliable reports than standalone LLMs.  					AI-generat...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 13. ThinkDial is an open-source framework that implements controllable reasoning in large language models through discrete operational modes, achieving performance while reducing computational effort.  					AI-generated summary 				 Large language models (LLMs) with chain-of-thought reasoning have demon...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 14. MovieCORE is a video question answering dataset that uses multiple large language models to generate deep cognitive questions, and introduces an agentic enhancement module to improve VQA model performance.  					AI-generated summary 				 This paper introduces MovieCORE, a novel video question answer...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 15. MoE models introduce sparsity that affects memorization and reasoning capabilities differently in large language models, with reasoning performance potentially regressing despite increased parameters.  					AI-generated summary 				 Empirical scaling laws have driven the evolution of large language ...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 16. CTF-Dojo, a large-scale executable runtime with 658 CTF challenges, enables rapid training of LLM-based agents with verifiable feedback, achieving state-of-the-art performance in competitive benchmarks.  					AI-generated summary 				 Large language models (LLMs) have demonstrated exceptional capabi...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 17. ObjFiller-3D uses video editing models to achieve high-quality and consistent 3D object completion, outperforming previous methods in terms of reconstruction fidelity and practical deployment.  					AI-generated summary 				 3D inpainting often relies on multi-view 2D image inpainting, where the inh...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 18. QueryBandits, a bandit framework, effectively mitigates hallucinations in LLMs by proactively rewriting queries based on linguistic features, outperforming static prompting strategies.  					AI-generated summary 				 Advanced reasoning capabilities in Large Language Models (LLMs) have caused higher ...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 19. SciReas and SciReas-Pro benchmarks, along with KRUX framework, provide insights into the distinct roles of knowledge and reasoning in scientific tasks, highlighting critical bottlenecks and improvements for LLMs.  					AI-generated summary 				 Scientific problem solving poses unique challenges for ...
[27.08.2025 12:21] ********************************************************************************
[27.08.2025 12:21] Abstract 20. A network-based framework links cognitive skills, LLM architectures, and datasets, revealing unique emergent skill patterns in LLMs that benefit from dynamic, cross-regional interactions.  					AI-generated summary 				 Large Language Models (LLMs) have reshaped our world with significant advancemen...
[27.08.2025 12:21] Read previous papers.
[27.08.2025 12:21] Generating reviews via LLM API.
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#benchmark", "#science", "#dataset"], "emoji": "üî¨", "ru": {"title": "–ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –ò–ò: —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á –ø–æ —Ñ–∏–∑–∏–∫–µ –∫–æ–Ω–¥–µ–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è", "desc": "CMPhysBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª–∞—Å—Ç–∏ —Ñ–∏–∑–∏–∫–∏ –∫–æ–Ω–¥–µ–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#training", "#rl", "#rlhf"], "emoji": "üå≥", "ru": {"title": "TreePO: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç TreePO - –∞–ª–≥–æ—Ä–∏—Ç–º —Å–∞–º–æ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#open_source", "#long_context", "#diffusion", "#audio"], "emoji": "üó£Ô∏è", "ru": {"title": "VibeVoice: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∏–Ω—Ç–µ–∑–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–π –º–Ω–æ–≥–æ–≥–æ–ª–æ—Å–æ–π —Ä–µ—á–∏", "desc": "VibeVoice - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–π –º–Ω–æ–≥–æ–≥–æ–ª–æ—Å–æ–π —Ä–µ—á–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—é —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#games", "#dataset", "#synthetic", "#3d"], "emoji": "üî®", "ru": {"title": "–¢–æ—á–Ω–æ–µ 3D-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤", "desc": "VoxHammer - —ç—Ç–æ –º–µ—Ç–æ–¥ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è 3D-–º–æ–¥–µ–ª–µ–π –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ç–æ—á–Ω–æ–µ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#science", "#data", "#multimodal", "#dataset"], "emoji": "üß¨", "ru": {"title": "Spacer: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏—è—Ö", "desc": "Spacer - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞—É—á–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –Ω–∞–º–µ—Ä–µ–Ω–Ω—É—é –¥–µ–∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã—Ö –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#architecture", "#interpretability", "#optimization", "#games", "#multimodal", "#video"], "emoji": "üé≠", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å OmniHuman-1.5 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#architecture", "#long_context", "#optimization", "#training"], "emoji": "üß†", "ru": {"title": "UltraMemV2: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å MoE –±–µ–∑ –≤—ã—Å–æ–∫–∏—Ö –∑–∞—Ç—Ä–∞—Ç –Ω–∞ –ø–∞–º—è—Ç—å", "desc": "UltraMemV2 - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–ª–æ—è –ø–∞–º—è—Ç–∏ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 8-—ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö 
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#inference", "#synthetic", "#3d"], "emoji": "üß†", "ru": {"title": "–ë—ã—Å—Ç—Ä–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤ 3D-—Å—Ü–µ–Ω —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π", "desc": "PIXIE - —ç—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞ 3D-—Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö 
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#architecture", "#training", "#video", "#optimization"], "emoji": "üé¨", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤–∏–¥–µ–æ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–∞—Å–æ–∫", "desc": "AUSM - —ç—Ç–æ –º–æ–¥–µ–ª—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –ø–æ–¥—Ö–æ–¥—ã —Å –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏ –∏ –±–µ–∑ –Ω–∏—Ö. –û–Ω–∞ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#story_generation", "#audio", "#games", "#benchmark", "#video"], "emoji": "üé≠", "ru": {"title": "Wan-S2V: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–Ω–∏–º–∞—Ü–∏–∏ –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ", "desc": "–ú–æ–¥–µ–ª—å Wan-S2V, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∞—É–¥–∏–æ, —É–ª—É—á—à–∞–µ—Ç –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –∞–Ω–∏–º–∞—Ü–∏–∏ –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#inference", "#cv", "#video"], "emoji": "üé¨", "ru": {"title": "CineScale: –ü—Ä–æ—Ä—ã–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "CineScale - —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –≤—ã–≤–æ–¥–∞, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –±–µ–∑ 
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#games", "#optimization", "#3d"], "emoji": "üé®", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 3D-–º–æ–¥–µ–ª–µ–π: —Ä–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π, —Ä–∞–∑–¥–µ–ª—è—é—â–∞—è –ø—Ä–æ—Ü–µ—Å—Å—ã —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ—Ä—à–∏–Ω –∏ –≥—Ä–∞–Ω–µ–π. –î–ª—è –≤–µ—Ä—à–∏–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#survey", "#agents", "#open_source"], "emoji": "üî¨", "ru": {"title": "ReportBench: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ AI-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π", "desc": "ReportBench - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ 
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#architecture", "#open_source", "#training", "#optimization", "#reasoning", "#agi", "#rl"], "emoji": "üß†", "ru": {"title": "ThinkDial: –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "ThinkDial - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ–∞–ª–∏–∑—É–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#video", "#reasoning", "#alignment", "#agents", "#benchmark", "#dataset"], "emoji": "üé¨", "ru": {"title": "MovieCORE: –ì–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ–∏–ª—å–º–æ–≤ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "MovieCORE - —ç—Ç–æ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#architecture", "#training", "#open_source"], "emoji": "üß†", "ru": {"title": "–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –≤ MoE: –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –º–æ–¥–µ–ª—è—Ö Mixture-of-Experts (MoE) –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#open_source", "#training", "#dataset", "#games", "#benchmark", "#agents"], "emoji": "üèÜ", "ru": {"title": "CTF-Dojo: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∏—Å–ø–æ–ª–Ω—è–µ–º—É—é —Å—Ä–µ–¥—É", "desc": "CTF-Dojo - —ç—Ç–æ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–∞—è –∏—Å–ø–æ–ª–Ω—è–µ–º–∞—è —Å—Ä–µ–¥–∞ —Å 658 –∑–∞–¥–∞—á–∞–º–∏ CTF, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –±—ã—Å—Ç—Ä–æ –æ–±—É—á–∞—Ç—å –∞–≥
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#video", "#optimization", "#3d"], "emoji": "üé®", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-–∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥–µ: –æ—Ç –≤–∏–¥–µ–æ –∫ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º –æ–±—ä–µ–∫—Ç–∞–º", "desc": "ObjFiller-3D - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –¥–ª
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#reasoning", "#training", "#hallucinations", "#multimodal", "#rl"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–æ–µ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–±–µ–∂–¥–∞–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –ò–ò", "desc": "QueryBandits - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º—ã –º–Ω–æ–≥–æ—Ä—É–∫–∏—Ö –±–∞–Ω–¥–∏—Ç–æ–≤ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö 
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#benchmark", "#multimodal", "#science"], "emoji": "üß†", "ru": {"title": "–†–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –Ω–∞—É—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –Ø–ú", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ SciReas –∏ SciReas-Pro –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞—É—á–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è —É —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç
[27.08.2025 12:21] Using data from previous issue: {"categories": ["#architecture", "#science", "#training", "#interpretability", "#dataset"], "emoji": "üß†", "ru": {"title": "–°–µ—Ç–µ–≤–æ–π –∞–Ω–∞–ª–∏–∑ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ—Ç–µ–≤—É—é –º–æ–¥–µ–ª—å, —Å–≤—è–∑—ã–≤–∞—é—â—É—é –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –Ω–∞–≤—ã–∫–∏, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ
[27.08.2025 12:21] Renaming data file.
[27.08.2025 12:21] Renaming previous data. hf_papers.json to ./d/2025-08-27.json
[27.08.2025 12:21] Saving new data file.
[27.08.2025 12:21] Generating page.
[27.08.2025 12:21] Renaming previous page.
[27.08.2025 12:21] Renaming previous data. index.html to ./d/2025-08-27.html
[27.08.2025 12:21] Writing result.
[27.08.2025 12:21] Renaming log file.
[27.08.2025 12:21] Renaming previous data. log.txt to ./logs/2025-08-27_last_log.txt
