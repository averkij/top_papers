[27.08.2025 07:12] Read previous papers.
[27.08.2025 07:12] Generating top page (month).
[27.08.2025 07:12] Writing top page (month).
[27.08.2025 08:15] Read previous papers.
[27.08.2025 08:15] Get feed.
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18124
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19205
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17661
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19209
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18756
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17445
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19247
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.17437
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19242
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18621
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15774
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.15804
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18773
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18672
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18370
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.16697
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19188
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19026
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18271
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.19202
[27.08.2025 08:15] Get page data from previous paper. URL: https://huggingface.co/papers/2508.18192
[27.08.2025 08:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.08.2025 08:15] No deleted papers detected.
[27.08.2025 08:15] Downloading and parsing papers (pdf, html). Total: 21.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.18124.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.18124.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.18124.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.19205.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.19205.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.19205.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.17661.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.17661.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.17661.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.19209.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.19209.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.19209.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.18756.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.18756.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.18756.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.17445.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.17445.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.17445.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.19247.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.19247.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.19247.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.17437.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.17437.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.17437.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.19242.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.19242.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.19242.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.18621.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.18621.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.18621.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.15774.
[27.08.2025 08:15] Downloading paper 2508.15774 from http://arxiv.org/pdf/2508.15774v1...
[27.08.2025 08:15] Extracting affiliations from text.
[27.08.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 CineScale: Free Lunch in High-Resolution Cinematic Visual Generation Haonan Qiu*, Ning Yu(cid:66), Ziqi Huang, Paul Debevec, Ziwei Liu(cid:66) 5 2 0 2 1 ] . [ 1 4 7 7 5 1 . 8 0 5 2 : r AbstractVisual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of highresolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained these methods are still prone to producing lowmodels. However, quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/. Index TermsDiffusion Models, Image Generation, Video Generation, High Resolution Diffusi"
[27.08.2025 08:15] Response: ```python
[]
```
[27.08.2025 08:15] Extracting affiliations from text.
[27.08.2025 08:15] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 CineScale: Free Lunch in High-Resolution Cinematic Visual Generation Haonan Qiu*, Ning Yu(cid:66), Ziqi Huang, Paul Debevec, Ziwei Liu(cid:66) 5 2 0 2 1 ] . [ 1 4 7 7 5 1 . 8 0 5 2 : r AbstractVisual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of highresolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained these methods are still prone to producing lowmodels. However, quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/. Index TermsDiffusion Models, Image Generation, Video Generation, High ResolutionDiffusion models have revolutionized visual generation [1] [6], empowering individuals without any artistic expertise to effortlessly create distinctive and personalized designs, graphics, and short films using specific textual descriptions. Nonetheless, current visual diffusion models are generally trained on data with limited resolution, such as 5122 for SD 1.5 [7], 10242 for SDXL [1], and 320 512 for VideoCrafter2 [4], hampering their ability to generate highfidelity images or videos at higher resolutions. Given the scarcity of high-resolution visual data and the substantially greater model capacity required for modeling such *This work was done during an internship at Netflix Eyeline Studios, (cid:66)corresponding authors H. Qiu, Z. Huang, and Z. Liu are with Nanyang Technological University. Email: {HAONAN002, ZIQI002}@e.ntu.edu.sg, ziwei.liu@ntu.edu.sg H. Qiu, N. Yu, and P. Debevec are with Netflix Eyeline Studios. Email: {ning.yu, debevec}@scanlinevfx.com data, recent efforts have focused on employing tuning-free strategies for high-resolution visual generation to inherit the strong generation capacities of existing pre-trained diffusion models. Despite the advances achieved by existing methods, they are still prone to producing low-quality images or videos, particularly manifesting as repetitive object occurrences and unreasonable object structures. ScaleCrafter [8] puts forward that the primary cause of the object repetition issue is the limited convolutional receptive field and uses dilated convolutional layers to achieve tuning-free higher-resolution sampling. But the generated results of ScaleCrafter still suffer from the problem of local repetition. Inspired by MultiDiffusion [9] fusing the local patches of the whole images, DemoFusion [10] designed mechanism by fusing the local patches and global patches, almost eliminating the local repetition. Essentially, this solution just transfers the extra signal of the object to the background, leading to small object repetition generation. FouriScale [11] reduces those extra signals by removing the high-frequency signals of the latent before the convolution operation. Although FouriScale completely eliminates all types of repetition, the generated results always have weird colors and textures due to its violent editing on the frequency domain. To generate satisfactory visual contents without any unexpected repetition, we propose FreeScale, tuning-free inference paradigm that enables pre-trained image and video diffusion models to generate vivid higher-resolution results. Building on past effective modules [8], [12], we first propose tailored self-cascade upscaling and restrained dilated convolution to gain the basic visual structure and maintain the quality in higher-resolution generation. To further eliminate all kinds of unexpected object repetitions, FreeScale processes information from different receptive scales and then fuses it by extracting desired frequency components, ensuring both the structures overall rationality and the objects local quality. This fusion is smoothly integrated into the original self-attention layers, thereby bringing only minimal additional time overhead. Finally, we demonstrate the effectiveness of our model on both the text-to-image model and the text-to-video model, pushing the boundaries of image generation even up to an 8k resolution. Benefiting from the exceptional scalability, DiT has become the dominant architecture in the development of recent foundational diffusion models. Nevertheless, FreeScale and the majority of existing works are built upon the UNet 00000000/00$00.00 2021 IEEE JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST architecture. Due to the architectural gap, these methods exhibit limited effectiveness on DiT-based models. Specifically, the major challenge faced by DiT-based models in highresolution generation is the substantial increase in token count, which results in untrained positional embeddings and overly diluted attention, ultimately hindering generation quality. Indeed, these challenges have been thoroughly explored in large language models for long-text generation [13], [14], providing valuable empirical knowledge like NTK-aware interpolation and attention reweighting. Combining those technologies, we extend the original FreeScale framework by tailoring it to the architectural properties of DiT, yielding new variant that supports high-resolution generation on DiT-based models. Although tuning-free stra"
[27.08.2025 08:15] Mistral response. {"id": "986f9232c72543ab9dcc02dd1569ffb9", "created": 1756282537, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1383, "total_tokens": 1413, "completion_tokens": 30}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Netflix Eyeline Studios\",\n    \"Nanyang Technological University\",\n    \"Scanline VFX\"\n]\n```"}}]}
[27.08.2025 08:15] Response: ```python
[
    "Netflix Eyeline Studios",
    "Nanyang Technological University",
    "Scanline VFX"
]
```
[27.08.2025 08:15] Deleting PDF ./assets/pdf/2508.15774.pdf.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.15804.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.15804.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.15804.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.18773.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.18773.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.18773.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.18672.
[27.08.2025 08:15] Downloading paper 2508.18672 from http://arxiv.org/pdf/2508.18672v1...
[27.08.2025 08:15] Extracting affiliations from text.
[27.08.2025 08:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks Taishi Nakamura 1 2 Satoki Ishikawa 1 Masaki Kawamura 1 Takumi Okamoto 1 2 Daisuke Nohara 1 Jun Suzuki 3 2 4 Rio Yokota "
[27.08.2025 08:15] Response: []
[27.08.2025 08:15] Extracting affiliations from text.
[27.08.2025 08:15] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks Taishi Nakamura 1 2 Satoki Ishikawa 1 Masaki Kawamura 1 Takumi Okamoto 1 2 Daisuke Nohara 1 Jun Suzuki 3 2 4 Rio Yokota1. Introduction 5 2 0 2 6 2 ] . [ 1 2 7 6 8 1 . 8 0 5 2 : r Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization and reasoning. We train families of MoE Transformers that systematically vary total parameters, active parameters, and top-k routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap. Memorization benchmarks improve monotonically with total parameters, mirroring training loss. By contrast, reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top-k alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as sparsity. Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly sparse models. Our model checkpoints, code and logs are open-source at https://github.com/ rioyokotalab/optimal-sparsity. 1Institute of Science Tokyo, Tokyo, Japan 2Research and Development Center for Large Language Models, National Institute of Informatics, Tokyo, Japan 3Tohoku University, Sendai, Japan 4RIKEN, Tokyo, Japan. Correspondence to: Taishi Nakamura <taishi@rio.scrc.iir.isct.ac.jp>, Rio Yokota <rioyokota@rio.scrc.iir.isct.ac.jp>. 1 The recent evolution of large language models (LLMs) has been driven by empirical scaling laws (Hestness et al., 2017) that link training loss to model size, dataset size, and compute budget. Kaplan et al. showed that these laws hold across seven orders of magnitude, establishing them as reliable extrapolation tool for dense Transformers (Kaplan et al., 2020). Subsequent work by Hoffmann et al. demonstrated that scaling curves can be inverted to choose the compute-optimal combination of parameters and tokens for fixed budget (Hoffmann et al., 2022). Together, these results have made scaling analysis cornerstone of model planning at both academic and industrial labs. Yet the coefficients of the scaling laws are not universal. Highly expressive models trained under different optimizers or architectures often follow the same loss trajectory but diverge substantially on downstream reasoning benchmarks (Liu et al., 2023). Brandfonbrener et al. extend the classic laws with loss-to-loss prediction, showing that the mapping between training and test distributions admits its own power law when the distributions differ substantially (Brandfonbrener et al., 2025). These observations imply that optimal budgets must be re-estimated whenever we modify the model or the data pipeline. particularly compelling architectural modification is the Mixture-of-Experts (MoE) paradigm, offering high capacity at fixed FLOPs by routing each token through sparse subset of experts (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2021). Modern flagship models, e.g., Gemini 2.5 Pro (Gemini Team, 2025), DeepSeek-V3 (DeepSeek-AI, 2025b), and Qwen3 (Qwen Team, 2025) now rely on MoE as de-facto standard for economical scaling. Abnar et al. derive parameters-vs-FLOPs frontier and locate an optimal sparsity for given compute budget (Abnar et al., 2025). These findings emphasize that the classical dense-model frontier is an incomplete picture, and one must account for architectural knobs such as MoE sparsity and top-k routing. Furthermore, loss-based scaling curves do not always predict the performance on downstream tasks. Jelassi et al. report that increasing MoE sparsity improves memorization benchmarks, but saturates for reasoning performance (Jelassi et al., 2025). However, the Mixture of Parrots paper (JeOptimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks lassi et al., 2025) only explores the number of active vs. total parameters, ignoring the effect of routing strategies beyond standard top-2 routing. They also do not consider the effect of reinforcement learning and test-time compute on their reasoning benchmarks. Evaluating reasoning performance immediately after pre-training overlooks both the benefits of post-training adaptation and the leverage of additional testtime compute. Post-training methods such as GRPO, which use reinforcement signals to encourage coherent chain-ofthought generation, sharpen models reasoning on complex tasks (OpenAI, 2024b; DeepSeek-AI, 2025a). Beyond these refinements, models can further improve outputs at test time by adopting calibrated decoding strategies that mirror how humans pause to reconsider difficult problems. These test-time approaches not only boost routine benchmark performance but, when properly tuned, substantially enhance multi-step mathematical reasoning, demonstrating that adaptive computing at test time is powerful complement to both model scale and post-training adaptation. In this paper, we aim to identify how the optimal sparsity of MoE changes between memorization (TriviaQA, HellaSwag) and reasoning (GSM8K, GSM-Plus) tasks. In this work, we use the term dense models to refer to standard Transformers with single feed-forward network per layer. For MoE models, we define sparsity as sparsity = 1 Top-k Experts following the convention that sparsity measures the fraction of inactive parameters. We train families of MoEs varying not only the total vs. active parameters, but also the number of top-k experts. For each model, we measure the loss on the pre-training data, the task loss on the downstream benchmarks, and the accuracy on those benchmarks. This allows us to disentangle the generalization gap between the train vs. test loss, and the gap between loss vs. accuracy. For both memorization and reasoning benchmarks, the train loss decreases monotonically with the total parameters. The task loss and accuracy follow the same monotonic trend as the trai"
[27.08.2025 08:15] Mistral response. {"object": "error", "message": "Service tier capacity exceeded for this model.", "type": "service_tier_capacity_exceeded", "param": null, "code": "3505"}
[27.08.2025 08:15] Failed to download and parse paper https://huggingface.co/papers/2508.18672: 'choices'
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.18370.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.18370.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.18370.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.16697.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.16697.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.16697.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.19188.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.19188.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.19188.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.19026.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.19026.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.19026.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.18271.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.18271.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.18271.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.19202.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.19202.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.19202.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Downloading and parsing paper https://huggingface.co/papers/2508.18192.
[27.08.2025 08:15] Extra JSON file exists (./assets/json/2508.18192.json), skip PDF parsing.
[27.08.2025 08:15] Paper image links file exists (./assets/img_data/2508.18192.json), skip HTML parsing.
[27.08.2025 08:15] Success.
[27.08.2025 08:15] Enriching papers with extra data.
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 0. CMPhysBench evaluates LLMs in condensed matter physics using calculation problems and a new SEED score for partial credit assessment, revealing significant capability gaps.  					AI-generated summary 				 We introduce CMPhysBench, designed to assess the proficiency of Large Language Models (LLMs) in...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 1. VibeVoice synthesizes long-form multi-speaker speech using next-token diffusion and a highly efficient continuous speech tokenizer, achieving superior performance and fidelity.  					AI-generated summary 				 This report presents VibeVoice, a novel model designed to synthesize long-form speech with ...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 2. Spacer, a scientific discovery system, uses deliberate decontextualization to generate creative and factually grounded scientific concepts from keyword sets, achieving high accuracy and similarity to leading publications.  					AI-generated summary 				 Recent advances in LLMs have made automated sc...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 3. A framework using Multimodal Large Language Models and a specialized Multimodal DiT architecture generates semantically coherent and expressive character animations from multimodal inputs.  					AI-generated summary 				 Existing video avatar models can produce fluid human animations, yet they strug...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 4. UltraMemV2, a redesigned memory-layer architecture, achieves performance parity with 8-expert MoE models while significantly reducing memory access costs.  					AI-generated summary 				 While Mixture of Experts (MoE) models achieve remarkable efficiency by activating only subsets of parameters, the...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 5. TreePO, a self-guided rollout algorithm for sequence generation, reduces computational cost and enhances exploration diversity in reinforcement learning for large language models.  					AI-generated summary 				 Recent advancements in aligning large language models via reinforcement learning have ac...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 6. VoxHammer is a training-free method that performs precise and coherent 3D editing in latent space, ensuring consistency in preserved regions and high-quality overall results.  					AI-generated summary 				 3D local editing of specified regions is crucial for game industry and robot interaction. Rec...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 7. PIXIE, a neural network method, predicts physical properties of 3D scenes from visual features, enabling fast and realistic physics simulation using supervised learning and pretrained visual features.  					AI-generated summary 				 Inferring the physical properties of 3D scenes from visual informat...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 8. AUSM, an autoregressive universal segmentation model, unifies prompted and unprompted video segmentation by treating it as sequential mask prediction, achieving superior performance and faster training on standard benchmarks.  					AI-generated summary 				 Recent video foundation models such as SAM...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 9. Wan-S2V, an audio-driven model built on Wan, enhances expressiveness and fidelity in cinematic character animation compared to existing methods.  					AI-generated summary 				 Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenario...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 10. CineScale is a novel inference paradigm that enables high-resolution visual generation for both images and videos without extensive fine-tuning, addressing issues of repetitive patterns and high-frequency information accumulation.  					AI-generated summary 				 Visual diffusion models achieve remar...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 11. ReportBench evaluates the content quality of research reports generated by large language models, focusing on cited literature quality and statement faithfulness, demonstrating that commercial Deep Research agents produce more comprehensive and reliable reports than standalone LLMs.  					AI-generat...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 12. ThinkDial is an open-source framework that implements controllable reasoning in large language models through discrete operational modes, achieving performance while reducing computational effort.  					AI-generated summary 				 Large language models (LLMs) with chain-of-thought reasoning have demon...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 13. MoE models introduce sparsity that affects memorization and reasoning capabilities differently in large language models, with reasoning performance potentially regressing despite increased parameters.  					AI-generated summary 				 Empirical scaling laws have driven the evolution of large language ...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 14. CTF-Dojo, a large-scale executable runtime with 658 CTF challenges, enables rapid training of LLM-based agents with verifiable feedback, achieving state-of-the-art performance in competitive benchmarks.  					AI-generated summary 				 Large language models (LLMs) have demonstrated exceptional capabi...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 15. QueryBandits, a bandit framework, effectively mitigates hallucinations in LLMs by proactively rewriting queries based on linguistic features, outperforming static prompting strategies.  					AI-generated summary 				 Advanced reasoning capabilities in Large Language Models (LLMs) have caused higher ...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 16. A framework for efficient artistic mesh generation reduces redundancy by separating vertex and face generation, using an autoregressive model for vertices and a bidirectional transformer for faces, and includes a fidelity enhancer and post-processing to improve quality and speed.  					AI-generated ...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 17. MovieCORE is a video question answering dataset that uses multiple large language models to generate deep cognitive questions, and introduces an agentic enhancement module to improve VQA model performance.  					AI-generated summary 				 This paper introduces MovieCORE, a novel video question answer...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 18. ObjFiller-3D uses video editing models to achieve high-quality and consistent 3D object completion, outperforming previous methods in terms of reconstruction fidelity and practical deployment.  					AI-generated summary 				 3D inpainting often relies on multi-view 2D image inpainting, where the inh...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 19. SciReas and SciReas-Pro benchmarks, along with KRUX framework, provide insights into the distinct roles of knowledge and reasoning in scientific tasks, highlighting critical bottlenecks and improvements for LLMs.  					AI-generated summary 				 Scientific problem solving poses unique challenges for ...
[27.08.2025 08:15] ********************************************************************************
[27.08.2025 08:15] Abstract 20. A network-based framework links cognitive skills, LLM architectures, and datasets, revealing unique emergent skill patterns in LLMs that benefit from dynamic, cross-regional interactions.  					AI-generated summary 				 Large Language Models (LLMs) have reshaped our world with significant advancemen...
[27.08.2025 08:15] Read previous papers.
[27.08.2025 08:15] Generating reviews via LLM API.
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#benchmark", "#science", "#dataset"], "emoji": "🔬", "ru": {"title": "Новый вызов для ИИ: решение задач по физике конденсированного состояния", "desc": "CMPhysBench - это новый бенчмарк для оценки способностей больших языковых моделей (LLM) в области физики конденсированного состояни
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#open_source", "#long_context", "#diffusion", "#audio"], "emoji": "🗣️", "ru": {"title": "VibeVoice: революция в синтезе длительной многоголосой речи", "desc": "VibeVoice - это новая модель для синтеза длительной многоголосой речи, использующая диффузию следующего токена и эффективны
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#science", "#data", "#multimodal", "#dataset"], "emoji": "🧬", "ru": {"title": "Spacer: Революция в автоматизированных научных открытиях", "desc": "Spacer - это система научных открытий, использующая намеренную деконтекстуализацию для генерации креативных и фактически обоснованных на
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#architecture", "#interpretability", "#optimization", "#games", "#multimodal", "#video"], "emoji": "🎭", "ru": {"title": "Семантически осмысленная анимация персонажей с помощью мультимодального ИИ", "desc": "Эта статья представляет новую модель OmniHuman-1.5 для генерации анимации пе
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#architecture", "#long_context", "#optimization", "#training"], "emoji": "🧠", "ru": {"title": "UltraMemV2: Эффективность MoE без высоких затрат на память", "desc": "UltraMemV2 - это новая архитектура слоя памяти для нейронных сетей, которая достигает производительности 8-экспертных 
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#training", "#rl", "#rlhf"], "emoji": "🌳", "ru": {"title": "TreePO: Эффективное обучение с подкреплением для языковых моделей", "desc": "Статья представляет TreePO - алгоритм самонаправляемой генерации последовательностей для обучения с подкреплением б
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#games", "#dataset", "#synthetic", "#3d"], "emoji": "🔨", "ru": {"title": "Точное 3D-редактирование без компромиссов", "desc": "VoxHammer - это метод редактирования 3D-моделей в латентном пространстве без дополнительного обучения. Он обеспечивает точное и согласованное редактирование
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#inference", "#synthetic", "#3d"], "emoji": "🧠", "ru": {"title": "Быстрое предсказание физических свойств 3D-сцен с помощью нейронных сетей", "desc": "PIXIE - это нейросетевой метод, который предсказывает физические свойства 3D-сцен на основе визуальных 
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#architecture", "#training", "#video", "#optimization"], "emoji": "🎬", "ru": {"title": "Универсальная сегментация видео как последовательное предсказание масок", "desc": "AUSM - это модель универсальной сегментации видео, объединяющая подходы с подсказками и без них. Она рассматрива
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#story_generation", "#audio", "#games", "#benchmark", "#video"], "emoji": "🎭", "ru": {"title": "Wan-S2V: Революция в анимации кинематографических персонажей на основе аудио", "desc": "Модель Wan-S2V, основанная на аудио, улучшает выразительность и точность анимации кинематографическ
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#inference", "#cv", "#video"], "emoji": "🎬", "ru": {"title": "CineScale: Прорыв в генерации высококачественного визуального контента", "desc": "CineScale - это новая парадигма вывода, позволяющая генерировать изображения и видео высокого разрешения без 
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#survey", "#agents", "#open_source"], "emoji": "🔬", "ru": {"title": "ReportBench: новый стандарт оценки AI-исследований", "desc": "ReportBench - это новый метод оценки качества исследовательских отчетов, сгенерированных большими языковыми моделями 
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#architecture", "#open_source", "#training", "#optimization", "#reasoning", "#agi", "#rl"], "emoji": "🧠", "ru": {"title": "ThinkDial: контролируемое рассуждение в больших языковых моделях", "desc": "ThinkDial - это фреймворк с открытым исходным кодом, который реализует контролируемо
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#architecture", "#training", "#open_source"], "emoji": "🧠", "ru": {"title": "Разреженность в MoE: компромисс между запоминанием и рассуждением", "desc": "Статья исследует влияние разреженности в моделях Mixture-of-Experts (MoE) на способности больших я
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#open_source", "#training", "#dataset", "#games", "#benchmark", "#agents"], "emoji": "🏆", "ru": {"title": "CTF-Dojo: революция в обучении ИИ-агентов через исполняемую среду", "desc": "CTF-Dojo - это крупномасштабная исполняемая среда с 658 задачами CTF, позволяющая быстро обучать аг
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#reasoning", "#training", "#hallucinations", "#multimodal", "#rl"], "emoji": "🎯", "ru": {"title": "Умное переписывание запросов побеждает галлюцинации ИИ", "desc": "QueryBandits - это фреймворк, использующий алгоритмы многоруких бандитов для снижения галлюцинаций в больших языковых 
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#games", "#optimization", "#3d"], "emoji": "🎨", "ru": {"title": "Эффективная генерация 3D-моделей: разделяй и властвуй", "desc": "Предложена эффективная система генерации художественных 3D-моделей, разделяющая процессы создания вершин и граней. Для вершин используется авторегрессион
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#video", "#reasoning", "#alignment", "#agents", "#benchmark", "#dataset"], "emoji": "🎬", "ru": {"title": "MovieCORE: Глубокое понимание фильмов с помощью ИИ", "desc": "MovieCORE - это новый набор данных для ответов на вопросы по видео, который использует несколько больших языковых м
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#video", "#optimization", "#3d"], "emoji": "🎨", "ru": {"title": "Революция в 3D-инпейнтинге: от видео к реалистичным объектам", "desc": "ObjFiller-3D - это новый метод для заполнения и редактирования трехмерных объектов высокого качества. Он использует модели редактирования видео дл
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#benchmark", "#multimodal", "#science"], "emoji": "🧠", "ru": {"title": "Разделяй и властвуй: новый подход к оценке научного мышления ЯМ", "desc": "Статья представляет новые бенчмарки SciReas и SciReas-Pro для оценки научного мышления у языковых моделей. Авт
[27.08.2025 08:15] Using data from previous issue: {"categories": ["#architecture", "#science", "#training", "#interpretability", "#dataset"], "emoji": "🧠", "ru": {"title": "Сетевой анализ раскрывает когнитивные паттерны в языковых моделях", "desc": "Данная статья представляет сетевую модель, связывающую когнитивные навыки, архитектуры языковых моде
[27.08.2025 08:15] Renaming data file.
[27.08.2025 08:15] Renaming previous data. hf_papers.json to ./d/2025-08-27.json
[27.08.2025 08:15] Saving new data file.
[27.08.2025 08:15] Generating page.
[27.08.2025 08:15] Renaming previous page.
[27.08.2025 08:15] Renaming previous data. index.html to ./d/2025-08-27.html
[27.08.2025 08:15] Writing result.
[27.08.2025 08:15] Renaming log file.
[27.08.2025 08:15] Renaming previous data. log.txt to ./logs/2025-08-27_last_log.txt
