[20.10.2025 03:45] Read previous papers.
[20.10.2025 03:45] Generating top page (month).
[20.10.2025 03:45] Writing top page (month).
[20.10.2025 04:18] Read previous papers.
[20.10.2025 04:18] Get feed.
[20.10.2025 04:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15870
[20.10.2025 04:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15869
[20.10.2025 04:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15868
[20.10.2025 04:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15301
[20.10.2025 04:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14265
[20.10.2025 04:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15742
[20.10.2025 04:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.14438
[20.10.2025 04:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15857
[20.10.2025 04:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15564
[20.10.2025 04:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15232
[20.10.2025 04:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15831
[20.10.2025 04:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15262
[20.10.2025 04:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15110
[20.10.2025 04:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.15842
[20.10.2025 04:18] Extract page data from URL. URL: https://huggingface.co/papers/2510.15162
[20.10.2025 04:18] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[20.10.2025 04:18] No deleted papers detected.
[20.10.2025 04:18] Downloading and parsing papers (pdf, html). Total: 15.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.15870.
[20.10.2025 04:18] Extra JSON file exists (./assets/json/2510.15870.json), skip PDF parsing.
[20.10.2025 04:18] Paper image links file exists (./assets/img_data/2510.15870.json), skip HTML parsing.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.15869.
[20.10.2025 04:18] Extra JSON file exists (./assets/json/2510.15869.json), skip PDF parsing.
[20.10.2025 04:18] Paper image links file exists (./assets/img_data/2510.15869.json), skip HTML parsing.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.15868.
[20.10.2025 04:18] Extra JSON file exists (./assets/json/2510.15868.json), skip PDF parsing.
[20.10.2025 04:18] Paper image links file exists (./assets/img_data/2510.15868.json), skip HTML parsing.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.15301.
[20.10.2025 04:18] Extra JSON file exists (./assets/json/2510.15301.json), skip PDF parsing.
[20.10.2025 04:18] Paper image links file exists (./assets/img_data/2510.15301.json), skip HTML parsing.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.14265.
[20.10.2025 04:18] Extra JSON file exists (./assets/json/2510.14265.json), skip PDF parsing.
[20.10.2025 04:18] Paper image links file exists (./assets/img_data/2510.14265.json), skip HTML parsing.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.15742.
[20.10.2025 04:18] Extra JSON file exists (./assets/json/2510.15742.json), skip PDF parsing.
[20.10.2025 04:18] Paper image links file exists (./assets/img_data/2510.15742.json), skip HTML parsing.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.14438.
[20.10.2025 04:18] Extra JSON file exists (./assets/json/2510.14438.json), skip PDF parsing.
[20.10.2025 04:18] Paper image links file exists (./assets/img_data/2510.14438.json), skip HTML parsing.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.15857.
[20.10.2025 04:18] Extra JSON file exists (./assets/json/2510.15857.json), skip PDF parsing.
[20.10.2025 04:18] Paper image links file exists (./assets/img_data/2510.15857.json), skip HTML parsing.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.15564.
[20.10.2025 04:18] Extra JSON file exists (./assets/json/2510.15564.json), skip PDF parsing.
[20.10.2025 04:18] Paper image links file exists (./assets/img_data/2510.15564.json), skip HTML parsing.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.15232.
[20.10.2025 04:18] Extra JSON file exists (./assets/json/2510.15232.json), skip PDF parsing.
[20.10.2025 04:18] Paper image links file exists (./assets/img_data/2510.15232.json), skip HTML parsing.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.15831.
[20.10.2025 04:18] Extra JSON file exists (./assets/json/2510.15831.json), skip PDF parsing.
[20.10.2025 04:18] Paper image links file exists (./assets/img_data/2510.15831.json), skip HTML parsing.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.15262.
[20.10.2025 04:18] Extra JSON file exists (./assets/json/2510.15262.json), skip PDF parsing.
[20.10.2025 04:18] Paper image links file exists (./assets/img_data/2510.15262.json), skip HTML parsing.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.15110.
[20.10.2025 04:18] Extra JSON file exists (./assets/json/2510.15110.json), skip PDF parsing.
[20.10.2025 04:18] Paper image links file exists (./assets/img_data/2510.15110.json), skip HTML parsing.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.15842.
[20.10.2025 04:18] Extra JSON file exists (./assets/json/2510.15842.json), skip PDF parsing.
[20.10.2025 04:18] Paper image links file exists (./assets/img_data/2510.15842.json), skip HTML parsing.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Downloading and parsing paper https://huggingface.co/papers/2510.15162.
[20.10.2025 04:18] Downloading paper 2510.15162 from http://arxiv.org/pdf/2510.15162v1...
[20.10.2025 04:18] Extracting affiliations from text.
[20.10.2025 04:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Weizhi Wang1,2 Rongmei Lin2 Sanket Lokegaonkar2 Shiyang Li2 Colin Lockard2 Ritesh Sarkhel2 Jingbo Shang2,3 Xifeng Yan1 Nasser Zalmout2 Xian Li2 1UC Santa Barbara 2Amazon Stores Foundational AI 3UC San Diego "
[20.10.2025 04:18] Response: ```python
["UC Santa Barbara", "Amazon Stores Foundational AI", "UC San Diego"]
```
[20.10.2025 04:18] Deleting PDF ./assets/pdf/2510.15162.pdf.
[20.10.2025 04:18] Success.
[20.10.2025 04:18] Enriching papers with extra data.
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 0. OmniVinci, an open-source omni-modal LLM, enhances cross-modal understanding and performance across audio, vision, and robotics applications with innovative architecture and efficient data curation.  					AI-generated summary 				 Advancing machine intelligence requires developing the ability to per...
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 1. Skyfall-GS creates large-scale, high-quality 3D urban scenes using satellite imagery and diffusion models, offering real-time exploration and improved geometry and texture consistency.  					AI-generated summary 				 Synthesizing large-scale, explorable, and geometrically accurate 3D urban scenes is...
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 2. LightsOut enhances Single Image Flare Removal by reconstructing off-frame light sources using a diffusion-based outpainting framework, improving performance across challenging scenarios.  					AI-generated summary 				 Lens flare significantly degrades image quality, impacting critical computer visi...
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 3. SVG, a novel latent diffusion model without VAEs, uses self-supervised representations to enable efficient training, few-step sampling, and high-quality visual generation with semantic and discriminative capabilities.  					AI-generated summary 				 Recent progress in diffusion-based visual generati...
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 4. MorphoBench is a benchmark that evaluates large models' reasoning capabilities using multidisciplinary questions, adaptive difficulty, and simulation-generated questions.  					AI-generated summary 				 With the advancement of powerful large-scale reasoning models, effectively evaluating the reasoni...
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 5. Ditto framework addresses data scarcity in instruction-based video editing by generating a large dataset and using a curriculum learning strategy to train Editto, achieving superior instruction-following ability.  					AI-generated summary 				 Instruction-based video editing promises to democratize...
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 6. A new paradigm, Explore to Evolve, is proposed to enhance web agents' information aggregation by constructing a large dataset and developing foundation models that outperform existing models on a challenging benchmark.  					AI-generated summary 				 Deep research web agents not only retrieve inform...
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 7. BLIP3o-NEXT, a unified text-to-image generation and image editing model, uses an Autoregressive + Diffusion architecture to achieve superior performance and realism.  					AI-generated summary 				 We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3 series that advances the nex...
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 8. A vision-guided 3D layout generation system uses an image generation model and scene graphs to produce rich and coherent 3D scenes from prompts.  					AI-generated summary 				 Generating artistic and coherent 3D scene layouts is crucial in digital content creation. Traditional optimization-based me...
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 9. FinTrust is a benchmark designed to evaluate the trustworthiness of LLMs in finance applications, focusing on alignment issues and revealing gaps in legal awareness.  					AI-generated summary 				 Recent LLMs have demonstrated promising ability in solving finance related problems. However, applying...
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 10. VISTA, a multi-agent system, iteratively refines user prompts to enhance video quality and alignment with user intent, outperforming existing methods.  					AI-generated summary 				 Despite rapid advances in text-to-video synthesis, generated video quality remains critically dependent on precise us...
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 11. A new weight-decay scaling rule for AdamW is introduced to preserve sublayer gain across widths in modern scale-invariant architectures, enabling zero-shot transfer of learning rate and weight decay.  					AI-generated summary 				 Empirical scaling laws prescribe how to allocate parameters, data, a...
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 12. DLER, a reinforcement learning training recipe, improves the accuracy-efficiency trade-off in reasoning language models by addressing challenges in advantage estimation, entropy collapse, and sparse reward signals, leading to shorter outputs and better test-time scaling.  					AI-generated summary 	...
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 13. Paper2Web is a benchmark and evaluation framework for academic webpage generation, featuring PWAgent, an autonomous pipeline that enhances content and layout through MCP tools, outperforming end-to-end baselines.  					AI-generated summary 				 Academic project websites can more effectively dissemin...
[20.10.2025 04:18] ********************************************************************************
[20.10.2025 04:18] Abstract 14. UniFilter, a Unified Multimodal Data Quality Classifier, enhances Multimodal Large Language Models (MLLMs) by filtering high-quality image-text and interleaved data, leading to improved zero-shot reasoning and in-context learning.  					AI-generated summary 				 The Multimodal Large Language Models ...
[20.10.2025 04:18] Read previous papers.
[20.10.2025 04:18] Generating reviews via LLM API.
[20.10.2025 04:18] Using data from previous issue: {"categories": ["#architecture", "#open_source", "#robotics", "#reasoning", "#healthcare", "#multimodal", "#data", "#alignment"], "emoji": "🎭", "ru": {"title": "Видеть, слышать и понимать: омни-модальный AI с минимальными затратами", "desc": "OmniVinci — это open-source омни-модальная LLM, которая в
[20.10.2025 04:18] Using data from previous issue: {"categories": ["#diffusion", "#synthetic", "#3d"], "emoji": "🛰️", "ru": {"title": "Городские 3D-сцены из спутниковых снимков и диффузии", "desc": "Skyfall-GS — это первая система для создания городских 3D-сцен масштаба целого квартала без дорогостоящей 3D-разметки. Метод использует спутниковые сним
[20.10.2025 04:18] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#optimization", "#training"], "emoji": "💡", "ru": {"title": "Гасим свет: реконструкция источников бликов за пределами кадра", "desc": "Блики от объективов серьёзно ухудшают качество изображений и мешают работе систем компьютерного зрения, особенно в автономном в
[20.10.2025 04:18] Using data from previous issue: {"categories": ["#training", "#optimization", "#cv", "#diffusion"], "emoji": "🎨", "ru": {"title": "Генерация изображений через self-supervised представления без VAE", "desc": "Авторы представляют SVG — новую модель latent diffusion, которая отказывается от использования VAE (variational autoencoder)
[20.10.2025 04:18] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#benchmark"], "emoji": "🔄", "ru": {"title": "Адаптивный бенчмарк с эволюционирующей сложностью для оценки рассуждений LLM", "desc": "MorphoBench — это новый бенчмарк для оценки способностей к рассуждению у больших языковых моделей, который использует м
[20.10.2025 04:18] Using data from previous issue: {"categories": ["#data", "#agents", "#synthetic", "#training", "#dataset", "#cv"], "emoji": "🎬", "ru": {"title": "Миллион примеров для обучения редактированию видео по текстовым инструкциям", "desc": "Фреймворк Ditto решает проблему нехватки данных для редактирования видео по текстовым инструкциям. 
[20.10.2025 04:18] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#science", "#benchmark", "#agents"], "emoji": "🕷️", "ru": {"title": "Учимся не просто искать, а думать: веб-агенты, которые анализируют информацию", "desc": "Исследователи предложили новую парадигму Explore to Evolve для обучения веб-агентов не только нах
[20.10.2025 04:18] Using data from previous issue: {"categories": ["#architecture", "#rl", "#open_source", "#benchmark", "#diffusion", "#cv", "#training", "#multimodal"], "emoji": "🎨", "ru": {"title": "Объединение авторегрессии и диффузии для реалистичной генерации изображений", "desc": "BLIP3o-NEXT — это полностью open-source модель, объединяющая г
[20.10.2025 04:18] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#graphs", "#games", "#3d"], "emoji": "🎨", "ru": {"title": "Создание богатых 3D сцен с помощью визуально управляемой генерации", "desc": "В статье представлена новая система генерации 3D макетов, управляемая визуальными данными, которая использует модель 
[20.10.2025 04:18] Using data from previous issue: {"categories": ["#alignment", "#ethics", "#benchmark"], "emoji": "🏦", "ru": {"title": "FinTrust: проверка AI на надёжность в финансах", "desc": "Исследователи представили FinTrust - benchmark для оценки надёжности LLM в финансовых приложениях. Benchmark фокусируется на проблемах alignment и включает
[20.10.2025 04:18] Using data from previous issue: {"categories": ["#multimodal", "#games", "#video", "#optimization", "#agents", "#reasoning"], "emoji": "🎬", "ru": {"title": "Мультиагентная система для автоматического улучшения промптов в генерации видео", "desc": "В работе представлена VISTA — мультиагентная система, которая итеративно улучшает ка
[20.10.2025 04:18] Using data from previous issue: {"categories": ["#optimization", "#synthetic", "#transfer_learning", "#training", "#architecture"], "emoji": "⚖️", "ru": {"title": "Масштабирование weight decay для переноса гиперпараметров между разными размерами моделей", "desc": "Исследователи решили проблему переноса learning rate и weight decay
[20.10.2025 04:18] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#optimization", "#training"], "emoji": "✂️", "ru": {"title": "Короче и точнее: как научить LLM рассуждать эффективно", "desc": "Статья представляет DLER — метод обучения языковых моделей с reinforcement learning, который решает проблему избыточной длины рассужде
[20.10.2025 04:18] Using data from previous issue: {"categories": ["#science", "#dataset", "#agents", "#benchmark"], "emoji": "🌐", "ru": {"title": "Превращаем научные статьи в красивые интерактивные сайты с помощью AI-агента", "desc": "Статья представляет Paper2Web — бенчмарк и фреймворк для оценки генерации академических веб-страниц из научных стат
[20.10.2025 04:18] Querying the API.
[20.10.2025 04:18] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

UniFilter, a Unified Multimodal Data Quality Classifier, enhances Multimodal Large Language Models (MLLMs) by filtering high-quality image-text and interleaved data, leading to improved zero-shot reasoning and in-context learning.  					AI-generated summary 				 The Multimodal Large Language Models (MLLMs) are continually pre-trained on a mixture of image-text caption data and interleaved document data, while the high-quality data filtering towards image-text interleaved document data is under-explored. We propose to train an efficient MLLM as a Unified Mulitmodal Data Quality Classifier to Filter both high-quality image-text caption and interleaved data (UniFilter). To address the challenge of collecting diverse labeled multimodal data, we introduce a semi-synthetic approach that leverages readily available raw images and generates corresponding text across four quality levels. This method enables efficient creation of sample-score pairs for both caption and interleaved document data to train UniFilter. We apply UniFilter to curate high-quality caption data from DataComp caption dataset and interleaved data from the OBELICS image-text interleaved dataset. MLLMs pre-trained on the filtered data demonstrate significantly enhanced capabilities compared to those trained on baseline-filtered data, achieving stronger zero-shot reasoning and in-context learning capabilities. After visual supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger performance on various benchmarks, highlighting the downstream benefits of high-quality multimodal pre-training. We release the synthetic training data used for training UniFilter, the UniFilter model checkpoints, and the high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to the community for reproduction and further development.
[20.10.2025 04:18] Response: ```json
{
  "desc": "Статья представляет UniFilter - единый мультимодальный классификатор качества данных для улучшения Multimodal Large Language Models (MLLMs). Авторы предлагают полусинтетический подход для создания обучающих данных, генерируя тексты четырёх уровней качества к существующим изображениям. UniFilter фильтрует высококачественные пары изображение-текст и чередующиеся документы из датасетов DataComp и OBELICS. MLLMs, предобученные на отфильтрованных данных, демонстрируют значительное улучшение в zero-shot reasoning и in-context learning по сравнению с базовыми методами фильтрации.",
  "emoji": "🔍",
  "title": "Умная фильтрация мультимодальных данных для более сильных LLM"
}
```
[20.10.2025 04:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"UniFilter, a Unified Multimodal Data Quality Classifier, enhances Multimodal Large Language Models (MLLMs) by filtering high-quality image-text and interleaved data, leading to improved zero-shot reasoning and in-context learning.  					AI-generated summary 				 The Multimodal Large Language Models (MLLMs) are continually pre-trained on a mixture of image-text caption data and interleaved document data, while the high-quality data filtering towards image-text interleaved document data is under-explored. We propose to train an efficient MLLM as a Unified Mulitmodal Data Quality Classifier to Filter both high-quality image-text caption and interleaved data (UniFilter). To address the challenge of collecting diverse labeled multimodal data, we introduce a semi-synthetic approach that leverages readily available raw images and generates corresponding text across four quality levels. This method enables efficient creation of sample-score pairs for both caption and interleaved document data to train UniFilter. We apply UniFilter to curate high-quality caption data from DataComp caption dataset and interleaved data from the OBELICS image-text interleaved dataset. MLLMs pre-trained on the filtered data demonstrate significantly enhanced capabilities compared to those trained on baseline-filtered data, achieving stronger zero-shot reasoning and in-context learning capabilities. After visual supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger performance on various benchmarks, highlighting the downstream benefits of high-quality multimodal pre-training. We release the synthetic training data used for training UniFilter, the UniFilter model checkpoints, and the high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to the community for reproduction and further development."

[20.10.2025 04:18] Response: ```python
['DATASET', 'DATA', 'MULTIMODAL', 'TRAINING', 'BENCHMARK']
```
[20.10.2025 04:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"UniFilter, a Unified Multimodal Data Quality Classifier, enhances Multimodal Large Language Models (MLLMs) by filtering high-quality image-text and interleaved data, leading to improved zero-shot reasoning and in-context learning.  					AI-generated summary 				 The Multimodal Large Language Models (MLLMs) are continually pre-trained on a mixture of image-text caption data and interleaved document data, while the high-quality data filtering towards image-text interleaved document data is under-explored. We propose to train an efficient MLLM as a Unified Mulitmodal Data Quality Classifier to Filter both high-quality image-text caption and interleaved data (UniFilter). To address the challenge of collecting diverse labeled multimodal data, we introduce a semi-synthetic approach that leverages readily available raw images and generates corresponding text across four quality levels. This method enables efficient creation of sample-score pairs for both caption and interleaved document data to train UniFilter. We apply UniFilter to curate high-quality caption data from DataComp caption dataset and interleaved data from the OBELICS image-text interleaved dataset. MLLMs pre-trained on the filtered data demonstrate significantly enhanced capabilities compared to those trained on baseline-filtered data, achieving stronger zero-shot reasoning and in-context learning capabilities. After visual supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger performance on various benchmarks, highlighting the downstream benefits of high-quality multimodal pre-training. We release the synthetic training data used for training UniFilter, the UniFilter model checkpoints, and the high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to the community for reproduction and further development."

[20.10.2025 04:18] Response: ```python
['REASONING', 'SYNTHETIC', 'OPEN_SOURCE']
```
[20.10.2025 04:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"UniFilter is a novel approach that improves Multimodal Large Language Models (MLLMs) by filtering and selecting high-quality image-text and interleaved data. It addresses the challenge of obtaining diverse labeled multimodal data through a semi-synthetic method, generating text from raw images across different quality levels. By training the UniFilter model, researchers can enhance the quality of the data used for pre-training MLLMs, leading to better performance in zero-shot reasoning and in-context learning tasks. The results show that MLLMs trained on filtered data outperform those trained on standard datasets, demonstrating the importance of data quality in machine learning.","title":"Enhancing MLLMs with Quality Data Filtering"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='UniFilter is a novel approach that improves Multimodal Large Language Models (MLLMs) by filtering and selecting high-quality image-text and interleaved data. It addresses the challenge of obtaining diverse labeled multimodal data through a semi-synthetic method, generating text from raw images across different quality levels. By training the UniFilter model, researchers can enhance the quality of the data used for pre-training MLLMs, leading to better performance in zero-shot reasoning and in-context learning tasks. The results show that MLLMs trained on filtered data outperform those trained on standard datasets, demonstrating the importance of data quality in machine learning.', title='Enhancing MLLMs with Quality Data Filtering'))
[20.10.2025 04:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"UniFilter是一种统一的多模态数据质量分类器，旨在提升多模态大型语言模型（MLLMs）的性能。它通过过滤高质量的图像-文本和交错数据，改善了模型的零-shot推理和上下文学习能力。为了解决多样化标注多模态数据收集的挑战，UniFilter采用了一种半合成的方法，利用现成的原始图像生成对应的文本。经过过滤的数据训练的MLLMs在多个基准测试中表现出显著的增强能力，展示了高质量多模态预训练的下游好处。","title":"提升多模态模型的质量与能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='UniFilter是一种统一的多模态数据质量分类器，旨在提升多模态大型语言模型（MLLMs）的性能。它通过过滤高质量的图像-文本和交错数据，改善了模型的零-shot推理和上下文学习能力。为了解决多样化标注多模态数据收集的挑战，UniFilter采用了一种半合成的方法，利用现成的原始图像生成对应的文本。经过过滤的数据训练的MLLMs在多个基准测试中表现出显著的增强能力，展示了高质量多模态预训练的下游好处。', title='提升多模态模型的质量与能力'))
[20.10.2025 04:18] Renaming data file.
[20.10.2025 04:18] Renaming previous data. hf_papers.json to ./d/2025-10-20.json
[20.10.2025 04:18] Saving new data file.
[20.10.2025 04:18] Generating page.
[20.10.2025 04:18] Renaming previous page.
[20.10.2025 04:18] Renaming previous data. index.html to ./d/2025-10-20.html
[20.10.2025 04:18] Writing result.
[20.10.2025 04:18] Renaming log file.
[20.10.2025 04:18] Renaming previous data. log.txt to ./logs/2025-10-20_last_log.txt
