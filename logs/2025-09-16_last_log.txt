[16.09.2025 14:12] Read previous papers.
[16.09.2025 14:12] Generating top page (month).
[16.09.2025 14:12] Writing top page (month).
[16.09.2025 15:12] Read previous papers.
[16.09.2025 15:12] Get feed.
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.12201
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.11543
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.10813
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.12203
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.10708
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.09672
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.09658
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.10884
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.11986
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.11444
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.12132
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.11866
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.11648
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.11452
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.11362
[16.09.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.10844
[16.09.2025 15:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.09.2025 15:12] No deleted papers detected.
[16.09.2025 15:12] Downloading and parsing papers (pdf, html). Total: 16.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.12201.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.12201.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.12201.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.11543.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.11543.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.11543.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.10813.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.10813.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.10813.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.12203.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.12203.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.12203.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.10708.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.10708.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.10708.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.09672.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.09672.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.09672.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.09658.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.09658.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.09658.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.10884.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.10884.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.10884.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.11986.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.11986.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.11986.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.11444.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.11444.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.11444.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.12132.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.12132.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.12132.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.11866.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.11866.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.11866.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.11648.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.11648.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.11648.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.11452.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.11452.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.11452.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.11362.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.11362.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.11362.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2509.10844.
[16.09.2025 15:12] Extra JSON file exists (./assets/json/2509.10844.json), skip PDF parsing.
[16.09.2025 15:12] Paper image links file exists (./assets/img_data/2509.10844.json), skip HTML parsing.
[16.09.2025 15:12] Success.
[16.09.2025 15:12] Enriching papers with extra data.
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 0. OmniWorld, a large-scale, multi-domain, multi-modal dataset, addresses the limitations of existing 4D world modeling datasets and benchmarks, enabling significant performance improvements in 4D reconstruction and video generation.  					AI-generated summary 				 The field of 4D world modeling - aimi...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 1. Semi-online Reinforcement Learning addresses the limitations of offline and online RL by simulating online RL on offline trajectories, achieving state-of-the-art performance in dynamic benchmarks.  					AI-generated summary 				 Graphical User Interface (GUI) agents have demonstrated remarkable prog...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 2. InternScenes is a large-scale, diverse, and realistic indoor scene dataset that addresses limitations in existing datasets, enabling better scene layout generation and point-goal navigation.  					AI-generated summary 				 The advancement of Embodied AI heavily relies on large-scale, simulatable 3D ...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 3. LazyDrag, a drag-based image editing method for Multi-Modal Diffusion Transformers, eliminates implicit point matching, enabling precise geometric control and text guidance without test-time optimization.  					AI-generated summary 				 The reliance on implicit point matching via attention has becom...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 4. SearchInstruct enhances supervised fine-tuning datasets for large language models by expanding domain-specific questions and retrieving accurate answers, improving model performance and enabling efficient model editing.  					AI-generated summary 				 Supervised Fine-Tuning (SFT) is essential for tr...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 5. Research shows that locality in deep diffusion models is a statistical property of image datasets rather than an inductive bias of convolutional neural networks, leading to the development of a more accurate analytical denoiser.  					AI-generated summary 				 Among generative models, diffusion mode...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 6. HumbleBench evaluates multimodal large language models' ability to reject incorrect answers, addressing the issue of hallucinations in visual question answering and decision-making.  					AI-generated summary 				 Hallucinations in multimodal large language models (MLLMs) -- where the model generate...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 7. Nav-R1, an embodied foundation model, enhances navigation by integrating structured reasoning and decoupled control mechanisms, outperforming existing approaches on benchmarks and in real-world scenarios.  					AI-generated summary 				 Embodied navigation requires agents to integrate perception, re...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 8. Two approaches are introduced to analyze and quantify information loss in vision-language models during the projection of visual inputs into the language model's embedding space, revealing significant distortions and their impact on model performance.  					AI-generated summary 				 Vision--language...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 9. CognitiveSky, a transformer-based framework, analyzes sentiment, emotion, and narratives on Bluesky, providing insights through a dynamic dashboard and supporting various applications in computational social science.  					AI-generated summary 				 The emergence of decentralized social media platfor...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 10. Reflection-V enhances visual reasoning by constructing vision-centered data and using a visual attention reward model, improving reliance on visual information.  					AI-generated summary 				 Recent advances in text-only "slow-thinking" reasoning have prompted efforts to transfer this capability to...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 11. Dr.V, a hierarchical framework with Dr.V-Bench and Dr.V-Agent, addresses video hallucinations through fine-grained spatial-temporal grounding and cognitive reasoning, enhancing video understanding.  					AI-generated summary 				 Recent advancements in large video models (LVMs) have significantly en...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 12. EthicsMH is a dataset of 125 scenarios designed to evaluate AI systems' ethical reasoning in mental health contexts, focusing on decision accuracy, explanation quality, and alignment with professional norms.  					AI-generated summary 				 The deployment of large language models (LLMs) in mental hea...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 13. Dynamic reward weighting in multi-objective reinforcement learning adaptively adjusts weights during training to explore Pareto fronts effectively, outperforming fixed-weight scalarization methods.  					AI-generated summary 				 Prior works in multi-objective reinforcement learning typically use li...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 14. PersonaX, a multimodal dataset, combines behavioral traits, facial imagery, and biographical information to enable comprehensive analysis and causal reasoning using large language models.  					AI-generated summary 				 Understanding human behavior traits is central to applications in human-computer...
[16.09.2025 15:12] ********************************************************************************
[16.09.2025 15:12] Abstract 15. GAPrune, a pruning framework that considers domain importance and general linguistic foundation, effectively compresses models while maintaining and enhancing domain-specific performance.  					AI-generated summary 				 Domain-specific embedding models have shown promise for applications that requir...
[16.09.2025 15:12] Read previous papers.
[16.09.2025 15:12] Generating reviews via LLM API.
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#games", "#multimodal", "#dataset", "#video", "#synthetic", "#benchmark"], "emoji": "üåê", "ru": {"title": "OmniWorld: –ø—Ä–æ—Ä—ã–≤ –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ 4D-–º–∏—Ä–æ–≤", "desc": "OmniWorld - —ç—Ç–æ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –º–Ω–æ–≥–æ–¥–æ–º–µ–Ω–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è 4D-–º–∏—Ä–æ–≤. –û–Ω –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#rl", "#games", "#agents", "#reasoning", "#optimization", "#rlhf", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–ü–æ–ª—É–æ–Ω–ª–∞–π–Ω–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º: –ª—É—á—à–µ–µ –∏–∑ –¥–≤—É—Ö –º–∏—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º - –ø–æ–ª—É–æ–Ω–ª–∞–π–Ω–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ (Semi-onli
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#3d", "#data", "#games", "#dataset", "#training", "#open_source", "#benchmark"], "emoji": "üè†", "ru": {"title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∏–Ω—Ç–µ—Ä—å–µ—Ä—ã –¥–ª—è –ø—Ä–æ—Ä—ã–≤–∞ –≤ Embodied AI", "desc": "InternScenes - —ç—Ç–æ –Ω–æ–≤—ã–π –º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ –∏–Ω—Ç–µ—Ä—å–µ—Ä–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –æ–∫–æ–ª–æ 40 000 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#multimodal", "#cv", "#diffusion", "#benchmark"], "emoji": "üñºÔ∏è", "ru": {"title": "LazyDrag: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "LazyDrag - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. –û–Ω —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#training", "#open_source", "#data"], "emoji": "üîç", "ru": {"title": "SearchInstruct: –£–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "SearchInstruct - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#cv", "#diffusion", "#data"], "emoji": "üîç", "ru": {"title": "–õ–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö: —Å–≤–æ–π—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö, –∞ –Ω–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å –≤ –≥–ª—É–±–æ–∫–∏—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —è–≤–ª—è–µ—Ç—Å—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–º —Å–≤–æ–π—Å—Ç–≤–æ–º –Ω–∞–±–æ—Ä–æ
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#hallucinations", "#multimodal", "#dataset", "#open_source", "#benchmark"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–ø–∏—Å—Ç–µ–º–∏—á–µ—Å–∫–æ–π —Å–∫—Ä–æ–º–Ω–æ—Å—Ç–∏ MLLM", "desc": "HumbleBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –æ—Ç–≤–µ—Ä–≥–∞—Ç—å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#3d", "#rl", "#agents", "#reasoning", "#dataset", "#optimization", "#training"], "emoji": "üß≠", "ru": {"title": "Nav-R1: –ò–ò-–Ω–∞–≤–∏–≥–∞—Ç–æ—Ä –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º", "desc": "Nav-R1 - —ç—Ç–æ –º–æ–¥–µ–ª—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –≤ —Ç—Ä–µ—Ö–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –æ–±—ä–µ–¥–∏–Ω—è
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#cv", "#games"], "emoji": "üîç", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ—Ç–µ—Ä—å –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞ –∫ –∞–Ω–∞–ª–∏–∑—É –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–µ –ø–æ—Ç–µ—Ä–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏—Ö –∑—Ä–µ–Ω
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#data", "#healthcare", "#open_source", "#dataset", "#multimodal", "#science"], "emoji": "üß†", "ru": {"title": "–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –≤ –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å–æ—Ü—Å–µ—Ç—è—Ö —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "CognitiveSky - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π, —ç–º–æ—Ü–∏–π –∏ –Ω–∞—Ä—Ä–∞—Ç–∏–≤–æ–≤ –≤ —Å–æ
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#cv", "#agents", "#dataset", "#games", "#rl"], "emoji": "üß†", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò —á–µ—Ä–µ–∑ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ –∏ –≤–Ω–∏–º–∞–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å Reflection-V, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#video", "#hallucinations", "#interpretability", "#dataset", "#reasoning", "#benchmark"], "emoji": "üé•", "ru": {"title": "Dr.V: –õ–µ—á–∏–º –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π", "desc": "Dr.V - —ç—Ç–æ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è—Ö (LVM). –û–Ω–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –Ω–∞–±
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#benchmark", "#ethics", "#healthcare", "#alignment", "#dataset"], "emoji": "üß†", "ru": {"title": "–≠—Ç–∏—á–Ω—ã–π –ò–ò –≤ –ø—Å–∏—Ö–∏—á–µ—Å–∫–æ–º –∑–¥–æ—Ä–æ–≤—å–µ: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏", "desc": "–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –¥–∞—Ç–∞—Å–µ—Ç EthicsMH, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 125 —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º –∏
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#rl", "#rlhf", "#reasoning", "#training", "#optimization", "#alignment"], "emoji": "‚öñÔ∏è", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ —Ü–µ–ª–µ–π –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º - –¥–∏–Ω–∞–º–∏—á
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#multimodal", "#dataset", "#synthetic"], "emoji": "üß†", "ru": {"title": "PersonaX: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª–∏—á–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "PersonaX - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ —á–µ—Ä—Ç—ã, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ª–∏—Ü –∏ –±–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫—É
[16.09.2025 15:12] Using data from previous issue: {"categories": ["#training", "#inference", "#optimization"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –º–æ–¥–µ–ª–µ–π: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ–µ, –æ—Ç—Å–µ–∫–∞–µ–º –ª–∏—à–Ω–µ–µ", "desc": "GAPrune - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø—Ä—É–Ω–∏–Ω–≥–∞ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –¥–æ–º–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –∏ –æ–±—â–µ–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –æ—Å–Ω–æ
[16.09.2025 15:12] Renaming data file.
[16.09.2025 15:12] Renaming previous data. hf_papers.json to ./d/2025-09-16.json
[16.09.2025 15:12] Saving new data file.
[16.09.2025 15:12] Generating page.
[16.09.2025 15:12] Renaming previous page.
[16.09.2025 15:12] Renaming previous data. index.html to ./d/2025-09-16.html
[16.09.2025 15:12] Writing result.
[16.09.2025 15:12] Renaming log file.
[16.09.2025 15:12] Renaming previous data. log.txt to ./logs/2025-09-16_last_log.txt
