[14.07.2025 03:27] Read previous papers.
[14.07.2025 03:27] Generating top page (month).
[14.07.2025 03:27] Writing top page (month).
[14.07.2025 04:34] Read previous papers.
[14.07.2025 04:34] Get feed.
[14.07.2025 04:34] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08776
[14.07.2025 04:34] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01951
[14.07.2025 04:34] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08800
[14.07.2025 04:34] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08801
[14.07.2025 04:34] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08772
[14.07.2025 04:34] Extract page data from URL. URL: https://huggingface.co/papers/2507.08794
[14.07.2025 04:34] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07151
[14.07.2025 04:34] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.07.2025 04:34] No deleted papers detected.
[14.07.2025 04:34] Downloading and parsing papers (pdf, html). Total: 7.
[14.07.2025 04:34] Downloading and parsing paper https://huggingface.co/papers/2507.08776.
[14.07.2025 04:34] Extra JSON file exists (./assets/json/2507.08776.json), skip PDF parsing.
[14.07.2025 04:34] Paper image links file exists (./assets/img_data/2507.08776.json), skip HTML parsing.
[14.07.2025 04:34] Success.
[14.07.2025 04:34] Downloading and parsing paper https://huggingface.co/papers/2507.01951.
[14.07.2025 04:34] Extra JSON file exists (./assets/json/2507.01951.json), skip PDF parsing.
[14.07.2025 04:34] Paper image links file exists (./assets/img_data/2507.01951.json), skip HTML parsing.
[14.07.2025 04:34] Success.
[14.07.2025 04:34] Downloading and parsing paper https://huggingface.co/papers/2507.08800.
[14.07.2025 04:34] Extra JSON file exists (./assets/json/2507.08800.json), skip PDF parsing.
[14.07.2025 04:34] Paper image links file exists (./assets/img_data/2507.08800.json), skip HTML parsing.
[14.07.2025 04:34] Success.
[14.07.2025 04:34] Downloading and parsing paper https://huggingface.co/papers/2507.08801.
[14.07.2025 04:34] Extra JSON file exists (./assets/json/2507.08801.json), skip PDF parsing.
[14.07.2025 04:34] Paper image links file exists (./assets/img_data/2507.08801.json), skip HTML parsing.
[14.07.2025 04:34] Success.
[14.07.2025 04:34] Downloading and parsing paper https://huggingface.co/papers/2507.08772.
[14.07.2025 04:34] Extra JSON file exists (./assets/json/2507.08772.json), skip PDF parsing.
[14.07.2025 04:34] Paper image links file exists (./assets/img_data/2507.08772.json), skip HTML parsing.
[14.07.2025 04:34] Success.
[14.07.2025 04:34] Downloading and parsing paper https://huggingface.co/papers/2507.08794.
[14.07.2025 04:34] Downloading paper 2507.08794 from http://arxiv.org/pdf/2507.08794v1...
[14.07.2025 04:34] Extracting affiliations from text.
[14.07.2025 04:34] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"One Token to Fool LLM-as-a-Judge One Token to Fool LLM-as-a-Judge Yulai Zhao ,1,2 , Haolin Liu,1,3 , Dian Yu1 , S.Y. Kung2 , Haitao Mi1 , and Dong Yu1 1Tencent AI Lab 2Princeton University 3University of Virginia "
[14.07.2025 04:34] Response: ```python
["Tencent AI Lab", "Princeton University", "University of Virginia"]
```
[14.07.2025 04:34] Deleting PDF ./assets/pdf/2507.08794.pdf.
[14.07.2025 04:34] Success.
[14.07.2025 04:34] Downloading and parsing paper https://huggingface.co/papers/2507.07151.
[14.07.2025 04:34] Downloading paper 2507.07151 from http://arxiv.org/pdf/2507.07151v1...
[14.07.2025 04:34] Extracting affiliations from text.
[14.07.2025 04:34] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Zongmeng Zhang 1 Wengang Zhou 2 Jie Zhao 3 Houqiang Li "
[14.07.2025 04:34] Response: []
[14.07.2025 04:34] Extracting affiliations from text.
[14.07.2025 04:34] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Zongmeng Zhang 1 Wengang Zhou 2 Jie Zhao 3 Houqiang LiDespite the impressive capabilities of multimodal large language models (MLLMs) in visionlanguage tasks, they are prone to hallucinations in real-world scenarios. This paper investigates the hallucination phenomenon in MLLMs from the perspective of modality conflict. Unlike existing works focusing on the conflicts between model responses and inputs, we study the inherent conflicts in inputs from different modalities that place MLLMs in dilemma and directly lead to hallucinations. We formally define the modality conflict and construct dataset named Multimodal Modality Conflict (MMMC) to simulate this phenomenon in vision-language tasks. Three methods based on prompt engineering, supervised finetuning, and reinforcement learning are proposed to alleviate the hallucination caused by modality conflict. Extensive experiments are conducted on the MMMC dataset to analyze the merits and demerits of these methods. Our results show that the reinforcement learning method achieves the best performance in mitigating the hallucination under modality conflict, while the supervised finetuning method shows promising and stable performance. Our work sheds light on the unnoticed modality conflict that leads to hallucinations and provides more insights into the robustness of MLLMs. The code and dataset are available at https://github.com/zmzhang2000/ MMMC. 5 2 0 2 9 ] . [ 1 1 5 1 7 0 . 7 0 5 2 : r 1School of Artificial Intelligence and Data Science, University of Science and Technology of China 2Department of Electronic Engineering and Information Science, University of Science and Technology of China 3Huawei Technologies Co., Ltd.. Correspondence to: Wengang Zhou <zhwg@ustc.edu.cn>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 Figure 1. An example of modality conflict in vision-language tasks. Given an image describing dog surfing on the sea, the user may ask the question What color is the ball?. The model may hallucinate response The ball in the image is green, while there is no ball in the image. We expect the model to recognize the conflict between the visual input and the textual input and give response like The image does not contain ball. 1. Introduction The recent success of multimodal large language models (MLLMs) has advanced the development of artificial intelligence in vision-language tasks (Dai et al., 2023; Liu et al., 2023; 2024b; Bai et al., 2023; Wang et al., 2024). These models enable the joint reasoning over visual and textual inputs, and have achieved state-of-the-art performance in various vision-language tasks that require multimodal reasoning (Fu et al., 2024; Yue et al., 2024; Yu et al., 2024c; Lu et al., 2024; Liu et al., 2025). The powerful capabilities of these MLLMs are typically achieved by pretraining separate language and vision models on large-scale datasets, and then Robust Multimodal Large Language Models Against Modality Conflict aligning their features to enable multimodal reasoning (Liu et al., 2023; 2024b). Despite the impressive performance of MLLMs, they are prone to hallucinations in real-world scenarios (Huang et al., 2024; Yu et al., 2024b). Hallucinations refer to the phenomenon where MLLMs generate incorrect or misleading information not supported by the input data (Ji et al., 2023). Existing works have proposed various methods to alleviate hallucinations in MLLMs, such as improving the quality of training data (Liu et al., 2024a; Yu et al., 2024a), adjusting the decoding strategies (Leng et al., 2024; Huang et al., 2024), and align the model with human preference (Zhao et al., 2024; Yu et al., 2024b). These methods mainly target more precise alignment between the features of different modalities to reduce hallucinations. However, existing works on alleviating hallucinations in MLLMs mainly focus on the conflicts between the model responses and the inputs, neglecting possible source of hallucinations: the conflicts between the inputs from different modalities, which we call modality conflict. For instance, as shown in Figure 1, given an image describing dog surfing on the sea, the user may ask the question What color is the ball?. In this case, the question supposes ball exists in the image, and the model may hallucinate response The ball in the image is green, while there is no ball in the image. We expect the model to recognize the conflict between the visual input and the textual input and give response like The image does not contain ball. Even with the capability of perfectly aligning features of different modalities, MLLMs may still fall into dilemma when facing such intrinsically conflicted information between inputs. To this end, we aim to investigate such hallucination phenomenon in MLLMs from the perspective of modality conflict. In this paper, we first give formal definition of modality conflict in vision-language tasks in terms of objects, attributes, and relationships in the visual and textual inputs. Based on the definition, we construct dataset named MultiModal Modality Conflict (MMMC) to simulate the modality conflict in vision-language tasks. We evaluate various prevalent MLLMs (Dai et al., 2023; Liu et al., 2024b; Bai et al., 2023; Wang et al., 2024) on the MMMC dataset and find that most of them lack the ability to recognize the modality conflict and are prone to hallucinations. To alleviate the hallucination caused by the modality conflict and work towards more robust MLLMs, we investigate the effectiveness of three methods: prompt engineering, supervised fine-tuning, and reinforcement learning. We conduct extensive experiments on the MMMC dataset to analyze the merits and demerits of these methods. Our results show that the reinforcement learning method achieves the best performance in mitigating the hallucination under modality conflict, while the supervised fine-tuning method shows promising and stable performance. Our work sheds light on the unnoticed modality conflict that causes hallucinations and provides more insights into the robustness of MLLMs. To summarize, the contributions of this paper are as follows: This paper reveals an unnoticed source of hallucinations in MLLMs: modality conflict. The formal definition of modality conflict is presented in the level of objects, attributes, and relationships. We construct dataset called Multimodal Modality Conflict (MMMC) to simulate t"
[14.07.2025 04:34] Mistral response. {"id": "66d65ab3ef0c4ad097bf677505b47686", "object": "chat.completion", "created": 1752467662, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"School of Artificial Intelligence and Data Science, University of Science and Technology of China\",\n    \"Department of Electronic Engineering and Information Science, University of Science and Technology of China\",\n    \"Huawei Technologies Co., Ltd.\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1670, "total_tokens": 1734, "completion_tokens": 64}}
[14.07.2025 04:34] Response: ```python
[
    "School of Artificial Intelligence and Data Science, University of Science and Technology of China",
    "Department of Electronic Engineering and Information Science, University of Science and Technology of China",
    "Huawei Technologies Co., Ltd."
]
```
[14.07.2025 04:34] Deleting PDF ./assets/pdf/2507.07151.pdf.
[14.07.2025 04:34] Success.
[14.07.2025 04:34] Enriching papers with extra data.
[14.07.2025 04:34] ********************************************************************************
[14.07.2025 04:34] Abstract 0. A neural rendering method uses compressed light-field tokens to efficiently represent scenes and render novel views with varying compute budgets.  					AI-generated summary 				 This paper proposes a neural rendering approach that represents a scene as "compressed light-field tokens (CLiFTs)", retai...
[14.07.2025 04:34] ********************************************************************************
[14.07.2025 04:34] Abstract 1. MetaStone-S1, a reflective generative model using a self-supervised process reward model, achieves efficient reasoning and scalable performance with fewer parameters compared to existing models.  					AI-generated summary 				 We introduce our first reflective generative model MetaStone-S1, which ob...
[14.07.2025 04:34] ********************************************************************************
[14.07.2025 04:34] Abstract 2. NeuralOS uses a combination of RNNs and diffusion-based rendering to simulate OS GUIs by predicting screen frames from user inputs, demonstrating realistic GUI rendering and state transitions.  					AI-generated summary 				 We introduce NeuralOS, a neural framework that simulates graphical user int...
[14.07.2025 04:34] ********************************************************************************
[14.07.2025 04:34] Abstract 3. Lumos-1 is an autoregressive video generator that uses a modified LLM architecture with MM-RoPE and AR-DF to address spatiotemporal correlation and frame-wise loss imbalance, achieving competitive performance with fewer resources.  					AI-generated summary 				 Autoregressive large language models ...
[14.07.2025 04:34] ********************************************************************************
[14.07.2025 04:34] Abstract 4. A part-aware diffusion framework, CoPart, enhances 3D generation by decomposing objects into contextual parts, improving complexity handling, relationship modeling, and part-level conditioning.  					AI-generated summary 				 Recent advances in 3D generation have transitioned from multi-view 2D rend...
[14.07.2025 04:34] ********************************************************************************
[14.07.2025 04:34] Abstract 5. Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.  					AI-generated summary 				 Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are incre...
[14.07.2025 04:34] ********************************************************************************
[14.07.2025 04:34] Abstract 6. Investigation of modality conflict in multimodal large language models reveals its role in causing hallucinations, with reinforcement learning emerging as the most effective mitigation strategy.  					AI-generated summary 				 Despite the impressive capabilities of multimodal large language models (...
[14.07.2025 04:34] Read previous papers.
[14.07.2025 04:34] Generating reviews via LLM API.
[14.07.2025 04:34] Using data from previous issue: {"categories": ["#benchmark", "#3d", "#dataset"], "emoji": "ğŸ¥", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¶Ğ°Ñ‚Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ", "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ (CLiFTs) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½. Ğœ
[14.07.2025 04:34] Using data from previous issue: {"categories": ["#training", "#open_source", "#architecture", "#rl", "#small_models", "#reasoning"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸: MetaStone-S1 Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸", "desc": "MetaStone-S1 - ÑÑ‚Ğ¾ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰
[14.07.2025 04:34] Using data from previous issue: {"categories": ["#games", "#diffusion", "#dataset", "#agents", "#multimodal", "#cv"], "emoji": "ğŸ–¥ï¸", "ru": {"title": "ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ GUI: ÑˆĞ°Ğ³ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾", "desc": "NeuralOS - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ
[14.07.2025 04:34] Using data from previous issue: {"categories": ["#training", "#games", "#architecture", "#video", "#optimization", "#multimodal"], "emoji": "ğŸ¬", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Lumos-1 - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚
[14.07.2025 04:34] Using data from previous issue: {"categories": ["#games", "#3d", "#diffusion", "#dataset"], "emoji": "ğŸ§©", "ru": {"title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²", "desc": "CoPart - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸, 
[14.07.2025 04:34] Querying the API.
[14.07.2025 04:34] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.  					AI-generated summary 				 Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning openers like "Thought process:" and "Let's solve this problem step by step." can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at https://huggingface.co/sarosavo/Master-RM and https://huggingface.co/datasets/sarosavo/Master-RM.
[14.07.2025 04:34] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ğ±Ğ¼Ğ°Ğ½ÑƒÑ‚Ñ‹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ñ„Ñ€Ğ°Ğ·-Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM.",
  "emoji": "ğŸ›¡ï¸",
  "title": "Ğ£ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹"
}
[14.07.2025 04:34] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.  					AI-generated summary 				 Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning openers like "Thought process:" and "Let's solve this problem step by step." can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at https://huggingface.co/sarosavo/Master-RM and https://huggingface.co/datasets/sarosavo/Master-RM."

[14.07.2025 04:34] Response: ```python
['RL', 'RLHF', 'DATASET', 'DATA', 'TRAINING']
```
[14.07.2025 04:34] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.  					AI-generated summary 				 Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning openers like "Thought process:" and "Let's solve this problem step by step." can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at https://huggingface.co/sarosavo/Master-RM and https://huggingface.co/datasets/sarosavo/Master-RM."

[14.07.2025 04:34] Response: ```python
["HALLUCINATIONS", "OPTIMIZATION", "SYNTHETIC"]
```
[14.07.2025 04:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the vulnerabilities of generative reward models, which use large language models (LLMs) to assess the quality of answers in reinforcement learning scenarios. The authors reveal that these models can be easily tricked by superficial changes in the input, such as adding non-word symbols or specific phrases, leading to incorrect evaluations. To address this issue, they propose a data augmentation strategy that enhances the robustness of the generative reward models against such manipulations. The study emphasizes the importance of developing more reliable evaluation methods for LLMs in reinforcement learning applications.","title":"Strengthening LLMs: Combatting Superficial Manipulations in Reward Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the vulnerabilities of generative reward models, which use large language models (LLMs) to assess the quality of answers in reinforcement learning scenarios. The authors reveal that these models can be easily tricked by superficial changes in the input, such as adding non-word symbols or specific phrases, leading to incorrect evaluations. To address this issue, they propose a data augmentation strategy that enhances the robustness of the generative reward models against such manipulations. The study emphasizes the importance of developing more reliable evaluation methods for LLMs in reinforcement learning applications.', title='Strengthening LLMs: Combatting Superficial Manipulations in Reward Models'))
[14.07.2025 04:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆLLMsä½œä¸ºè¯„åˆ¤è€…ï¼‰åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°ç­”æ¡ˆè´¨é‡æ—¶ï¼Œå®¹æ˜“å—åˆ°è¡¨é¢æ“æ§çš„å½±å“ã€‚å°½ç®¡è¿™ç§æ¯”è¾ƒä»»åŠ¡çœ‹ä¼¼ç®€å•ï¼Œä½†æˆ‘ä»¬å‘ç°ç”Ÿæˆå¥–åŠ±æ¨¡å‹åœ¨é¢å¯¹éå•è¯ç¬¦å·æˆ–æ¨ç†å¼€å¤´è¯æ—¶ï¼Œå¸¸å¸¸ä¼šäº§ç”Ÿé”™è¯¯çš„æ­£å‘å¥–åŠ±ã€‚è¿™ç§è„†å¼±æ€§åœ¨ä¸åŒçš„LLMã€æ•°æ®é›†å’Œæç¤ºæ ¼å¼ä¸­æ™®éå­˜åœ¨ï¼Œå¨èƒåˆ°ä¾èµ–ç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„æ ¸å¿ƒç®—æ³•èŒƒå¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œè®­ç»ƒå‡ºä¸€ç§å…·æœ‰æ˜¾è‘—æ”¹è¿›é²æ£’æ€§çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹ã€‚","title":"æå‡ç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆLLMsä½œä¸ºè¯„åˆ¤è€…ï¼‰åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°ç­”æ¡ˆè´¨é‡æ—¶ï¼Œå®¹æ˜“å—åˆ°è¡¨é¢æ“æ§çš„å½±å“ã€‚å°½ç®¡è¿™ç§æ¯”è¾ƒä»»åŠ¡çœ‹ä¼¼ç®€å•ï¼Œä½†æˆ‘ä»¬å‘ç°ç”Ÿæˆå¥–åŠ±æ¨¡å‹åœ¨é¢å¯¹éå•è¯ç¬¦å·æˆ–æ¨ç†å¼€å¤´è¯æ—¶ï¼Œå¸¸å¸¸ä¼šäº§ç”Ÿé”™è¯¯çš„æ­£å‘å¥–åŠ±ã€‚è¿™ç§è„†å¼±æ€§åœ¨ä¸åŒçš„LLMã€æ•°æ®é›†å’Œæç¤ºæ ¼å¼ä¸­æ™®éå­˜åœ¨ï¼Œå¨èƒåˆ°ä¾èµ–ç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„æ ¸å¿ƒç®—æ³•èŒƒå¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œè®­ç»ƒå‡ºä¸€ç§å…·æœ‰æ˜¾è‘—æ”¹è¿›é²æ£’æ€§çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹ã€‚', title='æå‡ç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„é²æ£’æ€§'))
[14.07.2025 04:34] Using data from previous issue: {"categories": ["#training", "#interpretability", "#dataset", "#rl", "#hallucinations", "#multimodal"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑÑ…", "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM) Ğ¸Ğ·-Ğ·Ğ° 
[14.07.2025 04:34] Renaming data file.
[14.07.2025 04:34] Renaming previous data. hf_papers.json to ./d/2025-07-14.json
[14.07.2025 04:34] Saving new data file.
[14.07.2025 04:34] Generating page.
[14.07.2025 04:34] Renaming previous page.
[14.07.2025 04:34] Renaming previous data. index.html to ./d/2025-07-14.html
[14.07.2025 04:34] Writing result.
[14.07.2025 04:34] Renaming log file.
[14.07.2025 04:34] Renaming previous data. log.txt to ./logs/2025-07-14_last_log.txt
