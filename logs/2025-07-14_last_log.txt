[14.07.2025 11:12] Read previous papers.
[14.07.2025 11:12] Generating top page (month).
[14.07.2025 11:12] Writing top page (month).
[14.07.2025 12:23] Read previous papers.
[14.07.2025 12:23] Get feed.
[14.07.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01951
[14.07.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08776
[14.07.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08800
[14.07.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08799
[14.07.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05397
[14.07.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08801
[14.07.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2507.05255
[14.07.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08772
[14.07.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08794
[14.07.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08441
[14.07.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06952
[14.07.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2507.06261
[14.07.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08771
[14.07.2025 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07151
[14.07.2025 12:23] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.07.2025 12:23] No deleted papers detected.
[14.07.2025 12:23] Downloading and parsing papers (pdf, html). Total: 14.
[14.07.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2507.01951.
[14.07.2025 12:23] Extra JSON file exists (./assets/json/2507.01951.json), skip PDF parsing.
[14.07.2025 12:23] Paper image links file exists (./assets/img_data/2507.01951.json), skip HTML parsing.
[14.07.2025 12:23] Success.
[14.07.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2507.08776.
[14.07.2025 12:23] Extra JSON file exists (./assets/json/2507.08776.json), skip PDF parsing.
[14.07.2025 12:23] Paper image links file exists (./assets/img_data/2507.08776.json), skip HTML parsing.
[14.07.2025 12:23] Success.
[14.07.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2507.08800.
[14.07.2025 12:23] Extra JSON file exists (./assets/json/2507.08800.json), skip PDF parsing.
[14.07.2025 12:23] Paper image links file exists (./assets/img_data/2507.08800.json), skip HTML parsing.
[14.07.2025 12:23] Success.
[14.07.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2507.08799.
[14.07.2025 12:23] Extra JSON file exists (./assets/json/2507.08799.json), skip PDF parsing.
[14.07.2025 12:23] Paper image links file exists (./assets/img_data/2507.08799.json), skip HTML parsing.
[14.07.2025 12:23] Success.
[14.07.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2507.05397.
[14.07.2025 12:23] Extra JSON file exists (./assets/json/2507.05397.json), skip PDF parsing.
[14.07.2025 12:23] Paper image links file exists (./assets/img_data/2507.05397.json), skip HTML parsing.
[14.07.2025 12:23] Success.
[14.07.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2507.08801.
[14.07.2025 12:23] Extra JSON file exists (./assets/json/2507.08801.json), skip PDF parsing.
[14.07.2025 12:23] Paper image links file exists (./assets/img_data/2507.08801.json), skip HTML parsing.
[14.07.2025 12:23] Success.
[14.07.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2507.05255.
[14.07.2025 12:23] Extra JSON file exists (./assets/json/2507.05255.json), skip PDF parsing.
[14.07.2025 12:23] Paper image links file exists (./assets/img_data/2507.05255.json), skip HTML parsing.
[14.07.2025 12:23] Success.
[14.07.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2507.08772.
[14.07.2025 12:23] Extra JSON file exists (./assets/json/2507.08772.json), skip PDF parsing.
[14.07.2025 12:23] Paper image links file exists (./assets/img_data/2507.08772.json), skip HTML parsing.
[14.07.2025 12:23] Success.
[14.07.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2507.08794.
[14.07.2025 12:23] Extra JSON file exists (./assets/json/2507.08794.json), skip PDF parsing.
[14.07.2025 12:23] Paper image links file exists (./assets/img_data/2507.08794.json), skip HTML parsing.
[14.07.2025 12:23] Success.
[14.07.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2507.08441.
[14.07.2025 12:23] Extra JSON file exists (./assets/json/2507.08441.json), skip PDF parsing.
[14.07.2025 12:23] Paper image links file exists (./assets/img_data/2507.08441.json), skip HTML parsing.
[14.07.2025 12:23] Success.
[14.07.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2507.06952.
[14.07.2025 12:23] Extra JSON file exists (./assets/json/2507.06952.json), skip PDF parsing.
[14.07.2025 12:23] Paper image links file exists (./assets/img_data/2507.06952.json), skip HTML parsing.
[14.07.2025 12:23] Success.
[14.07.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2507.06261.
[14.07.2025 12:23] Extra JSON file exists (./assets/json/2507.06261.json), skip PDF parsing.
[14.07.2025 12:23] Paper image links file exists (./assets/img_data/2507.06261.json), skip HTML parsing.
[14.07.2025 12:23] Success.
[14.07.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2507.08771.
[14.07.2025 12:23] Extra JSON file exists (./assets/json/2507.08771.json), skip PDF parsing.
[14.07.2025 12:23] Paper image links file exists (./assets/img_data/2507.08771.json), skip HTML parsing.
[14.07.2025 12:23] Success.
[14.07.2025 12:23] Downloading and parsing paper https://huggingface.co/papers/2507.07151.
[14.07.2025 12:23] Extra JSON file exists (./assets/json/2507.07151.json), skip PDF parsing.
[14.07.2025 12:23] Paper image links file exists (./assets/img_data/2507.07151.json), skip HTML parsing.
[14.07.2025 12:23] Success.
[14.07.2025 12:23] Enriching papers with extra data.
[14.07.2025 12:23] ********************************************************************************
[14.07.2025 12:23] Abstract 0. MetaStone-S1, a reflective generative model using a self-supervised process reward model, achieves efficient reasoning and scalable performance with fewer parameters compared to existing models.  					AI-generated summary 				 We introduce our first reflective generative model MetaStone-S1, which ob...
[14.07.2025 12:23] ********************************************************************************
[14.07.2025 12:23] Abstract 1. A neural rendering method uses compressed light-field tokens to efficiently represent scenes and render novel views with varying compute budgets.  					AI-generated summary 				 This paper proposes a neural rendering approach that represents a scene as "compressed light-field tokens (CLiFTs)", retai...
[14.07.2025 12:23] ********************************************************************************
[14.07.2025 12:23] Abstract 2. NeuralOS uses a combination of RNNs and diffusion-based rendering to simulate OS GUIs by predicting screen frames from user inputs, demonstrating realistic GUI rendering and state transitions.  					AI-generated summary 				 We introduce NeuralOS, a neural framework that simulates graphical user int...
[14.07.2025 12:23] ********************************************************************************
[14.07.2025 12:23] Abstract 3. Cache steering improves reasoning in language models through a single intervention in the key-value cache, enhancing both reasoning structure and task performance.  					AI-generated summary 				 We propose cache steering, a lightweight method for implicit steering of language models via a one-shot ...
[14.07.2025 12:23] ********************************************************************************
[14.07.2025 12:23] Abstract 4. LoongX uses multimodal neurophysiological signals and diffusion models for hands-free image editing, achieving performance comparable to text-driven methods and outperforming them when combined with speech.  					AI-generated summary 				 Traditional image editing typically relies on manual promptin...
[14.07.2025 12:23] ********************************************************************************
[14.07.2025 12:23] Abstract 5. Lumos-1 is an autoregressive video generator that uses a modified LLM architecture with MM-RoPE and AR-DF to address spatiotemporal correlation and frame-wise loss imbalance, achieving competitive performance with fewer resources.  					AI-generated summary 				 Autoregressive large language models ...
[14.07.2025 12:23] ********************************************************************************
[14.07.2025 12:23] Abstract 6. A two-stage paradigm involving cold-start fine-tuning and multimodal reinforcement learning enhances visual reasoning in large language models, achieving top performance on various benchmarks.  					AI-generated summary 				 The remarkable reasoning capability of large language models (LLMs) stems f...
[14.07.2025 12:23] ********************************************************************************
[14.07.2025 12:23] Abstract 7. A part-aware diffusion framework, CoPart, enhances 3D generation by decomposing objects into contextual parts, improving complexity handling, relationship modeling, and part-level conditioning.  					AI-generated summary 				 Recent advances in 3D generation have transitioned from multi-view 2D rend...
[14.07.2025 12:23] ********************************************************************************
[14.07.2025 12:23] Abstract 8. Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.  					AI-generated summary 				 Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are incre...
[14.07.2025 12:23] ********************************************************************************
[14.07.2025 12:23] Abstract 9. A novel image tokenizer built on pre-trained vision foundation models improves image reconstruction, generation quality, and token efficiency, enhancing autoregressive generation and class-conditional synthesis.  					AI-generated summary 				 Leveraging the powerful representations of pre-trained v...
[14.07.2025 12:23] ********************************************************************************
[14.07.2025 12:23] Abstract 10. Foundation models, despite excelling in training tasks, often fail to generalize to new tasks due to task-specific heuristics rather than capturing underlying world models.  					AI-generated summary 				 Foundation models are premised on the idea that sequence prediction can uncover deeper domain u...
[14.07.2025 12:23] ********************************************************************************
[14.07.2025 12:23] Abstract 11. The Gemini 2.X model family offers varying levels of capability and efficiency for complex problem-solving, including multimodal understanding and reasoning.  					AI-generated summary 				 In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our ...
[14.07.2025 12:23] ********************************************************************************
[14.07.2025 12:23] Abstract 12. To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while ...
[14.07.2025 12:23] ********************************************************************************
[14.07.2025 12:23] Abstract 13. Investigation of modality conflict in multimodal large language models reveals its role in causing hallucinations, with reinforcement learning emerging as the most effective mitigation strategy.  					AI-generated summary 				 Despite the impressive capabilities of multimodal large language models (...
[14.07.2025 12:23] Read previous papers.
[14.07.2025 12:23] Generating reviews via LLM API.
[14.07.2025 12:23] Using data from previous issue: {"categories": ["#training", "#open_source", "#architecture", "#rl", "#small_models", "#reasoning"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –º–µ–Ω—å—à–∏–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏: MetaStone-S1 –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏", "desc": "MetaStone-S1 - —ç—Ç–æ —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—é—â
[14.07.2025 12:23] Using data from previous issue: {"categories": ["#benchmark", "#3d", "#dataset"], "emoji": "üé•", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –Ω–µ–π—Ä–æ—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Å –ø–æ–º–æ—â—å—é —Å–∂–∞—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Å–≤–µ—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Å–∂–∞—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã —Å–≤–µ—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è (CLiFTs) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—Ü–µ–Ω. –ú
[14.07.2025 12:23] Using data from previous issue: {"categories": ["#games", "#diffusion", "#dataset", "#agents", "#multimodal", "#cv"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è GUI: —à–∞–≥ –∫ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º –±—É–¥—É—â–µ–≥–æ", "desc": "NeuralOS - —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, —Å–∏–º—É–ª–∏—Ä—É—é—â–∞—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –ø—É—Ç–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
[14.07.2025 12:23] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#benchmark", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ò–ò —á–µ—Ä–µ–∑ –æ–¥–Ω–æ—Ä–∞–∑–æ–≤–æ–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ –≤ –∫—ç—à", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ –∫—ç—à-—É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –≠—Ç–æ—Ç –ª–µ–≥–∫–æ–≤–µ—Å
[14.07.2025 12:23] Using data from previous issue: {"categories": ["#diffusion", "#dataset", "#cv", "#multimodal", "#open_source"], "emoji": "üß†", "ru": {"title": "–ú—ã—Å–ª–µ–Ω–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å—é", "desc": "LoongX - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä—É–∫, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –Ω–µ–π—Ä–æ—Ñ–∏–∑–∏
[14.07.2025 12:23] Using data from previous issue: {"categories": ["#training", "#games", "#architecture", "#video", "#optimization", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Lumos-1 - —ç—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç
[14.07.2025 12:23] Using data from previous issue: {"categories": ["#training", "#benchmark", "#rl", "#transfer_learning", "#open_source", "#multimodal", "#reasoning"], "emoji": "üß†", "ru": {"title": "–î–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ
[14.07.2025 12:23] Using data from previous issue: {"categories": ["#games", "#3d", "#diffusion", "#dataset"], "emoji": "üß©", "ru": {"title": "–†–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤", "desc": "CoPart - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω–∞ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –æ–±—ä–µ–∫—Ç—ã –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏, 
[14.07.2025 12:23] Using data from previous issue: {"categories": ["#training", "#rl", "#data", "#dataset", "#optimization", "#hallucinations", "#synthetic", "#rlhf"], "emoji": "üõ°Ô∏è", "ru": {"title": "–£–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ç–∏–≤ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —É—è–∑–≤–∏–º–æ—Å—Ç—è–º –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥
[14.07.2025 12:23] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#optimization", "#training", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "VFMTok: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π VFMTok, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã
[14.07.2025 12:23] Using data from previous issue: {"categories": ["#transfer_learning", "#dataset", "#synthetic", "#benchmark"], "emoji": "üß†", "ru": {"title": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏: —É—Å–ø–µ—Ö –≤ –æ–±—É—á–µ–Ω–∏–∏, –ø—Ä–æ–≤–∞–ª –≤ –æ–±–æ–±—â–µ–Ω–∏–∏", "desc": "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∏—Ö —É—Å–ø–µ—Ö–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏, —á–∞—Å—Ç–æ –Ω–µ –º–æ–≥—É—Ç –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –∏–∑-–∑–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏
[14.07.2025 12:23] Using data from previous issue: {"categories": ["#long_context", "#agents", "#benchmark", "#architecture", "#multimodal", "#reasoning"], "emoji": "üß†", "ru": {"title": "Gemini 2.X: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Gemini 2.X, –≤–∫–ª—é—á–∞—è Gemini 2.5 Pro –∏ Gemini 2.5 Flash
[14.07.2025 12:23] Using data from previous issue: {"categories": ["#architecture", "#inference", "#optimization", "#low_resource", "#open_source", "#training"], "emoji": "üöÄ", "ru": {"title": "BlockFFN: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (MoE
[14.07.2025 12:23] Using data from previous issue: {"categories": ["#training", "#interpretability", "#dataset", "#rl", "#hallucinations", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ë–æ—Ä—å–±–∞ —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM) –∏–∑-–∑–∞ 
[14.07.2025 12:23] Renaming data file.
[14.07.2025 12:23] Renaming previous data. hf_papers.json to ./d/2025-07-14.json
[14.07.2025 12:23] Saving new data file.
[14.07.2025 12:23] Generating page.
[14.07.2025 12:23] Renaming previous page.
[14.07.2025 12:23] Renaming previous data. index.html to ./d/2025-07-14.html
[14.07.2025 12:23] Writing result.
[14.07.2025 12:23] Renaming log file.
[14.07.2025 12:23] Renaming previous data. log.txt to ./logs/2025-07-14_last_log.txt
