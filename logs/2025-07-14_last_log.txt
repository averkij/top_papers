[14.07.2025 03:27] Read previous papers.
[14.07.2025 03:27] Generating top page (month).
[14.07.2025 03:27] Writing top page (month).
[14.07.2025 04:34] Read previous papers.
[14.07.2025 04:34] Get feed.
[14.07.2025 04:34] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08776
[14.07.2025 04:34] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01951
[14.07.2025 04:34] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08800
[14.07.2025 04:34] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08801
[14.07.2025 04:34] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08772
[14.07.2025 04:34] Extract page data from URL. URL: https://huggingface.co/papers/2507.08794
[14.07.2025 04:34] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07151
[14.07.2025 04:34] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.07.2025 04:34] No deleted papers detected.
[14.07.2025 04:34] Downloading and parsing papers (pdf, html). Total: 7.
[14.07.2025 04:34] Downloading and parsing paper https://huggingface.co/papers/2507.08776.
[14.07.2025 04:34] Extra JSON file exists (./assets/json/2507.08776.json), skip PDF parsing.
[14.07.2025 04:34] Paper image links file exists (./assets/img_data/2507.08776.json), skip HTML parsing.
[14.07.2025 04:34] Success.
[14.07.2025 04:34] Downloading and parsing paper https://huggingface.co/papers/2507.01951.
[14.07.2025 04:34] Extra JSON file exists (./assets/json/2507.01951.json), skip PDF parsing.
[14.07.2025 04:34] Paper image links file exists (./assets/img_data/2507.01951.json), skip HTML parsing.
[14.07.2025 04:34] Success.
[14.07.2025 04:34] Downloading and parsing paper https://huggingface.co/papers/2507.08800.
[14.07.2025 04:34] Extra JSON file exists (./assets/json/2507.08800.json), skip PDF parsing.
[14.07.2025 04:34] Paper image links file exists (./assets/img_data/2507.08800.json), skip HTML parsing.
[14.07.2025 04:34] Success.
[14.07.2025 04:34] Downloading and parsing paper https://huggingface.co/papers/2507.08801.
[14.07.2025 04:34] Extra JSON file exists (./assets/json/2507.08801.json), skip PDF parsing.
[14.07.2025 04:34] Paper image links file exists (./assets/img_data/2507.08801.json), skip HTML parsing.
[14.07.2025 04:34] Success.
[14.07.2025 04:34] Downloading and parsing paper https://huggingface.co/papers/2507.08772.
[14.07.2025 04:34] Extra JSON file exists (./assets/json/2507.08772.json), skip PDF parsing.
[14.07.2025 04:34] Paper image links file exists (./assets/img_data/2507.08772.json), skip HTML parsing.
[14.07.2025 04:34] Success.
[14.07.2025 04:34] Downloading and parsing paper https://huggingface.co/papers/2507.08794.
[14.07.2025 04:34] Downloading paper 2507.08794 from http://arxiv.org/pdf/2507.08794v1...
[14.07.2025 04:34] Extracting affiliations from text.
[14.07.2025 04:34] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"One Token to Fool LLM-as-a-Judge One Token to Fool LLM-as-a-Judge Yulai Zhao ,1,2 , Haolin Liu,1,3 , Dian Yu1 , S.Y. Kung2 , Haitao Mi1 , and Dong Yu1 1Tencent AI Lab 2Princeton University 3University of Virginia "
[14.07.2025 04:34] Response: ```python
["Tencent AI Lab", "Princeton University", "University of Virginia"]
```
[14.07.2025 04:34] Deleting PDF ./assets/pdf/2507.08794.pdf.
[14.07.2025 04:34] Success.
[14.07.2025 04:34] Downloading and parsing paper https://huggingface.co/papers/2507.07151.
[14.07.2025 04:34] Downloading paper 2507.07151 from http://arxiv.org/pdf/2507.07151v1...
[14.07.2025 04:34] Extracting affiliations from text.
[14.07.2025 04:34] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Zongmeng Zhang 1 Wengang Zhou 2 Jie Zhao 3 Houqiang Li "
[14.07.2025 04:34] Response: []
[14.07.2025 04:34] Extracting affiliations from text.
[14.07.2025 04:34] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Zongmeng Zhang 1 Wengang Zhou 2 Jie Zhao 3 Houqiang LiDespite the impressive capabilities of multimodal large language models (MLLMs) in visionlanguage tasks, they are prone to hallucinations in real-world scenarios. This paper investigates the hallucination phenomenon in MLLMs from the perspective of modality conflict. Unlike existing works focusing on the conflicts between model responses and inputs, we study the inherent conflicts in inputs from different modalities that place MLLMs in dilemma and directly lead to hallucinations. We formally define the modality conflict and construct dataset named Multimodal Modality Conflict (MMMC) to simulate this phenomenon in vision-language tasks. Three methods based on prompt engineering, supervised finetuning, and reinforcement learning are proposed to alleviate the hallucination caused by modality conflict. Extensive experiments are conducted on the MMMC dataset to analyze the merits and demerits of these methods. Our results show that the reinforcement learning method achieves the best performance in mitigating the hallucination under modality conflict, while the supervised finetuning method shows promising and stable performance. Our work sheds light on the unnoticed modality conflict that leads to hallucinations and provides more insights into the robustness of MLLMs. The code and dataset are available at https://github.com/zmzhang2000/ MMMC. 5 2 0 2 9 ] . [ 1 1 5 1 7 0 . 7 0 5 2 : r 1School of Artificial Intelligence and Data Science, University of Science and Technology of China 2Department of Electronic Engineering and Information Science, University of Science and Technology of China 3Huawei Technologies Co., Ltd.. Correspondence to: Wengang Zhou <zhwg@ustc.edu.cn>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 Figure 1. An example of modality conflict in vision-language tasks. Given an image describing dog surfing on the sea, the user may ask the question What color is the ball?. The model may hallucinate response The ball in the image is green, while there is no ball in the image. We expect the model to recognize the conflict between the visual input and the textual input and give response like The image does not contain ball. 1. Introduction The recent success of multimodal large language models (MLLMs) has advanced the development of artificial intelligence in vision-language tasks (Dai et al., 2023; Liu et al., 2023; 2024b; Bai et al., 2023; Wang et al., 2024). These models enable the joint reasoning over visual and textual inputs, and have achieved state-of-the-art performance in various vision-language tasks that require multimodal reasoning (Fu et al., 2024; Yue et al., 2024; Yu et al., 2024c; Lu et al., 2024; Liu et al., 2025). The powerful capabilities of these MLLMs are typically achieved by pretraining separate language and vision models on large-scale datasets, and then Robust Multimodal Large Language Models Against Modality Conflict aligning their features to enable multimodal reasoning (Liu et al., 2023; 2024b). Despite the impressive performance of MLLMs, they are prone to hallucinations in real-world scenarios (Huang et al., 2024; Yu et al., 2024b). Hallucinations refer to the phenomenon where MLLMs generate incorrect or misleading information not supported by the input data (Ji et al., 2023). Existing works have proposed various methods to alleviate hallucinations in MLLMs, such as improving the quality of training data (Liu et al., 2024a; Yu et al., 2024a), adjusting the decoding strategies (Leng et al., 2024; Huang et al., 2024), and align the model with human preference (Zhao et al., 2024; Yu et al., 2024b). These methods mainly target more precise alignment between the features of different modalities to reduce hallucinations. However, existing works on alleviating hallucinations in MLLMs mainly focus on the conflicts between the model responses and the inputs, neglecting possible source of hallucinations: the conflicts between the inputs from different modalities, which we call modality conflict. For instance, as shown in Figure 1, given an image describing dog surfing on the sea, the user may ask the question What color is the ball?. In this case, the question supposes ball exists in the image, and the model may hallucinate response The ball in the image is green, while there is no ball in the image. We expect the model to recognize the conflict between the visual input and the textual input and give response like The image does not contain ball. Even with the capability of perfectly aligning features of different modalities, MLLMs may still fall into dilemma when facing such intrinsically conflicted information between inputs. To this end, we aim to investigate such hallucination phenomenon in MLLMs from the perspective of modality conflict. In this paper, we first give formal definition of modality conflict in vision-language tasks in terms of objects, attributes, and relationships in the visual and textual inputs. Based on the definition, we construct dataset named MultiModal Modality Conflict (MMMC) to simulate the modality conflict in vision-language tasks. We evaluate various prevalent MLLMs (Dai et al., 2023; Liu et al., 2024b; Bai et al., 2023; Wang et al., 2024) on the MMMC dataset and find that most of them lack the ability to recognize the modality conflict and are prone to hallucinations. To alleviate the hallucination caused by the modality conflict and work towards more robust MLLMs, we investigate the effectiveness of three methods: prompt engineering, supervised fine-tuning, and reinforcement learning. We conduct extensive experiments on the MMMC dataset to analyze the merits and demerits of these methods. Our results show that the reinforcement learning method achieves the best performance in mitigating the hallucination under modality conflict, while the supervised fine-tuning method shows promising and stable performance. Our work sheds light on the unnoticed modality conflict that causes hallucinations and provides more insights into the robustness of MLLMs. To summarize, the contributions of this paper are as follows: This paper reveals an unnoticed source of hallucinations in MLLMs: modality conflict. The formal definition of modality conflict is presented in the level of objects, attributes, and relationships. We construct dataset called Multimodal Modality Conflict (MMMC) to simulate t"
[14.07.2025 04:34] Mistral response. {"id": "66d65ab3ef0c4ad097bf677505b47686", "object": "chat.completion", "created": 1752467662, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"School of Artificial Intelligence and Data Science, University of Science and Technology of China\",\n    \"Department of Electronic Engineering and Information Science, University of Science and Technology of China\",\n    \"Huawei Technologies Co., Ltd.\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1670, "total_tokens": 1734, "completion_tokens": 64}}
[14.07.2025 04:34] Response: ```python
[
    "School of Artificial Intelligence and Data Science, University of Science and Technology of China",
    "Department of Electronic Engineering and Information Science, University of Science and Technology of China",
    "Huawei Technologies Co., Ltd."
]
```
[14.07.2025 04:34] Deleting PDF ./assets/pdf/2507.07151.pdf.
[14.07.2025 04:34] Success.
[14.07.2025 04:34] Enriching papers with extra data.
[14.07.2025 04:34] ********************************************************************************
[14.07.2025 04:34] Abstract 0. A neural rendering method uses compressed light-field tokens to efficiently represent scenes and render novel views with varying compute budgets.  					AI-generated summary 				 This paper proposes a neural rendering approach that represents a scene as "compressed light-field tokens (CLiFTs)", retai...
[14.07.2025 04:34] ********************************************************************************
[14.07.2025 04:34] Abstract 1. MetaStone-S1, a reflective generative model using a self-supervised process reward model, achieves efficient reasoning and scalable performance with fewer parameters compared to existing models.  					AI-generated summary 				 We introduce our first reflective generative model MetaStone-S1, which ob...
[14.07.2025 04:34] ********************************************************************************
[14.07.2025 04:34] Abstract 2. NeuralOS uses a combination of RNNs and diffusion-based rendering to simulate OS GUIs by predicting screen frames from user inputs, demonstrating realistic GUI rendering and state transitions.  					AI-generated summary 				 We introduce NeuralOS, a neural framework that simulates graphical user int...
[14.07.2025 04:34] ********************************************************************************
[14.07.2025 04:34] Abstract 3. Lumos-1 is an autoregressive video generator that uses a modified LLM architecture with MM-RoPE and AR-DF to address spatiotemporal correlation and frame-wise loss imbalance, achieving competitive performance with fewer resources.  					AI-generated summary 				 Autoregressive large language models ...
[14.07.2025 04:34] ********************************************************************************
[14.07.2025 04:34] Abstract 4. A part-aware diffusion framework, CoPart, enhances 3D generation by decomposing objects into contextual parts, improving complexity handling, relationship modeling, and part-level conditioning.  					AI-generated summary 				 Recent advances in 3D generation have transitioned from multi-view 2D rend...
[14.07.2025 04:34] ********************************************************************************
[14.07.2025 04:34] Abstract 5. Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.  					AI-generated summary 				 Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are incre...
[14.07.2025 04:34] ********************************************************************************
[14.07.2025 04:34] Abstract 6. Investigation of modality conflict in multimodal large language models reveals its role in causing hallucinations, with reinforcement learning emerging as the most effective mitigation strategy.  					AI-generated summary 				 Despite the impressive capabilities of multimodal large language models (...
[14.07.2025 04:34] Read previous papers.
[14.07.2025 04:34] Generating reviews via LLM API.
[14.07.2025 04:34] Using data from previous issue: {"categories": ["#benchmark", "#3d", "#dataset"], "emoji": "🎥", "ru": {"title": "Эффективный нейрорендеринг с помощью сжатых токенов светового поля", "desc": "Эта статья представляет нейронный метод рендеринга, использующий сжатые токены светового поля (CLiFTs) для эффективного представления сцен. М
[14.07.2025 04:34] Using data from previous issue: {"categories": ["#training", "#open_source", "#architecture", "#rl", "#small_models", "#reasoning"], "emoji": "🧠", "ru": {"title": "Эффективное рассуждение с меньшими ресурсами: MetaStone-S1 переопределяет генеративные модели", "desc": "MetaStone-S1 - это рефлексивная генеративная модель, использующ
[14.07.2025 04:34] Using data from previous issue: {"categories": ["#games", "#diffusion", "#dataset", "#agents", "#multimodal", "#cv"], "emoji": "🖥️", "ru": {"title": "Нейронная симуляция GUI: шаг к адаптивным интерфейсам будущего", "desc": "NeuralOS - это нейронная система, симулирующая графические интерфейсы операционных систем путем предсказания
[14.07.2025 04:34] Using data from previous issue: {"categories": ["#training", "#games", "#architecture", "#video", "#optimization", "#multimodal"], "emoji": "🎬", "ru": {"title": "Эффективная генерация видео с помощью модифицированных языковых моделей", "desc": "Lumos-1 - это авторегрессионный генератор видео, использующий модифицированную архитект
[14.07.2025 04:34] Using data from previous issue: {"categories": ["#games", "#3d", "#diffusion", "#dataset"], "emoji": "🧩", "ru": {"title": "Разделяй и властвуй: новый подход к генерации 3D-объектов", "desc": "CoPart - это новая система для генерации трехмерных объектов, основанная на диффузионных моделях. Она разбивает объекты на отдельные части, 
[14.07.2025 04:34] Querying the API.
[14.07.2025 04:34] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.  					AI-generated summary 				 Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning openers like "Thought process:" and "Let's solve this problem step by step." can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at https://huggingface.co/sarosavo/Master-RM and https://huggingface.co/datasets/sarosavo/Master-RM.
[14.07.2025 04:34] Response: {
  "desc": "Статья посвящена уязвимостям генеративных моделей вознаграждения, использующих большие языковые модели (LLM) для оценки качества ответов. Авторы обнаружили, что такие модели могут быть обмануты поверхностными манипуляциями, такими как добавление символов или фраз-заполнителей. Для решения этой проблемы предложена стратегия аугментации данных, позволяющая создать более надежную модель оценки. Исследование подчеркивает необходимость разработки более надежных методов оценки на основе LLM.",
  "emoji": "🛡️",
  "title": "Укрепление генеративных моделей вознаграждения против поверхностных манипуляций"
}
[14.07.2025 04:34] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.  					AI-generated summary 				 Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning openers like "Thought process:" and "Let's solve this problem step by step." can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at https://huggingface.co/sarosavo/Master-RM and https://huggingface.co/datasets/sarosavo/Master-RM."

[14.07.2025 04:34] Response: ```python
['RL', 'RLHF', 'DATASET', 'DATA', 'TRAINING']
```
[14.07.2025 04:34] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.  					AI-generated summary 				 Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning openers like "Thought process:" and "Let's solve this problem step by step." can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at https://huggingface.co/sarosavo/Master-RM and https://huggingface.co/datasets/sarosavo/Master-RM."

[14.07.2025 04:34] Response: ```python
["HALLUCINATIONS", "OPTIMIZATION", "SYNTHETIC"]
```
[14.07.2025 04:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper discusses the vulnerabilities of generative reward models, which use large language models (LLMs) to assess the quality of answers in reinforcement learning scenarios. The authors reveal that these models can be easily tricked by superficial changes in the input, such as adding non-word symbols or specific phrases, leading to incorrect evaluations. To address this issue, they propose a data augmentation strategy that enhances the robustness of the generative reward models against such manipulations. The study emphasizes the importance of developing more reliable evaluation methods for LLMs in reinforcement learning applications.","title":"Strengthening LLMs: Combatting Superficial Manipulations in Reward Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the vulnerabilities of generative reward models, which use large language models (LLMs) to assess the quality of answers in reinforcement learning scenarios. The authors reveal that these models can be easily tricked by superficial changes in the input, such as adding non-word symbols or specific phrases, leading to incorrect evaluations. To address this issue, they propose a data augmentation strategy that enhances the robustness of the generative reward models against such manipulations. The study emphasizes the importance of developing more reliable evaluation methods for LLMs in reinforcement learning applications.', title='Strengthening LLMs: Combatting Superficial Manipulations in Reward Models'))
[14.07.2025 04:34] Response: ParsedChatCompletionMessage[Article](content='{"desc":"生成奖励模型（LLMs作为评判者）在使用大型语言模型评估答案质量时，容易受到表面操控的影响。尽管这种比较任务看似简单，但我们发现生成奖励模型在面对非单词符号或推理开头词时，常常会产生错误的正向奖励。这种脆弱性在不同的LLM、数据集和提示格式中普遍存在，威胁到依赖生成奖励模型的核心算法范式。为了解决这个问题，我们提出了一种简单有效的数据增强策略，训练出一种具有显著改进鲁棒性的生成奖励模型。","title":"提升生成奖励模型的鲁棒性"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='生成奖励模型（LLMs作为评判者）在使用大型语言模型评估答案质量时，容易受到表面操控的影响。尽管这种比较任务看似简单，但我们发现生成奖励模型在面对非单词符号或推理开头词时，常常会产生错误的正向奖励。这种脆弱性在不同的LLM、数据集和提示格式中普遍存在，威胁到依赖生成奖励模型的核心算法范式。为了解决这个问题，我们提出了一种简单有效的数据增强策略，训练出一种具有显著改进鲁棒性的生成奖励模型。', title='提升生成奖励模型的鲁棒性'))
[14.07.2025 04:34] Using data from previous issue: {"categories": ["#training", "#interpretability", "#dataset", "#rl", "#hallucinations", "#multimodal"], "emoji": "🤖", "ru": {"title": "Борьба с галлюцинациями в мультимодальных нейросетях", "desc": "Исследование посвящено проблеме галлюцинаций в мультимодальных больших языковых моделях (MLLM) из-за 
[14.07.2025 04:34] Renaming data file.
[14.07.2025 04:34] Renaming previous data. hf_papers.json to ./d/2025-07-14.json
[14.07.2025 04:34] Saving new data file.
[14.07.2025 04:34] Generating page.
[14.07.2025 04:34] Renaming previous page.
[14.07.2025 04:34] Renaming previous data. index.html to ./d/2025-07-14.html
[14.07.2025 04:34] Writing result.
[14.07.2025 04:34] Renaming log file.
[14.07.2025 04:34] Renaming previous data. log.txt to ./logs/2025-07-14_last_log.txt
