[04.08.2025 17:18] Read previous papers.
[04.08.2025 17:18] Generating top page (month).
[04.08.2025 17:18] Writing top page (month).
[04.08.2025 18:18] Read previous papers.
[04.08.2025 18:18] Get feed.
[04.08.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2508.00819
[04.08.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2507.23268
[04.08.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2507.23361
[04.08.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2508.00265
[04.08.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2507.23478
[04.08.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2507.23348
[04.08.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2508.00454
[04.08.2025 18:18] Extract page data from URL. URL: https://huggingface.co/papers/2508.00414
[04.08.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2508.00782
[04.08.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2508.00632
[04.08.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2507.22720
[04.08.2025 18:18] Extract page data from URL. URL: https://huggingface.co/papers/2507.19634
[04.08.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2508.00823
[04.08.2025 18:18] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.08.2025 18:18] No deleted papers detected.
[04.08.2025 18:18] Downloading and parsing papers (pdf, html). Total: 13.
[04.08.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2508.00819.
[04.08.2025 18:18] Extra JSON file exists (./assets/json/2508.00819.json), skip PDF parsing.
[04.08.2025 18:18] Paper image links file exists (./assets/img_data/2508.00819.json), skip HTML parsing.
[04.08.2025 18:18] Success.
[04.08.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2507.23268.
[04.08.2025 18:18] Extra JSON file exists (./assets/json/2507.23268.json), skip PDF parsing.
[04.08.2025 18:18] Paper image links file exists (./assets/img_data/2507.23268.json), skip HTML parsing.
[04.08.2025 18:18] Success.
[04.08.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2507.23361.
[04.08.2025 18:18] Extra JSON file exists (./assets/json/2507.23361.json), skip PDF parsing.
[04.08.2025 18:18] Paper image links file exists (./assets/img_data/2507.23361.json), skip HTML parsing.
[04.08.2025 18:18] Success.
[04.08.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2508.00265.
[04.08.2025 18:18] Extra JSON file exists (./assets/json/2508.00265.json), skip PDF parsing.
[04.08.2025 18:18] Paper image links file exists (./assets/img_data/2508.00265.json), skip HTML parsing.
[04.08.2025 18:18] Success.
[04.08.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2507.23478.
[04.08.2025 18:18] Extra JSON file exists (./assets/json/2507.23478.json), skip PDF parsing.
[04.08.2025 18:18] Paper image links file exists (./assets/img_data/2507.23478.json), skip HTML parsing.
[04.08.2025 18:18] Success.
[04.08.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2507.23348.
[04.08.2025 18:18] Extra JSON file exists (./assets/json/2507.23348.json), skip PDF parsing.
[04.08.2025 18:18] Paper image links file exists (./assets/img_data/2507.23348.json), skip HTML parsing.
[04.08.2025 18:18] Success.
[04.08.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2508.00454.
[04.08.2025 18:18] Extra JSON file exists (./assets/json/2508.00454.json), skip PDF parsing.
[04.08.2025 18:18] Paper image links file exists (./assets/img_data/2508.00454.json), skip HTML parsing.
[04.08.2025 18:18] Success.
[04.08.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2508.00414.
[04.08.2025 18:18] Downloading paper 2508.00414 from http://arxiv.org/pdf/2508.00414v1...
[04.08.2025 18:18] Extracting affiliations from text.
[04.08.2025 18:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 ] . [ 1 4 1 4 0 0 . 8 0 5 2 : r a Cognitive Kernel-Pro: Framework for Deep Research Agents and Agent Foundation Models Training Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, Hongming Zhang, Haitao Mi, Dong Yu https://github.com/Tencent/CognitiveKernel-Pro Figure 1: (a) Performance comparison on the full GAIA development set (number of examples n=165). The left panel presents results from our open-source Cognitive Kernel-Pro framework, utilizing our Qwen3-8B SFT model and Claude-3.7 as foundation models with exclusively free tools. The right panel displays Pass@1 scores for proprietary agents and open-source systems employing paid tools. (b) Performance on the text-only GAIA subset (n=103), demonstrating our 8B models superiority over 7B models in the WebDancer/WebSailor family (2% higher Pass@1, over 10% higher Pass@3). Abstract General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present Cognitive Kernel-Pro, fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of"
[04.08.2025 18:18] Response: ```python
[]
```
[04.08.2025 18:18] Extracting affiliations from text.
[04.08.2025 18:18] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 ] . [ 1 4 1 4 0 0 . 8 0 5 2 : r aCognitive Kernel-Pro: Framework for Deep Research Agents and Agent Foundation Models Training Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, Hongming Zhang, Haitao Mi, Dong Yuhttps://github.com/Tencent/CognitiveKernel-Pro Figure 1: (a) Performance comparison on the full GAIA development set (number of examples n=165). The left panel presents results from our open-source Cognitive Kernel-Pro framework, utilizing our Qwen3-8B SFT model and Claude-3.7 as foundation models with exclusively free tools. The right panel displays Pass@1 scores for proprietary agents and open-source systems employing paid tools. (b) Performance on the text-only GAIA subset (n=103), demonstrating our 8B models superiority over 7B models in the WebDancer/WebSailor family (2% higher Pass@1, over 10% higher Pass@3). Abstract General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present Cognitive Kernel-Pro, fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-ofthe-art results among open-source and free agents. Notably, our 8B-parameter opensource model surpasses previous leading systems such as WebDancer and WebSailor, establishing new performance standard for accessible, high-capability AI agents. Note: The term Cognitive Kernel (Zhang et al., 2024) refers to the core computational framework of the agent, designed to emulate the cognitive processes of the human mind. Equal Core ContributorsFigure 2: Technical roadmap showcasing prior innovations from Tencent AI Lab (Cognitive Kernel; Zhang et al., 2024, WebVoyager; He et al., 2024a, etc) and their integration to Cognitive KernelPro via three core components, agent framework development, agent data construction, and agent foundation model training. Yellow blocks highlight novel contributions in this work and the corresponding section numbers.The rapid advancement of Deep Research Agents (Monica.Im, 2025; OpenAI, 2025) has transformed the landscape of automated knowledge discovery and problem-solving. These agents, powered by large language models (LLMs) and vision-language models (VLMs), excel in tasks such as coding, web navigation, file processing, and complex reasoning. However, efforts toward fully open-source agent frameworks (Roucher et al., 2025; Wu et al., 2025a; Li et al., 2025a) remain limited. Existing open-source implementations (Zhu et al., 2025; Hu et al., 2025) often rely on proprietary tools like Jina Reader, FireCrawl, or Chunkr to achieve competitive performance, creating barriers to accessibility and reproducibility, or lack of multimodal or general agentic abilities (Wu et al., 2025a; Li et al., 2025a). This dependency on paid tools underscores the need for robust, fully opensource framework that maximizes the inherent capabilities of LLMs and VLMs without external dependencies. To address this gap, we propose Cognitive Kernel-Pro, multi-module, hierarchical agent framework designed to facilitate fully open-source agent development. Cognitive Kernel-Pro leverages Python code as its action space, harnessing the full reasoning and code-generation potential of modern LLMs. The framework adopts modular architecture, featuring main agent that orchestrates specialized sub-agents for web navigation, file handling, and tool invocation. Each module operates independently, ensuring modularity and extensibility while simplifying the collection of task-specific training data. By minimizing reliance on proprietary tools, Cognitive Kernel-Pro emphasizes the intrinsic capabilities of Agent Foundation Models. In addition to the framework, we introduce comprehensive training recipe tailored for Cognitive Kernel-Pro, covering diverse domains such as web navigation, file processing, code generation, and reasoning. Our approach includes the construction of verifiable agent query-answer pairs, ensuring high-quality training data. To enhance data collection, we incorporate intermediate process hints and employ hint-based rejection sampling, which significantly improves the quality and relevance of the collected data. This structured training methodology enables Cognitive Kernel-Pro to achieve robust performance across diverse tasks while maintaining full open-source compatibility."
[04.08.2025 18:18] Mistral response. {"id": "bf1a33959fb9477f9c0f4ef4e172dbae", "created": 1754331518, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1321, "total_tokens": 1327, "completion_tokens": 6}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "[\"Tencent\"]"}}]}
[04.08.2025 18:18] Response: ["Tencent"]
[04.08.2025 18:18] Deleting PDF ./assets/pdf/2508.00414.pdf.
[04.08.2025 18:18] Success.
[04.08.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2508.00782.
[04.08.2025 18:18] Extra JSON file exists (./assets/json/2508.00782.json), skip PDF parsing.
[04.08.2025 18:18] Paper image links file exists (./assets/img_data/2508.00782.json), skip HTML parsing.
[04.08.2025 18:18] Success.
[04.08.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2508.00632.
[04.08.2025 18:18] Extra JSON file exists (./assets/json/2508.00632.json), skip PDF parsing.
[04.08.2025 18:18] Paper image links file exists (./assets/img_data/2508.00632.json), skip HTML parsing.
[04.08.2025 18:18] Success.
[04.08.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2507.22720.
[04.08.2025 18:18] Extra JSON file exists (./assets/json/2507.22720.json), skip PDF parsing.
[04.08.2025 18:18] Paper image links file exists (./assets/img_data/2507.22720.json), skip HTML parsing.
[04.08.2025 18:18] Success.
[04.08.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2507.19634.
[04.08.2025 18:18] Downloading paper 2507.19634 from http://arxiv.org/pdf/2507.19634v1...
[04.08.2025 18:18] Extracting affiliations from text.
[04.08.2025 18:18] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"MCIF: MULTIMODAL CROSSLINGUAL INSTRUCTIONFOLLOWING BENCHMARK FROM SCIENTIFIC TALKS Sara Papi1, Maike ufle2, Marco Gaido1, Beatrice Savoldi1, Danni Liu2, Ioannis Douros3, Luisa Bentivogli1, Jan Niehues2 1Fondazione Bruno Kessler (Italy), 2Karlsruhe Institute of Technology (Germany), 3Translated (Italy) {spapi,mgaido,bsavoldi,bentivo}@fbk.eu, {maike.zuefle, danni.liu,jan.niehues}@kit.edu, ioannis@translated.com "
[04.08.2025 18:18] Response: ```python
["Fondazione Bruno Kessler (Italy)", "Karlsruhe Institute of Technology (Germany)", "Translated (Italy)"]
```
[04.08.2025 18:18] Deleting PDF ./assets/pdf/2507.19634.pdf.
[04.08.2025 18:18] Success.
[04.08.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2508.00823.
[04.08.2025 18:18] Extra JSON file exists (./assets/json/2508.00823.json), skip PDF parsing.
[04.08.2025 18:18] Paper image links file exists (./assets/img_data/2508.00823.json), skip HTML parsing.
[04.08.2025 18:18] Success.
[04.08.2025 18:18] Enriching papers with extra data.
[04.08.2025 18:18] ********************************************************************************
[04.08.2025 18:18] Abstract 0. DAEDAL, a novel training-free denoising strategy, enables dynamic length adaptation in Diffusion Large Language Models, improving performance and computational efficiency.  					AI-generated summary 				 Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant A...
[04.08.2025 18:18] ********************************************************************************
[04.08.2025 18:18] Abstract 1. Pixel Neural Field Diffusion (PixNerd) achieves high-quality image generation in a single-scale, single-stage process without VAEs or complex pipelines, and extends to text-to-image applications with competitive performance.  					AI-generated summary 				 The current success of diffusion transforme...
[04.08.2025 18:18] ********************************************************************************
[04.08.2025 18:18] Abstract 2. SWE-Exp enhances software issue resolution by systematically accumulating and leveraging repair expertise from past agent experiences, improving resolution rates.  					AI-generated summary 				 Recent advances in large language model (LLM) agents have shown remarkable progress in software issue res...
[04.08.2025 18:18] ********************************************************************************
[04.08.2025 18:18] Abstract 3. A survey of multimodal referring segmentation techniques, covering advancements in convolutional neural networks, transformers, and large language models for segmenting objects in images, videos, and 3D scenes based on text or audio instructions.  					AI-generated summary 				 Multimodal referring ...
[04.08.2025 18:18] ********************************************************************************
[04.08.2025 18:18] Abstract 4. 3D-R1 enhances 3D scene understanding through a high-quality synthetic dataset, reinforcement learning with GRPO, and dynamic view selection, achieving significant improvements in reasoning and generalization.  					AI-generated summary 				 Large vision-language models (VLMs) have made significant ...
[04.08.2025 18:18] ********************************************************************************
[04.08.2025 18:18] Abstract 5. SWE-Debate, a competitive multi-agent framework, enhances issue resolution in software engineering by promoting diverse reasoning and achieving better issue localization and fix planning.  					AI-generated summary 				 Issue resolution has made remarkable progress thanks to the advanced reasoning c...
[04.08.2025 18:18] ********************************************************************************
[04.08.2025 18:18] Abstract 6. An efficient multi-turn dialogue evaluator aggregates multiple LLM judgments into a single model to assess dialogue quality with reduced computational cost.  					AI-generated summary 				 Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mai...
[04.08.2025 18:18] ********************************************************************************
[04.08.2025 18:18] Abstract 7. Cognitive Kernel-Pro is an open-source multi-module agent framework that enhances AI agent robustness and performance through data curation and novel test-time strategies, achieving state-of-the-art results.  					AI-generated summary 				 General AI Agents are increasingly recognized as foundationa...
[04.08.2025 18:18] ********************************************************************************
[04.08.2025 18:18] Abstract 8. SpA2V generates realistic videos aligned with input audio by leveraging spatial auditory cues and integrating them into diffusion models through video scene layouts.  					AI-generated summary 				 Audio-driven video generation aims to synthesize realistic videos that align with input audio recordin...
[04.08.2025 18:18] ********************************************************************************
[04.08.2025 18:18] Abstract 9. A multi-agent system using an omni-modal evaluation metric improves JavaScript game and animation generation but struggles with custom assets and audio-visual feedback.  					AI-generated summary 				 While AI excels at generating text, audio, images, and videos, creating interactive audio-visual co...
[04.08.2025 18:18] ********************************************************************************
[04.08.2025 18:18] Abstract 10. LLMs generate fewer hallucinations in Mandarin compared to Hindi and Farsi across multiple models.  					AI-generated summary 				 Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually inco...
[04.08.2025 18:18] ********************************************************************************
[04.08.2025 18:18] Abstract 11. MCIF is a multilingual, human-annotated benchmark for evaluating instruction-following in crosslingual, multimodal settings using scientific talks.  					AI-generated summary 				 Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, ...
[04.08.2025 18:18] ********************************************************************************
[04.08.2025 18:18] Abstract 12. IGL-Nav uses an incremental 3D Gaussian representation for efficient and accurate image-goal navigation in 3D space, outperforming existing methods and applicable in real-world settings.  					AI-generated summary 				 Visual navigation with an image as goal is a fundamental and challenging problem....
[04.08.2025 18:18] Read previous papers.
[04.08.2025 18:18] Generating reviews via LLM API.
[04.08.2025 18:18] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#diffusion", "#long_context"], "emoji": "üîÑ", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª–∏–Ω—ã —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "DAEDAL - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–ª–∏–Ω—ã –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö 
[04.08.2025 18:18] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#benchmark"], "emoji": "üñºÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä", "desc": "PixNerd (Pixel Neural Field Diffusion) - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ä–∞–±–æ—Ç–∞—é—â–∏–π –≤ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã—Ö 
[04.08.2025 18:18] Using data from previous issue: {"categories": ["#training", "#optimization", "#agents", "#open_source"], "emoji": "üß†", "ru": {"title": "–û–ø—ã—Ç - –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º –≤ –ü–û", "desc": "SWE-Exp - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º –≤ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–º –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –æ–ø—ã—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–ø—ã—Ç–æ–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏
[04.08.2025 18:18] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#3d", "#survey", "#video", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è: –æ—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ —Å—Å—ã–ª–∫–∞–º, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ —Å–≤–µ—Ä—Ç–æ
[04.08.2025 18:18] Using data from previous issue: {"categories": ["#benchmark", "#3d", "#dataset", "#reasoning", "#rlhf", "#synthetic"], "emoji": "üß†", "ru": {"title": "3D-R1: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–ú–æ–¥–µ–ª—å 3D-R1 —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω —Å –ø–æ–º–æ—â—å—é –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –æ–±—É—á–µ–Ω–∏—è 
[04.08.2025 18:18] Using data from previous issue: {"categories": ["#optimization", "#agents", "#open_source", "#reasoning", "#games", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–î–µ–±–∞—Ç—ã –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ü–û", "desc": "SWE-Debate - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –Ω–µ—Å–∫–æ–ª—å–∫–æ 
[04.08.2025 18:18] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#inference", "#alignment", "#benchmark"], "emoji": "üó£Ô∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤: –º—É–¥—Ä–æ—Å—Ç—å –º–Ω–æ–≥–∏—Ö –≤ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à
[04.08.2025 18:18] Querying the API.
[04.08.2025 18:18] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Cognitive Kernel-Pro is an open-source multi-module agent framework that enhances AI agent robustness and performance through data curation and novel test-time strategies, achieving state-of-the-art results.  					AI-generated summary 				 General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present Cognitive Kernel-Pro, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro
[04.08.2025 18:18] Response: {
  "desc": "Cognitive Kernel-Pro - —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –º–Ω–æ–≥–æ–º–æ–¥—É–ª—å–Ω–∞—è –∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, —É–ª—É—á—à–∞—é—â–∞—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–æ–≤—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ê–≥–µ–Ω—Ç–Ω—ã—Ö –§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ú–æ–¥–µ–ª–µ–π –≤ —á–µ—Ç—ã—Ä–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö: –≤–µ–±, —Ñ–∞–π–ª—ã, –∫–æ–¥ –∏ –æ–±—â–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. Cognitive Kernel-Pro –∏—Å—Å–ª–µ–¥—É–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –∏ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∏—Ö –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏. –°–∏—Å—Ç–µ–º–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–µ–¥–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ GAIA, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –≤–µ–¥—É—â–∏–µ —Å–∏—Å—Ç–µ–º—ã.",

  "emoji": "üß†",

  "title": "–û—Ç–∫—Ä—ã—Ç–∞—è –∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ò–ò –¥–ª—è –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π"
}
[04.08.2025 18:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Cognitive Kernel-Pro is an open-source multi-module agent framework that enhances AI agent robustness and performance through data curation and novel test-time strategies, achieving state-of-the-art results.  					AI-generated summary 				 General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present Cognitive Kernel-Pro, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro"

[04.08.2025 18:18] Response: ```python
['AGENTS', 'DATA', 'TRAINING']
```
[04.08.2025 18:18] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Cognitive Kernel-Pro is an open-source multi-module agent framework that enhances AI agent robustness and performance through data curation and novel test-time strategies, achieving state-of-the-art results.  					AI-generated summary 				 General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present Cognitive Kernel-Pro, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro"

[04.08.2025 18:18] Response: ```python
['AGI', 'OPEN_SOURCE', 'REASONING']
```
[04.08.2025 18:18] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Cognitive Kernel-Pro is an open-source framework designed to improve the robustness and performance of AI agents through effective data curation and innovative test-time strategies. It focuses on creating high-quality training data for Agent Foundation Models by constructing queries and trajectories across various domains like web interaction and coding. The framework also introduces new methods for agent reflection and voting, which enhance decision-making capabilities. By achieving state-of-the-art results on the GAIA benchmark, Cognitive Kernel-Pro sets a new standard for accessible and high-performance AI agents.","title":"Democratizing AI Agent Development with Cognitive Kernel-Pro"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Cognitive Kernel-Pro is an open-source framework designed to improve the robustness and performance of AI agents through effective data curation and innovative test-time strategies. It focuses on creating high-quality training data for Agent Foundation Models by constructing queries and trajectories across various domains like web interaction and coding. The framework also introduces new methods for agent reflection and voting, which enhance decision-making capabilities. By achieving state-of-the-art results on the GAIA benchmark, Cognitive Kernel-Pro sets a new standard for accessible and high-performance AI agents.', title='Democratizing AI Agent Development with Cognitive Kernel-Pro'))
[04.08.2025 18:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Cognitive Kernel-ProÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂ§öÊ®°Âùó‰ª£ÁêÜÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÊï∞ÊçÆÊï¥ÁêÜÂíåÊñ∞È¢ñÁöÑÊµãËØïÁ≠ñÁï•Êù•Â¢ûÂº∫AI‰ª£ÁêÜÁöÑÈ≤ÅÊ£íÊÄßÂíåÊÄßËÉΩ„ÄÇËØ•Ê°ÜÊû∂ÊîØÊåÅÂ§çÊùÇÊé®ÁêÜ„ÄÅÁΩëÁªú‰∫§‰∫í„ÄÅÁºñÁ†ÅÂíåËá™‰∏ªÁ†îÁ©∂ËÉΩÂäõÔºåÊé®Âä®‰∏ã‰∏Ä‰ª£‰∫∫Â∑•Êô∫ËÉΩÁöÑÂèëÂ±ï„ÄÇÊàë‰ª¨Á≥ªÁªüÂú∞Á†îÁ©∂‰∫ÜÈ´òË¥®ÈáèËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊï¥ÁêÜÔºåÈáçÁÇπÂÖ≥Ê≥®Êü•ËØ¢„ÄÅËΩ®ËøπÂíåÂèØÈ™åËØÅÁ≠îÊ°àÁöÑÊûÑÂª∫„ÄÇCognitive Kernel-ProÂú®GAIA‰∏äËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂèñÂæó‰∫ÜÂºÄÊ∫êÂíåÂÖçË¥π‰ª£ÁêÜ‰∏≠ÁöÑÊúÄ‰Ω≥ÁªìÊûúÔºåËÆæÁ´ã‰∫ÜÈ´òËÉΩÂäõAI‰ª£ÁêÜÁöÑÊñ∞Ê†áÂáÜ„ÄÇ","title":"ÂºÄÊîæÊ∫ê‰ª£Á†ÅÔºåÊèêÂçáAI‰ª£ÁêÜÁöÑÊú™Êù•ÔºÅ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Cognitive Kernel-ProÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂ§öÊ®°Âùó‰ª£ÁêÜÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÊï∞ÊçÆÊï¥ÁêÜÂíåÊñ∞È¢ñÁöÑÊµãËØïÁ≠ñÁï•Êù•Â¢ûÂº∫AI‰ª£ÁêÜÁöÑÈ≤ÅÊ£íÊÄßÂíåÊÄßËÉΩ„ÄÇËØ•Ê°ÜÊû∂ÊîØÊåÅÂ§çÊùÇÊé®ÁêÜ„ÄÅÁΩëÁªú‰∫§‰∫í„ÄÅÁºñÁ†ÅÂíåËá™‰∏ªÁ†îÁ©∂ËÉΩÂäõÔºåÊé®Âä®‰∏ã‰∏Ä‰ª£‰∫∫Â∑•Êô∫ËÉΩÁöÑÂèëÂ±ï„ÄÇÊàë‰ª¨Á≥ªÁªüÂú∞Á†îÁ©∂‰∫ÜÈ´òË¥®ÈáèËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊï¥ÁêÜÔºåÈáçÁÇπÂÖ≥Ê≥®Êü•ËØ¢„ÄÅËΩ®ËøπÂíåÂèØÈ™åËØÅÁ≠îÊ°àÁöÑÊûÑÂª∫„ÄÇCognitive Kernel-ProÂú®GAIA‰∏äËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂèñÂæó‰∫ÜÂºÄÊ∫êÂíåÂÖçË¥π‰ª£ÁêÜ‰∏≠ÁöÑÊúÄ‰Ω≥ÁªìÊûúÔºåËÆæÁ´ã‰∫ÜÈ´òËÉΩÂäõAI‰ª£ÁêÜÁöÑÊñ∞Ê†áÂáÜ„ÄÇ', title='ÂºÄÊîæÊ∫ê‰ª£Á†ÅÔºåÊèêÂçáAI‰ª£ÁêÜÁöÑÊú™Êù•ÔºÅ'))
[04.08.2025 18:19] Using data from previous issue: {"categories": ["#audio", "#games", "#video", "#diffusion", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ó–≤—É–∫ –≤ –¥–≤–∏–∂–µ–Ω–∏–∏: –æ—Ç –∞—É–¥–∏–æ –∫ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–º—É –≤–∏–¥–µ–æ", "desc": "SpA2V - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–≤—É–∫–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏
[04.08.2025 18:19] Using data from previous issue: {"categories": ["#audio", "#multimodal", "#games", "#optimization", "#video", "#agents"], "emoji": "üéÆ", "ru": {"title": "–ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–≥—Ä: –ø—Ä–æ–≥—Ä–µ—Å—Å –∏ –ø—Ä–æ–±–ª–µ–º—ã", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è JavaScript-–∏–≥—Ä –∏ –∞–Ω–∏–º–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω–æ–π
[04.08.2025 18:19] Using data from previous issue: {"categories": ["#dataset", "#multilingual", "#hallucinations"], "emoji": "üó£Ô∏è", "ru": {"title": "–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –º–µ–Ω—å—à–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∏—Ä—É—é—Ç –ø–æ-–∫–∏—Ç–∞–π—Å–∫–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ —Ç—Ä–µ—Ö —è–∑—ã–∫–æ–≤: —Ö–∏–Ω–¥–∏, —Ñ–∞—Ä—Å–∏ –∏ –º–∞–Ω–¥–∞—Ä–∏–Ω—Å–∫–æ–≥–æ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ. 
[04.08.2025 18:19] Querying the API.
[04.08.2025 18:19] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MCIF is a multilingual, human-annotated benchmark for evaluating instruction-following in crosslingual, multimodal settings using scientific talks.  					AI-generated summary 				 Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, a key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at a time, rely on short-form contexts, or lack human annotations -- hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core modalities -- speech, vision, and text -- and four diverse languages (English, German, Italian, and Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0 license to encourage open research and progress in MLLMs development.
[04.08.2025 18:19] Response: {
  "desc": "MCIF - —ç—Ç–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –æ—Ç –ª—é–¥–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –≤ –∫—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –Ω–∞—É—á–Ω—ã–µ –¥–æ–∫–ª–∞–¥—ã. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ - —Ä–µ—á—å, –∑—Ä–µ–Ω–∏–µ –∏ —Ç–µ–∫—Å—Ç - –Ω–∞ —á–µ—Ç—ã—Ä–µ—Ö —è–∑—ã–∫–∞—Ö, –ø–æ–∑–≤–æ–ª—è—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –∏ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –∏—Ö —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π. MCIF —Å–æ–∑–¥–∞–Ω –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –∞–Ω–≥–ª–∏–π—Å–∫–∏–º —è–∑—ã–∫–æ–º, —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ –æ–¥–Ω–æ–π –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö. –≠—Ç–æ—Ç –±–µ–Ω—á–º–∞—Ä–∫ –≤—ã–ø—É—â–µ–Ω –ø–æ–¥ –ª–∏—Ü–µ–Ω–∑–∏–µ–π CC-BY 4.0 –¥–ª—è –ø–æ–æ—â—Ä–µ–Ω–∏—è –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ MLLM.",
  "emoji": "üåê",
  "title": "MCIF: –ü–µ—Ä–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ MLLM"
}
[04.08.2025 18:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MCIF is a multilingual, human-annotated benchmark for evaluating instruction-following in crosslingual, multimodal settings using scientific talks.  					AI-generated summary 				 Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, a key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at a time, rely on short-form contexts, or lack human annotations -- hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core modalities -- speech, vision, and text -- and four diverse languages (English, German, Italian, and Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0 license to encourage open research and progress in MLLMs development."

[04.08.2025 18:19] Response: ```python
['DATASET', 'BENCHMARK', 'MULTIMODAL', 'MULTILINGUAL']
```
[04.08.2025 18:19] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MCIF is a multilingual, human-annotated benchmark for evaluating instruction-following in crosslingual, multimodal settings using scientific talks.  					AI-generated summary 				 Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, a key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at a time, rely on short-form contexts, or lack human annotations -- hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core modalities -- speech, vision, and text -- and four diverse languages (English, German, Italian, and Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0 license to encourage open research and progress in MLLMs development."

[04.08.2025 18:19] Response: ```python
['OPEN_SOURCE', 'LONG_CONTEXT', 'TRANSLATION']
```
[04.08.2025 18:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MCIF is a new benchmark designed to evaluate how well multilingual, multimodal language models can follow instructions in different languages and formats. It focuses on three modalities: speech, vision, and text, and includes human annotations to ensure quality assessments. Unlike previous benchmarks, MCIF allows for both short and long context evaluations across four languages: English, German, Italian, and Chinese. This comprehensive approach aims to enhance the understanding of model performance in real-world, crosslingual scenarios.","title":"MCIF: A New Benchmark for Multilingual Instruction-Following in Multimodal AI"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MCIF is a new benchmark designed to evaluate how well multilingual, multimodal language models can follow instructions in different languages and formats. It focuses on three modalities: speech, vision, and text, and includes human annotations to ensure quality assessments. Unlike previous benchmarks, MCIF allows for both short and long context evaluations across four languages: English, German, Italian, and Chinese. This comprehensive approach aims to enhance the understanding of model performance in real-world, crosslingual scenarios.', title='MCIF: A New Benchmark for Multilingual Instruction-Following in Multimodal AI'))
[04.08.2025 18:19] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MCIFÊòØ‰∏Ä‰∏™Â§öËØ≠Ë®ÄÁöÑ‰∫∫Á±ªÊ†áÊ≥®Âü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Ë∑®ËØ≠Ë®Ä„ÄÅÂ§öÊ®°ÊÄÅÁéØÂ¢É‰∏ãÁöÑÊåá‰ª§Ë∑üÈöèËÉΩÂäõ„ÄÇÂÆÉÁªìÂêà‰∫ÜÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåËßÜËßâ‰∏âÁßçÊ†∏ÂøÉÊ®°ÊÄÅÔºåÂπ∂ÊîØÊåÅËã±ËØ≠„ÄÅÂæ∑ËØ≠„ÄÅÊÑèÂ§ßÂà©ËØ≠Âíå‰∏≠ÊñáÂõõÁßçËØ≠Ë®Ä„ÄÇMCIFÁöÑËÆæËÆ°Êó®Âú®Â°´Ë°•Áé∞ÊúâÂü∫ÂáÜÂú®Â§öËØ≠Ë®ÄÂíåÂ§öÊ®°ÊÄÅËØÑ‰º∞ÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÁâπÂà´ÊòØÂú®ÈïøÁü≠ÊñáÊú¨ËæìÂÖ•ÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÈÄöËøáMCIFÔºåÁ†îÁ©∂‰∫∫ÂëòÂèØ‰ª•Êõ¥ÂÖ®Èù¢Âú∞ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÄßËÉΩÂíåËÉΩÂäõ„ÄÇ","title":"MCIFÔºöË∑®ËØ≠Ë®ÄÂ§öÊ®°ÊÄÅÊåá‰ª§Ë∑üÈöèÁöÑËØÑ‰º∞Êñ∞Âü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MCIFÊòØ‰∏Ä‰∏™Â§öËØ≠Ë®ÄÁöÑ‰∫∫Á±ªÊ†áÊ≥®Âü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Ë∑®ËØ≠Ë®Ä„ÄÅÂ§öÊ®°ÊÄÅÁéØÂ¢É‰∏ãÁöÑÊåá‰ª§Ë∑üÈöèËÉΩÂäõ„ÄÇÂÆÉÁªìÂêà‰∫ÜÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåËßÜËßâ‰∏âÁßçÊ†∏ÂøÉÊ®°ÊÄÅÔºåÂπ∂ÊîØÊåÅËã±ËØ≠„ÄÅÂæ∑ËØ≠„ÄÅÊÑèÂ§ßÂà©ËØ≠Âíå‰∏≠ÊñáÂõõÁßçËØ≠Ë®Ä„ÄÇMCIFÁöÑËÆæËÆ°Êó®Âú®Â°´Ë°•Áé∞ÊúâÂü∫ÂáÜÂú®Â§öËØ≠Ë®ÄÂíåÂ§öÊ®°ÊÄÅËØÑ‰º∞ÊñπÈù¢ÁöÑ‰∏çË∂≥ÔºåÁâπÂà´ÊòØÂú®ÈïøÁü≠ÊñáÊú¨ËæìÂÖ•ÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÈÄöËøáMCIFÔºåÁ†îÁ©∂‰∫∫ÂëòÂèØ‰ª•Êõ¥ÂÖ®Èù¢Âú∞ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÄßËÉΩÂíåËÉΩÂäõ„ÄÇ', title='MCIFÔºöË∑®ËØ≠Ë®ÄÂ§öÊ®°ÊÄÅÊåá‰ª§Ë∑üÈöèÁöÑËØÑ‰º∞Êñ∞Âü∫ÂáÜ'))
[04.08.2025 18:19] Using data from previous issue: {"categories": ["#robotics", "#3d", "#optimization", "#agents", "#games"], "emoji": "üß≠", "ru": {"title": "–ù–∞–≤–∏–≥–∞—Ü–∏—è –≤ 3D —Å –ø–æ–º–æ—â—å—é –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –≥–∞—É—Å—Å–∏–∞–Ω–æ–≤", "desc": "IGL-Nav - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é-—Ü–µ–ª–∏ –≤ —Ç—Ä–µ—Ö–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ 3D –≥
[04.08.2025 18:19] Renaming data file.
[04.08.2025 18:19] Renaming previous data. hf_papers.json to ./d/2025-08-04.json
[04.08.2025 18:19] Saving new data file.
[04.08.2025 18:19] Generating page.
[04.08.2025 18:19] Renaming previous page.
[04.08.2025 18:19] Renaming previous data. index.html to ./d/2025-08-04.html
[04.08.2025 18:19] Writing result.
[04.08.2025 18:19] Renaming log file.
[04.08.2025 18:19] Renaming previous data. log.txt to ./logs/2025-08-04_last_log.txt
