[20.02.2026 20:22] Read previous papers.
[20.02.2026 20:22] Generating top page (month).
[20.02.2026 20:22] Writing top page (month).
[20.02.2026 21:19] Read previous papers.
[20.02.2026 21:19] Get feed.
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13515
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17270
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16855
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15569
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16699
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17004
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16968
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13579
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14457
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17288
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16928
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17365
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17259
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16849
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17363
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15823
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17588
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16756
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14857
[20.02.2026 21:19] Extract page data from URL. URL: https://huggingface.co/papers/2602.10377
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16915
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16835
[20.02.2026 21:19] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16802
[20.02.2026 21:19] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[20.02.2026 21:19] No deleted papers detected.
[20.02.2026 21:19] Downloading and parsing papers (pdf, html). Total: 23.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.13515.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.13515.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.13515.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.17270.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.17270.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.17270.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.16855.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.16855.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.16855.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.15569.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.15569.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.15569.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.16699.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.16699.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.16699.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.17004.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.17004.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.17004.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.16968.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.16968.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.16968.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.13579.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.13579.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.13579.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.14457.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.14457.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.14457.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.17288.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.17288.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.17288.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.16928.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.16928.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.16928.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.17365.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.17365.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.17365.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.17259.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.17259.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.17259.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.16849.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.16849.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.16849.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.17363.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.17363.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.17363.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.15823.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.15823.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.15823.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.17588.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.17588.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.17588.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.16756.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.16756.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.16756.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.14857.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.14857.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.14857.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.10377.
[20.02.2026 21:19] Downloading paper 2602.10377 from https://arxiv.org/pdf/2602.10377v1...
[20.02.2026 21:19] Extracting affiliations from text.
[20.02.2026 21:19] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs Luoyang Sun1,2,3,4, Jiwen Jiang1,2,4, Yifeng Ding4, Fengfa Li4, Yan Song5, Haifeng Zhang2,3, Jian Ying1, Lei Ren4,, Kun Zhan4, Wei Chen4,, Yan Xie4 and Cheng Deng6, 1AI Lab, The Yangtze River Delta, 2Institution of Automation, Chinese Academy of Sciences, 3University of Chinese Academy of Sciences, 4Li Auto, 5University College London, 6The University of Edinburgh Vision-Language-Action Models (VLAs) have emerged as key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design game-changing requirement for on-device LLM deployment, where each hardware platform demands tailored architectural solution. We propose hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge,"
[20.02.2026 21:19] Response: ```python
[
    "AI Lab, The Yangtze River Delta",
    "Institution of Automation, Chinese Academy of Sciences",
    "University of Chinese Academy of Sciences",
    "Li Auto",
    "University College London",
    "The University of Edinburgh"
]
```
[20.02.2026 21:19] Deleting PDF ./assets/pdf/2602.10377.pdf.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.16915.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.16915.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.16915.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.16835.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.16835.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.16835.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Downloading and parsing paper https://huggingface.co/papers/2602.16802.
[20.02.2026 21:19] Extra JSON file exists (./assets/json/2602.16802.json), skip PDF parsing.
[20.02.2026 21:19] Paper image links file exists (./assets/img_data/2602.16802.json), skip HTML parsing.
[20.02.2026 21:19] Success.
[20.02.2026 21:19] Enriching papers with extra data.
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 0. A trainable sparse attention method called SpargeAttention2 is proposed that achieves high sparsity in diffusion models while maintaining generation quality through hybrid masking rules and distillation-inspired fine-tuning.  					AI-generated summary 				 Many training-free sparse attention methods...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 1. Unified Latents framework learns joint latent representations using diffusion prior regularization and diffusion model decoding, achieving competitive FID scores with reduced training compute.  					AI-generated summary 				 We present Unified Latents (UL), a framework for learning latent representa...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 2. GUI-Owl-1.5 is a multi-platform GUI agent model with varying sizes that achieves superior performance across GUI automation, grounding, tool-calling, and memory tasks through innovations in data pipelines, unified reasoning enhancement, and multi-platform reinforcement learning.  					AI-generated s...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 3. Users prefer adaptive feedback mechanisms in in-car AI assistants, starting with high transparency to build trust and then reducing verbosity as reliability increases, particularly in attention-critical driving scenarios.  					AI-generated summary 				 Agentic AI assistants that autonomously perfor...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 4. Large language models can be improved for complex tasks by explicitly reasoning about cost-uncertainty tradeoffs through a Calibrate-Then-Act framework that enhances decision-making in sequential environments.  					AI-generated summary 				 LLMs are increasingly being used for complex problems whic...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 5. Arcee Trinity models are sparse Mixture-of-Experts architectures with varying parameter counts and activation patterns, utilizing advanced attention mechanisms and training optimizations.  					AI-generated summary 				 We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Exp...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 6. Dynamic tokenization improves diffusion transformer efficiency by adjusting patch sizes based on content complexity and denoising timestep, achieving significant speedup without quality loss.  					AI-generated summary 				 Diffusion Transformers (DiTs) have achieved state-of-the-art performance in ...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 7. TactAlign enables transfer of human tactile demonstrations to robots with different embodiments through cross-embodiment tactile alignment without requiring paired data or manual labels.  					AI-generated summary 				 Human demonstrations collected by wearable devices (e.g., tactile gloves) provide...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 8. Frontier AI risk analysis assesses critical dimensions including cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R&D, and self-replication, proposing mitigation strategies for secure deployment of advanced AI systems.  					AI-generated summary 				 To understand and...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 9. Training a 1.36B-parameter scientific language model from raw arXiv LaTeX sources demonstrates the impact of preprocessing decisions, tokenization, and infrastructure constraints on model development under limited computational resources.  					AI-generated summary 				 While frontier large language...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 10. AlphaEvolve, an evolutionary coding agent using large language models, automatically discovers new multiagent learning algorithms for imperfect-information games by evolving regret minimization and population-based training variants.  					AI-generated summary 				 Much of the advancement of Multi-A...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 11. A world model for desktop software that predicts UI state changes through textual description followed by visual synthesis, improving decision quality and execution robustness in computer-using tasks.  					AI-generated summary 				 Agents operating in complex software environments benefit from reas...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 12. FRAPPE addresses limitations in world modeling for robotics by using parallel progressive expansion to improve representation alignment and reduce error accumulation in predictive models.  					AI-generated summary 				 Enabling VLA models to predict environmental dynamics, known as world modeling, ...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 13. Two-layer neural networks solve modular addition by learning Fourier features through phase symmetry and frequency diversification, enabling robust computation via majority voting despite individual neuron noise.  					AI-generated summary 				 We present a comprehensive analysis of how two-layer ne...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 14. Researchers enhance linear attention by simplifying Mamba-2 and improving its architectural components to achieve near-softmax accuracy while maintaining memory efficiency for long sequences.  					AI-generated summary 				 Linear attention transformers have become a strong alternative to softmax at...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 15. CrispEdit is a second-order editing algorithm for large language models that preserves capabilities by constraining updates to low-curvature subspaces of the capability-loss landscape using Bregman divergence and efficient Kronecker-factored approximations.  					AI-generated summary 				 A central ...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 16. Human intervention patterns in web navigation are modeled to improve agent adaptability and collaboration, with language models achieving better intervention prediction and user satisfaction.  					AI-generated summary 				 Despite rapid progress in autonomous web agents, human involvement remains e...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 17. NESSiE benchmark reveals safety vulnerabilities in large language models through simple security tests, demonstrating that even state-of-the-art models fail basic safety requirements without adversarial attacks.  					AI-generated summary 				 We introduce NESSiE, the NEceSsary SafEty benchmark for ...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 18. StarWM, a world model for StarCraft II, predicts future observations under partial observability using a structured textual representation and achieves significant performance improvements in win rate and decision-making stability.  					AI-generated summary 				 Large Language Models (LLMs) have re...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 19. A hardware-software co-design framework for on-device large language model deployment that establishes accuracy-latency relationships through training loss modeling and roofline analysis, enabling rapid architecture selection and improved performance.  					AI-generated summary 				 Vision-Language-...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 20. StereoAdapter-2 improves underwater stereo depth estimation by replacing ConvGRU with a selective state space ConvSS2D operator for efficient long-range propagation and introduces a large-scale synthetic underwater dataset.  					AI-generated summary 				 Stereo depth estimation is fundamental to un...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 21. NeST is a lightweight safety alignment framework that selectively adapts safety-relevant neurons while keeping the rest of the model frozen, achieving significant reductions in unsafe generations with minimal trainable parameters.  					AI-generated summary 				 Safety alignment is essential for the...
[20.02.2026 21:19] ********************************************************************************
[20.02.2026 21:19] Abstract 22. Reference-guided LLM-evaluators enhance LLM alignment by serving as soft verifiers, improving judge accuracy and enabling effective post-training in non-verifiable domains through self-improvement techniques.  					AI-generated summary 				 While Reinforcement Learning with Verifiable Rewards (RLVR)...
[20.02.2026 21:19] Read previous papers.
[20.02.2026 21:19] Generating reviews via LLM API.
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#optimization", "#inference", "#video", "#architecture", "#diffusion", "#training"], "emoji": "‚ö°", "ru": {"title": "–û–±—É—á–∞–µ–º–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –º–æ–¥–µ–ª—è–º–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è SpargeAttention2 ‚Äî –æ–±—É—á–∞–µ–º—ã–π –º–µ—Ç–æ–¥ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è 
[20.02.2026 21:19] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–ï–¥–∏–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Unified Latents ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–µ–≥—É–ª—è—Ä–∏–∑—É—é—Ç—Å—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º –ø—Ä–∏–æ—Ä–æ–º –∏ –¥–µ–∫–æ–¥–∏—Ä—É—é—Ç—Å—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é. 
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#optimization", "#rl", "#data", "#open_source", "#agents", "#benchmark", "#training"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π GUI-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º", "desc": "GUI-Owl-1.5 ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–ø–ª–∞—Ç—Ñ–æ—Ä–º–Ω–∞—è –º–æ–¥–µ–ª—å-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º
[20.02.2026 21:19] Using data from previous issue: {"categories": [], "emoji": "üöó", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –≤ –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞—Ö: –æ—Ç –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –∫–∞–∫ –∞–≥–µ–Ω—Ç–Ω—ã–µ LLM-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –∫–æ–º–º—É–Ω–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –∑–∞–¥–∞—á –≤ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#rl", "#optimization", "#reasoning", "#training", "#agents", "#plp"], "emoji": "‚öñÔ∏è", "ru": {"title": "–Ø–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –æ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–µ –∑–∞—Ç—Ä–∞—Ç –∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –≤ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Calibrate-Then-Act, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –±–æ–ª—å—à–∏
[20.02.2026 21:19] Using data from previous issue: {"categories": [], "emoji": "‚öôÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Mixture-of-Experts —Å –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ç—Ä–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-of-Experts –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Arcee Trinity —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: Trinity L
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#optimization", "#inference", "#video", "#architecture", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#multimodal", "#transfer_learning", "#training", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ç–∞–∫—Ç–∏–ª—å–Ω—ã—Ö –æ—â—É—â–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Ä–æ–±–æ—Ç–∞ –±–µ–∑ –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "TactAlign ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ç–∞–∫—Ç–∏–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ –∫ —Ä–æ–±–æ—Ç—É —Å —Ä–∞–∑–ª–∏—á–Ω–æ–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–µ–π, –∏—Å–ø–æ–ª—å–∑
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#agi", "#alignment", "#security"], "emoji": "‚ö†Ô∏è", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏ –≥—Ä–∞–Ω–∏—Ü AI: –æ—Ç –≤—ã—è–≤–ª–µ–Ω–∏—è —É–≥—Ä–æ–∑ –∫ –Ω–∞–¥–µ–∂–Ω—ã–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º –∑–∞—â–∏—Ç—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤ –ø–µ—Ä–µ–¥–æ–≤—ã—Ö AI —Å–∏—Å—Ç–µ–º, –≤–∫–ª—é—á–∞—é—â–∞—è –∞–Ω–∞–ª–∏–∑ –ø—è—Ç–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π: –∫–∏–±–µ—Ä–∞—Ç–∞–∫, –º
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#training", "#open_source", "#data", "#plp", "#small_models", "#science", "#optimization"], "emoji": "üî¨", "ru": {"title": "–ò–Ω–∂–µ–Ω–µ—Ä–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞—É—á–Ω–æ–π LLM –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö: –æ—Ç —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#games", "#rl", "#agents", "#architecture", "#training"], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω AlphaEvolve ‚Äî —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#multimodal", "#rl", "#agents", "#cv", "#training"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ: –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è —É–º–Ω–æ–≥–æ –ø–æ–º–æ—â–Ω–∏–∫–∞ –≤ –¥–µ—Å–∫—Ç–æ–ø–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –¥–µ—Å–∫—Ç–æ–ø–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è (CUWM), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∏
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#training", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –±—É–¥—É—â–µ–≥–æ –¥–ª—è –æ—Å–æ–∑–Ω–∞—é—â–µ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "FRAPPE ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–∏—Ä–∞ –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é —ç–∫—Å–ø–∞–Ω
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#architecture", "#math", "#training"], "emoji": "üî¢", "ru": {"title": "–ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞ —á–µ—Ä–µ–∑ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –§—É—Ä—å–µ", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç, –∫–∞–∫ –¥–≤—É—Ö—Å–ª–æ–π–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Ä–µ—à–∞—é—Ç –∑–∞–¥–∞—á—É –º–æ–¥—É–ª—å–Ω–æ–≥–æ —Å–ª–æ–∂–µ–Ω–∏—è, –æ–±—É—á–∞—è—Å—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#optimization", "#inference", "#open_source", "#architecture", "#training", "#long_context"], "emoji": "‚ö°", "ru": {"title": "–õ–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —É–ª—É—á—à–∏–ª–∏ –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –ø—É—Ç—ë–º —É–ø—Ä–æ—â–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mamba-2 –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –µ—ë –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#training"], "emoji": "‚úèÔ∏è", "ru": {"title": "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ø—Ä–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–∏–∑–∫–æ–∫—Ä–∏–≤–∏–∑–Ω—ã–µ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "CrispEdit ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#training", "#agents", "#dataset"], "emoji": "ü§ù", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —á–µ–ª–æ–≤–µ–∫–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∑–∞–¥–∞—á–∞ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —á–µ–ª–æ–≤–µ–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤–µ–±-–∑–∞–¥–∞—á. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç 
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#security", "#alignment", "#open_source", "#ethics", "#dataset", "#benchmark"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏: –∫–∞–∫ –ø—Ä–æ—Å—Ç—ã–µ —Ç–µ—Å—Ç—ã –≤—ã—è–≤–ª—è—é—Ç –ø—Ä–æ–≤–∞–ª—ã LLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç NESSiE ‚Äî –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ
[20.02.2026 21:19] Using data from previous issue: {"categories": ["#games", "#reasoning", "#synthetic"], "emoji": "üéÆ", "ru": {"title": "–ú–æ–¥–µ–ª—å –º–∏—Ä–∞ –Ω–∞ —è–∑—ã–∫–µ –¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π –≤ StarCraft II", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω StarWM ‚Äî –ø–µ—Ä–≤–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –∏–≥—Ä—ã StarCraft II, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –±—É–¥—É—â–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –≤ —É—Å–ª–æ–≤–∏—è—Ö —á–∞—Å—Ç–∏—á–Ω–æ–π –≤–∏–¥–∏–º–æ
[20.02.2026 21:19] Querying the API.
[20.02.2026 21:19] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A hardware-software co-design framework for on-device large language model deployment that establishes accuracy-latency relationships through training loss modeling and roofline analysis, enabling rapid architecture selection and improved performance.  					AI-generated summary 				 Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.
[20.02.2026 21:20] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–ø–ø–∞—Ä–∞—Ç–Ω–æ–≥–æ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –ø—Ä–∏ —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∑–∞–∫–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç –ø–æ—Ç–µ—Ä—é –æ–±—É—á–µ–Ω–∏—è –∫–∞–∫ —Ñ—É–Ω–∫—Ü–∏—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç –∑–∞–¥–µ—Ä–∂–∫—É –≤—ã–≤–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é –∞–Ω–∞–ª–∏–∑–∞ roofline. –ü—É—Ç—ë–º –æ–±—É—á–µ–Ω–∏—è 170 –º–æ–¥–µ–ª–µ–π –Ω–∞ 10 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ –ø—Ä—è–º–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å—é, –æ–ø—Ä–µ–¥–µ–ª–∏–≤ —Ñ—Ä–æ–Ω—Ç –ü–∞—Ä–µ—Ç–æ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—Ä–µ–º—è –≤—ã–±–æ—Ä–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–µ—Å—è—Ü–µ–≤ –¥–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–Ω–µ–π, –¥–æ—Å—Ç–∏–≥–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.",
  "emoji": "‚ö°",
  "title": "–°–æ–≤–º–µ—Å—Ç–Ω–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: –æ—Ç –º–µ—Å—è—Ü–µ–≤ –ø–æ–∏—Å–∫–∞ –∫ –¥–Ω—è–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ LLM"
}
```
[20.02.2026 21:20] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A hardware-software co-design framework for on-device large language model deployment that establishes accuracy-latency relationships through training loss modeling and roofline analysis, enabling rapid architecture selection and improved performance.  					AI-generated summary 				 Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available."

[20.02.2026 21:20] Response: ```python
['INFERENCE', 'ARCHITECTURE', 'TRAINING', 'BENCHMARK', 'SMALL_MODELS', 'ROBOTICS', 'MULTIMODAL']
```
[20.02.2026 21:20] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A hardware-software co-design framework for on-device large language model deployment that establishes accuracy-latency relationships through training loss modeling and roofline analysis, enabling rapid architecture selection and improved performance.  					AI-generated summary 				 Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available."

[20.02.2026 21:20] Response: ```python
['OPTIMIZATION', 'OPEN_SOURCE']
```

**Justification:**

- **OPTIMIZATION**: The paper focuses on optimizing LLM deployment through hardware-software co-design, architecture selection, scaling laws, and joint optimization over precision and performance to balance accuracy and latency constraints.

- **OPEN_SOURCE**: The paper explicitly states "We will make the code and related checkpoints publicly available," indicating a commitment to releasing resources to the public.
[20.02.2026 21:20] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

- **OPTIMIZATION**: The paper focuses on optimizing LLM deployment through hardware-software co-design, architecture selection, scaling laws, and joint optimization over precision and performance to balance accuracy and latency constraints.

- **OPEN_SOURCE**: The paper explicitly states "We will make the code and related checkpoints publicly available," indicating a commitment to releasing resources to the public.
[20.02.2026 21:20] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper presents a framework for deploying large language models (LLMs) on devices with limited resources, focusing on the balance between accuracy and latency. It introduces a hardware-software co-design approach that models training loss based on architectural choices and uses roofline analysis to evaluate inference performance. By empirically testing nearly 2,000 architectures, the authors establish a scaling law that connects model architecture to training loss and latency, allowing for efficient architecture selection. The proposed method significantly reduces the time needed for architecture selection and improves performance metrics, making it a valuable contribution to the field of on-device AI.","title":"Optimizing Large Language Models for On-Device Performance"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a framework for deploying large language models (LLMs) on devices with limited resources, focusing on the balance between accuracy and latency. It introduces a hardware-software co-design approach that models training loss based on architectural choices and uses roofline analysis to evaluate inference performance. By empirically testing nearly 2,000 architectures, the authors establish a scaling law that connects model architecture to training loss and latency, allowing for efficient architecture selection. The proposed method significantly reduces the time needed for architecture selection and improves performance metrics, making it a valuable contribution to the field of on-device AI.', title='Optimizing Large Language Models for On-Device Performance'))
[20.02.2026 21:20] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ°¨‰ª∂-ËΩØ‰ª∂ÂçèÂêåËÆæËÆ°Ê°ÜÊû∂ÔºåÁî®‰∫éÂú®ËÆæÂ§á‰∏äÈÉ®ÁΩ≤Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáËÆ≠ÁªÉÊçüÂ§±Âª∫Ê®°ÂíåÂ±ãÈ°∂Á∫øÂàÜÊûêÔºåÂª∫Á´ã‰∫ÜÂáÜÁ°ÆÊÄß‰∏éÂª∂Ëøü‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºå‰ªéËÄåÂä†ÈÄüÊû∂ÊûÑÈÄâÊã©Âπ∂ÊèêÂçáÊÄßËÉΩ„ÄÇÊàë‰ª¨ÂØπ1942ÁßçÂÄôÈÄâÊû∂ÊûÑËøõË°å‰∫ÜÂÆûËØÅËØÑ‰º∞ÔºåÂπ∂ÈÄöËøáÁªìÂêàÁº©ÊîæÊ≥ïÂàô‰∏éÂª∂ËøüÂª∫Ê®°ÔºåÁ°ÆÂÆö‰∫ÜÁ°¨‰ª∂ÂçèÂêåËÆæËÆ°LLMÁöÑÂ∏ïÁ¥ØÊâòÂâçÊ≤ø„ÄÇËØ•ÊñπÊ≥ïÂ∞ÜÊû∂ÊûÑÈÄâÊã©ÁöÑÊó∂Èó¥‰ªéÂá†‰∏™ÊúàÁº©Áü≠Âà∞Âá†Â§©ÔºåÂπ∂Âú®ÁõÆÊ†áÁ°¨‰ª∂‰∏äÂÆûÁé∞‰∫ÜÊõ¥‰ΩéÁöÑÂõ∞ÊÉëÂ∫¶„ÄÇ","title":"Á°¨‰ª∂-ËΩØ‰ª∂ÂçèÂêåËÆæËÆ°ÔºåÊèêÂçáLLMÈÉ®ÁΩ≤ÊïàÁéá"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ°¨‰ª∂-ËΩØ‰ª∂ÂçèÂêåËÆæËÆ°Ê°ÜÊû∂ÔºåÁî®‰∫éÂú®ËÆæÂ§á‰∏äÈÉ®ÁΩ≤Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáËÆ≠ÁªÉÊçüÂ§±Âª∫Ê®°ÂíåÂ±ãÈ°∂Á∫øÂàÜÊûêÔºåÂª∫Á´ã‰∫ÜÂáÜÁ°ÆÊÄß‰∏éÂª∂Ëøü‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºå‰ªéËÄåÂä†ÈÄüÊû∂ÊûÑÈÄâÊã©Âπ∂ÊèêÂçáÊÄßËÉΩ„ÄÇÊàë‰ª¨ÂØπ1942ÁßçÂÄôÈÄâÊû∂ÊûÑËøõË°å‰∫ÜÂÆûËØÅËØÑ‰º∞ÔºåÂπ∂ÈÄöËøáÁªìÂêàÁº©ÊîæÊ≥ïÂàô‰∏éÂª∂ËøüÂª∫Ê®°ÔºåÁ°ÆÂÆö‰∫ÜÁ°¨‰ª∂ÂçèÂêåËÆæËÆ°LLMÁöÑÂ∏ïÁ¥ØÊâòÂâçÊ≤ø„ÄÇËØ•ÊñπÊ≥ïÂ∞ÜÊû∂ÊûÑÈÄâÊã©ÁöÑÊó∂Èó¥‰ªéÂá†‰∏™ÊúàÁº©Áü≠Âà∞Âá†Â§©ÔºåÂπ∂Âú®ÁõÆÊ†áÁ°¨‰ª∂‰∏äÂÆûÁé∞‰∫ÜÊõ¥‰ΩéÁöÑÂõ∞ÊÉëÂ∫¶„ÄÇ', title='Á°¨‰ª∂-ËΩØ‰ª∂ÂçèÂêåËÆæËÆ°ÔºåÊèêÂçáLLMÈÉ®ÁΩ≤ÊïàÁéá'))
[20.02.2026 21:20] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#open_source", "#cv", "#synthetic", "#architecture"], "emoji": "üåä", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å—Ç–µ—Ä–µ–æ–≥–ª—É–±–∏–Ω–∞ –ø–æ–¥ –≤–æ–¥–æ–π —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–∏–≤–Ω—ã–µ state space –º–æ–¥–µ–ª–∏", "desc": "StereoAdapter-2 —É–ª—É—á—à–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –≥–ª—É–±–∏–Ω—ã –≤ –ø–æ–¥–≤–æ–¥–Ω—ã—Ö —Å—Ç–µ—Ä–µ–æ–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, –∑–∞–º–µ–Ω–∏–≤ ConvGRU –Ω
[20.02.2026 21:20] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#benchmark", "#alignment", "#security", "#training"], "emoji": "üõ°Ô∏è", "ru": {"title": "–¶–µ–ª–µ–≤–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: –º–∏–Ω–∏–º—É–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –º–∞–∫—Å–∏–º—É–º –∑–∞—â–∏—Ç—ã", "desc": "NeST ‚Äî —ç—Ç–æ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Å–µ–ª–µ–∫
[20.02.2026 21:20] Using data from previous issue: {"categories": ["#alignment", "#reasoning"], "emoji": "üéØ", "ru": {"title": "–≠—Ç–∞–ª–æ–Ω–Ω—ã–µ LLM-–æ—Ü–µ–Ω—â–∏–∫–∏ –∫–∞–∫ –º–æ—Å—Ç–∏–∫ –∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é –≤ –Ω–µ–≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º—è–≥–∫–∏—Ö –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –æ–±–ª–∞—Å—Ç—è—Ö –±–µ–∑ –æ–±—ä–µ–∫—Ç–∏–≤–Ω
[20.02.2026 21:20] Renaming data file.
[20.02.2026 21:20] Renaming previous data. hf_papers.json to ./d/2026-02-20.json
[20.02.2026 21:20] Saving new data file.
[20.02.2026 21:20] Generating page.
[20.02.2026 21:20] Renaming previous page.
[20.02.2026 21:20] Renaming previous data. index.html to ./d/2026-02-20.html
[20.02.2026 21:20] Writing result.
[20.02.2026 21:20] Renaming log file.
[20.02.2026 21:20] Renaming previous data. log.txt to ./logs/2026-02-20_last_log.txt
