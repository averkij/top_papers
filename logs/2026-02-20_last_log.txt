[20.02.2026 08:34] Read previous papers.
[20.02.2026 08:34] Generating top page (month).
[20.02.2026 08:34] Writing top page (month).
[20.02.2026 09:34] Read previous papers.
[20.02.2026 09:34] Get feed.
[20.02.2026 09:34] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13515
[20.02.2026 09:34] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17270
[20.02.2026 09:34] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16855
[20.02.2026 09:34] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14457
[20.02.2026 09:34] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16968
[20.02.2026 09:34] Extract page data from URL. URL: https://huggingface.co/papers/2602.17288
[20.02.2026 09:34] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17004
[20.02.2026 09:34] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16928
[20.02.2026 09:34] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15569
[20.02.2026 09:34] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17259
[20.02.2026 09:34] Extract page data from URL. URL: https://huggingface.co/papers/2602.14857
[20.02.2026 09:34] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17365
[20.02.2026 09:34] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17363
[20.02.2026 09:34] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16802
[20.02.2026 09:34] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13579
[20.02.2026 09:34] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[20.02.2026 09:34] No deleted papers detected.
[20.02.2026 09:34] Downloading and parsing papers (pdf, html). Total: 15.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.13515.
[20.02.2026 09:34] Extra JSON file exists (./assets/json/2602.13515.json), skip PDF parsing.
[20.02.2026 09:34] Paper image links file exists (./assets/img_data/2602.13515.json), skip HTML parsing.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.17270.
[20.02.2026 09:34] Extra JSON file exists (./assets/json/2602.17270.json), skip PDF parsing.
[20.02.2026 09:34] Paper image links file exists (./assets/img_data/2602.17270.json), skip HTML parsing.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.16855.
[20.02.2026 09:34] Extra JSON file exists (./assets/json/2602.16855.json), skip PDF parsing.
[20.02.2026 09:34] Paper image links file exists (./assets/img_data/2602.16855.json), skip HTML parsing.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.14457.
[20.02.2026 09:34] Extra JSON file exists (./assets/json/2602.14457.json), skip PDF parsing.
[20.02.2026 09:34] Paper image links file exists (./assets/img_data/2602.14457.json), skip HTML parsing.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.16968.
[20.02.2026 09:34] Extra JSON file exists (./assets/json/2602.16968.json), skip PDF parsing.
[20.02.2026 09:34] Paper image links file exists (./assets/img_data/2602.16968.json), skip HTML parsing.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.17288.
[20.02.2026 09:34] Downloading paper 2602.17288 from https://arxiv.org/pdf/2602.17288v1...
[20.02.2026 09:34] Extracting affiliations from text.
[20.02.2026 09:34] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ARXIV-TO-MODEL: PRACTICAL STUDY OF SCIENTIFIC LM TRAINING 6 2 0 2 9 1 ] . [ 1 8 8 2 7 1 . 2 0 6 2 : r Anuj Gupta Independent Researcher India anuj0456@gmail.com "
[20.02.2026 09:34] Response: ```python
["Independent Researcher"]
```
[20.02.2026 09:34] Deleting PDF ./assets/pdf/2602.17288.pdf.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.17004.
[20.02.2026 09:34] Extra JSON file exists (./assets/json/2602.17004.json), skip PDF parsing.
[20.02.2026 09:34] Paper image links file exists (./assets/img_data/2602.17004.json), skip HTML parsing.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.16928.
[20.02.2026 09:34] Extra JSON file exists (./assets/json/2602.16928.json), skip PDF parsing.
[20.02.2026 09:34] Paper image links file exists (./assets/img_data/2602.16928.json), skip HTML parsing.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.15569.
[20.02.2026 09:34] Extra JSON file exists (./assets/json/2602.15569.json), skip PDF parsing.
[20.02.2026 09:34] Paper image links file exists (./assets/img_data/2602.15569.json), skip HTML parsing.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.17259.
[20.02.2026 09:34] Extra JSON file exists (./assets/json/2602.17259.json), skip PDF parsing.
[20.02.2026 09:34] Paper image links file exists (./assets/img_data/2602.17259.json), skip HTML parsing.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.14857.
[20.02.2026 09:34] Downloading paper 2602.14857 from https://arxiv.org/pdf/2602.14857v1...
[20.02.2026 09:34] Extracting affiliations from text.
[20.02.2026 09:34] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Yixin Zhang1,2 , Ziyi Wang1,2 , Yiming Rong1,2 , Haoxi Wang1,2 , Jinling Jiang1,2 , Shuang Xu1 , Haoran Wu1 , Shiyu Zhou1 , Bo Xu1,2 1The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences 2School of Artificial Intelligence, University of Chinese Academy of Sciences {zhangyixin2024, wuhaoran2018, shiyu.zhou, xubo}@ia.ac.cn 6 2 0 2 6 1 ] . [ 1 7 5 8 4 1 . 2 0 6 2 : r Abstract Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2s hybrid dynamics, we introduce structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instructiontuning dataset for SC2 dynamics prediction. We further develop multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWMs substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, worldmodel-augmented decision system that integrates StarWM into GenerateSimulateRefine decision loop for foresight-driven policy refinement. Online evaluation against SC2s built-in AI demonstrates consistent improvements, yielding winrate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and"
[20.02.2026 09:34] Response: ```python
[
    "The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences",
    "School of Artificial Intelligence, University of Chinese Academy of Sciences"
]
```
[20.02.2026 09:34] Deleting PDF ./assets/pdf/2602.14857.pdf.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.17365.
[20.02.2026 09:34] Extra JSON file exists (./assets/json/2602.17365.json), skip PDF parsing.
[20.02.2026 09:34] Paper image links file exists (./assets/img_data/2602.17365.json), skip HTML parsing.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.17363.
[20.02.2026 09:34] Extra JSON file exists (./assets/json/2602.17363.json), skip PDF parsing.
[20.02.2026 09:34] Paper image links file exists (./assets/img_data/2602.17363.json), skip HTML parsing.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.16802.
[20.02.2026 09:34] Extra JSON file exists (./assets/json/2602.16802.json), skip PDF parsing.
[20.02.2026 09:34] Paper image links file exists (./assets/img_data/2602.16802.json), skip HTML parsing.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Downloading and parsing paper https://huggingface.co/papers/2602.13579.
[20.02.2026 09:34] Extra JSON file exists (./assets/json/2602.13579.json), skip PDF parsing.
[20.02.2026 09:34] Paper image links file exists (./assets/img_data/2602.13579.json), skip HTML parsing.
[20.02.2026 09:34] Success.
[20.02.2026 09:34] Enriching papers with extra data.
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 0. A trainable sparse attention method called SpargeAttention2 is proposed that achieves high sparsity in diffusion models while maintaining generation quality through hybrid masking rules and distillation-inspired fine-tuning.  					AI-generated summary 				 Many training-free sparse attention methods...
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 1. Unified Latents framework learns joint latent representations using diffusion prior regularization and diffusion model decoding, achieving competitive FID scores with reduced training compute.  					AI-generated summary 				 We present Unified Latents (UL), a framework for learning latent representa...
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 2. GUI-Owl-1.5 is a multi-platform GUI agent model with varying sizes that achieves superior performance across GUI automation, grounding, tool-calling, and memory tasks through innovations in data pipelines, unified reasoning enhancement, and multi-platform reinforcement learning.  					AI-generated s...
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 3. Frontier AI risk analysis assesses critical dimensions including cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R&D, and self-replication, proposing mitigation strategies for secure deployment of advanced AI systems.  					AI-generated summary 				 To understand and...
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 4. Dynamic tokenization improves diffusion transformer efficiency by adjusting patch sizes based on content complexity and denoising timestep, achieving significant speedup without quality loss.  					AI-generated summary 				 Diffusion Transformers (DiTs) have achieved state-of-the-art performance in ...
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 5. Training a 1.36B-parameter scientific language model from raw arXiv LaTeX sources demonstrates the impact of preprocessing decisions, tokenization, and infrastructure constraints on model development under limited computational resources.  					AI-generated summary 				 While frontier large language...
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 6. Arcee Trinity models are sparse Mixture-of-Experts architectures with varying parameter counts and activation patterns, utilizing advanced attention mechanisms and training optimizations.  					AI-generated summary 				 We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Exp...
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 7. AlphaEvolve, an evolutionary coding agent using large language models, automatically discovers new multiagent learning algorithms for imperfect-information games by evolving regret minimization and population-based training variants.  					AI-generated summary 				 Much of the advancement of Multi-A...
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 8. Users prefer adaptive feedback mechanisms in in-car AI assistants, starting with high transparency to build trust and then reducing verbosity as reliability increases, particularly in attention-critical driving scenarios.  					AI-generated summary 				 Agentic AI assistants that autonomously perfor...
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 9. FRAPPE addresses limitations in world modeling for robotics by using parallel progressive expansion to improve representation alignment and reduce error accumulation in predictive models.  					AI-generated summary 				 Enabling VLA models to predict environmental dynamics, known as world modeling, ...
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 10. StarWM, a world model for StarCraft II, predicts future observations under partial observability using a structured textual representation and achieves significant performance improvements in win rate and decision-making stability.  					AI-generated summary 				 Large Language Models (LLMs) have re...
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 11. A world model for desktop software that predicts UI state changes through textual description followed by visual synthesis, improving decision quality and execution robustness in computer-using tasks.  					AI-generated summary 				 Agents operating in complex software environments benefit from reas...
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 12. Researchers enhance linear attention by simplifying Mamba-2 and improving its architectural components to achieve near-softmax accuracy while maintaining memory efficiency for long sequences.  					AI-generated summary 				 Linear attention transformers have become a strong alternative to softmax at...
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 13. Reference-guided LLM-evaluators enhance LLM alignment by serving as soft verifiers, improving judge accuracy and enabling effective post-training in non-verifiable domains through self-improvement techniques.  					AI-generated summary 				 While Reinforcement Learning with Verifiable Rewards (RLVR)...
[20.02.2026 09:34] ********************************************************************************
[20.02.2026 09:34] Abstract 14. TactAlign enables transfer of human tactile demonstrations to robots with different embodiments through cross-embodiment tactile alignment without requiring paired data or manual labels.  					AI-generated summary 				 Human demonstrations collected by wearable devices (e.g., tactile gloves) provide...
[20.02.2026 09:34] Read previous papers.
[20.02.2026 09:34] Generating reviews via LLM API.
[20.02.2026 09:34] Using data from previous issue: {"categories": ["#optimization", "#inference", "#video", "#architecture", "#diffusion", "#training"], "emoji": "‚ö°", "ru": {"title": "–û–±—É—á–∞–µ–º–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –º–æ–¥–µ–ª—è–º–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è SpargeAttention2 ‚Äî –æ–±—É—á–∞–µ–º—ã–π –º–µ—Ç–æ–¥ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è 
[20.02.2026 09:34] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–ï–¥–∏–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Unified Latents ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–µ–≥—É–ª—è—Ä–∏–∑—É—é—Ç—Å—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º –ø—Ä–∏–æ—Ä–æ–º –∏ –¥–µ–∫–æ–¥–∏—Ä—É—é—Ç—Å—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é. 
[20.02.2026 09:34] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#optimization", "#rl", "#data", "#open_source", "#agents", "#benchmark", "#training"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π GUI-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º", "desc": "GUI-Owl-1.5 ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–ø–ª–∞—Ç—Ñ–æ—Ä–º–Ω–∞—è –º–æ–¥–µ–ª—å-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º
[20.02.2026 09:34] Using data from previous issue: {"categories": ["#agi", "#alignment", "#security"], "emoji": "‚ö†Ô∏è", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏ –≥—Ä–∞–Ω–∏—Ü AI: –æ—Ç –≤—ã—è–≤–ª–µ–Ω–∏—è —É–≥—Ä–æ–∑ –∫ –Ω–∞–¥–µ–∂–Ω—ã–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º –∑–∞—â–∏—Ç—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤ –ø–µ—Ä–µ–¥–æ–≤—ã—Ö AI —Å–∏—Å—Ç–µ–º, –≤–∫–ª—é—á–∞—é—â–∞—è –∞–Ω–∞–ª–∏–∑ –ø—è—Ç–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π: –∫–∏–±–µ—Ä–∞—Ç–∞–∫, –º
[20.02.2026 09:34] Using data from previous issue: {"categories": ["#optimization", "#inference", "#video", "#architecture", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä
[20.02.2026 09:34] Querying the API.
[20.02.2026 09:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Training a 1.36B-parameter scientific language model from raw arXiv LaTeX sources demonstrates the impact of preprocessing decisions, tokenization, and infrastructure constraints on model development under limited computational resources.  					AI-generated summary 				 While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.
[20.02.2026 09:34] Response: ```json
{
  "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞—É—á–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å 1.36 –º–∏–ª–ª–∏–∞—Ä–¥–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ —Å—ã—Ä—ã—Ö LaTeX –∏—Å—Ö–æ–¥–Ω–∏–∫–∞—Ö –∏–∑ arXiv. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –ø–æ–ª–Ω—ã–π pipeline, –≤–∫–ª—é—á–∞—é—â–∏–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, –≤–∞–ª–∏–¥–∞—Ü–∏—é –∞—Ä—Ö–∏–≤–æ–≤, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ LaTeX, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞, –¥–æ–º–µ–Ω–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏ –æ–±—É—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö (2 GPU A100). –ß–µ—Ä–µ–∑ 24 —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è, –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ—Ç–µ—Ä–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —É–∑–∫–∏–µ –º–µ—Å—Ç–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–ª–∏—è—é—Ç –Ω–∞ –æ–±—ä–µ–º –ø–æ–ª–µ–∑–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤–ª–∏—è–µ—Ç –Ω–∞ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å, –∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –∏ –≤–≤–æ–¥–∞-–≤—ã–≤–æ–¥–∞ –º–æ–≥—É—Ç –±—ã—Ç—å —Ç–∞–∫–∏–º–∏ –∂–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–º–∏, –∫–∞–∫ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã.",
  "emoji": "üî¨",
  "title": "–ò–Ω–∂–µ–Ω–µ—Ä–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞—É—á–Ω–æ–π LLM –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö: –æ—Ç —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"
}
```
[20.02.2026 09:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Training a 1.36B-parameter scientific language model from raw arXiv LaTeX sources demonstrates the impact of preprocessing decisions, tokenization, and infrastructure constraints on model development under limited computational resources.  					AI-generated summary 				 While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models."

[20.02.2026 09:34] Response: ```python
['SMALL_MODELS', 'DATA', 'TRAINING', 'PLP']
```
[20.02.2026 09:34] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Training a 1.36B-parameter scientific language model from raw arXiv LaTeX sources demonstrates the impact of preprocessing decisions, tokenization, and infrastructure constraints on model development under limited computational resources.  					AI-generated summary 				 While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models."

[20.02.2026 09:34] Response: ```python
['SCIENCE', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[20.02.2026 09:34] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper discusses the training of a 1.36 billion-parameter scientific language model using raw LaTeX sources from arXiv, focusing on the importance of preprocessing, tokenization, and infrastructure limitations. The authors present a comprehensive pipeline that includes steps like metadata filtering and domain-aware tokenization, all while using limited computational resources. Through extensive experiments, they analyze factors such as training stability and data yield losses, revealing how preprocessing choices can significantly influence model performance. The findings aim to assist researchers with moderate compute budgets in developing specialized models by providing a clear and practical approach to the training process.","title":"Building Scientific Language Models on a Budget"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper discusses the training of a 1.36 billion-parameter scientific language model using raw LaTeX sources from arXiv, focusing on the importance of preprocessing, tokenization, and infrastructure limitations. The authors present a comprehensive pipeline that includes steps like metadata filtering and domain-aware tokenization, all while using limited computational resources. Through extensive experiments, they analyze factors such as training stability and data yield losses, revealing how preprocessing choices can significantly influence model performance. The findings aim to assist researchers with moderate compute budgets in developing specialized models by providing a clear and practical approach to the training process.', title='Building Scientific Language Models on a Budget'))
[20.02.2026 09:34] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨Á†îÁ©∂Â±ïÁ§∫‰∫ÜÂ¶Ç‰Ωï‰ªéÂéüÂßãÁöÑarXiv LaTeXÊ∫êÊñá‰ª∂‰∏≠ËÆ≠ÁªÉ‰∏Ä‰∏™1.36BÂèÇÊï∞ÁöÑÁßëÂ≠¶ËØ≠Ë®ÄÊ®°Âûã„ÄÇÊàë‰ª¨ËØ¶ÁªÜÊèèËø∞‰∫Ü‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÊµÅÁ®ãÔºåÂåÖÊã¨ÂÖÉÊï∞ÊçÆËøáÊª§„ÄÅÂΩíÊ°£È™åËØÅ„ÄÅLaTeXÊèêÂèñ„ÄÅÊñáÊú¨ËßÑËåÉÂåñ„ÄÅÈ¢ÜÂüüÊÑüÁü•ÁöÑÂàÜËØçÂíåÂú®ÊúâÈôêËÆ°ÁÆóËµÑÊ∫ê‰∏ãÁöÑÂØÜÈõÜÂèòÊç¢Âô®ËÆ≠ÁªÉ„ÄÇÈÄöËøá24Ê¨°ÂÆûÈ™åÔºåÊàë‰ª¨ÂàÜÊûê‰∫ÜËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄß„ÄÅÊâ©Â±ïË°å‰∏∫„ÄÅÊï∞ÊçÆÊçüÂ§±ÂíåÂü∫Á°ÄËÆæÊñΩÁì∂È¢à„ÄÇÁ†îÁ©∂ÁªìÊûúÂº∫Ë∞É‰∫ÜÈ¢ÑÂ§ÑÁêÜÂÜ≥Á≠ñÂØπÂèØÁî®Ê†áËÆ∞ÈáèÁöÑÊòæËëóÂΩ±ÂìçÔºå‰ª•ÂèäÂ≠òÂÇ®ÂíåI/OÈôêÂà∂Â¶Ç‰Ωï‰∏éËÆ°ÁÆóËÉΩÂäõÁõ∏Â™≤Áæé„ÄÇ","title":"‰ªéÂéüÂßãÊï∞ÊçÆËÆ≠ÁªÉÁßëÂ≠¶ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆûË∑µÊé¢Á¥¢"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Á†îÁ©∂Â±ïÁ§∫‰∫ÜÂ¶Ç‰Ωï‰ªéÂéüÂßãÁöÑarXiv LaTeXÊ∫êÊñá‰ª∂‰∏≠ËÆ≠ÁªÉ‰∏Ä‰∏™1.36BÂèÇÊï∞ÁöÑÁßëÂ≠¶ËØ≠Ë®ÄÊ®°Âûã„ÄÇÊàë‰ª¨ËØ¶ÁªÜÊèèËø∞‰∫Ü‰∏Ä‰∏™Á´ØÂà∞Á´ØÁöÑÊµÅÁ®ãÔºåÂåÖÊã¨ÂÖÉÊï∞ÊçÆËøáÊª§„ÄÅÂΩíÊ°£È™åËØÅ„ÄÅLaTeXÊèêÂèñ„ÄÅÊñáÊú¨ËßÑËåÉÂåñ„ÄÅÈ¢ÜÂüüÊÑüÁü•ÁöÑÂàÜËØçÂíåÂú®ÊúâÈôêËÆ°ÁÆóËµÑÊ∫ê‰∏ãÁöÑÂØÜÈõÜÂèòÊç¢Âô®ËÆ≠ÁªÉ„ÄÇÈÄöËøá24Ê¨°ÂÆûÈ™åÔºåÊàë‰ª¨ÂàÜÊûê‰∫ÜËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄß„ÄÅÊâ©Â±ïË°å‰∏∫„ÄÅÊï∞ÊçÆÊçüÂ§±ÂíåÂü∫Á°ÄËÆæÊñΩÁì∂È¢à„ÄÇÁ†îÁ©∂ÁªìÊûúÂº∫Ë∞É‰∫ÜÈ¢ÑÂ§ÑÁêÜÂÜ≥Á≠ñÂØπÂèØÁî®Ê†áËÆ∞ÈáèÁöÑÊòæËëóÂΩ±ÂìçÔºå‰ª•ÂèäÂ≠òÂÇ®ÂíåI/OÈôêÂà∂Â¶Ç‰Ωï‰∏éËÆ°ÁÆóËÉΩÂäõÁõ∏Â™≤Áæé„ÄÇ', title='‰ªéÂéüÂßãÊï∞ÊçÆËÆ≠ÁªÉÁßëÂ≠¶ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆûË∑µÊé¢Á¥¢'))
[20.02.2026 09:34] Using data from previous issue: {"categories": [], "emoji": "‚öôÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Mixture-of-Experts —Å –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ç—Ä–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-of-Experts –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Arcee Trinity —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: Trinity L
[20.02.2026 09:34] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#games", "#rl", "#agents", "#architecture", "#training"], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω AlphaEvolve ‚Äî —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º
[20.02.2026 09:34] Using data from previous issue: {"categories": [], "emoji": "üöó", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –≤ –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞—Ö: –æ—Ç –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –∫–∞–∫ –∞–≥–µ–Ω—Ç–Ω—ã–µ LLM-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –∫–æ–º–º—É–Ω–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –∑–∞–¥–∞—á –≤ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª
[20.02.2026 09:34] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#training", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –±—É–¥—É—â–µ–≥–æ –¥–ª—è –æ—Å–æ–∑–Ω–∞—é—â–µ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "FRAPPE ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–∏—Ä–∞ –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é —ç–∫—Å–ø–∞–Ω
[20.02.2026 09:34] Querying the API.
[20.02.2026 09:34] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

StarWM, a world model for StarCraft II, predicts future observations under partial observability using a structured textual representation and achieves significant performance improvements in win rate and decision-making stability.  					AI-generated summary 				 Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.
[20.02.2026 09:35] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω StarWM ‚Äî –ø–µ—Ä–≤–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –∏–≥—Ä—ã StarCraft II, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –±—É–¥—É—â–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –≤ —É—Å–ª–æ–≤–∏—è—Ö —á–∞—Å—Ç–∏—á–Ω–æ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –Ω–∞ –ø—è—Ç—å –º–æ–¥—É–ª–µ–π –∏ —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç SC2-Dynamics-50k –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é –¥–∏–Ω–∞–º–∏–∫–∏ –∏–≥—Ä—ã. StarWM-Agent –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –≤ —Ü–∏–∫–ª Generate-Simulate-Refine, –ø–æ–∑–≤–æ–ª—è—è –∞–≥–µ–Ω—Ç—É —É–ª—É—á—à–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è: 60% –ø—Ä–∏—Ä–æ—Å—Ç –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ 30% –ø—Ä–∏—Ä–æ—Å—Ç win-rate –ø—Ä–æ—Ç–∏–≤ —Å–∏–ª—å–Ω—ã—Ö –ø—Ä–æ—Ç–∏–≤–Ω–∏–∫–æ–≤.",
  "emoji": "üéÆ",
  "title": "–ú–æ–¥–µ–ª—å –º–∏—Ä–∞ –Ω–∞ —è–∑—ã–∫–µ –¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π –≤ StarCraft II"
}
```
[20.02.2026 09:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"StarWM, a world model for StarCraft II, predicts future observations under partial observability using a structured textual representation and achieves significant performance improvements in win rate and decision-making stability.  					AI-generated summary 				 Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment."

[20.02.2026 09:35] Response: ```python
["AGENTS", "DATASET", "BENCHMARK", "TRAINING"]
```

**Justification:**

- **AGENTS**: The paper explicitly develops "StarWM-Agent, a world-model-augmented decision system" and discusses LLM-based agents for StarCraft II decision-making in complex environments.

- **DATASET**: The paper introduces "SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction," which is a new dataset contribution.

- **BENCHMARK**: The paper proposes "a multi-dimensional offline evaluation framework for predicted structured observations" and includes online evaluation against SC2's built-in AI, establishing evaluation methodologies.

- **TRAINING**: The paper focuses on training a world model (StarWM) with instruction-tuning and discusses fine-tuning approaches for decision-making policies in the context of SC2.
[20.02.2026 09:35] Error. Failed to parse JSON from LLM. ["AGENTS", "DATASET", "BENCHMARK", "TRAINING"]


**Justification:**

- **AGENTS**: The paper explicitly develops "StarWM-Agent, a world-model-augmented decision system" and discusses LLM-based agents for StarCraft II decision-making in complex environments.

- **DATASET**: The paper introduces "SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction," which is a new dataset contribution.

- **BENCHMARK**: The paper proposes "a multi-dimensional offline evaluation framework for predicted structured observations" and includes online evaluation against SC2"s built-in AI, establishing evaluation methodologies.

- **TRAINING**: The paper focuses on training a world model (StarWM) with instruction-tuning and discusses fine-tuning approaches for decision-making policies in the context of SC2.
[20.02.2026 09:35] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"StarWM, a world model for StarCraft II, predicts future observations under partial observability using a structured textual representation and achieves significant performance improvements in win rate and decision-making stability.  					AI-generated summary 				 Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment."

[20.02.2026 09:35] Response: ```python
["GAMES", "REASONING", "SYNTHETIC"]
```
[20.02.2026 09:35] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"StarWM is a novel world model designed for StarCraft II that enhances decision-making under partial observability by predicting future observations. It utilizes a structured textual representation to break down observations into five semantic modules, allowing for better understanding of the game\'s dynamics. The model is trained on a new dataset, SC2-Dynamics-50k, which focuses on instruction-tuning for predicting game dynamics. StarWM significantly improves win rates and decision-making stability, demonstrating its effectiveness in complex environments like StarCraft II.","title":"StarWM: Enhancing StarCraft II Decision-Making with Predictive World Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="StarWM is a novel world model designed for StarCraft II that enhances decision-making under partial observability by predicting future observations. It utilizes a structured textual representation to break down observations into five semantic modules, allowing for better understanding of the game's dynamics. The model is trained on a new dataset, SC2-Dynamics-50k, which focuses on instruction-tuning for predicting game dynamics. StarWM significantly improves win rates and decision-making stability, demonstrating its effectiveness in complex environments like StarCraft II.", title='StarWM: Enhancing StarCraft II Decision-Making with Predictive World Models'))
[20.02.2026 09:35] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"StarWMÊòØ‰∏Ä‰∏™Áî®‰∫é„ÄäÊòüÈôÖ‰∫âÈú∏II„ÄãÁöÑ‰∏ñÁïåÊ®°ÂûãÔºåËÉΩÂ§üÂú®ÈÉ®ÂàÜÂèØËßÇÊµãÁöÑÊÉÖÂÜµ‰∏ãÈ¢ÑÊµãÊú™Êù•ÁöÑËßÇÂØüÁªìÊûú„ÄÇÂÆÉÈÄöËøáÁªìÊûÑÂåñÁöÑÊñáÊú¨Ë°®Á§∫ÔºåÂ∞ÜËßÇÂØüÂàÜËß£‰∏∫‰∫î‰∏™ËØ≠‰πâÊ®°ÂùóÔºå‰ªéËÄåÊúâÊïàÂ≠¶‰π†Ê∏∏ÊàèÁöÑÊ∑∑ÂêàÂä®ÊÄÅ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåStarWMÂú®ËµÑÊ∫êÈ¢ÑÊµãÂáÜÁ°ÆÊÄßÂíåËá™ÊàëÂÆèËßÇÊÉÖÂÜµ‰∏ÄËá¥ÊÄßÊñπÈù¢ÔºåËæÉÈõ∂Ê†∑Êú¨Âü∫Á∫øÊúâËøë60%ÁöÑÊèêÂçá„ÄÇÈÄöËøáÂ∞ÜStarWMÈõÜÊàêÂà∞ÁîüÊàê-Ê®°Êãü-Á≤æÁÇºÁöÑÂÜ≥Á≠ñÂæ™ÁéØ‰∏≠ÔºåStarWM-AgentÂú®‰∏éÊ∏∏ÊàèÂÜÖÁΩÆAIÁöÑÂØπÊäó‰∏≠ÔºåËµ¢ÂæóÁéáÂàÜÂà´ÊèêÈ´ò‰∫Ü30%„ÄÅ15%Âíå30%„ÄÇ","title":"StarWMÔºöÊèêÂçá„ÄäÊòüÈôÖ‰∫âÈú∏II„ÄãÂÜ≥Á≠ñÁöÑ‰∏ñÁïåÊ®°Âûã"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='StarWMÊòØ‰∏Ä‰∏™Áî®‰∫é„ÄäÊòüÈôÖ‰∫âÈú∏II„ÄãÁöÑ‰∏ñÁïåÊ®°ÂûãÔºåËÉΩÂ§üÂú®ÈÉ®ÂàÜÂèØËßÇÊµãÁöÑÊÉÖÂÜµ‰∏ãÈ¢ÑÊµãÊú™Êù•ÁöÑËßÇÂØüÁªìÊûú„ÄÇÂÆÉÈÄöËøáÁªìÊûÑÂåñÁöÑÊñáÊú¨Ë°®Á§∫ÔºåÂ∞ÜËßÇÂØüÂàÜËß£‰∏∫‰∫î‰∏™ËØ≠‰πâÊ®°ÂùóÔºå‰ªéËÄåÊúâÊïàÂ≠¶‰π†Ê∏∏ÊàèÁöÑÊ∑∑ÂêàÂä®ÊÄÅ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåStarWMÂú®ËµÑÊ∫êÈ¢ÑÊµãÂáÜÁ°ÆÊÄßÂíåËá™ÊàëÂÆèËßÇÊÉÖÂÜµ‰∏ÄËá¥ÊÄßÊñπÈù¢ÔºåËæÉÈõ∂Ê†∑Êú¨Âü∫Á∫øÊúâËøë60%ÁöÑÊèêÂçá„ÄÇÈÄöËøáÂ∞ÜStarWMÈõÜÊàêÂà∞ÁîüÊàê-Ê®°Êãü-Á≤æÁÇºÁöÑÂÜ≥Á≠ñÂæ™ÁéØ‰∏≠ÔºåStarWM-AgentÂú®‰∏éÊ∏∏ÊàèÂÜÖÁΩÆAIÁöÑÂØπÊäó‰∏≠ÔºåËµ¢ÂæóÁéáÂàÜÂà´ÊèêÈ´ò‰∫Ü30%„ÄÅ15%Âíå30%„ÄÇ', title='StarWMÔºöÊèêÂçá„ÄäÊòüÈôÖ‰∫âÈú∏II„ÄãÂÜ≥Á≠ñÁöÑ‰∏ñÁïåÊ®°Âûã'))
[20.02.2026 09:35] Using data from previous issue: {"categories": ["#multimodal", "#rl", "#agents", "#cv", "#training"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ: –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è —É–º–Ω–æ–≥–æ –ø–æ–º–æ—â–Ω–∏–∫–∞ –≤ –¥–µ—Å–∫—Ç–æ–ø–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –¥–µ—Å–∫—Ç–æ–ø–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è (CUWM), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∏
[20.02.2026 09:35] Using data from previous issue: {"categories": ["#optimization", "#inference", "#open_source", "#architecture", "#training", "#long_context"], "emoji": "‚ö°", "ru": {"title": "–õ–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —É–ª—É—á—à–∏–ª–∏ –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –ø—É—Ç—ë–º —É–ø—Ä–æ—â–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mamba-2 –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –µ—ë –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ
[20.02.2026 09:35] Using data from previous issue: {"categories": ["#alignment", "#reasoning"], "emoji": "üéØ", "ru": {"title": "–≠—Ç–∞–ª–æ–Ω–Ω—ã–µ LLM-–æ—Ü–µ–Ω—â–∏–∫–∏ –∫–∞–∫ –º–æ—Å—Ç–∏–∫ –∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é –≤ –Ω–µ–≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º—è–≥–∫–∏—Ö –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –æ–±–ª–∞—Å—Ç—è—Ö –±–µ–∑ –æ–±—ä–µ–∫—Ç–∏–≤–Ω
[20.02.2026 09:35] Using data from previous issue: {"categories": ["#multimodal", "#transfer_learning", "#training", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ç–∞–∫—Ç–∏–ª—å–Ω—ã—Ö –æ—â—É—â–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Ä–æ–±–æ—Ç–∞ –±–µ–∑ –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "TactAlign ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ç–∞–∫—Ç–∏–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ –∫ —Ä–æ–±–æ—Ç—É —Å —Ä–∞–∑–ª–∏—á–Ω–æ–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–µ–π, –∏—Å–ø–æ–ª—å–∑
[20.02.2026 09:35] Renaming data file.
[20.02.2026 09:35] Renaming previous data. hf_papers.json to ./d/2026-02-20.json
[20.02.2026 09:35] Saving new data file.
[20.02.2026 09:35] Generating page.
[20.02.2026 09:35] Renaming previous page.
[20.02.2026 09:35] Renaming previous data. index.html to ./d/2026-02-20.html
[20.02.2026 09:35] Writing result.
[20.02.2026 09:35] Renaming log file.
[20.02.2026 09:35] Renaming previous data. log.txt to ./logs/2026-02-20_last_log.txt
