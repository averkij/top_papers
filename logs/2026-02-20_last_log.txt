[20.02.2026 16:30] Read previous papers.
[20.02.2026 16:30] Generating top page (month).
[20.02.2026 16:30] Writing top page (month).
[20.02.2026 17:35] Read previous papers.
[20.02.2026 17:35] Get feed.
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13515
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17270
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16855
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15569
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16968
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17004
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14457
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13579
[20.02.2026 17:35] Extract page data from URL. URL: https://huggingface.co/papers/2602.16699
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17365
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17288
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17259
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16928
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16849
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16756
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14857
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17363
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16915
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16835
[20.02.2026 17:35] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16802
[20.02.2026 17:35] Extract page data from URL. URL: https://huggingface.co/papers/2602.15823
[20.02.2026 17:35] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[20.02.2026 17:35] No deleted papers detected.
[20.02.2026 17:35] Downloading and parsing papers (pdf, html). Total: 21.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.13515.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.13515.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.13515.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.17270.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.17270.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.17270.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.16855.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.16855.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.16855.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.15569.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.15569.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.15569.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.16968.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.16968.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.16968.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.17004.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.17004.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.17004.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.14457.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.14457.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.14457.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.13579.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.13579.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.13579.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.16699.
[20.02.2026 17:35] Downloading paper 2602.16699 from https://arxiv.org/pdf/2602.16699v2...
[20.02.2026 17:35] Extracting affiliations from text.
[20.02.2026 17:35] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents Wenxuan Ding 1 Nicholas Tomlin 1 Greg Durrett 1 6 2 0 2 9 1 ] . [ 2 9 9 6 6 1 . 2 0 6 2 : r a "
[20.02.2026 17:35] Response: ```python
[]
```
[20.02.2026 17:35] Extracting affiliations from text.
[20.02.2026 17:35] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents Wenxuan Ding 1 Nicholas Tomlin 1 Greg Durrett 1 6 2 0 2 9 1 ] . [ 2 9 9 6 6 1 . 2 0 6 2 : r aLLMs are increasingly being used for complex problems which are not necessarily resolved in single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent costuncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on programming task, an LLM should test generated code snippet if it is uncertain about the correctness of that code; the cost of writing test is nonzero, but typically lower than the cost of making mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via prior which is passed to the LLM agent. We introduce framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies. 1. Introduction Large language model (LLM) agents are increasingly tasked with operating in environments where information is incomplete. Behaving rationally requires gaining information by exploring the environment. However, exploration comes with cost: every additional step increases API costs, interaction latency, and user burden. 1New York University. Ding <wd2403@nyu.edu>. https://github.com/Wenwen-D/env-explorer. Correspondence to: Wenxuan Code and data available at Preprint. February 20, 2026. 1 Figure 1. Given the same task, coding agent may either verify assumptions via intermediate checks carefully (right) or attempt direct solution as soon as possible (left). The optimal choice depends on uncertainty and specific cost constraints. CalibrateThen-Act (CTA) materializes this information for better decisionmaking. This exploration and its cost come in many forms. In software development and debugging, agents must decide whether to run targeted checks or perform full execution before committing to solution (Zhou et al., 2025). In machine learning experimentation, practitioners balance inexpensive proxy evaluations against costly full training runs under limited compute budgets (Ji & Carin, 2007; Hennig et al., 2024; Xu et al., 2025). In diagnosis and scientific discovery, additional tests or experiments reduce uncertainty but incur monetary, temporal, or safety costs (Karkkainen et al., 2019; Li & Oliva, 2025; Gupta et al., 2025). Online decisionmaking settings such as shopping (Yang et al., 2023; Wang et al., 2025d), recommendation (Herlihy et al., 2024), and tool-augmented question answering (Yao et al., 2023) exhibit the same structure, as agents weigh further information gathering against acting with partial information. Agent policies for this exploration depend in complex way on their prompt, their inputs, and their training data. However, these policies are frequently static. For instance, ChatGPT Deep Research always asks single round of clarifying questions before searching (Deng et al., 2025), and coding agents like SWE-agent (Yang et al., 2024) start by reading through an existing codebase. This contrasts Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents with settings like Figure 1, where we see that model can act more efficiently if it has confidence that it understands the problem setup and appropriately trades off exploration against action costs. In this work, we ask: how can we develop LLMs that explore in Pareto-optimal way under varying cost and uncertainty profiles? Leveraging the strong reasoning capabilities of modern LLMs, we propose framework called Calibrate-Then-Act (CTA), which decouples the calibration of uncertainty from the reasoning of action selection. The key insight is that by presenting priors explicitly to the model, we induce the model to reason about an underlying sequential decision-making problem abstractly and discover the optimal action. We first study synthetic task to illustrate this. On these Pandoras Box problems (Weitzman, 1979), an agent faces multiple hidden boxes with known prior reward distributions and must decide which boxes to open (each incurring cost) and when to stop and commit to the best observed option. Even small thinking model (Qwen3-8B) can compute the optimal action. We then study two more realistic settings: (1) knowledge QA with optional retrieval, where uncertainty is inferred from the models own confidence; and (2) coding tasks where priors regarding environmental structure (e.g., file schemas) are derived from cues learned through past experience. Feeding calibration information via prompt (CTA-PROMPTED) enables dynamic decision-making that is lacking from basic LLMs. Crucially, this behavior does not emerge from basic RL alone: an LLM trained with RL on the coding task fails to internalize the relevant priors from end-to-end training and does not reliably adopt the correct adaptive strategy. In contrast, CTA explicitly exposes these priors to the policy and can be combined with RL to yield further, consistent performance gains. Our contributions are: (1) Framing environment exploration as sequential decision-making problem; (2) Introducing Calibrate-Then-Act, which induces LLMs to reason about the optimality of their actions and achieve better costperformance tradeoffs than baselines. spaces; LLMs produce string actions (code, API calls, etc.) and receive string-valued responses from the environment. The observation function produces string realizations of the underlying environment; e.g., in Figure 1, the results of executing commands return string output in the terminal reflecting the underlying state of the environment. The environment contains problem-critical unobserved features that will determine the agents performance, e.g., details about the formatting of the unobserved file in Figure 1. These can be thought of as subset of the information in . We represent these as random variable taking values . The agent interac"
[20.02.2026 17:35] Mistral response. {"id": "34676c4d02b044f9bd80d64a73e453fa", "created": 1771608953, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1459, "total_tokens": 1469, "completion_tokens": 10, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"New York University\"]\n```"}}]}
[20.02.2026 17:35] Response: ```python
["New York University"]
```
[20.02.2026 17:35] Deleting PDF ./assets/pdf/2602.16699.pdf.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.17365.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.17365.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.17365.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.17288.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.17288.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.17288.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.17259.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.17259.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.17259.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.16928.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.16928.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.16928.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.16849.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.16849.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.16849.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.16756.
[20.02.2026 17:35] Downloading paper 2602.16756 from https://arxiv.org/pdf/2602.16756v1...
[20.02.2026 17:35] Extracting affiliations from text.
[20.02.2026 17:35] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 8 1 ] . [ 1 6 5 7 6 1 . 2 0 6 2 : r NESSIE: THE NECESSARY SAFETY BENCHMARK - Johannes Bertram University of Tubingen & Max-Planck Institute for Intelligent Systems jb@w3a.de Jonas Geiping ELLIS Institute Tubingen & Max-Planck Institute for Intelligent Systems Tubingen AI Center jonas@tue.ellis.eu "
[20.02.2026 17:35] Response: ```python
[
    "University of Tubingen",
    "Max-Planck Institute for Intelligent Systems",
    "ELLIS Institute Tubingen",
    "Tubingen AI Center"
]
```
[20.02.2026 17:35] Deleting PDF ./assets/pdf/2602.16756.pdf.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.14857.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.14857.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.14857.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.17363.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.17363.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.17363.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.16915.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.16915.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.16915.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.16835.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.16835.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.16835.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.16802.
[20.02.2026 17:35] Extra JSON file exists (./assets/json/2602.16802.json), skip PDF parsing.
[20.02.2026 17:35] Paper image links file exists (./assets/img_data/2602.16802.json), skip HTML parsing.
[20.02.2026 17:35] Success.
[20.02.2026 17:35] Downloading and parsing paper https://huggingface.co/papers/2602.15823.
[20.02.2026 17:35] Downloading paper 2602.15823 from https://arxiv.org/pdf/2602.15823v1...
[20.02.2026 17:36] Extracting affiliations from text.
[20.02.2026 17:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 7 1 ] . [ 1 3 2 8 5 1 . 2 0 6 2 : r CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing Zarif Ikram, Arad Firouzkouhi, Stephen Tu, Mahdi Soltanolkotabi, Paria Rashidinejad {zikram,firouzko,stephen.tu,soltanol,paria.rashidinejad}@usc.edu University of Southern California central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the GaussNewton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors. Date: February 17, 2026 Website: https://crispedit.github.io Code: https://github.com/zarifikram/CrispEdit Large language models (LLMs) are rapidly becoming shared backbone for knowledge work, spanning search and question answering (Gao et al., 2023b; Lewis et al., 2020), science (Jumper et al., 2021), software development (Chen, 2021), decision support (Lopez-Lira and Tang, 2023), and education (Kasne"
[20.02.2026 17:36] Response: ```python
["University of Southern California"]
```
[20.02.2026 17:36] Deleting PDF ./assets/pdf/2602.15823.pdf.
[20.02.2026 17:36] Success.
[20.02.2026 17:36] Enriching papers with extra data.
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 0. A trainable sparse attention method called SpargeAttention2 is proposed that achieves high sparsity in diffusion models while maintaining generation quality through hybrid masking rules and distillation-inspired fine-tuning.  					AI-generated summary 				 Many training-free sparse attention methods...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 1. Unified Latents framework learns joint latent representations using diffusion prior regularization and diffusion model decoding, achieving competitive FID scores with reduced training compute.  					AI-generated summary 				 We present Unified Latents (UL), a framework for learning latent representa...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 2. GUI-Owl-1.5 is a multi-platform GUI agent model with varying sizes that achieves superior performance across GUI automation, grounding, tool-calling, and memory tasks through innovations in data pipelines, unified reasoning enhancement, and multi-platform reinforcement learning.  					AI-generated s...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 3. Users prefer adaptive feedback mechanisms in in-car AI assistants, starting with high transparency to build trust and then reducing verbosity as reliability increases, particularly in attention-critical driving scenarios.  					AI-generated summary 				 Agentic AI assistants that autonomously perfor...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 4. Dynamic tokenization improves diffusion transformer efficiency by adjusting patch sizes based on content complexity and denoising timestep, achieving significant speedup without quality loss.  					AI-generated summary 				 Diffusion Transformers (DiTs) have achieved state-of-the-art performance in ...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 5. Arcee Trinity models are sparse Mixture-of-Experts architectures with varying parameter counts and activation patterns, utilizing advanced attention mechanisms and training optimizations.  					AI-generated summary 				 We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Exp...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 6. Frontier AI risk analysis assesses critical dimensions including cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R&D, and self-replication, proposing mitigation strategies for secure deployment of advanced AI systems.  					AI-generated summary 				 To understand and...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 7. TactAlign enables transfer of human tactile demonstrations to robots with different embodiments through cross-embodiment tactile alignment without requiring paired data or manual labels.  					AI-generated summary 				 Human demonstrations collected by wearable devices (e.g., tactile gloves) provide...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 8. Large language models can be improved for complex tasks by explicitly reasoning about cost-uncertainty tradeoffs through a Calibrate-Then-Act framework that enhances decision-making in sequential environments.  					AI-generated summary 				 LLMs are increasingly being used for complex problems whic...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 9. A world model for desktop software that predicts UI state changes through textual description followed by visual synthesis, improving decision quality and execution robustness in computer-using tasks.  					AI-generated summary 				 Agents operating in complex software environments benefit from reas...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 10. Training a 1.36B-parameter scientific language model from raw arXiv LaTeX sources demonstrates the impact of preprocessing decisions, tokenization, and infrastructure constraints on model development under limited computational resources.  					AI-generated summary 				 While frontier large language...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 11. FRAPPE addresses limitations in world modeling for robotics by using parallel progressive expansion to improve representation alignment and reduce error accumulation in predictive models.  					AI-generated summary 				 Enabling VLA models to predict environmental dynamics, known as world modeling, ...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 12. AlphaEvolve, an evolutionary coding agent using large language models, automatically discovers new multiagent learning algorithms for imperfect-information games by evolving regret minimization and population-based training variants.  					AI-generated summary 				 Much of the advancement of Multi-A...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 13. Two-layer neural networks solve modular addition by learning Fourier features through phase symmetry and frequency diversification, enabling robust computation via majority voting despite individual neuron noise.  					AI-generated summary 				 We present a comprehensive analysis of how two-layer ne...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 14. NESSiE benchmark reveals safety vulnerabilities in large language models through simple security tests, demonstrating that even state-of-the-art models fail basic safety requirements without adversarial attacks.  					AI-generated summary 				 We introduce NESSiE, the NEceSsary SafEty benchmark for ...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 15. StarWM, a world model for StarCraft II, predicts future observations under partial observability using a structured textual representation and achieves significant performance improvements in win rate and decision-making stability.  					AI-generated summary 				 Large Language Models (LLMs) have re...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 16. Researchers enhance linear attention by simplifying Mamba-2 and improving its architectural components to achieve near-softmax accuracy while maintaining memory efficiency for long sequences.  					AI-generated summary 				 Linear attention transformers have become a strong alternative to softmax at...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 17. StereoAdapter-2 improves underwater stereo depth estimation by replacing ConvGRU with a selective state space ConvSS2D operator for efficient long-range propagation and introduces a large-scale synthetic underwater dataset.  					AI-generated summary 				 Stereo depth estimation is fundamental to un...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 18. NeST is a lightweight safety alignment framework that selectively adapts safety-relevant neurons while keeping the rest of the model frozen, achieving significant reductions in unsafe generations with minimal trainable parameters.  					AI-generated summary 				 Safety alignment is essential for the...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 19. Reference-guided LLM-evaluators enhance LLM alignment by serving as soft verifiers, improving judge accuracy and enabling effective post-training in non-verifiable domains through self-improvement techniques.  					AI-generated summary 				 While Reinforcement Learning with Verifiable Rewards (RLVR)...
[20.02.2026 17:36] ********************************************************************************
[20.02.2026 17:36] Abstract 20. CrispEdit is a second-order editing algorithm for large language models that preserves capabilities by constraining updates to low-curvature subspaces of the capability-loss landscape using Bregman divergence and efficient Kronecker-factored approximations.  					AI-generated summary 				 A central ...
[20.02.2026 17:36] Read previous papers.
[20.02.2026 17:36] Generating reviews via LLM API.
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#optimization", "#inference", "#video", "#architecture", "#diffusion", "#training"], "emoji": "‚ö°", "ru": {"title": "–û–±—É—á–∞–µ–º–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –º–æ–¥–µ–ª—è–º–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è SpargeAttention2 ‚Äî –æ–±—É—á–∞–µ–º—ã–π –º–µ—Ç–æ–¥ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è 
[20.02.2026 17:36] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–ï–¥–∏–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Unified Latents ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–µ–≥—É–ª—è—Ä–∏–∑—É—é—Ç—Å—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º –ø—Ä–∏–æ—Ä–æ–º –∏ –¥–µ–∫–æ–¥–∏—Ä—É—é—Ç—Å—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é. 
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#optimization", "#rl", "#data", "#open_source", "#agents", "#benchmark", "#training"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π GUI-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º", "desc": "GUI-Owl-1.5 ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–ø–ª–∞—Ç—Ñ–æ—Ä–º–Ω–∞—è –º–æ–¥–µ–ª—å-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º
[20.02.2026 17:36] Using data from previous issue: {"categories": [], "emoji": "üöó", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –≤ –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞—Ö: –æ—Ç –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –∫–∞–∫ –∞–≥–µ–Ω—Ç–Ω—ã–µ LLM-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –∫–æ–º–º—É–Ω–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –∑–∞–¥–∞—á –≤ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#optimization", "#inference", "#video", "#architecture", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä
[20.02.2026 17:36] Using data from previous issue: {"categories": [], "emoji": "‚öôÔ∏è", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Mixture-of-Experts —Å –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ç—Ä–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-of-Experts –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Arcee Trinity —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: Trinity L
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#agi", "#alignment", "#security"], "emoji": "‚ö†Ô∏è", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏ –≥—Ä–∞–Ω–∏—Ü AI: –æ—Ç –≤—ã—è–≤–ª–µ–Ω–∏—è —É–≥—Ä–æ–∑ –∫ –Ω–∞–¥–µ–∂–Ω—ã–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º –∑–∞—â–∏—Ç—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤ –ø–µ—Ä–µ–¥–æ–≤—ã—Ö AI —Å–∏—Å—Ç–µ–º, –≤–∫–ª—é—á–∞—é—â–∞—è –∞–Ω–∞–ª–∏–∑ –ø—è—Ç–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π: –∫–∏–±–µ—Ä–∞—Ç–∞–∫, –º
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#multimodal", "#transfer_learning", "#training", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ç–∞–∫—Ç–∏–ª—å–Ω—ã—Ö –æ—â—É—â–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Ä–æ–±–æ—Ç–∞ –±–µ–∑ –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "TactAlign ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ç–∞–∫—Ç–∏–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ –∫ —Ä–æ–±–æ—Ç—É —Å —Ä–∞–∑–ª–∏—á–Ω–æ–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–µ–π, –∏—Å–ø–æ–ª—å–∑
[20.02.2026 17:36] Querying the API.
[20.02.2026 17:36] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Large language models can be improved for complex tasks by explicitly reasoning about cost-uncertainty tradeoffs through a Calibrate-Then-Act framework that enhances decision-making in sequential environments.  					AI-generated summary 				 LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.
[20.02.2026 17:36] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Calibrate-Then-Act, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –ª—É—á—à–µ —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, —Ç—Ä–µ–±—É—é—â–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º. LLM –¥–æ–ª–∂–Ω—ã —è–≤–Ω–æ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –æ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–µ –º–µ–∂–¥—É –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å—é, –Ω–∞–ø—Ä–∏–º–µ—Ä —Ä–µ—à–∞—è, —Å—Ç–æ–∏—Ç –ª–∏ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –∫–æ–¥ –∏–ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è —Å —Ç–µ–∫—É—â–∏–º –æ—Ç–≤–µ—Ç–æ–º. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º–∞–ª–∏–∑—É—é—Ç –∑–∞–¥–∞—á–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞–∫ –ø—Ä–æ–±–ª–µ–º—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –∏ –ø–µ—Ä–µ–¥–∞—é—Ç –º–æ–¥–µ–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —è–≤–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ trade-off'–æ–≤ –ø–æ–º–æ–≥–∞–µ—Ç –∞–≥–µ–Ω—Ç–∞–º –Ω–∞—Ö–æ–¥–∏—Ç—å –±–æ–ª–µ–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è.",
  "emoji": "‚öñÔ∏è",
  "title": "–Ø–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –æ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–µ –∑–∞—Ç—Ä–∞—Ç –∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –≤ LLM"
}
```
[20.02.2026 17:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models can be improved for complex tasks by explicitly reasoning about cost-uncertainty tradeoffs through a Calibrate-Then-Act framework that enhances decision-making in sequential environments.  					AI-generated summary 				 LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies."

[20.02.2026 17:36] Response: ```python
["AGENTS", "RL", "PLP", "TRAINING"]
```
[20.02.2026 17:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Large language models can be improved for complex tasks by explicitly reasoning about cost-uncertainty tradeoffs through a Calibrate-Then-Act framework that enhances decision-making in sequential environments.  					AI-generated summary 				 LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies."

[20.02.2026 17:36] Response: ```python
["REASONING", "OPTIMIZATION"]
```
[20.02.2026 17:36] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper presents a framework called Calibrate-Then-Act (CTA) to enhance large language models (LLMs) in making decisions in complex tasks. The framework helps LLMs reason about the tradeoffs between the costs of exploring options and the uncertainties involved in those options. By formalizing tasks like information retrieval and coding as sequential decision-making problems, the LLM can better evaluate when to explore further or commit to a solution. The results demonstrate that incorporating cost-uncertainty reasoning leads to more optimal decision-making strategies in various tasks.","title":"Enhancing LLM Decision-Making with Cost-Uncertainty Tradeoffs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a framework called Calibrate-Then-Act (CTA) to enhance large language models (LLMs) in making decisions in complex tasks. The framework helps LLMs reason about the tradeoffs between the costs of exploring options and the uncertainties involved in those options. By formalizing tasks like information retrieval and coding as sequential decision-making problems, the LLM can better evaluate when to explore further or commit to a solution. The results demonstrate that incorporating cost-uncertainty reasoning leads to more optimal decision-making strategies in various tasks.', title='Enhancing LLM Decision-Making with Cost-Uncertainty Tradeoffs'))
[20.02.2026 17:36] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‚ÄúÊ†°ÂáÜÂêéË°åÂä®‚ÄùÔºàCalibrate-Then-Act, CTAÔºâÁöÑÊ°ÜÊû∂Ôºå‰ª•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑÂÜ≥Á≠ñËÉΩÂäõ„ÄÇÈÄöËøáÊòéÁ°ÆËÄÉËôëÊàêÊú¨‰∏é‰∏çÁ°ÆÂÆöÊÄß‰πãÈó¥ÁöÑÊùÉË°°ÔºåLLMËÉΩÂ§üÂú®Êé¢Á¥¢ÂíåÂÅöÂá∫ÂÜ≥Á≠ñÊó∂ËøõË°åÊõ¥‰ºòÁöÑÁéØÂ¢É‰∫§‰∫í„ÄÇÊàë‰ª¨Â∞Ü‰ø°ÊÅØÊ£ÄÁ¥¢ÂíåÁºñÁ®ãÁ≠â‰ªªÂä°ÂΩ¢ÂºèÂåñ‰∏∫‰∏çÁ°ÆÂÆöÊÄß‰∏ãÁöÑÂ∫èÂàóÂÜ≥Á≠ñÈóÆÈ¢òÔºåÂπ∂ÈÄöËøáÂÖàÈ™åÁü•ËØÜÂ∏ÆÂä©LLMËøõË°åÊé®ÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®CTAÊ°ÜÊû∂ÂèØ‰ª•‰Ωø‰ª£ÁêÜÂèëÁé∞Êõ¥‰ºòÁöÑÂÜ≥Á≠ñÁ≠ñÁï•ÔºåÊèêÂçáÂÖ∂Âú®‰ø°ÊÅØËé∑ÂèñÂíåÁºñÁ®ã‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ","title":"‰ºòÂåñÂÜ≥Á≠ñÔºåÊèêÂçáLLMËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‚ÄúÊ†°ÂáÜÂêéË°åÂä®‚ÄùÔºàCalibrate-Then-Act, CTAÔºâÁöÑÊ°ÜÊû∂Ôºå‰ª•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑÂÜ≥Á≠ñËÉΩÂäõ„ÄÇÈÄöËøáÊòéÁ°ÆËÄÉËôëÊàêÊú¨‰∏é‰∏çÁ°ÆÂÆöÊÄß‰πãÈó¥ÁöÑÊùÉË°°ÔºåLLMËÉΩÂ§üÂú®Êé¢Á¥¢ÂíåÂÅöÂá∫ÂÜ≥Á≠ñÊó∂ËøõË°åÊõ¥‰ºòÁöÑÁéØÂ¢É‰∫§‰∫í„ÄÇÊàë‰ª¨Â∞Ü‰ø°ÊÅØÊ£ÄÁ¥¢ÂíåÁºñÁ®ãÁ≠â‰ªªÂä°ÂΩ¢ÂºèÂåñ‰∏∫‰∏çÁ°ÆÂÆöÊÄß‰∏ãÁöÑÂ∫èÂàóÂÜ≥Á≠ñÈóÆÈ¢òÔºåÂπ∂ÈÄöËøáÂÖàÈ™åÁü•ËØÜÂ∏ÆÂä©LLMËøõË°åÊé®ÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®CTAÊ°ÜÊû∂ÂèØ‰ª•‰Ωø‰ª£ÁêÜÂèëÁé∞Êõ¥‰ºòÁöÑÂÜ≥Á≠ñÁ≠ñÁï•ÔºåÊèêÂçáÂÖ∂Âú®‰ø°ÊÅØËé∑ÂèñÂíåÁºñÁ®ã‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇ', title='‰ºòÂåñÂÜ≥Á≠ñÔºåÊèêÂçáLLMËÉΩÂäõ'))
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#multimodal", "#rl", "#agents", "#cv", "#training"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ: –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è —É–º–Ω–æ–≥–æ –ø–æ–º–æ—â–Ω–∏–∫–∞ –≤ –¥–µ—Å–∫—Ç–æ–ø–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –¥–µ—Å–∫—Ç–æ–ø–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è (CUWM), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∏
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#training", "#open_source", "#data", "#plp", "#small_models", "#science", "#optimization"], "emoji": "üî¨", "ru": {"title": "–ò–Ω–∂–µ–Ω–µ—Ä–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞—É—á–Ω–æ–π LLM –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö: –æ—Ç —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#training", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –±—É–¥—É—â–µ–≥–æ –¥–ª—è –æ—Å–æ–∑–Ω–∞—é—â–µ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "FRAPPE ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–∏—Ä–∞ –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é —ç–∫—Å–ø–∞–Ω
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#games", "#rl", "#agents", "#architecture", "#training"], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω AlphaEvolve ‚Äî —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#architecture", "#math", "#training"], "emoji": "üî¢", "ru": {"title": "–ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞ —á–µ—Ä–µ–∑ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –§—É—Ä—å–µ", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç, –∫–∞–∫ –¥–≤—É—Ö—Å–ª–æ–π–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Ä–µ—à–∞—é—Ç –∑–∞–¥–∞—á—É –º–æ–¥—É–ª—å–Ω–æ–≥–æ —Å–ª–æ–∂–µ–Ω–∏—è, –æ–±—É—á–∞—è—Å—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#security", "#alignment", "#open_source", "#ethics", "#dataset", "#benchmark"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏: –∫–∞–∫ –ø—Ä–æ—Å—Ç—ã–µ —Ç–µ—Å—Ç—ã –≤—ã—è–≤–ª—è—é—Ç –ø—Ä–æ–≤–∞–ª—ã LLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç NESSiE ‚Äî –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#games", "#reasoning", "#synthetic"], "emoji": "üéÆ", "ru": {"title": "–ú–æ–¥–µ–ª—å –º–∏—Ä–∞ –Ω–∞ —è–∑—ã–∫–µ –¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π –≤ StarCraft II", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω StarWM ‚Äî –ø–µ—Ä–≤–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –∏–≥—Ä—ã StarCraft II, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –±—É–¥—É—â–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –≤ —É—Å–ª–æ–≤–∏—è—Ö —á–∞—Å—Ç–∏—á–Ω–æ–π –≤–∏–¥–∏–º–æ
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#optimization", "#inference", "#open_source", "#architecture", "#training", "#long_context"], "emoji": "‚ö°", "ru": {"title": "–õ–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —É–ª—É—á—à–∏–ª–∏ –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –ø—É—Ç—ë–º —É–ø—Ä–æ—â–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mamba-2 –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –µ—ë –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#open_source", "#cv", "#synthetic", "#architecture"], "emoji": "üåä", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å—Ç–µ—Ä–µ–æ–≥–ª—É–±–∏–Ω–∞ –ø–æ–¥ –≤–æ–¥–æ–π —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–∏–≤–Ω—ã–µ state space –º–æ–¥–µ–ª–∏", "desc": "StereoAdapter-2 —É–ª—É—á—à–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –≥–ª—É–±–∏–Ω—ã –≤ –ø–æ–¥–≤–æ–¥–Ω—ã—Ö —Å—Ç–µ—Ä–µ–æ–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, –∑–∞–º–µ–Ω–∏–≤ ConvGRU –Ω
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#benchmark", "#alignment", "#security", "#training"], "emoji": "üõ°Ô∏è", "ru": {"title": "–¶–µ–ª–µ–≤–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: –º–∏–Ω–∏–º—É–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –º–∞–∫—Å–∏–º—É–º –∑–∞—â–∏—Ç—ã", "desc": "NeST ‚Äî —ç—Ç–æ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Å–µ–ª–µ–∫
[20.02.2026 17:36] Using data from previous issue: {"categories": ["#alignment", "#reasoning"], "emoji": "üéØ", "ru": {"title": "–≠—Ç–∞–ª–æ–Ω–Ω—ã–µ LLM-–æ—Ü–µ–Ω—â–∏–∫–∏ –∫–∞–∫ –º–æ—Å—Ç–∏–∫ –∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é –≤ –Ω–µ–≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º—è–≥–∫–∏—Ö –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –æ–±–ª–∞—Å—Ç—è—Ö –±–µ–∑ –æ–±—ä–µ–∫—Ç–∏–≤–Ω
[20.02.2026 17:36] Querying the API.
[20.02.2026 17:36] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

CrispEdit is a second-order editing algorithm for large language models that preserves capabilities by constraining updates to low-curvature subspaces of the capability-loss landscape using Bregman divergence and efficient Kronecker-factored approximations.  					AI-generated summary 				 A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.
[20.02.2026 17:36] Response: ```json
{
  "desc": "CrispEdit ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –≤–Ω–µ—Å–µ–Ω–∏–∏ —Ü–µ–ª–µ–≤—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∫ –∑–∞–¥–∞—á—É —É—Å–ª–æ–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –ë—Ä–µ–≥–ª–º–∞–Ω–∞ –¥–ª—è –ø—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –Ω–∞ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –Ω–∏–∑–∫–æ–π –∫—Ä–∏–≤–∏–∑–Ω—ã –ª–∞–Ω–¥—à–∞—Ñ—Ç–∞ –ø–æ—Ç–µ—Ä—å. –î–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —Ä–∞–∑–º–µ—Ä LLM –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è Kronecker-—Ñ–∞–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è –∫—Ä–∏–≤–∏–∑–Ω—ã –∏ –º–∞—Ç—Ä–∏—á–Ω–æ-—Å–≤–æ–±–æ–¥–Ω—ã–π –ø—Ä–æ–µ–∫—Ç–æ—Ä, –∏–∑–±–µ–≥–∞—é—â–∏–π –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Å—Å–∏–≤–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü –ø—Ä–æ–µ–∫—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ CrispEdit –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–≥–æ —É—Å–ø–µ—Ö–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –æ–±—â–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–µ–Ω–µ–µ 1% –≤ —Å—Ä–µ–¥–Ω–µ–º.",
  "emoji": "‚úèÔ∏è",
  "title": "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ø—Ä–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–∏–∑–∫–æ–∫—Ä–∏–≤–∏–∑–Ω—ã–µ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞"
}
```
[20.02.2026 17:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CrispEdit is a second-order editing algorithm for large language models that preserves capabilities by constraining updates to low-curvature subspaces of the capability-loss landscape using Bregman divergence and efficient Kronecker-factored approximations.  					AI-generated summary 				 A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors."

[20.02.2026 17:36] Response: ```python
["TRAINING"]
```
[20.02.2026 17:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CrispEdit is a second-order editing algorithm for large language models that preserves capabilities by constraining updates to low-curvature subspaces of the capability-loss landscape using Bregman divergence and efficient Kronecker-factored approximations.  					AI-generated summary 				 A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors."

[20.02.2026 17:36] Response: ```python
['OPTIMIZATION', 'ALIGNMENT']
```

**Reasoning:**

1. **OPTIMIZATION**: The paper presents CrispEdit, a second-order editing algorithm that uses constrained optimization, Bregman divergence, Kronecker-factored approximations, and efficient optimization techniques (K-FAC, matrix-free projector) to improve training/editing procedures for LLMs.

2. **ALIGNMENT**: The paper addresses capability preservation and preventing "degenerate behaviors reminiscent of proxy/reward hacking" when editing LLMs. This is fundamentally about aligning model behavior - ensuring that edits to targeted behavior don't corrupt the model's general capabilities and intended behavior, which is a core alignment concern.
[20.02.2026 17:36] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "ALIGNMENT"]


**Reasoning:**

1. **OPTIMIZATION**: The paper presents CrispEdit, a second-order editing algorithm that uses constrained optimization, Bregman divergence, Kronecker-factored approximations, and efficient optimization techniques (K-FAC, matrix-free projector) to improve training/editing procedures for LLMs.

2. **ALIGNMENT**: The paper addresses capability preservation and preventing "degenerate behaviors reminiscent of proxy/reward hacking" when editing LLMs. This is fundamentally about aligning model behavior - ensuring that edits to targeted behavior don"t corrupt the model"s general capabilities and intended behavior, which is a core alignment concern.
[20.02.2026 17:36] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"CrispEdit is a novel second-order editing algorithm designed for large language models (LLMs) that focuses on preserving the model\'s capabilities while making targeted edits. It addresses the challenge of capability preservation by constraining updates to low-curvature areas of the capability-loss landscape, which helps prevent unwanted changes in the model\'s general behavior. The algorithm employs Bregman divergence to express capability constraints and utilizes efficient Kronecker-factored approximations to manage computational demands at scale. As a result, CrispEdit demonstrates high editing success rates with minimal capability degradation, outperforming previous editing methods.","title":"CrispEdit: Smart Edits Without Losing Skills"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="CrispEdit is a novel second-order editing algorithm designed for large language models (LLMs) that focuses on preserving the model's capabilities while making targeted edits. It addresses the challenge of capability preservation by constraining updates to low-curvature areas of the capability-loss landscape, which helps prevent unwanted changes in the model's general behavior. The algorithm employs Bregman divergence to express capability constraints and utilizes efficient Kronecker-factored approximations to manage computational demands at scale. As a result, CrispEdit demonstrates high editing success rates with minimal capability degradation, outperforming previous editing methods.", title='CrispEdit: Smart Edits Without Losing Skills'))
[20.02.2026 17:36] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"CrispEditÊòØ‰∏ÄÁßçÈíàÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∫åÈò∂ÁºñËæëÁÆóÊ≥ïÔºåÊó®Âú®Âú®ÁºñËæëËøáÁ®ã‰∏≠‰øùÊåÅÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇÂÆÉÈÄöËøáÂ∞ÜÊõ¥Êñ∞ÈôêÂà∂Âú®ËÉΩÂäõÊçüÂ§±ÊôØËßÇÁöÑ‰ΩéÊõ≤ÁéáÂ≠êÁ©∫Èó¥‰∏≠Ôºå‰ΩøÁî®BregmanÊï£Â∫¶ÂíåÈ´òÊïàÁöÑKroneckerÂàÜËß£Ëøë‰ººÊù•ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†á„ÄÇCrispEditÂ∞ÜÁºñËæëËßÜ‰∏∫Á∫¶Êùü‰ºòÂåñÈóÆÈ¢òÔºåÂπ∂ÈÄöËøáÊäïÂΩ±ÁºñËæëÊõ¥Êñ∞Êù•Âº∫Âà∂ÊâßË°åËøô‰∏ÄÁ∫¶Êùü„ÄÇÂÆûÈ™åË°®ÊòéÔºåCrispEditÂú®Ê†áÂáÜÊ®°ÂûãÁºñËæëÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÈ´òÊàêÂäüÁéáÔºåÂêåÊó∂Âú®ÂêÑÊï∞ÊçÆÈõÜ‰∏äÁöÑËÉΩÂäõÈÄÄÂåñÂπ≥Âùá‰Ωé‰∫é1%„ÄÇ","title":"CrispEditÔºö‰øùÊåÅËÉΩÂäõÁöÑÊô∫ËÉΩÁºñËæëÁÆóÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CrispEditÊòØ‰∏ÄÁßçÈíàÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∫åÈò∂ÁºñËæëÁÆóÊ≥ïÔºåÊó®Âú®Âú®ÁºñËæëËøáÁ®ã‰∏≠‰øùÊåÅÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇÂÆÉÈÄöËøáÂ∞ÜÊõ¥Êñ∞ÈôêÂà∂Âú®ËÉΩÂäõÊçüÂ§±ÊôØËßÇÁöÑ‰ΩéÊõ≤ÁéáÂ≠êÁ©∫Èó¥‰∏≠Ôºå‰ΩøÁî®BregmanÊï£Â∫¶ÂíåÈ´òÊïàÁöÑKroneckerÂàÜËß£Ëøë‰ººÊù•ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†á„ÄÇCrispEditÂ∞ÜÁºñËæëËßÜ‰∏∫Á∫¶Êùü‰ºòÂåñÈóÆÈ¢òÔºåÂπ∂ÈÄöËøáÊäïÂΩ±ÁºñËæëÊõ¥Êñ∞Êù•Âº∫Âà∂ÊâßË°åËøô‰∏ÄÁ∫¶Êùü„ÄÇÂÆûÈ™åË°®ÊòéÔºåCrispEditÂú®Ê†áÂáÜÊ®°ÂûãÁºñËæëÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó‰∫ÜÈ´òÊàêÂäüÁéáÔºåÂêåÊó∂Âú®ÂêÑÊï∞ÊçÆÈõÜ‰∏äÁöÑËÉΩÂäõÈÄÄÂåñÂπ≥Âùá‰Ωé‰∫é1%„ÄÇ', title='CrispEditÔºö‰øùÊåÅËÉΩÂäõÁöÑÊô∫ËÉΩÁºñËæëÁÆóÊ≥ï'))
[20.02.2026 17:36] Renaming data file.
[20.02.2026 17:36] Renaming previous data. hf_papers.json to ./d/2026-02-20.json
[20.02.2026 17:36] Saving new data file.
[20.02.2026 17:36] Generating page.
[20.02.2026 17:36] Renaming previous page.
[20.02.2026 17:36] Renaming previous data. index.html to ./d/2026-02-20.html
[20.02.2026 17:36] Writing result.
[20.02.2026 17:36] Renaming log file.
[20.02.2026 17:36] Renaming previous data. log.txt to ./logs/2026-02-20_last_log.txt
