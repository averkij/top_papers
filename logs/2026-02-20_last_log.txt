[19.02.2026 23:23] Read previous papers.
[19.02.2026 23:23] Generating top page (month).
[19.02.2026 23:23] Writing top page (month).
[20.02.2026 01:18] Read previous papers.
[20.02.2026 01:18] Get feed.
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12675
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14979
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16705
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16317
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14080
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16008
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16666
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16301
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15922
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16704
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15989
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16682
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16493
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16173
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.07345
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15927
[20.02.2026 01:18] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08392
[20.02.2026 01:18] Extract page data from URL. URL: https://huggingface.co/papers/2602.14602
[20.02.2026 01:18] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[20.02.2026 01:18] No deleted papers detected.
[20.02.2026 01:18] Downloading and parsing papers (pdf, html). Total: 18.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.12675.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.12675.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.12675.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.14979.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.14979.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.14979.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.16705.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.16705.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.16705.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.16317.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.16317.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.16317.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.14080.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.14080.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.14080.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.16008.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.16008.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.16008.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.16666.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.16666.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.16666.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.16301.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.16301.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.16301.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.15922.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.15922.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.15922.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.16704.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.16704.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.16704.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.15989.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.15989.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.15989.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.16682.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.16682.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.16682.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.16493.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.16493.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.16493.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.16173.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.16173.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.16173.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.07345.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.07345.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.07345.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.15927.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.15927.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.15927.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.08392.
[20.02.2026 01:18] Extra JSON file exists (./assets/json/2602.08392.json), skip PDF parsing.
[20.02.2026 01:18] Paper image links file exists (./assets/img_data/2602.08392.json), skip HTML parsing.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Downloading and parsing paper https://huggingface.co/papers/2602.14602.
[20.02.2026 01:18] Downloading paper 2602.14602 from https://arxiv.org/pdf/2602.14602v1...
[20.02.2026 01:18] Extracting affiliations from text.
[20.02.2026 01:18] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Opbench: Graph Benchmark to Combat the Opioid Crisis Tianyi Ma1, Yiyang Li1, Yiyue Qian3, Zheyuan Zhang1, Zehong Wang1, Chuxu Zhang2, and Yanfang Ye1 1University of Notre Dame, 2University of Connecticut, 3Amazon {tma2, yli62}@nd.edu, yyqian5@gmail.com, {zzhang42, zwang43}@nd.edu, chuxu.zhang@uconn.edu, yye7@nd.edu 6 2 0 2 6 1 ] . [ 1 2 0 6 4 1 . 2 0 6 2 : r The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as promising paradigm for modeling complex drug-related phenomena. However, significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. This gap persists due to three key challenges: (i) the multidimensional nature of the crisis spanning the supply and demand sides of the drug ecosystem, (ii) the structural complexity of drugrelated data involving heterogeneous and higher-order interactions, and (iii) the scarcity of publicly available annotated datasets due to privacy constraints and labor-intensive expert labeling. To bridge this gap, we introduce Opbench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, Opbench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish unified evaluation framework with standardized protocols, predefined data splits, and reproduci"
[20.02.2026 01:18] Response: ```python
[
    "University of Notre Dame",
    "University of Connecticut",
    "Amazon"
]
```
[20.02.2026 01:18] Deleting PDF ./assets/pdf/2602.14602.pdf.
[20.02.2026 01:18] Success.
[20.02.2026 01:18] Enriching papers with extra data.
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 0. SLA2 improves sparse-linear attention in diffusion models by introducing a learnable router, direct attention formulation, and quantization-aware fine-tuning for enhanced efficiency and quality.  					AI-generated summary 				 Sparse-Linear Attention (SLA) combines sparse and linear attention to acc...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 1. RynnBrain is an open-source spatiotemporal foundation model for embodied intelligence that unifies perception, reasoning, and planning capabilities across multiple scales and task-specific variants.  					AI-generated summary 				 Despite rapid progress in multimodal foundation models, embodied inte...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 2. HERO enables humanoid robots to perform object manipulation in diverse real-world environments by combining accurate end-effector control with open-vocabulary vision models for generalizable scene understanding.  					AI-generated summary 				 Visual loco-manipulation of arbitrary objects in the wil...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 3. CADEvolve presents an evolution-based approach using VLM-guided edits to generate complex CAD programs from simple primitives, creating a large dataset for improved Image2CAD performance.  					AI-generated summary 				 Computer-Aided Design (CAD) delivers rapid, editable modeling for engineering an...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 4. LLMs demonstrate near-complete factual encoding but struggle with retrieval accessibility, where errors stem from access limitations rather than knowledge gaps, with reasoning improving recall of encoded information.  					AI-generated summary 				 Standard factuality evaluations of LLMs treat all e...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 5. MAEB is a large-scale audio benchmark evaluating 50+ models across 30 tasks in speech, music, and environmental sounds, revealing diverse model strengths and establishing correlations with audio LLM performance.  					AI-generated summary 				 We introduce the Massive Audio Embedding Benchmark (MAEB...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 6. Traditional benchmark evaluations of AI agents fail to capture critical reliability issues, prompting the development of comprehensive metrics that assess consistency, robustness, predictability, and safety across multiple dimensions.  					AI-generated summary 				 AI agents are increasingly deploy...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 7. Sequence models enable cooperative behavior emergence in multi-agent reinforcement learning through in-context learning without hardcoded assumptions or timescale separation.  					AI-generated summary 				 Achieving cooperation among self-interested agents remains a fundamental challenge in multi-a...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 8. DreamZero is a World Action Model that leverages video diffusion to enable better generalization of physical motions across novel environments and embodiments compared to vision-language-action models.  					AI-generated summary 				 State-of-the-art Vision-Language-Action (VLA) models excel at sema...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 9. REFINE is a reinforcement learning framework that improves fast weight models for long-context modeling by training under next-sequence prediction instead of next-token prediction, enhancing their ability to capture long-range dependencies.  					AI-generated summary 				 Fast weight architectures o...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 10. A promptable 3D human mesh recovery model using a novel parametric representation and encoder-decoder architecture achieves state-of-the-art performance with strong generalization across diverse conditions.  					AI-generated summary 				 We introduce SAM 3D Body (3DB), a promptable model for single...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 11. SAW-Bench presents a new benchmark for evaluating egocentric situated awareness in multimodal foundation models through real-world video datasets with human-annotated question-answer pairs, focusing on observer-centric spatial reasoning tasks.  					AI-generated summary 				 A core aspect of human p...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 12. Multimodal Memory Agent (MMA) improves long-horizon agent performance by dynamically scoring memory reliability and handling visual biases in retrieval-augmented systems.  					AI-generated summary 				 Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval oft...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 13. PAHF framework enables continual personalization of AI agents through explicit user memory and dual feedback channels, allowing rapid adaptation to changing user preferences.  					AI-generated summary 				 Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving prefer...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 14. Adaptive Matching Distillation introduces a self-correcting mechanism to improve generative model training by detecting and escaping unstable regions in the optimization landscape.  					AI-generated summary 				 Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its s...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 15. Visual Memory Injection attack enables covert manipulation of generative vision-language models through manipulated images that trigger targeted responses only under specific prompts during multi-turn conversations.  					AI-generated summary 				 Generative large vision-language models (LVLMs) have...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 16. BiManiBench evaluates multimodal large language models on bimanual robotic tasks, revealing limitations in spatial grounding and control despite strong high-level reasoning capabilities.  					AI-generated summary 				 Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI,...
[20.02.2026 01:18] ********************************************************************************
[20.02.2026 01:18] Abstract 17. The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena...
[20.02.2026 01:18] Read previous papers.
[20.02.2026 01:18] Generating reviews via LLM API.
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#video", "#optimization", "#diffusion", "#inference", "#training", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SLA2, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ-–ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤ –¥–∏—Ñ
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#architecture", "#robotics", "#open_source", "#transfer_learning", "#multimodal", "#reasoning", "#benchmark", "#training", "#cv"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–æ–±–æ—Ç–∞", "desc": "RynnBrain ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –ø—Ä–æ—Å—Ç
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#robotics", "#training"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ–≥–¥–∞ —Ä–æ–±–æ—Ç —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ª–æ–≤–∫–∏–º: —Ç–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ + –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã", "desc": "HERO ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞–Ω–∏—é –æ–±—ä–µ–∫—Ç–∞–º–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ –ø—É—Ç—ë–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ —É–ø—Ä
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#synthetic", "#plp", "#benchmark", "#multimodal", "#science"], "emoji": "üèóÔ∏è", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–æ–∂–Ω—ã—Ö CAD-–ø—Ä–æ–≥—Ä–∞–º–º —á–µ—Ä–µ–∑ –Ω–∞–ø—Ä–∞–≤–ª—è–µ–º–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ VLM", "desc": "CADEvolve –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#hallucinations", "#dataset", "#reasoning", "#benchmark"], "emoji": "üîë", "ru": {"title": "–ó–Ω–∞–Ω–∏—è –µ—Å—Ç—å, –Ω–æ –∫–ª—é—á–∏ –ø–æ—Ç–µ—Ä—è–Ω—ã: –∫–∞–∫ LLM –∫–æ–¥–∏—Ä—É—é—Ç —Ñ–∞–∫—Ç—ã, –Ω–æ –Ω–µ –º–æ–≥—É—Ç –∏—Ö –≤—Å–ø–æ–º–Ω–∏—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –∫–æ–¥–∏—Ä—É—é—Ç –ø–æ—á—Ç–∏ –≤—Å–µ —Ñ–∞–∫—Ç—ã (9
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#dataset", "#audio", "#multimodal", "#open_source", "#survey", "#multilingual", "#low_resource", "#benchmark"], "emoji": "üéµ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –∞—É–¥–∏–æ–º–æ–¥–µ–ª–µ–π: –Ω–µ—Ç –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–π –Ω–∞ –≤—Å–µ —Å–ª—É—á–∞–∏ –∂–∏–∑–Ω–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Massive Audio Embedding Benchmark (MAEB) ‚Äî –∫—Ä—É–ø–Ω
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "‚öôÔ∏è", "ru": {"title": "–ù–∞–¥—ë–∂–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ AI –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –∫—Ä–∏—Ç–∏–∫—É–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ AI –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –º–µ—Ç—Ä–∏–∫—É —É—Å–ø–µ—Ö–∞ –∏ –Ω–µ –≤—ã—è–≤–ª—è—é—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏. –ê
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#rl", "#agents", "#games", "#reasoning", "#architecture"], "emoji": "ü§ù", "ru": {"title": "–ö–æ–æ–ø–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã) –ø–æ–∑–≤–æ–ª—è—é—Ç –∞–≥–µ–Ω—Ç–∞–º –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —Å
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#video", "#optimization", "#diffusion", "#multimodal", "#training", "#robotics", "#transfer_learning"], "emoji": "ü§ñ", "ru": {"title": "–î–∏–Ω–∞–º–∏–∫–∞ –≤–º–µ—Å—Ç–æ —Å–µ–º–∞–Ω—Ç–∏–∫–∏: –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º", "desc": "DreamZero ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π (World Action Model)
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#rl", "#architecture", "#long_context", "#optimization", "#reasoning", "#training"], "emoji": "üîÑ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤–º–µ—Å—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞", "desc": "REFINE ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#3d", "#benchmark", "#open_source", "#data", "#architecture", "#dataset"], "emoji": "üßç", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç SAM 3D Body (3DB) ‚Äî –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#video", "#survey", "#benchmark", "#multimodal", "#reasoning", "#dataset"], "emoji": "üëì", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∏—Ä–∞ —á–µ—Ä–µ–∑ –≥–ª–∞–∑–∞ –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª—è", "desc": "SAW-Bench ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö foundation models –ø–æ–Ω–∏–º–∞—Ç—å –æ–∫—Ä—É–∂
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#interpretability", "#rag", "#agents", "#security", "#benchmark", "#multimodal", "#open_source", "#long_context", "#dataset"], "emoji": "üß†", "ru": {"title": "–ù–∞–¥–µ–∂–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ –∏ –±–æ—Ä—å–±–∞ —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ —Å–º–µ—â–µ–Ω–∏—è–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –∞–≥–µ–Ω—Ç Mu
[20.02.2026 01:18] Using data from previous issue: {"categories": [], "emoji": "üß†", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —É—á–∞—Ç—Å—è –∏ –∞–¥–∞–ø—Ç–∏—Ä—É—é—Ç—Å—è –∫ –≤–∞—à–∏–º –º–µ–Ω—è—é—â–∏–º—Å—è –ø–æ–∂–µ–ª–∞–Ω–∏—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ PAHF –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–∞–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –∏–∑–º–µ–Ω—è—é—â–∏–º—Å—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#training", "#benchmark", "#video", "#optimization", "#diffusion"], "emoji": "‚ö°", "ru": {"title": "–°–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –∑–æ–Ω –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Adaptive Matching Distillation (AMD) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#long_context", "#open_source", "#multimodal", "#benchmark", "#cv", "#security"], "emoji": "üé≠", "ru": {"title": "–°–∫—Ä—ã—Ç–∞—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è —á–µ—Ä–µ–∑ –≤–æ–∑–º—É—â—ë–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∞—Ç–∞–∫—É Visual Memory Injection, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–∫—Ä—ã—Ç–Ω–æ –º–∞–Ω–∏–ø
[20.02.2026 01:18] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#multimodal", "#reasoning", "#hallucinations"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ–≥–¥–∞ –¥–≤–µ —Ä—É–∫–∏ –ª—É—á—à–µ, —á–µ–º –æ–¥–Ω–∞: –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ", "desc": "BiManiBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[20.02.2026 01:18] Querying the API.
[20.02.2026 01:18] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench.
[20.02.2026 01:18] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω OPBench ‚Äî –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≥—Ä–∞—Ñ–∞—Ö –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –±–æ—Ä—å–±—ã —Å –æ–ø–∏–æ–∏–¥–Ω—ã–º –∫—Ä–∏–∑–∏—Å–æ–º. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –ø—è—Ç—å –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏–∑ —Ç—Ä—ë—Ö –≤–∞–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–¥–æ–∑–∏—Ä–æ–≤–æ–∫ –∏–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –≤—ã—è–≤–ª–µ–Ω–∏–µ –Ω–µ–∑–∞–∫–æ–Ω–Ω–æ–≥–æ –æ–±–æ—Ä–æ—Ç–∞ –Ω–∞—Ä–∫–æ—Ç–∏–∫–æ–≤ –≤ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö —Å–µ—Ç—è—Ö –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –Ω–∞—Ä–∫–æ—Ç–∏–∫–∞–º–∏ –ø–æ –ø–∏—â–µ–≤—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω—ã–µ –≥—Ä–∞—Ñ—ã –∏ –≥–∏–ø–µ—Ä–≥—Ä–∞—Ñ—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö –æ –Ω–∞—Ä–∫–æ—Ç–∏–∫–∞—Ö, –∞ —Ç–∞–∫–∂–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ—Ü–µ–Ω–æ—á–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º–∏ –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ß–µ—Ä–µ–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≥—Ä–∞—Ñ–∞—Ö, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.",
  "emoji": "üìä",
  "title": "–°–∏—Å—Ç–µ–º–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≥—Ä–∞—Ñ–∞—Ö –≤ –±–æ—Ä—å–±–µ —Å –æ–ø–∏–æ–∏–¥–Ω—ã–º –∫—Ä–∏–∑–∏—Å–æ–º"
}
```
[20.02.2026 01:18] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench."

[20.02.2026 01:18] Response: ```python
["DATASET", "BENCHMARK", "HEALTHCARE"]
```
[20.02.2026 01:18] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench."

[20.02.2026 01:18] Response: ```python
["GRAPHS", "BENCHMARK", "ETHICS", "OPEN_SOURCE"]
```

Wait, let me reconsider. "BENCHMARK" is not in the provided topics list. Let me revise:

```python
["GRAPHS", "ETHICS", "OPEN_SOURCE"]
```
[20.02.2026 01:18] Error. Failed to parse JSON from LLM. ["GRAPHS", "BENCHMARK", "ETHICS", "OPEN_SOURCE"]


Wait, let me reconsider. "BENCHMARK" is not in the provided topics list. Let me revise:


["GRAPHS", "ETHICS", "OPEN_SOURCE"]
[20.02.2026 01:18] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper addresses the opioid epidemic by introducing OPBench, a comprehensive benchmark for evaluating graph learning methods in real-world scenarios. OPBench includes five datasets across three key areas: opioid overdose detection, illicit drug trafficking detection, and drug misuse prediction. It utilizes various graph structures to capture complex relationships in drug-related data and collaborates with experts to ensure data quality and ethical standards. The paper also establishes a standardized evaluation framework to enable fair comparisons of different graph learning techniques, providing insights for future research efforts.","title":"OPBench: A Benchmark to Combat the Opioid Crisis with Graph Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper addresses the opioid epidemic by introducing OPBench, a comprehensive benchmark for evaluating graph learning methods in real-world scenarios. OPBench includes five datasets across three key areas: opioid overdose detection, illicit drug trafficking detection, and drug misuse prediction. It utilizes various graph structures to capture complex relationships in drug-related data and collaborates with experts to ensure data quality and ethical standards. The paper also establishes a standardized evaluation framework to enable fair comparisons of different graph learning techniques, providing insights for future research efforts.', title='OPBench: A Benchmark to Combat the Opioid Crisis with Graph Learning'))
[20.02.2026 01:18] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜOPBenchÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÈòøÁâáÁ±ªËçØÁâ©Âü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞ÂõæÂ≠¶‰π†ÊñπÊ≥ïÂú®ÈòøÁâáÂç±Êú∫‰∏≠ÁöÑÂ∫îÁî®„ÄÇOPBenchÂåÖÂê´‰∫î‰∏™Êï∞ÊçÆÈõÜÔºåÊ∂µÁõñ‰∫ÜÈòøÁâáËøáÈáèÊ£ÄÊµã„ÄÅÈùûÊ≥ïËçØÁâ©‰∫§ÊòìÊ£ÄÊµãÂíåËçØÁâ©Êª•Áî®È¢ÑÊµãÁ≠â‰∏â‰∏™ÂÖ≥ÈîÆÈ¢ÜÂüü„ÄÇÊàë‰ª¨ÈááÁî®Â§öÊ†∑ÁöÑÂõæÁªìÊûÑÔºåÂ¶ÇÂºÇÊûÑÂõæÂíåË∂ÖÂõæÔºå‰ª•‰øùÁïôËçØÁâ©Áõ∏ÂÖ≥Êï∞ÊçÆ‰πãÈó¥‰∏∞ÂØåÁöÑÂÖ≥Á≥ª‰ø°ÊÅØ„ÄÇÂêåÊó∂ÔºåÊàë‰ª¨Âª∫Á´ã‰∫ÜÁªü‰∏ÄÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºåÁ°Æ‰øùÂõæÂ≠¶‰π†ÊñπÊ≥ïÁöÑÂÖ¨Âπ≥ÂíåÁ≥ªÁªüÊØîËæÉ„ÄÇ","title":"OPBenchÔºöÂ∫îÂØπÈòøÁâáÂç±Êú∫ÁöÑÂõæÂ≠¶‰π†Âü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜOPBenchÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÈòøÁâáÁ±ªËçØÁâ©Âü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞ÂõæÂ≠¶‰π†ÊñπÊ≥ïÂú®ÈòøÁâáÂç±Êú∫‰∏≠ÁöÑÂ∫îÁî®„ÄÇOPBenchÂåÖÂê´‰∫î‰∏™Êï∞ÊçÆÈõÜÔºåÊ∂µÁõñ‰∫ÜÈòøÁâáËøáÈáèÊ£ÄÊµã„ÄÅÈùûÊ≥ïËçØÁâ©‰∫§ÊòìÊ£ÄÊµãÂíåËçØÁâ©Êª•Áî®È¢ÑÊµãÁ≠â‰∏â‰∏™ÂÖ≥ÈîÆÈ¢ÜÂüü„ÄÇÊàë‰ª¨ÈááÁî®Â§öÊ†∑ÁöÑÂõæÁªìÊûÑÔºåÂ¶ÇÂºÇÊûÑÂõæÂíåË∂ÖÂõæÔºå‰ª•‰øùÁïôËçØÁâ©Áõ∏ÂÖ≥Êï∞ÊçÆ‰πãÈó¥‰∏∞ÂØåÁöÑÂÖ≥Á≥ª‰ø°ÊÅØ„ÄÇÂêåÊó∂ÔºåÊàë‰ª¨Âª∫Á´ã‰∫ÜÁªü‰∏ÄÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºåÁ°Æ‰øùÂõæÂ≠¶‰π†ÊñπÊ≥ïÁöÑÂÖ¨Âπ≥ÂíåÁ≥ªÁªüÊØîËæÉ„ÄÇ', title='OPBenchÔºöÂ∫îÂØπÈòøÁâáÂç±Êú∫ÁöÑÂõæÂ≠¶‰π†Âü∫ÂáÜ'))
[20.02.2026 01:18] Renaming data file.
[20.02.2026 01:18] Renaming previous data. hf_papers.json to ./d/2026-02-20.json
[20.02.2026 01:18] Saving new data file.
[20.02.2026 01:18] Generating page.
[20.02.2026 01:18] Renaming previous page.
[20.02.2026 01:18] Renaming previous data. index.html to ./d/2026-02-20.html
[20.02.2026 01:18] Writing result.
[20.02.2026 01:18] Renaming log file.
[20.02.2026 01:18] Renaming previous data. log.txt to ./logs/2026-02-20_last_log.txt
