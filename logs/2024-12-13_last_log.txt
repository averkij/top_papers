[13.12.2024 17:09] Read previous papers.
[13.12.2024 17:09] Generating top page (month).
[13.12.2024 17:09] Writing top page (month).
[13.12.2024 18:14] Read previous papers.
[13.12.2024 18:14] Get feed.
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09596
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08905
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08737
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08635
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09618
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09605
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09619
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09593
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05994
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09501
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09569
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08972
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09405
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09349
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09586
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09585
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09622
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09460
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09013
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09370
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09573
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09025
[13.12.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05552
[13.12.2024 18:14] Extract page data from URL. URL: https://huggingface.co/papers/2412.08687
[13.12.2024 18:14] Extract page data from URL. URL: https://huggingface.co/papers/2412.06745
[13.12.2024 18:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.12.2024 18:14] No deleted papers detected.
[13.12.2024 18:14] Downloading and parsing papers (pdf, html). Total: 25.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09596.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09596.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09596.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.08905.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.08905.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.08905.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.08737.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.08737.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.08737.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.08635.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.08635.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.08635.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09618.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09618.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09618.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09605.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09605.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09605.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09619.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09619.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09619.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09593.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09593.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09593.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.05994.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.05994.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.05994.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09501.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09501.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09501.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09569.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09569.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09569.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.08972.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.08972.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.08972.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09405.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09405.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09405.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09349.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09349.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09349.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09586.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09586.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09586.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09585.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09585.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09585.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09622.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09622.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09622.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09460.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09460.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09460.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09013.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09013.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09013.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09370.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09370.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09370.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09573.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09573.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09573.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.09025.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.09025.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.09025.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.05552.
[13.12.2024 18:14] Extra JSON file exists (./assets/json/2412.05552.json), skip PDF parsing.
[13.12.2024 18:14] Paper image links file exists (./assets/img_data/2412.05552.json), skip HTML parsing.
[13.12.2024 18:14] Success.
[13.12.2024 18:14] Downloading and parsing paper https://huggingface.co/papers/2412.08687.
[13.12.2024 18:14] Downloading paper 2412.08687 from http://arxiv.org/pdf/2412.08687v1...
[13.12.2024 18:15] Extracting affiliations from text.
[13.12.2024 18:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"4 2 0 2 1 1 ] . [ 1 7 8 6 8 0 . 2 1 4 2 : r VisionArena: 230K Real World User-VLM Conversations with Preference Labels Christopher Chou* Stanford Lisa Dunlap* UC Berkeley Joseph E. Gonzalez UC Berkeley Wei-Lin Chiang UC Berkeley Figure 1. Samples from VisionArena Conversations. VisionArena contains conversations from real users covering variety of domains. "
[13.12.2024 18:15] Response: ```python
["Stanford", "UC Berkeley", "UC Berkeley", "UC Berkeley"]
```
[13.12.2024 18:15] Deleting PDF ./assets/pdf/2412.08687.pdf.
[13.12.2024 18:15] Success.
[13.12.2024 18:15] Downloading and parsing paper https://huggingface.co/papers/2412.06745.
[13.12.2024 18:15] Downloading paper 2412.06745 from http://arxiv.org/pdf/2412.06745v1...
[13.12.2024 18:15] Extracting affiliations from text.
[13.12.2024 18:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"4 2 0 2 9 ] . [ 1 5 4 7 6 0 . 2 1 4 2 : r ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities Adhiraj Ghosh1,2 Sebastian Dziadzio1,2 Ameya Prabhu1,2 Vishaal Udandarao1,2,3 Samuel Albanie Matthias Bethge1,2 1Tubingen AI Center, University of Tubingen 2Open-Ψ (Open-Sci) Collective 3University of Cambridge github.com/bethgelab/onebench ı huggingface.co/datasets/bethgelab/onebench Abstract Traditional fixed test datasets fall short in evaluating the open-ended capabilities of foundation models. To address this limitation, we propose ONEBench (OpeN-Ended Benchmarking), new testing paradigm that consolidates individual evaluation datasets into unified, ever-expanding sample pool. ONEBench allows users to generate custom, open-ended evaluation benchmarks from this sample pool, corresponding to specific capabilities of interest. By aggregating and reusing samples across test sets, ONEBench enables the assessment of diverse capabilities beyond those covered by the original test sets, while mitigating overfitting and dataset bias. Most importantly, it frames model evaluation as collective process of selecting and aggregating sample-level tests. The shift from task-specific benchmarks to ONEBench introduces two key challenges: (1) heterogeneity and (2) incompleteness. Heterogeneity refers to the aggregation over diverse metrics, including binary, numeric, and ordinal data, while incompleteness describes comparing models evaluated on different subsets of testing data. To address these challenges, we explore algorithms to aggregate sparse, unequal measurements into reliable model scores. Our aggregation algorithm ensures identifiability (asymptotically recovering ground-truth scores) and rapid convergence, enabling accurate model comparisons with relatively little data. On homogenous datasets, we show our aggregation algorithm provides rankings that highly correlate with those produced by average scores. Furthermore, we demonstrate robustness to ove"
[13.12.2024 18:15] Response: ```python
[
    "Tubingen AI Center, University of Tubingen",
    "Open-Ψ (Open-Sci) Collective",
    "University of Cambridge"
]
```
[13.12.2024 18:15] Deleting PDF ./assets/pdf/2412.06745.pdf.
[13.12.2024 18:15] Success.
[13.12.2024 18:15] Enriching papers with extra data.
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 0. Creating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuou...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 1. We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data ...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 2. Multimodal large language models (MLLMs) have made rapid progress in recent years, yet continue to struggle with low-level visual perception (LLVP) -- particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robot...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 3. Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 4. Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among image...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 5. Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective t...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 6. Existing text-to-image (T2I) diffusion models face several limitations, including large model sizes, slow runtime, and low-quality generation on mobile devices. This paper aims to address all of these challenges by developing an extremely small and fast T2I model that generates high-resolution and h...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 7. Recovering the geometry and materials of objects from a single image is challenging due to its under-constrained nature. In this paper, we present Neural LightRig, a novel framework that boosts intrinsic estimation by leveraging auxiliary multi-lighting conditions from 2D diffusion priors. Specifica...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 8. The approximation of Partial Differential Equations (PDEs) using neural networks has seen significant advancements through Physics-Informed Neural Networks (PINNs). Despite their straightforward optimization framework and flexibility in implementing various PDEs, PINNs often suffer from limited accu...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 9. As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond single-domain capabilities is essential to meet the demands for more versatile and efficient AI. However, previous omni-models have insufficiently explored speech, neglecting its integration with multi-modality. We introduce Lyra,...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 10. Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this ...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 11. This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains -- airline baggage fees, NBA transactions, and tax regulations -- RuleArena assesses ...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 12. Modern sensors produce increasingly rich streams of high-resolution data. Due to resource constraints, machine learning systems discard the vast majority of this information via resolution reduction. Compressed-domain learning allows models to operate on compact latent representations, allowing high...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 13. Controllable human image animation aims to generate videos from reference images using driving videos. Due to the limited control signals provided by sparse guidance (e.g., skeleton pose), recent works have attempted to introduce additional dense conditions (e.g., depth map) to ensure motion alignme...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 14. We address the problem of gaze target estimation, which aims to predict where a person is looking in a scene. Predicting a person's gaze target requires reasoning both about the person's appearance and the contents of the scene. Prior works have developed increasingly complex, hand-crafted pipelines...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 15. The standard practice for developing contemporary MLLMs is to feed features from vision encoder(s) into the LLM and train with natural language supervision. In this work, we posit an overlooked opportunity to optimize the intermediate LLM representations through a vision perspective (objective), i.e...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 16. Recent advances in text-to-image customization have enabled high-fidelity, context-rich generation of personalized images, allowing specific concepts to appear in a variety of scenarios. However, current methods struggle with combining multiple personalized models, often leading to attribute entangl...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 17. The use of copyrighted materials in training generative language models raises critical legal and ethical questions. This paper presents a framework for and the results of empirically assessing the impact of copyrighted materials on the performance of large language models (LLMs) for Norwegian. We f...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 18. This study presents a new image super-resolution (SR) technique based on diffusion inversion, aiming at harnessing the rich image priors encapsulated in large pre-trained diffusion models to improve SR performance. We design a Partial noise Prediction strategy to construct an intermediate state of t...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 19. Word Sense Disambiguation (WSD) is the task of associating a word in a given context with its most suitable meaning among a set of possible candidates. While the task has recently witnessed renewed interest, with systems achieving performances above the estimated inter-annotator agreement, at the ti...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 20. Existing sparse-view reconstruction models heavily rely on accurate known camera poses. However, deriving camera extrinsics and intrinsics from sparse-view images presents significant challenges. In this work, we present FreeSplatter, a highly scalable, feed-forward reconstruction framework capable ...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 21. Neural Machine Translation (NMT) models are typically trained on datasets with limited exposure to Scientific, Technical and Educational domains. Translation models thus, in general, struggle with tasks that involve scientific understanding or technical jargon. Their performance is found to be even ...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 22. The academic field of learning instruction-guided visual navigation can be generally categorized into high-level category-specific search and low-level language-guided navigation, depending on the granularity of language instruction, in which the former emphasizes the exploration process, while the ...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 23. With the growing adoption and capabilities of vision-language models (VLMs) comes the need for benchmarks that capture authentic user-VLM interactions. In response, we create VisionArena, a dataset of 230K real-world conversations between users and VLMs. Collected from Chatbot Arena - an open-source...
[13.12.2024 18:15] ********************************************************************************
[13.12.2024 18:15] Abstract 24. Traditional fixed test sets fall short in evaluating open-ended capabilities of foundation models. To address this, we propose ONEBench(OpeN-Ended Benchmarking), a new testing paradigm that consolidates individual evaluation datasets into a unified, ever-expanding sample pool. ONEBench allows users ...
[13.12.2024 18:15] Read previous papers.
[13.12.2024 18:15] Generating reviews via LLM API.
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#long_context", "#audio", "#cv", "#multimodal", "#reasoning"], "emoji": "🧠", "ru": {"title": "Непрерывное восприятие и рассуждение: новый шаг к человекоподобному ИИ", "desc": "Данная статья представляет новый подход к созданию систем искусственного интеллекта, способных к длительном
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#data", "#reasoning", "#synthetic", "#training", "#benchmark", "#architecture"], "emoji": "🧠", "ru": {"title": "Качество данных - ключ к превосходству языковой модели", "desc": "Статья представляет phi-4 - языковую модель с 14 миллиардами параметров, разработанную с акцентом на каче
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#low_resource", "#synthetic", "#multimodal", "#optimization", "#training", "#data"], "emoji": "📐", "ru": {"title": "Прорыв в геометрическом восприятии для мультимодальных ИИ-моделей", "desc": "Статья представляет новый бенчмарк Geoperception для оценки
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#audio", "#video", "#cv", "#multimodal", "#training"], "emoji": "🧠", "ru": {"title": "LatentLM: Объединяя дискретное и непрерывное в мультимодальных моделях", "desc": "Статья представляет Latent Language Modeling (LatentLM) - унифицированный подход к о
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#multimodal", "#diffusion", "#optimization", "#training"], "emoji": "🖼️", "ru": {"title": "EasyRef: умная адаптация диффузионных моделей к нескольким референсам", "desc": "Статья представляет EasyRef - новый метод адаптации для диффузионных моделей, позволя
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#data", "#dataset", "#training", "#optimization", "#agents", "#synthetic"], "emoji": "🤖", "ru": {"title": "AgentTrek: Революция в обучении GUI-агентов с помощью веб-руководств", "desc": "Статья представляет AgentTrek - масштабируемый конвейер для синтеза данных, который генерирует в
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#architecture", "#small_models", "#training", "#optimization", "#diffusion", "#benchmark"], "emoji": "📱", "ru": {"title": "Компактная и быстрая генерация изображений на мобильных устройствах", "desc": "Статья представляет новую модель генерации изображений по тексту, SnapGen, разраб
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#diffusion", "#synthetic", "#open_source", "#cv", "#3d", "#dataset"], "emoji": "💡", "ru": {"title": "Улучшение оценки 3D-геометрии с помощью синтетического освещения", "desc": "Статья представляет Neural LightRig - новый подход к оценке внутренних свойств объектов на изображении. Ме
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#optimization", "#math", "#architecture"], "emoji": "📊", "ru": {"title": "Адаптивные гауссовы функции для точного решения сложных дифференциальных уравнений", "desc": "В статье представлен новый подход к аппроксимации дифференциальных уравнений в частных производных (ДУЧП) с использ
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#long_context", "#multimodal", "#benchmark"], "emoji": "🎭", "ru": {"title": "Lyra: Эффективная мультимодальная модель для комплексного понимания речи и изображений", "desc": "Lyra - это эффективная мультимодальная большая языковая модель (MLLM), которая у
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#interpretability", "#alignment", "#benchmark", "#rlhf"], "emoji": "⚖️", "ru": {"title": "LLM как беспристрастные судьи ИИ-систем", "desc": "Это исследование посвящено оценке качества языковых моделей (LLM) в роли судей для сравнения и выбора между различными генеративными ИИ-систем
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#long_context", "#math", "#benchmark", "#reasoning"], "emoji": "🧠", "ru": {"title": "Большие языковые модели спотыкаются на сложных правилах реального мира", "desc": "RuleArena - это новый сложный бенчмарк для оценки способности больших языковых моделей следовать сложным правилам в 
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#architecture", "#inference", "#synthetic", "#dataset", "#optimization", "#multimodal"], "emoji": "🗜️", "ru": {"title": "WaLLoC: эффективное сжатие для машинного обучения на данных высокого разрешения", "desc": "Статья представляет WaLLoC - новую архитектуру нейронного кодека для сж
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#video"], "emoji": "🎭", "ru": {"title": "DisPose: Улучшенный контроль анимации человека без дополнительных входных данных", "desc": "Статья представляет DisPose - новый метод для контролируемой анимации изображений человека. Авторы предлагают разделить скелетну
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#cv", "#reasoning"], "emoji": "👀", "ru": {"title": "Простой и эффективный метод оценки направления взгляда с помощью трансформеров", "desc": "Статья представляет Gaze-LLE - новый метод оценки направления взгляда человека на основе тран
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#open_source", "#cv", "#multimodal"], "emoji": "👁️", "ru": {"title": "Оптимизация визуального понимания языковых моделей через призму зрения", "desc": "Статья представляет новый подход к обучению мультимодальных языковых моделей (MLLM)
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#training", "#multimodal", "#cv"], "emoji": "🎨", "ru": {"title": "LoRACLR: Гармоничное слияние концепций для персонализированной генерации изображений", "desc": "LoRACLR - это новый подход к генерации изображений с множественными концепциями, объединяющий несколько моделей LoRA без 
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#dataset", "#benchmark", "#ethics"], "emoji": "📚", "ru": {"title": "Авторское право в эпоху искусственного интеллекта: оценка влияния на языковые модели", "desc": "Статья исследует влияние защищенных авторским правом материалов на эффективность боль
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#cv", "#open_source", "#dataset", "#architecture", "#diffusion"], "emoji": "🔍", "ru": {"title": "Улучшение сверхразрешения изображений с помощью инверсии диффузии", "desc": "Это исследование представляет новую технику сверхразрешения изображений, основанную на инверсии диффузии. Мет
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#multilingual", "#reasoning", "#dataset", "#architecture"], "emoji": "🔗", "ru": {"title": "От WSD к WSL: Новый подход к пониманию смысла слов в тексте", "desc": "Статья представляет новую задачу под названием Word Sense Linking (WSL), которая расширяет традиционную задачу Word Sense
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#architecture", "#3d"], "emoji": "🔍", "ru": {"title": "Революция в 3D-реконструкции: от неоткалиброванных изображений к точным моделям", "desc": "FreeSplatter - это инновационная модель реконструкции 3D-объектов, работающая с неоткалиброванными изображениями с разных ракурсов. Она и
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#benchmark", "#multilingual", "#low_resource", "#machine_translation", "#dataset", "#open_source", "#training"], "emoji": "🌏", "ru": {"title": "Новый корпус для улучшения машинного перевода индийских языков", "desc": "Эта статья представляет новый многоязычный параллельный корпус дл
[13.12.2024 18:15] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#agents", "#optimization"], "emoji": "🧭", "ru": {"title": "Универсальный агент для визуальной навигации с языковыми инструкциями", "desc": "Это исследование предлагает универсальную модель для различных задач визуальной навигации с языковыми инструкциями
[13.12.2024 18:15] Querying the API.
[13.12.2024 18:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

With the growing adoption and capabilities of vision-language models (VLMs) comes the need for benchmarks that capture authentic user-VLM interactions. In response, we create VisionArena, a dataset of 230K real-world conversations between users and VLMs. Collected from Chatbot Arena - an open-source platform where users interact with VLMs and submit preference votes - VisionArena spans 73K unique users, 45 VLMs, and 138 languages. Our dataset contains three subsets: VisionArena-Chat, 200k single and multi-turn conversations between a user and a VLM; VisionArena-Battle, 30K conversations comparing two anonymous VLMs with user preference votes; and VisionArena-Bench, an automatic benchmark of 500 diverse user prompts that efficiently approximate the live Chatbot Arena model rankings. Additionally, we highlight the types of question asked by users, the influence of response style on preference, and areas where models often fail. We find open-ended tasks like captioning and humor are highly style-dependent, and current VLMs struggle with spatial reasoning and planning tasks. Lastly, we show finetuning the same base model on VisionArena-Chat outperforms Llava-Instruct-158K, with a 17-point gain on MMMU and a 46-point gain on the WildVision benchmark. Dataset at https://huggingface.co/lmarena-ai
[13.12.2024 18:15] Response: {
  "desc": "Статья представляет VisionArena - набор данных из 230 тысяч реальных диалогов между пользователями и мультимодальными языковыми моделями (VLM). Датасет включает три подмножества: чаты, сравнения моделей и автоматический бенчмарк. Исследователи анализируют типы вопросов пользователей, влияние стиля ответов на предпочтения и области, где модели часто ошибаются. Дообучение базовой модели на данных VisionArena-Chat значительно улучшает ее производительность на нескольких бенчмарках.",
  "emoji": "🖼️",
  "title": "VisionArena: реальные диалоги для оценки зрительно-языковых моделей"
}
[13.12.2024 18:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"With the growing adoption and capabilities of vision-language models (VLMs) comes the need for benchmarks that capture authentic user-VLM interactions. In response, we create VisionArena, a dataset of 230K real-world conversations between users and VLMs. Collected from Chatbot Arena - an open-source platform where users interact with VLMs and submit preference votes - VisionArena spans 73K unique users, 45 VLMs, and 138 languages. Our dataset contains three subsets: VisionArena-Chat, 200k single and multi-turn conversations between a user and a VLM; VisionArena-Battle, 30K conversations comparing two anonymous VLMs with user preference votes; and VisionArena-Bench, an automatic benchmark of 500 diverse user prompts that efficiently approximate the live Chatbot Arena model rankings. Additionally, we highlight the types of question asked by users, the influence of response style on preference, and areas where models often fail. We find open-ended tasks like captioning and humor are highly style-dependent, and current VLMs struggle with spatial reasoning and planning tasks. Lastly, we show finetuning the same base model on VisionArena-Chat outperforms Llava-Instruct-158K, with a 17-point gain on MMMU and a 46-point gain on the WildVision benchmark. Dataset at https://huggingface.co/lmarena-ai"

[13.12.2024 18:15] Response: ```python
['DATASET', 'BENCHMARK', 'MULTIMODAL', 'TRAINING']
```
[13.12.2024 18:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"With the growing adoption and capabilities of vision-language models (VLMs) comes the need for benchmarks that capture authentic user-VLM interactions. In response, we create VisionArena, a dataset of 230K real-world conversations between users and VLMs. Collected from Chatbot Arena - an open-source platform where users interact with VLMs and submit preference votes - VisionArena spans 73K unique users, 45 VLMs, and 138 languages. Our dataset contains three subsets: VisionArena-Chat, 200k single and multi-turn conversations between a user and a VLM; VisionArena-Battle, 30K conversations comparing two anonymous VLMs with user preference votes; and VisionArena-Bench, an automatic benchmark of 500 diverse user prompts that efficiently approximate the live Chatbot Arena model rankings. Additionally, we highlight the types of question asked by users, the influence of response style on preference, and areas where models often fail. We find open-ended tasks like captioning and humor are highly style-dependent, and current VLMs struggle with spatial reasoning and planning tasks. Lastly, we show finetuning the same base model on VisionArena-Chat outperforms Llava-Instruct-158K, with a 17-point gain on MMMU and a 46-point gain on the WildVision benchmark. Dataset at https://huggingface.co/lmarena-ai"

[13.12.2024 18:15] Response: ```python
['OPEN_SOURCE', 'GAMES', 'INTERPRETABILITY', 'REASONING']
```
[13.12.2024 18:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces VisionArena, a comprehensive dataset designed to evaluate user interactions with vision-language models (VLMs). It consists of 230,000 real-world conversations collected from an open-source platform, featuring diverse user inputs across 138 languages and 45 different VLMs. The dataset is divided into three subsets: VisionArena-Chat for general conversations, VisionArena-Battle for comparative analysis of VLMs, and VisionArena-Bench for benchmarking model performance. The findings reveal that VLMs struggle with tasks requiring spatial reasoning and planning, while performance improves significantly when models are fine-tuned on the VisionArena-Chat data.","title":"VisionArena: Benchmarking User Interactions with Vision-Language Models"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper introduces VisionArena, a comprehensive dataset designed to evaluate user interactions with vision-language models (VLMs). It consists of 230,000 real-world conversations collected from an open-source platform, featuring diverse user inputs across 138 languages and 45 different VLMs. The dataset is divided into three subsets: VisionArena-Chat for general conversations, VisionArena-Battle for comparative analysis of VLMs, and VisionArena-Bench for benchmarking model performance. The findings reveal that VLMs struggle with tasks requiring spatial reasoning and planning, while performance improves significantly when models are fine-tuned on the VisionArena-Chat data.', title='VisionArena: Benchmarking User Interactions with Vision-Language Models'))
[13.12.2024 18:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"随着视觉语言模型（VLM）的广泛应用，用户与VLM之间真实互动的基准测试变得越来越重要。为此，我们创建了VisionArena，这是一个包含23万条用户与VLM之间真实对话的数据集。该数据集来自Chatbot Arena平台，涵盖了73,000名独特用户、45个VLM和138种语言。我们发现，开放式任务如图像描述和幽默感对响应风格高度依赖，而当前的VLM在空间推理和规划任务上表现不佳。","title":"真实互动的视觉语言模型基准测试"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='随着视觉语言模型（VLM）的广泛应用，用户与VLM之间真实互动的基准测试变得越来越重要。为此，我们创建了VisionArena，这是一个包含23万条用户与VLM之间真实对话的数据集。该数据集来自Chatbot Arena平台，涵盖了73,000名独特用户、45个VLM和138种语言。我们发现，开放式任务如图像描述和幽默感对响应风格高度依赖，而当前的VLM在空间推理和规划任务上表现不佳。', title='真实互动的视觉语言模型基准测试'))
[13.12.2024 18:15] Querying the API.
[13.12.2024 18:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Traditional fixed test sets fall short in evaluating open-ended capabilities of foundation models. To address this, we propose ONEBench(OpeN-Ended Benchmarking), a new testing paradigm that consolidates individual evaluation datasets into a unified, ever-expanding sample pool. ONEBench allows users to generate custom, open-ended evaluation benchmarks from this pool, corresponding to specific capabilities of interest. By aggregating samples across test sets, ONEBench enables the assessment of diverse capabilities beyond those covered by the original test sets, while mitigating overfitting and dataset bias. Most importantly, it frames model evaluation as a collective process of selecting and aggregating sample-level tests.   The shift from task-specific benchmarks to ONEBench introduces two challenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the aggregation over diverse metrics, while incompleteness describes comparing models evaluated on different data subsets. To address these challenges, we explore algorithms to aggregate sparse measurements into reliable model scores. Our aggregation algorithm ensures identifiability(asymptotically recovering ground-truth scores) and rapid convergence, enabling accurate model ranking with less data. On homogenous datasets, we show our aggregation algorithm provides rankings that highly correlate with those produced by average scores. We also demonstrate robustness to ~95% of measurements missing, reducing evaluation cost by up to 20x with little-to-no change in model rankings. We introduce ONEBench-LLM for language models and ONEBench-LMM for vision-language models, unifying evaluations across these domains. Overall, we present a technique for open-ended evaluation, which can aggregate over incomplete, heterogeneous sample-level measurements to continually grow a benchmark alongside the rapidly developing foundation models.
[13.12.2024 18:16] Response: {
  "desc": "ONEBench - это новая парадигма тестирования, объединяющая отдельные наборы данных для оценки в единый расширяемый пул образцов. Она позволяет генерировать пользовательские тесты для оценки конкретных возможностей моделей, решая проблемы гетерогенности и неполноты данных. Предложенный алгоритм агрегации обеспечивает надежное ранжирование моделей даже при отсутствии до 95% измерений. ONEBench представлен в версиях для языковых и мультимодальных моделей, объединяя оценки в этих областях.",
  "emoji": "🧪",
  "title": "Единый открытый бенчмарк для комплексной оценки фундаментальных моделей"
}
[13.12.2024 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Traditional fixed test sets fall short in evaluating open-ended capabilities of foundation models. To address this, we propose ONEBench(OpeN-Ended Benchmarking), a new testing paradigm that consolidates individual evaluation datasets into a unified, ever-expanding sample pool. ONEBench allows users to generate custom, open-ended evaluation benchmarks from this pool, corresponding to specific capabilities of interest. By aggregating samples across test sets, ONEBench enables the assessment of diverse capabilities beyond those covered by the original test sets, while mitigating overfitting and dataset bias. Most importantly, it frames model evaluation as a collective process of selecting and aggregating sample-level tests.   The shift from task-specific benchmarks to ONEBench introduces two challenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the aggregation over diverse metrics, while incompleteness describes comparing models evaluated on different data subsets. To address these challenges, we explore algorithms to aggregate sparse measurements into reliable model scores. Our aggregation algorithm ensures identifiability(asymptotically recovering ground-truth scores) and rapid convergence, enabling accurate model ranking with less data. On homogenous datasets, we show our aggregation algorithm provides rankings that highly correlate with those produced by average scores. We also demonstrate robustness to ~95% of measurements missing, reducing evaluation cost by up to 20x with little-to-no change in model rankings. We introduce ONEBench-LLM for language models and ONEBench-LMM for vision-language models, unifying evaluations across these domains. Overall, we present a technique for open-ended evaluation, which can aggregate over incomplete, heterogeneous sample-level measurements to continually grow a benchmark alongside the rapidly developing foundation models."

[13.12.2024 18:16] Response: ```python
["BENCHMARK", "DATASET"]
```
[13.12.2024 18:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Traditional fixed test sets fall short in evaluating open-ended capabilities of foundation models. To address this, we propose ONEBench(OpeN-Ended Benchmarking), a new testing paradigm that consolidates individual evaluation datasets into a unified, ever-expanding sample pool. ONEBench allows users to generate custom, open-ended evaluation benchmarks from this pool, corresponding to specific capabilities of interest. By aggregating samples across test sets, ONEBench enables the assessment of diverse capabilities beyond those covered by the original test sets, while mitigating overfitting and dataset bias. Most importantly, it frames model evaluation as a collective process of selecting and aggregating sample-level tests.   The shift from task-specific benchmarks to ONEBench introduces two challenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the aggregation over diverse metrics, while incompleteness describes comparing models evaluated on different data subsets. To address these challenges, we explore algorithms to aggregate sparse measurements into reliable model scores. Our aggregation algorithm ensures identifiability(asymptotically recovering ground-truth scores) and rapid convergence, enabling accurate model ranking with less data. On homogenous datasets, we show our aggregation algorithm provides rankings that highly correlate with those produced by average scores. We also demonstrate robustness to ~95% of measurements missing, reducing evaluation cost by up to 20x with little-to-no change in model rankings. We introduce ONEBench-LLM for language models and ONEBench-LMM for vision-language models, unifying evaluations across these domains. Overall, we present a technique for open-ended evaluation, which can aggregate over incomplete, heterogeneous sample-level measurements to continually grow a benchmark alongside the rapidly developing foundation models."

[13.12.2024 18:16] Response: ```python
["OPTIMIZATION", "SURVEY"]
```
[13.12.2024 18:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces ONEBench, a new approach for evaluating foundation models that overcomes the limitations of traditional fixed test sets. ONEBench creates a flexible and expanding pool of evaluation samples, allowing users to design custom benchmarks that assess various capabilities of models. The method addresses challenges like heterogeneity and incompleteness by using algorithms that aggregate sparse data into reliable scores, ensuring accurate model rankings even with missing measurements. This innovative framework supports ongoing evaluation as models evolve, making it suitable for both language and vision-language models.","title":"ONEBench: Evolving Evaluation for Foundation Models"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper introduces ONEBench, a new approach for evaluating foundation models that overcomes the limitations of traditional fixed test sets. ONEBench creates a flexible and expanding pool of evaluation samples, allowing users to design custom benchmarks that assess various capabilities of models. The method addresses challenges like heterogeneity and incompleteness by using algorithms that aggregate sparse data into reliable scores, ensuring accurate model rankings even with missing measurements. This innovative framework supports ongoing evaluation as models evolve, making it suitable for both language and vision-language models.', title='ONEBench: Evolving Evaluation for Foundation Models'))
[13.12.2024 18:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"传统的固定测试集无法有效评估基础模型的开放能力。为了解决这个问题，我们提出了ONEBench（开放式基准测试），这是一种新的测试范式，将各个评估数据集整合成一个统一且不断扩展的样本池。ONEBench允许用户从这个样本池中生成自定义的开放式评估基准，以对应特定的能力需求。通过聚合不同测试集的样本，ONEBench能够评估超出原始测试集覆盖范围的多样化能力，同时减轻过拟合和数据集偏差的问题。","title":"ONEBench：开放式评估的新范式"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='传统的固定测试集无法有效评估基础模型的开放能力。为了解决这个问题，我们提出了ONEBench（开放式基准测试），这是一种新的测试范式，将各个评估数据集整合成一个统一且不断扩展的样本池。ONEBench允许用户从这个样本池中生成自定义的开放式评估基准，以对应特定的能力需求。通过聚合不同测试集的样本，ONEBench能够评估超出原始测试集覆盖范围的多样化能力，同时减轻过拟合和数据集偏差的问题。', title='ONEBench：开放式评估的新范式'))
[13.12.2024 18:16] Loading Chinese text from previous data.
[13.12.2024 18:16] Renaming data file.
[13.12.2024 18:16] Renaming previous data. hf_papers.json to ./d/2024-12-13.json
[13.12.2024 18:16] Saving new data file.
[13.12.2024 18:16] Generating page.
[13.12.2024 18:16] Renaming previous page.
[13.12.2024 18:16] Renaming previous data. index.html to ./d/2024-12-13.html
[13.12.2024 18:16] [Experimental] Generating Chinese page for reading.
[13.12.2024 18:16] Chinese vocab [{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '创建', 'pinyin': 'chuàng jiàn', 'trans': 'create'}, {'word': '能够', 'pinyin': 'néng gòu', 'trans': 'be able to'}, {'word': '长时间', 'pinyin': 'cháng shí jiān', 'trans': 'long period of time'}, {'word': '互动', 'pinyin': 'hù dòng', 'trans': 'interact'}, {'word': '人工智能', 'pinyin': 'rén gōng zhì néng', 'trans': 'artificial intelligence'}, {'word': '系统', 'pinyin': 'xì tǒng', 'trans': 'system'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'}, {'word': '目标', 'pinyin': 'mù biāo', 'trans': 'goal'}, {'word': '多模态', 'pinyin': 'duō mó tài', 'trans': 'multimodal'}, {'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'}, {'word': '开放', 'pinyin': 'kāi fàng', 'trans': 'open'}, {'word': '世界', 'pinyin': 'shì jiè', 'trans': 'world'}, {'word': '理解', 'pinyin': 'lǐ jiě', 'trans': 'understand'}, {'word': '方面', 'pinyin': 'fāng miàn', 'trans': 'aspect'}, {'word': '取得', 'pinyin': 'qǔ dé', 'trans': 'achieve'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '进展', 'pinyin': 'jìn zhǎn', 'trans': 'progress'}, {'word': '然而', 'pinyin': 'rán ér', 'trans': 'however'}, {'word': '同时', 'pinyin': 'tóng shí', 'trans': 'simultaneously'}, {'word': '持续', 'pinyin': 'chí xù', 'trans': 'continuous'}, {'word': '感知', 'pinyin': 'gǎn zhī', 'trans': 'perceive'}, {'word': '记忆', 'pinyin': 'jì yì', 'trans': 'memory'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reason'}, {'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'}, {'word': '未', 'pinyin': 'wèi', 'trans': 'not yet'}, {'word': '充分', 'pinyin': 'chōng fèn', 'trans': 'adequately'}, {'word': '探索', 'pinyin': 'tàn suǒ', 'trans': 'explore'}, {'word': '当前', 'pinyin': 'dāng qián', 'trans': 'current'}, {'word': '由于', 'pinyin': 'yóu yú', 'trans': 'due to'}, {'word': '其', 'pinyin': 'qí', 'trans': 'its'}, {'word': '序列', 'pinyin': 'xù liè', 'trans': 'sequence'}, {'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'}, {'word': '无法', 'pinyin': 'wú fǎ', 'trans': 'unable to'}, {'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'process'}, {'word': '输入', 'pinyin': 'shū rù', 'trans': 'input'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '响应', 'pinyin': 'xiǎng yìng', 'trans': 'response'}, {'word': '因此', 'pinyin': 'yīn cǐ', 'trans': 'therefore'}, {'word': '项目', 'pinyin': 'xiàng mù', 'trans': 'project'}, {'word': '引入', 'pinyin': 'yǐn rù', 'trans': 'introduce'}, {'word': '分离', 'pinyin': 'fēn lí', 'trans': 'separate'}, {'word': '机制', 'pinyin': 'jī zhì', 'trans': 'mechanism'}, {'word': '使得', 'pinyin': 'shǐ dé', 'trans': 'enable'}, {'word': '实时', 'pinyin': 'shí shí', 'trans': 'real-time'}, {'word': '视频', 'pinyin': 'shì pín', 'trans': 'video'}, {'word': '音频', 'pinyin': 'yīn pín', 'trans': 'audio'}]
[13.12.2024 18:16] Renaming previous Chinese page.
[13.12.2024 18:16] Renaming previous data. zh.html to ./d/2024-12-12_zh_reading_task.html
[13.12.2024 18:16] Writing Chinese reading task.
[13.12.2024 18:16] Writing result.
[13.12.2024 18:16] Renaming log file.
[13.12.2024 18:16] Renaming previous data. log.txt to ./logs/2024-12-13_last_log.txt
