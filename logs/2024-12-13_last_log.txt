[13.12.2024 08:31] Read previous papers.
[13.12.2024 08:31] Generating top page (month).
[13.12.2024 08:31] Writing top page (month).
[13.12.2024 09:11] Read previous papers.
[13.12.2024 09:11] Get feed.
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09596
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08737
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08635
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09605
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09618
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08905
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09593
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05994
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09619
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08972
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09586
[13.12.2024 09:11] Extract page data from URL. URL: https://huggingface.co/papers/2412.09569
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09501
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09405
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09585
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09370
[13.12.2024 09:11] Extract page data from URL. URL: https://huggingface.co/papers/2412.09349
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09622
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09573
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05552
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09013
[13.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09025
[13.12.2024 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.12.2024 09:11] No deleted papers detected.
[13.12.2024 09:11] Downloading and parsing papers (pdf, html). Total: 22.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09596.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09596.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09596.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.08737.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.08737.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.08737.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.08635.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.08635.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.08635.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09605.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09605.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09605.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09618.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09618.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09618.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.08905.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.08905.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.08905.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09593.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09593.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09593.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.05994.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.05994.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.05994.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09619.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09619.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09619.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.08972.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.08972.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.08972.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09586.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09586.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09586.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09569.
[13.12.2024 09:11] Downloading paper 2412.09569 from http://arxiv.org/pdf/2412.09569v1...
[13.12.2024 09:11] Extracting affiliations from text.
[13.12.2024 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"JuStRank: Benchmarking LLM Judges for System Ranking Ariel Gera, Odellia Boni, Yotam Perlitz, Roy Bar-Haim, Lilach Eden and Asaf Yehudai IBM Research 4 2 0 2 2 1 ] . [ 1 9 6 5 9 0 . 2 1 4 2 : r a "
[13.12.2024 09:11] Response: ```python
["IBM Research"]
```
[13.12.2024 09:11] Deleting PDF ./assets/pdf/2412.09569.pdf.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09501.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09501.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09501.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09405.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09405.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09405.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09585.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09585.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09585.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09370.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09370.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09370.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09349.
[13.12.2024 09:11] Downloading paper 2412.09349 from http://arxiv.org/pdf/2412.09349v1...
[13.12.2024 09:11] Extracting affiliations from text.
[13.12.2024 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"4 2 0 2 2 1 ] . [ 1 9 4 3 9 0 . 2 1 4 2 : r a DISPOSE: DISENTANGLING POSE GUIDANCE FOR CONTROLLABLE HUMAN IMAGE ANIMATION Hongxiang Li1, Yaowei Li1, Yuhang Yang2, Junjie Cao3, Zhihong Zhu1, Xuxin Cheng1, Long Chen4 1Peking University 3Tsinghua University 2University of Science and Technology of China 4 Hong Kong University of Science and Technology Figure 1: Our method demonstrates its ability to produce diverse animations and preserve consistency of appearance. "
[13.12.2024 09:11] Response: ```python
["Peking University", "Tsinghua University", "University of Science and Technology of China", "Hong Kong University of Science and Technology"]
```
[13.12.2024 09:11] Deleting PDF ./assets/pdf/2412.09349.pdf.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09622.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09622.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09622.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09573.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09573.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09573.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.05552.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.05552.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.05552.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09013.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09013.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09013.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09025.
[13.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09025.json), skip PDF parsing.
[13.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09025.json), skip HTML parsing.
[13.12.2024 09:11] Success.
[13.12.2024 09:11] Enriching papers with extra data.
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 0. Creating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuou...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 1. Multimodal large language models (MLLMs) have made rapid progress in recent years, yet continue to struggle with low-level visual perception (LLVP) -- particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robot...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 2. Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 3. Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective t...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 4. Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among image...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 5. We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data ...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 6. Recovering the geometry and materials of objects from a single image is challenging due to its under-constrained nature. In this paper, we present Neural LightRig, a novel framework that boosts intrinsic estimation by leveraging auxiliary multi-lighting conditions from 2D diffusion priors. Specifica...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 7. The approximation of Partial Differential Equations (PDEs) using neural networks has seen significant advancements through Physics-Informed Neural Networks (PINNs). Despite their straightforward optimization framework and flexibility in implementing various PDEs, PINNs often suffer from limited accu...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 8. Existing text-to-image (T2I) diffusion models face several limitations, including large model sizes, slow runtime, and low-quality generation on mobile devices. This paper aims to address all of these challenges by developing an extremely small and fast T2I model that generates high-resolution and h...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 9. This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains -- airline baggage fees, NBA transactions, and tax regulations -- RuleArena assesses ...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 10. We address the problem of gaze target estimation, which aims to predict where a person is looking in a scene. Predicting a person's gaze target requires reasoning both about the person's appearance and the contents of the scene. Prior works have developed increasingly complex, hand-crafted pipelines...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 11. Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this ...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 12. As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond single-domain capabilities is essential to meet the demands for more versatile and efficient AI. However, previous omni-models have insufficiently explored speech, neglecting its integration with multi-modality. We introduce Lyra,...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 13. Modern sensors produce increasingly rich streams of high-resolution data. Due to resource constraints, machine learning systems discard the vast majority of this information via resolution reduction. Compressed-domain learning allows models to operate on compact latent representations, allowing high...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 14. The standard practice for developing contemporary MLLMs is to feed features from vision encoder(s) into the LLM and train with natural language supervision. In this work, we posit an overlooked opportunity to optimize the intermediate LLM representations through a vision perspective (objective), i.e...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 15. Word Sense Disambiguation (WSD) is the task of associating a word in a given context with its most suitable meaning among a set of possible candidates. While the task has recently witnessed renewed interest, with systems achieving performances above the estimated inter-annotator agreement, at the ti...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 16. Controllable human image animation aims to generate videos from reference images using driving videos. Due to the limited control signals provided by sparse guidance (e.g., skeleton pose), recent works have attempted to introduce additional dense conditions (e.g., depth map) to ensure motion alignme...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 17. Recent advances in text-to-image customization have enabled high-fidelity, context-rich generation of personalized images, allowing specific concepts to appear in a variety of scenarios. However, current methods struggle with combining multiple personalized models, often leading to attribute entangl...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 18. Existing sparse-view reconstruction models heavily rely on accurate known camera poses. However, deriving camera extrinsics and intrinsics from sparse-view images presents significant challenges. In this work, we present FreeSplatter, a highly scalable, feed-forward reconstruction framework capable ...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 19. The academic field of learning instruction-guided visual navigation can be generally categorized into high-level category-specific search and low-level language-guided navigation, depending on the granularity of language instruction, in which the former emphasizes the exploration process, while the ...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 20. This study presents a new image super-resolution (SR) technique based on diffusion inversion, aiming at harnessing the rich image priors encapsulated in large pre-trained diffusion models to improve SR performance. We design a Partial noise Prediction strategy to construct an intermediate state of t...
[13.12.2024 09:11] ********************************************************************************
[13.12.2024 09:11] Abstract 21. Neural Machine Translation (NMT) models are typically trained on datasets with limited exposure to Scientific, Technical and Educational domains. Translation models thus, in general, struggle with tasks that involve scientific understanding or technical jargon. Their performance is found to be even ...
[13.12.2024 09:11] Read previous papers.
[13.12.2024 09:11] Generating reviews via LLM API.
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#long_context", "#audio", "#cv", "#multimodal", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –Ω–æ–≤—ã–π —à–∞–≥ –∫ —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω–æ–º—É –ò–ò", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∫ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–º
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#low_resource", "#synthetic", "#multimodal", "#optimization", "#training", "#data"], "emoji": "üìê", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Geoperception –¥–ª—è –æ—Ü–µ–Ω–∫–∏
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#audio", "#video", "#cv", "#multimodal", "#training"], "emoji": "üß†", "ru": {"title": "LatentLM: –û–±—ä–µ–¥–∏–Ω—è—è –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–µ –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Latent Language Modeling (LatentLM) - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#data", "#dataset", "#training", "#optimization", "#agents", "#synthetic"], "emoji": "ü§ñ", "ru": {"title": "AgentTrek: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ GUI-–∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤–µ–±-—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AgentTrek - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#multimodal", "#diffusion", "#optimization", "#training"], "emoji": "üñºÔ∏è", "ru": {"title": "EasyRef: —É–º–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EasyRef - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ–∑–≤–æ–ª—è
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#data", "#reasoning", "#synthetic", "#training", "#benchmark", "#architecture"], "emoji": "üß†", "ru": {"title": "–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö - –∫–ª—é—á –∫ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤—É —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç phi-4 - —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å —Å 14 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –∫–∞—á–µ
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#diffusion", "#synthetic", "#open_source", "#cv", "#3d", "#dataset"], "emoji": "üí°", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ 3D-–≥–µ–æ–º–µ—Ç—Ä–∏–∏ —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Å–≤–µ—â–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Neural LightRig - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–≤–æ–π—Å—Ç–≤ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –ú–µ
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#optimization", "#math", "#architecture"], "emoji": "üìä", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≥–∞—É—Å—Å–æ–≤—ã —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É—Ä–∞–≤–Ω–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É—Ä–∞–≤–Ω–µ–Ω–∏–π –≤ —á–∞—Å—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö (–î–£–ß–ü) —Å –∏—Å–ø–æ–ª—å–∑
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#architecture", "#small_models", "#training", "#optimization", "#diffusion", "#benchmark"], "emoji": "üì±", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É, SnapGen, —Ä–∞–∑—Ä–∞–±
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#long_context", "#math", "#benchmark", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å–ø–æ—Ç—ã–∫–∞—é—Ç—Å—è –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª–∞—Ö —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞", "desc": "RuleArena - —ç—Ç–æ –Ω–æ–≤—ã–π —Å–ª–æ–∂–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–ª–µ–¥–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–º –ø—Ä–∞–≤–∏–ª–∞–º –≤ 
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#cv", "#reasoning"], "emoji": "üëÄ", "ru": {"title": "–ü—Ä–æ—Å—Ç–æ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∑–≥–ª—è–¥–∞ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Gaze-LLE - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∑–≥–ª—è–¥–∞ —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω
[13.12.2024 09:11] Querying the API.
[13.12.2024 09:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge's positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judge's quality is assessed by comparing the resulting system ranking to a human-based ranking. Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias.
[13.12.2024 09:11] Response: {
  "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –æ—Ü–µ–Ω–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —Ä–æ–ª–∏ —Å—É–¥–µ–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ –≤—ã–±–æ—Ä–∞ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –ò–ò-—Å–∏—Å—Ç–µ–º–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ LLM-—Å—É–¥–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º, –∞ –Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –û–Ω–∏ –ø—Ä–æ–≤–æ–¥—è—Ç –º–∞—Å—à—Ç–∞–±–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, —Å—Ä–∞–≤–Ω–∏–≤–∞—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º LLM-—Å—É–¥—å—è–º–∏ —Å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º –Ω–∞ –æ—Ü–µ–Ω–∫–∞—Ö –ª—é–¥–µ–π. –ê–Ω–∞–ª–∏–∑ –≤–∫–ª—é—á–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –ø–æ–≤–µ–¥–µ–Ω–∏—è —Å—É–¥–µ–π, –≤–∫–ª—é—á–∞—è –∏—Ö —Ä–µ—à–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å.",
  "emoji": "‚öñÔ∏è",
  "title": "LLM –∫–∞–∫ –±–µ—Å–ø—Ä–∏—Å—Ç—Ä–∞—Å—Ç–Ω—ã–µ —Å—É–¥—å–∏ –ò–ò-—Å–∏—Å—Ç–µ–º"
}
[13.12.2024 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge's positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judge's quality is assessed by comparing the resulting system ranking to a human-based ranking. Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias."

[13.12.2024 09:11] Response: ```python
["BENCHMARK", "RLHF"]
```
[13.12.2024 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge's positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judge's quality is assessed by comparing the resulting system ranking to a human-based ranking. Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias."

[13.12.2024 09:11] Response: ```python
['INTERPRETABILITY', 'ALIGNMENT']
```
[13.12.2024 09:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenge of evaluating generative AI models by proposing the use of large language model (LLM) judges for systematic comparisons. It highlights the importance of validating the quality of these LLM judges, as previous methods have only assessed them based on individual responses without considering their biases towards different systems. The authors conduct a large-scale study to evaluate LLM judges as system rankers, comparing their rankings to those made by humans. Additionally, the study provides insights into the behavior of LLM judges, including their decisiveness and potential biases, which are crucial for accurate model evaluation.","title":"Evaluating AI Models with LLM Judges: A Systematic Approach"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper addresses the challenge of evaluating generative AI models by proposing the use of large language model (LLM) judges for systematic comparisons. It highlights the importance of validating the quality of these LLM judges, as previous methods have only assessed them based on individual responses without considering their biases towards different systems. The authors conduct a large-scale study to evaluate LLM judges as system rankers, comparing their rankings to those made by humans. Additionally, the study provides insights into the behavior of LLM judges, including their decisiveness and potential biases, which are crucial for accurate model evaluation.', title='Evaluating AI Models with LLM Judges: A Systematic Approach'))
[13.12.2024 09:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ÈöèÁùÄÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÁöÑÂø´ÈÄüÂèëÂ±ïÔºåÁ≥ªÁªüÊØîËæÉÂíåÈÄâÊã©‰ºóÂ§öÊ®°ÂûãÂíåÈÖçÁΩÆÁöÑÈúÄÊ±ÇÊó•ÁõäËø´Âàá„ÄÇÊú¨ÊñáÊèêÂá∫‰ΩøÁî®Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑËØÑ‰º∞ËÄÖÊù•Ëß£ÂÜ≥Ëøô‰∏ÄÊåëÊàòÔºå‰ΩÜÈ¶ñÂÖàÈúÄË¶ÅÈ™åËØÅLLMËØÑ‰º∞ËÄÖÁöÑË¥®Èáè„ÄÇ‰ª•ÂæÄÁöÑÁ†îÁ©∂‰∏ªË¶ÅÂÖ≥Ê≥®‰∫éÂØπLLMËØÑ‰º∞ËÄÖÁöÑÂÆû‰æãËØÑ‰º∞ÔºåËÄåÂøΩËßÜ‰∫ÜÂΩ±ÂìçÁ≥ªÁªüÁ∫ßÊéíÂêçÁöÑÈáçË¶ÅÂõ†Á¥†ÔºåÂ¶ÇËØÑ‰º∞ËÄÖÂØπÊüê‰∫õÁ≥ªÁªüÁöÑÂÅèËßÅ„ÄÇÊàë‰ª¨ËøõË°å‰∫Ü‰∏ÄÈ°πÂ§ßËßÑÊ®°Á†îÁ©∂ÔºåËØÑ‰º∞LLMËØÑ‰º∞ËÄÖ‰Ωú‰∏∫Á≥ªÁªüÊéíÂêçËÄÖÁöÑË°®Áé∞ÔºåÂπ∂ÈÄöËøá‰∏é‰∫∫Á±ªÊéíÂêçÁöÑÊØîËæÉÊù•ËØÑ‰º∞ÂÖ∂Ë¥®Èáè„ÄÇ","title":"Á≥ªÁªüÊéíÂêçÁöÑÊñ∞ËßÜËßíÔºöËØÑ‰º∞LLMÁöÑË¥®Èáè‰∏éÂÅèËßÅ"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='ÈöèÁùÄÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÁöÑÂø´ÈÄüÂèëÂ±ïÔºåÁ≥ªÁªüÊØîËæÉÂíåÈÄâÊã©‰ºóÂ§öÊ®°ÂûãÂíåÈÖçÁΩÆÁöÑÈúÄÊ±ÇÊó•ÁõäËø´Âàá„ÄÇÊú¨ÊñáÊèêÂá∫‰ΩøÁî®Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑËØÑ‰º∞ËÄÖÊù•Ëß£ÂÜ≥Ëøô‰∏ÄÊåëÊàòÔºå‰ΩÜÈ¶ñÂÖàÈúÄË¶ÅÈ™åËØÅLLMËØÑ‰º∞ËÄÖÁöÑË¥®Èáè„ÄÇ‰ª•ÂæÄÁöÑÁ†îÁ©∂‰∏ªË¶ÅÂÖ≥Ê≥®‰∫éÂØπLLMËØÑ‰º∞ËÄÖÁöÑÂÆû‰æãËØÑ‰º∞ÔºåËÄåÂøΩËßÜ‰∫ÜÂΩ±ÂìçÁ≥ªÁªüÁ∫ßÊéíÂêçÁöÑÈáçË¶ÅÂõ†Á¥†ÔºåÂ¶ÇËØÑ‰º∞ËÄÖÂØπÊüê‰∫õÁ≥ªÁªüÁöÑÂÅèËßÅ„ÄÇÊàë‰ª¨ËøõË°å‰∫Ü‰∏ÄÈ°πÂ§ßËßÑÊ®°Á†îÁ©∂ÔºåËØÑ‰º∞LLMËØÑ‰º∞ËÄÖ‰Ωú‰∏∫Á≥ªÁªüÊéíÂêçËÄÖÁöÑË°®Áé∞ÔºåÂπ∂ÈÄöËøá‰∏é‰∫∫Á±ªÊéíÂêçÁöÑÊØîËæÉÊù•ËØÑ‰º∞ÂÖ∂Ë¥®Èáè„ÄÇ', title='Á≥ªÁªüÊéíÂêçÁöÑÊñ∞ËßÜËßíÔºöËØÑ‰º∞LLMÁöÑË¥®Èáè‰∏éÂÅèËßÅ'))
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#long_context", "#multimodal", "#benchmark"], "emoji": "üé≠", "ru": {"title": "Lyra: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ—á–∏ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Lyra - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (MLLM), –∫–æ—Ç–æ—Ä–∞—è —É
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#architecture", "#inference", "#synthetic", "#dataset", "#optimization", "#multimodal"], "emoji": "üóúÔ∏è", "ru": {"title": "WaLLoC: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç WaLLoC - –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–µ–∫–∞ –¥–ª—è —Å–∂
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#open_source", "#cv", "#multimodal"], "emoji": "üëÅÔ∏è", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –∑—Ä–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM)
[13.12.2024 09:11] Using data from previous issue: {"categories": ["#multilingual", "#reasoning", "#dataset", "#architecture"], "emoji": "üîó", "ru": {"title": "–û—Ç WSD –∫ WSL: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–º—ã—Å–ª–∞ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Word Sense Linking (WSL), –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—É—é –∑–∞–¥–∞—á—É Word Sense
[13.12.2024 09:11] Querying the API.
[13.12.2024 09:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Controllable human image animation aims to generate videos from reference images using driving videos. Due to the limited control signals provided by sparse guidance (e.g., skeleton pose), recent works have attempted to introduce additional dense conditions (e.g., depth map) to ensure motion alignment. However, such strict dense guidance impairs the quality of the generated video when the body shape of the reference character differs significantly from that of the driving video. In this paper, we present DisPose to mine more generalizable and effective control signals without additional dense input, which disentangles the sparse skeleton pose in human image animation into motion field guidance and keypoint correspondence. Specifically, we generate a dense motion field from a sparse motion field and the reference image, which provides region-level dense guidance while maintaining the generalization of the sparse pose control. We also extract diffusion features corresponding to pose keypoints from the reference image, and then these point features are transferred to the target pose to provide distinct identity information. To seamlessly integrate into existing models, we propose a plug-and-play hybrid ControlNet that improves the quality and consistency of generated videos while freezing the existing model parameters. Extensive qualitative and quantitative experiments demonstrate the superiority of DisPose compared to current methods. Code: https://github.com/lihxxx/DisPose{https://github.com/lihxxx/DisPose}.
[13.12.2024 09:12] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DisPose - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Å–∫–µ–ª–µ—Ç–Ω—É—é –ø–æ–∑—É –Ω–∞ –ø–æ–ª–µ –¥–≤–∏–∂–µ–Ω–∏—è –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–ª–æ—Ç–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. DisPose –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–ª–æ—Ç–Ω–æ–≥–æ –ø–æ–ª—è –¥–≤–∏–∂–µ–Ω–∏—è –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ ControlNet, —É–ª—É—á—à–∞—è –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –≤–∏–¥–µ–æ.",
  "emoji": "üé≠",
  "title": "DisPose: –£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –∞–Ω–∏–º–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
}
[13.12.2024 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Controllable human image animation aims to generate videos from reference images using driving videos. Due to the limited control signals provided by sparse guidance (e.g., skeleton pose), recent works have attempted to introduce additional dense conditions (e.g., depth map) to ensure motion alignment. However, such strict dense guidance impairs the quality of the generated video when the body shape of the reference character differs significantly from that of the driving video. In this paper, we present DisPose to mine more generalizable and effective control signals without additional dense input, which disentangles the sparse skeleton pose in human image animation into motion field guidance and keypoint correspondence. Specifically, we generate a dense motion field from a sparse motion field and the reference image, which provides region-level dense guidance while maintaining the generalization of the sparse pose control. We also extract diffusion features corresponding to pose keypoints from the reference image, and then these point features are transferred to the target pose to provide distinct identity information. To seamlessly integrate into existing models, we propose a plug-and-play hybrid ControlNet that improves the quality and consistency of generated videos while freezing the existing model parameters. Extensive qualitative and quantitative experiments demonstrate the superiority of DisPose compared to current methods. Code: https://github.com/lihxxx/DisPose{https://github.com/lihxxx/DisPose}."

[13.12.2024 09:12] Response: ```python
['VIDEO', 'CV', 'MULTIMODAL']
```
[13.12.2024 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Controllable human image animation aims to generate videos from reference images using driving videos. Due to the limited control signals provided by sparse guidance (e.g., skeleton pose), recent works have attempted to introduce additional dense conditions (e.g., depth map) to ensure motion alignment. However, such strict dense guidance impairs the quality of the generated video when the body shape of the reference character differs significantly from that of the driving video. In this paper, we present DisPose to mine more generalizable and effective control signals without additional dense input, which disentangles the sparse skeleton pose in human image animation into motion field guidance and keypoint correspondence. Specifically, we generate a dense motion field from a sparse motion field and the reference image, which provides region-level dense guidance while maintaining the generalization of the sparse pose control. We also extract diffusion features corresponding to pose keypoints from the reference image, and then these point features are transferred to the target pose to provide distinct identity information. To seamlessly integrate into existing models, we propose a plug-and-play hybrid ControlNet that improves the quality and consistency of generated videos while freezing the existing model parameters. Extensive qualitative and quantitative experiments demonstrate the superiority of DisPose compared to current methods. Code: https://github.com/lihxxx/DisPose{https://github.com/lihxxx/DisPose}."

[13.12.2024 09:12] Response: ```python
[]
```
[13.12.2024 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces DisPose, a method for controllable human image animation that enhances video generation from reference images using driving videos. It addresses the limitations of sparse guidance by creating a dense motion field from a sparse skeleton pose, allowing for better motion alignment without requiring additional dense inputs. DisPose also extracts and transfers diffusion features from reference images to target poses, ensuring distinct identity representation. The proposed hybrid ControlNet integrates seamlessly with existing models, improving video quality and consistency while keeping model parameters unchanged.","title":"Enhancing Human Animation with DisPose: More Control, Less Complexity!"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='This paper introduces DisPose, a method for controllable human image animation that enhances video generation from reference images using driving videos. It addresses the limitations of sparse guidance by creating a dense motion field from a sparse skeleton pose, allowing for better motion alignment without requiring additional dense inputs. DisPose also extracts and transfers diffusion features from reference images to target poses, ensuring distinct identity representation. The proposed hybrid ControlNet integrates seamlessly with existing models, improving video quality and consistency while keeping model parameters unchanged.', title='Enhancing Human Animation with DisPose: More Control, Less Complexity!'))
[13.12.2024 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫DisPoseÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÂú®‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÂØÜÈõÜËæìÂÖ•ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊåñÊéòÊõ¥ÂÖ∑ÊôÆÈÅçÊÄßÂíåÊúâÊïàÊÄßÁöÑÊéßÂà∂‰ø°Âè∑„ÄÇÊàë‰ª¨Â∞ÜÁ®ÄÁñèÁöÑÈ™®Êû∂ÂßøÂäøÂàÜËß£‰∏∫ËøêÂä®Âú∫ÂºïÂØºÂíåÂÖ≥ÈîÆÁÇπÂØπÂ∫îÔºå‰ªéËÄåÁîüÊàêÂØÜÈõÜÁöÑËøêÂä®Âú∫Ôºå‰ª•Êèê‰æõÂå∫ÂüüÁ∫ßÁöÑÂØÜÈõÜÊåáÂØºÔºåÂêåÊó∂‰øùÊåÅÁ®ÄÁñèÂßøÂäøÊéßÂà∂ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄöËøáÊèêÂèñ‰∏éÂßøÂäøÂÖ≥ÈîÆÁÇπÂØπÂ∫îÁöÑÊâ©Êï£ÁâπÂæÅÔºåÂπ∂Â∞ÜËøô‰∫õÁâπÂæÅËΩ¨ÁßªÂà∞ÁõÆÊ†áÂßøÂäø‰∏äÔºåÊàë‰ª¨‰∏∫ÁîüÊàêÁöÑËßÜÈ¢ëÊèê‰æõ‰∫ÜÁã¨ÁâπÁöÑË∫´‰ªΩ‰ø°ÊÅØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDisPoseÂú®ËßÜÈ¢ëÁîüÊàêÁöÑË¥®ÈáèÂíå‰∏ÄËá¥ÊÄßÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ","title":"Êó†ÂØÜÈõÜËæìÂÖ•ÁöÑÂèØÊéß‰∫∫ÂÉèÂä®Áîª"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫DisPoseÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÂú®‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÂØÜÈõÜËæìÂÖ•ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊåñÊéòÊõ¥ÂÖ∑ÊôÆÈÅçÊÄßÂíåÊúâÊïàÊÄßÁöÑÊéßÂà∂‰ø°Âè∑„ÄÇÊàë‰ª¨Â∞ÜÁ®ÄÁñèÁöÑÈ™®Êû∂ÂßøÂäøÂàÜËß£‰∏∫ËøêÂä®Âú∫ÂºïÂØºÂíåÂÖ≥ÈîÆÁÇπÂØπÂ∫îÔºå‰ªéËÄåÁîüÊàêÂØÜÈõÜÁöÑËøêÂä®Âú∫Ôºå‰ª•Êèê‰æõÂå∫ÂüüÁ∫ßÁöÑÂØÜÈõÜÊåáÂØºÔºåÂêåÊó∂‰øùÊåÅÁ®ÄÁñèÂßøÂäøÊéßÂà∂ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄöËøáÊèêÂèñ‰∏éÂßøÂäøÂÖ≥ÈîÆÁÇπÂØπÂ∫îÁöÑÊâ©Êï£ÁâπÂæÅÔºåÂπ∂Â∞ÜËøô‰∫õÁâπÂæÅËΩ¨ÁßªÂà∞ÁõÆÊ†áÂßøÂäø‰∏äÔºåÊàë‰ª¨‰∏∫ÁîüÊàêÁöÑËßÜÈ¢ëÊèê‰æõ‰∫ÜÁã¨ÁâπÁöÑË∫´‰ªΩ‰ø°ÊÅØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDisPoseÂú®ËßÜÈ¢ëÁîüÊàêÁöÑË¥®ÈáèÂíå‰∏ÄËá¥ÊÄßÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ', title='Êó†ÂØÜÈõÜËæìÂÖ•ÁöÑÂèØÊéß‰∫∫ÂÉèÂä®Áîª'))
[13.12.2024 09:12] Using data from previous issue: {"categories": ["#training", "#multimodal", "#cv"], "emoji": "üé®", "ru": {"title": "LoRACLR: –ì–∞—Ä–º–æ–Ω–∏—á–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "LoRACLR - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π LoRA –±–µ–∑ 
[13.12.2024 09:12] Using data from previous issue: {"categories": ["#architecture", "#3d"], "emoji": "üîç", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏: –æ—Ç –Ω–µ–æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫ —Ç–æ—á–Ω—ã–º –º–æ–¥–µ–ª—è–º", "desc": "FreeSplatter - —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∞—è —Å –Ω–µ–æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —Å —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –û–Ω–∞ –∏
[13.12.2024 09:12] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#agents", "#optimization"], "emoji": "üß≠", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤–∏–∑—É–∞–ª—å–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
[13.12.2024 09:12] Using data from previous issue: {"categories": ["#cv", "#open_source", "#dataset", "#architecture", "#diffusion"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Å–≤–µ—Ä—Ö—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω–≤–µ—Ä—Å–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Ç–µ—Ö–Ω–∏–∫—É —Å–≤–µ—Ä—Ö—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –∏–Ω–≤–µ—Ä—Å–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏. –ú–µ—Ç
[13.12.2024 09:12] Using data from previous issue: {"categories": ["#benchmark", "#multilingual", "#low_resource", "#machine_translation", "#dataset", "#open_source", "#training"], "emoji": "üåè", "ru": {"title": "–ù–æ–≤—ã–π –∫–æ—Ä–ø—É—Å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∫–æ—Ä–ø—É—Å –¥–ª
[13.12.2024 09:12] Trying to get texts in Chinese.
[13.12.2024 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Creating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuous and simultaneous streaming perception, memory, and reasoning remains largely unexplored. Current MLLMs are constrained by their sequence-to-sequence architecture, which limits their ability to process inputs and generate responses simultaneously, akin to being unable to think while perceiving. Furthermore, relying on long contexts to store historical data is impractical for long-term interactions, as retaining all information becomes costly and inefficient. Therefore, rather than relying on a single foundation model to perform all functions, this project draws inspiration from the concept of the Specialized Generalist AI and introduces disentangled streaming perception, reasoning, and memory mechanisms, enabling real-time interaction with streaming video and audio input. The proposed framework InternLM-XComposer2.5-OmniLive (IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module: Processes multimodal information in real-time, storing key details in memory and triggering reasoning in response to user queries. (2) Multi-modal Long Memory Module: Integrates short-term and long-term memory, compressing short-term memories into long-term ones for efficient retrieval and improved accuracy. (3) Reasoning Module: Responds to queries and executes reasoning tasks, coordinating with the perception and memory modules. This project simulates human-like cognition, enabling multimodal large language models to provide continuous and adaptive service over time.
[13.12.2024 09:12] Mistral response. {"id": "2c4da2028f694f0ea549a2b06f2fde40", "object": "chat.completion", "created": 1734081132, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u521b\u5efa\u80fd\u591f\u957f\u65f6\u95f4\u4e0e\u73af\u5883\u4e92\u52a8\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u7814\u7a76\u76ee\u6807\u3002\u6700\u8fd1\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5f00\u653e\u4e16\u754c\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u540c\u65f6\u8fdb\u884c\u6301\u7eed\u7684\u611f\u77e5\u3001\u8bb0\u5fc6\u548c\u63a8\u7406\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\u3002\u5f53\u524d\u7684MLLMs\u7531\u4e8e\u5176\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u67b6\u6784\uff0c\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u8f93\u5165\u548c\u751f\u6210\u54cd\u5e94\u3002\u56e0\u6b64\uff0c\u8fd9\u4e2a\u9879\u76ee\u5f15\u5165\u4e86\u5206\u79bb\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u8bb0\u5fc6\u673a\u5236\uff0c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u5b9e\u65f6\u5904\u7406\u89c6\u9891\u548c\u97f3\u9891\u8f93\u5165\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 400, "total_tokens": 581, "completion_tokens": 181}}
[13.12.2024 09:12] Response: ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÂàõÂª∫ËÉΩÂ§üÈïøÊó∂Èó¥‰∏éÁéØÂ¢É‰∫íÂä®ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÁöÑÁ†îÁ©∂ÁõÆÊ†á„ÄÇÊúÄËøëÔºåÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÂºÄÊîæ‰∏ñÁïåÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåÂêåÊó∂ËøõË°åÊåÅÁª≠ÁöÑÊÑüÁü•„ÄÅËÆ∞ÂøÜÂíåÊé®ÁêÜ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Êú™ÂÖÖÂàÜÊé¢Á¥¢ÁöÑÊåëÊàò„ÄÇÂΩìÂâçÁöÑMLLMsÁî±‰∫éÂÖ∂Â∫èÂàóÂà∞Â∫èÂàóÁöÑÊû∂ÊûÑÔºåÊó†Ê≥ïÂêåÊó∂Â§ÑÁêÜËæìÂÖ•ÂíåÁîüÊàêÂìçÂ∫î„ÄÇÂõ†Ê≠§ÔºåËøô‰∏™È°πÁõÆÂºïÂÖ•‰∫ÜÂàÜÁ¶ªÁöÑÊÑüÁü•„ÄÅÊé®ÁêÜÂíåËÆ∞ÂøÜÊú∫Âà∂Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂÆûÊó∂Â§ÑÁêÜËßÜÈ¢ëÂíåÈü≥È¢ëËæìÂÖ•„ÄÇ
[13.12.2024 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÂàõÂª∫ËÉΩÂ§üÈïøÊó∂Èó¥‰∏éÁéØÂ¢É‰∫íÂä®ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÁöÑÁ†îÁ©∂ÁõÆÊ†á„ÄÇÊúÄËøëÔºåÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÂºÄÊîæ‰∏ñÁïåÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåÂêåÊó∂ËøõË°åÊåÅÁª≠ÁöÑÊÑüÁü•„ÄÅËÆ∞ÂøÜÂíåÊé®ÁêÜ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Êú™ÂÖÖÂàÜÊé¢Á¥¢ÁöÑÊåëÊàò„ÄÇÂΩìÂâçÁöÑMLLMsÁî±‰∫éÂÖ∂Â∫èÂàóÂà∞Â∫èÂàóÁöÑÊû∂ÊûÑÔºåÊó†Ê≥ïÂêåÊó∂Â§ÑÁêÜËæìÂÖ•ÂíåÁîüÊàêÂìçÂ∫î„ÄÇÂõ†Ê≠§ÔºåËøô‰∏™È°πÁõÆÂºïÂÖ•‰∫ÜÂàÜÁ¶ªÁöÑÊÑüÁü•„ÄÅÊé®ÁêÜÂíåËÆ∞ÂøÜÊú∫Âà∂Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂÆûÊó∂Â§ÑÁêÜËßÜÈ¢ëÂíåÈü≥È¢ëËæìÂÖ•„ÄÇ
[13.12.2024 09:12] Mistral response. {"id": "f28a6cd2962c4d7aa8ed19191721872a", "object": "chat.completion", "created": 1734081136, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u521b\u5efa\u80fd\u591f\u957f\u65f6\u95f4\u4e0e\u73af\u5883\u4e92\u52a8\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u7814\u7a76\u76ee\u6807\u3002\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng t\u01ceol\u00f9n le chu\u00e0ngji\u00e0n n\u00e9ngg\u00f2u ch\u00e1ng sh\u00edji\u0101n y\u01d4 hu\u00e1nj\u00ecng h\u00f9d\u00f2ng de r\u00e9ng\u014dng zh\u00ecn\u00e9ng x\u00ect\u01d2ng de y\u00e1nji\u016b m\u00f9bi\u0101o.\n\n\u6700\u8fd1\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5f00\u653e\u4e16\u754c\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\nZu\u00ecj\u00ecn, du\u014d m\u00f3shu\u00e0i d\u00e0 y\u01d4y\u00e1n m\u00f3x\u00edng (MLLMs) z\u00e0i k\u0101if\u00e0ng sh\u00ecji\u00e8 l\u01d0ji\u011b f\u0101ngmi\u00e0n q\u01d4d\u00e9 le xi\u01cenzh\u00f9 j\u00ecnzh\u01cen.\n\n\u7136\u800c\uff0c\u540c\u65f6\u8fdb\u884c\u6301\u7eed\u7684\u611f\u77e5\u3001\u8bb0\u5fc6\u548c\u63a8\u7406\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\u3002\nR\u00e1n'\u00e9r, t\u00f3ngsh\u00ed j\u00ecnx\u00edng ch\u00edx\u00f9 de g\u01cenzhi, j\u00ecy\u00ec h\u00e9 tu\u012bl\u01d0 r\u00e9ngr\u00e1n sh\u00ec y\u012bg\u00e8 w\u00e8i ch\u014dngf\u0113n t\u00e0nsu\u01d2 de ti\u01ceozh\u00e0n.\n\n\u5f53\u524d\u7684MLLMs\u7531\u4e8e\u5176\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u67b6\u6784\uff0c\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u8f93\u5165\u548c\u751f\u6210\u54cd\u5e94\u3002\nD\u0101ngqi\u00e1n de MLLMs y\u00f3uy\u00fa q\u00ed x\u00f9li\u00e8 d\u00e0o x\u00f9li\u00e8 de ji\u00e0g\u00f2u, w\u00faf\u01ce t\u00f3ngsh\u00ed ch\u01d4l\u01d0 sh\u016br\u00f9 h\u00e9 sh\u0113ngch\u00e9ng xi\u01cengy\u00ecng.\n\n\u56e0\u6b64\uff0c\u8fd9\u4e2a\u9879\u76ee\u5f15\u5165\u4e86\u5206\u79bb\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u8bb0\u5fc6\u673a\u5236\uff0c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u5b9e\u65f6\u5904\u7406\u89c6\u9891\u548c\u97f3\u9891\u8f93\u5165\u3002\nY\u012bnc\u01d0, zh\u00e8g\u00e8 xi\u00e0ngm\u00f9 y\u01d0nr\u00f9 le f\u0113nl\u00ed de g\u01cenzhi, tu\u012bl\u01d0 h\u00e9 j\u00ecy\u00ec j\u012bzh\u00ec, sh\u01d0d\u00e9 m\u00f3x\u00edng n\u00e9ngg\u00f2u sh\u00edsh\u00ed ch\u01d4l\u01d0 sh\u00ecp\u00edn h\u00e9 y\u012bnp\u00edn sh\u016br\u00f9."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 196, "total_tokens": 755, "completion_tokens": 559}}
[13.12.2024 09:12] Response: ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÂàõÂª∫ËÉΩÂ§üÈïøÊó∂Èó¥‰∏éÁéØÂ¢É‰∫íÂä®ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÁöÑÁ†îÁ©∂ÁõÆÊ†á„ÄÇ
Zh√® piƒÅn w√©nzhƒÅng t«éol√πn le chu√†ngji√†n n√©ngg√≤u ch√°ng sh√≠jiƒÅn y«î hu√°nj√¨ng h√πd√≤ng de r√©ng≈çng zh√¨n√©ng x√¨t«íng de y√°nji≈´ m√πbiƒÅo.

ÊúÄËøëÔºåÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÂºÄÊîæ‰∏ñÁïåÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇ
Zu√¨j√¨n, du≈ç m√≥shu√†i d√† y«îy√°n m√≥x√≠ng (MLLMs) z√†i kƒÅif√†ng sh√¨ji√® l«êjiƒõ fƒÅngmi√†n q«îd√© le xi«énzh√π j√¨nzh«én.

ÁÑ∂ËÄåÔºåÂêåÊó∂ËøõË°åÊåÅÁª≠ÁöÑÊÑüÁü•„ÄÅËÆ∞ÂøÜÂíåÊé®ÁêÜ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Êú™ÂÖÖÂàÜÊé¢Á¥¢ÁöÑÊåëÊàò„ÄÇ
R√°n'√©r, t√≥ngsh√≠ j√¨nx√≠ng ch√≠x√π de g«énzhi, j√¨y√¨ h√© tuƒ´l«ê r√©ngr√°n sh√¨ yƒ´g√® w√®i ch≈çngfƒìn t√†nsu«í de ti«éozh√†n.

ÂΩìÂâçÁöÑMLLMsÁî±‰∫éÂÖ∂Â∫èÂàóÂà∞Â∫èÂàóÁöÑÊû∂ÊûÑÔºåÊó†Ê≥ïÂêåÊó∂Â§ÑÁêÜËæìÂÖ•ÂíåÁîüÊàêÂìçÂ∫î„ÄÇ
DƒÅngqi√°n de MLLMs y√≥uy√∫ q√≠ x√πli√® d√†o x√πli√® de ji√†g√≤u, w√∫f«é t√≥ngsh√≠ ch«îl«ê sh≈´r√π h√© shƒìngch√©ng xi«éngy√¨ng.

Âõ†Ê≠§ÔºåËøô‰∏™È°πÁõÆÂºïÂÖ•‰∫ÜÂàÜÁ¶ªÁöÑÊÑüÁü•„ÄÅÊé®ÁêÜÂíåËÆ∞ÂøÜÊú∫Âà∂Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂÆûÊó∂Â§ÑÁêÜËßÜÈ¢ëÂíåÈü≥È¢ëËæìÂÖ•„ÄÇ
Yƒ´nc«ê, zh√®g√® xi√†ngm√π y«ênr√π le fƒìnl√≠ de g«énzhi, tuƒ´l«ê h√© j√¨y√¨ jƒ´zh√¨, sh«êd√© m√≥x√≠ng n√©ngg√≤u sh√≠sh√≠ ch«îl«ê sh√¨p√≠n h√© yƒ´np√≠n sh≈´r√π.
[13.12.2024 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÂàõÂª∫ËÉΩÂ§üÈïøÊó∂Èó¥‰∏éÁéØÂ¢É‰∫íÂä®ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÁöÑÁ†îÁ©∂ÁõÆÊ†á„ÄÇÊúÄËøëÔºåÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÂºÄÊîæ‰∏ñÁïåÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåÂêåÊó∂ËøõË°åÊåÅÁª≠ÁöÑÊÑüÁü•„ÄÅËÆ∞ÂøÜÂíåÊé®ÁêÜ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Êú™ÂÖÖÂàÜÊé¢Á¥¢ÁöÑÊåëÊàò„ÄÇÂΩìÂâçÁöÑMLLMsÁî±‰∫éÂÖ∂Â∫èÂàóÂà∞Â∫èÂàóÁöÑÊû∂ÊûÑÔºåÊó†Ê≥ïÂêåÊó∂Â§ÑÁêÜËæìÂÖ•ÂíåÁîüÊàêÂìçÂ∫î„ÄÇÂõ†Ê≠§ÔºåËøô‰∏™È°πÁõÆÂºïÂÖ•‰∫ÜÂàÜÁ¶ªÁöÑÊÑüÁü•„ÄÅÊé®ÁêÜÂíåËÆ∞ÂøÜÊú∫Âà∂Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂÆûÊó∂Â§ÑÁêÜËßÜÈ¢ëÂíåÈü≥È¢ëËæìÂÖ•„ÄÇ
[13.12.2024 09:12] Mistral response. {"id": "1cfabef4f7984f659d19afd56a14b85a", "object": "chat.completion", "created": 1734081149, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u8ba8\u8bba', 'pinyin': 't\u01ceo l\u00f9n', 'trans': 'discuss'},\n{'word': '\u521b\u5efa', 'pinyin': 'chu\u00e0ng ji\u00e0n', 'trans': 'create'},\n{'word': '\u80fd\u591f', 'pinyin': 'n\u00e9ng g\u00f2u', 'trans': 'be able to'},\n{'word': '\u957f\u65f6\u95f4', 'pinyin': 'ch\u00e1ng sh\u00ed ji\u0101n', 'trans': 'long period of time'},\n{'word': '\u4e92\u52a8', 'pinyin': 'h\u00f9 d\u00f2ng', 'trans': 'interact'},\n{'word': '\u4eba\u5de5\u667a\u80fd', 'pinyin': 'r\u00e9n g\u014dng zh\u00ec n\u00e9ng', 'trans': 'artificial intelligence'},\n{'word': '\u7cfb\u7edf', 'pinyin': 'x\u00ec t\u01d2ng', 'trans': 'system'},\n{'word': '\u7814\u7a76', 'pinyin': 'y\u00e1n ji\u016b', 'trans': 'research'},\n{'word': '\u76ee\u6807', 'pinyin': 'm\u00f9 bi\u0101o', 'trans': 'goal'},\n{'word': '\u591a\u6a21\u6001', 'pinyin': 'du\u014d m\u00f3 t\u00e0i', 'trans': 'multimodal'},\n{'word': '\u5927\u8bed\u8a00\u6a21\u578b', 'pinyin': 'd\u00e0 y\u01d4 y\u00e1n m\u00f3 x\u00edng', 'trans': 'large language model'},\n{'word': '\u5f00\u653e', 'pinyin': 'k\u0101i f\u00e0ng', 'trans': 'open'},\n{'word': '\u4e16\u754c', 'pinyin': 'sh\u00ec ji\u00e8', 'trans': 'world'},\n{'word': '\u7406\u89e3', 'pinyin': 'l\u01d0 ji\u011b', 'trans': 'understand'},\n{'word': '\u65b9\u9762', 'pinyin': 'f\u0101ng mi\u00e0n', 'trans': 'aspect'},\n{'word': '\u53d6\u5f97', 'pinyin': 'q\u01d4 d\u00e9', 'trans': 'achieve'},\n{'word': '\u663e\u8457', 'pinyin': 'xi\u01cen zh\u00f9', 'trans': 'significant'},\n{'word': '\u8fdb\u5c55', 'pinyin': 'j\u00ecn zh\u01cen', 'trans': 'progress'},\n{'word': '\u7136\u800c', 'pinyin': 'r\u00e1n \u00e9r', 'trans': 'however'},\n{'word': '\u540c\u65f6', 'pinyin': 't\u00f3ng sh\u00ed', 'trans': 'simultaneously'},\n{'word': '\u6301\u7eed', 'pinyin': 'ch\u00ed x\u00f9', 'trans': 'continuous'},\n{'word': '\u611f\u77e5', 'pinyin': 'g\u01cen zh\u012b', 'trans': 'perceive'},\n{'word': '\u8bb0\u5fc6', 'pinyin': 'j\u00ec y\u00ec', 'trans': 'memory'},\n{'word': '\u63a8\u7406', 'pinyin': 'tu\u012b l\u01d0', 'trans': 'reason'},\n{'word': '\u6311\u6218', 'pinyin': 'ti\u01ceo zh\u00e0n', 'trans': 'challenge'},\n{'word': '\u672a', 'pinyin': 'w\u00e8i', 'trans': 'not yet'},\n{'word': '\u5145\u5206', 'pinyin': 'ch\u014dng f\u00e8n', 'trans': 'adequately'},\n{'word': '\u63a2\u7d22', 'pinyin': 't\u00e0n su\u01d2', 'trans': 'explore'},\n{'word': '\u5f53\u524d', 'pinyin': 'd\u0101ng qi\u00e1n', 'trans': 'current'},\n{'word': '\u7531\u4e8e', 'pinyin': 'y\u00f3u y\u00fa', 'trans': 'due to'},\n{'word': '\u5176', 'pinyin': 'q\u00ed', 'trans': 'its'},\n{'word': '\u5e8f\u5217', 'pinyin': 'x\u00f9 li\u00e8', 'trans': 'sequence'},\n{'word': '\u67b6\u6784', 'pinyin': 'ji\u00e0 g\u00f2u', 'trans': 'architecture'},\n{'word': '\u65e0\u6cd5', 'pinyin': 'w\u00fa f\u01ce', 'trans': 'unable to'},\n{'word': '\u5904\u7406', 'pinyin': 'ch\u01d4 l\u01d0', 'trans': 'process'},\n{'word': '\u8f93\u5165', 'pinyin': 'sh\u016b r\u00f9', 'trans': 'input'},\n{'word': '\u751f\u6210', 'pinyin': 'sh\u0113ng ch\u00e9ng', 'trans': 'generate'},\n{'word': '\u54cd\u5e94', 'pinyin': 'xi\u01ceng y\u00ecng', 'trans': 'response'},\n{'word': '\u56e0\u6b64', 'pinyin': 'y\u012bn c\u01d0', 'trans': 'therefore'},\n{'word': '\u9879\u76ee', 'pinyin': 'xi\u00e0ng m\u00f9', 'trans': 'project'},\n{'word': '\u5f15\u5165', 'pinyin': 'y\u01d0n r\u00f9', 'trans': 'introduce'},\n{'word': '\u5206\u79bb', 'pinyin': 'f\u0113n l\u00ed', 'trans': 'separate'},\n{'word': '\u673a\u5236', 'pinyin': 'j\u012b zh\u00ec', 'trans': 'mechanism'},\n{'word': '\u4f7f\u5f97', 'pinyin': 'sh\u01d0 d\u00e9', 'trans': 'enable'},\n{'word': '\u5b9e\u65f6', 'pinyin': 'sh\u00ed sh\u00ed', 'trans': 'real-time'},\n{'word': '\u89c6\u9891', 'pinyin': 'sh\u00ec p\u00edn', 'trans': 'video'},\n{'word': '\u97f3\u9891', 'pinyin': 'y\u012bn p\u00edn', 'trans': 'audio'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 228, "total_tokens": 1514, "completion_tokens": 1286}}
[13.12.2024 09:12] Response: [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'},
{'word': 'ÂàõÂª∫', 'pinyin': 'chu√†ng ji√†n', 'trans': 'create'},
{'word': 'ËÉΩÂ§ü', 'pinyin': 'n√©ng g√≤u', 'trans': 'be able to'},
{'word': 'ÈïøÊó∂Èó¥', 'pinyin': 'ch√°ng sh√≠ jiƒÅn', 'trans': 'long period of time'},
{'word': '‰∫íÂä®', 'pinyin': 'h√π d√≤ng', 'trans': 'interact'},
{'word': '‰∫∫Â∑•Êô∫ËÉΩ', 'pinyin': 'r√©n g≈çng zh√¨ n√©ng', 'trans': 'artificial intelligence'},
{'word': 'Á≥ªÁªü', 'pinyin': 'x√¨ t«íng', 'trans': 'system'},
{'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'},
{'word': 'ÁõÆÊ†á', 'pinyin': 'm√π biƒÅo', 'trans': 'goal'},
{'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'},
{'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'},
{'word': 'ÂºÄÊîæ', 'pinyin': 'kƒÅi f√†ng', 'trans': 'open'},
{'word': '‰∏ñÁïå', 'pinyin': 'sh√¨ ji√®', 'trans': 'world'},
{'word': 'ÁêÜËß£', 'pinyin': 'l«ê jiƒõ', 'trans': 'understand'},
{'word': 'ÊñπÈù¢', 'pinyin': 'fƒÅng mi√†n', 'trans': 'aspect'},
{'word': 'ÂèñÂæó', 'pinyin': 'q«î d√©', 'trans': 'achieve'},
{'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'significant'},
{'word': 'ËøõÂ±ï', 'pinyin': 'j√¨n zh«én', 'trans': 'progress'},
{'word': 'ÁÑ∂ËÄå', 'pinyin': 'r√°n √©r', 'trans': 'however'},
{'word': 'ÂêåÊó∂', 'pinyin': 't√≥ng sh√≠', 'trans': 'simultaneously'},
{'word': 'ÊåÅÁª≠', 'pinyin': 'ch√≠ x√π', 'trans': 'continuous'},
{'word': 'ÊÑüÁü•', 'pinyin': 'g«én zhƒ´', 'trans': 'perceive'},
{'word': 'ËÆ∞ÂøÜ', 'pinyin': 'j√¨ y√¨', 'trans': 'memory'},
{'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reason'},
{'word': 'ÊåëÊàò', 'pinyin': 'ti«éo zh√†n', 'trans': 'challenge'},
{'word': 'Êú™', 'pinyin': 'w√®i', 'trans': 'not yet'},
{'word': 'ÂÖÖÂàÜ', 'pinyin': 'ch≈çng f√®n', 'trans': 'adequately'},
{'word': 'Êé¢Á¥¢', 'pinyin': 't√†n su«í', 'trans': 'explore'},
{'word': 'ÂΩìÂâç', 'pinyin': 'dƒÅng qi√°n', 'trans': 'current'},
{'word': 'Áî±‰∫é', 'pinyin': 'y√≥u y√∫', 'trans': 'due to'},
{'word': 'ÂÖ∂', 'pinyin': 'q√≠', 'trans': 'its'},
{'word': 'Â∫èÂàó', 'pinyin': 'x√π li√®', 'trans': 'sequence'},
{'word': 'Êû∂ÊûÑ', 'pinyin': 'ji√† g√≤u', 'trans': 'architecture'},
{'word': 'Êó†Ê≥ï', 'pinyin': 'w√∫ f«é', 'trans': 'unable to'},
{'word': 'Â§ÑÁêÜ', 'pinyin': 'ch«î l«ê', 'trans': 'process'},
{'word': 'ËæìÂÖ•', 'pinyin': 'sh≈´ r√π', 'trans': 'input'},
{'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generate'},
{'word': 'ÂìçÂ∫î', 'pinyin': 'xi«éng y√¨ng', 'trans': 'response'},
{'word': 'Âõ†Ê≠§', 'pinyin': 'yƒ´n c«ê', 'trans': 'therefore'},
{'word': 'È°πÁõÆ', 'pinyin': 'xi√†ng m√π', 'trans': 'project'},
{'word': 'ÂºïÂÖ•', 'pinyin': 'y«ên r√π', 'trans': 'introduce'},
{'word': 'ÂàÜÁ¶ª', 'pinyin': 'fƒìn l√≠', 'trans': 'separate'},
{'word': 'Êú∫Âà∂', 'pinyin': 'jƒ´ zh√¨', 'trans': 'mechanism'},
{'word': '‰ΩøÂæó', 'pinyin': 'sh«ê d√©', 'trans': 'enable'},
{'word': 'ÂÆûÊó∂', 'pinyin': 'sh√≠ sh√≠', 'trans': 'real-time'},
{'word': 'ËßÜÈ¢ë', 'pinyin': 'sh√¨ p√≠n', 'trans': 'video'},
{'word': 'Èü≥È¢ë', 'pinyin': 'yƒ´n p√≠n', 'trans': 'audio'}]
[13.12.2024 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÂàõÂª∫ËÉΩÂ§üÈïøÊó∂Èó¥‰∏éÁéØÂ¢É‰∫íÂä®ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÁöÑÁ†îÁ©∂ÁõÆÊ†á„ÄÇÊúÄËøëÔºåÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÂºÄÊîæ‰∏ñÁïåÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåÂêåÊó∂ËøõË°åÊåÅÁª≠ÁöÑÊÑüÁü•„ÄÅËÆ∞ÂøÜÂíåÊé®ÁêÜ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Êú™ÂÖÖÂàÜÊé¢Á¥¢ÁöÑÊåëÊàò„ÄÇÂΩìÂâçÁöÑMLLMsÁî±‰∫éÂÖ∂Â∫èÂàóÂà∞Â∫èÂàóÁöÑÊû∂ÊûÑÔºåÊó†Ê≥ïÂêåÊó∂Â§ÑÁêÜËæìÂÖ•ÂíåÁîüÊàêÂìçÂ∫î„ÄÇÂõ†Ê≠§ÔºåËøô‰∏™È°πÁõÆÂºïÂÖ•‰∫ÜÂàÜÁ¶ªÁöÑÊÑüÁü•„ÄÅÊé®ÁêÜÂíåËÆ∞ÂøÜÊú∫Âà∂Ôºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂÆûÊó∂Â§ÑÁêÜËßÜÈ¢ëÂíåÈü≥È¢ëËæìÂÖ•„ÄÇ
[13.12.2024 09:12] Mistral response. {"id": "3e7cbac84b754ed189bb599168bbf65a", "object": "chat.completion", "created": 1734081168, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses the research goals of creating artificial intelligence systems that can interact with the environment for extended periods. Recently, multimodal large language models (MLLMs) have made significant progress in understanding the open world. However, simultaneously performing continuous perception, memory, and reasoning remains an insufficiently explored challenge. Current MLLMs, due to their sequence-to-sequence architecture, are unable to process inputs and generate responses concurrently. Therefore, this project introduces separate mechanisms for perception, reasoning, and memory, enabling the model to process video and audio inputs in real-time."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 194, "total_tokens": 316, "completion_tokens": 122}}
[13.12.2024 09:12] Response: This article discusses the research goals of creating artificial intelligence systems that can interact with the environment for extended periods. Recently, multimodal large language models (MLLMs) have made significant progress in understanding the open world. However, simultaneously performing continuous perception, memory, and reasoning remains an insufficiently explored challenge. Current MLLMs, due to their sequence-to-sequence architecture, are unable to process inputs and generate responses concurrently. Therefore, this project introduces separate mechanisms for perception, reasoning, and memory, enabling the model to process video and audio inputs in real-time.
[13.12.2024 09:12] Renaming data file.
[13.12.2024 09:12] Renaming previous data. hf_papers.json to ./d/2024-12-13.json
[13.12.2024 09:12] Saving new data file.
[13.12.2024 09:12] Generating page.
[13.12.2024 09:12] Renaming previous page.
[13.12.2024 09:12] Renaming previous data. index.html to ./d/2024-12-13.html
[13.12.2024 09:12] [Experimental] Generating Chinese page for reading.
[13.12.2024 09:12] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'ÂàõÂª∫', 'pinyin': 'chu√†ng ji√†n', 'trans': 'create'}, {'word': 'ËÉΩÂ§ü', 'pinyin': 'n√©ng g√≤u', 'trans': 'be able to'}, {'word': 'ÈïøÊó∂Èó¥', 'pinyin': 'ch√°ng sh√≠ jiƒÅn', 'trans': 'long period of time'}, {'word': '‰∫íÂä®', 'pinyin': 'h√π d√≤ng', 'trans': 'interact'}, {'word': '‰∫∫Â∑•Êô∫ËÉΩ', 'pinyin': 'r√©n g≈çng zh√¨ n√©ng', 'trans': 'artificial intelligence'}, {'word': 'Á≥ªÁªü', 'pinyin': 'x√¨ t«íng', 'trans': 'system'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'ÁõÆÊ†á', 'pinyin': 'm√π biƒÅo', 'trans': 'goal'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'ÂºÄÊîæ', 'pinyin': 'kƒÅi f√†ng', 'trans': 'open'}, {'word': '‰∏ñÁïå', 'pinyin': 'sh√¨ ji√®', 'trans': 'world'}, {'word': 'ÁêÜËß£', 'pinyin': 'l«ê jiƒõ', 'trans': 'understand'}, {'word': 'ÊñπÈù¢', 'pinyin': 'fƒÅng mi√†n', 'trans': 'aspect'}, {'word': 'ÂèñÂæó', 'pinyin': 'q«î d√©', 'trans': 'achieve'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'significant'}, {'word': 'ËøõÂ±ï', 'pinyin': 'j√¨n zh«én', 'trans': 'progress'}, {'word': 'ÁÑ∂ËÄå', 'pinyin': 'r√°n √©r', 'trans': 'however'}, {'word': 'ÂêåÊó∂', 'pinyin': 't√≥ng sh√≠', 'trans': 'simultaneously'}, {'word': 'ÊåÅÁª≠', 'pinyin': 'ch√≠ x√π', 'trans': 'continuous'}, {'word': 'ÊÑüÁü•', 'pinyin': 'g«én zhƒ´', 'trans': 'perceive'}, {'word': 'ËÆ∞ÂøÜ', 'pinyin': 'j√¨ y√¨', 'trans': 'memory'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reason'}, {'word': 'ÊåëÊàò', 'pinyin': 'ti«éo zh√†n', 'trans': 'challenge'}, {'word': 'Êú™', 'pinyin': 'w√®i', 'trans': 'not yet'}, {'word': 'ÂÖÖÂàÜ', 'pinyin': 'ch≈çng f√®n', 'trans': 'adequately'}, {'word': 'Êé¢Á¥¢', 'pinyin': 't√†n su«í', 'trans': 'explore'}, {'word': 'ÂΩìÂâç', 'pinyin': 'dƒÅng qi√°n', 'trans': 'current'}, {'word': 'Áî±‰∫é', 'pinyin': 'y√≥u y√∫', 'trans': 'due to'}, {'word': 'ÂÖ∂', 'pinyin': 'q√≠', 'trans': 'its'}, {'word': 'Â∫èÂàó', 'pinyin': 'x√π li√®', 'trans': 'sequence'}, {'word': 'Êû∂ÊûÑ', 'pinyin': 'ji√† g√≤u', 'trans': 'architecture'}, {'word': 'Êó†Ê≥ï', 'pinyin': 'w√∫ f«é', 'trans': 'unable to'}, {'word': 'Â§ÑÁêÜ', 'pinyin': 'ch«î l«ê', 'trans': 'process'}, {'word': 'ËæìÂÖ•', 'pinyin': 'sh≈´ r√π', 'trans': 'input'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generate'}, {'word': 'ÂìçÂ∫î', 'pinyin': 'xi«éng y√¨ng', 'trans': 'response'}, {'word': 'Âõ†Ê≠§', 'pinyin': 'yƒ´n c«ê', 'trans': 'therefore'}, {'word': 'È°πÁõÆ', 'pinyin': 'xi√†ng m√π', 'trans': 'project'}, {'word': 'ÂºïÂÖ•', 'pinyin': 'y«ên r√π', 'trans': 'introduce'}, {'word': 'ÂàÜÁ¶ª', 'pinyin': 'fƒìn l√≠', 'trans': 'separate'}, {'word': 'Êú∫Âà∂', 'pinyin': 'jƒ´ zh√¨', 'trans': 'mechanism'}, {'word': '‰ΩøÂæó', 'pinyin': 'sh«ê d√©', 'trans': 'enable'}, {'word': 'ÂÆûÊó∂', 'pinyin': 'sh√≠ sh√≠', 'trans': 'real-time'}, {'word': 'ËßÜÈ¢ë', 'pinyin': 'sh√¨ p√≠n', 'trans': 'video'}, {'word': 'Èü≥È¢ë', 'pinyin': 'yƒ´n p√≠n', 'trans': 'audio'}]
[13.12.2024 09:12] Renaming previous Chinese page.
[13.12.2024 09:12] Renaming previous data. zh.html to ./d/2024-12-12_zh_reading_task.html
[13.12.2024 09:12] Writing Chinese reading task.
[13.12.2024 09:12] Writing result.
[13.12.2024 09:12] Renaming log file.
[13.12.2024 09:12] Renaming previous data. log.txt to ./logs/2024-12-13_last_log.txt
