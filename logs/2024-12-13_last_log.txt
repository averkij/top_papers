[13.12.2024 15:11] Read previous papers.
[13.12.2024 15:11] Generating top page (month).
[13.12.2024 15:11] Writing top page (month).
[13.12.2024 16:13] Read previous papers.
[13.12.2024 16:13] Get feed.
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09596
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08905
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08737
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08635
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09618
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09605
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09593
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09619
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05994
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09501
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09405
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09569
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.08972
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09349
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09586
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09622
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09585
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09460
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09013
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09370
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09573
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09025
[13.12.2024 16:13] Get page data from previous paper. URL: https://huggingface.co/papers/2412.05552
[13.12.2024 16:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.12.2024 16:13] No deleted papers detected.
[13.12.2024 16:13] Downloading and parsing papers (pdf, html). Total: 23.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09596.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09596.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09596.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.08905.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.08905.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.08905.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.08737.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.08737.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.08737.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.08635.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.08635.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.08635.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09618.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09618.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09618.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09605.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09605.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09605.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09593.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09593.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09593.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09619.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09619.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09619.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.05994.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.05994.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.05994.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09501.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09501.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09501.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09405.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09405.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09405.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09569.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09569.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09569.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.08972.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.08972.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.08972.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09349.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09349.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09349.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09586.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09586.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09586.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09622.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09622.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09622.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09585.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09585.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09585.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09460.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09460.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09460.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09013.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09013.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09013.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09370.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09370.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09370.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09573.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09573.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09573.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.09025.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.09025.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.09025.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Downloading and parsing paper https://huggingface.co/papers/2412.05552.
[13.12.2024 16:13] Extra JSON file exists (./assets/json/2412.05552.json), skip PDF parsing.
[13.12.2024 16:13] Paper image links file exists (./assets/img_data/2412.05552.json), skip HTML parsing.
[13.12.2024 16:13] Success.
[13.12.2024 16:13] Enriching papers with extra data.
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 0. Creating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuou...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 1. We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data ...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 2. Multimodal large language models (MLLMs) have made rapid progress in recent years, yet continue to struggle with low-level visual perception (LLVP) -- particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robot...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 3. Multimodal generative models require a unified approach to handle both discrete data (e.g., text and code) and continuous data (e.g., image, audio, video). In this work, we propose Latent Language Modeling (LatentLM), which seamlessly integrates continuous and discrete data using causal Transformers...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 4. Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among image...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 5. Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective t...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 6. Recovering the geometry and materials of objects from a single image is challenging due to its under-constrained nature. In this paper, we present Neural LightRig, a novel framework that boosts intrinsic estimation by leveraging auxiliary multi-lighting conditions from 2D diffusion priors. Specifica...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 7. Existing text-to-image (T2I) diffusion models face several limitations, including large model sizes, slow runtime, and low-quality generation on mobile devices. This paper aims to address all of these challenges by developing an extremely small and fast T2I model that generates high-resolution and h...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 8. The approximation of Partial Differential Equations (PDEs) using neural networks has seen significant advancements through Physics-Informed Neural Networks (PINNs). Despite their straightforward optimization framework and flexibility in implementing various PDEs, PINNs often suffer from limited accu...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 9. As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond single-domain capabilities is essential to meet the demands for more versatile and efficient AI. However, previous omni-models have insufficiently explored speech, neglecting its integration with multi-modality. We introduce Lyra,...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 10. Modern sensors produce increasingly rich streams of high-resolution data. Due to resource constraints, machine learning systems discard the vast majority of this information via resolution reduction. Compressed-domain learning allows models to operate on compact latent representations, allowing high...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 11. Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this ...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 12. This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains -- airline baggage fees, NBA transactions, and tax regulations -- RuleArena assesses ...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 13. Controllable human image animation aims to generate videos from reference images using driving videos. Due to the limited control signals provided by sparse guidance (e.g., skeleton pose), recent works have attempted to introduce additional dense conditions (e.g., depth map) to ensure motion alignme...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 14. We address the problem of gaze target estimation, which aims to predict where a person is looking in a scene. Predicting a person's gaze target requires reasoning both about the person's appearance and the contents of the scene. Prior works have developed increasingly complex, hand-crafted pipelines...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 15. Recent advances in text-to-image customization have enabled high-fidelity, context-rich generation of personalized images, allowing specific concepts to appear in a variety of scenarios. However, current methods struggle with combining multiple personalized models, often leading to attribute entangl...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 16. The standard practice for developing contemporary MLLMs is to feed features from vision encoder(s) into the LLM and train with natural language supervision. In this work, we posit an overlooked opportunity to optimize the intermediate LLM representations through a vision perspective (objective), i.e...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 17. The use of copyrighted materials in training generative language models raises critical legal and ethical questions. This paper presents a framework for and the results of empirically assessing the impact of copyrighted materials on the performance of large language models (LLMs) for Norwegian. We f...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 18. This study presents a new image super-resolution (SR) technique based on diffusion inversion, aiming at harnessing the rich image priors encapsulated in large pre-trained diffusion models to improve SR performance. We design a Partial noise Prediction strategy to construct an intermediate state of t...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 19. Word Sense Disambiguation (WSD) is the task of associating a word in a given context with its most suitable meaning among a set of possible candidates. While the task has recently witnessed renewed interest, with systems achieving performances above the estimated inter-annotator agreement, at the ti...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 20. Existing sparse-view reconstruction models heavily rely on accurate known camera poses. However, deriving camera extrinsics and intrinsics from sparse-view images presents significant challenges. In this work, we present FreeSplatter, a highly scalable, feed-forward reconstruction framework capable ...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 21. Neural Machine Translation (NMT) models are typically trained on datasets with limited exposure to Scientific, Technical and Educational domains. Translation models thus, in general, struggle with tasks that involve scientific understanding or technical jargon. Their performance is found to be even ...
[13.12.2024 16:13] ********************************************************************************
[13.12.2024 16:13] Abstract 22. The academic field of learning instruction-guided visual navigation can be generally categorized into high-level category-specific search and low-level language-guided navigation, depending on the granularity of language instruction, in which the former emphasizes the exploration process, while the ...
[13.12.2024 16:13] Read previous papers.
[13.12.2024 16:13] Generating reviews via LLM API.
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#long_context", "#audio", "#cv", "#multimodal", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –Ω–æ–≤—ã–π —à–∞–≥ –∫ —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω–æ–º—É –ò–ò", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∫ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–º
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#data", "#reasoning", "#synthetic", "#training", "#benchmark", "#architecture"], "emoji": "üß†", "ru": {"title": "–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö - –∫–ª—é—á –∫ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤—É —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç phi-4 - —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å —Å 14 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –∫–∞—á–µ
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#benchmark", "#architecture", "#low_resource", "#synthetic", "#multimodal", "#optimization", "#training", "#data"], "emoji": "üìê", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Geoperception –¥–ª—è –æ—Ü–µ–Ω–∫–∏
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#audio", "#video", "#cv", "#multimodal", "#training"], "emoji": "üß†", "ru": {"title": "LatentLM: –û–±—ä–µ–¥–∏–Ω—è—è –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–µ –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Latent Language Modeling (LatentLM) - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#multimodal", "#diffusion", "#optimization", "#training"], "emoji": "üñºÔ∏è", "ru": {"title": "EasyRef: —É–º–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–∞–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EasyRef - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ–∑–≤–æ–ª—è
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#data", "#dataset", "#training", "#optimization", "#agents", "#synthetic"], "emoji": "ü§ñ", "ru": {"title": "AgentTrek: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ GUI-–∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤–µ–±-—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AgentTrek - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#diffusion", "#synthetic", "#open_source", "#cv", "#3d", "#dataset"], "emoji": "üí°", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ 3D-–≥–µ–æ–º–µ—Ç—Ä–∏–∏ —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Å–≤–µ—â–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Neural LightRig - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–≤–æ–π—Å—Ç–≤ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –ú–µ
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#architecture", "#small_models", "#training", "#optimization", "#diffusion", "#benchmark"], "emoji": "üì±", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É, SnapGen, —Ä–∞–∑—Ä–∞–±
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#optimization", "#math", "#architecture"], "emoji": "üìä", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≥–∞—É—Å—Å–æ–≤—ã —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É—Ä–∞–≤–Ω–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É—Ä–∞–≤–Ω–µ–Ω–∏–π –≤ —á–∞—Å—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö (–î–£–ß–ü) —Å –∏—Å–ø–æ–ª—å–∑
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#open_source", "#dataset", "#long_context", "#multimodal", "#benchmark"], "emoji": "üé≠", "ru": {"title": "Lyra: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ—á–∏ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Lyra - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (MLLM), –∫–æ—Ç–æ—Ä–∞—è —É
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#architecture", "#inference", "#synthetic", "#dataset", "#optimization", "#multimodal"], "emoji": "üóúÔ∏è", "ru": {"title": "WaLLoC: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç WaLLoC - –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–µ–∫–∞ –¥–ª—è —Å–∂
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#interpretability", "#alignment", "#benchmark", "#rlhf"], "emoji": "‚öñÔ∏è", "ru": {"title": "LLM –∫–∞–∫ –±–µ—Å–ø—Ä–∏—Å—Ç—Ä–∞—Å—Ç–Ω—ã–µ —Å—É–¥—å–∏ –ò–ò-—Å–∏—Å—Ç–µ–º", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –æ—Ü–µ–Ω–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —Ä–æ–ª–∏ —Å—É–¥–µ–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ –≤—ã–±–æ—Ä–∞ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –ò–ò-—Å–∏—Å—Ç–µ–º
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#long_context", "#math", "#benchmark", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å–ø–æ—Ç—ã–∫–∞—é—Ç—Å—è –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª–∞—Ö —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞", "desc": "RuleArena - —ç—Ç–æ –Ω–æ–≤—ã–π —Å–ª–æ–∂–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–ª–µ–¥–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–º –ø—Ä–∞–≤–∏–ª–∞–º –≤ 
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#video"], "emoji": "üé≠", "ru": {"title": "DisPose: –£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –∞–Ω–∏–º–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DisPose - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Å–∫–µ–ª–µ—Ç–Ω—É
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#cv", "#reasoning"], "emoji": "üëÄ", "ru": {"title": "–ü—Ä–æ—Å—Ç–æ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∑–≥–ª—è–¥–∞ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Gaze-LLE - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∑–≥–ª—è–¥–∞ —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#training", "#multimodal", "#cv"], "emoji": "üé®", "ru": {"title": "LoRACLR: –ì–∞—Ä–º–æ–Ω–∏—á–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "LoRACLR - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π LoRA –±–µ–∑ 
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#architecture", "#open_source", "#cv", "#multimodal"], "emoji": "üëÅÔ∏è", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É –∑—Ä–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM)
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#low_resource", "#multilingual", "#dataset", "#benchmark", "#ethics"], "emoji": "üìö", "ru": {"title": "–ê–≤—Ç–æ—Ä—Å–∫–æ–µ –ø—Ä–∞–≤–æ –≤ —ç–ø–æ—Ö—É –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –æ—Ü–µ–Ω–∫–∞ –≤–ª–∏—è–Ω–∏—è –Ω–∞ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –∑–∞—â–∏—â–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—Ä—Å–∫–∏–º –ø—Ä–∞–≤–æ–º –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–æ–ª—å
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#cv", "#open_source", "#dataset", "#architecture", "#diffusion"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Å–≤–µ—Ä—Ö—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω–≤–µ—Ä—Å–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Ç–µ—Ö–Ω–∏–∫—É —Å–≤–µ—Ä—Ö—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –∏–Ω–≤–µ—Ä—Å–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏. –ú–µ—Ç
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#multilingual", "#reasoning", "#dataset", "#architecture"], "emoji": "üîó", "ru": {"title": "–û—Ç WSD –∫ WSL: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–º—ã—Å–ª–∞ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Word Sense Linking (WSL), –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—É—é –∑–∞–¥–∞—á—É Word Sense
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#architecture", "#3d"], "emoji": "üîç", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏: –æ—Ç –Ω–µ–æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫ —Ç–æ—á–Ω—ã–º –º–æ–¥–µ–ª—è–º", "desc": "FreeSplatter - —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∞—è —Å –Ω–µ–æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —Å —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –û–Ω–∞ –∏
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#benchmark", "#multilingual", "#low_resource", "#machine_translation", "#dataset", "#open_source", "#training"], "emoji": "üåè", "ru": {"title": "–ù–æ–≤—ã–π –∫–æ—Ä–ø—É—Å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∫–æ—Ä–ø—É—Å –¥–ª
[13.12.2024 16:13] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#agents", "#optimization"], "emoji": "üß≠", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤–∏–∑—É–∞–ª—å–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
[13.12.2024 16:13] Loading Chinese text from previous data.
[13.12.2024 16:13] Renaming data file.
[13.12.2024 16:13] Renaming previous data. hf_papers.json to ./d/2024-12-13.json
[13.12.2024 16:13] Saving new data file.
[13.12.2024 16:13] Generating page.
[13.12.2024 16:13] Renaming previous page.
[13.12.2024 16:13] Renaming previous data. index.html to ./d/2024-12-13.html
[13.12.2024 16:13] [Experimental] Generating Chinese page for reading.
[13.12.2024 16:13] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'ÂàõÂª∫', 'pinyin': 'chu√†ng ji√†n', 'trans': 'create'}, {'word': 'ËÉΩÂ§ü', 'pinyin': 'n√©ng g√≤u', 'trans': 'be able to'}, {'word': 'ÈïøÊó∂Èó¥', 'pinyin': 'ch√°ng sh√≠ jiƒÅn', 'trans': 'long period of time'}, {'word': '‰∫íÂä®', 'pinyin': 'h√π d√≤ng', 'trans': 'interact'}, {'word': '‰∫∫Â∑•Êô∫ËÉΩ', 'pinyin': 'r√©n g≈çng zh√¨ n√©ng', 'trans': 'artificial intelligence'}, {'word': 'Á≥ªÁªü', 'pinyin': 'x√¨ t«íng', 'trans': 'system'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°n ji≈´', 'trans': 'research'}, {'word': 'ÁõÆÊ†á', 'pinyin': 'm√π biƒÅo', 'trans': 'goal'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'ÂºÄÊîæ', 'pinyin': 'kƒÅi f√†ng', 'trans': 'open'}, {'word': '‰∏ñÁïå', 'pinyin': 'sh√¨ ji√®', 'trans': 'world'}, {'word': 'ÁêÜËß£', 'pinyin': 'l«ê jiƒõ', 'trans': 'understand'}, {'word': 'ÊñπÈù¢', 'pinyin': 'fƒÅng mi√†n', 'trans': 'aspect'}, {'word': 'ÂèñÂæó', 'pinyin': 'q«î d√©', 'trans': 'achieve'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«én zh√π', 'trans': 'significant'}, {'word': 'ËøõÂ±ï', 'pinyin': 'j√¨n zh«én', 'trans': 'progress'}, {'word': 'ÁÑ∂ËÄå', 'pinyin': 'r√°n √©r', 'trans': 'however'}, {'word': 'ÂêåÊó∂', 'pinyin': 't√≥ng sh√≠', 'trans': 'simultaneously'}, {'word': 'ÊåÅÁª≠', 'pinyin': 'ch√≠ x√π', 'trans': 'continuous'}, {'word': 'ÊÑüÁü•', 'pinyin': 'g«én zhƒ´', 'trans': 'perceive'}, {'word': 'ËÆ∞ÂøÜ', 'pinyin': 'j√¨ y√¨', 'trans': 'memory'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reason'}, {'word': 'ÊåëÊàò', 'pinyin': 'ti«éo zh√†n', 'trans': 'challenge'}, {'word': 'Êú™', 'pinyin': 'w√®i', 'trans': 'not yet'}, {'word': 'ÂÖÖÂàÜ', 'pinyin': 'ch≈çng f√®n', 'trans': 'adequately'}, {'word': 'Êé¢Á¥¢', 'pinyin': 't√†n su«í', 'trans': 'explore'}, {'word': 'ÂΩìÂâç', 'pinyin': 'dƒÅng qi√°n', 'trans': 'current'}, {'word': 'Áî±‰∫é', 'pinyin': 'y√≥u y√∫', 'trans': 'due to'}, {'word': 'ÂÖ∂', 'pinyin': 'q√≠', 'trans': 'its'}, {'word': 'Â∫èÂàó', 'pinyin': 'x√π li√®', 'trans': 'sequence'}, {'word': 'Êû∂ÊûÑ', 'pinyin': 'ji√† g√≤u', 'trans': 'architecture'}, {'word': 'Êó†Ê≥ï', 'pinyin': 'w√∫ f«é', 'trans': 'unable to'}, {'word': 'Â§ÑÁêÜ', 'pinyin': 'ch«î l«ê', 'trans': 'process'}, {'word': 'ËæìÂÖ•', 'pinyin': 'sh≈´ r√π', 'trans': 'input'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generate'}, {'word': 'ÂìçÂ∫î', 'pinyin': 'xi«éng y√¨ng', 'trans': 'response'}, {'word': 'Âõ†Ê≠§', 'pinyin': 'yƒ´n c«ê', 'trans': 'therefore'}, {'word': 'È°πÁõÆ', 'pinyin': 'xi√†ng m√π', 'trans': 'project'}, {'word': 'ÂºïÂÖ•', 'pinyin': 'y«ên r√π', 'trans': 'introduce'}, {'word': 'ÂàÜÁ¶ª', 'pinyin': 'fƒìn l√≠', 'trans': 'separate'}, {'word': 'Êú∫Âà∂', 'pinyin': 'jƒ´ zh√¨', 'trans': 'mechanism'}, {'word': '‰ΩøÂæó', 'pinyin': 'sh«ê d√©', 'trans': 'enable'}, {'word': 'ÂÆûÊó∂', 'pinyin': 'sh√≠ sh√≠', 'trans': 'real-time'}, {'word': 'ËßÜÈ¢ë', 'pinyin': 'sh√¨ p√≠n', 'trans': 'video'}, {'word': 'Èü≥È¢ë', 'pinyin': 'yƒ´n p√≠n', 'trans': 'audio'}]
[13.12.2024 16:13] Renaming previous Chinese page.
[13.12.2024 16:13] Renaming previous data. zh.html to ./d/2024-12-12_zh_reading_task.html
[13.12.2024 16:13] Writing Chinese reading task.
[13.12.2024 16:13] Writing result.
[13.12.2024 16:13] Renaming log file.
[13.12.2024 16:13] Renaming previous data. log.txt to ./logs/2024-12-13_last_log.txt
