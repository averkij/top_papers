[16.10.2024 16:14] Read previous papers.
[16.10.2024 16:14] Get feed.
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11779
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11710
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09342
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2406.15786
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11096
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10626
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10816
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09704
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11805
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11795
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11419
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09754
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10814
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10934
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08001
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09745
[16.10.2024 16:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06593
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 0. Multimodal Large Language Models (MLLMs) frequently exhibit hallucination phenomena, but the underlying reasons remain poorly understood. In this paper, we present an empirical analysis and find that, although MLLMs incorrectly generate the objects in the final output, they are actually able to reco...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 1. Large Language Models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations: (1). Insufficient evaluati...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 2. Enlarging the context window of large language models (LLMs) has become a crucial research area, particularly for applications involving extremely long texts. In this work, we propose a novel training-free framework for processing long texts, utilizing a divide-and-conquer strategy to achieve compre...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 3. While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy ...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 4. Existing works have established multiple benchmarks to highlight the security risks associated with Code GenAI. These risks are primarily reflected in two areas: a model potential to generate insecure code (insecure coding) and its utility in cyberattacks (cyberattack helpfulness). While these bench...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 5. Adapting medical Large Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages. To address this, we first construct a high-quality medical dataset and conduct analysis to ensur...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 6. The efficacy of video generation models heavily depends on the quality of their training datasets. Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos. However, th...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 7. Echocardiography is the most widely used cardiac imaging modality, capturing ultrasound video data to assess cardiac structure and function. Artificial intelligence (AI) in echocardiography has the potential to streamline manual tasks and improve reproducibility and precision. However, most echocard...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 8. Large language models (LLMs) combined with tool learning have gained impressive results in real-world applications. During tool learning, LLMs may call multiple tools in nested orders, where the latter tool call may take the former response as its input parameters. However, current research on the n...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 9. As one of the most popular and sought-after generative models in the recent years, diffusion models have sparked the interests of many researchers and steadily shown excellent advantage in various generative tasks such as image synthesis, video generation, molecule design, 3D scene rendering and mul...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 10. We present a spatial and angular Gaussian based representation and a triple splatting process, for real-time, high-quality novel lighting-and-view synthesis from multi-view point-lit input images. To describe complex appearance, we employ a Lambertian plus a mixture of angular Gaussians as an effect...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 11. Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 12. While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 13. Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 14. The increasing demand for versatile robotic systems to operate in diverse and dynamic environments has emphasized the importance of a generalist policy, which leverages a large cross-embodiment data corpus to facilitate broad adaptability and high-level reasoning. However, the generalist would strug...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 15. The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced. However, this mechanism has not been adequately demon...
[16.10.2024 16:14] ********************************************************************************
[16.10.2024 16:14] Abstract 16. Recent approaches attempt to adapt powerful interactive segmentation models, such as SAM, to interactive matting and fine-tune the models based on synthetic matting datasets. However, models trained on synthetic data fail to generalize to complex and occlusion scenes. We address this challenge by pr...
[16.10.2024 16:14] Read previous papers.
[16.10.2024 16:14] Generating reviews via LLM API.
[16.10.2024 16:14] Using data from previous issue: {"desc": "Статья исследует проблему галлюцинаций в мультимодальных больших языковых моделях (MLLM). Авторы обнаружили, что MLLM способны распознавать визуальные объекты в промежуточных слоях, но в итоговом выводе могут их некорректно генерировать. Предполагается, что это связано с подавлением визуал
[16.10.2024 16:14] Using data from previous issue: {"desc": "В этой статье представлен новый бенчмарк MTU-Bench для оценки навыков использования инструментов большими языковыми моделями (LLM). MTU-Bench охватывает пять сценариев использования инструментов, от простых до сложных и нестандартных задач. Оценка производится без использования API GPT или
[16.10.2024 16:14] Using data from previous issue: {"desc": "Статья представляет новый подход к обработке длинных текстов с помощью больших языковых моделей (LLM). Авторы предлагают фреймворк LLM×MapReduce, который разделяет документ на части для обработки, а затем объединяет промежуточные результаты. Для решения проблем потери информации при раздел
[16.10.2024 16:14] Using data from previous issue: {"desc": "Это исследование посвящено проблеме избыточности в архитектуре больших языковых моделей (LLM) на базе трансформеров. Авторы обнаружили, что значительная часть слоев внимания (attention layers) обладает высокой схожестью и может быть удалена без существенной потери производительности. Напри
[16.10.2024 16:14] Using data from previous issue: {"desc": "Статья представляет SecCodePLT - новую платформу для оценки рисков безопасности генеративных моделей AI для кода. Авторы разработали методологию создания данных, сочетающую экспертную оценку и автоматическую генерацию, а также внедрили динамические метрики оценки. SecCodePLT позволяет оцен
[16.10.2024 16:14] Using data from previous issue: {"desc": "Статья посвящена адаптации больших языковых моделей (LLM) для медицинских целей в различных языках. Авторы создают качественный медицинский датасет и исследуют внутренние механизмы многоязычных LLM с использованием модульности Mixture of Experts (MoE). Они предлагают новый метод маршрутиза
[16.10.2024 16:14] Using data from previous issue: {"desc": "Статья представляет новый датасет LVD-2M для обучения моделей генерации длинных видео. Датасет содержит 2 миллиона видео длительностью более 10 секунд, снятых одним кадром и сопровождаемых плотными временными подписями. Авторы разработали методику отбора качественных видео и создания аннот
[16.10.2024 16:14] Using data from previous issue: {"desc": "EchoPrime - это многозадачная модель искусственного интеллекта для анализа эхокардиограмм, обученная на более чем 12 миллионах пар видео-отчетов. Она использует контрастное обучение для создания единой модели вложений для всех стандартных проекций в комплексном эхокардиографическом исследо
[16.10.2024 16:14] Using data from previous issue: {"desc": "Статья представляет новый датасет NesTools для оценки способностей больших языковых моделей (LLM) к обучению вложенному использованию инструментов. NesTools включает метод автоматической генерации данных для создания крупномасштабных вложенных вызовов инструментов с различными структурами 
[16.10.2024 16:14] Using data from previous issue: {"desc": "Эта статья представляет собой обзор диффузионных моделей, одного из самых популярных и востребованных типов генеративных моделей последних лет. Авторы предоставляют комплексный анализ принципов работы и эффективных практик применения диффузионных моделей в различных задачах, таких как синт
[16.10.2024 16:14] Using data from previous issue: {"desc": "Статья представляет новый метод синтеза изображений с изменением освещения и ракурса в реальном времени. Авторы используют представление на основе пространственных и угловых гауссианов, а также процесс тройного сплаттинга. Для описания сложных визуальных эффектов применяется функция отраже
[16.10.2024 16:14] Using data from previous issue: {"desc": "Статья представляет архитектуру SimBa для масштабирования параметров в глубоком обучении с подкреплением (RL). SimBa включает нормализацию наблюдений, резидуальный блок прямой связи и нормализацию слоев для внедрения смещения к простоте. Эксперименты показывают, что SimBa улучшает эффектив
[16.10.2024 16:14] Using data from previous issue: {"desc": "Исследование показывает, что маршрутизаторы экспертов в моделях Mixture-of-Experts (MoE) LLM могут служить готовой моделью для создания эмбеддингов с многообещающей производительностью на разнообразных задачах, не требуя дополнительной настройки. Анализ демонстрирует, что веса маршрутизаци
[16.10.2024 16:14] Using data from previous issue: {"desc": "Статья представляет новый подход к оценке агентных систем искусственного интеллекта - Agent-as-a-Judge. Этот метод использует агентные системы для оценки других агентных систем, что позволяет получать промежуточную обратную связь на протяжении всего процесса решения задачи. Авторы применяю
[16.10.2024 16:14] Using data from previous issue: {"desc": "RoboDual - это синергетическая двойная система, объединяющая преимущества обобщенной и специализированной политик для роботов. Обобщенная политика на основе vision-language-action обеспечивает высокоуровневое понимание задач, а специализированная политика на основе диффузионного трансформе
[16.10.2024 16:14] Using data from previous issue: {"desc": "Статья исследует взаимоусиливающий эффект (MRE) между классификацией на уровне слов и текста в задачах классификации текста. Авторы проводят эмпирические эксперименты на 21 наборе данных MRE Mix для подтверждения теории MRE. Результаты сравнительных экспериментов с использованием тонкой на
[16.10.2024 16:14] Using data from previous issue: {"desc": "В статье представлен новый датасет COCO-Matting для задачи интерактивного маттинга изображений, основанный на реальных сложных сценах из COCO. Авторы предлагают архитектуру SEMat, которая улучшает существующие методы на основе SAM, используя feature-aligned transformer и matte-aligned deco
[16.10.2024 16:14] Loading Chinese text from previous data.
[16.10.2024 16:14] Renaming data file.
[16.10.2024 16:14] Renaming previous data. hf_papers.json to 2024-10-15_hf_papers.json
[16.10.2024 16:14] Saving new data file.
[16.10.2024 16:14] Generating page.
[16.10.2024 16:14] Generating Chinese page for reading.
[16.10.2024 16:14] Chinese vocab [{'word': '多模态', 'pinyin': 'duō mó shuài', 'trans': 'multimodal'}, {'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'}, {'word': '幻觉', 'pinyin': 'huàn jué', 'trans': 'hallucination'}, {'word': '现象', 'pinyin': 'xiàn xiàng', 'trans': 'phenomenon'}, {'word': '原因', 'pinyin': 'yuán yīn', 'trans': 'reason'}, {'word': '错误', 'pinyin': 'cuò wù', 'trans': 'error'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '对象', 'pinyin': 'duì xiàng', 'trans': 'object'}, {'word': '识别', 'pinyin': 'shí bié', 'trans': 'recognize'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '强', 'pinyin': 'qiáng', 'trans': 'strong'}, {'word': '知识', 'pinyin': 'zhī shi', 'trans': 'knowledge'}, {'word': '先验', 'pinyin': 'xiān yàn', 'trans': 'prior'}, {'word': '抑制', 'pinyin': 'yì zhì', 'trans': 'suppress'}, {'word': '信息', 'pinyin': 'xìn xī', 'trans': 'information'}, {'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'}, {'word': '纠正', 'pinyin': 'jiū zhèng', 'trans': 'correct'}, {'word': '解码', 'pinyin': 'jiě mǎ', 'trans': 'decode'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '适应性', 'pinyin': 'shì yìng xìng', 'trans': 'adaptive'}, {'word': '选择', 'pinyin': 'xuǎn zé', 'trans': 'select'}, {'word': '合适', 'pinyin': 'hé shì', 'trans': 'suitable'}, {'word': '整合', 'pinyin': 'zhěng hé', 'trans': 'integrate'}, {'word': '调整', 'pinyin': 'tiáo zhěng', 'trans': 'adjust'}, {'word': '经典', 'pinyin': 'jīng diǎn', 'trans': 'classical'}, {'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'}, {'word': '无缝', 'pinyin': 'wú fēng', 'trans': 'seamless'}, {'word': '结合', 'pinyin': 'jié hé', 'trans': 'combine'}, {'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'apply'}, {'word': '不同', 'pinyin': 'bù tóng', 'trans': 'different'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '降低', 'pinyin': 'jiàng dī', 'trans': 'reduce'}, {'word': '率', 'pinyin': 'lǜ', 'trans': 'rate'}]
[16.10.2024 16:14] Renaming previous page.
[16.10.2024 16:14] Renaming previous data. index.html to 2024-10-15_hf_papers.html
[16.10.2024 16:14] Renaming previous Chinese page.
[16.10.2024 16:14] Renaming previous data. zh.html to 2024-10-15_zh_reading_task.html
[16.10.2024 16:14] Writing result.
[16.10.2024 16:14] Writing Chinese reading task.
[16.10.2024 16:14] Renaming log file.
[16.10.2024 16:14] Renaming previous data. log.txt to 2024-10-15_last_log.txt
