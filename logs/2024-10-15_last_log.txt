[16.10.2024 12:23] Read previous papers.
[16.10.2024 12:23] Get feed.
[16.10.2024 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11779
[16.10.2024 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11710
[16.10.2024 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11096
[16.10.2024 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10816
[16.10.2024 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2406.15786
[16.10.2024 12:23] Extract page data from URL. URL: https://huggingface.co/papers/2410.09342
[16.10.2024 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11419
[16.10.2024 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10814
[16.10.2024 12:23] Extract page data from URL. URL: https://huggingface.co/papers/2410.09754
[16.10.2024 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10626
[16.10.2024 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08001
[16.10.2024 12:23] Extract page data from URL. URL: https://huggingface.co/papers/2410.09745
[16.10.2024 12:23] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11795
[16.10.2024 12:23] ********************************************************************************
[16.10.2024 12:23] Abstract 0. Multimodal Large Language Models (MLLMs) frequently exhibit hallucination phenomena, but the underlying reasons remain poorly understood. In this paper, we present an empirical analysis and find that, although MLLMs incorrectly generate the objects in the final output, they are actually able to reco...
[16.10.2024 12:23] ********************************************************************************
[16.10.2024 12:23] Abstract 1. Large Language Models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations: (1). Insufficient evaluati...
[16.10.2024 12:23] ********************************************************************************
[16.10.2024 12:23] Abstract 2. Existing works have established multiple benchmarks to highlight the security risks associated with Code GenAI. These risks are primarily reflected in two areas: a model potential to generate insecure code (insecure coding) and its utility in cyberattacks (cyberattack helpfulness). While these bench...
[16.10.2024 12:23] ********************************************************************************
[16.10.2024 12:23] Abstract 3. The efficacy of video generation models heavily depends on the quality of their training datasets. Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos. However, th...
[16.10.2024 12:23] ********************************************************************************
[16.10.2024 12:23] Abstract 4. While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy ...
[16.10.2024 12:23] ********************************************************************************
[16.10.2024 12:23] Abstract 5. Enlarging the context window of large language models (LLMs) has become a crucial research area, particularly for applications involving extremely long texts. In this work, we propose a novel training-free framework for processing long texts, utilizing a divide-and-conquer strategy to achieve compre...
[16.10.2024 12:23] ********************************************************************************
[16.10.2024 12:23] Abstract 6. We present a spatial and angular Gaussian based representation and a triple splatting process, for real-time, high-quality novel lighting-and-view synthesis from multi-view point-lit input images. To describe complex appearance, we employ a Lambertian plus a mixture of angular Gaussians as an effect...
[16.10.2024 12:23] ********************************************************************************
[16.10.2024 12:23] Abstract 7. While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture...
[16.10.2024 12:23] ********************************************************************************
[16.10.2024 12:23] Abstract 8. Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models...
[16.10.2024 12:23] ********************************************************************************
[16.10.2024 12:23] Abstract 9. Adapting medical Large Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages. To address this, we first construct a high-quality medical dataset and conduct analysis to ensur...
[16.10.2024 12:23] ********************************************************************************
[16.10.2024 12:23] Abstract 10. The increasing demand for versatile robotic systems to operate in diverse and dynamic environments has emphasized the importance of a generalist policy, which leverages a large cross-embodiment data corpus to facilitate broad adaptability and high-level reasoning. However, the generalist would strug...
[16.10.2024 12:23] ********************************************************************************
[16.10.2024 12:23] Abstract 11. The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced. However, this mechanism has not been adequately demon...
[16.10.2024 12:23] ********************************************************************************
[16.10.2024 12:23] Abstract 12. As one of the most popular and sought-after generative models in the recent years, diffusion models have sparked the interests of many researchers and steadily shown excellent advantage in various generative tasks such as image synthesis, video generation, molecule design, 3D scene rendering and mul...
[16.10.2024 12:23] Read previous papers.
[16.10.2024 12:23] Generating reviews via LLM API.
[16.10.2024 12:23] Using data from previous issue: {"desc": "Статья исследует проблему галлюцинаций в мультимодальных больших языковых моделях (MLLM). Авторы обнаружили, что MLLM способны распознавать визуальные объекты в промежуточных слоях, но в итоговом выводе могут их некорректно генерировать. Предполагается, что это связано с подавлением визуал
[16.10.2024 12:23] Using data from previous issue: {"desc": "В этой статье представлен новый бенчмарк MTU-Bench для оценки навыков использования инструментов большими языковыми моделями (LLM). MTU-Bench охватывает пять сценариев использования инструментов, от простых до сложных и нестандартных задач. Оценка производится без использования API GPT или
[16.10.2024 12:23] Using data from previous issue: {"desc": "Статья представляет SecCodePLT - новую платформу для оценки рисков безопасности генеративных моделей AI для кода. Авторы разработали методологию создания данных, сочетающую экспертную оценку и автоматическую генерацию, а также внедрили динамические метрики оценки. SecCodePLT позволяет оцен
[16.10.2024 12:23] Using data from previous issue: {"desc": "Статья представляет новый датасет LVD-2M для обучения моделей генерации длинных видео. Датасет содержит 2 миллиона видео длительностью более 10 секунд, снятых одним кадром и сопровождаемых плотными временными подписями. Авторы разработали методику отбора качественных видео и создания аннот
[16.10.2024 12:23] Using data from previous issue: {"desc": "Это исследование посвящено проблеме избыточности в архитектуре больших языковых моделей (LLM) на базе трансформеров. Авторы обнаружили, что значительная часть слоев внимания (attention layers) обладает высокой схожестью и может быть удалена без существенной потери производительности. Напри
[16.10.2024 12:23] Querying the API.
[16.10.2024 12:23] Got response. {
  "desc": "Статья представляет новый подход к обработке длинных текстов с помощью больших языковых моделей (LLM). Авторы предлагают фреймворк LLM×MapReduce, который разделяет документ на части для обработки, а затем объединяет промежуточные результаты. Для решения проблем потери информации при разделении текста разработаны протокол структурированной информации и механизм калибровки уверенности в контексте. Эксперименты показывают, что LLM×MapReduce превосходит существующие модели с длинным контекстом и применим к различным LLM.",
  "categories": ["#nlp", "#rag", "#benchmark"],
  "emoji": "📄",
  "title": "Эффективная обработка длинных текстов без переобучения LLM"
}
[16.10.2024 12:23] Get embedding for a paper via LLM API.
[16.10.2024 12:23] Using data from previous issue: {"desc": "Статья представляет новый метод синтеза изображений с изменением освещения и ракурса в реальном времени. Авторы используют представление на основе пространственных и угловых гауссианов, а также процесс тройного сплаттинга. Для описания сложных визуальных эффектов применяется функция отраже
[16.10.2024 12:23] Using data from previous issue: {"desc": "Исследование показывает, что маршрутизаторы экспертов в моделях Mixture-of-Experts (MoE) LLM могут служить готовой моделью для создания эмбеддингов с многообещающей производительностью на разнообразных задачах, не требуя дополнительной настройки. Анализ демонстрирует, что веса маршрутизаци
[16.10.2024 12:23] Querying the API.
[16.10.2024 12:23] Got response. {
  "desc": "Статья представляет архитектуру SimBa для масштабирования параметров в глубоком обучении с подкреплением (RL). SimBa включает нормализацию наблюдений, резидуальный блок прямой связи и нормализацию слоев для внедрения смещения к простоте. Эксперименты показывают, что SimBa улучшает эффективность выборки для различных алгоритмов RL, включая off-policy, on-policy и unsupervised методы. Интеграция SimBa в алгоритм SAC позволяет достичь или превзойти современные методы RL на различных средах с высокой вычислительной эффективностью.",
  "categories": ["#rl", "#deeplearning", "#architecture", "#scalability"],
  "emoji": "🤖",
  "title": "SimBa: Простота и масштабируемость для глубокого обучения с подкреплением"
}
[16.10.2024 12:23] Get embedding for a paper via LLM API.
[16.10.2024 12:23] Using data from previous issue: {"desc": "Статья посвящена адаптации больших языковых моделей (LLM) для медицинских целей в различных языках. Авторы создают качественный медицинский датасет и исследуют внутренние механизмы многоязычных LLM с использованием модульности Mixture of Experts (MoE). Они предлагают новый метод маршрутиза
[16.10.2024 12:23] Using data from previous issue: {"desc": "RoboDual - это синергетическая двойная система, объединяющая преимущества обобщенной и специализированной политик для роботов. Обобщенная политика на основе vision-language-action обеспечивает высокоуровневое понимание задач, а специализированная политика на основе диффузионного трансформе
[16.10.2024 12:23] Querying the API.
[16.10.2024 12:23] Got response. {
  "desc": "Статья исследует взаимоусиливающий эффект (MRE) между классификацией на уровне слов и текста в задачах классификации текста. Авторы проводят эмпирические эксперименты на 21 наборе данных MRE Mix для подтверждения теории MRE. Результаты сравнительных экспериментов с использованием тонкой настройки подтверждают существование MRE. Применение MRE к обучению с подсказками, используя информацию на уровне слов в качестве вербализатора, значительно улучшило F1-меру на 18 из 21 набора данных.",
  "categories": ["#nlp", "#classification", "#finetuning", "#promptlearning"],
  "emoji": "🔄",
  "title": "Взаимное усиление классификации слов и текста улучшает понимание языка"
}
[16.10.2024 12:23] Get embedding for a paper via LLM API.
[16.10.2024 12:23] Using data from previous issue: {"desc": "Эта статья представляет собой обзор диффузионных моделей, одного из самых популярных и востребованных типов генеративных моделей последних лет. Авторы предоставляют комплексный анализ принципов работы и эффективных практик применения диффузионных моделей в различных задачах, таких как синт
[16.10.2024 12:23] Loading Chinese text from previous data.
[16.10.2024 12:23] Renaming data file.
[16.10.2024 12:23] Renaming previous data. hf_papers.json to 2024-10-15_hf_papers.json
[16.10.2024 12:23] Saving new data file.
[16.10.2024 12:23] Generating page.
[16.10.2024 12:23] Generating Chinese page for reading.
[16.10.2024 12:23] Chinese vocab [{'word': '多模态', 'pinyin': 'duō mó shuài', 'trans': 'multimodal'}, {'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'}, {'word': '幻觉', 'pinyin': 'huàn jué', 'trans': 'hallucination'}, {'word': '现象', 'pinyin': 'xiàn xiàng', 'trans': 'phenomenon'}, {'word': '原因', 'pinyin': 'yuán yīn', 'trans': 'reason'}, {'word': '错误', 'pinyin': 'cuò wù', 'trans': 'error'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '对象', 'pinyin': 'duì xiàng', 'trans': 'object'}, {'word': '识别', 'pinyin': 'shí bié', 'trans': 'recognize'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '强', 'pinyin': 'qiáng', 'trans': 'strong'}, {'word': '知识', 'pinyin': 'zhī shi', 'trans': 'knowledge'}, {'word': '先验', 'pinyin': 'xiān yàn', 'trans': 'prior'}, {'word': '抑制', 'pinyin': 'yì zhì', 'trans': 'suppress'}, {'word': '信息', 'pinyin': 'xìn xī', 'trans': 'information'}, {'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'}, {'word': '纠正', 'pinyin': 'jiū zhèng', 'trans': 'correct'}, {'word': '解码', 'pinyin': 'jiě mǎ', 'trans': 'decode'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '适应性', 'pinyin': 'shì yìng xìng', 'trans': 'adaptive'}, {'word': '选择', 'pinyin': 'xuǎn zé', 'trans': 'select'}, {'word': '合适', 'pinyin': 'hé shì', 'trans': 'suitable'}, {'word': '整合', 'pinyin': 'zhěng hé', 'trans': 'integrate'}, {'word': '调整', 'pinyin': 'tiáo zhěng', 'trans': 'adjust'}, {'word': '经典', 'pinyin': 'jīng diǎn', 'trans': 'classical'}, {'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'}, {'word': '无缝', 'pinyin': 'wú fēng', 'trans': 'seamless'}, {'word': '结合', 'pinyin': 'jié hé', 'trans': 'combine'}, {'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'apply'}, {'word': '不同', 'pinyin': 'bù tóng', 'trans': 'different'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '降低', 'pinyin': 'jiàng dī', 'trans': 'reduce'}, {'word': '率', 'pinyin': 'lǜ', 'trans': 'rate'}]
[16.10.2024 12:23] Renaming previous page.
[16.10.2024 12:23] Renaming previous data. index.html to 2024-10-15_hf_papers.html
[16.10.2024 12:23] Renaming previous Chinese page.
[16.10.2024 12:23] Renaming previous data. zh.html to 2024-10-15_zh_reading_task.html
[16.10.2024 12:23] Writing result.
[16.10.2024 12:23] Writing Chinese reading task.
[16.10.2024 12:23] Renaming log file.
[16.10.2024 12:23] Renaming previous data. log.txt to 2024-10-15_last_log.txt
