[31.12.2024 08:13] Read previous papers.
[31.12.2024 08:13] Generating top page (month).
[31.12.2024 08:13] Writing top page (month).
[31.12.2024 09:10] Read previous papers.
[31.12.2024 09:10] Get feed.
[31.12.2024 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.18525
[31.12.2024 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.20070
[31.12.2024 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.20993
[31.12.2024 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.21079
[31.12.2024 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.21037
[31.12.2024 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.20422
[31.12.2024 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.21199
[31.12.2024 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.21139
[31.12.2024 09:10] Get page data from previous paper. URL: https://huggingface.co/papers/2412.20005
[31.12.2024 09:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[31.12.2024 09:10] No deleted papers detected.
[31.12.2024 09:10] Downloading and parsing papers (pdf, html). Total: 9.
[31.12.2024 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.18525.
[31.12.2024 09:10] Extra JSON file exists (./assets/json/2412.18525.json), skip PDF parsing.
[31.12.2024 09:10] Paper image links file exists (./assets/img_data/2412.18525.json), skip HTML parsing.
[31.12.2024 09:10] Success.
[31.12.2024 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.20070.
[31.12.2024 09:10] Extra JSON file exists (./assets/json/2412.20070.json), skip PDF parsing.
[31.12.2024 09:10] Paper image links file exists (./assets/img_data/2412.20070.json), skip HTML parsing.
[31.12.2024 09:10] Success.
[31.12.2024 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.20993.
[31.12.2024 09:10] Extra JSON file exists (./assets/json/2412.20993.json), skip PDF parsing.
[31.12.2024 09:10] Paper image links file exists (./assets/img_data/2412.20993.json), skip HTML parsing.
[31.12.2024 09:10] Success.
[31.12.2024 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.21079.
[31.12.2024 09:10] Extra JSON file exists (./assets/json/2412.21079.json), skip PDF parsing.
[31.12.2024 09:10] Paper image links file exists (./assets/img_data/2412.21079.json), skip HTML parsing.
[31.12.2024 09:10] Success.
[31.12.2024 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.21037.
[31.12.2024 09:10] Extra JSON file exists (./assets/json/2412.21037.json), skip PDF parsing.
[31.12.2024 09:10] Paper image links file exists (./assets/img_data/2412.21037.json), skip HTML parsing.
[31.12.2024 09:10] Success.
[31.12.2024 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.20422.
[31.12.2024 09:10] Extra JSON file exists (./assets/json/2412.20422.json), skip PDF parsing.
[31.12.2024 09:10] Paper image links file exists (./assets/img_data/2412.20422.json), skip HTML parsing.
[31.12.2024 09:10] Success.
[31.12.2024 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.21199.
[31.12.2024 09:10] Extra JSON file exists (./assets/json/2412.21199.json), skip PDF parsing.
[31.12.2024 09:10] Paper image links file exists (./assets/img_data/2412.21199.json), skip HTML parsing.
[31.12.2024 09:10] Success.
[31.12.2024 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.21139.
[31.12.2024 09:10] Extra JSON file exists (./assets/json/2412.21139.json), skip PDF parsing.
[31.12.2024 09:10] Paper image links file exists (./assets/img_data/2412.21139.json), skip HTML parsing.
[31.12.2024 09:10] Success.
[31.12.2024 09:10] Downloading and parsing paper https://huggingface.co/papers/2412.20005.
[31.12.2024 09:10] Extra JSON file exists (./assets/json/2412.20005.json), skip PDF parsing.
[31.12.2024 09:10] Paper image links file exists (./assets/img_data/2412.20005.json), skip HTML parsing.
[31.12.2024 09:10] Success.
[31.12.2024 09:10] Enriching papers with extra data.
[31.12.2024 09:10] ********************************************************************************
[31.12.2024 09:10] Abstract 0. Computer Vision (CV) has yet to fully achieve the zero-shot task generalization observed in Natural Language Processing (NLP), despite following many of the milestones established in NLP, such as large transformer models, extensive pre-training, and the auto-regression paradigm, among others. In thi...
[31.12.2024 09:10] ********************************************************************************
[31.12.2024 09:10] Abstract 1. Multimodal large language models (MLLMs) hold significant potential in the medical field, but their capabilities are often limited by insufficient data in certain medical domains, highlighting the need for understanding what kinds of images can be used by MLLMs for generalization. Current research s...
[31.12.2024 09:10] ********************************************************************************
[31.12.2024 09:10] Abstract 2. The rapid evolution of large language models (LLMs) has unlocked their capabilities in advanced reasoning tasks like mathematical problem-solving, code generation, and legal analysis. Central to this progress are inference-time reasoning algorithms, which refine outputs by exploring multiple solutio...
[31.12.2024 09:10] ********************************************************************************
[31.12.2024 09:10] Abstract 3. As a verified need, consistent editing across in-the-wild images remains a technical challenge arising from various unmanageable factors, like object poses, lighting conditions, and photography environments. Edicho steps in with a training-free solution based on diffusion models, featuring a fundame...
[31.12.2024 09:10] ********************************************************************************
[31.12.2024 09:10] Abstract 4. We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model with 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio in just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models lies in the difficulty of creating preference pairs, as TTA lacks st...
[31.12.2024 09:10] ********************************************************************************
[31.12.2024 09:10] Abstract 5. Recent advancements in generative modeling now enable the creation of 4D content (moving 3D objects) controlled with text prompts. 4D generation has large potential in applications like virtual worlds, media, and gaming, but existing methods provide limited control over the appearance and geometry o...
[31.12.2024 09:10] ********************************************************************************
[31.12.2024 09:10] Abstract 6. We introduce self-invoking code generation, a new task designed to evaluate the progressive reasoning and problem-solving capabilities of LLMs. In this task, models are presented with a base problem and a related, more complex problem. They must solve the base problem and then utilize its solution t...
[31.12.2024 09:10] ********************************************************************************
[31.12.2024 09:10] Abstract 7. We present SWE-Gym, the first environment for training real-world software engineering (SWE) agents. SWE-Gym contains 2,438 real-world Python task instances, each comprising a codebase with an executable runtime environment, unit tests, and a task specified in natural language. We use SWE-Gym to tra...
[31.12.2024 09:10] ********************************************************************************
[31.12.2024 09:10] Abstract 8. We introduce OneKE, a dockerized schema-guided knowledge extraction system, which can extract knowledge from the Web and raw PDF Books, and support various domains (science, news, etc.). Specifically, we design OneKE with multiple agents and a configure knowledge base. Different agents perform their...
[31.12.2024 09:10] Read previous papers.
[31.12.2024 09:10] Generating reviews via LLM API.
[31.12.2024 09:10] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#cv", "#multimodal", "#transfer_learning"], "emoji": "üî¨", "ru": {"title": "–õ–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ - –∫–ª—é—á –∫ –æ–±–æ–±—â–µ–Ω–∏—é –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∫ –æ–±–æ–±—â–µ–Ω–∏—é –Ω–∞ –Ω
[31.12.2024 09:10] Using data from previous issue: {"categories": ["#dataset", "#healthcare", "#open_source", "#multimodal", "#transfer_learning"], "emoji": "ü©∫", "ru": {"title": "–ö–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è - –∫–ª—é—á –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è MLLM", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –º
[31.12.2024 09:10] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#inference"], "emoji": "üß†", "ru": {"title": "Dynasor: —É–º–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö LLM-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º—É Dynasor, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—â—É—é –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã
[31.12.2024 09:10] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#open_source", "#inference"], "emoji": "üñºÔ∏è", "ru": {"title": "Edicho: —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Edicho - —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥
[31.12.2024 09:10] Using data from previous issue: {"categories": ["#dataset", "#audio", "#open_source", "#benchmark", "#alignment", "#rlhf", "#small_models"], "emoji": "üéµ", "ru": {"title": "TangoFlux: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "TangoFlux - —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –∞—É–¥–∏–æ (Text-to-Audio, TT
[31.12.2024 09:10] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#games", "#diffusion", "#video", "#3d"], "emoji": "üé≠", "ru": {"title": "–û–∂–∏–≤–ª–µ–Ω–∏–µ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–∞: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–Ω–∏–º–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫. –ê–≤—Ç
[31.12.2024 09:10] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#training", "#benchmark"], "emoji": "üß†", "ru": {"title": "–°–∞–º–æ–≤—ã–∑—ã–≤–∞—é—â–∏–π—Å—è –∫–æ–¥: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) - –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å–∞–º–æ–≤—ã–∑—ã–≤–∞—é—â–µ–≥–æ—Å—è –∫–æ–¥–∞. –í —Ä–∞–º–∫–∞—Ö —ç—Ç–æ–π –∑–∞–¥
[31.12.2024 09:10] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#agents", "#training"], "emoji": "ü§ñ", "ru": {"title": "SWE-Gym: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ü–û", "desc": "SWE-Gym - —ç—Ç–æ –Ω–æ–≤–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –û–Ω–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç 2438 —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –∑–∞–¥–∞—á –Ω–∞ P
[31.12.2024 09:10] Using data from previous issue: {"categories": ["#dataset", "#agents", "#open_source", "#benchmark", "#multimodal", "#science"], "emoji": "üß†", "ru": {"title": "OneKE: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∏–∑ —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", "desc": "OneKE - —ç—Ç–æ –¥–æ–∫–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π, —É–ø—Ä–∞–≤–ª—è–µ–º–∞—è —Å—Ö–µ–º–æ–π. –û–Ω–∞ —Å–ø–æ
[31.12.2024 09:10] Trying to get texts in Chinese.
[31.12.2024 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Computer Vision (CV) has yet to fully achieve the zero-shot task generalization observed in Natural Language Processing (NLP), despite following many of the milestones established in NLP, such as large transformer models, extensive pre-training, and the auto-regression paradigm, among others. In this paper, we explore the idea that CV adopts discrete and terminological task definitions (\eg, ``image segmentation''), which may be a key barrier to zero-shot task generalization. Our hypothesis is that without truly understanding previously-seen tasks--due to these terminological definitions--deep models struggle to generalize to novel tasks. To verify this, we introduce Explanatory Instructions, which provide an intuitive way to define CV task objectives through detailed linguistic transformations from input images to outputs. We create a large-scale dataset comprising 12 million ``image input to explanatory instruction to output'' triplets, and train an auto-regressive-based vision-language model (AR-based VLM) that takes both images and explanatory instructions as input. By learning to follow these instructions, the AR-based VLM achieves instruction-level zero-shot capabilities for previously-seen tasks and demonstrates strong zero-shot generalization for unseen CV tasks. Code and dataset will be openly available on our GitHub repository.
[31.12.2024 09:10] Mistral response. {"id": "7efd28c9d125466db28b9a0aefb32715", "object": "chat.completion", "created": 1735636217, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CV\uff09\u5c1a\u672a\u5b8c\u5168\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u89c2\u5bdf\u5230\u7684\u96f6\u6837\u672c\u4efb\u52a1\u6cdb\u5316\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0cCV \u4f7f\u7528\u79bb\u6563\u548c\u672f\u8bed\u5316\u7684\u4efb\u52a1\u5b9a\u4e49\uff08\u5982\u201c\u56fe\u50cf\u5206\u5272\u201d\uff09\uff0c\u8fd9\u53ef\u80fd\u662f\u96f6\u6837\u672c\u4efb\u52a1\u6cdb\u5316\u7684\u4e3b\u8981\u969c\u788d\u3002\u4e3a\u9a8c\u8bc1\u8fd9\u4e00\u70b9\uff0c\u4f5c\u8005\u5f15\u5165\u4e86\u89e3\u91ca\u6027\u6307\u4ee4\uff0c\u901a\u8fc7\u8be6\u7ec6\u7684\u8bed\u8a00\u8f6c\u6362\u6765\u5b9a\u4e49 CV \u4efb\u52a1\u76ee\u6807\u3002\u4ed6\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 1200 \u4e07\u4e2a\u201c\u56fe\u50cf\u8f93\u5165\u5230\u89e3\u91ca\u6027\u6307\u4ee4\u5230\u8f93\u51fa\u201d\u4e09\u5143\u7ec4\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u81ea\u56de\u5f52\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08AR-based VLM\uff09\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5b66\u4e60\u9075\u5faa\u8fd9\u4e9b\u6307\u4ee4\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u4efb\u52a1\u6cdb\u5316\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 312, "total_tokens": 545, "completion_tokens": 233}}
[31.12.2024 09:10] Response: ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜËÆ°ÁÆóÊú∫ËßÜËßâÔºàCVÔºâÂ∞öÊú™ÂÆåÂÖ®ÂÆûÁé∞Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâ‰∏≠ËßÇÂØüÂà∞ÁöÑÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñ„ÄÇ‰ΩúËÄÖËÆ§‰∏∫ÔºåCV ‰ΩøÁî®Á¶ªÊï£ÂíåÊúØËØ≠ÂåñÁöÑ‰ªªÂä°ÂÆö‰πâÔºàÂ¶Ç‚ÄúÂõæÂÉèÂàÜÂâ≤‚ÄùÔºâÔºåËøôÂèØËÉΩÊòØÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñÁöÑ‰∏ªË¶ÅÈöúÁ¢ç„ÄÇ‰∏∫È™åËØÅËøô‰∏ÄÁÇπÔºå‰ΩúËÄÖÂºïÂÖ•‰∫ÜËß£ÈáäÊÄßÊåá‰ª§ÔºåÈÄöËøáËØ¶ÁªÜÁöÑËØ≠Ë®ÄËΩ¨Êç¢Êù•ÂÆö‰πâ CV ‰ªªÂä°ÁõÆÊ†á„ÄÇ‰ªñ‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´ 1200 ‰∏á‰∏™‚ÄúÂõæÂÉèËæìÂÖ•Âà∞Ëß£ÈáäÊÄßÊåá‰ª§Âà∞ËæìÂá∫‚Äù‰∏âÂÖÉÁªÑÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂπ∂ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™Ëá™ÂõûÂΩíËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàAR-based VLMÔºâ„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂ≠¶‰π†ÈÅµÂæ™Ëøô‰∫õÊåá‰ª§ÔºåÂÆûÁé∞‰∫ÜÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñ„ÄÇ
[31.12.2024 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜËÆ°ÁÆóÊú∫ËßÜËßâÔºàCVÔºâÂ∞öÊú™ÂÆåÂÖ®ÂÆûÁé∞Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâ‰∏≠ËßÇÂØüÂà∞ÁöÑÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñ„ÄÇ‰ΩúËÄÖËÆ§‰∏∫ÔºåCV ‰ΩøÁî®Á¶ªÊï£ÂíåÊúØËØ≠ÂåñÁöÑ‰ªªÂä°ÂÆö‰πâÔºàÂ¶Ç‚ÄúÂõæÂÉèÂàÜÂâ≤‚ÄùÔºâÔºåËøôÂèØËÉΩÊòØÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñÁöÑ‰∏ªË¶ÅÈöúÁ¢ç„ÄÇ‰∏∫È™åËØÅËøô‰∏ÄÁÇπÔºå‰ΩúËÄÖÂºïÂÖ•‰∫ÜËß£ÈáäÊÄßÊåá‰ª§ÔºåÈÄöËøáËØ¶ÁªÜÁöÑËØ≠Ë®ÄËΩ¨Êç¢Êù•ÂÆö‰πâ CV ‰ªªÂä°ÁõÆÊ†á„ÄÇ‰ªñ‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´ 1200 ‰∏á‰∏™‚ÄúÂõæÂÉèËæìÂÖ•Âà∞Ëß£ÈáäÊÄßÊåá‰ª§Âà∞ËæìÂá∫‚Äù‰∏âÂÖÉÁªÑÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂπ∂ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™Ëá™ÂõûÂΩíËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàAR-based VLMÔºâ„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂ≠¶‰π†ÈÅµÂæ™Ëøô‰∫õÊåá‰ª§ÔºåÂÆûÁé∞‰∫ÜÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñ„ÄÇ
[31.12.2024 09:10] Mistral response. {"id": "5717bcedc9ff447cad153b50a17fef88", "object": "chat.completion", "created": 1735636223, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CV\uff09\u5c1a\u672a\u5b8c\u5168\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u89c2\u5bdf\u5230\u7684\u96f6\u6837\u672c\u4efb\u52a1\u6cdb\u5316\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0cCV \u4f7f\u7528\u79bb\u6563\u548c\u672f\u8bed\u5316\u7684\u4efb\u52a1\u5b9a\u4e49\uff08\u5982\u201c\u56fe\u50cf\u5206\u5272\u201d\uff09\uff0c\u8fd9\u53ef\u80fd\u662f\u96f6\u6837\u672c\u4efb\u52a1\u6cdb\u5316\u7684\u4e3b\u8981\u969c\u788d\u3002\u4e3a\u9a8c\u8bc1\u8fd9\u4e00\u70b9\uff0c\u4f5c\u8005\u5f15\u5165\u4e86\u89e3\u91ca\u6027\u6307\u4ee4\uff0c\u901a\u8fc7\u8be6\u7ec6\u7684\u8bed\u8a00\u8f6c\u6362\u6765\u5b9a\u4e49 CV \u4efb\u52a1\u76ee\u6807\u3002\u4ed6\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 1200 \u4e07\u4e2a\u201c\u56fe\u50cf\u8f93\u5165\u5230\u89e3\u91ca\u6027\u6307\u4ee4\u5230\u8f93\u51fa\u201d\u4e09\u5143\u7ec4\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u81ea\u56de\u5f52\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08AR-based VLM\uff09\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5b66\u4e60\u9075\u5faa\u8fd9\u4e9b\u6307\u4ee4\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u4efb\u52a1\u6cdb\u5316\u3002\n\nzh\u00e8 pi\u0101n w\u00e9n zh\u0101ng t\u01ceo l\u00f9n le j\u00ec su\u00e0n j\u012b sh\u00ec ji\u00e0o (CV) sh\u00e0ng w\u00e8i qu\u00e1n qu\u00e1n sh\u00ed xi\u00e0n z\u00ec r\u00e1n y\u01d4 y\u00e1n ch\u01d4 l\u01d0 (NLP) zh\u014dng gu\u0101n ch\u00e1 d\u00e0o de l\u00edng y\u00e0ng b\u01cen zh\u00ec f\u00e0n zh\u00e8ng. zu\u00f2 zh\u011b r\u00e8n w\u00e9i, CV sh\u01d0 y\u00f2ng l\u00ed s\u00e0n h\u00e9 sh\u00f9 y\u01d4 hu\u00e0 de r\u00e8n w\u00f9 d\u00ecng y\u00ec (r\u00fa \u201ct\u00fa xi\u00e0ng f\u0113n g\u00e9\u201d), zh\u00e8 k\u011b n\u00e9ng sh\u00ec l\u00edng y\u00e0ng b\u01cen zh\u00ec f\u00e0n zh\u00e8ng de zh\u01d4 y\u00e0o zh\u00e0ng \u00e0i. w\u00e8i y\u00e0n zh\u00e8ng zh\u00e8 y\u012b di\u01cen, zu\u00f2 zh\u011b y\u01d0n r\u00f9 le ji\u011b sh\u00ec x\u00ecng zh\u01d0 l\u00ecng, t\u014dng gu\u00f2 xi\u00e1ng x\u00ec de y\u01d4 y\u00e1n zhu\u01cen hu\u00e0n l\u00e1i d\u00ecng y\u00ec CV r\u00e8n w\u00f9 m\u00f9 bi\u0101o. t\u0101 men chu\u00e0ng ji\u00e0n le y\u012b g\u00e8 b\u0101o h\u00e1n 1200 w\u00e0n g\u00e8 \u201ct\u00fa xi\u00e0ng sh\u016b r\u00f9 d\u00e0o ji\u011b sh\u00ec x\u00ecng zh\u01d0 l\u00ecng d\u00e0o sh\u016b ch\u016b\u201d s\u0101n yu\u00e1n z\u01d4 de d\u00e0 gu\u012b m\u00f3 sh\u00f9 j\u00ed, b\u00ecng x\u00f9n li\u00e0n le y\u012b g\u00e8 z\u00ec hu\u00ed gu\u012b sh\u00ec ji\u00e0n y\u01d4 y\u00e1n m\u00f3 x\u00edng (AR-based VLM). g\u0101i m\u00f3 x\u00edng t\u014dng gu\u00f2 xu\u00e9 x\u00ed z\u016bn x\u00fan zh\u00e8 xi\u0113 zh\u01d0 l\u00ecng, sh\u00ed xi\u00e0n le l\u00edng y\u00e0ng b\u01cen zh\u00ec f\u00e0n zh\u00e8ng."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 248, "total_tokens": 956, "completion_tokens": 708}}
[31.12.2024 09:10] Response: ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜËÆ°ÁÆóÊú∫ËßÜËßâÔºàCVÔºâÂ∞öÊú™ÂÆåÂÖ®ÂÆûÁé∞Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâ‰∏≠ËßÇÂØüÂà∞ÁöÑÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñ„ÄÇ‰ΩúËÄÖËÆ§‰∏∫ÔºåCV ‰ΩøÁî®Á¶ªÊï£ÂíåÊúØËØ≠ÂåñÁöÑ‰ªªÂä°ÂÆö‰πâÔºàÂ¶Ç‚ÄúÂõæÂÉèÂàÜÂâ≤‚ÄùÔºâÔºåËøôÂèØËÉΩÊòØÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñÁöÑ‰∏ªË¶ÅÈöúÁ¢ç„ÄÇ‰∏∫È™åËØÅËøô‰∏ÄÁÇπÔºå‰ΩúËÄÖÂºïÂÖ•‰∫ÜËß£ÈáäÊÄßÊåá‰ª§ÔºåÈÄöËøáËØ¶ÁªÜÁöÑËØ≠Ë®ÄËΩ¨Êç¢Êù•ÂÆö‰πâ CV ‰ªªÂä°ÁõÆÊ†á„ÄÇ‰ªñ‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´ 1200 ‰∏á‰∏™‚ÄúÂõæÂÉèËæìÂÖ•Âà∞Ëß£ÈáäÊÄßÊåá‰ª§Âà∞ËæìÂá∫‚Äù‰∏âÂÖÉÁªÑÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂπ∂ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™Ëá™ÂõûÂΩíËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàAR-based VLMÔºâ„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂ≠¶‰π†ÈÅµÂæ™Ëøô‰∫õÊåá‰ª§ÔºåÂÆûÁé∞‰∫ÜÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñ„ÄÇ

zh√® piƒÅn w√©n zhƒÅng t«éo l√πn le j√¨ su√†n jƒ´ sh√¨ ji√†o (CV) sh√†ng w√®i qu√°n qu√°n sh√≠ xi√†n z√¨ r√°n y«î y√°n ch«î l«ê (NLP) zh≈çng guƒÅn ch√° d√†o de l√≠ng y√†ng b«én zh√¨ f√†n zh√®ng. zu√≤ zhƒõ r√®n w√©i, CV sh«ê y√≤ng l√≠ s√†n h√© sh√π y«î hu√† de r√®n w√π d√¨ng y√¨ (r√∫ ‚Äút√∫ xi√†ng fƒìn g√©‚Äù), zh√® kƒõ n√©ng sh√¨ l√≠ng y√†ng b«én zh√¨ f√†n zh√®ng de zh«î y√†o zh√†ng √†i. w√®i y√†n zh√®ng zh√® yƒ´ di«én, zu√≤ zhƒõ y«ên r√π le jiƒõ sh√¨ x√¨ng zh«ê l√¨ng, t≈çng gu√≤ xi√°ng x√¨ de y«î y√°n zhu«én hu√†n l√°i d√¨ng y√¨ CV r√®n w√π m√π biƒÅo. tƒÅ men chu√†ng ji√†n le yƒ´ g√® bƒÅo h√°n 1200 w√†n g√® ‚Äút√∫ xi√†ng sh≈´ r√π d√†o jiƒõ sh√¨ x√¨ng zh«ê l√¨ng d√†o sh≈´ ch≈´‚Äù sƒÅn yu√°n z«î de d√† guƒ´ m√≥ sh√π j√≠, b√¨ng x√πn li√†n le yƒ´ g√® z√¨ hu√≠ guƒ´ sh√¨ ji√†n y«î y√°n m√≥ x√≠ng (AR-based VLM). gƒÅi m√≥ x√≠ng t≈çng gu√≤ xu√© x√≠ z≈´n x√∫n zh√® xiƒì zh«ê l√¨ng, sh√≠ xi√†n le l√≠ng y√†ng b«én zh√¨ f√†n zh√®ng.
[31.12.2024 09:10] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜËÆ°ÁÆóÊú∫ËßÜËßâÔºàCVÔºâÂ∞öÊú™ÂÆåÂÖ®ÂÆûÁé∞Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâ‰∏≠ËßÇÂØüÂà∞ÁöÑÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñ„ÄÇ‰ΩúËÄÖËÆ§‰∏∫ÔºåCV ‰ΩøÁî®Á¶ªÊï£ÂíåÊúØËØ≠ÂåñÁöÑ‰ªªÂä°ÂÆö‰πâÔºàÂ¶Ç‚ÄúÂõæÂÉèÂàÜÂâ≤‚ÄùÔºâÔºåËøôÂèØËÉΩÊòØÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñÁöÑ‰∏ªË¶ÅÈöúÁ¢ç„ÄÇ‰∏∫È™åËØÅËøô‰∏ÄÁÇπÔºå‰ΩúËÄÖÂºïÂÖ•‰∫ÜËß£ÈáäÊÄßÊåá‰ª§ÔºåÈÄöËøáËØ¶ÁªÜÁöÑËØ≠Ë®ÄËΩ¨Êç¢Êù•ÂÆö‰πâ CV ‰ªªÂä°ÁõÆÊ†á„ÄÇ‰ªñ‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´ 1200 ‰∏á‰∏™‚ÄúÂõæÂÉèËæìÂÖ•Âà∞Ëß£ÈáäÊÄßÊåá‰ª§Âà∞ËæìÂá∫‚Äù‰∏âÂÖÉÁªÑÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂπ∂ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™Ëá™ÂõûÂΩíËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàAR-based VLMÔºâ„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂ≠¶‰π†ÈÅµÂæ™Ëøô‰∫õÊåá‰ª§ÔºåÂÆûÁé∞‰∫ÜÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñ„ÄÇ
[31.12.2024 09:10] Mistral response. {"id": "a31e65601b6e4a2ba6c7a1b7f7a2c17a", "object": "chat.completion", "created": 1735636239, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u8ba8\u8bba\", \"pinyin\": \"t\u01ceo l\u00f9n\", \"trans\": \"discuss\"},\n    {\"word\": \"\u8ba1\u7b97\u673a\u89c6\u89c9\", \"pinyin\": \"j\u00ec su\u00e0n j\u012b sh\u00ec ju\u00e9\", \"trans\": \"computer vision\"},\n    {\"word\": \"\u5c1a\u672a\", \"pinyin\": \"sh\u00e0ng w\u00e8i\", \"trans\": \"not yet\"},\n    {\"word\": \"\u5b8c\u5168\", \"pinyin\": \"w\u00e1n qu\u00e1n\", \"trans\": \"completely\"},\n    {\"word\": \"\u5b9e\u73b0\", \"pinyin\": \"sh\u00ed xi\u00e0n\", \"trans\": \"achieve\"},\n    {\"word\": \"\u81ea\u7136\u8bed\u8a00\u5904\u7406\", \"pinyin\": \"z\u00ec r\u00e1n y\u01d4 y\u00e1n ch\u01d4 l\u01d0\", \"trans\": \"natural language processing\"},\n    {\"word\": \"\u89c2\u5bdf\", \"pinyin\": \"gu\u0101n ch\u00e1\", \"trans\": \"observe\"},\n    {\"word\": \"\u96f6\u6837\u672c\u4efb\u52a1\u6cdb\u5316\", \"pinyin\": \"l\u00edng y\u00e0ng b\u011bn r\u00e8n w\u00f9 f\u00e0n hu\u00e0\", \"trans\": \"zero-shot task generalization\"},\n    {\"word\": \"\u969c\u788d\", \"pinyin\": \"zh\u00e0ng \u00e0i\", \"trans\": \"obstacle\"},\n    {\"word\": \"\u5f15\u5165\", \"pinyin\": \"y\u01d0n r\u00f9\", \"trans\": \"introduce\"},\n    {\"word\": \"\u89e3\u91ca\u6027\", \"pinyin\": \"ji\u011b sh\u00ec x\u00ecng\", \"trans\": \"explanatory\"},\n    {\"word\": \"\u6307\u4ee4\", \"pinyin\": \"zh\u01d0 l\u00ecng\", \"trans\": \"instruction\"},\n    {\"word\": \"\u8be6\u7ec6\", \"pinyin\": \"xi\u00e1ng x\u00ec\", \"trans\": \"detailed\"},\n    {\"word\": \"\u8f6c\u6362\", \"pinyin\": \"zhu\u01cen hu\u00e0n\", \"trans\": \"convert\"},\n    {\"word\": \"\u76ee\u6807\", \"pinyin\": \"m\u00f9 bi\u0101o\", \"trans\": \"goal\"},\n    {\"word\": \"\u521b\u5efa\", \"pinyin\": \"chu\u00e0ng ji\u00e0n\", \"trans\": \"create\"},\n    {\"word\": \"\u6570\u636e\u96c6\", \"pinyin\": \"sh\u00f9 j\u00f9 j\u00ed\", \"trans\": \"dataset\"},\n    {\"word\": \"\u8bad\u7ec3\", \"pinyin\": \"x\u00f9n li\u00e0n\", \"trans\": \"train\"},\n    {\"word\": \"\u81ea\u56de\u5f52\", \"pinyin\": \"z\u00ec hu\u00ed gu\u012b\", \"trans\": \"autoregressive\"},\n    {\"word\": \"\u6a21\u578b\", \"pinyin\": \"m\u00f3 x\u00edng\", \"trans\": \"model\"},\n    {\"word\": \"\u9075\u5faa\", \"pinyin\": \"z\u016bn x\u00f9n\", \"trans\": \"follow\"},\n    {\"word\": \"\u4e09\u5143\u7ec4\", \"pinyin\": \"s\u0101n yu\u00e1n z\u01d4\", \"trans\": \"triplet\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 280, "total_tokens": 975, "completion_tokens": 695}}
[31.12.2024 09:10] Response: [
    {"word": "ËÆ®ËÆ∫", "pinyin": "t«éo l√πn", "trans": "discuss"},
    {"word": "ËÆ°ÁÆóÊú∫ËßÜËßâ", "pinyin": "j√¨ su√†n jƒ´ sh√¨ ju√©", "trans": "computer vision"},
    {"word": "Â∞öÊú™", "pinyin": "sh√†ng w√®i", "trans": "not yet"},
    {"word": "ÂÆåÂÖ®", "pinyin": "w√°n qu√°n", "trans": "completely"},
    {"word": "ÂÆûÁé∞", "pinyin": "sh√≠ xi√†n", "trans": "achieve"},
    {"word": "Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ", "pinyin": "z√¨ r√°n y«î y√°n ch«î l«ê", "trans": "natural language processing"},
    {"word": "ËßÇÂØü", "pinyin": "guƒÅn ch√°", "trans": "observe"},
    {"word": "Èõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñ", "pinyin": "l√≠ng y√†ng bƒõn r√®n w√π f√†n hu√†", "trans": "zero-shot task generalization"},
    {"word": "ÈöúÁ¢ç", "pinyin": "zh√†ng √†i", "trans": "obstacle"},
    {"word": "ÂºïÂÖ•", "pinyin": "y«ên r√π", "trans": "introduce"},
    {"word": "Ëß£ÈáäÊÄß", "pinyin": "jiƒõ sh√¨ x√¨ng", "trans": "explanatory"},
    {"word": "Êåá‰ª§", "pinyin": "zh«ê l√¨ng", "trans": "instruction"},
    {"word": "ËØ¶ÁªÜ", "pinyin": "xi√°ng x√¨", "trans": "detailed"},
    {"word": "ËΩ¨Êç¢", "pinyin": "zhu«én hu√†n", "trans": "convert"},
    {"word": "ÁõÆÊ†á", "pinyin": "m√π biƒÅo", "trans": "goal"},
    {"word": "ÂàõÂª∫", "pinyin": "chu√†ng ji√†n", "trans": "create"},
    {"word": "Êï∞ÊçÆÈõÜ", "pinyin": "sh√π j√π j√≠", "trans": "dataset"},
    {"word": "ËÆ≠ÁªÉ", "pinyin": "x√πn li√†n", "trans": "train"},
    {"word": "Ëá™ÂõûÂΩí", "pinyin": "z√¨ hu√≠ guƒ´", "trans": "autoregressive"},
    {"word": "Ê®°Âûã", "pinyin": "m√≥ x√≠ng", "trans": "model"},
    {"word": "ÈÅµÂæ™", "pinyin": "z≈´n x√πn", "trans": "follow"},
    {"word": "‰∏âÂÖÉÁªÑ", "pinyin": "sƒÅn yu√°n z«î", "trans": "triplet"}
]
[31.12.2024 09:10] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜËÆ°ÁÆóÊú∫ËßÜËßâÔºàCVÔºâÂ∞öÊú™ÂÆåÂÖ®ÂÆûÁé∞Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâ‰∏≠ËßÇÂØüÂà∞ÁöÑÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñ„ÄÇ‰ΩúËÄÖËÆ§‰∏∫ÔºåCV ‰ΩøÁî®Á¶ªÊï£ÂíåÊúØËØ≠ÂåñÁöÑ‰ªªÂä°ÂÆö‰πâÔºàÂ¶Ç‚ÄúÂõæÂÉèÂàÜÂâ≤‚ÄùÔºâÔºåËøôÂèØËÉΩÊòØÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñÁöÑ‰∏ªË¶ÅÈöúÁ¢ç„ÄÇ‰∏∫È™åËØÅËøô‰∏ÄÁÇπÔºå‰ΩúËÄÖÂºïÂÖ•‰∫ÜËß£ÈáäÊÄßÊåá‰ª§ÔºåÈÄöËøáËØ¶ÁªÜÁöÑËØ≠Ë®ÄËΩ¨Êç¢Êù•ÂÆö‰πâ CV ‰ªªÂä°ÁõÆÊ†á„ÄÇ‰ªñ‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´ 1200 ‰∏á‰∏™‚ÄúÂõæÂÉèËæìÂÖ•Âà∞Ëß£ÈáäÊÄßÊåá‰ª§Âà∞ËæìÂá∫‚Äù‰∏âÂÖÉÁªÑÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂπ∂ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™Ëá™ÂõûÂΩíËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàAR-based VLMÔºâ„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂ≠¶‰π†ÈÅµÂæ™Ëøô‰∫õÊåá‰ª§ÔºåÂÆûÁé∞‰∫ÜÈõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñ„ÄÇ
[31.12.2024 09:10] Mistral response. {"id": "53a3e2f21a15452e9748b570b17a8399", "object": "chat.completion", "created": 1735636247, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article discusses the fact that computer vision (CV) has not fully achieved zero-shot task generalization as observed in natural language processing (NLP). The authors argue that CV's use of discrete and terminological task definitions (such as \"image segmentation\") may be a major obstacle to zero-shot task generalization. To validate this, the authors introduce explanatory instructions, defining CV task objectives through detailed language translations. They created a large-scale dataset containing 12 million \"image input to explanatory instruction to output\" triplets and trained an autoregressive vision-language model (AR-based VLM). The model achieved zero-shot task generalization by learning to follow these instructions."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 246, "total_tokens": 394, "completion_tokens": 148}}
[31.12.2024 09:10] Response: This article discusses the fact that computer vision (CV) has not fully achieved zero-shot task generalization as observed in natural language processing (NLP). The authors argue that CV's use of discrete and terminological task definitions (such as "image segmentation") may be a major obstacle to zero-shot task generalization. To validate this, the authors introduce explanatory instructions, defining CV task objectives through detailed language translations. They created a large-scale dataset containing 12 million "image input to explanatory instruction to output" triplets and trained an autoregressive vision-language model (AR-based VLM). The model achieved zero-shot task generalization by learning to follow these instructions.
[31.12.2024 09:10] Renaming data file.
[31.12.2024 09:10] Renaming previous data. hf_papers.json to ./d/2024-12-31.json
[31.12.2024 09:10] Saving new data file.
[31.12.2024 09:10] Generating page.
[31.12.2024 09:10] Renaming previous page.
[31.12.2024 09:10] Renaming previous data. index.html to ./d/2024-12-31.html
[31.12.2024 09:10] [Experimental] Generating Chinese page for reading.
[31.12.2024 09:10] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'ËÆ°ÁÆóÊú∫ËßÜËßâ', 'pinyin': 'j√¨ su√†n jƒ´ sh√¨ ju√©', 'trans': 'computer vision'}, {'word': 'Â∞öÊú™', 'pinyin': 'sh√†ng w√®i', 'trans': 'not yet'}, {'word': 'ÂÆåÂÖ®', 'pinyin': 'w√°n qu√°n', 'trans': 'completely'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}, {'word': 'Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ', 'pinyin': 'z√¨ r√°n y«î y√°n ch«î l«ê', 'trans': 'natural language processing'}, {'word': 'ËßÇÂØü', 'pinyin': 'guƒÅn ch√°', 'trans': 'observe'}, {'word': 'Èõ∂Ê†∑Êú¨‰ªªÂä°Ê≥õÂåñ', 'pinyin': 'l√≠ng y√†ng bƒõn r√®n w√π f√†n hu√†', 'trans': 'zero-shot task generalization'}, {'word': 'ÈöúÁ¢ç', 'pinyin': 'zh√†ng √†i', 'trans': 'obstacle'}, {'word': 'ÂºïÂÖ•', 'pinyin': 'y«ên r√π', 'trans': 'introduce'}, {'word': 'Ëß£ÈáäÊÄß', 'pinyin': 'jiƒõ sh√¨ x√¨ng', 'trans': 'explanatory'}, {'word': 'Êåá‰ª§', 'pinyin': 'zh«ê l√¨ng', 'trans': 'instruction'}, {'word': 'ËØ¶ÁªÜ', 'pinyin': 'xi√°ng x√¨', 'trans': 'detailed'}, {'word': 'ËΩ¨Êç¢', 'pinyin': 'zhu«én hu√†n', 'trans': 'convert'}, {'word': 'ÁõÆÊ†á', 'pinyin': 'm√π biƒÅo', 'trans': 'goal'}, {'word': 'ÂàõÂª∫', 'pinyin': 'chu√†ng ji√†n', 'trans': 'create'}, {'word': 'Êï∞ÊçÆÈõÜ', 'pinyin': 'sh√π j√π j√≠', 'trans': 'dataset'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πn li√†n', 'trans': 'train'}, {'word': 'Ëá™ÂõûÂΩí', 'pinyin': 'z√¨ hu√≠ guƒ´', 'trans': 'autoregressive'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥ x√≠ng', 'trans': 'model'}, {'word': 'ÈÅµÂæ™', 'pinyin': 'z≈´n x√πn', 'trans': 'follow'}, {'word': '‰∏âÂÖÉÁªÑ', 'pinyin': 'sƒÅn yu√°n z«î', 'trans': 'triplet'}]
[31.12.2024 09:10] Renaming previous Chinese page.
[31.12.2024 09:10] Renaming previous data. zh.html to ./d/2024-12-30_zh_reading_task.html
[31.12.2024 09:10] Writing Chinese reading task.
[31.12.2024 09:10] Writing result.
[31.12.2024 09:10] Renaming log file.
[31.12.2024 09:10] Renaming previous data. log.txt to ./logs/2024-12-31_last_log.txt
