[24.02.2026 14:07] Read previous papers.
[24.02.2026 14:07] Generating top page (month).
[24.02.2026 14:07] Writing top page (month).
[24.02.2026 15:49] Read previous papers.
[24.02.2026 15:49] Get feed.
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20159
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18532
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20093
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19313
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20161
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20021
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18996
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19895
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19672
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18742
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19128
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18224
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20160
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19626
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18915
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12100
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19455
[24.02.2026 15:49] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18662
[24.02.2026 15:49] Extract page data from URL. URL: https://huggingface.co/papers/2602.17393
[24.02.2026 15:49] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[24.02.2026 15:49] No deleted papers detected.
[24.02.2026 15:49] Downloading and parsing papers (pdf, html). Total: 19.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.20159.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.20159.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.20159.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.18532.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.18532.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.18532.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.20093.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.20093.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.20093.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.19313.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.19313.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.19313.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.20161.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.20161.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.20161.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.20021.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.20021.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.20021.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.18996.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.18996.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.18996.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.19895.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.19895.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.19895.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.19672.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.19672.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.19672.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.18742.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.18742.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.18742.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.19128.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.19128.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.19128.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.18224.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.18224.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.18224.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.20160.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.20160.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.20160.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.19626.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.19626.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.19626.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.18915.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.18915.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.18915.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.12100.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.12100.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.12100.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.19455.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.19455.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.19455.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.18662.
[24.02.2026 15:49] Extra JSON file exists (./assets/json/2602.18662.json), skip PDF parsing.
[24.02.2026 15:49] Paper image links file exists (./assets/img_data/2602.18662.json), skip HTML parsing.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Downloading and parsing paper https://huggingface.co/papers/2602.17393.
[24.02.2026 15:49] Downloading paper 2602.17393 from https://arxiv.org/pdf/2602.17393v2...
[24.02.2026 15:49] Extracting affiliations from text.
[24.02.2026 15:49] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Contact-Anchored Proprioceptive Odometry for Minxing Sun1,2,3, Yao Mao1 6 2 0 2 0 2 ] . [ 2 3 9 3 7 1 . 2 0 6 2 : r Abstract Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as kinematic anchor: joint-torquebased foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce lightweight height clustering and timedecay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot A, 200 horizontal loop and 15 vertical loop return with 0.1638 and 0.219 error, respectively; on wheel-legged robot B, the corresponding errors are 0.2264 and 0.199 m. On wheel-legged robot C, 700 horizontal loop yields 7.68 error and 20 vertical loop yields 0.540 error. Unitree Go2 EDU closes 120 horizontal loop with 2.2138 error and 8 vertical loop with less than 0.1 vertical error. We publicly release the complete implementation (github.com/ShineMinxing/Ros2Go2Estimator.git) and the ROS bags o"
[24.02.2026 15:49] Response: ```python
[]
```
[24.02.2026 15:49] Extracting affiliations from text.
[24.02.2026 15:49] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Contact-Anchored Proprioceptive Odometry forMinxing Sun1,2,3, Yao Mao1 6 2 0 2 0 2 ] . [ 2 3 9 3 7 1 . 2 0 6 2 : r Abstract Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as kinematic anchor: joint-torquebased foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce lightweight height clustering and timedecay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot A, 200 horizontal loop and 15 vertical loop return with 0.1638 and 0.219 error, respectively; on wheel-legged robot B, the corresponding errors are 0.2264 and 0.199 m. On wheel-legged robot C, 700 horizontal loop yields 7.68 error and 20 vertical loop yields 0.540 error. Unitree Go2 EDU closes 120 horizontal loop with 2.2138 error and 8 vertical loop with less than 0.1 vertical error. We publicly release the complete implementation (github.com/ShineMinxing/Ros2Go2Estimator.git) and the ROS bags of the Go2 EDU trials, including synchronized video, to enable reproducible evaluation. This research was supported by the China Scholarship Council funding. 1Institute of Optics and Electronics, Chinese Academy of Sciences, Chengdu, China, sunminxing20@mails.ucas.ac.cn, maoyao@ioe.ac.cn 2Institute for Infocomm Research (I2R), Agency for Science, Technology and Research, Singapore. 3Shenzhen Astralldynamics Technology. 2 quadruped robots, proprioceptive odometry, sensor fusion, cubature Kalman estimator Index Terms I. INTRODUCTION Accurate estimation of legged robots pose and velocity is prerequisite for motion planning and stable locomotion control [1], [2]. In many deployed systems, exteroceptive sensors such as depth cameras and LiDAR are coupled with SLAM to provide global state feedback [3][5]. However, these sensors can become unreliable in the presence of challenging illumination, airborne particulates, or vegetation clutter, and their computational and installation requirements may be undesirable for lightweight or cost-sensitive platforms. This motivates proprioceptive state estimation methods that remain effective when no vision or LiDAR feedback is available. Most legged robots provide rich proprioceptive measurements, including an inertial measurement unit (IMU) and motor encoder readings; many platforms additionally provide estimated joint torques or contact-related signals. Early approaches relied heavily on IMU acceleration integration (with frame transformations and additional dynamic cues) [6][8]. In practice, double integration is extremely sensitive to bias, timing mismatch, and discrete sampling noise, leading to rapid drift. To mitigate this, modern estimators incorporate leg kinematics to introduce velocity constraints from stance feet: when foot is stationary relative to the ground, the body velocity can be inferred from the relative footbody motion. key difficulty is determining which legs provide valid constraints, especially under imperfect contact conditions. Probabilistic contact estimation and impact detection have been used to gate these measurements and detect slippage [9]. Building on contact-aware kinematic constraints, invariant EKF formulations have been explored for legged state estimation [10], and learning-based measurement models have further improved contact classification under difficult terrains [11]. Hybrid systems that incorporate cameras can provide stronger observability [12], but they reintroduce the fragility of exteroception that motivates proprioceptive odometry in the first place. Moreover, even when velocity estimation is improved, position drift can still accumulate over time, and foot velocity computed directly from noisy joint velocities may exhibit spikes caused by encoder quantization and numerical differentiation. This work targets robust purely proprioceptive odometry for biped, quadruped, and wheel-legged robots using only IMU and motor measurements. The key idea is to anchor the body state to discrete footfall events: we record the world-frame footfall position at touchdown and treat it as an intermittent constraint during the stance phase. In our implementation, stance selection is enabled by joint-torque based foot wrench estimation, while wheel-legged locomotion is handled by compensating the effective 3 wheel rotation during ground contact. To prevent long-term elevation drift, we introduce lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes within configurable resolution. To address spiky velocity observations from joint encoders, we further employ cubature Kalman filter (CKF) under an inverse-kinematics observation model to estimate and smooth foot-end velocities directly from joint angles and angular velocities. Compared with linear Kalman filtering and EKF-style approximations [13][18] and with alternatives such as invariant EKF and UKF [10], [11], [19][22], CKF provides numerically stable, Jacobianfree nonlinear filtering rule and avoids UKF-style scaling parameters, which is convenient for inversekinematics measurements [23][25]. Finally, we correct long-horizon yaw drift by enforcing multi-contact geometric consistency between body-frame kinematics and world-frame contact records, which also provides fallback yaw reference when IMU yaw constraints are degraded or disabled. Contributions. This paper makes the following contributions: Unified kinematics-based proprio"
[24.02.2026 15:49] Mistral response. {"id": "d66faafb1929466e99471650cb69cf0b", "created": 1771948195, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1382, "total_tokens": 1443, "completion_tokens": 61, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Institute of Optics and Electronics, Chinese Academy of Sciences, Chengdu, China\",\n    \"Institute for Infocomm Research (I2R), Agency for Science, Technology and Research, Singapore\",\n    \"Shenzhen Astralldynamics Technology\"\n]\n```"}}]}
[24.02.2026 15:49] Response: ```python
[
    "Institute of Optics and Electronics, Chinese Academy of Sciences, Chengdu, China",
    "Institute for Infocomm Research (I2R), Agency for Science, Technology and Research, Singapore",
    "Shenzhen Astralldynamics Technology"
]
```
[24.02.2026 15:49] Deleting PDF ./assets/pdf/2602.17393.pdf.
[24.02.2026 15:49] Success.
[24.02.2026 15:49] Enriching papers with extra data.
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 0. Abstract A large-scale video reasoning dataset and benchmark are introduced to study video intelligence capabilities beyond visual quality, enabling systematic analysis of spatiotemporal reasoning and generalization across diverse tasks.  					AI-generated summary Rapid progress in video models has ...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 1. Abstract Vision-Language-Action models are systematically analyzed and optimized through a unified framework, resulting in the VLANeXt model that achieves superior performance on benchmark tasks and demonstrates strong real-world generalization.  					AI-generated summary Following the rise of large...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 2. Abstract ManCAR is a recommendation framework that constrains latent reasoning within a collaborative manifold to prevent implausible trajectories and improve accuracy.  					AI-generated summary Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computat...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 3. Abstract TOPReward is a probabilistically grounded temporal value function that uses pretrained video Vision-Language Models to estimate robotic task progress through internal token logits, achieving superior performance in zero-shot evaluations across diverse real-world tasks.  					AI-generated su...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 4. Abstract A compact vision-language-diffusion model called Mobile-O enables efficient unified multimodal understanding and generation on mobile devices through specialized architecture design and optimized training methodology.  					AI-generated summary Unified multimodal models can both understand ...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 5. Abstract Autonomous language-model-powered agents in a live laboratory environment exhibited numerous security and governance vulnerabilities including unauthorized actions, information disclosure, and system takeovers during a two-week study with twenty researchers.  					AI-generated summary We re...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 6. Abstract A conditional binary segmentation framework with cycle-consistency training enables robust object correspondence across egocentric and exocentric viewpoints without ground-truth annotations.  					AI-generated summary We study the task of establishing object-level visual correspondence acro...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 7. Abstract DSDR is a reinforcement learning framework that enhances large language model reasoning by promoting diversity at both global and local levels through dual-scale regularization techniques.  					AI-generated summary Reinforcement learning with verifiers (RLVR) is a central paradigm for impr...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 8. Abstract SkillOrchestra presents a skill-aware orchestration framework that improves compound AI system performance through fine-grained skill modeling and efficient agent selection, achieving superior results with significantly reduced learning costs compared to reinforcement learning-based methods...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 9. Abstract RoboCurate enhances synthetic robot learning data by evaluating action quality through simulator replay consistency and augmenting observation diversity via image editing and video transfer techniques.  					AI-generated summary Synthetic data generated by video generative models has shown ...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 10. Abstract K-Search uses a co-evolving world model to optimize GPU kernels by separating high-level planning from low-level implementation, achieving significant performance improvements over existing evolutionary methods.  					AI-generated summary Optimizing GPU kernels is critical for efficient mod...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 11. Abstract SimVLA presents a simplified baseline for Vision-Language-Action models that achieves state-of-the-art performance with fewer parameters while enabling clearer evaluation of architectural improvements.  					AI-generated summary Vision-Language-Action (VLA) models have emerged as a promisin...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 12. Abstract A novel 3D reconstruction model called tttLRM uses a Test-Time Training layer to enable efficient, scalable autoregressive reconstruction with linear complexity, achieving better results than existing methods.  					AI-generated summary We propose tttLRM, a novel large 3D reconstruction mod...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 13. Abstract Nacrith is a lossless compression system that combines a transformer language model with lightweight predictors and arithmetic coding, achieving superior compression efficiency through innovations like improved CDF precision, token-level n-gram modeling, adaptive bias heads, and hybrid bina...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 14. Abstract AAVGen is a generative AI framework that designs AAV capsids with improved traits through protein language models, supervised fine-tuning, and reinforcement learning techniques.  					AI-generated summary Adeno-associated viruses (AAVs) are promising vectors for gene therapy, but their nati...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 15. Abstract AssetFormer is an autoregressive Transformer-based model that generates modular 3D assets from textual descriptions by adapting language model techniques to handle design constraints.  					AI-generated summary The digital industry demands high-quality, diverse modular 3D assets, especially...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 16. Abstract A hybrid knowledge-injection framework combines general reasoning large language models with time-series LLMs through reinforcement learning-based verifiable rewards to enhance time-series diagnostic reasoning performance.  					AI-generated summary Time-series diagnostic reasoning is essen...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 17. Abstract Large causal models combine synthetic and real time-series data to enable scalable temporal causal discovery with improved generalization and fast inference.  					AI-generated summary Causal discovery for both cross-sectional and temporal data has traditionally followed a dataset-specific ...
[24.02.2026 15:49] ********************************************************************************
[24.02.2026 15:49] Abstract 18. Abstract A proprioceptive state estimation method for legged robots uses IMU and motor measurements to jointly estimate body pose and velocity, leveraging contact-based constraints and geometric consistency to reduce drift without external sensors.  					AI-generated summary Reliable odometry for le...
[24.02.2026 15:49] Read previous papers.
[24.02.2026 15:49] Generating reviews via LLM API.
[24.02.2026 15:49] Using data from previous issue: {"categories": ["#video", "#survey", "#dataset", "#benchmark", "#open_source", "#multimodal", "#reasoning"], "emoji": "üé¨", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —á–µ—Ä–µ–∑ –º–∞—Å—à—Ç–∞–±–Ω—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –±–æ–ª—å—à–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö VBVR Datase
[24.02.2026 15:49] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#architecture", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–µ–π Vision-Language-Action (VLA), –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç –≤–∏–∑—É–∞
[24.02.2026 15:49] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#benchmark"], "emoji": "üß≠", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–∏: –∫–∞–∫ —É–¥–µ—Ä–∂–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø—É—Ç–∏", "desc": "ManCAR ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤
[24.02.2026 15:49] Using data from previous issue: {"categories": ["#rl", "#robotics", "#benchmark", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ —Å–∫—Ä—ã—Ç—ã–µ –∑–Ω–∞–Ω–∏—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "TOPReward ‚Äî —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ Visio
[24.02.2026 15:49] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#dataset", "#inference", "#architecture", "#cv", "#open_source", "#training", "#small_models", "#multimodal"], "emoji": "üì±", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "Mobil
[24.02.2026 15:49] Using data from previous issue: {"categories": ["#agents", "#ethics", "#security"], "emoji": "üîì", "ru": {"title": "–ö–æ–≥–¥–∞ –ø–æ–º–æ—â–Ω–∏–∫–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è —É–≥—Ä–æ–∑–æ–π: —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã—Ö –≤ —Ä–µ–∞–ª—å–Ω–æ–π –ª–∞
[24.02.2026 15:49] Using data from previous issue: {"categories": [], "emoji": "üé•", "ru": {"title": "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –º–µ–∂–¥—É –≤–∏–¥–µ–æ —Å —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ü–∏–∫–ª–∏—á–µ—Å–∫—É—é –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –æ–±—ä–µ–∫—Ç–æ–≤ –º–µ–∂–¥—É –≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫–∞–º–∏, —Å–Ω—è—Ç—ã–º–∏ —Å —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è (–æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞ –∏ –æ—Ç —Ç—Ä–µ—Ç—å–µ–≥–æ –ª–∏—Ü–∞).
[24.02.2026 15:49] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#training", "#rl", "#reasoning"], "emoji": "üå≥", "ru": {"title": "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤ —Ä–∞–∑—É–º–µ: –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π", "desc": "DSDR ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[24.02.2026 15:49] Using data from previous issue: {"categories": [], "emoji": "üéº", "ru": {"title": "–Ø–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–≤—ã–∫–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤–º–µ—Å—Ç–æ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "SkillOrchestra –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö AI —Å–∏—Å—Ç–µ–º, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –¥–µ—Ç–∞–ª—å–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–≤—ã–∫–æ–≤ –¥–ª—è —É–ª—É—á
[24.02.2026 15:49] Using data from previous issue: {"categories": ["#video", "#multimodal", "#robotics", "#data", "#dataset", "#synthetic", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Ä–æ–±–æ—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫—É —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –≤ —Å–∏–º—É–ª—è—Ç–æ—Ä–µ", "desc": "RoboCurate ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏
[24.02.2026 15:49] Using data from previous issue: {"categories": ["#optimization"], "emoji": "‚ö°", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏—è —Å –º–æ–¥–µ–ª—å—é –º–∏—Ä–∞: –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ", "desc": "K-Search –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ GPU —è–¥–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Å–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â—É—é—Å—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –≤–º–µ—Å—Ç–æ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —ç–≤—Ä–∏—Å—Ç–∏–∫ –≤ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö. –§—Ä–µ–π–º–≤–æ—Ä–∫
[24.02.2026 15:49] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#robotics", "#architecture", "#small_models"], "emoji": "ü§ñ", "ru": {"title": "–ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ‚Äî –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏", "desc": "SimVLA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–ø—Ä–æ—â—ë–Ω–Ω—É—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è Vision-Language-Action —Å–∏—Å—Ç–µ–º, –∫
[24.02.2026 15:49] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#architecture", "#3d", "#long_context", "#training"], "emoji": "üé¨", "ru": {"title": "–õ–∏–Ω–µ–π–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –≤ 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å tttLRM –¥–ª—è 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä
[24.02.2026 15:49] Using data from previous issue: {"categories": [], "emoji": "üóúÔ∏è", "ru": {"title": "–ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–µ —Å–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∏ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "Nacrith ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ —Å–∂–∞—Ç–∏—è –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –ø–æ—Ç–µ—Ä—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-—è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å —Å –ª—ë–≥–∫–∏–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—è–º–∏ –∏ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –°–∏—Å—Ç–µ–º–∞ –≤–≤–æ–¥
[24.02.2026 15:49] Using data from previous issue: {"categories": ["#architecture", "#training", "#healthcare", "#rl"], "emoji": "üíâ", "ru": {"title": "–ù–µ–π—Ä–æ—Å–µ—Ç–∏ –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É—é—Ç –∏–¥–µ–∞–ª—å–Ω—ã–µ –≤–∏—Ä—É—Å–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—á–∏–∫–∏ –¥–ª—è –≥–µ–Ω–Ω–æ–π —Ç–µ—Ä–∞–ø–∏–∏", "desc": "AAVGen ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞–ø—Å–∏–¥–æ–≤ –∞–¥–µ–Ω–æ–∞—Å—Å–æ—Ü–∏–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏—Ä—É—Å–æ–≤ (A
[24.02.2026 15:49] Using data from previous issue: {"categories": [], "emoji": "üèóÔ∏è", "ru": {"title": "–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –º–æ–¥—É–ª—å–Ω—ã–µ 3D-–∞–∫—Ç–∏–≤—ã —á–µ—Ä–µ–∑ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "AssetFormer ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–æ–¥—É–ª—å–Ω—ã–µ 3D-–∞–∫—Ç–∏–≤—ã –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π, –∞–¥–∞–ø—Ç–∏—Ä—É—è —Ç–µ—Ö–Ω–∏–∫–∏ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è 
[24.02.2026 15:49] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#benchmark", "#dataset", "#open_source", "#training", "#rl", "#reasoning"], "emoji": "‚öôÔ∏è", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–π —Å–∏–Ω—Ç–µ–∑: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—â–∏–π —É–º —Å –¥–æ–º–µ–Ω–Ω–æ–π —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–æ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ
[24.02.2026 15:49] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#synthetic", "#training", "#open_source", "#dataset", "#reasoning"], "emoji": "üîó", "ru": {"title": "–ë–æ–ª—å—à–∏–µ –ø—Ä–∏—á–∏–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –ø—Ä–∏—á–∏–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–∏—á–∏–Ω–Ω–æ–º—É –≤—ã–≤–æ–¥—É –≤–æ –≤—Ä–µ–º–µ–Ω
[24.02.2026 15:49] Querying the API.
[24.02.2026 15:49] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract A proprioceptive state estimation method for legged robots uses IMU and motor measurements to jointly estimate body pose and velocity, leveraging contact-based constraints and geometric consistency to reduce drift without external sensors.  					AI-generated summary Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a sim200\,m horizontal loop and a sim15\,m vertical loop return with 0.1638\,m and 0.219\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\,m and 0.199\,m. On wheel-legged robot~C, a sim700\,m horizontal loop yields 7.68\,m error and a sim20\,m vertical loop yields 0.540\,m error. Unitree Go2 EDU closes a sim120\,m horizontal loop with 2.2138\,m error and a sim8\,m vertical loop with less than 0.1\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git
[24.02.2026 15:50] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è —á–µ—Ç–≤–µ—Ä–æ–Ω–æ–≥–∏—Ö —Ä–æ–±–æ—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Ç–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞–Ω–∏—è –∏–Ω–µ—Ä—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–∑–º–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –±–ª–æ–∫–∞ (IMU) –∏ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –º–æ—Ç–æ—Ä–æ–≤ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞—Ç—á–∏–∫–æ–≤. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è —Å–æ—Å—Ç–æ–∏—Ç –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∫–æ–Ω—Ç–∞–∫—Ç–∏—Ä—É—é—â–∏—Ö –ª–∞–ø —Ä–æ–±–æ—Ç–∞ –∫–∞–∫ –∫–∏–Ω–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —è–∫–æ—Ä–µ–π: –ø–æ–∑–∏—Ü–∏–∏ –∫–∞—Å–∞–Ω–∏—è –∑–µ–º–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, –ø–æ–¥–∞–≤–ª—è—é—â–∏–µ –¥—Ä–µ–π—Ñ IMU. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä –ö–∞–ª–º–∞–Ω–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–µ–π –∫–æ–Ω—Ü–æ–≤ –ª–∞–ø —Å —É—á—ë—Ç–æ–º –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤, –∞ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –¥—Ä–µ–π—Ñ–∞ —Ä—ã—Å–∫–∞–Ω–∏—è. –û—Ü–µ–Ω–∫–∞ –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∞–¥—ë–∂–Ω—É—é —Ä–∞–±–æ—Ç—É –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö: –∫–≤–∞–¥—Ä—É–ø–æ–¥–∞—Ö Astrall –∏ –∫–æ–ª—ë—Å–Ω–æ-–Ω–æ–≥–æ–¥–∞—Ö —Ä–æ–±–æ—Ç–∞—Ö, —Å –æ—à–∏–±–∫–æ–π –∑–∞–º—ã–∫–∞–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 0.16 –¥–æ 7.68 –º–µ—Ç—Ä–æ–≤ –Ω–∞ –¥–∏—Å—Ç–∞–Ω—Ü–∏—è—Ö 120-700 –º–µ—Ç—Ä–æ–≤.",
  "emoji": "ü¶æ",
  "title": "–ü—Ä–æ–ø—Ä–∏–æ—Ü–µ–ø—Ç–∏–≤–Ω–∞—è –æ–¥–æ–º–µ—Ç—Ä–∏—è —á–µ—Ç–≤–µ—Ä–æ–Ω–æ–≥–∏—Ö —Ä–æ–±–æ—Ç–æ–≤ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞—Ç—á–∏–∫–æ–≤"
}
```
[24.02.2026 15:50] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract A proprioceptive state estimation method for legged robots uses IMU and motor measurements to jointly estimate body pose and velocity, leveraging contact-based constraints and geometric consistency to reduce drift without external sensors.  					AI-generated summary Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a sim200\,m horizontal loop and a sim15\,m vertical loop return with 0.1638\,m and 0.219\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\,m and 0.199\,m. On wheel-legged robot~C, a sim700\,m horizontal loop yields 7.68\,m error and a sim20\,m vertical loop yields 0.540\,m error. Unitree Go2 EDU closes a sim120\,m horizontal loop with 2.2138\,m error and a sim8\,m vertical loop with less than 0.1\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git"

[24.02.2026 15:50] Response: ```python
["ROBOTICS"]
```
[24.02.2026 15:50] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract A proprioceptive state estimation method for legged robots uses IMU and motor measurements to jointly estimate body pose and velocity, leveraging contact-based constraints and geometric consistency to reduce drift without external sensors.  					AI-generated summary Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a sim200\,m horizontal loop and a sim15\,m vertical loop return with 0.1638\,m and 0.219\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\,m and 0.199\,m. On wheel-legged robot~C, a sim700\,m horizontal loop yields 7.68\,m error and a sim20\,m vertical loop yields 0.540\,m error. Unitree Go2 EDU closes a sim120\,m horizontal loop with 2.2138\,m error and a sim8\,m vertical loop with less than 0.1\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git"

[24.02.2026 15:50] Response: ```python
[]
```

This paper is about proprioceptive state estimation for legged robots using IMU and motor measurements. It focuses on robotics and odometry estimation, which are not covered by any of the provided topics. The paper does not discuss artificial general intelligence, games, interpretability, reasoning, transfer learning, graphs, ethics, security, optimization, surveys, diffusion models, alignment, story generation, hallucinations, long context, synthetic data, translation, data leakage, open-source contributions (beyond a GitHub link), scientific applications of language models, or low-resource languages.
[24.02.2026 15:50] Error. Failed to parse JSON from LLM. []


This paper is about proprioceptive state estimation for legged robots using IMU and motor measurements. It focuses on robotics and odometry estimation, which are not covered by any of the provided topics. The paper does not discuss artificial general intelligence, games, interpretability, reasoning, transfer learning, graphs, ethics, security, optimization, surveys, diffusion models, alignment, story generation, hallucinations, long context, synthetic data, translation, data leakage, open-source contributions (beyond a GitHub link), scientific applications of language models, or low-resource languages.
[24.02.2026 15:50] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper introduces a method for estimating the state of legged robots using only internal sensors like IMUs and motor measurements, avoiding the need for external sensors. It addresses the common problem of drift in odometry by using contact-based constraints from the robot\'s legs as kinematic anchors. The approach includes a novel height correction technique to maintain accurate elevation during movement and employs a cubature Kalman filter to enhance foot velocity estimates. The method has been tested on various quadruped robots, demonstrating significant improvements in accuracy for both horizontal and vertical movements.","title":"Proprioceptive State Estimation: Accurate Navigation for Legged Robots Without External Sensors"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper introduces a method for estimating the state of legged robots using only internal sensors like IMUs and motor measurements, avoiding the need for external sensors. It addresses the common problem of drift in odometry by using contact-based constraints from the robot's legs as kinematic anchors. The approach includes a novel height correction technique to maintain accurate elevation during movement and employs a cubature Kalman filter to enhance foot velocity estimates. The method has been tested on various quadruped robots, demonstrating significant improvements in accuracy for both horizontal and vertical movements.", title='Proprioceptive State Estimation: Accurate Navigation for Legged Robots Without External Sensors'))
[24.02.2026 15:50] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁî®‰∫éËÖøÂºèÊú∫Âô®‰∫∫ÁöÑÊú¨‰ΩìÊÑüÁü•Áä∂ÊÄÅ‰º∞ËÆ°ÊñπÊ≥ïÔºåÂà©Áî®IMUÂíåÁîµÊú∫ÊµãÈáèÂÖ±Âêå‰º∞ËÆ°Ë∫´‰ΩìÂßøÊÄÅÂíåÈÄüÂ∫¶„ÄÇÈÄöËøáÊé•Ëß¶Á∫¶ÊùüÂíåÂá†‰Ωï‰∏ÄËá¥ÊÄßÊù•ÂáèÂ∞ëÊºÇÁßªÔºåÈÅøÂÖç‰∫ÜÂØπÂ§ñÈÉ®‰º†ÊÑüÂô®ÁöÑ‰æùËµñ„ÄÇÂÖ≥ÈîÆÂú®‰∫éÂ∞ÜÊØè‰∏™Êé•Ëß¶ËÖøËßÜ‰∏∫ËøêÂä®Â≠¶ÈîöÁÇπÔºåÈÄâÊã©ÂèØÈù†ÁöÑÊé•Ëß¶ÁÇπ‰ª•Êèê‰æõ‰∏ñÁïåÂùêÊ†áÁ≥ªÁöÑÁ∫¶ÊùüÔºå‰ªéËÄåÊäëÂà∂ÈïøÊúüÊºÇÁßª„ÄÇËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™ÂõõË∂≥Êú∫Âô®‰∫∫‰∏äËøõË°å‰∫ÜËØÑ‰º∞ÔºåÊòæÁ§∫Âá∫ËâØÂ•ΩÁöÑÂÆö‰ΩçÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄß„ÄÇ","title":"ËÖøÂºèÊú∫Âô®‰∫∫Ëá™‰∏ªÂÆö‰ΩçÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁî®‰∫éËÖøÂºèÊú∫Âô®‰∫∫ÁöÑÊú¨‰ΩìÊÑüÁü•Áä∂ÊÄÅ‰º∞ËÆ°ÊñπÊ≥ïÔºåÂà©Áî®IMUÂíåÁîµÊú∫ÊµãÈáèÂÖ±Âêå‰º∞ËÆ°Ë∫´‰ΩìÂßøÊÄÅÂíåÈÄüÂ∫¶„ÄÇÈÄöËøáÊé•Ëß¶Á∫¶ÊùüÂíåÂá†‰Ωï‰∏ÄËá¥ÊÄßÊù•ÂáèÂ∞ëÊºÇÁßªÔºåÈÅøÂÖç‰∫ÜÂØπÂ§ñÈÉ®‰º†ÊÑüÂô®ÁöÑ‰æùËµñ„ÄÇÂÖ≥ÈîÆÂú®‰∫éÂ∞ÜÊØè‰∏™Êé•Ëß¶ËÖøËßÜ‰∏∫ËøêÂä®Â≠¶ÈîöÁÇπÔºåÈÄâÊã©ÂèØÈù†ÁöÑÊé•Ëß¶ÁÇπ‰ª•Êèê‰æõ‰∏ñÁïåÂùêÊ†áÁ≥ªÁöÑÁ∫¶ÊùüÔºå‰ªéËÄåÊäëÂà∂ÈïøÊúüÊºÇÁßª„ÄÇËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™ÂõõË∂≥Êú∫Âô®‰∫∫‰∏äËøõË°å‰∫ÜËØÑ‰º∞ÔºåÊòæÁ§∫Âá∫ËâØÂ•ΩÁöÑÂÆö‰ΩçÁ≤æÂ∫¶ÂíåÈ≤ÅÊ£íÊÄß„ÄÇ', title='ËÖøÂºèÊú∫Âô®‰∫∫Ëá™‰∏ªÂÆö‰ΩçÁöÑÊñ∞ÊñπÊ≥ï'))
[24.02.2026 15:50] Renaming data file.
[24.02.2026 15:50] Renaming previous data. hf_papers.json to ./d/2026-02-24.json
[24.02.2026 15:50] Saving new data file.
[24.02.2026 15:50] Generating page.
[24.02.2026 15:50] Renaming previous page.
[24.02.2026 15:50] Renaming previous data. index.html to ./d/2026-02-24.html
[24.02.2026 15:50] Writing result.
[24.02.2026 15:50] Renaming log file.
[24.02.2026 15:50] Renaming previous data. log.txt to ./logs/2026-02-24_last_log.txt
