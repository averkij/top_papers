[24.02.2026 19:46] Read previous papers.
[24.02.2026 19:46] Generating top page (month).
[24.02.2026 19:46] Writing top page (month).
[24.02.2026 20:27] Read previous papers.
[24.02.2026 20:27] Get feed.
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20159
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18532
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19672
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19313
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20161
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20093
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20021
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18996
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19895
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16863
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18742
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16872
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19320
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19128
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19626
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18224
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20160
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18915
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18640
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12100
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.19455
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18662
[24.02.2026 20:27] Extract page data from URL. URL: https://huggingface.co/papers/2602.18333
[24.02.2026 20:27] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17393
[24.02.2026 20:27] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[24.02.2026 20:27] No deleted papers detected.
[24.02.2026 20:27] Downloading and parsing papers (pdf, html). Total: 24.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.20159.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.20159.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.20159.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.18532.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.18532.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.18532.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.19672.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.19672.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.19672.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.19313.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.19313.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.19313.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.20161.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.20161.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.20161.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.20093.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.20093.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.20093.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.20021.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.20021.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.20021.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.18996.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.18996.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.18996.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.19895.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.19895.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.19895.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.16863.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.16863.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.16863.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.18742.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.18742.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.18742.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.16872.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.16872.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.16872.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.19320.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.19320.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.19320.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.19128.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.19128.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.19128.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.19626.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.19626.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.19626.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.18224.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.18224.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.18224.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.20160.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.20160.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.20160.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.18915.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.18915.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.18915.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.18640.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.18640.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.18640.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.12100.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.12100.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.12100.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.19455.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.19455.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.19455.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.18662.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.18662.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.18662.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.18333.
[24.02.2026 20:27] Downloading paper 2602.18333 from https://arxiv.org/pdf/2602.18333v1...
[24.02.2026 20:27] Extracting affiliations from text.
[24.02.2026 20:27] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"On the Induction Bias in Sequence Models M.Reza Ebrahimi 1 Micha√´l Defferrard 1 Sunny Panchal 1 Roland Memisevic 1 6 2 0 2 0 2 ] . [ 1 3 3 3 8 1 . 2 0 6 2 : r a "
[24.02.2026 20:27] Response: ```python
[]
```
[24.02.2026 20:27] Extracting affiliations from text.
[24.02.2026 20:27] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"On the Induction Bias in Sequence Models M.Reza Ebrahimi 1 Micha√´l Defferrard 1 Sunny Panchal 1 Roland Memisevic 1 6 2 0 2 0 2 ] . [ 1 3 3 3 8 1 . 2 0 6 2 : r aDespite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct largescale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with statespace size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains fundamental challenge for transformers, even when training and evaluation distributions match. 1. Introduction State tracking is key capability of most intelligent systems and of most models of computation. It is the process of monitoring and updating the status of an entity or process with which the system interacts over period of time. State tracking is particularly important in multi-hop, interactive tasks, such as that of an agent interacting with an interface or of dialogue system interacting with user across multiple turns. State tracking has become popular area of investigation in recent years, especially in the study of LLM capabilities and failure modes. In this context, numerous studies have shown that transformer-based models are fundamentally limited in their ability to perform state tracking (for example, (Anil et al., 2022; Dziri et al., 2024)). This contrasts with recurrent networks, which excel at state tracking (although their wide-spread applicability is unfortunately hampered by their relative training inefficiency). The limitations of transformers have been demonstrated as limitations in outof-distribution (OOD) generalization, specifically lengthgeneralization: after training models on tasks encoded in sequences of given range of lengths, they were evaluated on sequences with lengths that were not seen during training. In these scenarios, the trained models fail to consistently generate correct outputs on the evaluation data, although they are able to solve the tasks for even unseen sequences in the training range of lengths. It could be argued that in any real-world use cases, OOD state tracking failures may not be an issue as long as enough training data with step-by-step sequential supervision is available. If training data covers all sequence lengths that may be encountered at inference time, inference can rely entirely on in-distribution generalization. Unfortunately, although this argument is true in principle, it is hard to quantify enough in this context. It is also hard to quantify how the amount of training data required for any given task may depend on the length of the sequences and the size of the state space. To shed light on these questions, in this work we perform detailed and systematic empirical study of the in-distribution performance of transformer-based models and contrast these with recurrent models. To this end, we train and evaluate range of representative models on range of simple state tracking tasks. Independently varying sequence length and size of the state space in these tasks then allows us to discover regularities in the dependence of generalization error on these parameters. This in turn makes it possible to obtain crude lower bound of the minimal amount of training data likely required to solve such tasks. 1Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. Correspondence to: Reza Ebrahimi <ebrahimi@qualcomm.com>. key difference between transformer-based and recurrent models is that, at every timestep, the former compute outputs by applying function that depends on all inputs and 1 On the Induction Bias in Sequence Models outputs generated previously (the context window), making it possible, in principle, to re-calculate the required state from the past information globally at each time step. Recurrent networks, on the other hand, compute outputs by applying function that depends on only the current hidden state, making it impossible to perform such re-calculation. This makes it strictly necessary for recurrent network to encode any relevant information from the past within single hidden state vector. This inductive bias encourages recurrent network to incorporate the information from the current timestep into its representation of state at the moment where this information is available. Conversely, it discourages it from saving this information off to determine future state updates globally from the past information. The immediate state updates thus encourage the network to process the input sequence step-by-step, making any state update explicit as soon as this is possible, rather than potentially deferring such updates to later point in time. Such step-by-step state updates are natural inductive bias (Mitchell, 1997) in the context of simple state tracking tasks, as they make it possible to reduce complex multi-step dependencies to sequence of single-step computations. They also allow model to share weights across multiple different sequence lengths, as it breaks state updates into single-step, repeatable computations. By analogy to the induction step in mathematical proof, we shall refer to this kind of inductive bias in this work as induction bias (sic). We show that the degree of knowledge transfer across multiple different sequence lengths in the indistribution setting is highly correlated with the ability of model to length-generalize. 1.1. Related work range of studies has shown that transformer-based sequence models fail to length-generalize in state tracking tasks (Anil et al., 2022; Deletang et al., 2023; Dziri et al"
[24.02.2026 20:27] Mistral response. {"id": "41ffe1a8e9d045a183001bfb153a719e", "created": 1771964865, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1298, "total_tokens": 1309, "completion_tokens": 11, "prompt_tokens_details": {"cached_tokens": 0}}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Qualcomm AI Research\"]\n```"}}]}
[24.02.2026 20:27] Response: ```python
["Qualcomm AI Research"]
```
[24.02.2026 20:27] Deleting PDF ./assets/pdf/2602.18333.pdf.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Downloading and parsing paper https://huggingface.co/papers/2602.17393.
[24.02.2026 20:27] Extra JSON file exists (./assets/json/2602.17393.json), skip PDF parsing.
[24.02.2026 20:27] Paper image links file exists (./assets/img_data/2602.17393.json), skip HTML parsing.
[24.02.2026 20:27] Success.
[24.02.2026 20:27] Enriching papers with extra data.
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 0. Abstract A large-scale video reasoning dataset and benchmark are introduced to study video intelligence capabilities beyond visual quality, enabling systematic analysis of spatiotemporal reasoning and generalization across diverse tasks.  					AI-generated summary Rapid progress in video models has ...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 1. Abstract Vision-Language-Action models are systematically analyzed and optimized through a unified framework, resulting in the VLANeXt model that achieves superior performance on benchmark tasks and demonstrates strong real-world generalization.  					AI-generated summary Following the rise of large...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 2. Abstract SkillOrchestra presents a skill-aware orchestration framework that improves compound AI system performance through fine-grained skill modeling and efficient agent selection, achieving superior results with significantly reduced learning costs compared to reinforcement learning-based methods...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 3. Abstract TOPReward is a probabilistically grounded temporal value function that uses pretrained video Vision-Language Models to estimate robotic task progress through internal token logits, achieving superior performance in zero-shot evaluations across diverse real-world tasks.  					AI-generated su...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 4. Abstract A compact vision-language-diffusion model called Mobile-O enables efficient unified multimodal understanding and generation on mobile devices through specialized architecture design and optimized training methodology.  					AI-generated summary Unified multimodal models can both understand ...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 5. Abstract ManCAR is a recommendation framework that constrains latent reasoning within a collaborative manifold to prevent implausible trajectories and improve accuracy.  					AI-generated summary Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computat...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 6. Abstract Autonomous language-model-powered agents in a live laboratory environment exhibited numerous security and governance vulnerabilities including unauthorized actions, information disclosure, and system takeovers during a two-week study with twenty researchers.  					AI-generated summary We re...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 7. Abstract A conditional binary segmentation framework with cycle-consistency training enables robust object correspondence across egocentric and exocentric viewpoints without ground-truth annotations.  					AI-generated summary We study the task of establishing object-level visual correspondence acro...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 8. Abstract DSDR is a reinforcement learning framework that enhances large language model reasoning by promoting diversity at both global and local levels through dual-scale regularization techniques.  					AI-generated summary Reinforcement learning with verifiers (RLVR) is a central paradigm for impr...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 9. Abstract SimToolReal enables generalizable robot manipulation of diverse tools through procedural simulation and universal reinforcement learning policies without task-specific training.  					AI-generated summary The ability to manipulate tools significantly expands the set of tasks a robot can per...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 10. Abstract RoboCurate enhances synthetic robot learning data by evaluating action quality through simulator replay consistency and augmenting observation diversity via image editing and video transfer techniques.  					AI-generated summary Synthetic data generated by video generative models has shown ...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 11. Abstract Diffusion models are adapted for optical character recognition by using block discrete diffusion to enable faster, parallel processing while maintaining high accuracy.  					AI-generated summary Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 12. Abstract Agentic memory systems for LLM agents face empirical challenges including inadequate benchmarks, misaligned metrics, and performance variability that limit their practical effectiveness.  					AI-generated summary Agentic memory systems enable large language model (LLM) agents to maintain s...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 13. Abstract K-Search uses a co-evolving world model to optimize GPU kernels by separating high-level planning from low-level implementation, achieving significant performance improvements over existing evolutionary methods.  					AI-generated summary Optimizing GPU kernels is critical for efficient mod...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 14. Abstract Nacrith is a lossless compression system that combines a transformer language model with lightweight predictors and arithmetic coding, achieving superior compression efficiency through innovations like improved CDF precision, token-level n-gram modeling, adaptive bias heads, and hybrid bina...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 15. Abstract SimVLA presents a simplified baseline for Vision-Language-Action models that achieves state-of-the-art performance with fewer parameters while enabling clearer evaluation of architectural improvements.  					AI-generated summary Vision-Language-Action (VLA) models have emerged as a promisin...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 16. Abstract A novel 3D reconstruction model called tttLRM uses a Test-Time Training layer to enable efficient, scalable autoregressive reconstruction with linear complexity, achieving better results than existing methods.  					AI-generated summary We propose tttLRM, a novel large 3D reconstruction mod...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 17. Abstract AAVGen is a generative AI framework that designs AAV capsids with improved traits through protein language models, supervised fine-tuning, and reinforcement learning techniques.  					AI-generated summary Adeno-associated viruses (AAVs) are promising vectors for gene therapy, but their nati...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 18. Abstract GEARS presents a framework that reframes ranking optimization as an autonomous discovery process using specialized agent skills and validation hooks to balance algorithmic signals with ranking context while ensuring production reliability.  					AI-generated summary Modern large-scale ranki...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 19. Abstract AssetFormer is an autoregressive Transformer-based model that generates modular 3D assets from textual descriptions by adapting language model techniques to handle design constraints.  					AI-generated summary The digital industry demands high-quality, diverse modular 3D assets, especially...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 20. Abstract A hybrid knowledge-injection framework combines general reasoning large language models with time-series LLMs through reinforcement learning-based verifiable rewards to enhance time-series diagnostic reasoning performance.  					AI-generated summary Time-series diagnostic reasoning is essen...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 21. Abstract Large causal models combine synthetic and real time-series data to enable scalable temporal causal discovery with improved generalization and fast inference.  					AI-generated summary Causal discovery for both cross-sectional and temporal data has traditionally followed a dataset-specific ...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 22. Abstract Transformers require exponentially more training data than RNNs for state tracking tasks and fail to share learned mechanisms across different sequence lengths, while RNNs demonstrate effective amortized learning through weight sharing.  					AI-generated summary Despite the remarkable prac...
[24.02.2026 20:27] ********************************************************************************
[24.02.2026 20:27] Abstract 23. Abstract A proprioceptive state estimation method for legged robots uses IMU and motor measurements to jointly estimate body pose and velocity, leveraging contact-based constraints and geometric consistency to reduce drift without external sensors.  					AI-generated summary Reliable odometry for le...
[24.02.2026 20:27] Read previous papers.
[24.02.2026 20:27] Generating reviews via LLM API.
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#video", "#survey", "#dataset", "#benchmark", "#open_source", "#multimodal", "#reasoning"], "emoji": "üé¨", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —á–µ—Ä–µ–∑ –º–∞—Å—à—Ç–∞–±–Ω—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –±–æ–ª—å—à–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö VBVR Datase
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#architecture", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–µ–π Vision-Language-Action (VLA), –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç –≤–∏–∑—É–∞
[24.02.2026 20:27] Using data from previous issue: {"categories": [], "emoji": "üéº", "ru": {"title": "–Ø–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–≤—ã–∫–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤–º–µ—Å—Ç–æ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "SkillOrchestra –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö AI —Å–∏—Å—Ç–µ–º, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –¥–µ—Ç–∞–ª—å–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–≤—ã–∫–æ–≤ –¥–ª—è —É–ª—É—á
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#rl", "#robotics", "#benchmark", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ —Å–∫—Ä—ã—Ç—ã–µ –∑–Ω–∞–Ω–∏—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "TOPReward ‚Äî —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ Visio
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#dataset", "#inference", "#architecture", "#cv", "#open_source", "#training", "#small_models", "#multimodal"], "emoji": "üì±", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "Mobil
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#benchmark"], "emoji": "üß≠", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–∏: –∫–∞–∫ —É–¥–µ—Ä–∂–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø—É—Ç–∏", "desc": "ManCAR ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#agents", "#ethics", "#security"], "emoji": "üîì", "ru": {"title": "–ö–æ–≥–¥–∞ –ø–æ–º–æ—â–Ω–∏–∫–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è —É–≥—Ä–æ–∑–æ–π: —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã—Ö –≤ —Ä–µ–∞–ª—å–Ω–æ–π –ª–∞
[24.02.2026 20:27] Using data from previous issue: {"categories": [], "emoji": "üé•", "ru": {"title": "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –º–µ–∂–¥—É –≤–∏–¥–µ–æ —Å —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ü–∏–∫–ª–∏—á–µ—Å–∫—É—é –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –æ–±—ä–µ–∫—Ç–æ–≤ –º–µ–∂–¥—É –≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫–∞–º–∏, —Å–Ω—è—Ç—ã–º–∏ —Å —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è (–æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞ –∏ –æ—Ç —Ç—Ä–µ—Ç—å–µ–≥–æ –ª–∏—Ü–∞).
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#training", "#rl", "#reasoning"], "emoji": "üå≥", "ru": {"title": "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤ —Ä–∞–∑—É–º–µ: –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π", "desc": "DSDR ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#robotics", "#rl", "#training"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–ª—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –±–µ–∑ –∑–∞–¥–∞—á–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "SimToolReal –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º –¥–ª—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#video", "#multimodal", "#robotics", "#data", "#dataset", "#synthetic", "#open_source"], "emoji": "ü§ñ", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Ä–æ–±–æ—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫—É —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ –≤ —Å–∏–º—É–ª—è—Ç–æ—Ä–µ", "desc": "RoboCurate ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏
[24.02.2026 20:27] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –±–ª–æ—á–Ω—ã–π –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—é —Ç–µ–∫—Å—Ç–∞ (OCR) –Ω–∞ –æ—Å–Ω–æ–≤–µ diffusion models, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç–∞—é—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤–º–µ—Å—Ç–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –¥–µ
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#agents", "#survey", "#long_context", "#benchmark", "#architecture", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –ø—Ä–æ–ø–∞—Å—Ç–∏ –º–µ–∂–¥—É —Ç–µ–æ—Ä–∏–µ–π –∏ –ø—Ä–∞–∫—Ç–∏–∫–æ–π –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –ø–∞–º—è—Ç–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–∏—Å—Ç–µ–º—ã –ø–∞–º—è—Ç–∏ –¥–ª—è LLM-–∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#optimization"], "emoji": "‚ö°", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏—è —Å –º–æ–¥–µ–ª—å—é –º–∏—Ä–∞: –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ", "desc": "K-Search –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ GPU —è–¥–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Å–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â—É—é—Å—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –≤–º–µ—Å—Ç–æ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —ç–≤—Ä–∏—Å—Ç–∏–∫ –≤ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö. –§—Ä–µ–π–º–≤–æ—Ä–∫
[24.02.2026 20:27] Using data from previous issue: {"categories": [], "emoji": "üóúÔ∏è", "ru": {"title": "–ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–µ —Å–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∏ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "Nacrith ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ —Å–∂–∞—Ç–∏—è –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –ø–æ—Ç–µ—Ä—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-—è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å —Å –ª—ë–≥–∫–∏–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—è–º–∏ –∏ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –°–∏—Å—Ç–µ–º–∞ –≤–≤–æ–¥
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#robotics", "#architecture", "#small_models"], "emoji": "ü§ñ", "ru": {"title": "–ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ‚Äî –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏", "desc": "SimVLA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–ø—Ä–æ—â—ë–Ω–Ω—É—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è Vision-Language-Action —Å–∏—Å—Ç–µ–º, –∫
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#architecture", "#3d", "#long_context", "#training"], "emoji": "üé¨", "ru": {"title": "–õ–∏–Ω–µ–π–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –≤ 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å tttLRM –¥–ª—è 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#architecture", "#training", "#healthcare", "#rl"], "emoji": "üíâ", "ru": {"title": "–ù–µ–π—Ä–æ—Å–µ—Ç–∏ –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É—é—Ç –∏–¥–µ–∞–ª—å–Ω—ã–µ –≤–∏—Ä—É—Å–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—á–∏–∫–∏ –¥–ª—è –≥–µ–Ω–Ω–æ–π —Ç–µ—Ä–∞–ø–∏–∏", "desc": "AAVGen ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞–ø—Å–∏–¥–æ–≤ –∞–¥–µ–Ω–æ–∞—Å—Å–æ—Ü–∏–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏—Ä—É—Å–æ–≤ (A
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#optimization", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "GEARS ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∑–∞–¥–∞—á—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏
[24.02.2026 20:27] Using data from previous issue: {"categories": [], "emoji": "üèóÔ∏è", "ru": {"title": "–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –º–æ–¥—É–ª—å–Ω—ã–µ 3D-–∞–∫—Ç–∏–≤—ã —á–µ—Ä–µ–∑ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "AssetFormer ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–æ–¥—É–ª—å–Ω—ã–µ 3D-–∞–∫—Ç–∏–≤—ã –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π, –∞–¥–∞–ø—Ç–∏—Ä—É—è —Ç–µ—Ö–Ω–∏–∫–∏ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è 
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#transfer_learning", "#optimization", "#benchmark", "#dataset", "#open_source", "#training", "#rl", "#reasoning"], "emoji": "‚öôÔ∏è", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–π —Å–∏–Ω—Ç–µ–∑: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—â–∏–π —É–º —Å –¥–æ–º–µ–Ω–Ω–æ–π —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–æ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#synthetic", "#training", "#open_source", "#dataset", "#reasoning"], "emoji": "üîó", "ru": {"title": "–ë–æ–ª—å—à–∏–µ –ø—Ä–∏—á–∏–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –ø—Ä–∏—á–∏–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–∏—á–∏–Ω–Ω–æ–º—É –≤—ã–≤–æ–¥—É –≤–æ –≤—Ä–µ–º–µ–Ω
[24.02.2026 20:27] Querying the API.
[24.02.2026 20:27] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract Transformers require exponentially more training data than RNNs for state tracking tasks and fail to share learned mechanisms across different sequence lengths, while RNNs demonstrate effective amortized learning through weight sharing.  					AI-generated summary Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.
[24.02.2026 20:27] Response: ```json
{
  "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥—É—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è. –û–Ω–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Ç—Ä–µ–±—É—é—Ç —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –±–æ–ª—å—à–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å RNN, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –º–µ—Ö–∞–Ω–∏–∑–º—ã –ø–µ—Ä–µ–Ω–æ—Å–∞ –≤–µ—Å–æ–≤ —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω—ã–µ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—é—Ç, —á—Ç–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –Ω–µ –¥–µ–ª—è—Ç—Å—è –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏ –º–µ–∂–¥—É –¥–ª–∏–Ω–∞–º–∏, —Ä–µ—à–∞—è –∑–∞–¥–∞—á—É –¥–ª—è –∫–∞–∂–¥–æ–π –¥–ª–∏–Ω—ã –æ—Ç–¥–µ–ª—å–Ω–æ. –ù–∞–ø—Ä–æ—Ç–∏–≤, —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∞–º–æ—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º –ª—É—á—à–µ –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –Ω–æ–≤—ã–µ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π.",
  "emoji": "üîÑ",
  "title": "–ü–æ—á–µ–º—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Ç–µ—Ä—è—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è: –ø—Ä–æ–±–ª–µ–º–∞ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–µ—Å–æ–≤"
}
```
[24.02.2026 20:27] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Transformers require exponentially more training data than RNNs for state tracking tasks and fail to share learned mechanisms across different sequence lengths, while RNNs demonstrate effective amortized learning through weight sharing.  					AI-generated summary Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match."

[24.02.2026 20:27] Response: ```python
["ARCHITECTURE", "TRAINING"]
```
[24.02.2026 20:27] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Transformers require exponentially more training data than RNNs for state tracking tasks and fail to share learned mechanisms across different sequence lengths, while RNNs demonstrate effective amortized learning through weight sharing.  					AI-generated summary Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match."

[24.02.2026 20:27] Response: ```python
['REASONING']
```
[24.02.2026 20:27] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper investigates the limitations of transformer models in state tracking tasks compared to recurrent neural networks (RNNs). It highlights that transformers require significantly more training data as the sequence length increases, while RNNs efficiently share learned mechanisms across different lengths. The study reveals that transformers struggle with out-of-distribution generalization and show poor weight sharing, leading to isolated learning for each sequence length. In contrast, RNNs benefit from amortized learning, allowing them to leverage data from various lengths to enhance overall performance.","title":"Transformers vs RNNs: The State Tracking Challenge"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates the limitations of transformer models in state tracking tasks compared to recurrent neural networks (RNNs). It highlights that transformers require significantly more training data as the sequence length increases, while RNNs efficiently share learned mechanisms across different lengths. The study reveals that transformers struggle with out-of-distribution generalization and show poor weight sharing, leading to isolated learning for each sequence length. In contrast, RNNs benefit from amortized learning, allowing them to leverage data from various lengths to enhance overall performance.', title='Transformers vs RNNs: The State Tracking Challenge'))
[24.02.2026 20:27] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂèòÊç¢Âô®ÔºàTransformersÔºâÂú®Áä∂ÊÄÅË∑üË∏™‰ªªÂä°‰∏≠ÁöÑÊï∞ÊçÆÊïàÁéáÈóÆÈ¢ò„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂèòÊç¢Âô®ÈúÄË¶ÅÊØîÈÄíÂΩíÁ•ûÁªèÁΩëÁªúÔºàRNNsÔºâÊõ¥Â§öÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂ∞§ÂÖ∂ÊòØÂú®Áä∂ÊÄÅÁ©∫Èó¥ÂíåÂ∫èÂàóÈïøÂ∫¶Â¢ûÂä†Êó∂„ÄÇÂèòÊç¢Âô®Âú®‰∏çÂêåÂ∫èÂàóÈïøÂ∫¶‰πãÈó¥ÁöÑÊùÉÈáçÂÖ±‰∫´ËÉΩÂäõËæÉÂ∑ÆÔºåÂØºËá¥ÂÆÉ‰ª¨Âú®Â≠¶‰π†Êó∂ÂæÄÂæÄÂè™ÈíàÂØπÁâπÂÆöÈïøÂ∫¶ËøõË°å‰ºòÂåñ„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåRNNsÈÄöËøáÊùÉÈáçÂÖ±‰∫´ÂÆûÁé∞‰∫ÜÊúâÊïàÁöÑÂ≠¶‰π†Ôºå‰ΩøÂæó‰∏çÂêåÂ∫èÂàóÈïøÂ∫¶ÁöÑÊï∞ÊçÆËÉΩÂ§üÁõ∏‰∫íÊèêÂçáÊÄßËÉΩ„ÄÇ","title":"ÂèòÊç¢Âô®Âú®Áä∂ÊÄÅË∑üË∏™‰∏≠ÁöÑÊåëÊàò‰∏éRNNÁöÑ‰ºòÂäø"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂèòÊç¢Âô®ÔºàTransformersÔºâÂú®Áä∂ÊÄÅË∑üË∏™‰ªªÂä°‰∏≠ÁöÑÊï∞ÊçÆÊïàÁéáÈóÆÈ¢ò„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂèòÊç¢Âô®ÈúÄË¶ÅÊØîÈÄíÂΩíÁ•ûÁªèÁΩëÁªúÔºàRNNsÔºâÊõ¥Â§öÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂ∞§ÂÖ∂ÊòØÂú®Áä∂ÊÄÅÁ©∫Èó¥ÂíåÂ∫èÂàóÈïøÂ∫¶Â¢ûÂä†Êó∂„ÄÇÂèòÊç¢Âô®Âú®‰∏çÂêåÂ∫èÂàóÈïøÂ∫¶‰πãÈó¥ÁöÑÊùÉÈáçÂÖ±‰∫´ËÉΩÂäõËæÉÂ∑ÆÔºåÂØºËá¥ÂÆÉ‰ª¨Âú®Â≠¶‰π†Êó∂ÂæÄÂæÄÂè™ÈíàÂØπÁâπÂÆöÈïøÂ∫¶ËøõË°å‰ºòÂåñ„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåRNNsÈÄöËøáÊùÉÈáçÂÖ±‰∫´ÂÆûÁé∞‰∫ÜÊúâÊïàÁöÑÂ≠¶‰π†Ôºå‰ΩøÂæó‰∏çÂêåÂ∫èÂàóÈïøÂ∫¶ÁöÑÊï∞ÊçÆËÉΩÂ§üÁõ∏‰∫íÊèêÂçáÊÄßËÉΩ„ÄÇ', title='ÂèòÊç¢Âô®Âú®Áä∂ÊÄÅË∑üË∏™‰∏≠ÁöÑÊåëÊàò‰∏éRNNÁöÑ‰ºòÂäø'))
[24.02.2026 20:27] Using data from previous issue: {"categories": ["#robotics"], "emoji": "ü¶æ", "ru": {"title": "–ü—Ä–æ–ø—Ä–∏–æ—Ü–µ–ø—Ç–∏–≤–Ω–∞—è –æ–¥–æ–º–µ—Ç—Ä–∏—è —á–µ—Ç–≤–µ—Ä–æ–Ω–æ–≥–∏—Ö —Ä–æ–±–æ—Ç–æ–≤ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞—Ç—á–∏–∫–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è —á–µ—Ç–≤–µ—Ä–æ–Ω–æ–≥–∏—Ö —Ä–æ–±–æ—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Ç–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞–Ω–∏—è –∏–Ω–µ—Ä—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–∑–º–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –±–ª–æ–∫–∞ (IMU) –∏ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –º–æ—Ç–æ—Ä–æ–≤ –±–µ–∑ 
[24.02.2026 20:27] Renaming data file.
[24.02.2026 20:27] Renaming previous data. hf_papers.json to ./d/2026-02-24.json
[24.02.2026 20:27] Saving new data file.
[24.02.2026 20:27] Generating page.
[24.02.2026 20:27] Renaming previous page.
[24.02.2026 20:27] Renaming previous data. index.html to ./d/2026-02-24.html
[24.02.2026 20:27] Writing result.
[24.02.2026 20:27] Renaming log file.
[24.02.2026 20:27] Renaming previous data. log.txt to ./logs/2026-02-24_last_log.txt
