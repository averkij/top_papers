[23.02.2026 23:29] Read previous papers.
[23.02.2026 23:29] Generating top page (month).
[23.02.2026 23:29] Writing top page (month).
[24.02.2026 01:20] Read previous papers.
[24.02.2026 01:20] Get feed.
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10693
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.08354
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18422
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18292
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15727
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18071
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18432
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17807
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16742
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.15814
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18312
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17664
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17186
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17080
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.17022
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14279
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.13576
[24.02.2026 01:20] Get page data from previous paper. URL: https://huggingface.co/papers/2602.10094
[24.02.2026 01:20] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[24.02.2026 01:20] No deleted papers detected.
[24.02.2026 01:20] Downloading and parsing papers (pdf, html). Total: 18.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.10693.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.10693.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.10693.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.08354.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.08354.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.08354.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.18422.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.18422.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.18422.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.18292.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.18292.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.18292.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.15727.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.15727.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.15727.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.18071.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.18071.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.18071.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.18432.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.18432.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.18432.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.17807.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.17807.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.17807.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.16742.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.16742.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.16742.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.15814.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.15814.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.15814.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.18312.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.18312.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.18312.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.17664.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.17664.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.17664.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.17186.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.17186.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.17186.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.17080.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.17080.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.17080.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.17022.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.17022.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.17022.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.14279.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.14279.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.14279.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.13576.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.13576.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.13576.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Downloading and parsing paper https://huggingface.co/papers/2602.10094.
[24.02.2026 01:20] Extra JSON file exists (./assets/json/2602.10094.json), skip PDF parsing.
[24.02.2026 01:20] Paper image links file exists (./assets/img_data/2602.10094.json), skip HTML parsing.
[24.02.2026 01:20] Success.
[24.02.2026 01:20] Enriching papers with extra data.
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 0. VESPO addresses training instability in LLM reinforcement learning by using variational formulation with variance reduction to correct policy divergence without length normalization.  					AI-generated summary 				 Training stability remains a central challenge in reinforcement learning (RL) for lar...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 1. Large reasoning models can implicitly determine optimal stopping points for thinking, which SAGE-RL enhances by incorporating efficient reasoning patterns into pass@1 inference for improved accuracy and efficiency.  					AI-generated summary 				 Recent advancements in large reasoning models (LRMs) ...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 2. A human-centric video world model conditioned on tracked head and hand poses is introduced, enabling dexterous interactions through a bidirectional video diffusion model trained for egocentric virtual environment generation.  					AI-generated summary 				 Extended reality (XR) demands generative mo...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 3. Abstract Decoding is reinterpreted as a principled optimization layer that balances model scores with structural preferences, recovering existing methods as special cases and enabling the creation of new decoders like Best-of-K that improve accuracy in mathematical reasoning tasks.  					AI-generate...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 4. Abstract Visual analogy learning via dynamic composition of learned LoRA transformation primitives enables flexible image manipulation with improved generalization over fixed adaptation modules.  					AI-generated summary Visual analogy learning enables image manipulation through demonstration rathe...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 5. EgoPush enables robot manipulation in cluttered environments through perception-driven policy learning that uses object-centric latent spaces and stage-decomposed rewards for long-horizon tasks.  					AI-generated summary 				 Humans can rearrange objects in cluttered environments using egocentric p...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 6. A causal transformer-based variational autoencoder combined with flow matching enables real-time, spatially-aware conversational motion for embodied agents in virtual reality applications.  					AI-generated summary 				 As embodied agents become central to VR, telepresence, and digital human applic...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 7. Abstract A video segmentation model eliminates specialized tracking modules by using a Vision Transformer encoder with query propagation and fusion mechanisms for efficient, high-speed processing.  					AI-generated summary Existing online video segmentation models typically combine a per-frame segm...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 8. DeepVision-103K dataset enhances multimodal reasoning capabilities of large models through diverse mathematical content and visual elements.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 9. Abstract Compact pretrained bidirectional encoders based on Avey architecture outperform Transformer-based models on token classification and information retrieval tasks while scaling more efficiently to long contexts.  					AI-generated summary Compact pretrained bidirectional encoders remain the b...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 10. Reinforcement learning policies are improved by using action Jacobian penalty to eliminate unrealistic high-frequency signals, with a new Linear Policy Net architecture reducing computational overhead while enabling faster convergence and efficient inference for motion imitation tasks.  					AI-gene...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 11. Abstract Diffusion Language Models suffer from high inference costs due to iterative denoising, prompting the development of Sink-Aware Pruning that identifies and removes unstable attention sinks, improving efficiency without retraining.  					AI-generated summary Diffusion Language Models (DLMs) i...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 12. Visual Information Gain metric quantifies the contribution of visual input to prediction uncertainty, enabling selective training that improves visual grounding and reduces language bias in vision-language models.  					AI-generated summary 				 Large Vision Language Models (LVLMs) have achieved rem...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 13. Abstract A new class of optimizers combines orthogonalized momentum with norm-based noise adaptation, achieving improved convergence rates and training performance for large language models.  					AI-generated summary Efficient stochastic optimization typically integrates an update direction that pe...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 14. Abstract Conversational agents with tool integration face challenges from user-induced errors, but a test-time intervention method called Reasoning Inception (ReIn) enables error recovery by injecting external reasoning into the agent's decision-making process without modifying model parameters or p...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 15. Abstract Adaptive group elicitation framework combines LLM-based information gain scoring with graph neural networks to improve population-level predictions under budget constraints.  					AI-generated summary Eliciting information to reduce uncertainty about latent group-level properties from surve...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 16. Abstract LLM-based judges using natural-language rubrics for evaluation can exhibit systematic preference drift from minor rubric modifications, which can be exploited to manipulate alignment pipelines and degrade model performance.  					AI-generated summary Evaluation and alignment pipelines for l...
[24.02.2026 01:20] ********************************************************************************
[24.02.2026 01:20] Abstract 17. Abstract 4RC presents a unified feed-forward framework for 4D reconstruction from monocular videos that learns holistic scene geometry and motion dynamics through a transformer-based encoder-decoder architecture with conditional querying capabilities.  					AI-generated summary We present 4RC, a uni...
[24.02.2026 01:20] Read previous papers.
[24.02.2026 01:20] Generating reviews via LLM API.
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#rl", "#training", "#open_source", "#reasoning", "#math", "#optimization"], "emoji": "‚öñÔ∏è", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—É—é —Ä–µ–¥—É–∫—Ü–∏—é –¥–∏—Å–ø–µ—Ä—Å–∏–∏", "desc": "VESPO —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#benchmark", "#rl"], "emoji": "üß†", "ru": {"title": "–ú–æ–¥–µ–ª–∏ —Å–∞–º–∏ –∑–Ω–∞—é—Ç, –∫–æ–≥–¥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#video", "#architecture", "#training", "#multimodal", "#diffusion", "#3d"], "emoji": "ü•Ω", "ru": {"title": "–í–∏–¥–µ–æ–º–æ–¥–µ–ª—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞ —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –ø–æ–∑—ã –≥–æ–ª–æ–≤—ã –∏ —Ä—É–∫ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#inference", "#math", "#reasoning", "#optimization"], "emoji": "üéØ", "ru": {"title": "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∫ –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –µ–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–µcoders", "desc": "–í —Å—Ç–∞—Ç—å–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª—è–µ—Ç—Å—è –∫–∞–∫ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π —Å–ª–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –º–µ–∂–¥—É –æ—Ü–µ–Ω–∫
[24.02.2026 01:20] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∫–æ–º–ø–æ–∑–∏—Ü–∏—è LoRA –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤ –¥–ª—è –≥–∏–±–∫–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ LoRWeB –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–æ–≥–∏—è–º, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#cv", "#robotics", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–ú–∞–Ω–∏–ø—É–ª—è—Ü–∏—è —Ä–æ–±–æ—Ç–æ–º –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞ —á–µ—Ä–µ–∑ –æ–±—ä–µ–∫—Ç–Ω–æ-—Ü–µ–Ω—Ç—Ä–∏—á–Ω—ã–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞", "desc": "EgoPush ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º –≤ –∑–∞–≥—Ä–æ–º–æ–∂–¥—ë–Ω–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –≤–∏–¥–µ–Ω–∏—è. 
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#agents", "#dataset", "#video", "#architecture", "#multimodal", "#3d"], "emoji": "üé≠", "ru": {"title": "–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Å–≤–µ–¥–æ–º–ª—ë–Ω–Ω–æ—Å—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–∏—á–∏–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω—ã—Ö 
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#cv", "#video", "#architecture"], "emoji": "üé¨", "ru": {"title": "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –±–µ–∑ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è: –±—ã—Å—Ç—Ä–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ Vision Transformer –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#multimodal", "#dataset"], "emoji": "üìä", "ru": {"title": "–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö DeepVision-103K, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º—É
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#small_models"], "emoji": "‚ö°", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä –±–µ–∑ –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –±—ã—Å—Ç—Ä–µ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Avey –¥–ª—è —Ä–µ–∂–∏–º–∞ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –±–µ–∑ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è, –ø—Ä–µ–¥–ª–æ–∂–∏–≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ 
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#architecture", "#robotics", "#optimization", "#inference", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–ì–ª–∞–¥–∫–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ —à—Ç—Ä–∞—Ñ —è–∫–æ–±–∏–∞–Ω–∞ –¥–µ–π—Å—Ç–≤–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –ø—É—Ç—ë–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —à—Ç—Ä–∞—Ñ–∞ –Ω–∞ —è–∫–æ–±–∏–∞
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#optimization"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —É–∑–ª–æ–≤ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –≤—ã—Å–æ–∫–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –∏–∑-–∑–∞ –∏—Ç–µ
[24.02.2026 01:20] Using data from previous issue: {"categories": [], "emoji": "üëÅÔ∏è", "ru": {"title": "–ò–∑–º–µ—Ä—è–µ–º –≤–∫–ª–∞–¥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∞ Visual Information Gain (VIG), –∫–æ—Ç–æ—Ä–∞—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∏–∑—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–Ω–∏–∂–∞–µ—Ç –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π 
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture"], "emoji": "‚ö°", "ru": {"title": "–û—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º —à—É–º–æ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã –Ω–æ–≤—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã NAMO –∏ NAMO-D, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –ø–æ–¥—Å—Ç—Ä–æ–π–∫–æ–π –ø–æ–¥ —É—Ä–æ–≤–µ–Ω—å —à
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#alignment", "#training"], "emoji": "üîß", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏", "desc": "–†–∞–±–æ—Ç–∞ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç–æ–¥ Reasoning Inception (ReIn) –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –æ—Ç –æ—à–∏–±–æ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–æ –≤—Ä–µ–º—è –∏–Ω
[24.02.2026 01:20] Using data from previous issue: {"categories": [], "emoji": "üéØ", "ru": {"title": "–£–º–Ω—ã–π –≤—ã–±–æ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ —Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–Ω–µ–Ω–∏–π –æ–±—â–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–±–æ—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —É –≥—Ä—É–ø–ø –ª—é–¥–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—á–µ—Ç–∞–µ—Ç LLM-–±–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –≥—Ä–∞—Ñ–æ–≤—ã–º–∏
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#ethics", "#security", "#alignment"], "emoji": "‚ö†Ô∏è", "ru": {"title": "–†—É–±—Ä–∏–∫–∏ –∫–∞–∫ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å –∞—Ç–∞–∫–∏: –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è –æ—Ü–µ–Ω–∫–∞–º–∏ LLM-—Å—É–¥–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç —É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ LLM-—Å—É–¥–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑–≤–∞–Ω–Ω—É—é Rubric-Induced Preference Drift (RIPD). –î–∞–∂–µ –Ω–µ–∑–Ω–∞—á
[24.02.2026 01:20] Using data from previous issue: {"categories": ["#video", "#3d", "#architecture"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏ —Å—Ü–µ–Ω—ã –∏–∑ –≤–∏–¥–µ–æ", "desc": "4RC –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—É—é –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è 4D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∏–∑—É—á–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—é
[24.02.2026 01:20] Renaming data file.
[24.02.2026 01:20] Renaming previous data. hf_papers.json to ./d/2026-02-24.json
[24.02.2026 01:20] Saving new data file.
[24.02.2026 01:20] Generating page.
[24.02.2026 01:20] Renaming previous page.
[24.02.2026 01:20] Renaming previous data. index.html to ./d/2026-02-24.html
[24.02.2026 01:20] Writing result.
[24.02.2026 01:20] Renaming log file.
[24.02.2026 01:20] Renaming previous data. log.txt to ./logs/2026-02-24_last_log.txt
