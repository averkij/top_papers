[20.12.2025 18:35] Read previous papers.
[20.12.2025 18:35] Generating top page (month).
[20.12.2025 18:35] Writing top page (month).
[21.12.2025 02:20] Read previous papers.
[21.12.2025 02:20] Get feed.
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16776
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16301
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15745
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16922
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16915
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13507
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16923
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16913
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16625
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16905
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16636
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16924
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16561
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16649
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16920
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16918
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16912
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16900
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16899
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16864
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16501
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16106
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16378
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16921
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16615
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.11251
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16670
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16767
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12576
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10953
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16909
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15907
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15489
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14884
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12880
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12623
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15528
[21.12.2025 02:20] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14805
[21.12.2025 02:20] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.12.2025 02:20] No deleted papers detected.
[21.12.2025 02:20] Downloading and parsing papers (pdf, html). Total: 38.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16776.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16776.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16776.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16301.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16301.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16301.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.15745.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.15745.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.15745.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16922.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16922.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16922.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16915.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16915.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16915.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.13507.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.13507.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.13507.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16923.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16923.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16923.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16913.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16913.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16913.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16625.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16625.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16625.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16905.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16905.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16905.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16636.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16636.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16636.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16924.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16924.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16924.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16561.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16561.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16561.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16649.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16649.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16649.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16920.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16920.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16920.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16918.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16918.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16918.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16912.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16912.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16912.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16900.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16900.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16900.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16899.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16899.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16899.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16864.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16864.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16864.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16501.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16501.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16501.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16106.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16106.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16106.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16378.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16378.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16378.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16921.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16921.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16921.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16615.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16615.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16615.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.11251.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.11251.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.11251.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16670.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16670.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16670.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16767.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16767.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16767.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.12576.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.12576.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.12576.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.10953.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.10953.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.10953.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.16909.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.16909.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.16909.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.15907.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.15907.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.15907.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.15489.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.15489.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.15489.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.14884.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.14884.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.14884.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.12880.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.12880.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.12880.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.12623.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.12623.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.12623.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.15528.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.15528.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.15528.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Downloading and parsing paper https://huggingface.co/papers/2512.14805.
[21.12.2025 02:20] Extra JSON file exists (./assets/json/2512.14805.json), skip PDF parsing.
[21.12.2025 02:20] Paper image links file exists (./assets/img_data/2512.14805.json), skip HTML parsing.
[21.12.2025 02:20] Success.
[21.12.2025 02:20] Enriching papers with extra data.
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 0. Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.  					AI-generated summary 				 We present Kling-Omni, a generalist generative framework designed to synthesize high-fidel...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 1. This paper presents a framework for agent and tool adaptation in agentic AI systems, clarifying design strategies and identifying open challenges for improving AI capabilities.  					AI-generated summary 				 Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan,...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 2. LLaDA2.0 converts auto-regressive models into discrete diffusion large language models using a block-level training scheme, improving efficiency and performance at large scales.  					AI-generated summary 				 This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM)...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 3. Generative pretraining using next embedding prediction outperforms traditional self-supervised methods in visual learning tasks, achieving high accuracy on ImageNet and effective transfer to semantic segmentation.  					AI-generated summary 				 Inspired by the success of generative pretraining in n...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 4. StereoPilot, a feed-forward model leveraging a learnable domain switcher and cycle consistency loss, synthesizes high-quality stereo video directly without depth maps, outperforming existing methods in visual fidelity and computational efficiency.  					AI-generated summary 				 The rapid growth of ...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 5. Seedance 1.5 pro, a dual-branch Diffusion Transformer model, achieves high-quality audio-visual synchronization and generation through cross-modal integration, post-training optimizations, and an acceleration framework.  					AI-generated summary 				 Recent strides in video generation have paved th...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 6. Generative Refocusing uses semi-supervised learning with DeblurNet and BokehNet to achieve high-quality single-image refocusing with controllable bokeh and text-guided adjustments.  					AI-generated summary 				 Depth-of-field control is essential in photography, but getting the perfect focus often...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 7. A panoramic metric depth foundation model using DINOv3-Large and a three-stage pseudo-label pipeline achieves robust performance across diverse real-world scenes.  					AI-generated summary 				 In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene ...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 8. DeContext defends against unauthorized in-context image editing by weakening cross-attention pathways in multimodal attention layers, preserving visual quality while blocking unwanted modifications.  					AI-generated summary 				 In-context diffusion models allow users to modify images with remarka...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 9. Alchemist, a meta-gradient-based framework, automatically selects high-quality subsets from large-scale text-image datasets to improve visual quality and training efficiency in Text-to-Image models.  					AI-generated summary 				 Recent advances in Text-to-Image (T2I) generative models, such as Ima...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 10. REGLUE, a unified latent diffusion framework, enhances image synthesis by jointly modeling VAE latents, patch-level VFM semantics, and global tokens, improving semantic supervision and convergence.  					AI-generated summary 				 Latent diffusion models (LDMs) achieve state-of-the-art image synthesi...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 11. WorldCanvas generates coherent, controllable world events using a multimodal framework that integrates text, trajectories, and reference images.  					AI-generated summary 				 We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining te...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 12. N3D-VLM integrates native 3D perception and reasoning in vision-language models, enabling precise 3D localization and spatial understanding with a large-scale dataset.  					AI-generated summary 				 While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D obje...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 13. JustRL achieves state-of-the-art performance on reasoning models with minimal complexity, using single-stage training and fixed hyperparameters, outperforming sophisticated approaches in terms of compute and stability.  					AI-generated summary 				 Recent advances in reinforcement learning for lar...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 14. EasyV2V framework enhances video editing by combining diverse data sources, leveraging pretrained text-to-video models with LoRA fine-tuning, and implementing unified spatiotemporal control, achieving top results.  					AI-generated summary 				 While image editing has advanced rapidly, video editin...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 15. AdaTooler-V, a multimodal large language model, adaptively uses vision tools based on reinforcement learning, improving performance and reducing unnecessary tool invocations in visual reasoning tasks.  					AI-generated summary 				 Recent advances have shown that multimodal large language models (M...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 16. Reinforcement learning with verifiable rewards improves LLM reasoning through spurious rewards and entropy minimization, despite seemingly paradoxical effects, by reducing clipping bias and policy entropy.  					AI-generated summary 				 This paper examines the exploration-exploitation trade-off in ...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 17. FlashPortrait is a diffusion-based video transformer for long-portrait animation that ensures ID consistency and achieves 6x acceleration through a dynamic sliding-window scheme and higher-order latent derivatives.  					AI-generated summary 				 Current diffusion-based acceleration methods for long...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 18. Multimodal RewardBench 2 (MMRB2) is a benchmark for reward models on multimodal understanding and generation tasks, featuring expert-annotated preferences and state-of-the-art model evaluations.  					AI-generated summary 				 Reward models (RMs) are essential for training large language models (LLM...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 19. RePlan, a plan-then-execute framework, enhances instruction-based image editing by combining a vision-language planner with a diffusion editor, achieving superior performance in complex and intricate editing tasks using limited data.  					AI-generated summary 				 Instruction-based image editing en...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 20. VenusBench-GD is a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, offering a hierarchical evaluation framework with extensive data coverage and rich annotations.  					AI-generated summary 				 GUI grounding is a critical component in building capable GUI agents....
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 21. ModelTables benchmarks structured performance and configuration tables from model cards, GitHub READMEs, and papers to improve table-based retrieval and semantic understanding of model performance.  					AI-generated summary 				 We present ModelTables, a benchmark of tables in Model Lakes that capt...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 22. Hearing to Translate benchmarks SpeechLLMs and cascaded systems for speech-to-text translation, finding that cascaded systems are more reliable overall and highlighting the importance of integrating LLMs for high-quality translation.  					AI-generated summary 				 As Large Language Models (LLMs) ex...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 23. AuditDM, an automated framework using reinforcement learning, identifies and rectifies failure modes in multimodal LLMs by generating challenging examples, leading to improved performance across benchmarks.  					AI-generated summary 				 Conventional evaluation methods for multimodal LLMs (MLLMs) l...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 24. Log-linear Sparse Attention (LLSA) improves the efficiency of diffusion transformers by reducing computational costs for long token sequences through a hierarchical structure, enhancing training speed without sacrificing generation quality.  					AI-generated summary 				 Diffusion Transformers (DiT...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 25. Insight Miner, a large-scale multimodal model, generates high-quality time-series descriptions using a novel agentic workflow and outperforms existing models with the help of the TS-Insights dataset.  					AI-generated summary 				 Time-series data is critical across many scientific and industrial d...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 26. FrameDiffuser is an autoregressive neural rendering framework that generates temporally consistent photorealistic frames using G-buffer data and previous frame outputs, leveraging ControlNet and ControlLoRA for structural and temporal coherence.  					AI-generated summary 				 Neural rendering for i...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 27. A novel feed-forward framework, Make-It-Poseable, reformulates character posing as a latent-space transformation problem, using a latent posing transformer and dense pose representation to achieve superior posing quality and extend to 3D editing applications.  					AI-generated summary 				 Posing 3...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 28. CoVRL, a hybrid approach combining variational inference and reinforcement learning, enhances language model reasoning by coupling prior and posterior distributions, improving performance and coherence.  					AI-generated summary 				 While reinforcement learning have achieved impressive progress in...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 29. Bidirectional Normalizing Flow (BiFlow) enhances generative modeling by approximating the noise-to-data inverse mapping, significantly improving generation quality and sampling speed compared to standard Normalizing Flows.  					AI-generated summary 				 Normalizing Flows (NFs) have been established...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 30. MomaGraph-R1, a vision-language model trained with reinforcement learning, achieves state-of-the-art performance in predicting task-oriented scene graphs and zero-shot task planning in household environments using the MomaGraph-Scenes dataset and MomaGraph-Bench evaluation suite.  					AI-generated ...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 31. TabReX is a reference-less framework using graph-based reasoning to evaluate the quality of tables generated by LLMs, offering structural and factual fidelity scores.  					AI-generated summary 				 Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge:...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 32. Nemotron-Math, a large-scale mathematical reasoning dataset, enhances performance and robustness through diverse problem integration and efficient long-context training strategies.  					AI-generated summary 				 High-quality mathematical reasoning supervision requires diverse reasoning styles, long...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 33. Vibe Blending uses Vibe Space, a hierarchical graph manifold, to generate coherent image hybrids by learning geodesics in feature spaces, outperforming current methods in creativity and coherence.  					AI-generated summary 				 Creating new visual concepts often requires connecting distinct ideas t...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 34. Mixture of LoRAs within a shared feed-forward network restores expressivity in parameter-shared recursive transformers, achieving state-of-the-art performance with compact models.  					AI-generated summary 				 Parameter sharing in recursive transformers reduces model size but collapses layer-wise ...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 35. A dynamic multimodal latent reasoning framework improves cross-modal reasoning and perception performance by interleaving reasoning and perception using confidence-guided optimization and dynamic visual injection.  					AI-generated summary 				 Recent advancements in Multimodal Large Language Model...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 36. EmoCaliber, a confidence-aware Multimodal Large Language Model, enhances Visual Emotion Comprehension by verbalizing confidence in emotion predictions, leading to improved reliability and accuracy.  					AI-generated summary 				 Visual Emotion Comprehension (VEC) aims to infer sentiment polarities ...
[21.12.2025 02:20] ********************************************************************************
[21.12.2025 02:20] Abstract 37. Nightjar programming system introduces shared program state abstraction to facilitate interoperability between natural language code and Python, enhancing task accuracy and reducing code size.  					AI-generated summary 				 The rise of large language models (LLMs) has introduced a new type of progr...
[21.12.2025 02:20] Read previous papers.
[21.12.2025 02:20] Generating reviews via LLM API.
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#reasoning", "#inference", "#training", "#video"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ –∏–∑ –ª—é–±—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "Kling-Omni ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞—ë—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ –∏–∑ —Ä
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#agents", "#training"], "emoji": "üîß", "ru": {"title": "–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ AI —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–ª—è –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑–¥–µ–ª—è
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#rlhf", "#transfer_learning", "#training", "#diffusion", "#architecture", "#optimization", "#open_source", "#alignment"], "emoji": "üîÑ", "ru": {"title": "–û—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ LLaDA2
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#training", "#cv", "#architecture"], "emoji": "üîÆ", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–∞–∫ –ø—É—Ç—å –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–º—É –æ–±—É—á–µ–Ω–∏—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è NEPA (Next-Embedding Predictive Autoregression), –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –ø—Ä
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#video", "#dataset", "#benchmark", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ü—Ä—è–º–æ–π —Å–∏–Ω—Ç–µ–∑ —Å—Ç–µ—Ä–µ–æ–≤–∏–¥–µ–æ –±–µ–∑ –∫–∞—Ä—Ç –≥–ª—É–±–∏–Ω—ã", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å StereoPilot, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–µ –≤–∏–¥–µ–æ –≤ —Å—Ç–µ—Ä–µ–æ–≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –±–µ–∑ —è–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞—Ä—Ç –≥–ª—É–±
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#rlhf", "#open_source", "#video", "#architecture", "#diffusion", "#audio", "#inference", "#multimodal", "#training", "#multilingual"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–≤—É—Ö–≤–µ—Ç–≤–µ–≤–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã", "desc": "Seedance 1.5 pr
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#training", "#dataset", "#cv", "#synthetic", "#optimization"], "emoji": "üì∏", "ru": {"title": "–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –ø–µ—Ä–µ–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ —Ñ–æ–∫—É—Å–∞ —á–µ—Ä–µ–∑ –ø–æ–ª—É—Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ —Å–∏–Ω—Ç–µ–∑ –±–æ–∫–µ", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Generative Refocusing ‚Äî –º–µ—Ç–æ–¥ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –≥–ª—É–±–∏–Ω—ã —Ä–µ–∑–∫–æ—Å—Ç–∏ –Ω–∞ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö 
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#dataset", "#training", "#benchmark", "#architecture", "#3d", "#cv"], "emoji": "üåç", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω—ã –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö —Å—Ü–µ–Ω", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è 
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#multimodal", "#security", "#diffusion", "#architecture", "#cv"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –æ—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –æ—Å–ª–∞–±–ª–µ–Ω–∏–µ –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ DeContext –¥–ª—è –∑–∞—â–∏—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ—Ç –Ω–µ—Å–∞–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#training", "#multimodal", "#data", "#dataset"], "emoji": "‚öóÔ∏è", "ru": {"title": "–ú–µ—Ç–∞–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–∞—è –∞–ª—Ö–∏–º–∏—è: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–±–æ—Ä –ª—É—á—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "Alchemist ‚Äî —ç—Ç–æ meta-gradient-based —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –≤—ã—Å–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#cv", "#architecture", "#diffusion", "#training", "#open_source"], "emoji": "üé®", "ru": {"title": "–ü–µ—Ä–µ–ø–ª–µ—Ç–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏: —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ VAE, –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ Vision Foundation Models –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "REGLUE ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#video", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –º–∏—Ä–∞ —á–µ—Ä–µ–∑ —Ç–µ–∫—Å—Ç, –¥–≤–∏–∂–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–∑—ã", "desc": "WorldCanvas –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å—Ü–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è, —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ 
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#data", "#dataset"], "emoji": "üèóÔ∏è", "ru": {"title": "–î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–∏–Ω–Ω–æ–µ 3D –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤ —è–∑—ã–∫–æ–≤–æ-–≤–∏–∑—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏", "desc": "N3D-VLM ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –Ω–∞—Ç–∏–≤–Ω–æ–µ 3D –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≤ –º–æ–¥–µ–ª–∏ –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞, –ø–æ–∑–≤–æ–ª—è—è –∏–º –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç—ã –≤ —Ç—Ä
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#training", "#benchmark", "#small_models", "#rl", "#optimization", "#reasoning", "#open_source"], "emoji": "üéØ", "ru": {"title": "–ü—Ä–æ—Å—Ç–æ—Ç–∞ –ª—É—á—à–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π reinforcement learning –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "JustRL –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#training", "#dataset", "#video", "#architecture", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ü—Ä–æ—Å—Ç–æ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –ª—ë–≥–∫—É—é –¥–æ–ø–æ–¥–≥–æ—Ç–æ–≤–∫—É", "desc": "EasyV2V ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –∫–æ—Ç–æ—Ä—ã–π —Ä
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#open_source", "#rl", "#benchmark", "#video", "#inference", "#reasoning", "#multimodal", "#optimization", "#dataset"], "emoji": "üß†", "ru": {"title": "–£–º–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ AdaTooler-V ‚Äî –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#rlhf", "#reasoning", "#training", "#rl", "#alignment"], "emoji": "üéØ", "ru": {"title": "–ü–∞—Ä–∞–¥–æ–∫—Å –æ–±—É—á–µ–Ω–∏—è: –∫–∞–∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —É–ª—É—á—à–∞—é—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø–∞—Ä–∞–¥–æ–∫—Å–∞–ª—å–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏ (RLV
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#long_context", "#inference", "#optimization", "#architecture", "#diffusion", "#video"], "emoji": "üé¨", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –ø–æ—Ä—Ç—Ä–µ—Ç–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ª–∏—á–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã", "desc": "FlashPortrait ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π –≤–∏–¥–µ–æ-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–ª–∏—Ç–µ
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset"], "emoji": "üéØ", "ru": {"title": "–≠—Ç–∞–ª–æ–Ω–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Multimodal RewardBench 2 (MMRB2) ‚Äî –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤ –∑
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#diffusion", "#reasoning", "#multimodal", "#synthetic", "#cv", "#dataset"], "emoji": "üé®", "ru": {"title": "–ü–ª–∞–Ω –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ: –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è —Å–ª–æ–∂–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ vision-language —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ", "desc": "RePlan ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#dataset", "#survey", "#multimodal", "#benchmark", "#multilingual", "#agents"], "emoji": "üéØ", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤: –æ—Ç –±–∞–∑–æ–≤—ã—Ö –∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –∑–∞–¥–∞—á–∞–º", "desc": "VenusBench-GD ‚Äî —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–≤—É—è–∑—ã—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#benchmark", "#rag"], "emoji": "üìä", "ru": {"title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ ModelTables, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–±–∏—Ä–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#multilingual", "#multimodal", "#benchmark", "#audio"], "emoji": "üé§", "ru": {"title": "–ö–∞—Å–∫–∞–¥–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç SpeechLLM –≤ –ø–µ—Ä–µ–≤–æ–¥–µ —Ä–µ—á–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ Hearing to Translate –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä—è–º–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é SpeechLLM –∏ –∫–∞—Å–∫–∞–¥–Ω—ã—Ö 
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#rl", "#interpretability", "#security", "#multimodal", "#synthetic", "#training", "#small_models"], "emoji": "üîç", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞—É–¥–∏—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –∏—Ö —Å–ª–∞–±–æ—Å—Ç–µ–π", "desc": "AuditDM ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#cv", "#inference", "#architecture", "#diffusion", "#optimization", "#training", "#long_context", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Log-linear Sparse Attention (LLSA
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#agents", "#training", "#dataset", "#multimodal"], "emoji": "üìà", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–ø–∏—Å–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Insight Miner, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –∏—Å–ø
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#diffusion", "#video", "#games", "#architecture", "#inference", "#optimization"], "emoji": "üé¨", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –Ω–µ–π—Ä–æ–Ω–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å—é", "desc": "FrameDiffuser ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#3d", "#architecture"], "emoji": "üé≠", "ru": {"title": "–ü–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —á–µ—Ä–µ–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Make-It-Poseable –¥–ª—è –ø–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è 3D –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á—É –∫–∞–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –º–æ
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#optimization", "#training", "#rl", "#reasoning"], "emoji": "üß†", "ru": {"title": "–°–≤—è–∑–∞–Ω–Ω–æ–µ –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ª–æ–≥–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "CoVRL ‚Äî —ç—Ç–æ –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#optimization", "#diffusion"], "emoji": "üîÑ", "ru": {"title": "–î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—â–∏–µ –ø–æ—Ç–æ–∫–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Bidirectional Normalizing Flow (BiFlow) ‚Äî –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—â–∏—Ö –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#open_source", "#rl", "#benchmark", "#multimodal", "#dataset", "#graphs", "#cv", "#reasoning", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ì—Ä–∞—Ñ–∏–∫ —Å—Ü–µ–Ω—ã –¥–ª—è —É–º–Ω–æ–≥–æ —Ä–æ–±–æ—Ç–∞: –æ—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∫ –¥–µ–π—Å—Ç–≤–∏—é", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç MomaGraph ‚Äî —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—Ü–µ–Ω—ã –¥–ª—è –º–æ–±
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#dataset", "#benchmark"], "emoji": "üìä", "ru": {"title": "–ì—Ä–∞—Ñ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–∞–±–ª–∏—Ü –±–µ–∑ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "TabReX ‚Äî —ç—Ç–æ —ç—Ç–∞–ª–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–∞–±–ª–∏—Ü, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä–∞—Ñ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã —Ä
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#training", "#long_context", "#open_source", "#math", "#synthetic", "#dataset", "#optimization", "#reasoning"], "emoji": "üßÆ", "ru": {"title": "–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã", "desc": "Nemotron-Math ‚Äî —ç—Ç–æ –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#benchmark"], "emoji": "üé®", "ru": {"title": "–ü–æ–∏—Å–∫ –≥–µ–æ–¥–µ–∑–∏–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –æ–±—Ä–∞–∑–æ–≤ –¥–ª—è —Ç–≤–æ—Ä—á–µ—Å–∫–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Vibe Blending –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥–∞—Ä–º–æ–Ω–∏—á–Ω—ã—Ö –≥–∏–±—Ä–∏–¥–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—É—Ç—ë–º –ø–æ–∏—Å–∫–∞ –≥–µ–æ–¥–µ–∑–∏—á–µ—Å–∫–∏—Ö –ª–∏–Ω–∏–π –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#architecture", "#small_models", "#optimization", "#inference", "#training"], "emoji": "üéØ", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ —É—Å–ª–æ–≤–Ω–æ–µ –º–æ–¥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –≤–µ—Å–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ç–µ—Ö–Ω–∏–∫–∞ Mixture of LoRAs (MoL) –¥–ª—è –≤–æ
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#multimodal", "#inference", "#optimization"], "emoji": "üß†", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ DMLR ‚Äî –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#training"], "emoji": "üòä", "ru": {"title": "–£–≤–µ—Ä–µ–Ω–Ω–∞—è –≤ —Å–µ–±–µ –º–æ–¥–µ–ª—å: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å AI –≤—ã—Ä–∞–∂–∞—Ç—å —Å–æ–º–Ω–µ–Ω–∏—è –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ —ç–º–æ—Ü–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è EmoCaliber ‚Äî –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π
[21.12.2025 02:20] Using data from previous issue: {"categories": ["#optimization", "#training", "#architecture", "#plp"], "emoji": "üîó", "ru": {"title": "–û–±—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–∞–∫ –º–æ—Å—Ç –º–µ–∂–¥—É –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —è–∑—ã–∫–æ–º –∏ Python", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞ Nightjar, –∫–æ—Ç–æ—Ä–∞—è –≤–≤–æ–¥–∏—Ç –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—é –æ–±—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è 
[21.12.2025 02:20] Renaming data file.
[21.12.2025 02:20] Renaming previous data. hf_papers.json to ./d/2025-12-19.json
[21.12.2025 02:20] Saving new data file.
[21.12.2025 02:20] Generating page.
[21.12.2025 02:20] Renaming previous page.
[21.12.2025 02:20] Renaming previous data. index.html to ./d/2025-12-19.html
[21.12.2025 02:20] Writing result.
[21.12.2025 02:20] Renaming log file.
[21.12.2025 02:20] Renaming previous data. log.txt to ./logs/2025-12-21_last_log.txt
