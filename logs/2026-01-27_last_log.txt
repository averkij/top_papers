[27.01.2026 09:36] Read previous papers.
[27.01.2026 09:36] Generating top page (month).
[27.01.2026 09:36] Writing top page (month).
[27.01.2026 10:30] Read previous papers.
[27.01.2026 10:30] Get feed.
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18418
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17058
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17737
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17027
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17124
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17367
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18577
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18778
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18184
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15849
[27.01.2026 10:30] Extract page data from URL. URL: https://huggingface.co/papers/2601.15860
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18217
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17761
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18202
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18157
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18081
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17640
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17323
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17111
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13599
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18731
[27.01.2026 10:30] Extract page data from URL. URL: https://huggingface.co/papers/2601.18137
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16207
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14127
[27.01.2026 10:30] Extract page data from URL. URL: https://huggingface.co/papers/2601.12042
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18759
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18130
[27.01.2026 10:30] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17277
[27.01.2026 10:30] Extract page data from URL. URL: https://huggingface.co/papers/2601.17958
[27.01.2026 10:30] Extract page data from URL. URL: https://huggingface.co/papers/2601.15015
[27.01.2026 10:30] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.01.2026 10:30] No deleted papers detected.
[27.01.2026 10:30] Downloading and parsing papers (pdf, html). Total: 30.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.18418.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.18418.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.18418.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.17058.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.17058.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.17058.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.17737.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.17737.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.17737.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.17027.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.17027.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.17027.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.17124.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.17124.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.17124.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.17367.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.17367.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.17367.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.18577.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.18577.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.18577.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.18778.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.18778.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.18778.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.18184.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.18184.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.18184.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.15849.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.15849.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.15849.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.15860.
[27.01.2026 10:30] Downloading paper 2601.15860 from https://arxiv.org/pdf/2601.15860v1...
[27.01.2026 10:30] Extracting affiliations from text.
[27.01.2026 10:30] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 2 ] . [ 1 0 6 8 5 1 . 1 0 6 2 : r STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion Shui-Hsiang Hsu National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan g113056055@smail.nchu.edu.tw Tsung-Hsiang Chou National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan yumeow0122@smail.nchu.edu.tw Chen-Jui Yu National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan rui0828@smail.nchu.edu.tw Yao-Chung Fan National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan yfan@nchu.edu.tw Abstract Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective querytable alignment. We propose STAR (Semantic Table Representation), lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies headeraware K-means clustering to group semantically similar rows and selects representative centroid instances to construct diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the tables semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments "
[27.01.2026 10:30] Response: ```python
[
    "National Chung Hsing University",
    "Smart Sustainable New Agriculture Research Center (SMARTer)"
]
```
[27.01.2026 10:30] Deleting PDF ./assets/pdf/2601.15860.pdf.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.18217.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.18217.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.18217.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.17761.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.17761.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.17761.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.18202.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.18202.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.18202.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.18157.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.18157.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.18157.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.18081.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.18081.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.18081.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.17640.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.17640.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.17640.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.17323.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.17323.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.17323.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.17111.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.17111.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.17111.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.13599.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.13599.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.13599.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.18731.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.18731.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.18731.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.18137.
[27.01.2026 10:30] Downloading paper 2601.18137 from https://arxiv.org/pdf/2601.18137v1...
[27.01.2026 10:30] Extracting affiliations from text.
[27.01.2026 10:30] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DEEPPLANNING: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints Yinger Zhang*, Shutong Jiang*, Renhao Li*, Yang Su, Lianghao Deng, Xudong Guo, Chenxu Lv, Jianhong Tu(cid:0), Junyang Lin(cid:0) 6 2 0 2 6 ] . [ 1 7 3 1 8 1 . 1 0 6 2 : r Qwen Team, Alibaba Group {tujianhong.tjh, junyang.ljy}@alibaba-inc.com Homepage: https://qwenlm.github.io/Qwen-Agent/en/benchmarks/deepplanning/ "
[27.01.2026 10:30] Response: ```python
["Alibaba Group"]
```
[27.01.2026 10:30] Deleting PDF ./assets/pdf/2601.18137.pdf.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.16207.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.16207.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.16207.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.14127.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.14127.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.14127.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.12042.
[27.01.2026 10:30] Downloading paper 2601.12042 from https://arxiv.org/pdf/2601.12042v1...
[27.01.2026 10:30] Extracting affiliations from text.
[27.01.2026 10:30] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Less Is More Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models Zhaoxi Zhang University of Technology Sydney zhaoxi.zhang-1@student.uts.edu.au Xiaomei Zhang Griffith University xiaomei.zhang@griffithuni.edu.au Leo Yu Zhang Griffith University leo.zhang@griffith.edu.au 6 2 0 2 7 ] . [ 1 2 4 0 2 1 . 1 0 6 2 : r Yanjun Zhang Griffith University yanjun.zhang@griffith.edu.au Guanhong Tao University of Utah guanhong.tao@utah.edu Shirui Pan Griffith University s.pan@griffith.edu.au Abstract Visual token compression is widely adopted to improve the inference efficiency of Large VisionLanguage Models (LVLMs), enabling their deployment in latency-sensitive and resource-constrained scenarios. However, existing work has mainly focused on efficiency and performance, while the security implications of visual token compression remain largely unexplored. In this work, we first reveal that visual token compression substantially degrades the robustness of LVLMs: models that are robust under uncompressed inference become highly vulnerable once compression is enabled. These vulnerabilities are state-specific; failure modes emerge only in the compressed setting and completely disappear when compression is disabled, making them particularly hidden and difficult to diagnose. By analyzing the key stages of the compression process, we identify instability in token importance ranking as the primary cause of this robustness degradation. Small and imperceptible perturbations can significantly alter token rankings, leading the compression mechanism to mistakenly discard task-critical information and ultimately causing model failure. Motivated by this observation, we propose Compression-Aware Attack (CAA) to systematically study and exploit this vulnerability. CAA directly targets the token selection mechanism and induces failures exclusively under compressed inference. We further extend this approach to more realistic black-box settings and introduce "
[27.01.2026 10:30] Response: ```python
[
    "University of Technology Sydney",
    "Griffith University",
    "University of Utah"
]
```
[27.01.2026 10:30] Deleting PDF ./assets/pdf/2601.12042.pdf.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.18759.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.18759.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.18759.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.18130.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.18130.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.18130.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.17277.
[27.01.2026 10:30] Extra JSON file exists (./assets/json/2601.17277.json), skip PDF parsing.
[27.01.2026 10:30] Paper image links file exists (./assets/img_data/2601.17277.json), skip HTML parsing.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.17958.
[27.01.2026 10:30] Downloading paper 2601.17958 from https://arxiv.org/pdf/2601.17958v1...
[27.01.2026 10:30] Extracting affiliations from text.
[27.01.2026 10:30] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors Blavatnik School of Computer Science and AI, Tel Aviv University {idoatad,zimemran1,shaharkatz3}@mail.tau.ac.il, wolf@cs.tau.ac.il 6 2 0 2 5 2 ] . [ 1 8 5 9 7 1 . 1 0 6 2 : r a "
[27.01.2026 10:30] Response: ```python
["Blavatnik School of Computer Science and AI, Tel Aviv University"]
```
[27.01.2026 10:30] Deleting PDF ./assets/pdf/2601.17958.pdf.
[27.01.2026 10:30] Success.
[27.01.2026 10:30] Downloading and parsing paper https://huggingface.co/papers/2601.15015.
[27.01.2026 10:30] Downloading paper 2601.15015 from https://arxiv.org/pdf/2601.15015v1...
[27.01.2026 10:31] Extracting affiliations from text.
[27.01.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Jannis Becktepe 1 2 Aleksandra Franz 3 Nils Thuerey 3 Sebastian Peitz 1 2 6 2 0 2 1 2 ] . [ 1 5 1 0 5 1 . 1 0 6 2 : r a "
[27.01.2026 10:31] Response: ```python
[]
```
[27.01.2026 10:31] Extracting affiliations from text.
[27.01.2026 10:31] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control Jannis Becktepe 1 2 Aleksandra Franz 3 Nils Thuerey 3 Sebastian Peitz 1 2 6 2 0 2 1 2 ] . [ 1 5 1 0 5 1 . 1 0 6 2 : r aReinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes scalable foundation for future research in learning-based flow control, and is available at https://github. com/safe-autonomous-systems/ fluidgym. 1. Introduction Active flow control (AFC) plays central role in wide range of real-world systems, such as aerodynamics (Batikh et al., 2017), energy harvesting (Barthelmie et al., 2009), nuclear fusion (Pironti & Walker, 2005), and reduction of turbulence (Jimenez, 2013). Europe, for instance, could save 106 tonnes of CO2 per year by reducing more than 20 drag on cars using AFC (Brunton & Noack, 2015). 1TU Dortmund University, Dortmund, Germany 2Lamarr Institute for Machine Learning and Artificial Intelligence, Dortmund, Germany 3Technical University Munich, Munich, Germany. Correspondence to: Jannis Becktepe <jannis.becktepe@tudortmund.de>. Preprint. January 22, 2026. 1 Figure 1. The four uncontrolled environment classes in FluidGym. However, manually designing control strategies is challenging due to the high dimensionality and inherent nonlinearities of such systems (Duriez et al., 2017). Recently, reinforcement learning (RL) has demonstrated strong potential for advancing AFC in complex systems, e.g., stabilizing the plasma in Tokamak reactor (Degrave et al., 2022). Despite its success, research in RL for flow control remains fragmented, and establishing clear state of the art is difficult for several reasons. Experimental setups vary widely across studies in terms of actuators, sensor placements, and physical parameter settings as well as RL algorithms and hyperparameters (Viquerat et al., 2022; Moslem et al., 2025). This results in inconsistent problem formulations that hinder direct comparisons. Moreover, insufficiently rigorous evaluation and the use of few random seeds increase statistical variance (Henderson et al., 2018; Agarwal et al., 2021). Existing benchmarks (see Table 1) have seen limited adoption for two main reasons. First, most rely on external computational fluid dynamics (CFD) solvers that must be installed, configured, and coupled to Python RL code through additional interfaces, which demands CFD expertise and creates brittle software stacks. Second, differentiability is either absent or limited to small subset of scenarios, which prevents end-to-end use of differentiable predictive control (DPC, DrgoÀána et al. (2022)) and recent differentiable RL methods that can accelerate training and outperform classical RL (Xing et al., 2025; Lagemann et al., 2025b). Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control To address these limitations, we introduce FluidGym, the first standalone, fully differentiable RL benchmark for AFC in incompressible flows. Building entirely on PyTorch (Ansel et al., 2024), FluidGym requires no external solver dependencies and seamlessly integrates with common RL interfaces such as Gymnasium (Towers et al., 2024) or PettingZoo (Terry et al., 2021) and algorithm frameworks like Stable-Baselines3 (Raffin et al., 2021) or TorchRL (Bou et al., 2023). As all simulations and control interfaces live in one Python package, users can install FluidGym via pip and immediately run experiments with standard RL libraries, without compiling or coupling external CFD codes. Being inherently end-to-end differentiable, FluidGym enables researchers to use gradient-based control methods alongside classical RL without any further modifications. Our benchmark provides diverse environments with consistent task definitions, supports single-agent (SARL) and multiagent (MARL) settings, spans three difficulty levels in 2D and 3D, and enables transfer-learning studies. In summary, our main contributions are (1) the first standalone, fully differentiable, plug-and-play benchmark for RL in AFC, implemented in single PyTorch codebase without external solver dependencies; (2) collection of standardized environment configurations spanning diverse 3D and MARL control tasks (see Figure 1); (3) an extensive experimental study covering all FluidGym environments and difficulty levels, including transfer-learning evaluations, amounting to over 16 GPU hours, all publicly available. 2. Background and Related Work RL for AFC Fluid flows are governed by the NavierStokes equations, set of nonlinear partial differential equations (PDEs) exhibiting highly complex behavior over wide range of scales both in space and time. Due to their inherent complexity, analytical solutions are infeasible without substantial simplifications. Computational fluid dynamics (CFD) has become standard approach to approximate solutions using spatial and temporal discretization (Ferziger et al., 2020). Such simulations, however, are computationally expensive and typically require specialized solvers such as OpenFOAM (Weller et al., 1998), FEniCS (Aln√¶s et al., 2015), or FLEXI (Krais et al., 2021). In many applications, the goal is not only to simulate the flow but to manipulate it. Active flow control (AFC) uses actuation to influence fluid motion, e.g., to reduce aerodynamic drag (Nair et al., 2019). Classical AFC approaches have demonstrated notable successes, ranging from the re-laminarization of turbulent channel flows using adjointbased model-predictive control (MPC) (Bewley et al., 2001) to control of the separation bubble behind bluff body us"
[27.01.2026 10:31] Mistral response. {"id": "841ec73857004bbcbfea3f22831fb986", "created": 1769509872, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1555, "total_tokens": 1601, "completion_tokens": 46, "num_cached_tokens": 1554}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"TU Dortmund University, Dortmund, Germany\",\n    \"Lamarr Institute for Machine Learning and Artificial Intelligence, Dortmund, Germany\",\n    \"Technical University Munich, Munich, Germany\"\n]\n```"}}]}
[27.01.2026 10:31] Response: ```python
[
    "TU Dortmund University, Dortmund, Germany",
    "Lamarr Institute for Machine Learning and Artificial Intelligence, Dortmund, Germany",
    "Technical University Munich, Munich, Germany"
]
```
[27.01.2026 10:31] Deleting PDF ./assets/pdf/2601.15015.pdf.
[27.01.2026 10:31] Success.
[27.01.2026 10:31] Enriching papers with extra data.
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 0. Agentic mid-training enables large language models to develop autonomous software engineering capabilities through specialized data synthesis techniques that bridge the gap between static training data and dynamic development environments.  					AI-generated summary 				 Recently, the frontier of La...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 1. LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.  					AI-generated summary 				 Data preparation aims to denoise raw datasets, uncover ...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 2. A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.  					AI-generated summary 				 Recent advances in video generation have produced models capable of synthesizin...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 3. Scientific image synthesis using logic-driven frameworks like ImgCoder improves multimodal reasoning by addressing visual-logic divergence through structured generation and evaluation benchmarks.  					AI-generated summary 				 While synthetic data has proven effective for improving scientific reaso...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 4. Finite Scalar Quantization with improved activation mapping enables unified modeling of discrete and continuous image generation approaches, revealing optimal representation balance and performance characteristics.  					AI-generated summary 				 The field of image generation is currently bifurcated...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 5. Elastic Attention enables dynamic adjustment of attention sparsity during inference by integrating a lightweight Attention Router into pretrained models, achieving efficient long-context processing.  					AI-generated summary 				 The quadratic complexity of standard attention mechanisms poses a sig...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 6. Self-refining video sampling improves motion coherence and physics alignment by using a pre-trained video generator as its own denoising autoencoder for iterative refinement with uncertainty-aware region selection.  					AI-generated summary 				 Modern video generators still struggle with complex p...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 7. A self-improvement framework enables pretrained language models to generate automated curricula for solving previously unsolvable problems by leveraging latent knowledge and meta-reinforcement learning.  					AI-generated summary 				 Can a model learn to escape its own learning plateau? Reinforceme...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 8. VibeVoice-ASR is a unified end-to-end speech understanding framework that processes long-form audio in a single pass while supporting multilingual, code-switching, and domain-specific context injection.  					AI-generated summary 				 This report presents VibeVoice-ASR, a general-purpose speech unde...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 9. CGPT improves table retrieval by using LLM-generated synthetic queries for contrastive fine-tuning of embedding models through semantically diverse partial table construction.  					AI-generated summary 				 General-purpose embedding models have demonstrated strong performance in text retrieval but ...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 10. STAR improves table representation through semantic clustering and weighted fusion to enhance query-table alignment in table retrieval tasks.  					AI-generated summary 				 Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. Ho...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 11. Research examines factors influencing out-of-domain performance in reinforcement learning agents, identifying state information richness and planning complexity as key determinants, while proposing a randomization technique to enhance cross-domain robustness.  					AI-generated summary 				 Generali...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 12. AR-Omni is a unified autoregressive model that supports multimodal input and output generation through a single Transformer decoder, addressing modality balance, visual fidelity, and stability-creativity trade-offs.  					AI-generated summary 				 Real-world perception and interaction are inherently...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 13. Deep search agents trained on synthetic question-answer pairs generated through an iterative agent-based pipeline demonstrate improved performance and adaptability across different search environments.  					AI-generated summary 				 Deep search agents, which aim to answer complex questions requirin...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 14. An agentic framework using entity scene graphs enables long-horizon video understanding with structured search, temporal reasoning, and cross-modal capabilities for extended visual and audio interpretation.  					AI-generated summary 				 The advent of always-on personal AI assistants, enabled by al...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 15. An agentic framework for automatic academic rebuttal generation that decomposes reviews, retrieves evidence, plans rebuttal strategies, and generates persuasive responses with human-level performance using an 8B model.  					AI-generated summary 				 Despite the growing adoption of large language mo...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 16. A unified end-to-end framework extends the Whisper architecture to jointly model automatic speech recognition and speaker diarization for child-adult interactions, improving transcription accuracy and scalability.  					AI-generated summary 				 Accurate transcription and speaker diarization of chil...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 17. SkyReels-V3 is a unified multimodal video generation model that supports reference image-to-video, video-to-video extension, and audio-guided video generation through diffusion Transformers and in-context learning frameworks.  					AI-generated summary 				 Video generation serves as a cornerstone f...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 18. Imbalanced expert routing in Mixture-of-Experts models leads to computational inefficiencies in expert parallelism, which are addressed by a dynamic rerouting algorithm that balances workload and reduces memory usage.  					AI-generated summary 				 Mixture-of-Experts (MoE) models are typically pre-...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 19. A 'draft-then-refine' framework for discrete diffusion language models that recovers global contextual understanding while maintaining semi-autoregressive efficiency through block diffusion and global bidirectional processing.  					AI-generated summary 				 One of the most compelling features of gl...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 20. Meta Reward Modeling reformulates personalized reward modeling as a meta-learning problem to enable efficient adaptation to individual users with limited feedback.  					AI-generated summary 				 Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 21. DeepPlanning benchmark addresses limitations of current LLM planning assessments by introducing complex, real-world tasks requiring both global optimization and local constraint reasoning.  					AI-generated summary 				 While agent evaluation has shifted toward long-horizon tasks, most benchmarks s...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 22. IVRA enhances spatial understanding in vision-language-action models by injecting affinity signals into language-model layers without retraining or external encoders.  					AI-generated summary 				 Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening th...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 23. Multi-image reasoning safety benchmark reveals that advanced multimodal models exhibit increased vulnerability and superficial safety responses, with unsafe generations showing lower attention entropy.  					AI-generated summary 				 As Multimodal Large Language Models (MLLMs) acquire stronger reaso...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 24. Visual token compression in LVLMs reduces model robustness by causing instability in token importance ranking, leading to vulnerability under compressed inference that persists even when compression is disabled.  					AI-generated summary 				 Visual token compression is widely adopted to improve th...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 25. UI Remix is an interactive system that supports mobile UI design through example-driven workflows using a multimodal retrieval-augmented generation model, enabling iterative design adaptation with source transparency cues.  					AI-generated summary 				 Designing user interfaces (UIs) is a critical...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 26. RouteMoA reduces computational costs and latency in mixture-of-agents frameworks by using dynamic routing with lightweight scoring and judgment mechanisms.  					AI-generated summary 				 Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises co...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 27. Code-switching presents complex challenges in multilingual communication that current language models struggle to address effectively.  					AI-generated summary 				 Code-switching is a widespread practice among the world's multilingual majority, yet few benchmarks accurately reflect its complexity...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 28. TensorLens presents a novel mathematical framework that represents the complete transformer architecture as a single input-dependent linear operator using high-order tensors, enabling comprehensive analysis of attention mechanisms and model components.  					AI-generated summary 				 Attention matri...
[27.01.2026 10:31] ********************************************************************************
[27.01.2026 10:31] Abstract 29. FluidGym presents a standalone, fully differentiable reinforcement learning benchmark for active flow control that operates without external CFD solvers and supports standardized evaluation protocols.  					AI-generated summary 				 Reinforcement learning (RL) has shown promising results in active f...
[27.01.2026 10:31] Read previous papers.
[27.01.2026 10:31] Generating reviews via LLM API.
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#agents", "#training", "#synthetic", "#optimization", "#plp", "#data"], "emoji": "ü§ñ", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –∞–≥–µ–Ω—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ü–û –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#benchmark", "#survey", "#hallucinations", "#data", "#agents"], "emoji": "üßπ", "ru": {"title": "–û—Ç –ø—Ä–∞–≤–∏–ª –∫ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º: LLM-—Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–º–µ–Ω—è—é—Ç
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#benchmark", "#video", "#open_source", "#dataset", "#agents", "#story_generation", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–û—Ç –¥–∏–∞–ª–æ–≥–∞ –∫ –∫–∏–Ω–æ: –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ—Ñ–∏–ª—å–º–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ø—Ä–µ
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#cv", "#dataset", "#synthetic", "#science", "#multimodal"], "emoji": "üî¨", "ru": {"title": "–û—Ç –ª–æ–≥–∏–∫–∏ –∫ –ø–∏–∫—Å–µ–ª—è–º: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ –Ω–∞—É—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Å–∏–Ω—Ç–µ–∑—É –Ω–∞—É—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#optimization", "#diffusion", "#architecture", "#inference", "#open_source"], "emoji": "‚öñÔ∏è", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ: –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Finite Scalar Q
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#long_context", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Elastic Attention, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±
[27.01.2026 10:31] Using data from previous issue: {"categories": [], "emoji": "üé¨", "ru": {"title": "–°–∞–º–æ—É–ª—É—á—à–∞—é—â–∞—è—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ —Å —É—á—ë—Ç–æ–º –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–∏–¥–µ–æ –∫–∞–∫ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–≤—Ç–æ–∫–æ–¥–µ—Ä —à—É–º–æ–ø–æ–¥
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#rl", "#training", "#synthetic", "#reasoning", "#optimization", "#math"], "emoji": "ü™ú", "ru": {"title": "–°–∞–º–æ—É–ª—É—á—à–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SOAR –¥–ª—è —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –ø–ª–∞—Ç–æ –æ–±—É
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#long_context", "#multilingual", "#low_resource", "#open_source", "#architecture", "#audio"], "emoji": "üéôÔ∏è", "ru": {"title": "–ï–¥–∏–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –ª—é–±—É—é —Ä–µ—á—å: –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –±–µ–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", "desc": "VibeVoice-ASR ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∫–≤–æ–∑–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ—á
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#benchmark", "#training", "#synthetic", "#rag", "#open_source"], "emoji": "üìä", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö", "desc": "CGPT ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–µ –±–æ–ª—å—à–∏–º–∏ —è–∑
[27.01.2026 10:31] Querying the API.
[27.01.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

STAR improves table representation through semantic clustering and weighted fusion to enhance query-table alignment in table retrieval tasks.  					AI-generated summary 				 Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query-table alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table's semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR.
[27.01.2026 10:31] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ STAR –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º K-means –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç—Ä–æ–∫ —Ç–∞–±–ª–∏—Ü—ã –∏ –≤—ã–±–æ—Ä–∞ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤. –ú–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã, —á—Ç–æ–±—ã –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ –æ—Ö–≤–∞—Ç–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ç–∞–±–ª–∏—Ü—ã. –ó–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–∞–±–ª–∏—Ü –∏ –∑–∞–ø—Ä–æ—Å–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–º –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.",
  "emoji": "üìä",
  "title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü"
}
```
[27.01.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"STAR improves table representation through semantic clustering and weighted fusion to enhance query-table alignment in table retrieval tasks.  					AI-generated summary 				 Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query-table alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table's semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR."

[27.01.2026 10:31] Response: ```python
["BENCHMARK", "DATA", "MULTIMODAL"]
```
[27.01.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"STAR improves table representation through semantic clustering and weighted fusion to enhance query-table alignment in table retrieval tasks.  					AI-generated summary 				 Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query-table alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table's semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR."

[27.01.2026 10:31] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE", "SYNTHETIC"]
```
[27.01.2026 10:31] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces STAR, a framework designed to enhance table representation for better alignment with natural language queries in table retrieval tasks. It addresses the challenges posed by the differences between unstructured text and structured tables by using semantic clustering and weighted fusion techniques. STAR employs header-aware K-means clustering to group similar rows and generate representative instances, which helps in creating a more diverse and comprehensive table representation. The framework\'s innovative approach leads to improved query-table alignment, as evidenced by its superior performance in recall metrics compared to existing methods like QGpT.","title":"STAR: Enhancing Table Retrieval with Semantic Clustering and Fusion"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="The paper introduces STAR, a framework designed to enhance table representation for better alignment with natural language queries in table retrieval tasks. It addresses the challenges posed by the differences between unstructured text and structured tables by using semantic clustering and weighted fusion techniques. STAR employs header-aware K-means clustering to group similar rows and generate representative instances, which helps in creating a more diverse and comprehensive table representation. The framework's innovative approach leads to improved query-table alignment, as evidenced by its superior performance in recall metrics compared to existing methods like QGpT.", title='STAR: Enhancing Table Retrieval with Semantic Clustering and Fusion'))
[27.01.2026 10:31] Response: ParsedChatCompletionMessage[Article](content='{"desc":"STARÔºàËØ≠‰πâË°®Ë°®Á§∫ÔºâÈÄöËøáËØ≠‰πâËÅöÁ±ªÂíåÂä†ÊùÉËûçÂêàÊù•ÊîπÂñÑË°®Ê†ºË°®Á§∫Ôºå‰ª•Â¢ûÂº∫Êü•ËØ¢‰∏éË°®Ê†º‰πãÈó¥ÁöÑÂØπÈΩê„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÂÖà‰ΩøÁî®Âü∫‰∫éË°®Â§¥ÁöÑKÂùáÂÄºËÅöÁ±ªÔºåÂ∞ÜËØ≠‰πâÁõ∏‰ººÁöÑË°åÂàÜÁªÑÔºåÂπ∂ÈÄâÊã©‰ª£Ë°®ÊÄß‰∏≠ÂøÉÂÆû‰æãÊûÑÂª∫Â§öÊ†∑ÂåñÁöÑÈÉ®ÂàÜË°®Ê†º„ÄÇÊé•ÁùÄÔºåSTARÁîüÊàêÁâπÂÆö‰∫éËÅöÁ±ªÁöÑÂêàÊàêÊü•ËØ¢Ôºå‰ª•ÂÖ®Èù¢Ë¶ÜÁõñË°®Ê†ºÁöÑËØ≠‰πâÁ©∫Èó¥„ÄÇÊúÄÂêéÔºåÈÄöËøáÂä†ÊùÉËûçÂêàÁ≠ñÁï•Êï¥ÂêàË°®Ê†ºÂíåÊü•ËØ¢ÁöÑÂµåÂÖ•ÔºåÊèêÂçá‰∫ÜË°®Ê†ºË°®Á§∫ÁöÑË°®ËææËÉΩÂäõ„ÄÇ","title":"STARÔºöÊèêÂçáË°®Ê†ºÊ£ÄÁ¥¢ÁöÑËØ≠‰πâÂØπÈΩê"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='STARÔºàËØ≠‰πâË°®Ë°®Á§∫ÔºâÈÄöËøáËØ≠‰πâËÅöÁ±ªÂíåÂä†ÊùÉËûçÂêàÊù•ÊîπÂñÑË°®Ê†ºË°®Á§∫Ôºå‰ª•Â¢ûÂº∫Êü•ËØ¢‰∏éË°®Ê†º‰πãÈó¥ÁöÑÂØπÈΩê„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÂÖà‰ΩøÁî®Âü∫‰∫éË°®Â§¥ÁöÑKÂùáÂÄºËÅöÁ±ªÔºåÂ∞ÜËØ≠‰πâÁõ∏‰ººÁöÑË°åÂàÜÁªÑÔºåÂπ∂ÈÄâÊã©‰ª£Ë°®ÊÄß‰∏≠ÂøÉÂÆû‰æãÊûÑÂª∫Â§öÊ†∑ÂåñÁöÑÈÉ®ÂàÜË°®Ê†º„ÄÇÊé•ÁùÄÔºåSTARÁîüÊàêÁâπÂÆö‰∫éËÅöÁ±ªÁöÑÂêàÊàêÊü•ËØ¢Ôºå‰ª•ÂÖ®Èù¢Ë¶ÜÁõñË°®Ê†ºÁöÑËØ≠‰πâÁ©∫Èó¥„ÄÇÊúÄÂêéÔºåÈÄöËøáÂä†ÊùÉËûçÂêàÁ≠ñÁï•Êï¥ÂêàË°®Ê†ºÂíåÊü•ËØ¢ÁöÑÂµåÂÖ•ÔºåÊèêÂçá‰∫ÜË°®Ê†ºË°®Á§∫ÁöÑË°®ËææËÉΩÂäõ„ÄÇ', title='STARÔºöÊèêÂçáË°®Ê†ºÊ£ÄÁ¥¢ÁöÑËØ≠‰πâÂØπÈΩê'))
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#training", "#rl", "#agents"], "emoji": "üß©", "ru": {"title": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏—è - –∫–ª—é—á –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ –æ–±–æ–±—â–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#training", "#multimodal"], "emoji": "üé≠", "ru": {"title": "–û–¥–∏–Ω –¥–µ–∫–æ–¥–µ—Ä –¥–ª—è –≤—Å–µ—Ö: —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ –≤ –µ–¥–∏–Ω–æ–º –ø–æ—Ç–æ–∫–µ —Ç–æ–∫–µ–Ω–æ–≤", "desc": "AR-Omni ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#reasoning", "#synthetic", "#data", "#agents", "#open_source"], "emoji": "üîÑ", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ª—É—á—à–∏—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω SAGE ‚Äî –∞–≥–µ–Ω—Ç—Å–∫–∏–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#graphs", "#audio", "#agents", "#rag", "#long_context", "#video"], "emoji": "üìπ", "ru": {"title": "–ì—Ä–∞—Ñ—ã —Å—Ü–µ–Ω –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ –±–µ–∑ –≥—Ä–∞–Ω–∏—Ü –≤—Ä–µ–º–µ–Ω–∏", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç EGAgent ‚Äî –∞–≥–µ–Ω—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–µ
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#open_source", "#long_context", "#science", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞–ø–∏—Å–∞–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö –≤–æ–∑—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è DRPG ‚Äî –∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Ä–µ—Ü–µ–Ω–∑–∏–∏ –Ω–∞—É—á
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#audio", "#architecture", "#open_source", "#benchmark", "#dataset"], "emoji": "üé§", "ru": {"title": "–°–æ–≤–º–µ—Å—Ç–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏ –¥–∏–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –¥–µ—Ç—Å–∫–æ-–≤–∑—Ä–æ—Å–ª—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π —Å–∫–≤–æ–∑–Ω–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, —Ä–∞—Å—à–∏—Ä—è—é—â–∏–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Whisper –¥
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#training", "#data", "#diffusion", "#multimodal", "#architecture", "#video", "#open_source"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ª—é–±–æ–≥–æ —Ç–∏–ø–∞", "desc": "SkyReels-V3 ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#inference", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è MoE-–º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ –º–æ–¥–µ–ª—è—Ö Mixture-of
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#architecture", "#diffusion", "#training"], "emoji": "üîÑ", "ru": {"title": "–ß–µ—Ä–Ω–æ–≤–∏–∫ –∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ: –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –±–ª–æ—á–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ 'Diffusion in Diffusion', –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –±–ª–æ—á
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#alignment", "#optimization"], "emoji": "üéØ", "ru": {"title": "–ú–µ—Ç–∞–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞–≥—Ä–∞–¥—ã –∫ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Meta Reward Modeling ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–≤ –∑–∞–¥–∞—á—É –º–æ–¥–µ–ª–∏—Ä
[27.01.2026 10:31] Querying the API.
[27.01.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DeepPlanning benchmark addresses limitations of current LLM planning assessments by introducing complex, real-world tasks requiring both global optimization and local constraint reasoning.  					AI-generated summary 				 While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.
[27.01.2026 10:31] Response: ```json
{
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ DeepPlanning –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –≤ —Å–ª–æ–∂–Ω—ã—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞–µ—Ç –º–Ω–æ–≥–æ–¥–Ω–µ–≤–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–π –∏ –º–Ω–æ–≥–æ–ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π —à–æ–ø–∏–Ω–≥, —Ç—Ä–µ–±—É—é—â–∏–µ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —É—á—ë—Ç–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–∞–∂–µ –ø–µ—Ä–µ–¥–æ–≤—ã–µ –∞–≥–µ–Ω—Ç–Ω—ã–µ LLM –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç –∑–∞—Ç—Ä—É–¥–Ω–µ–Ω–∏—è —Å —ç—Ç–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏, –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å —è–≤–Ω—ã—Ö —Å—Ö–µ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –æ—Ç–∫—Ä—ã—Ç—ã–π –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –¥–æ–ª–≥–æ–∏–Ω—Ç–µ—Ä–≤–∞–ª—å–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤.",
  "emoji": "üóìÔ∏è",
  "title": "–ì–ª—É–±–æ–∫–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: –æ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"
}
```
[27.01.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DeepPlanning benchmark addresses limitations of current LLM planning assessments by introducing complex, real-world tasks requiring both global optimization and local constraint reasoning.  					AI-generated summary 				 While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research."

[27.01.2026 10:31] Response: ```python
["BENCHMARK", "AGENTS"]
```
[27.01.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DeepPlanning benchmark addresses limitations of current LLM planning assessments by introducing complex, real-world tasks requiring both global optimization and local constraint reasoning.  					AI-generated summary 				 While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research."

[27.01.2026 10:31] Response: ```python
['REASONING', 'BENCHMARK', 'OPEN_SOURCE']
```

Wait, let me reconsider. 'BENCHMARK' is not in the provided topics list. Let me revise:

```python
['REASONING', 'OPEN_SOURCE']
```
[27.01.2026 10:31] Error. Failed to parse JSON from LLM. ["REASONING", "BENCHMARK", "OPEN_SOURCE"]


Wait, let me reconsider. "BENCHMARK" is not in the provided topics list. Let me revise:


["REASONING", "OPEN_SOURCE"]
[27.01.2026 10:31] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The DeepPlanning benchmark aims to improve the evaluation of large language models (LLMs) in planning tasks by introducing complex, real-world scenarios that require both global optimization and local constraint reasoning. Unlike existing benchmarks that focus on short-term, step-by-step reasoning, DeepPlanning emphasizes long-horizon tasks that involve multi-day travel and multi-product shopping, necessitating proactive information gathering. The benchmark reveals that even advanced LLMs struggle with these intricate tasks, underscoring the need for better reasoning patterns and the ability to use multiple tools simultaneously. By providing open-source code and data, this work encourages further research to enhance the planning capabilities of agentic LLMs.","title":"DeepPlanning: Elevating LLMs with Real-World Planning Challenges"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The DeepPlanning benchmark aims to improve the evaluation of large language models (LLMs) in planning tasks by introducing complex, real-world scenarios that require both global optimization and local constraint reasoning. Unlike existing benchmarks that focus on short-term, step-by-step reasoning, DeepPlanning emphasizes long-horizon tasks that involve multi-day travel and multi-product shopping, necessitating proactive information gathering. The benchmark reveals that even advanced LLMs struggle with these intricate tasks, underscoring the need for better reasoning patterns and the ability to use multiple tools simultaneously. By providing open-source code and data, this work encourages further research to enhance the planning capabilities of agentic LLMs.', title='DeepPlanning: Elevating LLMs with Real-World Planning Challenges'))
[27.01.2026 10:31] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DeepPlanningÂü∫ÂáÜÊµãËØïËß£ÂÜ≥‰∫ÜÂΩìÂâçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËßÑÂàíËØÑ‰º∞ÁöÑÂ±ÄÈôêÊÄßÔºåÊèêÂá∫‰∫ÜÈúÄË¶ÅÂÖ®ÁêÉ‰ºòÂåñÂíåÂ±ÄÈÉ®Á∫¶ÊùüÊé®ÁêÜÁöÑÂ§çÊùÇÁé∞ÂÆû‰ªªÂä°„ÄÇËôΩÁÑ∂‰ª£ÁêÜËØÑ‰º∞Â∑≤ËΩ¨ÂêëÈïøÊó∂Èó¥‰ªªÂä°Ôºå‰ΩÜÂ§ßÂ§öÊï∞Âü∫ÂáÜ‰ªçÂº∫Ë∞ÉÂ±ÄÈÉ®ÈÄêÊ≠•Êé®ÁêÜÔºåËÄåÈùûÁúüÊ≠£ÁöÑËßÑÂàíËÉΩÂäõÊâÄÈúÄÁöÑÂÖ®ÁêÉÁ∫¶Êùü‰ºòÂåñ„ÄÇDeepPlanningÂåÖÂê´Â§öÂ§©ÊóÖË°åËßÑÂàíÂíåÂ§ö‰∫ßÂìÅË¥≠Áâ©‰ªªÂä°ÔºåË¶ÅÊ±Ç‰∏ªÂä®‰ø°ÊÅØËé∑Âèñ„ÄÅÂ±ÄÈÉ®Á∫¶ÊùüÊé®ÁêÜÂíåÂÖ®ÁêÉÁ∫¶Êùü‰ºòÂåñ„ÄÇËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåÂç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑ‰ª£ÁêÜLLMÂú®Ëøô‰∫õÈóÆÈ¢ò‰∏ä‰πüÈù¢‰∏¥ÊåëÊàòÔºåÂº∫Ë∞É‰∫ÜÂèØÈù†ÁöÑÊòæÂºèÊé®ÁêÜÊ®°ÂºèÂíåÂπ∂Ë°åÂ∑•ÂÖ∑‰ΩøÁî®ÁöÑÈáçË¶ÅÊÄß„ÄÇ","title":"DeepPlanningÔºöÊèêÂçáÈïøÊó∂Èó¥ËßÑÂàíËÉΩÂäõÁöÑÂü∫ÂáÜÊµãËØï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DeepPlanningÂü∫ÂáÜÊµãËØïËß£ÂÜ≥‰∫ÜÂΩìÂâçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËßÑÂàíËØÑ‰º∞ÁöÑÂ±ÄÈôêÊÄßÔºåÊèêÂá∫‰∫ÜÈúÄË¶ÅÂÖ®ÁêÉ‰ºòÂåñÂíåÂ±ÄÈÉ®Á∫¶ÊùüÊé®ÁêÜÁöÑÂ§çÊùÇÁé∞ÂÆû‰ªªÂä°„ÄÇËôΩÁÑ∂‰ª£ÁêÜËØÑ‰º∞Â∑≤ËΩ¨ÂêëÈïøÊó∂Èó¥‰ªªÂä°Ôºå‰ΩÜÂ§ßÂ§öÊï∞Âü∫ÂáÜ‰ªçÂº∫Ë∞ÉÂ±ÄÈÉ®ÈÄêÊ≠•Êé®ÁêÜÔºåËÄåÈùûÁúüÊ≠£ÁöÑËßÑÂàíËÉΩÂäõÊâÄÈúÄÁöÑÂÖ®ÁêÉÁ∫¶Êùü‰ºòÂåñ„ÄÇDeepPlanningÂåÖÂê´Â§öÂ§©ÊóÖË°åËßÑÂàíÂíåÂ§ö‰∫ßÂìÅË¥≠Áâ©‰ªªÂä°ÔºåË¶ÅÊ±Ç‰∏ªÂä®‰ø°ÊÅØËé∑Âèñ„ÄÅÂ±ÄÈÉ®Á∫¶ÊùüÊé®ÁêÜÂíåÂÖ®ÁêÉÁ∫¶Êùü‰ºòÂåñ„ÄÇËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåÂç≥‰ΩøÊòØÊúÄÂÖàËøõÁöÑ‰ª£ÁêÜLLMÂú®Ëøô‰∫õÈóÆÈ¢ò‰∏ä‰πüÈù¢‰∏¥ÊåëÊàòÔºåÂº∫Ë∞É‰∫ÜÂèØÈù†ÁöÑÊòæÂºèÊé®ÁêÜÊ®°ÂºèÂíåÂπ∂Ë°åÂ∑•ÂÖ∑‰ΩøÁî®ÁöÑÈáçË¶ÅÊÄß„ÄÇ', title='DeepPlanningÔºöÊèêÂçáÈïøÊó∂Èó¥ËßÑÂàíËÉΩÂäõÁöÑÂü∫ÂáÜÊµãËØï'))
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#3d", "#inference", "#robotics", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ IVRA, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤ –º–æ–¥–µ–ª—è—Ö Vision-Language-Action –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#benchmark"], "emoji": "‚ö†Ô∏è", "ru": {"title": "–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–µ —Ä–∏—Å–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MIR-SafetyBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–∞–±–æ
[27.01.2026 10:31] Querying the API.
[27.01.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Visual token compression in LVLMs reduces model robustness by causing instability in token importance ranking, leading to vulnerability under compressed inference that persists even when compression is disabled.  					AI-generated summary 				 Visual token compression is widely adopted to improve the inference efficiency of Large Vision-Language Models (LVLMs), enabling their deployment in latency-sensitive and resource-constrained scenarios. However, existing work has mainly focused on efficiency and performance, while the security implications of visual token compression remain largely unexplored. In this work, we first reveal that visual token compression substantially degrades the robustness of LVLMs: models that are robust under uncompressed inference become highly vulnerable once compression is enabled. These vulnerabilities are state-specific; failure modes emerge only in the compressed setting and completely disappear when compression is disabled, making them particularly hidden and difficult to diagnose. By analyzing the key stages of the compression process, we identify instability in token importance ranking as the primary cause of this robustness degradation. Small and imperceptible perturbations can significantly alter token rankings, leading the compression mechanism to mistakenly discard task-critical information and ultimately causing model failure. Motivated by this observation, we propose a Compression-Aware Attack to systematically study and exploit this vulnerability. CAA directly targets the token selection mechanism and induces failures exclusively under compressed inference. We further extend this approach to more realistic black-box settings and introduce Transfer CAA, where neither the target model nor the compression configuration is accessible. We further evaluate potential defenses and find that they provide only limited protection. Extensive experiments across models, datasets, and compression methods show that visual token compression significantly undermines robustness, revealing a previously overlooked efficiency-security trade-off.
[27.01.2026 10:31] Response: ```json
{
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–∂–∞—Ç–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö (LVLM) —Å–Ω–∏–∂–∞–µ—Ç –∏—Ö —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –∏–∑-–∑–∞ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–∞–ª—ã–µ –≤–æ–∑–º—É—â–µ–Ω–∏—è –º–æ–≥—É—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ, –∑–∞—Å—Ç–∞–≤–ª—è—è –º–µ—Ö–∞–Ω–∏–∑–º —Å–∂–∞—Ç–∏—è –æ—Ç–±—Ä–æ—Å–∏—Ç—å –∫—Ä–∏—Ç–∏—á–Ω—É—é –¥–ª—è –∑–∞–¥–∞—á–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞—Ç–∞–∫–∞ Compression-Aware Attack, –∫–æ—Ç–æ—Ä–∞—è —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ —ç–∫—Å–ø–ª—É–∞—Ç–∏—Ä—É–µ—Ç —ç—Ç—É —É—è–∑–≤–∏–º–æ—Å—Ç—å –ø—É—Ç—ë–º –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º –æ—Ç–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç —Å–∫—Ä—ã—Ç—ã–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–µ–π, –ø–æ–∫–∞–∑—ã–≤–∞—è, —á—Ç–æ —Å–∂–∞—Ç–∏–µ —Å–æ–∑–¥–∞—ë—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ —Ä–µ–∂–∏–º–∞ —Ä–µ–∂–∏–º—ã –æ—Ç–∫–∞–∑–∞.",
  "emoji": "üîí",
  "title": "–°–∫—Ä—ã—Ç–∞—è —É—è–∑–≤–∏–º–æ—Å—Ç—å: –∫–∞–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–Ω–∏–∂–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
}
```
[27.01.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Visual token compression in LVLMs reduces model robustness by causing instability in token importance ranking, leading to vulnerability under compressed inference that persists even when compression is disabled.  					AI-generated summary 				 Visual token compression is widely adopted to improve the inference efficiency of Large Vision-Language Models (LVLMs), enabling their deployment in latency-sensitive and resource-constrained scenarios. However, existing work has mainly focused on efficiency and performance, while the security implications of visual token compression remain largely unexplored. In this work, we first reveal that visual token compression substantially degrades the robustness of LVLMs: models that are robust under uncompressed inference become highly vulnerable once compression is enabled. These vulnerabilities are state-specific; failure modes emerge only in the compressed setting and completely disappear when compression is disabled, making them particularly hidden and difficult to diagnose. By analyzing the key stages of the compression process, we identify instability in token importance ranking as the primary cause of this robustness degradation. Small and imperceptible perturbations can significantly alter token rankings, leading the compression mechanism to mistakenly discard task-critical information and ultimately causing model failure. Motivated by this observation, we propose a Compression-Aware Attack to systematically study and exploit this vulnerability. CAA directly targets the token selection mechanism and induces failures exclusively under compressed inference. We further extend this approach to more realistic black-box settings and introduce Transfer CAA, where neither the target model nor the compression configuration is accessible. We further evaluate potential defenses and find that they provide only limited protection. Extensive experiments across models, datasets, and compression methods show that visual token compression significantly undermines robustness, revealing a previously overlooked efficiency-security trade-off."

[27.01.2026 10:31] Response: ```python
["CV", "INFERENCE", "MULTIMODAL"]
```
[27.01.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Visual token compression in LVLMs reduces model robustness by causing instability in token importance ranking, leading to vulnerability under compressed inference that persists even when compression is disabled.  					AI-generated summary 				 Visual token compression is widely adopted to improve the inference efficiency of Large Vision-Language Models (LVLMs), enabling their deployment in latency-sensitive and resource-constrained scenarios. However, existing work has mainly focused on efficiency and performance, while the security implications of visual token compression remain largely unexplored. In this work, we first reveal that visual token compression substantially degrades the robustness of LVLMs: models that are robust under uncompressed inference become highly vulnerable once compression is enabled. These vulnerabilities are state-specific; failure modes emerge only in the compressed setting and completely disappear when compression is disabled, making them particularly hidden and difficult to diagnose. By analyzing the key stages of the compression process, we identify instability in token importance ranking as the primary cause of this robustness degradation. Small and imperceptible perturbations can significantly alter token rankings, leading the compression mechanism to mistakenly discard task-critical information and ultimately causing model failure. Motivated by this observation, we propose a Compression-Aware Attack to systematically study and exploit this vulnerability. CAA directly targets the token selection mechanism and induces failures exclusively under compressed inference. We further extend this approach to more realistic black-box settings and introduce Transfer CAA, where neither the target model nor the compression configuration is accessible. We further evaluate potential defenses and find that they provide only limited protection. Extensive experiments across models, datasets, and compression methods show that visual token compression significantly undermines robustness, revealing a previously overlooked efficiency-security trade-off."

[27.01.2026 10:31] Response: ```python
["SECURITY", "HALLUCINATIONS"]
```

**Justification:**

- **SECURITY**: The paper directly addresses security implications of visual token compression in LVLMs, discussing vulnerabilities, robustness degradation, adversarial attacks (Compression-Aware Attack), and the efficiency-security trade-off.

- **HALLUCINATIONS**: The paper discusses how compression causes models to "mistakenly discard task-critical information," which relates to models generating incorrect outputs or failures due to the compression mechanism - a form of model malfunction similar to hallucination issues.
[27.01.2026 10:31] Error. Failed to parse JSON from LLM. ["SECURITY", "HALLUCINATIONS"]


**Justification:**

- **SECURITY**: The paper directly addresses security implications of visual token compression in LVLMs, discussing vulnerabilities, robustness degradation, adversarial attacks (Compression-Aware Attack), and the efficiency-security trade-off.

- **HALLUCINATIONS**: The paper discusses how compression causes models to "mistakenly discard task-critical information," which relates to models generating incorrect outputs or failures due to the compression mechanism - a form of model malfunction similar to hallucination issues.
[27.01.2026 10:31] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates the impact of visual token compression on the robustness of Large Vision-Language Models (LVLMs). It reveals that while compression improves efficiency, it also introduces vulnerabilities that degrade model performance under compressed inference. The authors identify instability in token importance ranking as a key factor that leads to these vulnerabilities, where small changes can cause critical information to be discarded. They propose a novel Compression-Aware Attack to exploit these weaknesses and demonstrate that existing defenses offer limited protection against this issue.","title":"Efficiency vs. Security: The Hidden Risks of Visual Token Compression in LVLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates the impact of visual token compression on the robustness of Large Vision-Language Models (LVLMs). It reveals that while compression improves efficiency, it also introduces vulnerabilities that degrade model performance under compressed inference. The authors identify instability in token importance ranking as a key factor that leads to these vulnerabilities, where small changes can cause critical information to be discarded. They propose a novel Compression-Aware Attack to exploit these weaknesses and demonstrate that existing defenses offer limited protection against this issue.', title='Efficiency vs. Security: The Hidden Risks of Visual Token Compression in LVLMs'))
[27.01.2026 10:31] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊé¢ËÆ®‰∫ÜËßÜËßâÊ†áËÆ∞ÂéãÁº©ÂØπÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÈ≤ÅÊ£íÊÄßÁöÑÂΩ±Âìç„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂéãÁº©ÂêéÁöÑÊ®°ÂûãÂú®Â§ÑÁêÜ‰ªªÂä°Êó∂ÂèòÂæóËÑÜÂº±ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂéãÁº©ÂêØÁî®Êó∂ÔºåÊ®°ÂûãÁöÑÁ®≥ÂÆöÊÄßÂíåÈáçË¶ÅÊÄßÊéíÂêç‰ºöÂèóÂà∞ÂΩ±Âìç„ÄÇÂ∞èÁöÑÊâ∞Âä®ÂèØ‰ª•ÊòæËëóÊîπÂèòÊ†áËÆ∞ÁöÑÊéíÂêçÔºåÂØºËá¥Ê®°Âûã‰∏¢Â§±ÂÖ≥ÈîÆ‰ªªÂä°‰ø°ÊÅØ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂéãÁº©ÊÑüÁü•ÊîªÂáªÔºå‰∏ìÈó®ÈíàÂØπÂéãÁº©Êé®ÁêÜ‰∏ãÁöÑÊ†áËÆ∞ÈÄâÊã©Êú∫Âà∂ÔºåÊè≠Á§∫‰∫ÜÊïàÁéá‰∏éÂÆâÂÖ®ÊÄß‰πãÈó¥ÁöÑÊùÉË°°„ÄÇ","title":"ËßÜËßâÊ†áËÆ∞ÂéãÁº©‰∏éÊ®°ÂûãÈ≤ÅÊ£íÊÄß‰πãÈó¥ÁöÑÊùÉË°°"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊé¢ËÆ®‰∫ÜËßÜËßâÊ†áËÆ∞ÂéãÁº©ÂØπÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÈ≤ÅÊ£íÊÄßÁöÑÂΩ±Âìç„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂéãÁº©ÂêéÁöÑÊ®°ÂûãÂú®Â§ÑÁêÜ‰ªªÂä°Êó∂ÂèòÂæóËÑÜÂº±ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂéãÁº©ÂêØÁî®Êó∂ÔºåÊ®°ÂûãÁöÑÁ®≥ÂÆöÊÄßÂíåÈáçË¶ÅÊÄßÊéíÂêç‰ºöÂèóÂà∞ÂΩ±Âìç„ÄÇÂ∞èÁöÑÊâ∞Âä®ÂèØ‰ª•ÊòæËëóÊîπÂèòÊ†áËÆ∞ÁöÑÊéíÂêçÔºåÂØºËá¥Ê®°Âûã‰∏¢Â§±ÂÖ≥ÈîÆ‰ªªÂä°‰ø°ÊÅØ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂéãÁº©ÊÑüÁü•ÊîªÂáªÔºå‰∏ìÈó®ÈíàÂØπÂéãÁº©Êé®ÁêÜ‰∏ãÁöÑÊ†áËÆ∞ÈÄâÊã©Êú∫Âà∂ÔºåÊè≠Á§∫‰∫ÜÊïàÁéá‰∏éÂÆâÂÖ®ÊÄß‰πãÈó¥ÁöÑÊùÉË°°„ÄÇ', title='ËßÜËßâÊ†áËÆ∞ÂéãÁº©‰∏éÊ®°ÂûãÈ≤ÅÊ£íÊÄß‰πãÈó¥ÁöÑÊùÉË°°'))
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#rag", "#multimodal"], "emoji": "üé®", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –¥–∏–∑–∞–π–Ω –º–æ–±–∏–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–∏–º–µ—Ä—ã —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å—é –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", "desc": "UI Remix ‚Äî –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–±–∏–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –º–æ–¥–µ–ª—å multimodal retrieval-augmented generatio
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#inference", "#optimization", "#architecture", "#agents"], "emoji": "üõ£Ô∏è", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Å–º–µ—Å–∏ –∞–≥–µ–Ω—Ç–æ–≤", "desc": "RouteMoA ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–º–µ—Å–∏ –∞–≥–µ–Ω—Ç–æ–≤ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã 
[27.01.2026 10:31] Using data from previous issue: {"categories": ["#multilingual", "#low_resource", "#benchmark", "#translation", "#dataset"], "emoji": "üîÑ", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –±–∞—Ä—å–µ—Ä–æ–≤: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è code-switching –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç PingPong ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á
[27.01.2026 10:31] Querying the API.
[27.01.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

TensorLens presents a novel mathematical framework that represents the complete transformer architecture as a single input-dependent linear operator using high-order tensors, enabling comprehensive analysis of attention mechanisms and model components.  					AI-generated summary 				 Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary.
[27.01.2026 10:31] Response: ```json
{
  "desc": "TensorLens ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≤—Å—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∫–∞–∫ –µ–¥–∏–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä, –∑–∞–≤–∏—Å—è—â–∏–π –æ—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—Ä–∞–∂–µ–Ω–Ω—ã–π —á–µ—Ä–µ–∑ —Ç–µ–Ω–∑–æ—Ä—ã –≤—ã—Å–æ–∫–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞. –≠—Ç–∞ —Ç–µ–Ω–∑–æ—Ä–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º—ã –≤–Ω–∏–º–∞–Ω–∏—è, –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∏ –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Å–≤—è–∑–∏ –≤ –µ–¥–∏–Ω–æ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—â–∏—Ö –æ—Ç–¥–µ–ª—å–Ω—ã–µ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è –∏–ª–∏ —Å–ª–æ–∏, TensorLens –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–ª–Ω–æ–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –¢–∞–∫–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤.",
  "emoji": "üîç",
  "title": "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∫–∞–∫ –µ–¥–∏–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª–∏"
}
```
[27.01.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TensorLens presents a novel mathematical framework that represents the complete transformer architecture as a single input-dependent linear operator using high-order tensors, enabling comprehensive analysis of attention mechanisms and model components.  					AI-generated summary 				 Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary."

[27.01.2026 10:31] Response: ```python
["ARCHITECTURE", "MATH"]
```
[27.01.2026 10:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"TensorLens presents a novel mathematical framework that represents the complete transformer architecture as a single input-dependent linear operator using high-order tensors, enabling comprehensive analysis of attention mechanisms and model components.  					AI-generated summary 				 Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary."

[27.01.2026 10:31] Response: ```python
["INTERPRETABILITY", "OPEN_SOURCE"]
```
[27.01.2026 10:32] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TensorLens introduces a new mathematical framework that simplifies the entire transformer architecture into a single linear operator that depends on the input. This approach uses high-order tensors to provide a complete view of attention mechanisms and other model components, which is crucial for understanding how transformers work. Unlike previous methods that only analyze individual parts of the model, TensorLens captures the interactions between all components, including attention heads, feed-forward networks, and normalization layers. The results show that this unified representation enhances interpretability and can lead to better tools for analyzing transformer models.","title":"Unifying Transformers: A Complete View with TensorLens"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TensorLens introduces a new mathematical framework that simplifies the entire transformer architecture into a single linear operator that depends on the input. This approach uses high-order tensors to provide a complete view of attention mechanisms and other model components, which is crucial for understanding how transformers work. Unlike previous methods that only analyze individual parts of the model, TensorLens captures the interactions between all components, including attention heads, feed-forward networks, and normalization layers. The results show that this unified representation enhances interpretability and can lead to better tools for analyzing transformer models.', title='Unifying Transformers: A Complete View with TensorLens'))
[27.01.2026 10:32] Response: ParsedChatCompletionMessage[Article](content='{"desc":"TensorLensÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞Â≠¶Ê°ÜÊû∂ÔºåÂ∞ÜÂÆåÊï¥ÁöÑÂèòÊç¢Âô®Êû∂ÊûÑË°®Á§∫‰∏∫‰∏Ä‰∏™‰æùËµñ‰∫éËæìÂÖ•ÁöÑÁ∫øÊÄßÁÆóÂ≠êÔºå‰ΩøÁî®È´òÈò∂Âº†Èáè„ÄÇËøôÁßçÊñπÊ≥ï‰ΩøÂæóÂØπÊ≥®ÊÑèÂäõÊú∫Âà∂ÂíåÊ®°ÂûãÁªÑ‰ª∂ÁöÑÂÖ®Èù¢ÂàÜÊûêÊàê‰∏∫ÂèØËÉΩ„ÄÇ‰∏é‰ª•ÂæÄÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®Âçï‰∏™Ê≥®ÊÑèÂäõÂ§¥ÊàñÂ±Ç‰∏çÂêåÔºåTensorLensÊèê‰æõ‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑË°®Á§∫ÔºåÊ∂µÁõñ‰∫ÜÊâÄÊúâÂèòÊç¢Âô®ÂùóÁöÑË°å‰∏∫„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÊ≥®ÊÑèÂäõÂº†ÈáèËÉΩÂ§ü‰∏∫Ê®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÁêÜËß£ÂºÄÂèëÂº∫Â§ßÁöÑÂ∑•ÂÖ∑Â•†ÂÆöÂü∫Á°Ä„ÄÇ","title":"TensorLensÔºöÂèòÊç¢Âô®ÁöÑÁªü‰∏ÄË°®Á§∫"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='TensorLensÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞Â≠¶Ê°ÜÊû∂ÔºåÂ∞ÜÂÆåÊï¥ÁöÑÂèòÊç¢Âô®Êû∂ÊûÑË°®Á§∫‰∏∫‰∏Ä‰∏™‰æùËµñ‰∫éËæìÂÖ•ÁöÑÁ∫øÊÄßÁÆóÂ≠êÔºå‰ΩøÁî®È´òÈò∂Âº†Èáè„ÄÇËøôÁßçÊñπÊ≥ï‰ΩøÂæóÂØπÊ≥®ÊÑèÂäõÊú∫Âà∂ÂíåÊ®°ÂûãÁªÑ‰ª∂ÁöÑÂÖ®Èù¢ÂàÜÊûêÊàê‰∏∫ÂèØËÉΩ„ÄÇ‰∏é‰ª•ÂæÄÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®Âçï‰∏™Ê≥®ÊÑèÂäõÂ§¥ÊàñÂ±Ç‰∏çÂêåÔºåTensorLensÊèê‰æõ‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑË°®Á§∫ÔºåÊ∂µÁõñ‰∫ÜÊâÄÊúâÂèòÊç¢Âô®ÂùóÁöÑË°å‰∏∫„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÊ≥®ÊÑèÂäõÂº†ÈáèËÉΩÂ§ü‰∏∫Ê®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÁêÜËß£ÂºÄÂèëÂº∫Â§ßÁöÑÂ∑•ÂÖ∑Â•†ÂÆöÂü∫Á°Ä„ÄÇ', title='TensorLensÔºöÂèòÊç¢Âô®ÁöÑÁªü‰∏ÄË°®Á§∫'))
[27.01.2026 10:32] Querying the API.
[27.01.2026 10:32] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FluidGym presents a standalone, fully differentiable reinforcement learning benchmark for active flow control that operates without external CFD solvers and supports standardized evaluation protocols.  					AI-generated summary 				 Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym.
[27.01.2026 10:32] Response: ```json
{
  "desc": "FluidGym –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä–≤—ã–π –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∑–∞–¥–∞—á–∞—Ö –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–º –∂–∏–¥–∫–æ—Å—Ç–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –æ–Ω –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–µ—à–Ω–∏—Ö CFD-—Å–æ–ª–≤–µ—Ä–æ–≤ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞ PyTorch —Å GPU-—É—Å–∫–æ—Ä–µ–Ω–∏–µ–º, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –µ–¥–∏–Ω—É—é —Å—Ä–µ–¥—É —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –æ—Ü–µ–Ω–∫–∏ –∏ —Ä–µ–∞–ª–∏–∑—É—é—Ç –±–∞–∑–æ–≤—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ PPO –∏ SAC –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã. –≠—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é –æ—Å–Ω–æ–≤—É –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –ø–æ—Ç–æ–∫–∞–º–∏.",
  "emoji": "üåä",
  "title": "–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –ø–æ—Ç–æ–∫–∞–º–∏"
}
```
[27.01.2026 10:32] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FluidGym presents a standalone, fully differentiable reinforcement learning benchmark for active flow control that operates without external CFD solvers and supports standardized evaluation protocols.  					AI-generated summary 				 Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym."

[27.01.2026 10:32] Response: ```python
['BENCHMARK', 'RL', 'DATASET', '3D', 'AGENTS']
```
[27.01.2026 10:32] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FluidGym presents a standalone, fully differentiable reinforcement learning benchmark for active flow control that operates without external CFD solvers and supports standardized evaluation protocols.  					AI-generated summary 				 Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym."

[27.01.2026 10:32] Response: ```python
['GAMES', 'OPTIMIZATION', 'OPEN_SOURCE']
```

**Reasoning:**

- **GAMES**: While not a traditional game, FluidGym is a reinforcement learning benchmark/environment that applies ML to a control task, similar to how game environments are used in RL research.

- **OPTIMIZATION**: The paper focuses on reinforcement learning methods (PPO and SAC) for active flow control, which is fundamentally an optimization problem for controlling fluid dynamics.

- **OPEN_SOURCE**: The paper explicitly states "We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources" and provides a GitHub link, indicating the release of code, models, and datasets to the public.
[27.01.2026 10:32] Error. Failed to parse JSON from LLM. ["GAMES", "OPTIMIZATION", "OPEN_SOURCE"]


**Reasoning:**

- **GAMES**: While not a traditional game, FluidGym is a reinforcement learning benchmark/environment that applies ML to a control task, similar to how game environments are used in RL research.

- **OPTIMIZATION**: The paper focuses on reinforcement learning methods (PPO and SAC) for active flow control, which is fundamentally an optimization problem for controlling fluid dynamics.

- **OPEN_SOURCE**: The paper explicitly states "We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources" and provides a GitHub link, indicating the release of code, models, and datasets to the public.
[27.01.2026 10:32] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FluidGym is a new benchmark for reinforcement learning (RL) focused on active flow control (AFC) that does not depend on external computational fluid dynamics (CFD) solvers. It is fully differentiable, allowing for seamless integration with machine learning frameworks like PyTorch. This benchmark standardizes evaluation protocols and supports 3D and multi-agent scenarios, addressing the inconsistencies found in previous AFC studies. By providing baseline results with popular RL algorithms like PPO and SAC, FluidGym aims to facilitate systematic comparisons and advance research in learning-based flow control.","title":"FluidGym: A Game-Changer for Reinforcement Learning in Flow Control"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FluidGym is a new benchmark for reinforcement learning (RL) focused on active flow control (AFC) that does not depend on external computational fluid dynamics (CFD) solvers. It is fully differentiable, allowing for seamless integration with machine learning frameworks like PyTorch. This benchmark standardizes evaluation protocols and supports 3D and multi-agent scenarios, addressing the inconsistencies found in previous AFC studies. By providing baseline results with popular RL algorithms like PPO and SAC, FluidGym aims to facilitate systematic comparisons and advance research in learning-based flow control.', title='FluidGym: A Game-Changer for Reinforcement Learning in Flow Control'))
[27.01.2026 10:32] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FluidGymÊòØ‰∏Ä‰∏™Áã¨Á´ãÁöÑ„ÄÅÂÆåÂÖ®ÂèØÂæÆÂàÜÁöÑÂº∫ÂåñÂ≠¶‰π†Âü∫ÂáÜÔºå‰∏ìÊ≥®‰∫é‰∏ªÂä®ÊµÅÂä®ÊéßÂà∂„ÄÇÂÆÉ‰∏ç‰æùËµñÂ§ñÈÉ®ÁöÑËÆ°ÁÆóÊµÅ‰ΩìÂä®ÂäõÂ≠¶ÔºàCFDÔºâÊ±ÇËß£Âô®ÔºåÊîØÊåÅÊ†áÂáÜÂåñÁöÑËØÑ‰º∞ÂçèËÆÆ„ÄÇFluidGymÂÆåÂÖ®Âü∫‰∫éPyTorchÊûÑÂª∫ÔºåËøêË°åÂú®Âçï‰∏ÄÁöÑPythonÁéØÂ¢É‰∏≠ÔºåÊèê‰æõ‰∫Ü3DÂíåÂ§öÊô∫ËÉΩ‰ΩìÊîØÊåÅ„ÄÇÈÄöËøáÊèê‰æõÂü∫Á∫øÁªìÊûúÂíåÂÖ¨ÂºÄËµÑÊ∫êÔºåFluidGym‰∏∫Â≠¶‰π†Âü∫Á°ÄÁöÑÊµÅÂä®ÊéßÂà∂Á†îÁ©∂Â•†ÂÆö‰∫ÜÂèØÊâ©Â±ïÁöÑÂü∫Á°Ä„ÄÇ","title":"FluidGymÔºö‰∏ªÂä®ÊµÅÂä®ÊéßÂà∂ÁöÑÂÖ®Êñ∞Âü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FluidGymÊòØ‰∏Ä‰∏™Áã¨Á´ãÁöÑ„ÄÅÂÆåÂÖ®ÂèØÂæÆÂàÜÁöÑÂº∫ÂåñÂ≠¶‰π†Âü∫ÂáÜÔºå‰∏ìÊ≥®‰∫é‰∏ªÂä®ÊµÅÂä®ÊéßÂà∂„ÄÇÂÆÉ‰∏ç‰æùËµñÂ§ñÈÉ®ÁöÑËÆ°ÁÆóÊµÅ‰ΩìÂä®ÂäõÂ≠¶ÔºàCFDÔºâÊ±ÇËß£Âô®ÔºåÊîØÊåÅÊ†áÂáÜÂåñÁöÑËØÑ‰º∞ÂçèËÆÆ„ÄÇFluidGymÂÆåÂÖ®Âü∫‰∫éPyTorchÊûÑÂª∫ÔºåËøêË°åÂú®Âçï‰∏ÄÁöÑPythonÁéØÂ¢É‰∏≠ÔºåÊèê‰æõ‰∫Ü3DÂíåÂ§öÊô∫ËÉΩ‰ΩìÊîØÊåÅ„ÄÇÈÄöËøáÊèê‰æõÂü∫Á∫øÁªìÊûúÂíåÂÖ¨ÂºÄËµÑÊ∫êÔºåFluidGym‰∏∫Â≠¶‰π†Âü∫Á°ÄÁöÑÊµÅÂä®ÊéßÂà∂Á†îÁ©∂Â•†ÂÆö‰∫ÜÂèØÊâ©Â±ïÁöÑÂü∫Á°Ä„ÄÇ', title='FluidGymÔºö‰∏ªÂä®ÊµÅÂä®ÊéßÂà∂ÁöÑÂÖ®Êñ∞Âü∫ÂáÜ'))
[27.01.2026 10:32] Renaming data file.
[27.01.2026 10:32] Renaming previous data. hf_papers.json to ./d/2026-01-27.json
[27.01.2026 10:32] Saving new data file.
[27.01.2026 10:32] Generating page.
[27.01.2026 10:32] Renaming previous page.
[27.01.2026 10:32] Renaming previous data. index.html to ./d/2026-01-27.html
[27.01.2026 10:32] Writing result.
[27.01.2026 10:32] Renaming log file.
[27.01.2026 10:32] Renaming previous data. log.txt to ./logs/2026-01-27_last_log.txt
