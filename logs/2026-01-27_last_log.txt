[27.01.2026 04:55] Read previous papers.
[27.01.2026 04:55] Generating top page (month).
[27.01.2026 04:55] Writing top page (month).
[27.01.2026 05:29] Read previous papers.
[27.01.2026 05:29] Get feed.
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17058
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17737
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17367
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17124
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17027
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18577
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18184
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17761
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17323
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17111
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15849
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18202
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18157
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18130
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18081
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16207
[27.01.2026 05:29] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18759
[27.01.2026 05:29] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.01.2026 05:29] No deleted papers detected.
[27.01.2026 05:29] Downloading and parsing papers (pdf, html). Total: 17.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.17058.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.17058.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.17058.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.17737.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.17737.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.17737.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.17367.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.17367.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.17367.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.17124.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.17124.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.17124.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.17027.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.17027.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.17027.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.18577.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.18577.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.18577.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.18184.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.18184.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.18184.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.17761.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.17761.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.17761.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.17323.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.17323.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.17323.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.17111.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.17111.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.17111.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.15849.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.15849.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.15849.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.18202.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.18202.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.18202.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.18157.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.18157.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.18157.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.18130.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.18130.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.18130.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.18081.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.18081.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.18081.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.16207.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.16207.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.16207.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Downloading and parsing paper https://huggingface.co/papers/2601.18759.
[27.01.2026 05:29] Extra JSON file exists (./assets/json/2601.18759.json), skip PDF parsing.
[27.01.2026 05:29] Paper image links file exists (./assets/img_data/2601.18759.json), skip HTML parsing.
[27.01.2026 05:29] Success.
[27.01.2026 05:29] Enriching papers with extra data.
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 0. LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.  					AI-generated summary 				 Data preparation aims to denoise raw datasets, uncover ...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 1. A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.  					AI-generated summary 				 Recent advances in video generation have produced models capable of synthesizin...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 2. Elastic Attention enables dynamic adjustment of attention sparsity during inference by integrating a lightweight Attention Router into pretrained models, achieving efficient long-context processing.  					AI-generated summary 				 The quadratic complexity of standard attention mechanisms poses a sig...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 3. Finite Scalar Quantization with improved activation mapping enables unified modeling of discrete and continuous image generation approaches, revealing optimal representation balance and performance characteristics.  					AI-generated summary 				 The field of image generation is currently bifurcated...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 4. Scientific image synthesis using logic-driven frameworks like ImgCoder improves multimodal reasoning by addressing visual-logic divergence through structured generation and evaluation benchmarks.  					AI-generated summary 				 While synthetic data has proven effective for improving scientific reaso...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 5. Self-refining video sampling improves motion coherence and physics alignment by using a pre-trained video generator as its own denoising autoencoder for iterative refinement with uncertainty-aware region selection.  					AI-generated summary 				 Modern video generators still struggle with complex p...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 6. VibeVoice-ASR is a unified end-to-end speech understanding framework that processes long-form audio in a single pass while supporting multilingual, code-switching, and domain-specific context injection.  					AI-generated summary 				 This report presents VibeVoice-ASR, a general-purpose speech unde...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 7. AR-Omni is a unified autoregressive model that supports multimodal input and output generation through a single Transformer decoder, addressing modality balance, visual fidelity, and stability-creativity trade-offs.  					AI-generated summary 				 Real-world perception and interaction are inherently...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 8. SkyReels-V3 is a unified multimodal video generation model that supports reference image-to-video, video-to-video extension, and audio-guided video generation through diffusion Transformers and in-context learning frameworks.  					AI-generated summary 				 Video generation serves as a cornerstone f...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 9. Imbalanced expert routing in Mixture-of-Experts models leads to computational inefficiencies in expert parallelism, which are addressed by a dynamic rerouting algorithm that balances workload and reduces memory usage.  					AI-generated summary 				 Mixture-of-Experts (MoE) models are typically pre-...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 10. CGPT improves table retrieval by using LLM-generated synthetic queries for contrastive fine-tuning of embedding models through semantically diverse partial table construction.  					AI-generated summary 				 General-purpose embedding models have demonstrated strong performance in text retrieval but ...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 11. Deep search agents trained on synthetic question-answer pairs generated through an iterative agent-based pipeline demonstrate improved performance and adaptability across different search environments.  					AI-generated summary 				 Deep search agents, which aim to answer complex questions requirin...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 12. An agentic framework using entity scene graphs enables long-horizon video understanding with structured search, temporal reasoning, and cross-modal capabilities for extended visual and audio interpretation.  					AI-generated summary 				 The advent of always-on personal AI assistants, enabled by al...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 13. RouteMoA reduces computational costs and latency in mixture-of-agents frameworks by using dynamic routing with lightweight scoring and judgment mechanisms.  					AI-generated summary 				 Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises co...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 14. An agentic framework for automatic academic rebuttal generation that decomposes reviews, retrieves evidence, plans rebuttal strategies, and generates persuasive responses with human-level performance using an 8B model.  					AI-generated summary 				 Despite the growing adoption of large language mo...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 15. IVRA enhances spatial understanding in vision-language-action models by injecting affinity signals into language-model layers without retraining or external encoders.  					AI-generated summary 				 Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening th...
[27.01.2026 05:29] ********************************************************************************
[27.01.2026 05:29] Abstract 16. UI Remix is an interactive system that supports mobile UI design through example-driven workflows using a multimodal retrieval-augmented generation model, enabling iterative design adaptation with source transparency cues.  					AI-generated summary 				 Designing user interfaces (UIs) is a critical...
[27.01.2026 05:29] Read previous papers.
[27.01.2026 05:29] Generating reviews via LLM API.
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#benchmark", "#survey", "#hallucinations", "#data", "#agents"], "emoji": "üßπ", "ru": {"title": "–û—Ç –ø—Ä–∞–≤–∏–ª –∫ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º: LLM-—Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–º–µ–Ω—è—é—Ç
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#benchmark", "#video", "#open_source", "#dataset", "#agents", "#story_generation", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–û—Ç –¥–∏–∞–ª–æ–≥–∞ –∫ –∫–∏–Ω–æ: –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ—Ñ–∏–ª—å–º–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ø—Ä–µ
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#long_context", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Elastic Attention, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#optimization", "#diffusion", "#architecture", "#inference", "#open_source"], "emoji": "‚öñÔ∏è", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ: –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Finite Scalar Q
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#cv", "#dataset", "#synthetic", "#science", "#multimodal"], "emoji": "üî¨", "ru": {"title": "–û—Ç –ª–æ–≥–∏–∫–∏ –∫ –ø–∏–∫—Å–µ–ª—è–º: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ –Ω–∞—É—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Å–∏–Ω—Ç–µ–∑—É –Ω–∞—É—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª
[27.01.2026 05:29] Using data from previous issue: {"categories": [], "emoji": "üé¨", "ru": {"title": "–°–∞–º–æ—É–ª—É—á—à–∞—é—â–∞—è—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ —Å —É—á—ë—Ç–æ–º –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–∏–¥–µ–æ –∫–∞–∫ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–≤—Ç–æ–∫–æ–¥–µ—Ä —à—É–º–æ–ø–æ–¥
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#long_context", "#multilingual", "#low_resource", "#open_source", "#architecture", "#audio"], "emoji": "üéôÔ∏è", "ru": {"title": "–ï–¥–∏–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –ª—é–±—É—é —Ä–µ—á—å: –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –±–µ–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", "desc": "VibeVoice-ASR ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∫–≤–æ–∑–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ—á
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#training", "#multimodal"], "emoji": "üé≠", "ru": {"title": "–û–¥–∏–Ω –¥–µ–∫–æ–¥–µ—Ä –¥–ª—è –≤—Å–µ—Ö: —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ –≤ –µ–¥–∏–Ω–æ–º –ø–æ—Ç–æ–∫–µ —Ç–æ–∫–µ–Ω–æ–≤", "desc": "AR-Omni ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#training", "#data", "#diffusion", "#multimodal", "#architecture", "#video", "#open_source"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ª—é–±–æ–≥–æ —Ç–∏–ø–∞", "desc": "SkyReels-V3 ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#inference", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è MoE-–º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ –º–æ–¥–µ–ª—è—Ö Mixture-of
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#benchmark", "#training", "#synthetic", "#rag", "#open_source"], "emoji": "üìä", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö", "desc": "CGPT ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–µ –±–æ–ª—å—à–∏–º–∏ —è–∑
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#reasoning", "#synthetic", "#data", "#agents", "#open_source"], "emoji": "üîÑ", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ª—É—á—à–∏—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω SAGE ‚Äî –∞–≥–µ–Ω—Ç—Å–∫–∏–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#graphs", "#audio", "#agents", "#rag", "#long_context", "#video"], "emoji": "üìπ", "ru": {"title": "–ì—Ä–∞—Ñ—ã —Å—Ü–µ–Ω –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ –±–µ–∑ –≥—Ä–∞–Ω–∏—Ü –≤—Ä–µ–º–µ–Ω–∏", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç EGAgent ‚Äî –∞–≥–µ–Ω—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–µ
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#inference", "#optimization", "#architecture", "#agents"], "emoji": "üõ£Ô∏è", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Å–º–µ—Å–∏ –∞–≥–µ–Ω—Ç–æ–≤", "desc": "RouteMoA ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–º–µ—Å–∏ –∞–≥–µ–Ω—Ç–æ–≤ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã 
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#open_source", "#long_context", "#science", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞–ø–∏—Å–∞–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö –≤–æ–∑—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è DRPG ‚Äî –∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Ä–µ—Ü–µ–Ω–∑–∏–∏ –Ω–∞—É—á
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#3d", "#inference", "#robotics", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ IVRA, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤ –º–æ–¥–µ–ª—è—Ö Vision-Language-Action –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.
[27.01.2026 05:29] Using data from previous issue: {"categories": ["#rag", "#multimodal"], "emoji": "üé®", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –¥–∏–∑–∞–π–Ω –º–æ–±–∏–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–∏–º–µ—Ä—ã —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å—é –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", "desc": "UI Remix ‚Äî –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–±–∏–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –º–æ–¥–µ–ª—å multimodal retrieval-augmented generatio
[27.01.2026 05:29] Renaming data file.
[27.01.2026 05:29] Renaming previous data. hf_papers.json to ./d/2026-01-27.json
[27.01.2026 05:29] Saving new data file.
[27.01.2026 05:29] Generating page.
[27.01.2026 05:29] Renaming previous page.
[27.01.2026 05:29] Renaming previous data. index.html to ./d/2026-01-27.html
[27.01.2026 05:29] Writing result.
[27.01.2026 05:29] Renaming log file.
[27.01.2026 05:29] Renaming previous data. log.txt to ./logs/2026-01-27_last_log.txt
