[27.01.2026 21:20] Read previous papers.
[27.01.2026 21:20] Generating top page (month).
[27.01.2026 21:20] Writing top page (month).
[27.01.2026 22:25] Read previous papers.
[27.01.2026 22:25] Get feed.
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17058
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18418
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17737
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17027
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17367
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17124
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18778
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18577
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18184
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15849
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18137
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15860
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18217
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17761
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18157
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18202
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18081
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17323
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17111
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16207
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18731
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17640
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18744
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17067
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13599
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17277
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15015
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14127
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.12042
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18790
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18759
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18753
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.18130
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.17958
[27.01.2026 22:25] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14103
[27.01.2026 22:25] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[27.01.2026 22:25] No deleted papers detected.
[27.01.2026 22:25] Downloading and parsing papers (pdf, html). Total: 35.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.17058.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.17058.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.17058.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18418.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18418.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18418.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.17737.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.17737.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.17737.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.17027.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.17027.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.17027.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.17367.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.17367.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.17367.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.17124.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.17124.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.17124.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18778.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18778.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18778.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18577.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18577.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18577.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18184.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18184.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18184.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.15849.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.15849.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.15849.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18137.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18137.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18137.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.15860.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.15860.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.15860.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18217.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18217.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18217.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.17761.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.17761.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.17761.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18157.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18157.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18157.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18202.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18202.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18202.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18081.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18081.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18081.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.17323.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.17323.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.17323.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.17111.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.17111.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.17111.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.16207.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.16207.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.16207.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18731.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18731.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18731.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.17640.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.17640.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.17640.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18744.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18744.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18744.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.17067.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.17067.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.17067.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.13599.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.13599.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.13599.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.17277.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.17277.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.17277.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.15015.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.15015.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.15015.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.14127.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.14127.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.14127.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.12042.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.12042.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.12042.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18790.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18790.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18790.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18759.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18759.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18759.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18753.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18753.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18753.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.18130.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.18130.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.18130.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.17958.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.17958.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.17958.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Downloading and parsing paper https://huggingface.co/papers/2601.14103.
[27.01.2026 22:25] Extra JSON file exists (./assets/json/2601.14103.json), skip PDF parsing.
[27.01.2026 22:25] Paper image links file exists (./assets/img_data/2601.14103.json), skip HTML parsing.
[27.01.2026 22:25] Success.
[27.01.2026 22:25] Enriching papers with extra data.
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 0. LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.  					AI-generated summary 				 Data preparation aims to denoise raw datasets, uncover ...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 1. Agentic mid-training enables large language models to develop autonomous software engineering capabilities through specialized data synthesis techniques that bridge the gap between static training data and dynamic development environments.  					AI-generated summary 				 Recently, the frontier of La...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 2. A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.  					AI-generated summary 				 Recent advances in video generation have produced models capable of synthesizin...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 3. Scientific image synthesis using logic-driven frameworks like ImgCoder improves multimodal reasoning by addressing visual-logic divergence through structured generation and evaluation benchmarks.  					AI-generated summary 				 While synthetic data has proven effective for improving scientific reaso...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 4. Elastic Attention enables dynamic adjustment of attention sparsity during inference by integrating a lightweight Attention Router into pretrained models, achieving efficient long-context processing.  					AI-generated summary 				 The quadratic complexity of standard attention mechanisms poses a sig...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 5. Finite Scalar Quantization with improved activation mapping enables unified modeling of discrete and continuous image generation approaches, revealing optimal representation balance and performance characteristics.  					AI-generated summary 				 The field of image generation is currently bifurcated...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 6. A self-improvement framework enables pretrained language models to generate automated curricula for solving previously unsolvable problems by leveraging latent knowledge and meta-reinforcement learning.  					AI-generated summary 				 Can a model learn to escape its own learning plateau? Reinforceme...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 7. Self-refining video sampling improves motion coherence and physics alignment by using a pre-trained video generator as its own denoising autoencoder for iterative refinement with uncertainty-aware region selection.  					AI-generated summary 				 Modern video generators still struggle with complex p...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 8. VibeVoice-ASR is a unified end-to-end speech understanding framework that processes long-form audio in a single pass while supporting multilingual, code-switching, and domain-specific context injection.  					AI-generated summary 				 This report presents VibeVoice-ASR, a general-purpose speech unde...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 9. CGPT improves table retrieval by using LLM-generated synthetic queries for contrastive fine-tuning of embedding models through semantically diverse partial table construction.  					AI-generated summary 				 General-purpose embedding models have demonstrated strong performance in text retrieval but ...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 10. DeepPlanning benchmark addresses limitations of current LLM planning assessments by introducing complex, real-world tasks requiring both global optimization and local constraint reasoning.  					AI-generated summary 				 While agent evaluation has shifted toward long-horizon tasks, most benchmarks s...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 11. STAR improves table representation through semantic clustering and weighted fusion to enhance query-table alignment in table retrieval tasks.  					AI-generated summary 				 Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. Ho...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 12. Research examines factors influencing out-of-domain performance in reinforcement learning agents, identifying state information richness and planning complexity as key determinants, while proposing a randomization technique to enhance cross-domain robustness.  					AI-generated summary 				 Generali...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 13. AR-Omni is a unified autoregressive model that supports multimodal input and output generation through a single Transformer decoder, addressing modality balance, visual fidelity, and stability-creativity trade-offs.  					AI-generated summary 				 Real-world perception and interaction are inherently...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 14. An agentic framework using entity scene graphs enables long-horizon video understanding with structured search, temporal reasoning, and cross-modal capabilities for extended visual and audio interpretation.  					AI-generated summary 				 The advent of always-on personal AI assistants, enabled by al...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 15. Deep search agents trained on synthetic question-answer pairs generated through an iterative agent-based pipeline demonstrate improved performance and adaptability across different search environments.  					AI-generated summary 				 Deep search agents, which aim to answer complex questions requirin...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 16. An agentic framework for automatic academic rebuttal generation that decomposes reviews, retrieves evidence, plans rebuttal strategies, and generates persuasive responses with human-level performance using an 8B model.  					AI-generated summary 				 Despite the growing adoption of large language mo...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 17. SkyReels-V3 is a unified multimodal video generation model that supports reference image-to-video, video-to-video extension, and audio-guided video generation through diffusion Transformers and in-context learning frameworks.  					AI-generated summary 				 Video generation serves as a cornerstone f...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 18. Imbalanced expert routing in Mixture-of-Experts models leads to computational inefficiencies in expert parallelism, which are addressed by a dynamic rerouting algorithm that balances workload and reduces memory usage.  					AI-generated summary 				 Mixture-of-Experts (MoE) models are typically pre-...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 19. IVRA enhances spatial understanding in vision-language-action models by injecting affinity signals into language-model layers without retraining or external encoders.  					AI-generated summary 				 Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening th...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 20. Meta Reward Modeling reformulates personalized reward modeling as a meta-learning problem to enable efficient adaptation to individual users with limited feedback.  					AI-generated summary 				 Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 21. A unified end-to-end framework extends the Whisper architecture to jointly model automatic speech recognition and speaker diarization for child-adult interactions, improving transcription accuracy and scalability.  					AI-generated summary 				 Accurate transcription and speaker diarization of chil...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 22. TSRBench presents a comprehensive multi-modal benchmark for evaluating time series reasoning capabilities across perception, reasoning, prediction, and decision-making dimensions, revealing that scaling laws and reasoning abilities do not uniformly apply to time series forecasting tasks.  					AI-ge...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 23. Video generation models are categorized based on state construction and dynamics modeling approaches, with emphasis on transitioning evaluation metrics from visual quality to functional capabilities like physical persistence and causal reasoning.  					AI-generated summary 				 Large-scale video gen...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 24. A 'draft-then-refine' framework for discrete diffusion language models that recovers global contextual understanding while maintaining semi-autoregressive efficiency through block diffusion and global bidirectional processing.  					AI-generated summary 				 One of the most compelling features of gl...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 25. Code-switching presents complex challenges in multilingual communication that current language models struggle to address effectively.  					AI-generated summary 				 Code-switching is a widespread practice among the world's multilingual majority, yet few benchmarks accurately reflect its complexity...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 26. FluidGym presents a standalone, fully differentiable reinforcement learning benchmark for active flow control that operates without external CFD solvers and supports standardized evaluation protocols.  					AI-generated summary 				 Reinforcement learning (RL) has shown promising results in active f...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 27. Multi-image reasoning safety benchmark reveals that advanced multimodal models exhibit increased vulnerability and superficial safety responses, with unsafe generations showing lower attention entropy.  					AI-generated summary 				 As Multimodal Large Language Models (MLLMs) acquire stronger reaso...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 28. Visual token compression in LVLMs reduces model robustness by causing instability in token importance ranking, leading to vulnerability under compressed inference that persists even when compression is disabled.  					AI-generated summary 				 Visual token compression is widely adopted to improve th...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 29. Specialized reasoning models prioritize task completion over safety, potentially ignoring critical emergencies during complex calculations.  					AI-generated summary 				 Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over ge...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 30. UI Remix is an interactive system that supports mobile UI design through example-driven workflows using a multimodal retrieval-augmented generation model, enabling iterative design adaptation with source transparency cues.  					AI-generated summary 				 Designing user interfaces (UIs) is a critical...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 31. A theoretical framework and detection method for identifying hallucinations in large language models by analyzing data-driven and reasoning-driven components through neural tangent kernel-based scoring.  					AI-generated summary 				 The reliability of Large Language Models (LLMs) in high-stakes do...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 32. RouteMoA reduces computational costs and latency in mixture-of-agents frameworks by using dynamic routing with lightweight scoring and judgment mechanisms.  					AI-generated summary 				 Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises co...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 33. TensorLens presents a novel mathematical framework that represents the complete transformer architecture as a single input-dependent linear operator using high-order tensors, enabling comprehensive analysis of attention mechanisms and model components.  					AI-generated summary 				 Attention matri...
[27.01.2026 22:25] ********************************************************************************
[27.01.2026 22:25] Abstract 34. Interp3D is a training-free framework for textured 3D morphing that preserves geometric consistency and texture alignment through generative priors and progressive alignment principles.  					AI-generated summary 				 Textured 3D morphing seeks to generate smooth and plausible transitions between tw...
[27.01.2026 22:25] Read previous papers.
[27.01.2026 22:25] Generating reviews via LLM API.
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#benchmark", "#survey", "#hallucinations", "#data", "#agents"], "emoji": "üßπ", "ru": {"title": "–û—Ç –ø—Ä–∞–≤–∏–ª –∫ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º: LLM-—Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–º–µ–Ω—è—é—Ç
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#agents", "#training", "#synthetic", "#optimization", "#plp", "#data"], "emoji": "ü§ñ", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –∞–≥–µ–Ω—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ü–û –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#benchmark", "#video", "#open_source", "#dataset", "#agents", "#story_generation", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–û—Ç –¥–∏–∞–ª–æ–≥–∞ –∫ –∫–∏–Ω–æ: –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ—Ñ–∏–ª—å–º–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –ø—Ä–µ
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#cv", "#dataset", "#synthetic", "#science", "#multimodal"], "emoji": "üî¨", "ru": {"title": "–û—Ç –ª–æ–≥–∏–∫–∏ –∫ –ø–∏–∫—Å–µ–ª—è–º: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ –Ω–∞—É—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Å–∏–Ω—Ç–µ–∑—É –Ω–∞—É—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#long_context", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Elastic Attention, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#optimization", "#diffusion", "#architecture", "#inference", "#open_source"], "emoji": "‚öñÔ∏è", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ: –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Finite Scalar Q
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#rl", "#training", "#synthetic", "#reasoning", "#optimization", "#math"], "emoji": "ü™ú", "ru": {"title": "–°–∞–º–æ—É–ª—É—á—à–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SOAR –¥–ª—è —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –ø–ª–∞—Ç–æ –æ–±—É
[27.01.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "üé¨", "ru": {"title": "–°–∞–º–æ—É–ª—É—á—à–∞—é—â–∞—è—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ —Å —É—á—ë—Ç–æ–º –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–∏–¥–µ–æ –∫–∞–∫ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–≤—Ç–æ–∫–æ–¥–µ—Ä —à—É–º–æ–ø–æ–¥
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#long_context", "#multilingual", "#low_resource", "#open_source", "#architecture", "#audio"], "emoji": "üéôÔ∏è", "ru": {"title": "–ï–¥–∏–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –ª—é–±—É—é —Ä–µ—á—å: –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –±–µ–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏", "desc": "VibeVoice-ASR ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∫–≤–æ–∑–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ—á
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#benchmark", "#training", "#synthetic", "#rag", "#open_source"], "emoji": "üìä", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö", "desc": "CGPT ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–µ –±–æ–ª—å—à–∏–º–∏ —è–∑
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "üóìÔ∏è", "ru": {"title": "–ì–ª—É–±–æ–∫–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: –æ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ DeepPlanning –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –≤ —Å–ª–æ–∂–Ω—ã—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#synthetic", "#optimization", "#data", "#multimodal"], "emoji": "üìä", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ STAR –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–∞
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#training", "#rl", "#agents"], "emoji": "üß©", "ru": {"title": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏—è - –∫–ª—é—á –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ –æ–±–æ–±—â–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#training", "#multimodal"], "emoji": "üé≠", "ru": {"title": "–û–¥–∏–Ω –¥–µ–∫–æ–¥–µ—Ä –¥–ª—è –≤—Å–µ—Ö: —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ –≤ –µ–¥–∏–Ω–æ–º –ø–æ—Ç–æ–∫–µ —Ç–æ–∫–µ–Ω–æ–≤", "desc": "AR-Omni ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#reasoning", "#multimodal", "#graphs", "#audio", "#agents", "#rag", "#long_context", "#video"], "emoji": "üìπ", "ru": {"title": "–ì—Ä–∞—Ñ—ã —Å—Ü–µ–Ω –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ –±–µ–∑ –≥—Ä–∞–Ω–∏—Ü –≤—Ä–µ–º–µ–Ω–∏", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç EGAgent ‚Äî –∞–≥–µ–Ω—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–µ
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#reasoning", "#synthetic", "#data", "#agents", "#open_source"], "emoji": "üîÑ", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ª—É—á—à–∏—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω SAGE ‚Äî –∞–≥–µ–Ω—Ç—Å–∫–∏–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#open_source", "#long_context", "#science", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞–ø–∏—Å–∞–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö –≤–æ–∑—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è DRPG ‚Äî –∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Ä–µ—Ü–µ–Ω–∑–∏–∏ –Ω–∞—É—á
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#training", "#data", "#diffusion", "#multimodal", "#architecture", "#video", "#open_source"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ª—é–±–æ–≥–æ —Ç–∏–ø–∞", "desc": "SkyReels-V3 ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#inference", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è MoE-–º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ –º–æ–¥–µ–ª—è—Ö Mixture-of
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#3d", "#inference", "#robotics", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ IVRA, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤ –º–æ–¥–µ–ª—è—Ö Vision-Language-Action –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#alignment", "#optimization"], "emoji": "üéØ", "ru": {"title": "–ú–µ—Ç–∞–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞–≥—Ä–∞–¥—ã –∫ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Meta Reward Modeling ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–≤ –∑–∞–¥–∞—á—É –º–æ–¥–µ–ª–∏—Ä
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#audio", "#architecture", "#open_source", "#benchmark", "#dataset"], "emoji": "üé§", "ru": {"title": "–°–æ–≤–º–µ—Å—Ç–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏ –¥–∏–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –¥–µ—Ç—Å–∫–æ-–≤–∑—Ä–æ—Å–ª—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π —Å–∫–≤–æ–∑–Ω–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, —Ä–∞—Å—à–∏—Ä—è—é—â–∏–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Whisper –¥
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#reasoning", "#open_source", "#benchmark", "#survey"], "emoji": "üìà", "ru": {"title": "–ö–æ–≥–¥–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —Å–ø–∞—Å–∞–µ—Ç: —Ä–∞–∑–≥–∞–¥—ã–≤–∞—è —Å–µ–∫—Ä–µ—Ç—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö", "desc": "TSRBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#architecture", "#video", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–û—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã—Ö –≤–∏–¥–µ–æ –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º —Å–∏–º—É–ª—è—Ç–æ—Ä–∞–º –º–∏—Ä–∞", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Ç–∞–∫—Å–æ–Ω–æ–º–∏—è –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –¥–≤—É—Ö –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö: –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –º–æ–¥–µ–ª–∏—Ä–æ
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#architecture", "#diffusion", "#training"], "emoji": "üîÑ", "ru": {"title": "–ß–µ—Ä–Ω–æ–≤–∏–∫ –∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ: –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –±–ª–æ—á–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ 'Diffusion in Diffusion', –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –±–ª–æ—á
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#multilingual", "#low_resource", "#benchmark", "#machine_translation", "#dataset"], "emoji": "üîÑ", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –±–∞—Ä—å–µ—Ä–æ–≤: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è code-switching –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç PingPong ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π 
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#3d", "#agents", "#rl"], "emoji": "üåä", "ru": {"title": "–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –ø–æ—Ç–æ–∫–∞–º–∏", "desc": "FluidGym –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä–≤—ã–π –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º 
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#benchmark"], "emoji": "‚ö†Ô∏è", "ru": {"title": "–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–µ —Ä–∏—Å–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MIR-SafetyBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–∞–±–æ
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#inference"], "emoji": "üîí", "ru": {"title": "–°–∫—Ä—ã—Ç–∞—è —É—è–∑–≤–∏–º–æ—Å—Ç—å: –∫–∞–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–Ω–∏–∂–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–∂–∞—Ç–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö (LVLM) —Å–Ω–∏–∂–∞–µ—Ç –∏—Ö —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –∏–∑-–∑–∞ –Ω–µ—Å—Ç
[27.01.2026 22:25] Using data from previous issue: {"categories": [], "emoji": "‚ö†Ô∏è", "ru": {"title": "–ö–æ–≥–¥–∞ —Ä–∞—Å—á—ë—Ç—ã –≤–∞–∂–Ω–µ–µ –∂–∏–∑–Ω–∏: –ø—Ä–æ–±–ª–µ–º–∞ —É–∑–∫–æ–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –æ–ø–∞—Å–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –≤ 
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#rag", "#multimodal"], "emoji": "üé®", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –¥–∏–∑–∞–π–Ω –º–æ–±–∏–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–∏–º–µ—Ä—ã —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å—é –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", "desc": "UI Remix ‚Äî –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–±–∏–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –º–æ–¥–µ–ª—å multimodal retrieval-augmented generatio
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#benchmark", "#interpretability", "#math", "#hallucinations"], "emoji": "üéØ", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Ç–µ–æ—Ä–∏—è –∏ –º–µ—Ç–æ–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ LLM —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —è–¥–µ—Ä –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫—É—é –æ—Å–Ω–æ–≤—É –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#inference", "#optimization", "#architecture", "#agents"], "emoji": "üõ£Ô∏è", "ru": {"title": "–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Å–º–µ—Å–∏ –∞–≥–µ–Ω—Ç–æ–≤", "desc": "RouteMoA ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–º–µ—Å–∏ –∞–≥–µ–Ω—Ç–æ–≤ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã 
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#math", "#interpretability", "#architecture", "#open_source"], "emoji": "üîç", "ru": {"title": "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∫–∞–∫ –µ–¥–∏–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª–∏", "desc": "TensorLens ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≤—Å—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∫–∞–∫ –µ–¥
[27.01.2026 22:25] Using data from previous issue: {"categories": ["#dataset", "#3d", "#open_source"], "emoji": "üîÑ", "ru": {"title": "–ü–ª–∞–≤–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ç–µ–∫—Å—Ç—É—Ä –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "Interp3D ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –º–æ—Ä—Ñ–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤
[27.01.2026 22:25] Renaming data file.
[27.01.2026 22:25] Renaming previous data. hf_papers.json to ./d/2026-01-27.json
[27.01.2026 22:25] Saving new data file.
[27.01.2026 22:25] Generating page.
[27.01.2026 22:25] Renaming previous page.
[27.01.2026 22:25] Renaming previous data. index.html to ./d/2026-01-27.html
[27.01.2026 22:25] Writing result.
[27.01.2026 22:25] Renaming log file.
[27.01.2026 22:25] Renaming previous data. log.txt to ./logs/2026-01-27_last_log.txt
