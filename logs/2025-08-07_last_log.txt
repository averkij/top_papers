[07.08.2025 14:17] Read previous papers.
[07.08.2025 14:17] Generating top page (month).
[07.08.2025 14:17] Writing top page (month).
[07.08.2025 15:14] Read previous papers.
[07.08.2025 15:14] Get feed.
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.04026
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01191
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02694
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.04700
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.04280
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.03501
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.03680
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.03159
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01858
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.03905
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.03560
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.03789
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2507.23785
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02215
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.02807
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.04664
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.04586
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.04295
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01928
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.00222
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2507.21974
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01197
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.04632
[07.08.2025 15:14] Extract page data from URL. URL: https://huggingface.co/papers/2508.01778
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01630
[07.08.2025 15:14] Extract page data from URL. URL: https://huggingface.co/papers/2508.04010
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.03983
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.03448
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.01311
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.00428
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2507.23313
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.04440
[07.08.2025 15:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.03178
[07.08.2025 15:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.08.2025 15:14] No deleted papers detected.
[07.08.2025 15:14] Downloading and parsing papers (pdf, html). Total: 33.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.04026.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.04026.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.04026.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.01191.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.01191.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.01191.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.02694.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.02694.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.02694.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.04700.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.04700.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.04700.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.04280.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.04280.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.04280.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.03501.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.03501.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.03501.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.03680.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.03680.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.03680.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.03159.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.03159.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.03159.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.01858.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.01858.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.01858.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.03905.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.03905.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.03905.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.03560.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.03560.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.03560.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.03789.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.03789.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.03789.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2507.23785.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2507.23785.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2507.23785.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.02215.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.02215.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.02215.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.02807.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.02807.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.02807.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.04664.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.04664.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.04664.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.04586.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.04586.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.04586.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.04295.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.04295.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.04295.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.01928.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.01928.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.01928.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.00222.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.00222.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.00222.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2507.21974.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2507.21974.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2507.21974.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.01197.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.01197.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.01197.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.04632.
[07.08.2025 15:14] Extra JSON file exists (./assets/json/2508.04632.json), skip PDF parsing.
[07.08.2025 15:14] Paper image links file exists (./assets/img_data/2508.04632.json), skip HTML parsing.
[07.08.2025 15:14] Success.
[07.08.2025 15:14] Downloading and parsing paper https://huggingface.co/papers/2508.01778.
[07.08.2025 15:14] Downloading paper 2508.01778 from http://arxiv.org/pdf/2508.01778v1...
[07.08.2025 15:14] Extracting affiliations from text.
[07.08.2025 15:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving via Online HD Map Diffusion Zhigang Sun1, Yiru Wang1,6, Anqing Jiang1, Shuo Wang1, Yu Gao1, Yuwen Heng1, Shouyi Zhang1, An He1, Hao Jiang3, Jinhao Chai2, Zichong Gu2, Wang Jijun4, Shichen Tang1, Lavdim Halilaj5, Juergen Luettin5, Hao Sun1 5 2 0 2 3 ] . [ 1 8 7 7 1 0 . 8 0 5 2 : r Abstract Autonomous driving requires accurate scene understanding, including road geometry, traffic agents, and their semantic relationships. In online HD map generation scenarios, raster-based representations are well-suited to vision models but lack geometric precision, while graph-based representations retain structural detail but become unstable without precise maps. To harness the complementary strengths of both, we propose DiffSemanticFusiona fusion framework for multimodal trajectory prediction and planning. Our approach reasons over semantic rasterfused BEV space, enhanced by map diffusion module that improves both the stability and expressiveness of online HD map representations. We validate our framework on two downstream tasks: trajectory prediction and planning-oriented end-to-end autonomous driving. Experiments on real-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate improved performance over several state-of-the-art (SOTA) methods. For the prediction task on nuScenes, we integrate DiffSemanticFusion with the online HD map informed QCNet, achieving 5.1% performance improvement. For end-to-end autonomous driving in NAVSIM, DiffSemanticFusion achieves SOTA results, with 15% performance gain in NavHard scenarios. In addition, extensive ablation and sensitivity studies show that our map diffusion module can be seamlessly integrated into other vector-based approaches to enhance performance. All artifacts are available at: https: //github.com/SunZhigang7/DiffSemanticFusion I. INTRODUCTION End-to-end autonomous driving has attracted increasing attention due to recent progress in learning based p"
[07.08.2025 15:14] Response: ```python
[]
```
[07.08.2025 15:14] Extracting affiliations from text.
[07.08.2025 15:14] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving via Online HD Map Diffusion Zhigang Sun1, Yiru Wang1,6, Anqing Jiang1, Shuo Wang1, Yu Gao1, Yuwen Heng1, Shouyi Zhang1, An He1, Hao Jiang3, Jinhao Chai2, Zichong Gu2, Wang Jijun4, Shichen Tang1, Lavdim Halilaj5, Juergen Luettin5, Hao Sun1 5 2 0 2 3 ] . [ 1 8 7 7 1 0 . 8 0 5 2 : r Abstract Autonomous driving requires accurate scene understanding, including road geometry, traffic agents, and their semantic relationships. In online HD map generation scenarios, raster-based representations are well-suited to vision models but lack geometric precision, while graph-based representations retain structural detail but become unstable without precise maps. To harness the complementary strengths of both, we propose DiffSemanticFusiona fusion framework for multimodal trajectory prediction and planning. Our approach reasons over semantic rasterfused BEV space, enhanced by map diffusion module that improves both the stability and expressiveness of online HD map representations. We validate our framework on two downstream tasks: trajectory prediction and planning-oriented end-to-end autonomous driving. Experiments on real-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate improved performance over several state-of-the-art (SOTA) methods. For the prediction task on nuScenes, we integrate DiffSemanticFusion with the online HD map informed QCNet, achieving 5.1% performance improvement. For end-to-end autonomous driving in NAVSIM, DiffSemanticFusion achieves SOTA results, with 15% performance gain in NavHard scenarios. In addition, extensive ablation and sensitivity studies show that our map diffusion module can be seamlessly integrated into other vector-based approaches to enhance performance. All artifacts are available at: https: //github.com/SunZhigang7/DiffSemanticFusion I. INTRODUCTION End-to-end autonomous driving has attracted increasing attention due to recent progress in learning based perception and planning systems, including object detection [1] [3], multi-object tracking [4], [5], online mapping [6][9], prediction [10][13] and planning [14][16]. These advances have enabled the learning of driving policies directly from raw sensor inputs, bypassing the need for hand-crafted rules. Two-stage end-to-end frameworks decompose the driving task into intermediate perception and planning modules, typically leveraging structured representations such as birdseye view (BEV) maps or object lists. This design enhances interpretability and training stability, but may suffer from sub-optimal performance due to error propagation between stages and limited capacity for joint optimization. While 1 Zhigang Sun, Yiru Wang, Anqing Jiang, Shuo Wang, Yu Gao, Yuwen Heng, Shouyi Zhang, An He, Shichen Tang, Hao Sun are with Bosch Corporate Research, Bosch (China) Investment Ltd., Shanghai, China. * means equal contribution 2 Jinhao Chai, Zichong Gu are with School of Communication and Information Engineering, Shanghai University, Shanghai, China 3 Hao Jiang is with Shanghai Jiaotong University, Shanghai, China 4 Wang Jijun, is with AIR, Tsinghua University, Beijing, China 5 Lavdim Halilaj, Juergen Luettin are with Robert Bosch GmbH 6 represents corresponding author, yiru.wang@cn.bosch.com Fig. 1. Semantic Raster Image Fusion map-informed prediction and planning have shown notable benefits by using high-definition (HD) maps to provide structural context, their effectiveness often diminishes in scenarios where online maps are incomplete, noisy, or misaligned. Such limitations can significantly affect downstream tasks like motion prediction and planning, particularly in dynamic or previously unseen environments. Moreover, strict reliance on pre-defined map structures can hinder generalization and robustness in real-world where map inaccuracies are inevitable. To address these challenges, we propose online HD map diffusion, novel framework that enhances the robustness of map-informed prediction under online mapping conditions. Rather than treating the map as fixed input, map diffusion introduces learnable mechanism that iteratively refines and denoises online map representations via diffusion-based generative process. This approach allows the model to adaptively recover reliable map features, even in the presence of uncertainty or noise, thereby improving the quality of trajectory prediction and downstream planning. By bridging the gap between static HD maps and dynamic online mapping, online HD map diffusion offers resilient and scalable solution for safe and efficient autonomous driving. In contrast, one-stage end-to-end approaches [17][21] directly regress control commands or trajectories from raw sensor inputs, enabling full-system differentiability and potentially higher performance. However, they often face challenges related to interpretability, data efficiency, and deployment in safety-critical settings. To combine the advantages of both paradigms, we propose semantic raster BEV fusion framework that incorporates intermediate representations within an end-to-end trainable architecture. This design retains the interpretability and structure of two-stage pipelines while enabling the global optimization benefits of one-stage systems. Such integration has shown promise Fig. 2. DiffSemanticFusion Overview: Sparse Perception utilizes Sparse4D [25] to extract bounding boxes of dynamic objects and map elements. Dense Perception utilizes LSS [26] and BEVDet [1] to extract BEV features. Vectorized Graph utilizes SemanticFormer [27] to extract graph information. Fusion fuses heterogeneous representations to unified space and Diffusion Planner serves as trajectories decoder. in achieving both high performance and reliable decisionmaking in complex driving scenarios. The main contributions are: We propose the online HD map diffusion module, which improves the robustness of online HD maps under noisy or incomplete conditions, enhancing downstream prediction and planning performance. We design semantic raster BEV fusion architecture that combines raster-based, graph-based map and BEVfeature representations in BEV space, leveraging their complementary strengths for more stable and informative scene understanding. We validate our method on two tasks: prediction on nuScenes [22] and planning oriented end-to-end autonomous driving in NAVSIM [23]. Both achieve SOTA performance. DiffSemanticFusion improves online HD map informed QCNet [24"
[07.08.2025 15:15] Mistral response. {"id": "0913399e51e14b18abdd24362551943f", "created": 1754579699, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1397, "total_tokens": 1472, "completion_tokens": 75}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Bosch Corporate Research, Bosch (China) Investment Ltd., Shanghai, China\",\n    \"School of Communication and Information Engineering, Shanghai University, Shanghai, China\",\n    \"Shanghai Jiaotong University, Shanghai, China\",\n    \"AIR, Tsinghua University, Beijing, China\",\n    \"Robert Bosch GmbH\"\n]\n```"}}]}
[07.08.2025 15:15] Response: ```python
[
    "Bosch Corporate Research, Bosch (China) Investment Ltd., Shanghai, China",
    "School of Communication and Information Engineering, Shanghai University, Shanghai, China",
    "Shanghai Jiaotong University, Shanghai, China",
    "AIR, Tsinghua University, Beijing, China",
    "Robert Bosch GmbH"
]
```
[07.08.2025 15:15] Deleting PDF ./assets/pdf/2508.01778.pdf.
[07.08.2025 15:15] Success.
[07.08.2025 15:15] Downloading and parsing paper https://huggingface.co/papers/2508.01630.
[07.08.2025 15:15] Extra JSON file exists (./assets/json/2508.01630.json), skip PDF parsing.
[07.08.2025 15:15] Paper image links file exists (./assets/img_data/2508.01630.json), skip HTML parsing.
[07.08.2025 15:15] Success.
[07.08.2025 15:15] Downloading and parsing paper https://huggingface.co/papers/2508.04010.
[07.08.2025 15:15] Downloading paper 2508.04010 from http://arxiv.org/pdf/2508.04010v1...
[07.08.2025 15:15] Extracting affiliations from text.
[07.08.2025 15:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 ] . [ 1 0 1 0 4 0 . 8 0 5 2 : r HARMONYGUARD: TOWARD SAFETY AND UTILITY IN WEB AGENTS VIA ADAPTIVE POLICY ENHANCEMENT AND DUAL-OBJECTIVE OPTIMIZATION Yurun Chen1, Xavier Hu1, Yuhan Liu2, Keting Yin1, Juncheng Li1, Zhuosheng Zhang3, Shengyu Zhang1 1Zhejiang University yurunchen.research@gmail.com {xavier.hu.research, yuhanliu.research}@gmail.com {yinkt, junchengli, sy zhang}@zju.edu.cn zhangzs@sjtu.edu.cn 2Xiamen University 3Shanghai Jiao Tong University "
[07.08.2025 15:15] Response: ```python
["Zhejiang University", "Xiamen University", "Shanghai Jiao Tong University"]
```
[07.08.2025 15:15] Deleting PDF ./assets/pdf/2508.04010.pdf.
[07.08.2025 15:15] Success.
[07.08.2025 15:15] Downloading and parsing paper https://huggingface.co/papers/2508.03983.
[07.08.2025 15:15] Extra JSON file exists (./assets/json/2508.03983.json), skip PDF parsing.
[07.08.2025 15:15] Paper image links file exists (./assets/img_data/2508.03983.json), skip HTML parsing.
[07.08.2025 15:15] Success.
[07.08.2025 15:15] Downloading and parsing paper https://huggingface.co/papers/2508.03448.
[07.08.2025 15:15] Extra JSON file exists (./assets/json/2508.03448.json), skip PDF parsing.
[07.08.2025 15:15] Paper image links file exists (./assets/img_data/2508.03448.json), skip HTML parsing.
[07.08.2025 15:15] Success.
[07.08.2025 15:15] Downloading and parsing paper https://huggingface.co/papers/2508.01311.
[07.08.2025 15:15] Extra JSON file exists (./assets/json/2508.01311.json), skip PDF parsing.
[07.08.2025 15:15] Paper image links file exists (./assets/img_data/2508.01311.json), skip HTML parsing.
[07.08.2025 15:15] Success.
[07.08.2025 15:15] Downloading and parsing paper https://huggingface.co/papers/2508.00428.
[07.08.2025 15:15] Extra JSON file exists (./assets/json/2508.00428.json), skip PDF parsing.
[07.08.2025 15:15] Paper image links file exists (./assets/img_data/2508.00428.json), skip HTML parsing.
[07.08.2025 15:15] Success.
[07.08.2025 15:15] Downloading and parsing paper https://huggingface.co/papers/2507.23313.
[07.08.2025 15:15] Extra JSON file exists (./assets/json/2507.23313.json), skip PDF parsing.
[07.08.2025 15:15] Paper image links file exists (./assets/img_data/2507.23313.json), skip HTML parsing.
[07.08.2025 15:15] Success.
[07.08.2025 15:15] Downloading and parsing paper https://huggingface.co/papers/2508.04440.
[07.08.2025 15:15] Extra JSON file exists (./assets/json/2508.04440.json), skip PDF parsing.
[07.08.2025 15:15] Paper image links file exists (./assets/img_data/2508.04440.json), skip HTML parsing.
[07.08.2025 15:15] Success.
[07.08.2025 15:15] Downloading and parsing paper https://huggingface.co/papers/2508.03178.
[07.08.2025 15:15] Extra JSON file exists (./assets/json/2508.03178.json), skip PDF parsing.
[07.08.2025 15:15] Paper image links file exists (./assets/img_data/2508.03178.json), skip HTML parsing.
[07.08.2025 15:15] Success.
[07.08.2025 15:15] Enriching papers with extra data.
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 0. VeriGUI is a novel dataset for evaluating GUI agents in long-horizon tasks, emphasizing long-chain complexity and subtask-level verifiability.  					AI-generated summary 				 Recent studies have delved into constructing autonomous agents capable of performing complex Graphical User Interface (GUI)-b...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 1. CoT reasoning in LLMs is found to be limited by the distribution discrepancy between training and test data, suggesting it is not a robust form of reasoning.  					AI-generated summary 				 Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various t...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 2. A study on the efficiency-effectiveness trade-off in LLM-driven agent systems identifies optimal agent framework design to reduce costs while maintaining performance.  					AI-generated summary 				 The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated sy...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 3. SEAgent, an agentic self-evolving framework, enables computer-use agents to autonomously master novel software through experiential learning and a curriculum of tasks, achieving superior performance compared to existing methods.  					AI-generated summary 				 Repurposing large vision-language model...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 4. A lightweight, hyperparameter-free RL algorithm, VL-DAC, enables VLMs to learn generalized policies from inexpensive simulators, improving performance on real-world benchmarks without sacrificing image understanding accuracy.  					AI-generated summary 				 Interactive multimodal agents must convert...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 5. Research on applications of Reinforcement Learning (RL) to Large Language Models (LLMs) has mostly been focused on single-turn problems, such as mathematical reasoning or single-shot code generation. While these problems can be viewed as token-level multi-turn MDPs, this view corresponds to a degene...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 6. Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions.  					AI-generated summary 				 We present Agent Lightning, a flexible and extensible framework that enables Reinfo...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 7. CoTox, a framework integrating LLMs with chain-of-thought reasoning, enhances multi-toxicity prediction by incorporating chemical structure data, biological pathways, and gene ontology terms, improving interpretability and predictive performance in drug development.  					AI-generated summary 				 D...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 8. A framework for web agents decomposes their capabilities into knowledge content learning and cognitive processes, using a structured dataset and a novel reasoning framework to enhance generalization and performance.  					AI-generated summary 				 Multimodal large-scale models have significantly adv...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 9. Sotopia-RL, a novel reinforcement learning framework, enhances social intelligence in large language models by refining feedback into utterance-level, multi-dimensional rewards, improving performance in social tasks.  					AI-generated summary 				 Social intelligence has become a critical capabilit...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 10. LaTCoder enhances layout preservation in design-to-code tasks by dividing webpage designs into blocks and using Chain-of-Thought reasoning with MLLMs, achieving significant improvements in metrics and human preference.  					AI-generated summary 				 Converting webpage designs into code (design-to-c...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 11. HPSv3, a human preference score using a wide-spectrum dataset and uncertainty-aware ranking loss, enhances text-to-image generation quality through iterative refinement.  					AI-generated summary 				 Evaluating text-to-image generation models requires alignment with human perception, yet existing ...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 12. A novel framework uses a Direct 4DMesh-to-GS Variation Field VAE and Gaussian Variation Field diffusion model to generate high-quality dynamic 3D content from single video inputs, demonstrating superior quality and generalization.  					AI-generated summary 				 In this paper, we present a novel fra...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 13. LeanK, a learning-based method, prunes unimportant key cache channels in large language models to reduce memory usage and accelerate decoding without sacrificing accuracy.  					AI-generated summary 				 Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 14. DreamVVT, a two-stage framework using Diffusion Transformers and LoRA adapters, enhances video virtual try-on by leveraging unpaired human-centric data and pretrained models to preserve garment details and temporal consistency.  					AI-generated summary 				 Video virtual try-on (VVT) technology ha...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 15. Sculptor, a framework for Active Context Management, enhances LLM performance on long contexts by enabling proactive attention and memory control, reducing proactive interference and improving reasoning reliability.  					AI-generated summary 				 Large Language Models (LLMs) suffer from significant...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 16. The paper diagnoses structural issues in AI conferences, including publication rates, carbon footprint, negative community sentiment, and logistical challenges, and proposes a Community-Federated Conference model to address these issues.  					AI-generated summary 				 Artificial Intelligence (AI) c...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 17. EvoC2Rust is an automated framework that translates entire C projects to Rust using a skeleton-guided approach, combining rule-based and LLM-based methods to improve syntax, semantics, and safety.  					AI-generated summary 				 Rust's compile-time safety guarantees make it ideal for safety-critical...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 18. IAUNet, a query-based U-Net architecture with a lightweight convolutional Pixel decoder and Transformer decoder, outperforms state-of-the-art models in biomedical instance segmentation.  					AI-generated summary 				 Instance segmentation is critical in biomedical imaging to accurately distinguish ...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 19. RL-PLUS, a hybrid-policy optimization approach, enhances LLM reasoning capabilities by integrating Multiple Importance Sampling and Exploration-Based Advantage Function, outperforming RLVR on various benchmarks and resolving capability boundary collapse.  					AI-generated summary 				 Reinforcement...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 20. A lightweight framework using Large Language Models (LLMs) with TeleLogs dataset and a two-stage training methodology improves Root Cause Analysis (RCA) in mobile networks by enhancing interpretability and reasoning quality.  					AI-generated summary 				 Root Cause Analysis (RCA) in mobile network...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 21. A benchmark and model for 3D occupancy grounding using natural language and voxel-level annotations improve object perception in autonomous driving.  					AI-generated summary 				 Visual grounding aims to identify objects or regions in a scene based on natural language descriptions, essential for s...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 22. Instruction Following Decorator enhances RLVR by improving sample efficiency, intent alignment, and reducing reward hacking in large language models.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction following capabilities of large language mo...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 23. DiffSemanticFusion enhances autonomous driving by fusing semantic raster and graph-based representations using a map diffusion module, improving trajectory prediction and end-to-end driving performance.  					AI-generated summary 				 Autonomous driving requires accurate scene understanding, includi...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 24. OpenMed NER, a suite of open-source transformer models using DAPT and LoRA, achieves state-of-the-art performance on diverse biomedical NER benchmarks with high efficiency and low computational cost.  					AI-generated summary 				 Named-entity recognition (NER) is fundamental to extracting structur...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 25. HarmonyGuard is a multi-agent framework that enhances policy compliance and task completion in web environments by adaptively updating security policies and optimizing dual objectives of safety and utility.  					AI-generated summary 				 Large language models enable agents to autonomously perform t...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 26. MiDashengLM is an open audio-language model using general audio captions for efficient and comprehensive audio understanding, offering faster processing and higher throughput compared to existing models.  					AI-generated summary 				 Current approaches for large audio language models (LALMs) often...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 27. SonicMaster, a unified generative model, improves music audio quality by addressing various artifacts using text-based control and a flow-matching generative training paradigm.  					AI-generated summary 				 Music recordings often suffer from audio quality issues such as excessive reverberation, di...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 28. A continual learning framework for 3D anomaly detection uses Kernel Attention mechanisms and parameter perturbation to handle multiple and emerging classes of point clouds.  					AI-generated summary 				 3D Anomaly Detection (AD) has shown great potential in detecting anomalies or defects of high-p...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 29. Sel3DCraft enhances text-to-3D generation through a dual-branch retrieval and generation system, multi-view hybrid scoring with MLLMs, and prompt-driven visual analytics, improving designer creativity.  					AI-generated summary 				 Text-to-3D (T23D) generation has transformed digital content creat...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 30. Transformer-based text-to-image diffusion models show varying degrees of content-style separation in generated artworks, as revealed by cross-attention heatmaps.  					AI-generated summary 				 Text-to-image diffusion models have demonstrated remarkable capabilities in generating artistic content by...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 31. ThinkingF, a data synthesis and training pipeline, enhances autoformalization by improving formal knowledge and informal-to-formal reasoning, achieving state-of-the-art results in formalization tasks.  					AI-generated summary 				 Autoformalization aims to translate natural-language mathematical s...
[07.08.2025 15:15] ********************************************************************************
[07.08.2025 15:15] Abstract 32. A framework using entropy-preserving supervised fine-tuning and token-wise entropy-adaptive reinforcement learning improves instruction adherence in LLMs by fostering rigorous reasoning processes.  					AI-generated summary 				 While advancements in the reasoning abilities of LLMs have significantl...
[07.08.2025 15:15] Read previous papers.
[07.08.2025 15:15] Generating reviews via LLM API.
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#games", "#agents", "#dataset", "#long_context"], "emoji": "üñ•Ô∏è", "ru": {"title": "VeriGUI: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö GUI-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "VeriGUI - —ç—Ç–æ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ GUI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∞–∫—Ü–µ–Ω—Ç–∏—Ä—É—é—â–∏–π –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#reasoning", "#data", "#training", "#interpretability"], "emoji": "üß†", "ru": {"title": "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç—å CoT-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ LLM: –º–∏—Ä–∞–∂ –≤–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ —Ü–µ–ø–æ—á–∫–µ (CoT) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã —Ä–∞—Å
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#agents", "#optimization", "#training", "#open_source", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º: –±–∞–ª–∞–Ω—Å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –∑–∞—Ç—Ä–∞—Ç", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∞–Ω–∞–ª–∏–∑—É –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞ –º–µ–∂–¥—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç—å—é –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –∞–≥–µ–Ω—Ç–æ–≤, —É–ø—Ä–∞–≤–ª
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#agents", "#agi", "#open_source", "#optimization", "#rl"], "emoji": "ü§ñ", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–∞—é—â–∏–µ—Å—è –∞–≥–µ–Ω—Ç—ã –¥–ª—è –æ—Å–≤–æ–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ –ü–û", "desc": "SEAgent - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞–º –∏—Å—Å–ª
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#rl", "#training", "#transfer_learning", "#synthetic", "#games", "#rlhf", "#multimodal", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–æ—Å—Ç–æ–µ RL-–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "VL-DAC - —ç—Ç–æ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#open_source", "#agents", "#benchmark", "#optimization", "#rl", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "RL –¥–ª—è LLM: –ø—Ä–æ—Ä—ã–≤ –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ü–û", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É—Å–ø–µ—à–Ω–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∫ –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#agents", "#rag", "#games", "#math", "#training", "#optimization", "#rl"], "emoji": "‚ö°", "ru": {"title": "Agent Lightning: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "Agent Lightning - —ç—Ç–æ –≥–∏–±–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞–∑–ª–∏—á
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#science", "#interpretability", "#healthcare", "#reasoning", "#data", "#multimodal"], "emoji": "üß™", "ru": {"title": "CoTox: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –ª–µ–∫–∞—Ä—Å—Ç–≤ —Å –ø–æ–º–æ—â—å—é LLM", "desc": "CoTox - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) —Å —Ä–∞
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#dataset", "#reasoning", "#open_source"], "emoji": "üï∏Ô∏è", "ru": {"title": "–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤, —Ä–∞–∑–¥–µ–ª—è—é—â—É—é –∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –Ω–∞ –∏–∑—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∑–Ω–∞–Ω–∏–π –∏ –∫–æ–≥–Ω–∏—Ç–∏–≤
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#games", "#alignment", "#rlhf", "#rl"], "emoji": "ü§ñ", "ru": {"title": "Sotopia-RL: –ø—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É –¥–ª—è –ò–ò", "desc": "Sotopia-RL - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ–±—Ä–∞—Ç–Ω
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#reasoning", "#architecture", "#multimodal", "#optimization"], "emoji": "üß©", "ru": {"title": "LaTCoder: —É–ª—É—á—à–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞–∫–µ—Ç–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –∏–∑ –¥–∏–∑–∞–π–Ω–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü", "desc": "LaTCoder - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∏–∑–∞–π–Ω–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü 
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#alignment", "#data", "#benchmark", "#dataset", "#cv", "#open_source"], "emoji": "üñºÔ∏è", "ru": {"title": "HPSv3: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "HPSv3 - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#video", "#synthetic", "#3d", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ 3D-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ 3D-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –≤–∏–¥–µ–æ–≤—Ö–æ–¥–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è VAE –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#training", "#long_context", "#inference", "#optimization"], "emoji": "üî™", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ –∫—ç—à–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "LeanK - —ç—Ç–æ –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω —É–º–µ–Ω—å—à–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ —É—Å–∫–æ—Ä—è–µ
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#video", "#cv", "#synthetic", "#multimodal", "#diffusion"], "emoji": "üëö", "ru": {"title": "DreamVVT: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–∏–º–µ—Ä–∫–∞ –æ–¥–µ–∂–¥—ã –Ω–∞ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "DreamVVT - —ç—Ç–æ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–º–µ—Ä–∫–∏ –æ–¥–µ–∂–¥—ã –Ω–∞ –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#multimodal", "#long_context", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "Sculptor: –£–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Sculptor –¥–ª—è –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#ethics", "#survey"], "emoji": "üåê", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–π –ò–ò: –æ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–∞—Ü–∏–∏ –∫ —Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–π –ø–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É, –≤–∫–ª—é—á–∞—è —Ä–æ—Å—Ç —á–∏—Å–ª–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–π, —É–≥–ª–µ—Ä–æ–¥–Ω—ã–π —Å–ª–µ–¥, –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–µ –Ω–∞—Å—Ç—Ä–æ
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#open_source", "#architecture", "#plp", "#optimization"], "emoji": "üîÑ", "ru": {"title": "EvoC2Rust: –≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –ø–µ—Ä–µ–≤–æ–¥—É C –≤ Rust", "desc": "EvoC2Rust - —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –ø—Ä–æ–µ–∫—Ç–æ–≤ –Ω–∞ C –≤ Rust, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫–µ–ª–µ—Ç
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#architecture", "#cv", "#games", "#benchmark", "#science", "#dataset", "#optimization", "#healthcare"], "emoji": "üî¨", "ru": {"title": "IAUNet: –ü—Ä–æ—Ä—ã–≤ –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–ª–µ—Ç–æ–∫ —Å –ø–æ–º–æ—â—å—é –≥–∏–±—Ä–∏–¥–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã", "desc": "IAUNet - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ U-Ne
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#training", "#benchmark", "#optimization", "#rl", "#reasoning"], "emoji": "üß†", "ru": {"title": "RL-PLUS: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RL-PLUS - –Ω–æ–≤—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è 
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#interpretability", "#dataset", "#training", "#open_source", "#multimodal"], "emoji": "üîç", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –ø–µ—Ä–≤–æ–ø—Ä–∏—á–∏–Ω –≤ –º–æ–±–∏–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö —Å –ø–æ–º–æ—â—å—é –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∏—Å–ø–æ–ª—å–∑—É—é
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#multimodal", "#games", "#benchmark", "#3d", "#optimization"], "emoji": "üöó", "ru": {"title": "–¢–æ—á–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –±–µ—Å–ø–∏–ª–æ—Ç–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π —Å –ø–æ–º–æ—â—å—é 3D –≥—Ä—É–Ω—Ç–æ–≤–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –∑–∞–¥–∞—á–∏ 3D –≥—Ä—É–Ω—Ç–æ–≤–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#rlhf", "#open_source", "#alignment", "#training", "#optimization", "#security", "#rl"], "emoji": "üéì", "ru": {"title": "IFDecorator: –ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Instruction Following Deco
[07.08.2025 15:15] Querying the API.
[07.08.2025 15:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DiffSemanticFusion enhances autonomous driving by fusing semantic raster and graph-based representations using a map diffusion module, improving trajectory prediction and end-to-end driving performance.  					AI-generated summary 				 Autonomous driving requires accurate scene understanding, including road geometry, traffic agents, and their semantic relationships. In online HD map generation scenarios, raster-based representations are well-suited to vision models but lack geometric precision, while graph-based representations retain structural detail but become unstable without precise maps. To harness the complementary strengths of both, we propose DiffSemanticFusion -- a fusion framework for multimodal trajectory prediction and planning. Our approach reasons over a semantic raster-fused BEV space, enhanced by a map diffusion module that improves both the stability and expressiveness of online HD map representations. We validate our framework on two downstream tasks: trajectory prediction and planning-oriented end-to-end autonomous driving. Experiments on real-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate improved performance over several state-of-the-art methods. For the prediction task on nuScenes, we integrate DiffSemanticFusion with the online HD map informed QCNet, achieving a 5.1\% performance improvement. For end-to-end autonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-art results, with a 15\% performance gain in NavHard scenarios. In addition, extensive ablation and sensitivity studies show that our map diffusion module can be seamlessly integrated into other vector-based approaches to enhance performance. All artifacts are available at https://github.com/SunZhigang7/DiffSemanticFusion.
[07.08.2025 15:15] Response: {
  "desc": "DiffSemanticFusion - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Ç—Ä–æ–≤—ã–µ –∏ –≥—Ä–∞—Ñ–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –º–æ–¥—É–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∫–∞—Ä—Ç—ã. –û–Ω –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–∫–≤–æ–∑–Ω–æ–≥–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –æ–Ω–ª–∞–π–Ω-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ HD-–∫–∞—Ä—Ç, –≥–¥–µ –æ–Ω —É–ª—É—á—à–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π.",

  "emoji": "üöó",

  "title": "–°–ª–∏—è–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∞–≤—Ç–æ–ø–∏–ª–æ—Ç–∞"
}
[07.08.2025 15:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DiffSemanticFusion enhances autonomous driving by fusing semantic raster and graph-based representations using a map diffusion module, improving trajectory prediction and end-to-end driving performance.  					AI-generated summary 				 Autonomous driving requires accurate scene understanding, including road geometry, traffic agents, and their semantic relationships. In online HD map generation scenarios, raster-based representations are well-suited to vision models but lack geometric precision, while graph-based representations retain structural detail but become unstable without precise maps. To harness the complementary strengths of both, we propose DiffSemanticFusion -- a fusion framework for multimodal trajectory prediction and planning. Our approach reasons over a semantic raster-fused BEV space, enhanced by a map diffusion module that improves both the stability and expressiveness of online HD map representations. We validate our framework on two downstream tasks: trajectory prediction and planning-oriented end-to-end autonomous driving. Experiments on real-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate improved performance over several state-of-the-art methods. For the prediction task on nuScenes, we integrate DiffSemanticFusion with the online HD map informed QCNet, achieving a 5.1\% performance improvement. For end-to-end autonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-art results, with a 15\% performance gain in NavHard scenarios. In addition, extensive ablation and sensitivity studies show that our map diffusion module can be seamlessly integrated into other vector-based approaches to enhance performance. All artifacts are available at https://github.com/SunZhigang7/DiffSemanticFusion."

[07.08.2025 15:15] Response: ```python
['AGENTS', 'MULTIMODAL', 'VIDEO', 'BENCHMARK']
```
[07.08.2025 15:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DiffSemanticFusion enhances autonomous driving by fusing semantic raster and graph-based representations using a map diffusion module, improving trajectory prediction and end-to-end driving performance.  					AI-generated summary 				 Autonomous driving requires accurate scene understanding, including road geometry, traffic agents, and their semantic relationships. In online HD map generation scenarios, raster-based representations are well-suited to vision models but lack geometric precision, while graph-based representations retain structural detail but become unstable without precise maps. To harness the complementary strengths of both, we propose DiffSemanticFusion -- a fusion framework for multimodal trajectory prediction and planning. Our approach reasons over a semantic raster-fused BEV space, enhanced by a map diffusion module that improves both the stability and expressiveness of online HD map representations. We validate our framework on two downstream tasks: trajectory prediction and planning-oriented end-to-end autonomous driving. Experiments on real-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate improved performance over several state-of-the-art methods. For the prediction task on nuScenes, we integrate DiffSemanticFusion with the online HD map informed QCNet, achieving a 5.1\% performance improvement. For end-to-end autonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-art results, with a 15\% performance gain in NavHard scenarios. In addition, extensive ablation and sensitivity studies show that our map diffusion module can be seamlessly integrated into other vector-based approaches to enhance performance. All artifacts are available at https://github.com/SunZhigang7/DiffSemanticFusion."

[07.08.2025 15:15] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[07.08.2025 15:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DiffSemanticFusion is a novel framework designed to improve autonomous driving by combining two types of data representations: semantic raster and graph-based models. The framework uses a map diffusion module to enhance the stability and detail of high-definition maps, which are crucial for accurate trajectory prediction and driving performance. By integrating these representations, DiffSemanticFusion leverages their strengths to provide better scene understanding and decision-making for autonomous vehicles. Experiments show that this approach significantly outperforms existing methods in real-world driving scenarios, demonstrating its effectiveness in both trajectory prediction and end-to-end driving tasks.","title":"Fusing Data for Smarter Autonomous Driving"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DiffSemanticFusion is a novel framework designed to improve autonomous driving by combining two types of data representations: semantic raster and graph-based models. The framework uses a map diffusion module to enhance the stability and detail of high-definition maps, which are crucial for accurate trajectory prediction and driving performance. By integrating these representations, DiffSemanticFusion leverages their strengths to provide better scene understanding and decision-making for autonomous vehicles. Experiments show that this approach significantly outperforms existing methods in real-world driving scenarios, demonstrating its effectiveness in both trajectory prediction and end-to-end driving tasks.', title='Fusing Data for Smarter Autonomous Driving'))
[07.08.2025 15:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DiffSemanticFusion ÊòØ‰∏ÄÁßçÂ¢ûÂº∫Ëá™‰∏ªÈ©æÈ©∂ÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáËûçÂêàËØ≠‰πâÊ†ÖÊ†ºÂíåÂü∫‰∫éÂõæÁöÑË°®Á§∫ÔºåÂà©Áî®Âú∞ÂõæÊâ©Êï£Ê®°ÂùóÊù•ÊèêÈ´òËΩ®ËøπÈ¢ÑÊµãÂíåÁ´ØÂà∞Á´ØÈ©æÈ©∂ÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫ÜÊ†ÖÊ†ºË°®Á§∫ÁöÑËßÜËßâÊ®°Âûã‰ºòÂäøÂíåÂõæË°®Á§∫ÁöÑÁªìÊûÑÁªÜËäÇÔºåËß£ÂÜ≥‰∫ÜÂú®Á∫øÈ´òÊ∏ÖÂú∞ÂõæÁîüÊàê‰∏≠ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÈóÆÈ¢ò„ÄÇÊàë‰ª¨Âú®ÁúüÂÆû‰∏ñÁïåÁöÑËá™‰∏ªÈ©æÈ©∂Âü∫ÂáÜÊµãËØï‰∏≠È™åËØÅ‰∫ÜËØ•Ê°ÜÊû∂ÔºåÁªìÊûúÊòæÁ§∫Âú®ËΩ®ËøπÈ¢ÑÊµãÂíåËßÑÂàí‰ªªÂä°‰∏äÂùá‰ºò‰∫éÂ§öÁßçÂÖàËøõÊñπÊ≥ï„ÄÇÂÆûÈ™åË°®ÊòéÔºåDiffSemanticFusion Âú®Â§ö‰∏™Âú∫ÊôØ‰∏≠ÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÁâπÂà´ÊòØÂú®Â§çÊùÇÁéØÂ¢É‰∏ãÁöÑËá™‰∏ªÈ©æÈ©∂Ë°®Áé∞„ÄÇ","title":"ËûçÂêàËØ≠‰πâ‰∏éÂõæÂΩ¢ÔºåÊèêÂçáËá™‰∏ªÈ©æÈ©∂ÊÄßËÉΩ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DiffSemanticFusion ÊòØ‰∏ÄÁßçÂ¢ûÂº∫Ëá™‰∏ªÈ©æÈ©∂ÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáËûçÂêàËØ≠‰πâÊ†ÖÊ†ºÂíåÂü∫‰∫éÂõæÁöÑË°®Á§∫ÔºåÂà©Áî®Âú∞ÂõæÊâ©Êï£Ê®°ÂùóÊù•ÊèêÈ´òËΩ®ËøπÈ¢ÑÊµãÂíåÁ´ØÂà∞Á´ØÈ©æÈ©∂ÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫ÜÊ†ÖÊ†ºË°®Á§∫ÁöÑËßÜËßâÊ®°Âûã‰ºòÂäøÂíåÂõæË°®Á§∫ÁöÑÁªìÊûÑÁªÜËäÇÔºåËß£ÂÜ≥‰∫ÜÂú®Á∫øÈ´òÊ∏ÖÂú∞ÂõæÁîüÊàê‰∏≠ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÈóÆÈ¢ò„ÄÇÊàë‰ª¨Âú®ÁúüÂÆû‰∏ñÁïåÁöÑËá™‰∏ªÈ©æÈ©∂Âü∫ÂáÜÊµãËØï‰∏≠È™åËØÅ‰∫ÜËØ•Ê°ÜÊû∂ÔºåÁªìÊûúÊòæÁ§∫Âú®ËΩ®ËøπÈ¢ÑÊµãÂíåËßÑÂàí‰ªªÂä°‰∏äÂùá‰ºò‰∫éÂ§öÁßçÂÖàËøõÊñπÊ≥ï„ÄÇÂÆûÈ™åË°®ÊòéÔºåDiffSemanticFusion Âú®Â§ö‰∏™Âú∫ÊôØ‰∏≠ÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÁâπÂà´ÊòØÂú®Â§çÊùÇÁéØÂ¢É‰∏ãÁöÑËá™‰∏ªÈ©æÈ©∂Ë°®Áé∞„ÄÇ', title='ËûçÂêàËØ≠‰πâ‰∏éÂõæÂΩ¢ÔºåÊèêÂçáËá™‰∏ªÈ©æÈ©∂ÊÄßËÉΩ'))
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#healthcare", "#ethics", "#dataset", "#training", "#transfer_learning", "#open_source", "#benchmark", "#data"], "emoji": "üß¨", "ru": {"title": "OpenMed NER: –û—Ç–∫—Ä—ã—Ç—ã–π –ø—Ä–æ—Ä—ã–≤ –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π", "desc": "OpenMed NER - —ç—Ç–æ –Ω–∞–±–æ—Ä –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –æ
[07.08.2025 15:15] Querying the API.
[07.08.2025 15:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

HarmonyGuard is a multi-agent framework that enhances policy compliance and task completion in web environments by adaptively updating security policies and optimizing dual objectives of safety and utility.  					AI-generated summary 				 Large language models enable agents to autonomously perform tasks in open web environments. However, as hidden threats within the web evolve, web agents face the challenge of balancing task performance with emerging risks during long-sequence operations. Although this challenge is critical, current research remains limited to single-objective optimization or single-turn scenarios, lacking the capability for collaborative optimization of both safety and utility in web environments. To address this gap, we propose HarmonyGuard, a multi-agent collaborative framework that leverages policy enhancement and objective optimization to jointly improve both utility and safety. HarmonyGuard features a multi-agent architecture characterized by two fundamental capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent within HarmonyGuard, which automatically extracts and maintains structured security policies from unstructured external documents, while continuously updating policies in response to evolving threats. (2) Dual-Objective Optimization: Based on the dual objectives of safety and utility, the Utility Agent integrated within HarmonyGuard performs the Markovian real-time reasoning to evaluate the objectives and utilizes metacognitive capabilities for their optimization. Extensive evaluations on multiple benchmarks show that HarmonyGuard improves policy compliance by up to 38% and task completion by up to 20% over existing baselines, while achieving over 90% policy compliance across all tasks. Our project is available here: https://github.com/YurunChen/HarmonyGuard.
[07.08.2025 15:15] Response: {
  "desc": "HarmonyGuard - —ç—Ç–æ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç –ø–æ–ª–∏—Ç–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –¥–≤–æ–π–Ω—ã–µ —Ü–µ–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –∞–≥–µ–Ω—Ç–∞ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ü–µ–ª–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –¥–ª—è –∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.",
  "emoji": "üõ°Ô∏è",
  "title": "–ì–∞—Ä–º–æ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –≤–µ–±-—Å—Ä–µ–¥–µ"
}
[07.08.2025 15:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HarmonyGuard is a multi-agent framework that enhances policy compliance and task completion in web environments by adaptively updating security policies and optimizing dual objectives of safety and utility.  					AI-generated summary 				 Large language models enable agents to autonomously perform tasks in open web environments. However, as hidden threats within the web evolve, web agents face the challenge of balancing task performance with emerging risks during long-sequence operations. Although this challenge is critical, current research remains limited to single-objective optimization or single-turn scenarios, lacking the capability for collaborative optimization of both safety and utility in web environments. To address this gap, we propose HarmonyGuard, a multi-agent collaborative framework that leverages policy enhancement and objective optimization to jointly improve both utility and safety. HarmonyGuard features a multi-agent architecture characterized by two fundamental capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent within HarmonyGuard, which automatically extracts and maintains structured security policies from unstructured external documents, while continuously updating policies in response to evolving threats. (2) Dual-Objective Optimization: Based on the dual objectives of safety and utility, the Utility Agent integrated within HarmonyGuard performs the Markovian real-time reasoning to evaluate the objectives and utilizes metacognitive capabilities for their optimization. Extensive evaluations on multiple benchmarks show that HarmonyGuard improves policy compliance by up to 38% and task completion by up to 20% over existing baselines, while achieving over 90% policy compliance across all tasks. Our project is available here: https://github.com/YurunChen/HarmonyGuard."

[07.08.2025 15:15] Response: ```python
['AGENTS', 'ARCHITECTURE', 'BENCHMARK']
```
[07.08.2025 15:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HarmonyGuard is a multi-agent framework that enhances policy compliance and task completion in web environments by adaptively updating security policies and optimizing dual objectives of safety and utility.  					AI-generated summary 				 Large language models enable agents to autonomously perform tasks in open web environments. However, as hidden threats within the web evolve, web agents face the challenge of balancing task performance with emerging risks during long-sequence operations. Although this challenge is critical, current research remains limited to single-objective optimization or single-turn scenarios, lacking the capability for collaborative optimization of both safety and utility in web environments. To address this gap, we propose HarmonyGuard, a multi-agent collaborative framework that leverages policy enhancement and objective optimization to jointly improve both utility and safety. HarmonyGuard features a multi-agent architecture characterized by two fundamental capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent within HarmonyGuard, which automatically extracts and maintains structured security policies from unstructured external documents, while continuously updating policies in response to evolving threats. (2) Dual-Objective Optimization: Based on the dual objectives of safety and utility, the Utility Agent integrated within HarmonyGuard performs the Markovian real-time reasoning to evaluate the objectives and utilizes metacognitive capabilities for their optimization. Extensive evaluations on multiple benchmarks show that HarmonyGuard improves policy compliance by up to 38% and task completion by up to 20% over existing baselines, while achieving over 90% policy compliance across all tasks. Our project is available here: https://github.com/YurunChen/HarmonyGuard."

[07.08.2025 15:15] Response: ```python
["SECURITY", "OPTIMIZATION"]
```
[07.08.2025 15:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HarmonyGuard is a multi-agent framework designed to improve how web agents comply with security policies while completing tasks. It addresses the challenge of balancing safety and utility in dynamic web environments by adaptively updating security policies. The framework includes a Policy Agent that extracts and updates security policies from external documents and a Utility Agent that optimizes task performance based on safety and utility objectives. Evaluations show that HarmonyGuard significantly enhances policy compliance and task completion compared to existing methods.","title":"Balancing Safety and Utility in Web Tasks with HarmonyGuard"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HarmonyGuard is a multi-agent framework designed to improve how web agents comply with security policies while completing tasks. It addresses the challenge of balancing safety and utility in dynamic web environments by adaptively updating security policies. The framework includes a Policy Agent that extracts and updates security policies from external documents and a Utility Agent that optimizes task performance based on safety and utility objectives. Evaluations show that HarmonyGuard significantly enhances policy compliance and task completion compared to existing methods.', title='Balancing Safety and Utility in Web Tasks with HarmonyGuard'))
[07.08.2025 15:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HarmonyGuardÊòØ‰∏Ä‰∏™Â§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáËá™ÈÄÇÂ∫îÊõ¥Êñ∞ÂÆâÂÖ®Á≠ñÁï•Âíå‰ºòÂåñÂÆâÂÖ®ÊÄß‰∏éÊïàÁî®ÁöÑÂèåÈáçÁõÆÊ†áÔºåÂ¢ûÂº∫ÁΩëÁªúÁéØÂ¢É‰∏≠ÁöÑÊîøÁ≠ñÂêàËßÑÊÄßÂíå‰ªªÂä°ÂÆåÊàêÂ∫¶„ÄÇËØ•Ê°ÜÊû∂ÂºïÂÖ•‰∫ÜÊîøÁ≠ñ‰ª£ÁêÜÔºåËÉΩÂ§ü‰ªéÈùûÁªìÊûÑÂåñÊñáÊ°£‰∏≠ÊèêÂèñÂíåÁª¥Êä§ÁªìÊûÑÂåñÁöÑÂÆâÂÖ®ÊîøÁ≠ñÔºåÂπ∂Ê†πÊçÆ‰∏çÊñ≠ÂèòÂåñÁöÑÂ®ÅËÉÅÊåÅÁª≠Êõ¥Êñ∞Ëøô‰∫õÊîøÁ≠ñ„ÄÇÂêåÊó∂ÔºåÊïàÁî®‰ª£ÁêÜÈÄöËøáÈ©¨Â∞îÂèØÂ§´ÂÆûÊó∂Êé®ÁêÜËØÑ‰º∞ÂÆâÂÖ®ÊÄßÂíåÊïàÁî®ÁöÑÂèåÈáçÁõÆÊ†áÔºåÂπ∂Âà©Áî®ÂÖÉËÆ§Áü•ËÉΩÂäõËøõË°å‰ºòÂåñ„ÄÇËØÑ‰º∞ÁªìÊûúË°®ÊòéÔºåHarmonyGuardÂú®ÊîøÁ≠ñÂêàËßÑÊÄßÂíå‰ªªÂä°ÂÆåÊàêÂ∫¶ÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÔºåÊîøÁ≠ñÂêàËßÑÊÄßË∂ÖËøá90%„ÄÇ","title":"HarmonyGuardÔºöÂÆâÂÖ®‰∏éÊïàÁî®ÁöÑÂèåÈáç‰ºòÂåñ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HarmonyGuardÊòØ‰∏Ä‰∏™Â§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáËá™ÈÄÇÂ∫îÊõ¥Êñ∞ÂÆâÂÖ®Á≠ñÁï•Âíå‰ºòÂåñÂÆâÂÖ®ÊÄß‰∏éÊïàÁî®ÁöÑÂèåÈáçÁõÆÊ†áÔºåÂ¢ûÂº∫ÁΩëÁªúÁéØÂ¢É‰∏≠ÁöÑÊîøÁ≠ñÂêàËßÑÊÄßÂíå‰ªªÂä°ÂÆåÊàêÂ∫¶„ÄÇËØ•Ê°ÜÊû∂ÂºïÂÖ•‰∫ÜÊîøÁ≠ñ‰ª£ÁêÜÔºåËÉΩÂ§ü‰ªéÈùûÁªìÊûÑÂåñÊñáÊ°£‰∏≠ÊèêÂèñÂíåÁª¥Êä§ÁªìÊûÑÂåñÁöÑÂÆâÂÖ®ÊîøÁ≠ñÔºåÂπ∂Ê†πÊçÆ‰∏çÊñ≠ÂèòÂåñÁöÑÂ®ÅËÉÅÊåÅÁª≠Êõ¥Êñ∞Ëøô‰∫õÊîøÁ≠ñ„ÄÇÂêåÊó∂ÔºåÊïàÁî®‰ª£ÁêÜÈÄöËøáÈ©¨Â∞îÂèØÂ§´ÂÆûÊó∂Êé®ÁêÜËØÑ‰º∞ÂÆâÂÖ®ÊÄßÂíåÊïàÁî®ÁöÑÂèåÈáçÁõÆÊ†áÔºåÂπ∂Âà©Áî®ÂÖÉËÆ§Áü•ËÉΩÂäõËøõË°å‰ºòÂåñ„ÄÇËØÑ‰º∞ÁªìÊûúË°®ÊòéÔºåHarmonyGuardÂú®ÊîøÁ≠ñÂêàËßÑÊÄßÂíå‰ªªÂä°ÂÆåÊàêÂ∫¶ÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÔºåÊîøÁ≠ñÂêàËßÑÊÄßË∂ÖËøá90%„ÄÇ', title='HarmonyGuardÔºöÂÆâÂÖ®‰∏éÊïàÁî®ÁöÑÂèåÈáç‰ºòÂåñ'))
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#audio", "#dataset", "#open_source"], "emoji": "üéß", "ru": {"title": "MiDashengLM: –û—Ç–∫—Ä—ã—Ç–∞—è –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∑–≤—É–∫–∞", "desc": "MiDashengLM - —ç—Ç–æ –Ω–æ–≤–∞—è –æ—Ç–∫—Ä—ã—Ç–∞—è –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—É–¥–∏–æ —Å –∏—Å–ø–æ–ª
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#audio", "#dataset", "#synthetic"], "emoji": "üéµ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –º—É–∑—ã–∫–∏ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "SonicMaster - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∞—É–¥–∏–æ –≤ –º—É–∑—ã–∫–µ. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ –∏ –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±—É—á
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#training", "#3d", "#dataset", "#optimization"], "emoji": "üîç", "ru": {"title": "–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è 3D-–∞–Ω–æ–º–∞–ª–∏–π: –æ—Ç –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ—Å—Ç–∏ –∫ –Ω–æ–≤—ã–º –∫–ª–∞—Å—Å–∞–º", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è C3D-AD –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ 3D-–æ–±–ª–∞–∫–∞—Ö —Ç–æ—á–µ–∫. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#games", "#3d", "#optimization", "#multimodal"], "emoji": "üé®", "ru": {"title": "Sel3DCraft: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è 3D-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "Sel3DCraft - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≤ 3D, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#multimodal", "#interpretability", "#cv", "#diffusion"], "emoji": "üé®", "ru": {"title": "–°–∫—Ä—ã—Ç–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–∞: –∫–∞–∫ –ò–ò —Ä–∞–∑–¥–µ–ª—è–µ—Ç —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏ —Å—Ç–∏–ª—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#reasoning", "#data", "#dataset"], "emoji": "üß†", "ru": {"title": "ThinkingF: –º–æ—Å—Ç –º–µ–∂–¥—É –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–æ–π –∏ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–π –º–∞—Ç–µ–º–∞—Ç–∏–∫–æ–π", "desc": "ThinkingF - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ—Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, —É–ª—É—á—à–∞—é—â–∏–π —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ—Ç –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ 
[07.08.2025 15:15] Using data from previous issue: {"categories": ["#optimization", "#rl", "#dataset", "#benchmark", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º —á–µ—Ä–µ–∑ –≥–ª—É–±–æ–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –ê–≤—Ç–æ—Ä
[07.08.2025 15:15] Renaming data file.
[07.08.2025 15:15] Renaming previous data. hf_papers.json to ./d/2025-08-07.json
[07.08.2025 15:15] Saving new data file.
[07.08.2025 15:15] Generating page.
[07.08.2025 15:15] Renaming previous page.
[07.08.2025 15:15] Renaming previous data. index.html to ./d/2025-08-07.html
[07.08.2025 15:15] Writing result.
[07.08.2025 15:15] Renaming log file.
[07.08.2025 15:15] Renaming previous data. log.txt to ./logs/2025-08-07_last_log.txt
