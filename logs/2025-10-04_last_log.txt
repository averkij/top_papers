[03.10.2025 23:10] Read previous papers.
[03.10.2025 23:10] Generating top page (month).
[03.10.2025 23:10] Writing top page (month).
[04.10.2025 01:43] Read previous papers.
[04.10.2025 01:43] Get feed.
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00446
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02283
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02245
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02314
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02297
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02209
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01149
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01444
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22067
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02286
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01591
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02250
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01265
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02240
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02190
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01284
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02294
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02253
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02173
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01346
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01179
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02295
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2509.21789
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00428
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26376
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01670
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01304
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02315
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02263
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02259
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01623
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01817
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01796
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00523
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24203
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01241
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02272
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01143
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00137
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25729
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2509.24304
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.02306
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01691
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01538
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01123
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00537
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26313
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22582
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01581
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00352
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26330
[04.10.2025 01:43] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01260
[04.10.2025 01:43] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.10.2025 01:43] No deleted papers detected.
[04.10.2025 01:43] Downloading and parsing papers (pdf, html). Total: 52.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.00446.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.00446.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.00446.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02283.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02283.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02283.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02245.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02245.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02245.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02314.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02314.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02314.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02297.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02297.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02297.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02209.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02209.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02209.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01149.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01149.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01149.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01444.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01444.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01444.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2509.22067.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2509.22067.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2509.22067.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02286.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02286.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02286.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01591.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01591.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01591.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02250.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02250.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02250.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01265.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01265.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01265.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02240.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02240.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02240.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02190.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02190.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02190.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01284.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01284.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01284.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02294.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02294.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02294.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02253.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02253.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02253.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02173.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02173.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02173.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01346.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01346.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01346.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01179.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01179.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01179.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02295.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02295.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02295.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2509.21789.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2509.21789.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2509.21789.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.00428.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.00428.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.00428.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2509.26376.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2509.26376.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2509.26376.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01670.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01670.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01670.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01304.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01304.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01304.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02315.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02315.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02315.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02263.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02263.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02263.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02259.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02259.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02259.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01623.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01623.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01623.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01817.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01817.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01817.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01796.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01796.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01796.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.00523.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.00523.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.00523.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2509.24203.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2509.24203.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2509.24203.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01241.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01241.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01241.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02272.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02272.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02272.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01143.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01143.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01143.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.00137.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.00137.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.00137.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2509.25729.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2509.25729.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2509.25729.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2509.24304.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2509.24304.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2509.24304.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.02306.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.02306.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.02306.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01691.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01691.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01691.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01538.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01538.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01538.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01123.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01123.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01123.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.00537.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.00537.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.00537.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2509.26313.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2509.26313.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2509.26313.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2509.22582.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2509.22582.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2509.22582.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01581.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01581.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01581.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.00352.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.00352.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.00352.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2509.26330.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2509.26330.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2509.26330.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Downloading and parsing paper https://huggingface.co/papers/2510.01260.
[04.10.2025 01:43] Extra JSON file exists (./assets/json/2510.01260.json), skip PDF parsing.
[04.10.2025 01:43] Paper image links file exists (./assets/img_data/2510.01260.json), skip HTML parsing.
[04.10.2025 01:43] Success.
[04.10.2025 01:43] Enriching papers with extra data.
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 0. LongCodeZip is a code compression framework for LLMs that uses dual-stage compression to reduce context size without degrading performance, improving efficiency in code intelligence applications.  					AI-generated summary 				 Code generation under long contexts is becoming increasingly critical as...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 1. A method is proposed to enhance long-horizon video generation by using sampled segments from self-generated long videos to guide student models, maintaining quality and consistency without additional supervision or retraining.  					AI-generated summary 				 Diffusion models have revolutionized imag...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 2. ExGRPO, a framework that prioritizes valuable reasoning experiences, improves and stabilizes reinforcement learning from verifiable rewards for large language models.  					AI-generated summary 				 Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm for improving the reaso...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 3. A novel density-guided poisoning method for 3D Gaussian Splatting enhances attack effectiveness by strategically injecting Gaussian points and disrupting multi-view consistency.  					AI-generated summary 				 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatti...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 4. Interactive Training is a framework that allows real-time, feedback-driven intervention during neural network training, improving stability and adaptability.  					AI-generated summary 				 Traditional neural network training typically follows fixed, predefined optimization recipes, lacking the flex...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 5. StockBench evaluates large language models in realistic stock trading environments, revealing challenges and opportunities in developing LLM-powered financial agents.  					AI-generated summary 				 Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, sho...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 6. ModernVBERT, a compact vision-language encoder, outperforms larger models in document retrieval by optimizing attention masking, image resolution, modality alignment, and contrastive objectives.  					AI-generated summary 				 Multimodal embedding models are gaining prevalence, notably for document ...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 7. VOGUE, a method that shifts exploration to the visual input space by quantifying policy sensitivity to visual perturbations, enhances multimodal reasoning in large language models.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) improves reasoning in large langu...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 8. Activation steering, intended to control LLM behavior, can instead increase harmful compliance and undermine model alignment safeguards.  					AI-generated summary 				 Activation steering is a promising technique for controlling LLM behavior by adding semantically meaningful vectors directly into a...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 9. DialTree-RPO, an on-policy reinforcement learning framework with tree search, autonomously discovers diverse multi-turn attack strategies against large language models, achieving higher attack success rates and uncovering new attack trajectories.  					AI-generated summary 				 Despite recent rapid ...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 10. Hidden states in Large Language Models encode correctness as a separable signature, enabling a minimalist verifier (CLUE) to outperform text-level and confidence-based methods in reranking and accuracy.  					AI-generated summary 				 Assessing the quality of Large Language Model (LLM) outputs prese...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 11. Behavior Best-of-N (bBoN) improves the reliability and success rates of computer-use agents by generating and selecting among multiple rollouts using behavior narratives, achieving state-of-the-art performance on OSWorld and strong generalization to different operating systems.  					AI-generated su...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 12. RLP, an information-driven reinforcement pretraining objective, enhances reasoning models by integrating exploration into pretraining, leading to significant performance improvements across various benchmarks.  					AI-generated summary 				 The dominant paradigm for training large reasoning models ...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 13. RewardMap, a multi-stage reinforcement learning framework, enhances multimodal large language models' visual understanding and reasoning skills through dense reward signals and a difficulty-aware reward design.  					AI-generated summary 				 Fine-grained visual reasoning remains a core challenge fo...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 14. A benchmark and evaluation framework for Deep Research Agents (DRAs) assesses their performance on complex tasks with multidimensional metrics.  					AI-generated summary 				 Artificial intelligence is undergoing the paradigm shift from closed language models to interconnected agent systems capable...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 15. Ovi is a unified audio-video generation model using twin-DiT modules with blockwise cross-modal fusion, enabling natural synchronization and high-quality multimodal outputs.  					AI-generated summary 				 Audio-video generation has often relied on complex multi-stage architectures or sequential syn...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 16. F2LLM, a suite of large language models, achieves high embedding performance with efficient fine-tuning from foundation models using open-source datasets.  					AI-generated summary 				 We introduce F2LLM - Foundation to Feature Large Language Models, a suite of state-of-the-art embedding models in...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 17. DragFlow leverages FLUX's strong generative priors and region-based editing with affine transformations to achieve state-of-the-art performance in drag-based image editing.  					AI-generated summary 				 Drag-based image editing has long suffered from distortions in the target region, largely becau...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 18. A reinforcement learning framework with span-level rewards improves hallucination span detection in large language models by incentivizing reasoning.  					AI-generated summary 				 Large language models (LLMs) often generate hallucinations -- unsupported content that undermines reliability. While m...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 19. Aristotle, an AI system combining formal verification and informal reasoning, achieves top performance on International Mathematical Olympiad problems using Lean proof search, lemma generation, and a geometry solver.  					AI-generated summary 				 We introduce Aristotle, an AI system that combines ...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 20. Toucan, a large publicly available tool-agentic dataset, enhances the performance of LLM agents by providing diverse, realistic, and complex multi-tool and multi-turn interactions.  					AI-generated summary 				 Large Language Model (LLM) agents are rapidly emerging as powerful systems for automati...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 21. VideoNSA, an adaptation of Native Sparse Attention to video-language models, enhances long-video understanding and temporal reasoning through end-to-end training and a hardware-aware hybrid attention approach.  					AI-generated summary 				 Video understanding in multimodal language models remains ...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 22. ViF mitigates visual hallucination snowballing in Multi-Agent Systems by enhancing visual attention and message relay through selected visual tokens.  					AI-generated summary 				 Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables challenging tasks but suffers from a novel f...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 23. Incorporating clinical context into automated structured radiology report generation improves report quality by addressing temporal hallucinations and utilizing comprehensive patient data.  					AI-generated summary 				 Automated structured radiology report generation (SRRG) from chest X-ray images...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 24. ScalingAR enhances next-token prediction in autoregressive image generation by using token entropy and adaptive scaling, improving model performance and efficiency.  					AI-generated summary 				 Test-time scaling (TTS) has demonstrated remarkable success in enhancing large language models, yet its...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 25. Computer-Use Agents consistently exhibit Blind Goal-Directedness, a bias that leads to risky behavior regardless of feasibility or context, as demonstrated by the BLIND-ACT benchmark.  					AI-generated summary 				 Computer-Use Agents (CUAs) are an increasingly deployed class of agents that take ac...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 26. AGILE, an interactive jigsaw-solving framework, enhances visual perception and reasoning in Vision-Language Models through iterative action and feedback, improving performance on jigsaw tasks and general vision tasks.  					AI-generated summary 				 Although current large Vision-Language Models (VLM...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 27. A theoretical framework and algorithms for improving multi-subject fidelity in text-to-image models through control over sampling dynamics.  					AI-generated summary 				 Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute ...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 28. Introducing reasoning abstractions in reinforcement learning improves structured exploration and generalization for complex problem-solving.  					AI-generated summary 				 Reasoning requires going beyond pattern matching or memorization of solutions to identify and implement "algorithmic procedures...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 29. Transformers trained directly on Cartesian coordinates can achieve competitive performance in molecular energy and force prediction without predefined graphs, demonstrating adaptability and scalability.  					AI-generated summary 				 Graph Neural Networks (GNNs) are the dominant architecture for mo...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 30. VLA-R1 enhances VLA models with RLVR and GRPO to improve reasoning and execution, achieving better generalization and real-world performance using a new dataset with chain-of-thought supervision.  					AI-generated summary 				 Vision-Language-Action (VLA) models aim to unify perception, language un...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 31. Sparse Query Attention (SQA) reduces computational complexity in Transformer models by decreasing the number of Query heads, leading to significant throughput improvements with minimal impact on model quality.  					AI-generated summary 				 The Transformer architecture, underpinned by the Multi-Hea...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 32. Hourglass MLP blocks, with skip connections in expanded dimensions and narrow bottlenecks, outperform conventional narrow-wide-narrow MLPs in generative tasks across image datasets.  					AI-generated summary 				 Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 33. VIRTUE, a novel Visual-InteRactive Text-Image Universal Embedder, integrates segmentation and vision-language models to enable visual interactions and localized grounding, achieving state-of-the-art performance in representation learning tasks.  					AI-generated summary 				 Multimodal representati...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 34. Off-policy reinforcement learning for large language models is explored through a new derivation of group-relative REINFORCE, offering insights into importance sampling, clipping, and data-weighting strategies.  					AI-generated summary 				 Off-policy reinforcement learning (RL) for large language...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 35. SKYLENAGE benchmarks evaluate LLMs on math reasoning, revealing performance gaps and ceiling effects across different educational levels.  					AI-generated summary 				 Large language models (LLMs) now perform strongly on many public math suites, yet frontier separation within mathematics increasin...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 36. Research investigates cross-linguistic transferability of reasoning capabilities in Large Reasoning Models, revealing significant variations and proposing a parallel training approach to improve generalization across languages.  					AI-generated summary 				 Recent advancements in Reinforcement Pos...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 37. Bridge enhances parallel LLM inference by generating interdependent responses, improving accuracy and consistency with minimal additional parameters.  					AI-generated summary 				 Parallel LLM inference scaling involves sampling a set of N>1 responses for a single input prompt. However, these N pa...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 38. A new training objective, MW loss, is introduced to improve retriever calibration and ranking quality by directly optimizing the Area under the ROC Curve (AUC), outperforming Contrastive Loss in retrieval-augmented generation tasks.  					AI-generated summary 				 Dual-encoder retrievers depend on t...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 39. A novel methodology for privacy-preserving synthetic text generation using entity-aware control codes and HIPS theory achieves a balance between privacy and utility in sensitive domains.  					AI-generated summary 				 Text anonymization is essential for responsibly developing and deploying AI in hi...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 40. FrameThinker, a novel framework, enhances video reasoning by iteratively interrogating video content through supervised fine-tuning and reinforcement learning, achieving significant improvements and efficiency over existing models.  					AI-generated summary 				 While Large Vision-Language Models (...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 41. Ignoring rating updates for draws in arena-style evaluations of large language models improves battle outcome prediction accuracy by 1-3% across different rating systems.  					AI-generated summary 				 In arena-style evaluation of large language models (LLMs), two LLMs respond to a user query, and ...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 42. MedQ-Bench introduces a benchmark for language-based evaluation of medical image quality using Multi-modal Large Language Models, focusing on both perceptual and reasoning capabilities.  					AI-generated summary 				 Medical Image Quality Assessment (IQA) serves as the first-mile safety gate for cl...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 43. TimeSeriesScientist (TSci) is an LLM-driven framework that automates time series forecasting with minimal human intervention, outperforming statistical and LLM-based methods and providing transparent reports.  					AI-generated summary 				 Time series forecasting is central to decision-making in do...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 44. Parallel-Distill-Refine (PDR) and Sequential Refinement (SR) improve the performance of LLMs by optimizing accuracy and latency through metacognitive strategies, with PDR showing significant gains on math tasks.  					AI-generated summary 				 Reasoning training incentivizes LLMs to produce long cha...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 45. Research on large language models reveals an asymmetric spectral scaling law in feed-forward networks, indicating that increasing width primarily adds low-energy directions while dominant modes saturate early, leading to underutilized latent space.  					AI-generated summary 				 As large language m...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 46. One-token rollout (OTR) enhances supervised fine-tuning of large language models by incorporating policy gradient methods to improve generalization using on-policy data.  					AI-generated summary 				 Supervised fine-tuning (SFT) is the predominant method for adapting large language models (LLMs), ...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 47. A study evaluates the effectiveness of large language models in identifying context-grounded hallucinations using a newly constructed benchmark and free-form textual descriptions, revealing challenges in distinguishing between missing details and unverifiable information.  					AI-generated summary ...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 48. TRAAC, an online post-training RL method, improves model accuracy and efficiency by adaptively adjusting reasoning steps based on task difficulty using self-attention.  					AI-generated summary 				 Recent thinking models solve complex reasoning tasks by scaling test-time compute, but this scaling ...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 49. AReUReDi, a discrete optimization algorithm, achieves Pareto optimality in multi-objective biomolecule sequence design, outperforming evolutionary and diffusion-based methods.  					AI-generated summary 				 Designing sequences that satisfy multiple, often conflicting, objectives is a central challe...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 50. SQUARE is a two-stage training-free framework that uses Multimodal Large Language Models to enhance zero-shot Composed Image Retrieval by enriching query embeddings and performing efficient batch reranking.  					AI-generated summary 				 Composed Image Retrieval (CIR) aims to retrieve target images...
[04.10.2025 01:43] ********************************************************************************
[04.10.2025 01:43] Abstract 51. IoT-MCP, a framework using the Model Context Protocol, enables seamless communication between Large Language Models and IoT devices, achieving high task success rates and low resource usage.  					AI-generated summary 				 The integration of Large Language Models (LLMs) with Internet-of-Things (IoT)...
[04.10.2025 01:43] Read previous papers.
[04.10.2025 01:43] Generating reviews via LLM API.
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#training", "#long_context", "#data", "#optimization", "#plp"], "emoji": "üóúÔ∏è", "ru": {"title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –∫–æ–¥–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "LongCodeZip ‚Äî —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–∂–∞—Ç–∏—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –∫–æ–¥–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å LLM, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é 
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#benchmark", "#diffusion", "#long_context", "#video"], "emoji": "üé¨", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –±–µ–∑ —É—á–∏—Ç–µ–ª—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –ø—Ä–∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rl", "#optimization"], "emoji": "üéØ", "ru": {"title": "–£—á–∏–º—Å—è –Ω–∞ —Ü–µ–Ω–Ω–æ–º –æ–ø—ã—Ç–µ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ExGRPO ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Å –ø
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#security", "#benchmark", "#3d"], "emoji": "üéØ", "ru": {"title": "–°–∫—Ä—ã—Ç–∞—è –∞—Ç–∞–∫–∞ –Ω–∞ 3D Gaussian Splatting —á–µ—Ä–µ–∑ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—é –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ 3D Gaussian Splatting (3DGS) –∫ –∞—Ç–∞–∫–∞–º —á–µ—Ä–µ–∑ –æ—Ç—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#training", "#open_source", "#optimization"], "emoji": "üéÆ", "ru": {"title": "–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Interactive Training ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#benchmark", "#agents"], "emoji": "üìà", "ru": {"title": "LLM-–∞–≥–µ–Ω—Ç—ã —É—á–∞—Ç—Å—è —Ç–æ—Ä–≥–æ–≤–∞—Ç—å –∞–∫—Ü–∏—è–º–∏, –Ω–æ –ø–æ–∫–∞ –ø—Ä–æ–∏–≥—Ä—ã–≤–∞—é—Ç –ø—Ä–æ—Å—Ç—ã–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º", "desc": "StockBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ —Ä–æ–ª–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–æ—Ä–≥–æ–≤
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#architecture", "#multimodal", "#open_source", "#training", "#optimization", "#dataset"], "emoji": "üîç", "ru": {"title": "–ú–∞–ª–µ–Ω—å–∫–∏–π, –Ω–æ –º–æ—â–Ω—ã–π: –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π encoder –ø–æ–±–µ–∂–¥–∞–µ—Ç –≥–∏–≥–∞–Ω—Ç–æ–≤ –≤ –ø–æ–∏—Å–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ ModernVBERT ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π vision-language enco
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#training", "#multimodal", "#rl", "#games", "#reasoning"], "emoji": "üîç", "ru": {"title": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω—É—é –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ VOGUE, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM –∑–∞ —Å—á—ë
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#inference", "#hallucinations", "#interpretability", "#alignment", "#rlhf"], "emoji": "üéØ", "ru": {"title": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏—è–º–∏: —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ activation steering ‚Äî —Ç–µ—Ö–Ω–∏–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º LLM —á–µ—Ä–µ–∑ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#security"], "emoji": "üå≥", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –∞—Ç–∞–∫ –Ω–∞ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ DialTree-RPO, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –º–µ—Ç–æ–¥—ã reinforcement learning –∏ tree search –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#rlhf", "#training", "#reasoning", "#interpretability"], "emoji": "üéØ", "ru": {"title": "–°–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è LLM –∫–∞–∫ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è (hidden states) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å–æ–¥–µ—Ä–∂–∞—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#agents", "#optimization", "#games"], "emoji": "üéØ", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –∏ —É–º–Ω—ã–π –≤—ã–±–æ—Ä", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Behavior Best-of-N (bBoN) –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É—é—â–∏—Ö –∑–∞–¥–∞—á–∏ –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ.
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#training", "#reasoning", "#rl", "#optimization"], "emoji": "üß†", "ru": {"title": "–£—á–∏–º –º–æ–¥–µ–ª–∏ –¥—É–º–∞—Ç—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω RLP ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø—ã reinforcement learning –Ω–∞ —ç—Ç–∞–ø–µ pre
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#reasoning", "#rag", "#dataset", "#rl", "#optimization", "#multimodal", "#benchmark"], "emoji": "üó∫Ô∏è", "ru": {"title": "–ú–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RewardMap ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#evaluation", "#optimization", "#reasoning"], "emoji": "üîç", "ru": {"title": "–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤ –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ Deep Research Agents (DRA) ‚Äî AI-–∞–≥–µ–Ω—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∫ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∑–∞–¥–∞—á, –ø
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#video", "#audio", "#multimodal", "#open_source", "#architecture"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π", "desc": "Ovi ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–±–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–∫ –µ–¥–∏–Ω—ã–π
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#training", "#open_source", "#dataset", "#small_models", "#optimization"], "emoji": "üéØ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ foundation –º–æ–¥–µ–ª–µ–π –±–µ–∑ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–µ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è", "desc": "F2LLM ‚Äî —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ language models –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–º–µ—Ä–æ–º 0.6B, 1.7B –∏ 4B –ø
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#dataset", "#cv", "#multimodal", "#benchmark", "#open_source", "#architecture"], "emoji": "üéØ", "ru": {"title": "–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ—Ç–∞—Å–∫–∏–≤–∞–Ω–∏–µ —Å FLUX: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "DragFlow ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–µ—Ç–æ–¥–æ–º –ø–µ
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#rlhf", "#optimization", "#hallucinations", "#reasoning", "#benchmark", "#rl"], "emoji": "üéØ", "ru": {"title": "Reinforcement learning —É—á–∏—Ç LLM —Ç–æ—á–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ", "desc": "–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —á–∞—Å—Ç–æ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ - –Ω–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π —Å–Ω
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#math", "#reasoning"], "emoji": "ü•á", "ru": {"title": "Aristotle: AI-—Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –æ–ª–∏–º–ø–∏–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∑–æ–ª–æ—Ç–æ–π –º–µ–¥–∞–ª–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ Aristotle, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º–∞–ª—å–Ω—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é —Å –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–º–∏ —Ä–∞
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#open_source", "#synthetic", "#agents", "#benchmark", "#dataset"], "emoji": "ü¶ú", "ru": {"title": "Toucan: –∫—Ä—É–ø–Ω–µ–π—à–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è AI-–∞–≥–µ–Ω—Ç–æ–≤ —Ä–∞–±–æ—Ç–µ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Toucan ‚Äî —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –ø—É–±–ª–∏—á–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#video", "#reasoning", "#long_context", "#architecture", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω VideoNSA ‚Äî –∞–¥–∞–ø—Ç–∞—Ü–∏—è Native Sparse Attention –¥–ª—è –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ä–µ—à–∞—é—â–∞—è –ø—Ä–æ–±–ª–µ–º—É –æ–≥—Ä–∞–Ω–∏—á
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#hallucinations", "#agents", "#benchmark", "#cv"], "emoji": "‚ùÑÔ∏è", "ru": {"title": "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–Ω–µ–∂–Ω—ã–π –∫–æ–º –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É ¬´—Å–Ω–µ–∂–Ω–æ–≥–æ –∫–æ–º–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π¬ª –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –Ω–∞ –±–∞–∑–µ Visual Language 
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#data", "#hallucinations", "#science", "#dataset", "#healthcare", "#multimodal", "#benchmark", "#open_source"], "emoji": "ü©ª", "ru": {"title": "–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ—Ç–∏–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#data", "#optimization", "#training", "#benchmark", "#architecture"], "emoji": "üéØ", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —ç—Ç–∞–ø–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ScalingAR ‚Äî –ø–µ—Ä–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ test-time scaling –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#reasoning", "#alignment", "#training", "#benchmark", "#agents", "#security"], "emoji": "üéØ", "ru": {"title": "–°–ª–µ–ø–æ–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–∏: –∫–∞–∫ AI-–∞–≥–µ–Ω—Ç—ã –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç –∑–¥—Ä–∞–≤—ã–π —Å–º—ã—Å–ª —Ä–∞–¥–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ AI-–∞–≥–µ–Ω—Ç—ã, —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ–∏
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#cv", "#rl", "#agents", "#reasoning", "#optimization", "#multimodal"], "emoji": "üß©", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ VLM —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é —Å–±–æ—Ä–∫—É –ø–∞–∑–ª–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AGILE ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Vision-Language Models —á–µ—Ä–µ–∑ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á-–≥–æ–ª–æ–≤–æ–ª–æ–º–æ–∫ (jigsaw puzzles) 
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#training", "#cv", "#diffusion"], "emoji": "üéØ", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ –≤ tex
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rlhf", "#rl"], "emoji": "üß©", "ru": {"title": "–ê–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é LLM —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ —á–µ—Ä–µ–∑ reasoning abstractions ‚Äî –∫—Ä–∞—Ç–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#graphs", "#architecture", "#dataset", "#optimization", "#science"], "emoji": "‚öõÔ∏è", "ru": {"title": "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –ø–æ–±–µ–∂–¥–∞—é—Ç –≥—Ä–∞—Ñ—ã –≤ –º–æ–ª–µ–∫—É–ª—è—Ä–Ω–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –æ–±—ã—á–Ω—ã–µ Transformer-–º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é –Ω–∞ –¥–µ–∫–∞—Ä—Ç–æ–≤—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö –∞—Ç–æ–º–æ–≤ –±–µ–∑ –ø
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#reasoning", "#open_source", "#dataset", "#rl", "#training", "#agents"], "emoji": "ü§ñ", "ru": {"title": "VLA —Å —è–≤–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª–µ–µ —É–º–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏", "desc": "VLA-R1 —É–ª—É—á—à–∞–µ—Ç Vision-Language-Action –º–æ–¥–µ–ª–∏, –¥–æ–±–∞–≤–ª—è—è —è–≤–Ω–æ–µ –ø–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ (chain-of-thought) –≤–º–µ
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#long_context", "#training", "#architecture", "#optimization", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ú–µ–Ω—å—à–µ –∑–∞–ø—Ä–æ—Å–æ–≤ ‚Äî –±–æ–ª—å—à–µ —Å–∫–æ—Ä–æ—Å—Ç–∏: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Transformer —á–µ—Ä–µ–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Sparse Query Attention (SQA), –∫–æ—Ç–æ—Ä–∞—è —Å–Ω–∏–∂–∞
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#dataset", "#architecture", "#optimization"], "emoji": "‚è≥", "ru": {"title": "–ü–µ—Å–æ—á–Ω—ã–µ —á–∞—Å—ã –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: skip connections –≤ —à–∏—Ä–æ–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Hourglass MLP, –∫–æ—Ç–æ—Ä–∞—è –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –¥–∏–∑–∞–π–Ω –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–æ–≤: skip connecti
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#interpretability", "#games", "#optimization", "#cv"], "emoji": "üëÜ", "ru": {"title": "Embeddings —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º: —É–∫–∞–∑—ã–≤–∞–π –Ω–∞ –æ–±—ä–µ–∫—Ç—ã –∏ –ø–æ–ª—É—á–∞–π —Ç–æ—á–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å VIRTUE, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –∏ visi
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#optimization", "#rl", "#agi", "#training", "#rlhf"], "emoji": "üéØ", "ru": {"title": "Off-policy –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è LLM: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ REINFORCE", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –∞–ª–≥–æ—Ä–∏—Ç–º REINFORCE –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ–∫–∞–∑—ã–≤–∞—è —á—Ç–æ –æ–Ω 
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#math", "#survey"], "emoji": "üìê", "ru": {"title": "SKYLENAGE: –Ω–æ–≤—ã–π —Å–ª–æ–∂–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –æ–±–Ω–∞–∂–∞–µ—Ç –ø—Ä–µ–¥–µ–ª—ã –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ reasoning –≤ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –¥–≤–∞ –Ω–æ–≤—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞ SKYLENAGE –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM: ReasoningM
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#multilingual", "#reasoning", "#low_resource", "#rlhf", "#transfer_learning"], "emoji": "üåê", "ru": {"title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è AI –Ω–µ –ø–µ—Ä–µ–Ω–æ—Å—è—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM), –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ, –ø–µ—Ä–µ–Ω
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#rl"], "emoji": "üåâ", "ru": {"title": "Bridge: –∫–æ–≥–¥–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã LLM —É—á–∞—Ç—Å—è –¥—Ä—É–≥ —É –¥—Ä—É–≥–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Bridge, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#training", "#optimization", "#rag", "#benchmark"], "emoji": "üìä", "ru": {"title": "MW loss: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è AUC –≤–º–µ—Å—Ç–æ Contrastive Loss –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å MW loss –¥–ª—è –æ–±—É—á–µ–Ω–∏—è dual-encoder retriever –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–ø—Ä—è–º—É—é –æ–ø
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#data", "#ethics", "#dataset", "#healthcare", "#synthetic"], "emoji": "üîí", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å—é —á–µ—Ä–µ–∑ —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ –∫–æ–¥—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –≤ —á—É
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#reasoning", "#video", "#long_context", "#rl", "#training", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–£–º–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –Ω–∞–¥ –≤–∏–¥–µ–æ: –∞–Ω–∞–ª–∏–∑ —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤", "desc": "FrameThinker - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞–¥ –≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º —Å –ø–æ–º–æ—â—å—é Large Vision-Language
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#benchmark", "#games"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ù–∏—á—å—è –≤ –∞—Ä–µ–Ω–µ LLM ‚Äî —ç—Ç–æ –Ω–µ —Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π, –∞ –ª—ë–≥–∫–æ—Å—Ç—å –≤–æ–ø—Ä–æ—Å–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ —Å–∏—Å—Ç–µ–º—É —Ä–µ–π—Ç–∏–Ω–≥–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ arena-style –æ—Ü–µ–Ω–∫–∞—Ö, –≥–¥–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –≤—ã–±–∏—Ä–∞—é—Ç –ª—É—á—à
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#healthcare", "#optimization", "#reasoning"], "emoji": "üè•", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "MedQ-Bench ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π benchmark –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#data", "#optimization", "#agents", "#interpretability", "#training", "#benchmark"], "emoji": "üìà", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM", "desc": "TimeSeriesScientist (TSci) ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ LLM, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#optimization", "#math", "#long_context", "#reasoning", "#training", "#inference", "#rl"], "emoji": "üîÑ", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –º—ã—Å–ª–µ–π: —Ç–æ—á–Ω–µ–µ –∏ –±—ã—Å—Ç—Ä–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Parallel-Distill-Refine (PDR), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#training", "#architecture", "#optimization"], "emoji": "üìä", "ru": {"title": "–®–∏—Ä–æ–∫–∏–µ —Å–µ—Ç–∏ –Ω–µ –∑–Ω–∞—á–∏—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ: –∑–∞–∫–æ–Ω —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —à–∏—Ä–∏–Ω—ã feed-forward —Å–µ—Ç–µ–π –≤ LLM –¥–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∏–∑–∫–æ—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–ø—Ä
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#training", "#optimization", "#benchmark", "#reasoning", "#rl"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∫–∞–∂–¥–æ–º —Ç–æ–∫–µ–Ω–µ: –∫–æ–≥–¥–∞ supervised learning –≤—Å—Ç—Ä–µ—á–∞–µ—Ç reinforcement learning", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ One-Token Rollout (OTR) –¥–ª—è —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#hallucinations", "#data"], "emoji": "üîç", "ru": {"title": "–ü–æ–∏—Å–∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π: –∫–æ–≥–¥–∞ LLM –ø—É—Ç–∞—é—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –æ—à–∏–±–∫–∞–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–∑—É—á–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞—Ö–æ–¥–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ ‚Äî —Å–ª—É—á–∞–∏, –∫–æ–≥–¥–∞ –≤—ã—Ö–æ–¥ 
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#optimization"], "emoji": "üéØ", "ru": {"title": "–î—É–º–∞–π —Å—Ç–æ–ª—å–∫–æ, —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ: –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "TRAAC ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥–∞ —Å reinforcement learning, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç –º–æ–¥–µ–ª—å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#math"], "emoji": "üß¨", "ru": {"title": "–ü–∞—Ä–µ—Ç–æ-–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–∏–∑–∞–π–Ω –±–∏–æ–º–æ–ª–µ–∫—É–ª —á–µ—Ä–µ–∑ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ —Å –æ—Ç–∂–∏–≥–æ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω AReUReDi ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –¥–∏–∑–∞–π–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –±–∏–æ–º–æ–ª–µ–∫—É–ª —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ü–µ–ª—è–º–∏. 
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#cv", "#reasoning", "#benchmark", "#multimodal", "#games"], "emoji": "üîç", "ru": {"title": "–î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–º –∑–∞–ø—Ä–æ—Å–∞–º –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "SQUARE ‚Äî —ç—Ç–æ framework –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–º –∑–∞–ø—Ä–æ—Å–∞–º (—Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ + —Ç–µ–∫—Å—Ç–æ–≤–∞—è –º–æ–¥–∏—Ñ–∏–∫
[04.10.2025 01:43] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#open_source", "#benchmark", "#low_resource"], "emoji": "üåâ", "ru": {"title": "–ú–æ—Å—Ç –º–µ–∂–¥—É LLM –∏ IoT —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏ —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç IoT-MCP ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å IoT-—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏ —á–µ—Ä–µ–∑ Model 
[04.10.2025 01:43] Renaming data file.
[04.10.2025 01:43] Renaming previous data. hf_papers.json to ./d/2025-10-03.json
[04.10.2025 01:43] Saving new data file.
[04.10.2025 01:43] Generating page.
[04.10.2025 01:43] Renaming previous page.
[04.10.2025 01:43] Renaming previous data. index.html to ./d/2025-10-03.html
[04.10.2025 01:43] Writing result.
[04.10.2025 01:43] Renaming log file.
[04.10.2025 01:43] Renaming previous data. log.txt to ./logs/2025-10-04_last_log.txt
