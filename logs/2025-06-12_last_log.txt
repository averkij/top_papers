[12.06.2025 00:55] Read previous papers.
[12.06.2025 00:55] Generating top page (month).
[12.06.2025 00:55] Writing top page (month).
[12.06.2025 02:41] Read previous papers.
[12.06.2025 02:41] Get feed.
[12.06.2025 02:41] Extract page data from URL. URL: https://huggingface.co/papers/2506.09995
[12.06.2025 02:41] Extract page data from URL. URL: https://huggingface.co/papers/2506.09003
[12.06.2025 02:41] Extract page data from URL. URL: https://huggingface.co/papers/2506.09790
[12.06.2025 02:41] Extract page data from URL. URL: https://huggingface.co/papers/2506.09113
[12.06.2025 02:41] Extract page data from URL. URL: https://huggingface.co/papers/2506.08889
[12.06.2025 02:41] Extract page data from URL. URL: https://huggingface.co/papers/2506.09937
[12.06.2025 02:41] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[12.06.2025 02:41] Downloading and parsing papers (pdf, html). Total: 6.
[12.06.2025 02:41] Downloading and parsing paper https://huggingface.co/papers/2506.09995.
[12.06.2025 02:41] Downloading paper 2506.09995 from http://arxiv.org/pdf/2506.09995v1...
[12.06.2025 02:42] Extracting affiliations from text.
[12.06.2025 02:42] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 5 9 9 9 0 . 6 0 5 2 : r PlayerOne: Egocentric World Simulator Yuanpeng Tu1,2 Hao Luo2,3 Xi Chen1 Xiang Bai4 1HKU 2DAMO Academy, Alibaba Group Fan Wang2 Hengshuang Zhao1, 4HUST 3Hupan Lab https://playerone.github.io Figure 1: Simulated videos of our PlayerOne. Given an egocentric image as the scene to be explored, we can simulate egocentric immersive videos that are accurately aligned with the users motion sequence captured by an exocentric camera. All the users have been anonymized and action videos are shot with the front camera. "
[12.06.2025 02:42] Response: ```python
["HKU", "DAMO Academy, Alibaba Group", "HUST", "Hupan Lab"]
```
[12.06.2025 02:42] Deleting PDF ./assets/pdf/2506.09995.pdf.
[12.06.2025 02:42] Success.
[12.06.2025 02:42] Downloading and parsing paper https://huggingface.co/papers/2506.09003.
[12.06.2025 02:42] Downloading paper 2506.09003 from http://arxiv.org/pdf/2506.09003v2...
[12.06.2025 02:42] Extracting affiliations from text.
[12.06.2025 02:42] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 2 3 0 0 9 0 . 6 0 5 2 : r SWE-Flow: Synthesizing Software Engineering Data in Test-Driven Manner Lei Zhang 1 Jiaxi Yang 1 Min Yang* 1 Jian Yang 2 Mouxiang Chen 3 Jiajun Zhang 4 Zeyu Cui 2 Binyuan Hui* 2 Junyang Lin "
[12.06.2025 02:42] Response: ```python
[]
```
[12.06.2025 02:42] Extracting affiliations from text.
[12.06.2025 02:42] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 2 3 0 0 9 0 . 6 0 5 2 : r SWE-Flow: Synthesizing Software Engineering Data in Test-Driven Manner Lei Zhang 1 Jiaxi Yang 1 Min Yang* 1 Jian Yang 2 Mouxiang Chen 3 Jiajun Zhang 4 Zeyu Cui 2 Binyuan Hui* 2 Junyang LinWe introduce SWE-Flow, novel data synthesis framework grounded in Test-Driven Development (TDD). Unlike existing software engineering data that rely on human-submitted issues, SWE-Flow automatically infers incremental development steps directly from unit tests, which inherently encapsulate high-level requirements. The core of SWE-Flow is the construction of Runtime Dependency Graph (RDG), which precisely captures function interactions, enabling the generation of structured, step-by-step development schedule. At each step, SWE-Flow produces partial codebase, the corresponding unit tests, and the necessary code modifications, resulting in fully verifiable TDD tasks. With this approach, we generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, creating the SWE-Flow-Bench benchmark. Our experiments show that fine-tuning open model on this dataset significantly improves performance in TDD-based coding. To facilitate further research, we release all code, datasets, models, and Docker images at Github. 1. Introduction In recent years, Large Language Models (LLMs) have achieved remarkable performance in code-related tasks (Chen et al., 2021). Training on large-scale code data, these models have made significant advancements in code completion, generation, debugging, and refactoring within software engineering (Rozi`ere et al., 2023; Guo et al., 2024; Hui et al., Work done during an internship at Alibaba Qwen. 1Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China 2Alibaba Group, Beijing, China 3Zhejiang University, Hangzhou, China 4University of Science and Technology of China, Hefei, China. Correspondence to: Min Yang <min.yang@siat.ac.cn>, Binyuan Hui <binyuan.hby@alibaba-inc.com>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 2024; Huang et al., 2024). As result, numerous LLMpowered code applications have emerged, including GitHub Copilot1, which provides code completion and answers to programming-related queries; Cursor2, which enables crossfile code modifications; and Devin3, an autonomous agent designed for fully automated software development. These tools are becoming essential for enhancing developer productivity and advancing intelligent software engineering. Despite their impressive capabilities, current LLMs still face limitations when applied to real-world software development. Existing evaluations, such as HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), primarily assess standalone function implementations, whereas practical development involves complex dependencies, incremental modifications, and multi-file interactions. Constructing datasets and evaluation methodologies that more accurately reflect real-world development challenges is thus critical and ongoing research problem. Recent efforts, such as SWEBench (Jimenez et al., 2023), have attempted to bridge this gap by mining Github Issues from open-source projects, capturing authentic bug fixes and feature enhancements. However, this approach heavily depends on the availability and quality of human-submitted issues, requiring extensive data cleaning and filtering. Furthermore, the reliance of SWE-Bench on human-generated commits derived from issue reports fails to encompass the full spectrum of development tasks and variations, thereby overlooking key aspects of the iterative and complex nature of real-world software development. To address these challenges, we introduce SWE-Flow, reverse data synthesis approach centered on Test-Driven Development (TDD) (Beck, 2002). TDD is highly structured methodology in which development is driven by test cases: developers write tests first, then implement the required functionality, and finally verify correctness by executing the tests. SWE-Flow automatically infers the incremental development process directly from unit tests, thereby generating high-quality training instances. The key insight is that each unit test inherently represents high-level expres1https://github.com/features/copilot 2https://www.cursor.com 3https://devin.ai SWE-Flow: Synthesizing Software Engineering Data in Test-Driven Manner sion of requirements. It specifies the behaviors the code must exhibit and implicitly encodes the developers intention and design considerations. Consequently, SWE-Flow eliminates the need for human commit histories by harnessing TDD to automatically produce development tasks with clear structures and explicit goals. Concretely, SWE-Flow captures the function call relationships during unit test execution to construct Runtime Dependency Graph (RDG) for the entire project. This tactic overcomes the limitations of traditional static code analysis tools, which often struggle to accurately parse the dependencies of functions and variables. Drawing on the RDG, SWE-Flow generates project development schedule that delineates how an entire codebase can be built from scratch in an incremental manner. At each step, new functions must be implemented on top of existing functionality to pass the corresponding unit tests. For each development step, SWE-Flow produces three types of training instances: (i) Partial Codebase: The codebase is stripped of the functions that need to be implemented in the current step, simulating the state of incomplete development. (ii) Requirement Document: Unit tests associated with the current step provides high-level specification of the required functionality. (iii) Reference Solution (diff): The difference between the complete codebase and the partial codebase, serving as guide for the development task. SWE-Flow offers three major advantages: We present dedicated benchmark for evaluating LLMs on realistic software engineering tasks, addressing significant gap in existing assessment methods. We generate 16,061 training instances and fine-tune Qwen2.5-Coder-32B-Instruct, demonstrating the efficacy of SWE-Flow data in empirical studies. We publicly release all code, models, datasets, and Docker images, fostering further research in the community. Additionally, data generated by SWE-Flow has the potential to support two future directions. Firstly, by scaling the SWEF"
[12.06.2025 02:42] Mistral response. {"id": "fff80896928b48119dc54686a2e1b88e", "object": "chat.completion", "created": 1749696132, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China\",\n    \"Alibaba Group, Beijing, China\",\n    \"Zhejiang University, Hangzhou, China\",\n    \"University of Science and Technology of China, Hefei, China\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1580, "total_tokens": 1661, "completion_tokens": 81}}
[12.06.2025 02:42] Response: ```python
[
    "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China",
    "Alibaba Group, Beijing, China",
    "Zhejiang University, Hangzhou, China",
    "University of Science and Technology of China, Hefei, China"
]
```
[12.06.2025 02:42] Deleting PDF ./assets/pdf/2506.09003.pdf.
[12.06.2025 02:42] Success.
[12.06.2025 02:42] Downloading and parsing paper https://huggingface.co/papers/2506.09790.
[12.06.2025 02:42] Downloading paper 2506.09790 from http://arxiv.org/pdf/2506.09790v1...
[12.06.2025 02:42] Extracting affiliations from text.
[12.06.2025 02:42] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 0 9 7 9 0 . 6 0 5 2 : r ComfyUI-R1: Exploring Reasoning Models for Workflow Generation ZHENRAN XU, Harbin Institute of Technology (Shenzhen), China YIYU WANG, Alibaba International Digital Commerce, China XUE YANG, Alibaba International Digital Commerce, China LONGYUE WANG, Alibaba International Digital Commerce, China WEIHUA LUO, Alibaba International Digital Commerce, China KAIFU ZHANG, Alibaba International Digital Commerce, China BAOTIAN HU, Harbin Institute of Technology (Shenzhen), China MIN ZHANG, Harbin Institute of Technology (Shenzhen), China Fig. 1. We introduce ComfyUI-R1, large reasoning model for automated workflow generation. Given user instruction, ComfyUI-R1 performs long chain-of-thought reasoning to generate code representation of ComfyUI workflow. The generated workflow adheres to the correct format, executes successfully, and produces an image that aligns with the users instruction. ComfyUI-R1 is integrated in https://github.com/AIDC-AI/ComfyUI-Copilot. Both authors contributed equally to this research. Corresponding author. Authors Contact Information: Zhenran Xu, Harbin Institute of Technology (Shenzhen), Shenzhen, China, xuzhenran@stu.hit.edu.cn; Yiyu Wang, Alibaba International Digital Commerce, Hangzhou, China, wangyiyu18@mails.ucas.ac.cn; Xue Yang, Alibaba International Digital Commerce, Hangzhou, China, yx9966@126.com; Longyue Wang, Alibaba International Digital Commerce, Hangzhou, China, vincentwang0229@ gmail.com; Weihua Luo, Alibaba International Digital Commerce, Hangzhou, China, weihua.luowh@alibaba-inc.com; Kaifu Zhang, Alibaba International Digital Commerce, Hangzhou, China, kaifu.zkf@alibaba-inc.com; Baotian Hu, Harbin Institute of Technology (Shenzhen), Shenzhen, China, hubaotian@hit.edu.cn; Min Zhang, Harbin Institute of Technology (Shenzhen), Shenzhen, China, zhangmin2021@hit.edu.cn. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee pro"
[12.06.2025 02:42] Response: ```python
[
    "Harbin Institute of Technology (Shenzhen), China",
    "Alibaba International Digital Commerce, China"
]
```
[12.06.2025 02:42] Deleting PDF ./assets/pdf/2506.09790.pdf.
[12.06.2025 02:42] Success.
[12.06.2025 02:42] Downloading and parsing paper https://huggingface.co/papers/2506.09113.
[12.06.2025 02:42] Downloading paper 2506.09113 from http://arxiv.org/pdf/2506.09113v1...
[12.06.2025 02:42] Extracting affiliations from text.
[12.06.2025 02:42] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Seedance 1.0: Exploring the Boundaries of Video Generation Models "
[12.06.2025 02:42] Response: []
[12.06.2025 02:42] Extracting affiliations from text.
[12.06.2025 02:42] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Seedance 1.0: Exploring the Boundaries of Video Generation ModelsNotable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving 10 inference speedup through multistage distillation strategies and system-level optimizations. Seedance 1.0 can generate 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation. Official Page: https://seed.bytedance.com/seedance 5 2 0 2 0 1 ] . [ 1 3 1 1 9 0 . 6 0 5 2 : r Figure 1 Overall evaluation. Left: Text-to-Video; Right: Image-to-Video. Seedance 1.0 ranks first on both the two video generation leaderboards of Artificial Analysis on Jun 10, 2025 (Due to unavailable public data, the Elo score for Kling 2.1 is taken from Kling 2.0). "Speed" denotes the inverse of the average generation time per second of video (from API).Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 2 Model Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1 Variational Autoencoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Diffusion Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Diffusion Refiner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Prompt Engineering (PE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 Data Pre-Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Video Captioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Efficient Engineering Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Model Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1 Pre-Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Continue Training (CT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Human Feedback Alignment (RLHF) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.1 Feedback Data Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.2 Reward Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.3 Base Model Feedback Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Super-Resolution RLHF Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.4 5 Inference Optimizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Model Acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Inference Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 6 Training Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.1 Pre-Training Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Post-Training Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Model Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1 Artificial Analysis Arena . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Comprehensive Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . SeedVideoBench 1.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2.1 7.2.2 Video Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2.3 Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 Multi-Shot Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4 Multi-Style Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.5 Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Contributions and Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 4 5 5 6 6 7 7 8 9 9 9 11 11 11 12 12 12 12 14 14 15 15 16 16 16 18 18 21 21 22 22 25With recent advances in diffusion models, the progress of video generation has been accelerated considerably. Leading open-source frameworks including Wan [26], Huanyuan Video [15], and CogVideoX [30], complemented by commercial systems such as Veo and Keling, have catalyzed broad academic and industrial adoption. However, current video generation foundation models still have critical challenges in balancing multidimensional requirements, particularly in prompt following, motion plausibility, and visual fidelity. To address these limitations, we present Seedance 1.0, f"
[12.06.2025 02:42] Mistral response. {"id": "20f6daef2e8e4bc5bd79b36efb711a5e", "object": "chat.completion", "created": 1749696173, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 2519, "total_tokens": 2527, "completion_tokens": 8}}
[12.06.2025 02:42] Response: ```python
[]
```
[12.06.2025 02:42] Deleting PDF ./assets/pdf/2506.09113.pdf.
[12.06.2025 02:42] Success.
[12.06.2025 02:42] Downloading and parsing paper https://huggingface.co/papers/2506.08889.
[12.06.2025 02:43] Downloading paper 2506.08889 from http://arxiv.org/pdf/2506.08889v1...
[12.06.2025 02:43] Extracting affiliations from text.
[12.06.2025 02:43] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 1 ] . [ 1 9 8 8 8 0 . 6 0 5 2 : r SeerAttention-R: Sparse Attention Adaptation for Long Reasoning Yizhao Gao 12 Lei Wang14 Shuming Guo 13 Lingxiao Ma1 Shijie Cao1 Yutao Sun15 Yuqing Xia Yu Cheng14 Tianzhu Ye15 Li Dong1 Hayden Kwok-Hay So2 Yu Hua3 Ting Cao Fan Yang1 Mao Yang1 1 Microsoft Research 2 The University of Hong Kong 3 Huazhong University of Science and Technology 4 Peking University 5 Tsinghua University "
[12.06.2025 02:43] Response: ```python
["Microsoft Research", "The University of Hong Kong", "Huazhong University of Science and Technology", "Peking University", "Tsinghua University"]
```
[12.06.2025 02:43] Deleting PDF ./assets/pdf/2506.08889.pdf.
[12.06.2025 02:43] Success.
[12.06.2025 02:43] Downloading and parsing paper https://huggingface.co/papers/2506.09937.
[12.06.2025 02:43] Downloading paper 2506.09937 from http://arxiv.org/pdf/2506.09937v1...
[12.06.2025 02:43] Extracting affiliations from text.
[12.06.2025 02:43] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 7 3 9 9 0 . 6 0 5 2 : r SAFE: Multitask Failure Detection for Vision-Language-Action Models Qiao Gu1,2,3 Yuanliang Ju1,2,3 Shengxiang Sun1,2 Igor Gilitschenski1,2,3 Haruki Nishimura4 Masha Itkina4 Florian Shkurti1,2,3 1University of Toronto (UofT), 2UofT Robotics Institute, 3Vector Institute, 4Toyota Research Institute (TRI) q.gu@mail.utoronto.ca "
[12.06.2025 02:43] Response: ```python
["University of Toronto (UofT)", "UofT Robotics Institute", "Vector Institute", "Toyota Research Institute (TRI)"]
```
[12.06.2025 02:43] Deleting PDF ./assets/pdf/2506.09937.pdf.
[12.06.2025 02:43] Success.
[12.06.2025 02:43] Enriching papers with extra data.
[12.06.2025 02:43] ********************************************************************************
[12.06.2025 02:43] Abstract 0. PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.  					AI-generated summary 				 We introduce PlayerOne, the first egocentric real...
[12.06.2025 02:43] ********************************************************************************
[12.06.2025 02:43] Abstract 1. A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.  					AI-generated summary 				 We intr...
[12.06.2025 02:43] ********************************************************************************
[12.06.2025 02:43] Abstract 2. ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.  					AI-generated summary 				 AI-generated content has evolved from monolithic models to modular...
[12.06.2025 02:43] ********************************************************************************
[12.06.2025 02:43] Abstract 3. Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.  					AI-generated summary 				 Notable breakthroughs in diffusion modeling have propelled...
[12.06.2025 02:43] ********************************************************************************
[12.06.2025 02:43] Abstract 4. SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.  					AI-generated summary 				 We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long...
[12.06.2025 02:43] ********************************************************************************
[12.06.2025 02:43] Abstract 5. SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.  					AI-generated summary 				 While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipul...
[12.06.2025 02:43] Read previous papers.
[12.06.2025 02:43] Generating reviews via LLM API.
[12.06.2025 02:43] Querying the API.
[12.06.2025 02:43] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.  					AI-generated summary 				 We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications.
[12.06.2025 02:43] Response: {
  "desc": "PlayerOne - это первый эгоцентрический симулятор реалистичного мира, способный генерировать видео на основе изображений, снятых пользователем. Система использует каскадный процесс обучения, начиная с предварительной тренировки на больших наборах эгоцентрических видео с текстовыми описаниями, а затем выполняет тонкую настройку на синхронизированных данных движения и видео. PlayerOne применяет усовершенствованные методы внедрения движения и реконструкции сцены для обеспечения точного контроля над движениями и согласованности сцены при генерации длительных видео. Эта система открывает новые возможности для моделирования мира и имеет широкий спектр потенциальных применений.",
  "emoji": "🎥",
  "title": "Погружение в реальность: эгоцентрическая симуляция мира с PlayerOne"
}
[12.06.2025 02:43] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.  					AI-generated summary 				 We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications."

[12.06.2025 02:43] Response: ```python
['VIDEO', 'MULTIMODAL', 'TRAINING']
```
[12.06.2025 02:43] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.  					AI-generated summary 				 We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications."

[12.06.2025 02:43] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[12.06.2025 02:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PlayerOne is an innovative egocentric realistic world simulator that generates videos based on user-captured images. It employs a coarse-to-fine training approach, initially pretraining on large datasets of egocentric text-video pairs, followed by fine-tuning with motion-video data for enhanced accuracy. The system features a part-disentangled motion injection scheme, allowing for detailed control over individual movements, and a joint reconstruction framework that maintains scene consistency across video frames. This pioneering work opens new avenues for world modeling and applications in immersive environments.","title":"Revolutionizing Egocentric World Simulation with PlayerOne"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PlayerOne is an innovative egocentric realistic world simulator that generates videos based on user-captured images. It employs a coarse-to-fine training approach, initially pretraining on large datasets of egocentric text-video pairs, followed by fine-tuning with motion-video data for enhanced accuracy. The system features a part-disentangled motion injection scheme, allowing for detailed control over individual movements, and a joint reconstruction framework that maintains scene consistency across video frames. This pioneering work opens new avenues for world modeling and applications in immersive environments.', title='Revolutionizing Egocentric World Simulation with PlayerOne'))
[12.06.2025 02:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"PlayerOne是一个以自我为中心的现实世界模拟器，能够根据用户捕捉的图像构建和生成视频。它采用粗到细的训练流程，首先在大规模的自我中心文本-视频对上进行预训练，然后在同步运动-视频数据上进行微调。该系统设计了部分解耦的运动注入方案，以实现对部分运动的精确控制，并通过联合重建框架确保长视频生成中的场景一致性。实验结果表明，PlayerOne在控制人类运动和建模多样场景方面具有出色的泛化能力。","title":"开创自我中心现实世界模拟的新纪元"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='PlayerOne是一个以自我为中心的现实世界模拟器，能够根据用户捕捉的图像构建和生成视频。它采用粗到细的训练流程，首先在大规模的自我中心文本-视频对上进行预训练，然后在同步运动-视频数据上进行微调。该系统设计了部分解耦的运动注入方案，以实现对部分运动的精确控制，并通过联合重建框架确保长视频生成中的场景一致性。实验结果表明，PlayerOne在控制人类运动和建模多样场景方面具有出色的泛化能力。', title='开创自我中心现实世界模拟的新纪元'))
[12.06.2025 02:43] Querying the API.
[12.06.2025 02:43] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.  					AI-generated summary 				 We introduce **SWE-Flow**, a novel data synthesis framework grounded in Test-Driven Development (TDD). Unlike existing software engineering data that rely on human-submitted issues, **SWE-Flow** automatically infers incremental development steps directly from unit tests, which inherently encapsulate high-level requirements. The core of **SWE-Flow** is the construction of a Runtime Dependency Graph (RDG), which precisely captures function interactions, enabling the generation of a structured, step-by-step *development schedule*. At each step, **SWE-Flow** produces a partial codebase, the corresponding unit tests, and the necessary code modifications, resulting in fully verifiable TDD tasks. With this approach, we generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, creating the **SWE-Flow-Eval** benchmark. Our experiments show that fine-tuning open model on this dataset significantly improves performance in TDD-based coding. To facilitate further research, we release all code, datasets, models, and Docker images at [Github](https://github.com/Hambaobao/SWE-Flow).
[12.06.2025 02:43] Response: {
  "desc": "SWE-Flow - это новая система синтеза данных для разработки программного обеспечения, основанная на разработке через тестирование (TDD). Она автоматически выводит этапы разработки из модульных тестов, создавая структурированный график разработки с помощью графа зависимостей времени выполнения (RDG). SWE-Flow генерирует частичную кодовую базу, соответствующие модульные тесты и необходимые изменения кода на каждом этапе. Эксперименты показывают, что дообучение открытых моделей на созданном наборе данных значительно улучшает их производительность в TDD-задачах.",
  "emoji": "🧪",
  "title": "SWE-Flow: автоматизация TDD для улучшения ИИ-моделей в программировании"
}
[12.06.2025 02:43] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.  					AI-generated summary 				 We introduce **SWE-Flow**, a novel data synthesis framework grounded in Test-Driven Development (TDD). Unlike existing software engineering data that rely on human-submitted issues, **SWE-Flow** automatically infers incremental development steps directly from unit tests, which inherently encapsulate high-level requirements. The core of **SWE-Flow** is the construction of a Runtime Dependency Graph (RDG), which precisely captures function interactions, enabling the generation of a structured, step-by-step *development schedule*. At each step, **SWE-Flow** produces a partial codebase, the corresponding unit tests, and the necessary code modifications, resulting in fully verifiable TDD tasks. With this approach, we generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, creating the **SWE-Flow-Eval** benchmark. Our experiments show that fine-tuning open model on this dataset significantly improves performance in TDD-based coding. To facilitate further research, we release all code, datasets, models, and Docker images at [Github](https://github.com/Hambaobao/SWE-Flow)."

[12.06.2025 02:43] Response: ```python
['DATASET', 'DATA', 'BENCHMARK', 'TRAINING']
```
[12.06.2025 02:43] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.  					AI-generated summary 				 We introduce **SWE-Flow**, a novel data synthesis framework grounded in Test-Driven Development (TDD). Unlike existing software engineering data that rely on human-submitted issues, **SWE-Flow** automatically infers incremental development steps directly from unit tests, which inherently encapsulate high-level requirements. The core of **SWE-Flow** is the construction of a Runtime Dependency Graph (RDG), which precisely captures function interactions, enabling the generation of a structured, step-by-step *development schedule*. At each step, **SWE-Flow** produces a partial codebase, the corresponding unit tests, and the necessary code modifications, resulting in fully verifiable TDD tasks. With this approach, we generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, creating the **SWE-Flow-Eval** benchmark. Our experiments show that fine-tuning open model on this dataset significantly improves performance in TDD-based coding. To facilitate further research, we release all code, datasets, models, and Docker images at [Github](https://github.com/Hambaobao/SWE-Flow)."

[12.06.2025 02:43] Response: ```python
['SYNTHETIC', 'OPEN_SOURCE']
```
[12.06.2025 02:43] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SWE-Flow is a new framework designed to enhance Test-Driven Development (TDD) by automatically generating development schedules from unit tests. It infers development steps directly from these tests, which represent high-level requirements, rather than relying on human-submitted issues. The framework constructs a Runtime Dependency Graph (RDG) to capture function interactions, allowing for a structured approach to coding tasks. By generating a large dataset from real-world projects, SWE-Flow significantly improves the performance of models fine-tuned for TDD-based coding.","title":"Automating TDD with SWE-Flow: Smarter Development Schedules"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SWE-Flow is a new framework designed to enhance Test-Driven Development (TDD) by automatically generating development schedules from unit tests. It infers development steps directly from these tests, which represent high-level requirements, rather than relying on human-submitted issues. The framework constructs a Runtime Dependency Graph (RDG) to capture function interactions, allowing for a structured approach to coding tasks. By generating a large dataset from real-world projects, SWE-Flow significantly improves the performance of models fine-tuned for TDD-based coding.', title='Automating TDD with SWE-Flow: Smarter Development Schedules'))
[12.06.2025 02:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SWE-Flow是一种新颖的数据合成框架，基于测试驱动开发（TDD）方法。它通过自动推断单元测试中的开发步骤，生成结构化的开发计划，从而提高了在真实项目上微调开放模型的性能。SWE-Flow的核心是构建运行时依赖图（RDG），准确捕捉函数之间的交互，确保每一步生成部分代码库及相应的单元测试。通过这种方法，我们从真实的GitHub项目中生成了大量的训练和测试实例，显著提升了基于TDD的编码性能。","title":"SWE-Flow：自动化测试驱动开发的创新框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SWE-Flow是一种新颖的数据合成框架，基于测试驱动开发（TDD）方法。它通过自动推断单元测试中的开发步骤，生成结构化的开发计划，从而提高了在真实项目上微调开放模型的性能。SWE-Flow的核心是构建运行时依赖图（RDG），准确捕捉函数之间的交互，确保每一步生成部分代码库及相应的单元测试。通过这种方法，我们从真实的GitHub项目中生成了大量的训练和测试实例，显著提升了基于TDD的编码性能。', title='SWE-Flow：自动化测试驱动开发的创新框架'))
[12.06.2025 02:44] Querying the API.
[12.06.2025 02:44] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.  					AI-generated summary 				 AI-generated content has evolved from monolithic models to modular workflows, particularly on platforms like ComfyUI, enabling customization in creative pipelines. However, crafting effective workflows requires great expertise to orchestrate numerous specialized components, presenting a steep learning curve for users. To address this challenge, we introduce ComfyUI-R1, the first large reasoning model for automated workflow generation. Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT fine-tuning for cold start, adapting models to the ComfyUI domain; (2) reinforcement learning for incentivizing reasoning capability, guided by a fine-grained rule-metric hybrid reward, ensuring format validity, structural integrity, and node-level fidelity. Experiments show that our 7B-parameter model achieves a 97\% format validity rate, along with high pass rate, node-level and graph-level F1 scores, significantly surpassing prior state-of-the-art methods that employ leading closed-source models such as GPT-4o and Claude series. Further analysis highlights the critical role of the reasoning process and the advantage of transforming workflows into code. Qualitative comparison reveals our strength in synthesizing intricate workflows with diverse nodes, underscoring the potential of long CoT reasoning in AI art creation.
[12.06.2025 02:44] Response: {
  "desc": "ComfyUI-R1 - это крупная модель машинного обучения для автоматической генерации рабочих процессов в сфере ИИ-искусства. Модель использует длинные цепочки рассуждений и обучение с подкреплением для создания эффективных рабочих процессов. ComfyUI-R1 обучается на наборе данных из 4000 рабочих процессов с использованием двухэтапного подхода: тонкая настройка цепочек рассуждений и обучение с подкреплением. Эксперименты показывают, что 7-миллиардная модель значительно превосходит существующие методы по различным метрикам.",
  "emoji": "🎨",
  "title": "ComfyUI-R1: ИИ-художник нового поколения"
}
[12.06.2025 02:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.  					AI-generated summary 				 AI-generated content has evolved from monolithic models to modular workflows, particularly on platforms like ComfyUI, enabling customization in creative pipelines. However, crafting effective workflows requires great expertise to orchestrate numerous specialized components, presenting a steep learning curve for users. To address this challenge, we introduce ComfyUI-R1, the first large reasoning model for automated workflow generation. Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT fine-tuning for cold start, adapting models to the ComfyUI domain; (2) reinforcement learning for incentivizing reasoning capability, guided by a fine-grained rule-metric hybrid reward, ensuring format validity, structural integrity, and node-level fidelity. Experiments show that our 7B-parameter model achieves a 97\% format validity rate, along with high pass rate, node-level and graph-level F1 scores, significantly surpassing prior state-of-the-art methods that employ leading closed-source models such as GPT-4o and Claude series. Further analysis highlights the critical role of the reasoning process and the advantage of transforming workflows into code. Qualitative comparison reveals our strength in synthesizing intricate workflows with diverse nodes, underscoring the potential of long CoT reasoning in AI art creation."

[12.06.2025 02:44] Response: ```python
['DATASET', 'RL', 'TRAINING', 'ARCHITECTURE']
```
[12.06.2025 02:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.  					AI-generated summary 				 AI-generated content has evolved from monolithic models to modular workflows, particularly on platforms like ComfyUI, enabling customization in creative pipelines. However, crafting effective workflows requires great expertise to orchestrate numerous specialized components, presenting a steep learning curve for users. To address this challenge, we introduce ComfyUI-R1, the first large reasoning model for automated workflow generation. Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT fine-tuning for cold start, adapting models to the ComfyUI domain; (2) reinforcement learning for incentivizing reasoning capability, guided by a fine-grained rule-metric hybrid reward, ensuring format validity, structural integrity, and node-level fidelity. Experiments show that our 7B-parameter model achieves a 97\% format validity rate, along with high pass rate, node-level and graph-level F1 scores, significantly surpassing prior state-of-the-art methods that employ leading closed-source models such as GPT-4o and Claude series. Further analysis highlights the critical role of the reasoning process and the advantage of transforming workflows into code. Qualitative comparison reveals our strength in synthesizing intricate workflows with diverse nodes, underscoring the potential of long CoT reasoning in AI art creation."

[12.06.2025 02:44] Response: ```python
['REASONING', 'LONG_CONTEXT', 'OPTIMIZATION']
```
[12.06.2025 02:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ComfyUI-R1 is a large reasoning model designed to automate the generation of workflows for creating AI art. It utilizes long chain-of-thought (CoT) reasoning and reinforcement learning to improve the process of workflow creation, making it easier for users to customize their creative pipelines. The model is trained on a dataset of 4,000 workflows and employs a two-stage framework that includes CoT fine-tuning and reinforcement learning with a hybrid reward system. Experimental results show that ComfyUI-R1 outperforms existing models in terms of format validity and overall workflow quality, highlighting the effectiveness of its reasoning capabilities.","title":"Automating AI Art Workflows with ComfyUI-R1"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ComfyUI-R1 is a large reasoning model designed to automate the generation of workflows for creating AI art. It utilizes long chain-of-thought (CoT) reasoning and reinforcement learning to improve the process of workflow creation, making it easier for users to customize their creative pipelines. The model is trained on a dataset of 4,000 workflows and employs a two-stage framework that includes CoT fine-tuning and reinforcement learning with a hybrid reward system. Experimental results show that ComfyUI-R1 outperforms existing models in terms of format validity and overall workflow quality, highlighting the effectiveness of its reasoning capabilities.', title='Automating AI Art Workflows with ComfyUI-R1'))
[12.06.2025 02:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ComfyUI-R1 是一个大型推理模型，专注于自动化工作流生成，特别是在 AI 艺术创作中表现出色。它通过长链推理和强化学习，帮助用户创建定制化的工作流，降低了学习曲线。该模型使用了 4K 工作流的数据集，经过两阶段的训练，确保了工作流的格式有效性和结构完整性。实验结果显示，ComfyUI-R1 在格式有效性和 F1 分数上显著超越了之前的先进方法，展示了长链推理在 AI 艺术创作中的潜力。","title":"ComfyUI-R1：自动化工作流生成的推理模型"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ComfyUI-R1 是一个大型推理模型，专注于自动化工作流生成，特别是在 AI 艺术创作中表现出色。它通过长链推理和强化学习，帮助用户创建定制化的工作流，降低了学习曲线。该模型使用了 4K 工作流的数据集，经过两阶段的训练，确保了工作流的格式有效性和结构完整性。实验结果显示，ComfyUI-R1 在格式有效性和 F1 分数上显著超越了之前的先进方法，展示了长链推理在 AI 艺术创作中的潜力。', title='ComfyUI-R1：自动化工作流生成的推理模型'))
[12.06.2025 02:44] Querying the API.
[12.06.2025 02:44] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.  					AI-generated summary 				 Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation.
[12.06.2025 02:44] Response: {
  "desc": "Seedance 1.0 - это высокопроизводительная модель генерации видео, объединяющая несколько ключевых технических улучшений. Она включает в себя многоисточниковую курацию данных с точными видеоподписями, эффективную архитектуру с поддержкой многокадровой генерации и совместным обучением задачам текст-в-видео и изображение-в-видео. Модель использует оптимизированные пост-тренировочные подходы, включая тонкую настройку под наблюдением и видео-специфичное RLHF. Благодаря многоэтапным стратегиям дистилляции и системным оптимизациям, Seedance 1.0 достигает ~10-кратного ускорения вывода.",

  "emoji": "🎬",

  "title": "Революция в генерации видео: быстро, качественно, многозадачно"
}
[12.06.2025 02:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.  					AI-generated summary 				 Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation."

[12.06.2025 02:44] Response: ```python
['DATASET', 'DATA', 'BENCHMARK', 'VIDEO', 'ARCHITECTURE', 'TRAINING', 'RLHF', 'INFERENCE']
```
[12.06.2025 02:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.  					AI-generated summary 				 Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation."

[12.06.2025 02:44] Response: ```python
["OPTIMIZATION", "DIFFUSION"]
```
[12.06.2025 02:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Seedance 1.0 is a cutting-edge video generation model that enhances performance through advanced data curation and an efficient architecture. It addresses key challenges in video generation, such as prompt adherence and visual quality, by integrating multi-source data and a novel training paradigm. The model employs optimized post-training techniques, including fine-tuning and reinforcement learning with multi-dimensional rewards, to boost its capabilities. With a remarkable inference speedup of approximately 10 times, Seedance 1.0 can produce high-quality 5-second videos at 1080p resolution in just 41.4 seconds.","title":"Seedance 1.0: Fast and High-Quality Video Generation Revolutionized"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Seedance 1.0 is a cutting-edge video generation model that enhances performance through advanced data curation and an efficient architecture. It addresses key challenges in video generation, such as prompt adherence and visual quality, by integrating multi-source data and a novel training paradigm. The model employs optimized post-training techniques, including fine-tuning and reinforcement learning with multi-dimensional rewards, to boost its capabilities. With a remarkable inference speedup of approximately 10 times, Seedance 1.0 can produce high-quality 5-second videos at 1080p resolution in just 41.4 seconds.', title='Seedance 1.0: Fast and High-Quality Video Generation Revolutionized'))
[12.06.2025 02:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Seedance 1.0 是一种高性能的视频生成模型，结合了先进的数据整理、有效的架构设计、后训练优化和模型加速技术，提供了卓越的质量和速度。该模型通过多源数据整理和精准的视频字幕，增强了对多样场景的全面学习能力。它的高效架构支持多镜头生成，并同时学习文本到视频和图像到视频的任务。Seedance 1.0 通过多阶段蒸馏策略实现了约10倍的推理加速，能够在41.4秒内生成5秒的1080p视频。","title":"Seedance 1.0：高效视频生成的新标杆"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Seedance 1.0 是一种高性能的视频生成模型，结合了先进的数据整理、有效的架构设计、后训练优化和模型加速技术，提供了卓越的质量和速度。该模型通过多源数据整理和精准的视频字幕，增强了对多样场景的全面学习能力。它的高效架构支持多镜头生成，并同时学习文本到视频和图像到视频的任务。Seedance 1.0 通过多阶段蒸馏策略实现了约10倍的推理加速，能够在41.4秒内生成5秒的1080p视频。', title='Seedance 1.0：高效视频生成的新标杆'))
[12.06.2025 02:44] Querying the API.
[12.06.2025 02:44] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.  					AI-generated summary 				 We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.
[12.06.2025 02:44] Response: {
  "desc": "SeerAttention-R - это разреженная система внимания для моделей рассуждения, которая сохраняет высокую точность и достигает значительного ускорения с помощью оптимизированных ядер разреженного декодирования. Она обучается на всего 0,4 миллиардах токенов и поддерживает точность рассуждений почти без потерь с бюджетом в 4000 токенов в тесте AIME при больших размерах блоков разреженного внимания. Используя TileLang, авторы разработали высокооптимизированное ядро разреженного декодирования, которое достигает почти теоретического ускорения до 9 раз по сравнению с FlashAttention-3 на GPU H100 при 90% разреженности. SeerAttention-R можно легко интегрировать в существующие предобученные модели без изменения исходных параметров.",
  "emoji": "🧠",
  "title": "Эффективное разреженное внимание для моделей рассуждения"
}
[12.06.2025 02:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.  					AI-generated summary 				 We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention."

[12.06.2025 02:44] Response: ```python
['ARCHITECTURE', 'TRAINING', 'BENCHMARK']
```
[12.06.2025 02:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.  					AI-generated summary 				 We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention."

[12.06.2025 02:44] Response: ```python
['REASONING', 'LONG_CONTEXT', 'OPTIMIZATION']
```
[12.06.2025 02:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SeerAttention-R is a new framework designed for sparse attention in reasoning models, focusing on efficient long decoding. It builds on the original SeerAttention by using a self-distilled gating mechanism to learn attention sparsity while eliminating query pooling for better auto-regressive decoding. This framework is lightweight and can be easily integrated into existing pretrained models without altering their parameters. Our experiments show that SeerAttention-R achieves high accuracy with significant speed improvements, processing up to 4K tokens efficiently on advanced hardware.","title":"SeerAttention-R: Speed and Accuracy in Sparse Attention for Reasoning Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SeerAttention-R is a new framework designed for sparse attention in reasoning models, focusing on efficient long decoding. It builds on the original SeerAttention by using a self-distilled gating mechanism to learn attention sparsity while eliminating query pooling for better auto-regressive decoding. This framework is lightweight and can be easily integrated into existing pretrained models without altering their parameters. Our experiments show that SeerAttention-R achieves high accuracy with significant speed improvements, processing up to 4K tokens efficiently on advanced hardware.', title='SeerAttention-R: Speed and Accuracy in Sparse Attention for Reasoning Models'))
[12.06.2025 02:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SeerAttention-R是一种稀疏注意力框架，专为推理模型的长解码而设计。它通过自蒸馏门控机制学习注意力稀疏性，同时去除了查询池化，以适应自回归解码。该框架轻量且灵活，可以无缝集成到现有的预训练模型中，而无需修改原始参数。实验表明，SeerAttention-R在AIME基准测试中以4K令牌预算保持接近无损的推理准确性，并在H100 GPU上实现了高达9倍的速度提升。","title":"SeerAttention-R：高效稀疏注意力推理框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SeerAttention-R是一种稀疏注意力框架，专为推理模型的长解码而设计。它通过自蒸馏门控机制学习注意力稀疏性，同时去除了查询池化，以适应自回归解码。该框架轻量且灵活，可以无缝集成到现有的预训练模型中，而无需修改原始参数。实验表明，SeerAttention-R在AIME基准测试中以4K令牌预算保持接近无损的推理准确性，并在H100 GPU上实现了高达9倍的速度提升。', title='SeerAttention-R：高效稀疏注意力推理框架'))
[12.06.2025 02:44] Querying the API.
[12.06.2025 02:44] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.  					AI-generated summary 				 While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out-of-the-box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, pi_0, and pi_0-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results can be found at https://vla-safe.github.io/.
[12.06.2025 02:44] Response: {
  "desc": "SAFE - это детектор ошибок для моделей зрения-языка-действия (VLA), который обобщается на новые задачи, обучаясь на высокоуровневых внутренних признаках моделей. Он анализирует пространство признаков VLA и обнаруживает, что эти модели имеют достаточные знания об успехе и неудаче задач, которые являются общими для разных задач. SAFE обучается на успешных и неудачных прогонах и оценивается на невиденных задачах. Детектор показывает лучшие результаты по сравнению с базовыми методами и обеспечивает оптимальный компромисс между точностью и временем обнаружения.",

  "emoji": "🤖",

  "title": "SAFE: универсальный детектор ошибок для роботов-генералистов"
}
[12.06.2025 02:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.  					AI-generated summary 				 While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out-of-the-box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, pi_0, and pi_0-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results can be found at https://vla-safe.github.io/."

[12.06.2025 02:44] Response: ```python
["AGENTS", "ROBOTICS"]
```
[12.06.2025 02:44] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.  					AI-generated summary 				 While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out-of-the-box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, pi_0, and pi_0-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results can be found at https://vla-safe.github.io/."

[12.06.2025 02:44] Response: ```python
["AGI", "SECURITY", "OPTIMIZATION"]
```
[12.06.2025 02:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SAFE is a novel failure detector designed for vision-language-action models (VLAs) that enables them to generalize to new tasks. It leverages high-level internal features of VLAs to predict the likelihood of task failure, allowing robots to respond appropriately in unfamiliar environments. Unlike traditional failure detectors that are limited to specific tasks, SAFE is trained on both successful and failed attempts across various tasks, enhancing its adaptability. The effectiveness of SAFE is demonstrated through extensive testing on multiple policy architectures, achieving superior performance in failure detection compared to existing methods.","title":"SAFE: Generalizing Failure Detection for Vision-Language-Action Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SAFE is a novel failure detector designed for vision-language-action models (VLAs) that enables them to generalize to new tasks. It leverages high-level internal features of VLAs to predict the likelihood of task failure, allowing robots to respond appropriately in unfamiliar environments. Unlike traditional failure detectors that are limited to specific tasks, SAFE is trained on both successful and failed attempts across various tasks, enhancing its adaptability. The effectiveness of SAFE is demonstrated through extensive testing on multiple policy architectures, achieving superior performance in failure detection compared to existing methods.', title='SAFE: Generalizing Failure Detection for Vision-Language-Action Models'))
[12.06.2025 02:44] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SAFE是一个用于视觉-语言-行动模型的故障检测器，它能够通过学习模型的高层内部特征来推广到未见过的任务。尽管视觉-语言-行动模型在多种操作任务中表现出色，但在新任务上的成功率有限。为了让机器人安全地与环境互动，我们需要一个能够及时发出警报的故障检测器，以便机器人可以停止、回溯或请求帮助。我们提出的SAFE能够从VLA的内部特征中学习，并预测任务失败的可能性，经过广泛测试，显示出优越的故障检测性能。","title":"SAFE：智能机器人故障检测的新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SAFE是一个用于视觉-语言-行动模型的故障检测器，它能够通过学习模型的高层内部特征来推广到未见过的任务。尽管视觉-语言-行动模型在多种操作任务中表现出色，但在新任务上的成功率有限。为了让机器人安全地与环境互动，我们需要一个能够及时发出警报的故障检测器，以便机器人可以停止、回溯或请求帮助。我们提出的SAFE能够从VLA的内部特征中学习，并预测任务失败的可能性，经过广泛测试，显示出优越的故障检测性能。', title='SAFE：智能机器人故障检测的新方法'))
[12.06.2025 02:44] Loading Chinese text from previous data.
[12.06.2025 02:44] Renaming data file.
[12.06.2025 02:44] Renaming previous data. hf_papers.json to ./d/2025-06-12.json
[12.06.2025 02:44] Saving new data file.
[12.06.2025 02:44] Generating page.
[12.06.2025 02:44] Renaming previous page.
[12.06.2025 02:44] Renaming previous data. index.html to ./d/2025-06-12.html
[12.06.2025 02:44] [Experimental] Generating Chinese page for reading.
[12.06.2025 02:44] Chinese vocab [{'word': '评估', 'pinyin': 'pínggū', 'trans': 'evaluate'}, {'word': '大型语言模型', 'pinyin': 'dàxíng yǔyán móxíng', 'trans': 'large language models'}, {'word': '地缘政治', 'pinyin': 'dìyuán zhèngzhì', 'trans': 'geopolitical'}, {'word': '偏见', 'pinyin': 'piānjiàn', 'trans': 'bias'}, {'word': '涉及', 'pinyin': 'shèjí', 'trans': 'involve'}, {'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'}, {'word': '数据集', 'pinyin': 'shùjùjí', 'trans': 'dataset'}, {'word': '中立', 'pinyin': 'zhōnglì', 'trans': 'neutral'}, {'word': '对立', 'pinyin': 'duìlì', 'trans': 'opposing'}, {'word': '观点', 'pinyin': 'guāndiǎn', 'trans': 'viewpoint'}, {'word': '倾向于', 'pinyin': 'qīngxiàngyú', 'trans': 'tend towards'}, {'word': '叙事', 'pinyin': 'xùshì', 'trans': 'narrative'}, {'word': '简单', 'pinyin': 'jiǎndān', 'trans': 'simple'}, {'word': '去偏见', 'pinyin': 'qù piānjiàn', 'trans': 'debias'}, {'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'}, {'word': '效果', 'pinyin': 'xiàoguǒ', 'trans': 'effect'}, {'word': '有限', 'pinyin': 'yǒuxiàn', 'trans': 'limited'}, {'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'}, {'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'show'}, {'word': '参与者', 'pinyin': 'cānyùzhě', 'trans': 'participant'}, {'word': '标签', 'pinyin': 'biāoqiān', 'trans': 'label'}, {'word': '更改', 'pinyin': 'gēnggǎi', 'trans': 'change'}, {'word': '敏感', 'pinyin': 'mǐngǎn', 'trans': 'sensitive'}, {'word': '放大', 'pinyin': 'fàngdà', 'trans': 'amplify'}, {'word': '识别', 'pinyin': 'shíbié', 'trans': 'identify'}, {'word': '不一致', 'pinyin': 'bù yīzhì', 'trans': 'inconsistent'}]
[12.06.2025 02:44] Renaming previous Chinese page.
[12.06.2025 02:44] Renaming previous data. zh.html to ./d/2025-06-11_zh_reading_task.html
[12.06.2025 02:44] Writing Chinese reading task.
[12.06.2025 02:44] Writing result.
[12.06.2025 02:44] Renaming log file.
[12.06.2025 02:44] Renaming previous data. log.txt to ./logs/2025-06-12_last_log.txt
