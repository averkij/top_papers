[13.06.2025 03:44] Read previous papers.
[13.06.2025 03:44] Generating top page (month).
[13.06.2025 03:44] Writing top page (month).
[13.06.2025 04:20] Read previous papers.
[13.06.2025 04:20] Get feed.
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09993
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09513
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10857
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10954
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10890
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10952
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10821
[13.06.2025 04:20] Extract page data from URL. URL: https://huggingface.co/papers/2506.09967
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08060
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09942
[13.06.2025 04:20] Extract page data from URL. URL: https://huggingface.co/papers/2506.10953
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10357
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09344
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06561
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05982
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08373
[13.06.2025 04:20] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.06.2025 04:20] No deleted papers detected.
[13.06.2025 04:20] Downloading and parsing papers (pdf, html). Total: 16.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.09993.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.09993.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.09993.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.09513.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.09513.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.09513.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.10857.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.10857.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.10857.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.10954.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.10954.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.10954.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.10890.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.10890.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.10890.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.10952.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.10952.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.10952.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.10821.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.10821.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.10821.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.09967.
[13.06.2025 04:20] Downloading paper 2506.09967 from http://arxiv.org/pdf/2506.09967v1...
[13.06.2025 04:20] Extracting affiliations from text.
[13.06.2025 04:20] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 7 6 9 9 0 . 6 0 5 2 : r Resa: Transparent Reasoning Models via SAEs Shangshang Wang, Julian Asilis, Ã–mer Faruk AkgÃ¼l, Enes Burak Bilgin, Ollie Liu, Deqing Fu, and Willie Neiswanger How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, family of 1.5B reasoning models trained via novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from source model, and then uses the trained SAE to guide standard supervised fine-tuning process to elicit such abilities in target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterparts reasoning performance while reducing training costs by >2000x to roughly $1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around $1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced. Notion Blog: https://shangshangwang.notion.site/resa Code Repository: https://github.com/shangshang-wang/Resa Training Logs: https://wandb.ai/upup-ashton-wang-usc/Resa Model Weights & Checkpoints: https://huggingface.co/Resa-Yi 1. Introduction Reasoning language models have demonstrated increasing performance in do"
[13.06.2025 04:20] Response: ```python
[]
```
[13.06.2025 04:20] Extracting affiliations from text.
[13.06.2025 04:20] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 7 6 9 9 0 . 6 0 5 2 : r Resa: Transparent Reasoning Models via SAEs Shangshang Wang, Julian Asilis, Ã–mer Faruk AkgÃ¼l, Enes Burak Bilgin, Ollie Liu, Deqing Fu, and Willie NeiswangerHow cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, family of 1.5B reasoning models trained via novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from source model, and then uses the trained SAE to guide standard supervised fine-tuning process to elicit such abilities in target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterparts reasoning performance while reducing training costs by >2000x to roughly $1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around $1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced. Notion Blog: https://shangshangwang.notion.site/resa Code Repository: https://github.com/shangshang-wang/Resa Training Logs: https://wandb.ai/upup-ashton-wang-usc/Resa Model Weights & Checkpoints: https://huggingface.co/Resa-Yi 1. Introduction Reasoning language models have demonstrated increasing performance in domains like math, coding, and science (Wang and Neiswanger, 2025, Xu et al., 2025). Despite the impressive reasoning performance elicited by reinforcement learning (RL) or supervised fine-tuning (SFT) (Chu et al., 2025), these methods often operate as black box. In other words, while they improve reasoning, how they alter the models internal representations to do so is largely opaque. Furthermore, RL-based workflows are notoriously resourceintensive, requiring vast computational power and long training time to converge. On the other hand, SFT hinges on the availability of high-quality Chain-of-Thought (CoT) reasoning traces, which are costly to curate (Muennighoff et al., 2025). This leaves critical gap in the field: The need for three-birds-one-stone method that can elicit strong reasoning abilities in way that is not only effective but also computationally efficient and transparent. In this paper, we bridge this gap with Resa, family of 1.5B reasoning models trained via sparse autoencoders (SAEs), using novel SAE-Tuning procedure. SAEs are unsupervised models designed to deconstruct models dense internal activations into sparse dictionary of more interpretable latent features (Anthropic, 2023, 2024). Our key insight is that within this dictionary, certain features must correspond to the fundamental building blocks of reasoning. By instilling latent reasoning features captured by an SAE back into model via tuning procedure, we can effectively and efficiently elicit the models reasoning abilities. Corresponding author(s): Shangshang Wang shangshangwang.github.io; Willie Neiswanger neiswang@usc.edu Resa: Transparent Reasoning Models via SAEs Figure 1: Comparison between Example Resa Models and Baselines The Tina models correspond to the best checkpoints in Wang et al. (2025a). Resa-STILL and Resa-DeepScaleR correspond to Resa-STILL-v5 and Resa-DeepScaleR-v3 in Table 4, respectively. For these Resa models, the required SAEs are trained from scratch (as shown in Section 3.2) and both computational and time costs are total costs for training SAEs and models. Reasoning performance denotes the average zero-shot Pass@1 score across AIME24/25, AMC23, MATH500, GPQA Diamond, and Minerva benchmarks. Specifically, SAE-Tuning involves two key stages: First, we use an SAE to probe the internal activations of source model, identifying and extracting dictionary of latent features that correspond to its reasoning processes. Second, we freeze this feature-rich SAE and insert it into target model to guide SFT process to elicit reasoning abilities in the target model. SAE-Tuning also distinguishes itself from existing methods by using SFT on minimal verified CoT-free question-answer data type. By verified, we mean that the answer correctness is ensured via methods like human annotation or language-model-based verification (Guha et al., 2025), while CoT-free signifies that our SAE-Tuning procedure functions without needing explicit step-by-step reasoning traces. Crucially, our control experiments in Table 4 demonstrate that performing standard SFT on this same CoT-free data without an SAE fails to elicit any meaningful reasoning, highlighting the vital role of the SAE. We summarize our core contributions as follows: Efficient Reasoning Ability Elicitation Purely using verified CoT-free data, we demonstrate that SAETuning can be applied in an end-to-end manner to certain base models with trained-from-scratch SAE to elicit reasoning abilities on par with those achieved via costly RL. This leads to substantial gains with peak training cost reductions of over 2000x (to approximately $1) and time reductions of over 450x (to under 20 minutes) compared to RL-based workflows, while maintaining comparable performance. Generalizable and Modular Reasoning Ability We establish the generality and modularity of the extracted reasoning abilities such that these abilities generalize across out-of-distribution datasets and can be attached to models within the same family at test time without additional training, functioning as portable reasoning adapter. Transparent Reasoning Feature Extraction We provide transparent view into the models reasoning abilities. Specifically, we propose novel prompt-only method to extract and quantify latent reasoning features using SAEs and show that the layer-wise distribution of these reasoning features correlates with reasoning performance of Resa models, offering data-driven path to optimizing SAE-Tuning. 2 Resa: Transpar"
[13.06.2025 04:20] Mistral response. {"id": "c5edecdb9ade4bcc9dd53b6cb65c48c0", "object": "chat.completion", "created": 1749788447, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1692, "total_tokens": 1700, "completion_tokens": 8}}
[13.06.2025 04:20] Response: ```python
[]
```
[13.06.2025 04:20] Deleting PDF ./assets/pdf/2506.09967.pdf.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.08060.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.08060.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.08060.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.09942.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.09942.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.09942.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.10953.
[13.06.2025 04:21] Downloading paper 2506.10953 from http://arxiv.org/pdf/2506.10953v1...
[13.06.2025 04:21] Extracting affiliations from text.
[13.06.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . [ 1 3 5 9 0 1 . 6 0 5 2 : r Build the web for agents, not agents for the web Xing Han LÃ¹ Gaurav Kamath Marius Mosbach Siva Reddy McGill University Mila Quebec AI Institute Equal Advising "
[13.06.2025 04:21] Response: ```python
["McGill University", "Mila", "Quebec AI Institute", "Equal Advising"]
```
[13.06.2025 04:21] Deleting PDF ./assets/pdf/2506.10953.pdf.
[13.06.2025 04:21] Success.
[13.06.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2506.10357.
[13.06.2025 04:21] Extra JSON file exists (./assets/json/2506.10357.json), skip PDF parsing.
[13.06.2025 04:21] Paper image links file exists (./assets/img_data/2506.10357.json), skip HTML parsing.
[13.06.2025 04:21] Success.
[13.06.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2506.09344.
[13.06.2025 04:21] Extra JSON file exists (./assets/json/2506.09344.json), skip PDF parsing.
[13.06.2025 04:21] Paper image links file exists (./assets/img_data/2506.09344.json), skip HTML parsing.
[13.06.2025 04:21] Success.
[13.06.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2506.06561.
[13.06.2025 04:21] Extra JSON file exists (./assets/json/2506.06561.json), skip PDF parsing.
[13.06.2025 04:21] Paper image links file exists (./assets/img_data/2506.06561.json), skip HTML parsing.
[13.06.2025 04:21] Success.
[13.06.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2506.05982.
[13.06.2025 04:21] Extra JSON file exists (./assets/json/2506.05982.json), skip PDF parsing.
[13.06.2025 04:21] Paper image links file exists (./assets/img_data/2506.05982.json), skip HTML parsing.
[13.06.2025 04:21] Success.
[13.06.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2506.08373.
[13.06.2025 04:21] Extra JSON file exists (./assets/json/2506.08373.json), skip PDF parsing.
[13.06.2025 04:21] Paper image links file exists (./assets/img_data/2506.08373.json), skip HTML parsing.
[13.06.2025 04:21] Success.
[13.06.2025 04:21] Enriching papers with extra data.
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 0. The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.  					AI-generated summary 				 Image restoration aims to recover degra...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 1. ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.  					AI-generated summary 				 Though reasoning-based large language models (LLMs) hav...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 2. VRBench evaluates long video understanding by assessing multi-step reasoning capabilities across temporal and procedural validity using human-labeled question-answering pairs and reasoning chains.  					AI-generated summary 				 We present VRBench, the first long narrative video benchmark crafted fo...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 3. An automated pipeline, SWE-Factory, is introduced to facilitate the creation of large-scale datasets for evaluating and training Large Language Models in GitHub issue resolution tasks, offering efficient environment building, standardized grading, and automated validation.  					AI-generated summary...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 4. CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.  					AI-generated summary 				 Graphic design plays a crucial role in both commercial and personal contexts, yet creating high-quality, editable, and...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 5. Domain2Vec decomposes datasets into meta-domains to optimize language model pretraining and downstream performance with reduced computational cost.  					AI-generated summary 				 We introduce~Domain2Vec, a novel approach that decomposes any dataset into a linear combination of several meta-domains,...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 6. VideoDeepResearch, a text-only reasoning model with modular tools, surpasses existing baselines in long video understanding tasks without extending context windows or enhancing visual perception capabilities.  					AI-generated summary 				 Long video understanding (LVU) presents a significant chall...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 7. SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.  					AI-generated summary 				 How cost-effectively can we elicit strong reasoning in language models by leveraging their und...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 8. Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.  					AI-generated summary 				 Large language models have transformed natural language processing, yet supervised...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 9. VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enha...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 10. A paradigm shift in web agent research is proposed, advocating for the development of Agentic Web Interfaces (AWIs) to optimize interaction for AI agents within web environments.  					AI-generated summary 				 Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spur...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 11. Optimus-3, an agent using knowledge-enhanced data generation, Mixture-of-Experts routing, and multimodal reasoning-augmented reinforcement learning, achieves superior performance across various tasks in Minecraft.  					AI-generated summary 				 Recently, agents based on multimodal large language mo...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 12. Ming-Omni is a unified multimodal model with dedicated encoders and modality-specific routers that can process images, text, audio, and video, and performs tasks like speech and image generation, context-aware chatting, and versatile image editing.  					AI-generated summary 				 We propose Ming-Omn...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 13. LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.  					AI-generated summary 				 Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been de...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 14. MCA-Bench is a multimodal benchmark suite for CAPTCHA security evaluation which fine-tunes specialized cracking agents using a shared vision-language model.  					AI-generated summary 				 As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious ...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 15. A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.  					AI-generated summary 				 Optimizing inference for long-context Large Language M...
[13.06.2025 04:21] Read previous papers.
[13.06.2025 04:21] Generating reviews via LLM API.
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#dataset", "#cv", "#benchmark", "#diffusion", "#hallucinations"], "emoji": "ğŸ“", "ru": {"title": "Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸", "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ° (TAIR) Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´Ğµ
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#benchmark", "#healthcare", "#reasoning", "#training"], "emoji": "ğŸ©º", "ru": {"title": "ReasonMed: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ˜Ğ˜", "desc": "ReasonMed - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· 37
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#reasoning", "#video", "#multimodal"], "emoji": "ğŸ¬", "ru": {"title": "VRBench: ĞÑ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ", "desc": "VRBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#data", "#science", "#dataset", "#agents", "#benchmark", "#open_source"], "emoji": "ğŸ­", "ru": {"title": "SWE-Factory: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ LLM Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ", "desc": "SWE-Factory - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#dataset", "#cv", "#benchmark", "#open_source", "#multimodal"], "emoji": "ğŸ¨", "ru": {"title": "CreatiPoster: Ğ˜Ğ˜-Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ", "desc": "CreatiPoster - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞº
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#transfer_learning", "#dataset", "#training", "#optimization", "#data"], "emoji": "ğŸ§©", "ru": {"title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Domain2Vec - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼ĞµÑ‚Ğ°-Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#long_context", "#video", "#multimodal", "#agents"], "emoji": "ğŸ¥", "ru": {"title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾", "desc": "VideoDeepResearch - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½
[13.06.2025 04:21] Querying the API.
[13.06.2025 04:21] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.  					AI-generated summary 				 How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around 1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced.
[13.06.2025 04:21] Response: {
  "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SAE-Tuning Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ (SAE) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğº Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. SAE-Tuning Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.",
  "emoji": "ğŸ§ ",
  "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ²"
}
[13.06.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.  					AI-generated summary 				 How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around 1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced."

[13.06.2025 04:21] Response: ```python
["TRAINING", "RL", "SMALL_MODELS"]
```
[13.06.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.  					AI-generated summary 				 How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around 1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced."

[13.06.2025 04:21] Response: ```python
["REASONING", "OPTIMIZATION", "OPEN_SOURCE"]
```
[13.06.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces SAE-Tuning, a method that enhances reasoning capabilities in language models using sparse autoencoders. This approach allows for significant performance improvements without the need for extensive retraining, achieving cost reductions of over 2000 times and time savings of over 450 times. By training a sparse autoencoder to capture reasoning skills from a source model, the method effectively guides the fine-tuning of a target model using verified question-answer data. The results show that the reasoning abilities gained are both generalizable across datasets and modular, allowing for easy integration into different models without retraining.","title":"Efficient Reasoning Enhancement in Language Models with SAE-Tuning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces SAE-Tuning, a method that enhances reasoning capabilities in language models using sparse autoencoders. This approach allows for significant performance improvements without the need for extensive retraining, achieving cost reductions of over 2000 times and time savings of over 450 times. By training a sparse autoencoder to capture reasoning skills from a source model, the method effectively guides the fine-tuning of a target model using verified question-answer data. The results show that the reasoning abilities gained are both generalizable across datasets and modular, allowing for easy integration into different models without retraining.', title='Efficient Reasoning Enhancement in Language Models with SAE-Tuning'))
[13.06.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SAE-Tuningæ˜¯ä¸€ç§é«˜æ•ˆçš„ç¨€ç–è‡ªç¼–ç å™¨è°ƒä¼˜æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨è¯­è¨€æ¨¡å‹ä¸­å¼•å‘å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿›è¡Œå¤§é‡çš„é‡æ–°è®­ç»ƒã€‚è¯¥æ–¹æ³•é¦–å…ˆè®­ç»ƒä¸€ä¸ªç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œä»¥ä»æºæ¨¡å‹ä¸­æ•æ‰æ¨ç†èƒ½åŠ›ï¼Œç„¶ååˆ©ç”¨è®­ç»ƒå¥½çš„SAEæŒ‡å¯¼æ ‡å‡†çš„ç›‘ç£å¾®è°ƒè¿‡ç¨‹ï¼Œä»è€Œåœ¨ç›®æ ‡æ¨¡å‹ä¸­å¼•å‘è¿™äº›èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSAE-Tuningåœ¨ä¿æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬å’Œæ—¶é—´ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæå–çš„æ¨ç†èƒ½åŠ›å…·æœ‰å¯æ³›åŒ–å’Œæ¨¡å—åŒ–çš„ç‰¹æ€§ï¼Œå¯ä»¥åœ¨ä¸åŒçš„æ•°æ®é›†å’Œæ¨¡å‹ä¹‹é—´çµæ´»åº”ç”¨ã€‚","title":"é«˜æ•ˆæ¨ç†ï¼šç¨€ç–è‡ªç¼–ç å™¨è°ƒä¼˜çš„åŠ›é‡"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SAE-Tuningæ˜¯ä¸€ç§é«˜æ•ˆçš„ç¨€ç–è‡ªç¼–ç å™¨è°ƒä¼˜æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨è¯­è¨€æ¨¡å‹ä¸­å¼•å‘å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿›è¡Œå¤§é‡çš„é‡æ–°è®­ç»ƒã€‚è¯¥æ–¹æ³•é¦–å…ˆè®­ç»ƒä¸€ä¸ªç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œä»¥ä»æºæ¨¡å‹ä¸­æ•æ‰æ¨ç†èƒ½åŠ›ï¼Œç„¶ååˆ©ç”¨è®­ç»ƒå¥½çš„SAEæŒ‡å¯¼æ ‡å‡†çš„ç›‘ç£å¾®è°ƒè¿‡ç¨‹ï¼Œä»è€Œåœ¨ç›®æ ‡æ¨¡å‹ä¸­å¼•å‘è¿™äº›èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSAE-Tuningåœ¨ä¿æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬å’Œæ—¶é—´ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæå–çš„æ¨ç†èƒ½åŠ›å…·æœ‰å¯æ³›åŒ–å’Œæ¨¡å—åŒ–çš„ç‰¹æ€§ï¼Œå¯ä»¥åœ¨ä¸åŒçš„æ•°æ®é›†å’Œæ¨¡å‹ä¹‹é—´çµæ´»åº”ç”¨ã€‚', title='é«˜æ•ˆæ¨ç†ï¼šç¨€ç–è‡ªç¼–ç å™¨è°ƒä¼˜çš„åŠ›é‡'))
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#rag", "#inference", "#training"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ±ĞµĞ· Ğ¸Ğ·
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#rl", "#benchmark", "#open_source", "#reasoning", "#training", "#rlhf"], "emoji": "ğŸ¤–", "ru": {"title": "VerIF: Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ RL Ğ² ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VerIF - Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ 
[13.06.2025 04:21] Querying the API.
[13.06.2025 04:21] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A paradigm shift in web agent research is proposed, advocating for the development of Agentic Web Interfaces (AWIs) to optimize interaction for AI agents within web environments.  					AI-generated summary 				 Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community.
[13.06.2025 04:21] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ğ²Ğ¾Ğ´Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ’ĞµĞ±-Ğ˜Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² (AWI). AWI Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ²ĞµĞ±-ÑÑ€ĞµĞ´Ğ¾Ğ¹, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ»ÑĞ´ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ ÑˆĞµÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° AWI, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… ÑƒÑĞ¸Ğ»Ğ¸Ğ¹ ML-ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°.",
  "emoji": "ğŸŒ",
  "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ’ĞµĞ±-Ğ˜Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ˜Ğ˜ Ñ Ğ²ĞµĞ±-ÑÑ€ĞµĞ´Ğ¾Ğ¹"
}
[13.06.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A paradigm shift in web agent research is proposed, advocating for the development of Agentic Web Interfaces (AWIs) to optimize interaction for AI agents within web environments.  					AI-generated summary 				 Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community."

[13.06.2025 04:21] Response: ```python
['AGENTS', 'MULTIMODAL']
```
[13.06.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A paradigm shift in web agent research is proposed, advocating for the development of Agentic Web Interfaces (AWIs) to optimize interaction for AI agents within web environments.  					AI-generated summary 				 Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community."

[13.06.2025 04:21] Response: ```python
["AGI", "OPTIMIZATION"]
```
[13.06.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper proposes a new approach to web agent research by introducing Agentic Web Interfaces (AWIs), which are specifically designed for AI agents to interact with web environments. Current web interfaces are not optimized for the capabilities of AI, leading to inefficiencies and challenges in task completion. The authors outline six guiding principles for designing AWIs, focusing on safety, efficiency, and standardization to benefit all stakeholders involved. This shift aims to enhance the performance and reliability of web agents, encouraging collaboration within the machine learning community.","title":"Redefining Web Interaction for AI Agents with AWIs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper proposes a new approach to web agent research by introducing Agentic Web Interfaces (AWIs), which are specifically designed for AI agents to interact with web environments. Current web interfaces are not optimized for the capabilities of AI, leading to inefficiencies and challenges in task completion. The authors outline six guiding principles for designing AWIs, focusing on safety, efficiency, and standardization to benefit all stakeholders involved. This shift aims to enhance the performance and reliability of web agents, encouraging collaboration within the machine learning community.', title='Redefining Web Interaction for AI Agents with AWIs'))
[13.06.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡æå‡ºäº†ç½‘ç»œä»£ç†ç ”ç©¶çš„èŒƒå¼è½¬å˜ï¼Œå€¡å¯¼å¼€å‘ä»£ç†ç½‘ç»œæ¥å£ï¼ˆAWIï¼‰ï¼Œä»¥ä¼˜åŒ–äººå·¥æ™ºèƒ½ä»£ç†åœ¨ç½‘ç»œç¯å¢ƒä¸­çš„äº¤äº’ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€æ¨¡å‹çš„è¿›æ­¥ï¼Œå¼€å‘èƒ½å¤Ÿè‡ªä¸»å¯¼èˆªå’Œå®Œæˆä»»åŠ¡çš„ç½‘ç»œä»£ç†å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å½“å‰çš„æ–¹æ³•é¢ä¸´ç€äººç±»è®¾è®¡çš„ç•Œé¢ä¸LLMèƒ½åŠ›ä¹‹é—´çš„æ ¹æœ¬ä¸åŒ¹é…é—®é¢˜ï¼Œå¯¼è‡´å¤„ç†å¤æ‚ç½‘ç»œè¾“å…¥æ—¶çš„å›°éš¾ã€‚æœ¬æ–‡æå‡ºçš„AWIæ¦‚å¿µæ—¨åœ¨ä¸ºä»£ç†è®¾è®¡ä¸“é—¨çš„äº¤äº’ç•Œé¢ï¼Œä»¥æé«˜å®‰å…¨æ€§ã€æ•ˆç‡å’Œæ ‡å‡†åŒ–ï¼Œæ¨åŠ¨æ›´é«˜æ•ˆã€å¯é å’Œé€æ˜çš„ç½‘ç»œä»£ç†è®¾è®¡ã€‚","title":"ä¸ºä»£ç†è®¾è®¡ä¼˜åŒ–ç½‘ç»œäº¤äº’ç•Œé¢"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡æå‡ºäº†ç½‘ç»œä»£ç†ç ”ç©¶çš„èŒƒå¼è½¬å˜ï¼Œå€¡å¯¼å¼€å‘ä»£ç†ç½‘ç»œæ¥å£ï¼ˆAWIï¼‰ï¼Œä»¥ä¼˜åŒ–äººå·¥æ™ºèƒ½ä»£ç†åœ¨ç½‘ç»œç¯å¢ƒä¸­çš„äº¤äº’ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€æ¨¡å‹çš„è¿›æ­¥ï¼Œå¼€å‘èƒ½å¤Ÿè‡ªä¸»å¯¼èˆªå’Œå®Œæˆä»»åŠ¡çš„ç½‘ç»œä»£ç†å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å½“å‰çš„æ–¹æ³•é¢ä¸´ç€äººç±»è®¾è®¡çš„ç•Œé¢ä¸LLMèƒ½åŠ›ä¹‹é—´çš„æ ¹æœ¬ä¸åŒ¹é…é—®é¢˜ï¼Œå¯¼è‡´å¤„ç†å¤æ‚ç½‘ç»œè¾“å…¥æ—¶çš„å›°éš¾ã€‚æœ¬æ–‡æå‡ºçš„AWIæ¦‚å¿µæ—¨åœ¨ä¸ºä»£ç†è®¾è®¡ä¸“é—¨çš„äº¤äº’ç•Œé¢ï¼Œä»¥æé«˜å®‰å…¨æ€§ã€æ•ˆç‡å’Œæ ‡å‡†åŒ–ï¼Œæ¨åŠ¨æ›´é«˜æ•ˆã€å¯é å’Œé€æ˜çš„ç½‘ç»œä»£ç†è®¾è®¡ã€‚', title='ä¸ºä»£ç†è®¾è®¡ä¼˜åŒ–ç½‘ç»œäº¤äº’ç•Œé¢'))
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#agents", "#rag", "#rl", "#reasoning", "#multimodal", "#games"], "emoji": "ğŸ¤–", "ru": {"title": "Optimus-3: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑĞµÑ‚ Minecraft", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Optimus-3 - Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¸Ğ³Ñ€Ñ‹ Minecraft, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞ³
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#audio", "#open_source", "#video", "#cv", "#multimodal"], "emoji": "ğŸ¤–", "ru": {"title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹: Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ming-Omni - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚, 
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#multimodal", "#interpretability", "#games"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´", "desc": "LaMP-Cap Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#security", "#open_source", "#multimodal"], "emoji": "ğŸ”", "ru": {"title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ CAPTCHA", "desc": "MCA-Bench - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ CAPTCHA, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#optimization", "#long_context", "#benchmark", "#architecture", "#training", "#inference"], "emoji": "ğŸš€", "ru": {"title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ˜Ğ˜ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ¼Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¾Ğ²", "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¸ÑĞ¿
[13.06.2025 04:21] Loading Chinese text from previous data.
[13.06.2025 04:21] Renaming data file.
[13.06.2025 04:21] Renaming previous data. hf_papers.json to ./d/2025-06-13.json
[13.06.2025 04:21] Saving new data file.
[13.06.2025 04:21] Generating page.
[13.06.2025 04:21] Renaming previous page.
[13.06.2025 04:21] Renaming previous data. index.html to ./d/2025-06-13.html
[13.06.2025 04:21] [Experimental] Generating Chinese page for reading.
[13.06.2025 04:21] Chinese vocab [{'word': 'Seedance', 'pinyin': 'SÄ«dÃ nsÃ¬', 'trans': 'Seedance'}, {'word': 'é«˜æ€§èƒ½', 'pinyin': 'gÄo xÃ¬ngnÃ©ng', 'trans': 'high performance'}, {'word': 'è§†é¢‘ç”Ÿæˆæ¨¡å‹', 'pinyin': 'shÃ¬pÃ­n shÄ“ngchÃ©ng mÃ³xÃ­ng', 'trans': 'video generation model'}, {'word': 'ç»“åˆ', 'pinyin': 'jiÃ©hÃ©', 'trans': 'combine'}, {'word': 'å…ˆè¿›', 'pinyin': 'xiÄnjÃ¬n', 'trans': 'advanced'}, {'word': 'æ•°æ®æ•´ç†', 'pinyin': 'shÃ¹jÃ¹ zhÄ›nglÇ', 'trans': 'data processing'}, {'word': 'é«˜æ•ˆ', 'pinyin': 'gÄoxiÃ o', 'trans': 'efficient'}, {'word': 'æ¶æ„è®¾è®¡', 'pinyin': 'jiÃ gÃ²u shÃ¨jÃ¬', 'trans': 'architecture design'}, {'word': 'è®­ç»ƒåä¼˜åŒ–', 'pinyin': 'xÃ¹nliÃ n hÃ²u yÅuhuÃ ', 'trans': 'post-training optimization'}, {'word': 'æ¨¡å‹åŠ é€Ÿ', 'pinyin': 'mÃ³xÃ­ng jiÄsÃ¹', 'trans': 'model acceleration'}, {'word': '1080påˆ†è¾¨ç‡', 'pinyin': '1080p fÄ“nbiÄnlÇœ', 'trans': '1080p resolution'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ngchÃ©ng', 'trans': 'generate'}, {'word': 'åªéœ€', 'pinyin': 'zhÇ xÅ«', 'trans': 'only need'}, {'word': 'é¡¶å°–', 'pinyin': 'dÇngjiÄn', 'trans': 'top-notch'}, {'word': 'è´¨é‡', 'pinyin': 'zhÃ¬liÃ ng', 'trans': 'quality'}, {'word': 'é€Ÿåº¦', 'pinyin': 'sÃ¹dÃ¹', 'trans': 'speed'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇoxiÃ n', 'trans': 'performance'}, {'word': 'å‡ºè‰²', 'pinyin': 'chÅ«sÃ¨', 'trans': 'outstanding'}, {'word': 'ä¼˜è¶Š', 'pinyin': 'yÅuyuÃ¨', 'trans': 'superior'}, {'word': 'æ—¶ç©ºæµç•…æ€§', 'pinyin': 'shÃ­kÅng liÃºchÃ ngxÃ¬ng', 'trans': 'spatiotemporal smoothness'}, {'word': 'ç»“æ„ç¨³å®šæ€§', 'pinyin': 'jiÃ©gÃ²u wÄ›ndÃ¬ngxÃ¬ng', 'trans': 'structural stability'}]
[13.06.2025 04:21] Renaming previous Chinese page.
[13.06.2025 04:21] Renaming previous data. zh.html to ./d/2025-06-12_zh_reading_task.html
[13.06.2025 04:21] Writing Chinese reading task.
[13.06.2025 04:21] Writing result.
[13.06.2025 04:21] Renaming log file.
[13.06.2025 04:21] Renaming previous data. log.txt to ./logs/2025-06-13_last_log.txt
