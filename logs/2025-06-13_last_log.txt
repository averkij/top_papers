[13.06.2025 06:19] Read previous papers.
[13.06.2025 06:19] Generating top page (month).
[13.06.2025 06:19] Writing top page (month).
[13.06.2025 07:12] Read previous papers.
[13.06.2025 07:12] Get feed.
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09513
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09993
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10954
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10857
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10540
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10974
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10960
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10821
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10890
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10952
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10741
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09967
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09344
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08060
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10357
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09942
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10953
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10910
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06694
[13.06.2025 07:12] Extract page data from URL. URL: https://huggingface.co/papers/2506.10036
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08373
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06950
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06561
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05982
[13.06.2025 07:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10378
[13.06.2025 07:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.06.2025 07:12] No deleted papers detected.
[13.06.2025 07:12] Downloading and parsing papers (pdf, html). Total: 25.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.09513.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.09513.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.09513.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.09993.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.09993.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.09993.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.10954.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.10954.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.10954.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.10857.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.10857.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.10857.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.10540.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.10540.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.10540.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.10974.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.10974.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.10974.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.10960.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.10960.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.10960.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.10821.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.10821.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.10821.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.10890.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.10890.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.10890.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.10952.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.10952.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.10952.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.10741.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.10741.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.10741.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.09967.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.09967.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.09967.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.09344.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.09344.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.09344.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.08060.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.08060.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.08060.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.10357.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.10357.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.10357.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.09942.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.09942.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.09942.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.10953.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.10953.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.10953.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.10910.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.10910.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.10910.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.06694.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.06694.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.06694.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.10036.
[13.06.2025 07:12] Downloading paper 2506.10036 from http://arxiv.org/pdf/2506.10036v1...
[13.06.2025 07:12] Extracting affiliations from text.
[13.06.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Javad Rajabi1,2 Soroush Mehraban1,2,3 Seyedmorteza Sadat4 Babak Taati1,2,3 1University of Toronto 3KITE Research Institute 2Vector Institute for Artificial Intelligence 4ETH Z√ºrich 5 2 0 2 J 0 1 ] . [ 1 6 3 0 0 1 . 6 0 5 2 : r {rajabi, taati}@cs.toronto.edu soroush.mehraban@mail.utoronto.ca seyedmorteza.sadat@inf.ethz.ch "
[13.06.2025 07:12] Response: ```python
["University of Toronto", "KITE Research Institute", "Vector Institute for Artificial Intelligence", "ETH Z√ºrich"]
```
[13.06.2025 07:12] Deleting PDF ./assets/pdf/2506.10036.pdf.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.08373.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.08373.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.08373.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.06950.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.06950.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.06950.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.06561.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.06561.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.06561.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.05982.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.05982.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.05982.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Downloading and parsing paper https://huggingface.co/papers/2506.10378.
[13.06.2025 07:12] Extra JSON file exists (./assets/json/2506.10378.json), skip PDF parsing.
[13.06.2025 07:12] Paper image links file exists (./assets/img_data/2506.10378.json), skip HTML parsing.
[13.06.2025 07:12] Success.
[13.06.2025 07:12] Enriching papers with extra data.
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 0. ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.  					AI-generated summary 				 Though reasoning-based large language models (LLMs) hav...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 1. The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.  					AI-generated summary 				 Image restoration aims to recover degra...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 2. An automated pipeline, SWE-Factory, is introduced to facilitate the creation of large-scale datasets for evaluating and training Large Language Models in GitHub issue resolution tasks, offering efficient environment building, standardized grading, and automated validation.  					AI-generated summary...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 3. VRBench evaluates long video understanding by assessing multi-step reasoning capabilities across temporal and procedural validity using human-labeled question-answering pairs and reasoning chains.  					AI-generated summary 				 We present VRBench, the first long narrative video benchmark crafted fo...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 4. AniMaker, a multi-agent framework using MCTS-Gen and AniEval, generates coherent storytelling videos from text input, outperforming existing models with better quality and efficiency.  					AI-generated summary 				 Despite rapid advancements in video generation models, generating coherent storytell...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 5. AutoMind, a flexible and knowledgeable LLM-agent framework, improves automated data science through expert knowledge integration, strategic solution exploration, and adaptive coding, outperforming existing systems.  					AI-generated summary 				 Large Language Model (LLM) agents have shown great po...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 6. A benchmark for Chinese harmful content detection, coupled with a knowledge-augmented baseline, improves the performance of smaller models without extensive resources.  					AI-generated summary 				 Large language models (LLMs) have been increasingly applied to automated harmful content detection t...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 7. VideoDeepResearch, a text-only reasoning model with modular tools, surpasses existing baselines in long video understanding tasks without extending context windows or enhancing visual perception capabilities.  					AI-generated summary 				 Long video understanding (LVU) presents a significant chall...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 8. CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.  					AI-generated summary 				 Graphic design plays a crucial role in both commercial and personal contexts, yet creating high-quality, editable, and...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 9. Domain2Vec decomposes datasets into meta-domains to optimize language model pretraining and downstream performance with reduced computational cost.  					AI-generated summary 				 We introduce~Domain2Vec, a novel approach that decomposes any dataset into a linear combination of several meta-domains,...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 10. PosterCraft improves aesthetic poster generation through a unified, modular pipeline with enhanced text rendering, region-aware fine-tuning, aesthetic reinforcement learning, and joint vision-language refinement.  					AI-generated summary 				 Generating aesthetic posters is more challenging than s...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 11. SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.  					AI-generated summary 				 How cost-effectively can we elicit strong reasoning in language models by leveraging their und...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 12. Ming-Omni is a unified multimodal model with dedicated encoders and modality-specific routers that can process images, text, audio, and video, and performs tasks like speech and image generation, context-aware chatting, and versatile image editing.  					AI-generated summary 				 We propose Ming-Omn...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 13. Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.  					AI-generated summary 				 Large language models have transformed natural language processing, yet supervised...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 14. Optimus-3, an agent using knowledge-enhanced data generation, Mixture-of-Experts routing, and multimodal reasoning-augmented reinforcement learning, achieves superior performance across various tasks in Minecraft.  					AI-generated summary 				 Recently, agents based on multimodal large language mo...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 15. VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enha...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 16. A paradigm shift in web agent research is proposed, advocating for the development of Agentic Web Interfaces (AWIs) to optimize interaction for AI agents within web environments.  					AI-generated summary 				 Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spur...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 17. A scalable reinforcement learning pipeline for training reasoning models demonstrates improvements in multimodal understanding, instruction following, and function calling without relying on existing implementations.  					AI-generated summary 				 We introduce Magistral, Mistral's first reasoning m...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 18. MoveGCL is a privacy-preserving framework using generative continual learning and a Mixture-of-Experts Transformer for training mobility foundation models without sharing raw data.  					AI-generated summary 				 Foundation models have revolutionized fields such as natural language processing and co...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 19. Token Perturbation Guidance (TPG) enhances diffusion model generation quality without training, by perturbing intermediate token representations, achieving CFG-like performance and improving unconditional generation.  					AI-generated summary 				 Classifier-free guidance (CFG) has become an essent...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 20. A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.  					AI-generated summary 				 Optimizing inference for long-context Large Language M...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 21. A framework for evaluating and optimizing natural language prompts in large language models is proposed, revealing correlations between prompt properties and their impact on reasoning tasks.  					AI-generated summary 				 As large language models (LLMs) have progressed towards more human-like and h...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 22. LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.  					AI-generated summary 				 Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been de...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 23. MCA-Bench is a multimodal benchmark suite for CAPTCHA security evaluation which fine-tunes specialized cracking agents using a shared vision-language model.  					AI-generated summary 				 As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious ...
[13.06.2025 07:12] ********************************************************************************
[13.06.2025 07:12] Abstract 24. The study proposes a causal representation learning framework to evaluate language model capabilities through latent factors, emphasizing the importance of controlling for base model variations to uncover underlying causal relationships.  					AI-generated summary 				 Faithful evaluation of languag...
[13.06.2025 07:12] Read previous papers.
[13.06.2025 07:12] Generating reviews via LLM API.
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#benchmark", "#healthcare", "#reasoning", "#training"], "emoji": "ü©∫", "ru": {"title": "ReasonMed: –ü—Ä–æ—Ä—ã–≤ –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –ò–ò", "desc": "ReasonMed - —ç—Ç–æ –∫—Ä—É–ø–Ω–µ–π—à–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ 37
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#dataset", "#cv", "#benchmark", "#diffusion", "#hallucinations"], "emoji": "üìù", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —É—á–µ—Ç–æ–º —Ç–µ–∫—Å—Ç–∞ (TAIR) –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#data", "#science", "#dataset", "#agents", "#benchmark", "#open_source"], "emoji": "üè≠", "ru": {"title": "SWE-Factory: –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è LLM –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ü–û", "desc": "SWE-Factory - —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –æ–±—É—á
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#reasoning", "#video", "#multimodal"], "emoji": "üé¨", "ru": {"title": "VRBench: –û—Ü–µ–Ω–∫–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "VRBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#video", "#multimodal", "#story_generation", "#agents"], "emoji": "üé¨", "ru": {"title": "AniMaker: —É–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∏–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "AniMaker - —ç—Ç–æ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∏–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é.
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#science", "#training", "#dataset", "#benchmark", "#agents"], "emoji": "ü§ñ", "ru": {"title": "AutoMind: –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –Ω–∞—É–∫–∏ –æ –¥–∞–Ω–Ω—ã—Ö", "desc": "AutoMind - —ç—Ç–æ –Ω–æ–≤–∞—è –≥–∏–±–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–¥–∞—á –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#multilingual", "#small_models", "#low_resource", "#ethics", "#dataset", "#benchmark"], "emoji": "üá®üá≥", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ —Å –ø–æ–º–æ—â—å—é –∑–Ω–∞–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –∫–∏—Ç–∞
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#long_context", "#video", "#multimodal", "#agents"], "emoji": "üé•", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ", "desc": "VideoDeepResearch - —ç—Ç–æ –Ω–æ–≤–∞—è –∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#dataset", "#cv", "#benchmark", "#open_source", "#multimodal"], "emoji": "üé®", "ru": {"title": "CreatiPoster: –ò–ò-—Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–º –¥–∏–∑–∞–π–Ω–µ", "desc": "CreatiPoster - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∫–æ–º–ø–æ–∑–∏—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#transfer_learning", "#dataset", "#training", "#optimization", "#data"], "emoji": "üß©", "ru": {"title": "–£–º–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Domain2Vec - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –Ω–∞ –º–µ—Ç–∞-–¥–æ–º–µ–Ω—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#rl", "#architecture", "#open_source", "#dataset", "#data", "#multimodal"], "emoji": "üé®", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Å–æ–∑–¥–∞–µ—Ç —ç—Å—Ç–µ—Ç–∏—á–Ω—ã–µ –ø–æ—Å—Ç–µ—Ä—ã –Ω–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è", "desc": "PosterCraft - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç—Å—Ç–µ—Ç–∏—á–Ω—ã—Ö –ø–æ—Å—Ç
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#rl", "#training", "#open_source", "#optimization", "#small_models", "#reasoning"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ SAE-Tuning –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#audio", "#open_source", "#video", "#cv", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π: –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ –æ–¥–Ω–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Ming-Omni - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω—É—é –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–µ–∫—Å—Ç, 
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#rag", "#inference", "#training"], "emoji": "üß†", "ru": {"title": "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã: –æ–±—É—á–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –±–µ–∑ –∏–∑
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#agents", "#rag", "#rl", "#reasoning", "#multimodal", "#games"], "emoji": "ü§ñ", "ru": {"title": "Optimus-3: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ò–ò-–∞–≥–µ–Ω—Ç –ø–æ–∫–æ—Ä—è–µ—Ç Minecraft", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Optimus-3 - –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∏–≥—Ä—ã Minecraft, –∏—Å–ø–æ–ª—å–∑—É—é—â–µ–≥
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#rl", "#benchmark", "#open_source", "#reasoning", "#training", "#rlhf"], "emoji": "ü§ñ", "ru": {"title": "VerIF: –ì–∏–±—Ä–∏–¥–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è RL –≤ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VerIF - –≥–∏–±—Ä–∏–¥–Ω—ã–π –º–µ—Ç–æ–¥ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏, —Å–æ—á–µ—Ç–∞—é—â–∏–π 
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#agents", "#agi", "#optimization", "#multimodal"], "emoji": "üåê", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω—ã–µ –í–µ–±-–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ –ò–ò —Å –≤–µ–±-—Å—Ä–µ–¥–æ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –¥–ª—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤, –≤–≤–æ–¥—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –ê–≥–µ–Ω—Ç–Ω—ã—Ö –í–µ–±-–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ (AWI). AWI –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#rl", "#reasoning", "#open_source", "#multimodal"], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Magistral - –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –æ–±—É—á–µ–Ω–Ω—É—é —Å –ø–æ–º–æ—â—å—é –º–∞—Å—à—Ç–∞
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#dataset", "#synthetic", "#data", "#training", "#open_source", "#architecture"], "emoji": "üö∂", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –º–æ–±–∏–ª—å–Ω–æ—Å—Ç–∏", "desc": "MoveGCL - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–æ–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–∞
[13.06.2025 07:12] Querying the API.
[13.06.2025 07:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Token Perturbation Guidance (TPG) enhances diffusion model generation quality without training, by perturbing intermediate token representations, achieving CFG-like performance and improving unconditional generation.  					AI-generated summary 				 Classifier-free guidance (CFG) has become an essential component of modern diffusion models to enhance both generation quality and alignment with input conditions. However, CFG requires specific training procedures and is limited to conditional generation. To address these limitations, we propose Token Perturbation Guidance (TPG), a novel method that applies perturbation matrices directly to intermediate token representations within the diffusion network. TPG employs a norm-preserving shuffling operation to provide effective and stable guidance signals that improve generation quality without architectural changes. As a result, TPG is training-free and agnostic to input conditions, making it readily applicable to both conditional and unconditional generation. We further analyze the guidance term provided by TPG and show that its effect on sampling more closely resembles CFG compared to existing training-free guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1 show that TPG achieves nearly a 2times improvement in FID for unconditional generation over the SDXL baseline, while closely matching CFG in prompt alignment. These results establish TPG as a general, condition-agnostic guidance method that brings CFG-like benefits to a broader class of diffusion models. The code is available at https://github.com/TaatiTeam/Token-Perturbation-Guidance
[13.06.2025 07:12] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Token Perturbation Guidance (TPG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. TPG –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–∞—Ç—Ä–∏—Ü—ã –≤–æ–∑–º—É—â–µ–Ω–∏—è –∫ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º —Ç–æ–∫–µ–Ω–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º –≤ —Å–µ—Ç–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –∫–∞–∫ –¥–ª—è —É—Å–ª–æ–≤–Ω–æ–π, —Ç–∞–∫ –∏ –¥–ª—è –±–µ–∑—É—Å–ª–æ–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å Classifier-free guidance (CFG). –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—è—Ö SDXL –∏ Stable Diffusion 2.1 –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ FID –¥–ª—è –±–µ–∑—É—Å–ª–æ–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.",
  "emoji": "üîÄ",
  "title": "–£–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"
}
[13.06.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Token Perturbation Guidance (TPG) enhances diffusion model generation quality without training, by perturbing intermediate token representations, achieving CFG-like performance and improving unconditional generation.  					AI-generated summary 				 Classifier-free guidance (CFG) has become an essential component of modern diffusion models to enhance both generation quality and alignment with input conditions. However, CFG requires specific training procedures and is limited to conditional generation. To address these limitations, we propose Token Perturbation Guidance (TPG), a novel method that applies perturbation matrices directly to intermediate token representations within the diffusion network. TPG employs a norm-preserving shuffling operation to provide effective and stable guidance signals that improve generation quality without architectural changes. As a result, TPG is training-free and agnostic to input conditions, making it readily applicable to both conditional and unconditional generation. We further analyze the guidance term provided by TPG and show that its effect on sampling more closely resembles CFG compared to existing training-free guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1 show that TPG achieves nearly a 2times improvement in FID for unconditional generation over the SDXL baseline, while closely matching CFG in prompt alignment. These results establish TPG as a general, condition-agnostic guidance method that brings CFG-like benefits to a broader class of diffusion models. The code is available at https://github.com/TaatiTeam/Token-Perturbation-Guidance"

[13.06.2025 07:12] Response: ```python
["CV", "TRAINING"]
```
[13.06.2025 07:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Token Perturbation Guidance (TPG) enhances diffusion model generation quality without training, by perturbing intermediate token representations, achieving CFG-like performance and improving unconditional generation.  					AI-generated summary 				 Classifier-free guidance (CFG) has become an essential component of modern diffusion models to enhance both generation quality and alignment with input conditions. However, CFG requires specific training procedures and is limited to conditional generation. To address these limitations, we propose Token Perturbation Guidance (TPG), a novel method that applies perturbation matrices directly to intermediate token representations within the diffusion network. TPG employs a norm-preserving shuffling operation to provide effective and stable guidance signals that improve generation quality without architectural changes. As a result, TPG is training-free and agnostic to input conditions, making it readily applicable to both conditional and unconditional generation. We further analyze the guidance term provided by TPG and show that its effect on sampling more closely resembles CFG compared to existing training-free guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1 show that TPG achieves nearly a 2times improvement in FID for unconditional generation over the SDXL baseline, while closely matching CFG in prompt alignment. These results establish TPG as a general, condition-agnostic guidance method that brings CFG-like benefits to a broader class of diffusion models. The code is available at https://github.com/TaatiTeam/Token-Perturbation-Guidance"

[13.06.2025 07:12] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[13.06.2025 07:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Token Perturbation Guidance (TPG) is a new method that improves the quality of images generated by diffusion models without needing additional training. It works by applying perturbation matrices to the intermediate token representations, which helps guide the generation process effectively. Unlike Classifier-free Guidance (CFG), TPG does not require specific training and can be used for both conditional and unconditional generation tasks. Experiments show that TPG significantly enhances generation quality, achieving nearly double the improvement in FID scores compared to existing methods, while maintaining alignment with prompts.","title":"Enhancing Diffusion Models with Token Perturbation Guidance"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Token Perturbation Guidance (TPG) is a new method that improves the quality of images generated by diffusion models without needing additional training. It works by applying perturbation matrices to the intermediate token representations, which helps guide the generation process effectively. Unlike Classifier-free Guidance (CFG), TPG does not require specific training and can be used for both conditional and unconditional generation tasks. Experiments show that TPG significantly enhances generation quality, achieving nearly double the improvement in FID scores compared to existing methods, while maintaining alignment with prompts.', title='Enhancing Diffusion Models with Token Perturbation Guidance'))
[13.06.2025 07:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫‰ª§ÁâåÊâ∞Âä®ÂºïÂØºÔºàTPGÔºâÔºåÊó®Âú®ÊèêÈ´òÊâ©Êï£Ê®°ÂûãÁöÑÁîüÊàêË¥®ÈáèÔºåËÄåÊó†ÈúÄËøõË°åËÆ≠ÁªÉ„ÄÇTPGÈÄöËøáÂØπÊâ©Êï£ÁΩëÁªú‰∏≠Èó¥‰ª§ÁâåË°®Á§∫ÊñΩÂä†Êâ∞Âä®Áü©ÈòµÔºåÊèê‰æõÊúâÊïà‰∏îÁ®≥ÂÆöÁöÑÂºïÂØº‰ø°Âè∑Ôºå‰ªéËÄåÊîπÂñÑÁîüÊàêÊïàÊûú„ÄÇ‰∏é‰º†ÁªüÁöÑÊó†ÂàÜÁ±ªÂô®ÂºïÂØºÊñπÊ≥ïÁõ∏ÊØîÔºåTPGÂú®Êó†Êù°‰ª∂ÁîüÊàê‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Êé•ËøëÂàÜÁ±ªÂô®Êó†ÂÖ≥ÂºïÂØºÁöÑÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTPGÂú®ÁîüÊàêË¥®Èáè‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÔºå‰∏îÈÄÇÁî®‰∫éÊù°‰ª∂ÂíåÊó†Êù°‰ª∂ÁîüÊàê„ÄÇ","title":"‰ª§ÁâåÊâ∞Âä®ÂºïÂØºÔºöÊó†ËÆ≠ÁªÉÁöÑÁîüÊàêË¥®ÈáèÊèêÂçá"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫‰ª§ÁâåÊâ∞Âä®ÂºïÂØºÔºàTPGÔºâÔºåÊó®Âú®ÊèêÈ´òÊâ©Êï£Ê®°ÂûãÁöÑÁîüÊàêË¥®ÈáèÔºåËÄåÊó†ÈúÄËøõË°åËÆ≠ÁªÉ„ÄÇTPGÈÄöËøáÂØπÊâ©Êï£ÁΩëÁªú‰∏≠Èó¥‰ª§ÁâåË°®Á§∫ÊñΩÂä†Êâ∞Âä®Áü©ÈòµÔºåÊèê‰æõÊúâÊïà‰∏îÁ®≥ÂÆöÁöÑÂºïÂØº‰ø°Âè∑Ôºå‰ªéËÄåÊîπÂñÑÁîüÊàêÊïàÊûú„ÄÇ‰∏é‰º†ÁªüÁöÑÊó†ÂàÜÁ±ªÂô®ÂºïÂØºÊñπÊ≥ïÁõ∏ÊØîÔºåTPGÂú®Êó†Êù°‰ª∂ÁîüÊàê‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Êé•ËøëÂàÜÁ±ªÂô®Êó†ÂÖ≥ÂºïÂØºÁöÑÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTPGÂú®ÁîüÊàêË¥®Èáè‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÂü∫Á∫øÔºå‰∏îÈÄÇÁî®‰∫éÊù°‰ª∂ÂíåÊó†Êù°‰ª∂ÁîüÊàê„ÄÇ', title='‰ª§ÁâåÊâ∞Âä®ÂºïÂØºÔºöÊó†ËÆ≠ÁªÉÁöÑÁîüÊàêË¥®ÈáèÊèêÂçá'))
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#long_context", "#benchmark", "#architecture", "#training", "#inference"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤—ã–≤–æ–¥–∞ –ò–ò —Å –ø–æ–º–æ—â—å—é —É–º–Ω—ã—Ö —á–µ—Ä–Ω–æ–≤–∏–∫–æ–≤", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –∏—Å–ø
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#survey", "#reasoning", "#training"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#multimodal", "#interpretability", "#games"], "emoji": "üñºÔ∏è", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥", "desc": "LaMP-Cap –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å –∏—Å–ø–æ–ª—å–∑–æ
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#security", "#open_source", "#multimodal"], "emoji": "üîê", "ru": {"title": "–ï–¥–∏–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ CAPTCHA", "desc": "MCA-Bench - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ CAPTCHA, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π
[13.06.2025 07:12] Using data from previous issue: {"categories": ["#science", "#dataset", "#interpretability", "#benchmark", "#reasoning", "#math"], "emoji": "üß†", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π –≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∫–∞—É–∑–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö
[13.06.2025 07:12] Loading Chinese text from previous data.
[13.06.2025 07:12] Renaming data file.
[13.06.2025 07:12] Renaming previous data. hf_papers.json to ./d/2025-06-13.json
[13.06.2025 07:12] Saving new data file.
[13.06.2025 07:12] Generating page.
[13.06.2025 07:12] Renaming previous page.
[13.06.2025 07:12] Renaming previous data. index.html to ./d/2025-06-13.html
[13.06.2025 07:12] [Experimental] Generating Chinese page for reading.
[13.06.2025 07:12] Chinese vocab [{'word': 'Seedance', 'pinyin': 'Sƒ´d√†ns√¨', 'trans': 'Seedance'}, {'word': 'È´òÊÄßËÉΩ', 'pinyin': 'gƒÅo x√¨ngn√©ng', 'trans': 'high performance'}, {'word': 'ËßÜÈ¢ëÁîüÊàêÊ®°Âûã', 'pinyin': 'sh√¨p√≠n shƒìngch√©ng m√≥x√≠ng', 'trans': 'video generation model'}, {'word': 'ÁªìÂêà', 'pinyin': 'ji√©h√©', 'trans': 'combine'}, {'word': 'ÂÖàËøõ', 'pinyin': 'xiƒÅnj√¨n', 'trans': 'advanced'}, {'word': 'Êï∞ÊçÆÊï¥ÁêÜ', 'pinyin': 'sh√πj√π zhƒõngl«ê', 'trans': 'data processing'}, {'word': 'È´òÊïà', 'pinyin': 'gƒÅoxi√†o', 'trans': 'efficient'}, {'word': 'Êû∂ÊûÑËÆæËÆ°', 'pinyin': 'ji√†g√≤u sh√®j√¨', 'trans': 'architecture design'}, {'word': 'ËÆ≠ÁªÉÂêé‰ºòÂåñ', 'pinyin': 'x√πnli√†n h√≤u y≈çuhu√†', 'trans': 'post-training optimization'}, {'word': 'Ê®°ÂûãÂä†ÈÄü', 'pinyin': 'm√≥x√≠ng jiƒÅs√π', 'trans': 'model acceleration'}, {'word': '1080pÂàÜËæ®Áéá', 'pinyin': '1080p fƒìnbiƒÅnl«ú', 'trans': '1080p resolution'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'}, {'word': 'Âè™ÈúÄ', 'pinyin': 'zh«ê x≈´', 'trans': 'only need'}, {'word': 'È°∂Â∞ñ', 'pinyin': 'd«êngjiƒÅn', 'trans': 'top-notch'}, {'word': 'Ë¥®Èáè', 'pinyin': 'zh√¨li√†ng', 'trans': 'quality'}, {'word': 'ÈÄüÂ∫¶', 'pinyin': 's√πd√π', 'trans': 'speed'}, {'word': 'Ë°®Áé∞', 'pinyin': 'bi«éoxi√†n', 'trans': 'performance'}, {'word': 'Âá∫Ëâ≤', 'pinyin': 'ch≈´s√®', 'trans': 'outstanding'}, {'word': '‰ºòË∂ä', 'pinyin': 'y≈çuyu√®', 'trans': 'superior'}, {'word': 'Êó∂Á©∫ÊµÅÁïÖÊÄß', 'pinyin': 'sh√≠k≈çng li√∫ch√†ngx√¨ng', 'trans': 'spatiotemporal smoothness'}, {'word': 'ÁªìÊûÑÁ®≥ÂÆöÊÄß', 'pinyin': 'ji√©g√≤u wƒõnd√¨ngx√¨ng', 'trans': 'structural stability'}]
[13.06.2025 07:12] Renaming previous Chinese page.
[13.06.2025 07:12] Renaming previous data. zh.html to ./d/2025-06-12_zh_reading_task.html
[13.06.2025 07:12] Writing Chinese reading task.
[13.06.2025 07:12] Writing result.
[13.06.2025 07:12] Renaming log file.
[13.06.2025 07:12] Renaming previous data. log.txt to ./logs/2025-06-13_last_log.txt
