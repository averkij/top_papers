[13.06.2025 03:44] Read previous papers.
[13.06.2025 03:44] Generating top page (month).
[13.06.2025 03:44] Writing top page (month).
[13.06.2025 04:20] Read previous papers.
[13.06.2025 04:20] Get feed.
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09993
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09513
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10857
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10954
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10890
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10952
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10821
[13.06.2025 04:20] Extract page data from URL. URL: https://huggingface.co/papers/2506.09967
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08060
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09942
[13.06.2025 04:20] Extract page data from URL. URL: https://huggingface.co/papers/2506.10953
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10357
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09344
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.06561
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.05982
[13.06.2025 04:20] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08373
[13.06.2025 04:20] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[13.06.2025 04:20] No deleted papers detected.
[13.06.2025 04:20] Downloading and parsing papers (pdf, html). Total: 16.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.09993.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.09993.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.09993.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.09513.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.09513.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.09513.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.10857.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.10857.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.10857.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.10954.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.10954.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.10954.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.10890.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.10890.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.10890.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.10952.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.10952.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.10952.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.10821.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.10821.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.10821.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.09967.
[13.06.2025 04:20] Downloading paper 2506.09967 from http://arxiv.org/pdf/2506.09967v1...
[13.06.2025 04:20] Extracting affiliations from text.
[13.06.2025 04:20] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 7 6 9 9 0 . 6 0 5 2 : r Resa: Transparent Reasoning Models via SAEs Shangshang Wang, Julian Asilis, Ömer Faruk Akgül, Enes Burak Bilgin, Ollie Liu, Deqing Fu, and Willie Neiswanger How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, family of 1.5B reasoning models trained via novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from source model, and then uses the trained SAE to guide standard supervised fine-tuning process to elicit such abilities in target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterparts reasoning performance while reducing training costs by >2000x to roughly $1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around $1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced. Notion Blog: https://shangshangwang.notion.site/resa Code Repository: https://github.com/shangshang-wang/Resa Training Logs: https://wandb.ai/upup-ashton-wang-usc/Resa Model Weights & Checkpoints: https://huggingface.co/Resa-Yi 1. Introduction Reasoning language models have demonstrated increasing performance in do"
[13.06.2025 04:20] Response: ```python
[]
```
[13.06.2025 04:20] Extracting affiliations from text.
[13.06.2025 04:20] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 7 6 9 9 0 . 6 0 5 2 : r Resa: Transparent Reasoning Models via SAEs Shangshang Wang, Julian Asilis, Ömer Faruk Akgül, Enes Burak Bilgin, Ollie Liu, Deqing Fu, and Willie NeiswangerHow cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, family of 1.5B reasoning models trained via novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from source model, and then uses the trained SAE to guide standard supervised fine-tuning process to elicit such abilities in target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterparts reasoning performance while reducing training costs by >2000x to roughly $1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around $1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced. Notion Blog: https://shangshangwang.notion.site/resa Code Repository: https://github.com/shangshang-wang/Resa Training Logs: https://wandb.ai/upup-ashton-wang-usc/Resa Model Weights & Checkpoints: https://huggingface.co/Resa-Yi 1. Introduction Reasoning language models have demonstrated increasing performance in domains like math, coding, and science (Wang and Neiswanger, 2025, Xu et al., 2025). Despite the impressive reasoning performance elicited by reinforcement learning (RL) or supervised fine-tuning (SFT) (Chu et al., 2025), these methods often operate as black box. In other words, while they improve reasoning, how they alter the models internal representations to do so is largely opaque. Furthermore, RL-based workflows are notoriously resourceintensive, requiring vast computational power and long training time to converge. On the other hand, SFT hinges on the availability of high-quality Chain-of-Thought (CoT) reasoning traces, which are costly to curate (Muennighoff et al., 2025). This leaves critical gap in the field: The need for three-birds-one-stone method that can elicit strong reasoning abilities in way that is not only effective but also computationally efficient and transparent. In this paper, we bridge this gap with Resa, family of 1.5B reasoning models trained via sparse autoencoders (SAEs), using novel SAE-Tuning procedure. SAEs are unsupervised models designed to deconstruct models dense internal activations into sparse dictionary of more interpretable latent features (Anthropic, 2023, 2024). Our key insight is that within this dictionary, certain features must correspond to the fundamental building blocks of reasoning. By instilling latent reasoning features captured by an SAE back into model via tuning procedure, we can effectively and efficiently elicit the models reasoning abilities. Corresponding author(s): Shangshang Wang shangshangwang.github.io; Willie Neiswanger neiswang@usc.edu Resa: Transparent Reasoning Models via SAEs Figure 1: Comparison between Example Resa Models and Baselines The Tina models correspond to the best checkpoints in Wang et al. (2025a). Resa-STILL and Resa-DeepScaleR correspond to Resa-STILL-v5 and Resa-DeepScaleR-v3 in Table 4, respectively. For these Resa models, the required SAEs are trained from scratch (as shown in Section 3.2) and both computational and time costs are total costs for training SAEs and models. Reasoning performance denotes the average zero-shot Pass@1 score across AIME24/25, AMC23, MATH500, GPQA Diamond, and Minerva benchmarks. Specifically, SAE-Tuning involves two key stages: First, we use an SAE to probe the internal activations of source model, identifying and extracting dictionary of latent features that correspond to its reasoning processes. Second, we freeze this feature-rich SAE and insert it into target model to guide SFT process to elicit reasoning abilities in the target model. SAE-Tuning also distinguishes itself from existing methods by using SFT on minimal verified CoT-free question-answer data type. By verified, we mean that the answer correctness is ensured via methods like human annotation or language-model-based verification (Guha et al., 2025), while CoT-free signifies that our SAE-Tuning procedure functions without needing explicit step-by-step reasoning traces. Crucially, our control experiments in Table 4 demonstrate that performing standard SFT on this same CoT-free data without an SAE fails to elicit any meaningful reasoning, highlighting the vital role of the SAE. We summarize our core contributions as follows: Efficient Reasoning Ability Elicitation Purely using verified CoT-free data, we demonstrate that SAETuning can be applied in an end-to-end manner to certain base models with trained-from-scratch SAE to elicit reasoning abilities on par with those achieved via costly RL. This leads to substantial gains with peak training cost reductions of over 2000x (to approximately $1) and time reductions of over 450x (to under 20 minutes) compared to RL-based workflows, while maintaining comparable performance. Generalizable and Modular Reasoning Ability We establish the generality and modularity of the extracted reasoning abilities such that these abilities generalize across out-of-distribution datasets and can be attached to models within the same family at test time without additional training, functioning as portable reasoning adapter. Transparent Reasoning Feature Extraction We provide transparent view into the models reasoning abilities. Specifically, we propose novel prompt-only method to extract and quantify latent reasoning features using SAEs and show that the layer-wise distribution of these reasoning features correlates with reasoning performance of Resa models, offering data-driven path to optimizing SAE-Tuning. 2 Resa: Transpar"
[13.06.2025 04:20] Mistral response. {"id": "c5edecdb9ade4bcc9dd53b6cb65c48c0", "object": "chat.completion", "created": 1749788447, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1692, "total_tokens": 1700, "completion_tokens": 8}}
[13.06.2025 04:20] Response: ```python
[]
```
[13.06.2025 04:20] Deleting PDF ./assets/pdf/2506.09967.pdf.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.08060.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.08060.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.08060.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.09942.
[13.06.2025 04:20] Extra JSON file exists (./assets/json/2506.09942.json), skip PDF parsing.
[13.06.2025 04:20] Paper image links file exists (./assets/img_data/2506.09942.json), skip HTML parsing.
[13.06.2025 04:20] Success.
[13.06.2025 04:20] Downloading and parsing paper https://huggingface.co/papers/2506.10953.
[13.06.2025 04:21] Downloading paper 2506.10953 from http://arxiv.org/pdf/2506.10953v1...
[13.06.2025 04:21] Extracting affiliations from text.
[13.06.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 2 1 ] . [ 1 3 5 9 0 1 . 6 0 5 2 : r Build the web for agents, not agents for the web Xing Han Lù Gaurav Kamath Marius Mosbach Siva Reddy McGill University Mila Quebec AI Institute Equal Advising "
[13.06.2025 04:21] Response: ```python
["McGill University", "Mila", "Quebec AI Institute", "Equal Advising"]
```
[13.06.2025 04:21] Deleting PDF ./assets/pdf/2506.10953.pdf.
[13.06.2025 04:21] Success.
[13.06.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2506.10357.
[13.06.2025 04:21] Extra JSON file exists (./assets/json/2506.10357.json), skip PDF parsing.
[13.06.2025 04:21] Paper image links file exists (./assets/img_data/2506.10357.json), skip HTML parsing.
[13.06.2025 04:21] Success.
[13.06.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2506.09344.
[13.06.2025 04:21] Extra JSON file exists (./assets/json/2506.09344.json), skip PDF parsing.
[13.06.2025 04:21] Paper image links file exists (./assets/img_data/2506.09344.json), skip HTML parsing.
[13.06.2025 04:21] Success.
[13.06.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2506.06561.
[13.06.2025 04:21] Extra JSON file exists (./assets/json/2506.06561.json), skip PDF parsing.
[13.06.2025 04:21] Paper image links file exists (./assets/img_data/2506.06561.json), skip HTML parsing.
[13.06.2025 04:21] Success.
[13.06.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2506.05982.
[13.06.2025 04:21] Extra JSON file exists (./assets/json/2506.05982.json), skip PDF parsing.
[13.06.2025 04:21] Paper image links file exists (./assets/img_data/2506.05982.json), skip HTML parsing.
[13.06.2025 04:21] Success.
[13.06.2025 04:21] Downloading and parsing paper https://huggingface.co/papers/2506.08373.
[13.06.2025 04:21] Extra JSON file exists (./assets/json/2506.08373.json), skip PDF parsing.
[13.06.2025 04:21] Paper image links file exists (./assets/img_data/2506.08373.json), skip HTML parsing.
[13.06.2025 04:21] Success.
[13.06.2025 04:21] Enriching papers with extra data.
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 0. The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.  					AI-generated summary 				 Image restoration aims to recover degra...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 1. ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.  					AI-generated summary 				 Though reasoning-based large language models (LLMs) hav...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 2. VRBench evaluates long video understanding by assessing multi-step reasoning capabilities across temporal and procedural validity using human-labeled question-answering pairs and reasoning chains.  					AI-generated summary 				 We present VRBench, the first long narrative video benchmark crafted fo...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 3. An automated pipeline, SWE-Factory, is introduced to facilitate the creation of large-scale datasets for evaluating and training Large Language Models in GitHub issue resolution tasks, offering efficient environment building, standardized grading, and automated validation.  					AI-generated summary...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 4. CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.  					AI-generated summary 				 Graphic design plays a crucial role in both commercial and personal contexts, yet creating high-quality, editable, and...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 5. Domain2Vec decomposes datasets into meta-domains to optimize language model pretraining and downstream performance with reduced computational cost.  					AI-generated summary 				 We introduce~Domain2Vec, a novel approach that decomposes any dataset into a linear combination of several meta-domains,...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 6. VideoDeepResearch, a text-only reasoning model with modular tools, surpasses existing baselines in long video understanding tasks without extending context windows or enhancing visual perception capabilities.  					AI-generated summary 				 Long video understanding (LVU) presents a significant chall...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 7. SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.  					AI-generated summary 				 How cost-effectively can we elicit strong reasoning in language models by leveraging their und...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 8. Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.  					AI-generated summary 				 Large language models have transformed natural language processing, yet supervised...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 9. VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.  					AI-generated summary 				 Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enha...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 10. A paradigm shift in web agent research is proposed, advocating for the development of Agentic Web Interfaces (AWIs) to optimize interaction for AI agents within web environments.  					AI-generated summary 				 Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spur...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 11. Optimus-3, an agent using knowledge-enhanced data generation, Mixture-of-Experts routing, and multimodal reasoning-augmented reinforcement learning, achieves superior performance across various tasks in Minecraft.  					AI-generated summary 				 Recently, agents based on multimodal large language mo...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 12. Ming-Omni is a unified multimodal model with dedicated encoders and modality-specific routers that can process images, text, audio, and video, and performs tasks like speech and image generation, context-aware chatting, and versatile image editing.  					AI-generated summary 				 We propose Ming-Omn...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 13. LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.  					AI-generated summary 				 Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been de...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 14. MCA-Bench is a multimodal benchmark suite for CAPTCHA security evaluation which fine-tunes specialized cracking agents using a shared vision-language model.  					AI-generated summary 				 As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious ...
[13.06.2025 04:21] ********************************************************************************
[13.06.2025 04:21] Abstract 15. A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.  					AI-generated summary 				 Optimizing inference for long-context Large Language M...
[13.06.2025 04:21] Read previous papers.
[13.06.2025 04:21] Generating reviews via LLM API.
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#dataset", "#cv", "#benchmark", "#diffusion", "#hallucinations"], "emoji": "📝", "ru": {"title": "Восстановление изображений с сохранением текстовой информации", "desc": "Предложенная система восстановления изображений с учетом текста (TAIR) объединяет многозадачную диффузионную моде
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#benchmark", "#healthcare", "#reasoning", "#training"], "emoji": "🩺", "ru": {"title": "ReasonMed: Прорыв в медицинских вопросно-ответных системах на основе ИИ", "desc": "ReasonMed - это крупнейший набор данных для медицинских рассуждений, состоящий из 37
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#reasoning", "#video", "#multimodal"], "emoji": "🎬", "ru": {"title": "VRBench: Оценка глубокого понимания видео через многоступенчатые рассуждения", "desc": "VRBench - это новый бенчмарк для оценки способностей моделей машинного обучения к многоступенч
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#data", "#science", "#dataset", "#agents", "#benchmark", "#open_source"], "emoji": "🏭", "ru": {"title": "SWE-Factory: автоматизация создания датасетов для LLM в разработке ПО", "desc": "SWE-Factory - это автоматизированный конвейер для создания масштабных датасетов для оценки и обуч
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#dataset", "#cv", "#benchmark", "#open_source", "#multimodal"], "emoji": "🎨", "ru": {"title": "CreatiPoster: ИИ-революция в графическом дизайне", "desc": "CreatiPoster - это новая система искусственного интеллекта для генерации высококачественных графических композиций на основе тек
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#transfer_learning", "#dataset", "#training", "#optimization", "#data"], "emoji": "🧩", "ru": {"title": "Умное разложение данных для эффективного обучения языковых моделей", "desc": "Domain2Vec - это новый подход к декомпозиции датасетов на мета-домены для оптимизации предобучения яз
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#long_context", "#video", "#multimodal", "#agents"], "emoji": "🎥", "ru": {"title": "Агентный подход превосходит мультимодальные модели в понимании длинных видео", "desc": "VideoDeepResearch - это новая агентная система для понимания длинных видео, основан
[13.06.2025 04:21] Querying the API.
[13.06.2025 04:21] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.  					AI-generated summary 				 How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around 1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced.
[13.06.2025 04:21] Response: {
  "desc": "Эта статья представляет метод SAE-Tuning для эффективного улучшения способностей языковых моделей к рассуждению. Метод использует разреженные автоэнкодеры (SAE) для извлечения навыков рассуждения из исходной модели и применения их к целевой модели. SAE-Tuning позволяет достичь производительности, сравнимой с RL-обучением, при значительно меньших затратах времени и ресурсов. Исследование также показывает, что извлеченные навыки рассуждения обладают свойствами обобщаемости и модульности.",
  "emoji": "🧠",
  "title": "Эффективное обучение рассуждению с помощью разреженных автоэнкодеров"
}
[13.06.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.  					AI-generated summary 				 How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around 1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced."

[13.06.2025 04:21] Response: ```python
["TRAINING", "RL", "SMALL_MODELS"]
```
[13.06.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.  					AI-generated summary 				 How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around 1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced."

[13.06.2025 04:21] Response: ```python
["REASONING", "OPTIMIZATION", "OPEN_SOURCE"]
```
[13.06.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces SAE-Tuning, a method that enhances reasoning capabilities in language models using sparse autoencoders. This approach allows for significant performance improvements without the need for extensive retraining, achieving cost reductions of over 2000 times and time savings of over 450 times. By training a sparse autoencoder to capture reasoning skills from a source model, the method effectively guides the fine-tuning of a target model using verified question-answer data. The results show that the reasoning abilities gained are both generalizable across datasets and modular, allowing for easy integration into different models without retraining.","title":"Efficient Reasoning Enhancement in Language Models with SAE-Tuning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces SAE-Tuning, a method that enhances reasoning capabilities in language models using sparse autoencoders. This approach allows for significant performance improvements without the need for extensive retraining, achieving cost reductions of over 2000 times and time savings of over 450 times. By training a sparse autoencoder to capture reasoning skills from a source model, the method effectively guides the fine-tuning of a target model using verified question-answer data. The results show that the reasoning abilities gained are both generalizable across datasets and modular, allowing for easy integration into different models without retraining.', title='Efficient Reasoning Enhancement in Language Models with SAE-Tuning'))
[13.06.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SAE-Tuning是一种高效的稀疏自编码器调优方法，能够在语言模型中引发强大的推理能力，而无需进行大量的重新训练。该方法首先训练一个稀疏自编码器（SAE），以从源模型中捕捉推理能力，然后利用训练好的SAE指导标准的监督微调过程，从而在目标模型中引发这些能力。通过这种方式，SAE-Tuning在保持推理性能的同时，显著降低了训练成本和时间。研究表明，提取的推理能力具有可泛化和模块化的特性，可以在不同的数据集和模型之间灵活应用。","title":"高效推理：稀疏自编码器调优的力量"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SAE-Tuning是一种高效的稀疏自编码器调优方法，能够在语言模型中引发强大的推理能力，而无需进行大量的重新训练。该方法首先训练一个稀疏自编码器（SAE），以从源模型中捕捉推理能力，然后利用训练好的SAE指导标准的监督微调过程，从而在目标模型中引发这些能力。通过这种方式，SAE-Tuning在保持推理性能的同时，显著降低了训练成本和时间。研究表明，提取的推理能力具有可泛化和模块化的特性，可以在不同的数据集和模型之间灵活应用。', title='高效推理：稀疏自编码器调优的力量'))
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#optimization", "#transfer_learning", "#rag", "#inference", "#training"], "emoji": "🧠", "ru": {"title": "Трансформеры: обучение в контексте как альтернатива тонкой настройке", "desc": "Статья исследует способность трансформеров аппроксимировать возможности обучения с учителем без из
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#dataset", "#optimization", "#rl", "#benchmark", "#open_source", "#reasoning", "#training", "#rlhf"], "emoji": "🤖", "ru": {"title": "VerIF: Гибридная верификация для улучшения RL в следовании инструкциям", "desc": "Статья представляет VerIF - гибридный метод верификации, сочетающий 
[13.06.2025 04:21] Querying the API.
[13.06.2025 04:21] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A paradigm shift in web agent research is proposed, advocating for the development of Agentic Web Interfaces (AWIs) to optimize interaction for AI agents within web environments.  					AI-generated summary 				 Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community.
[13.06.2025 04:21] Response: {
  "desc": "Статья предлагает новую парадигму для веб-агентов, вводя концепцию Агентных Веб-Интерфейсов (AWI). AWI оптимизированы для взаимодействия ИИ-агентов с веб-средой, преодолевая ограничения интерфейсов, созданных для людей. Авторы устанавливают шесть принципов дизайна AWI, учитывающих безопасность, эффективность и стандартизацию. Этот подход нацелен на создание более надежных и прозрачных веб-агентов, что потребует совместных усилий ML-сообщества.",
  "emoji": "🌐",
  "title": "Агентные Веб-Интерфейсы: революция во взаимодействии ИИ с веб-средой"
}
[13.06.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A paradigm shift in web agent research is proposed, advocating for the development of Agentic Web Interfaces (AWIs) to optimize interaction for AI agents within web environments.  					AI-generated summary 				 Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community."

[13.06.2025 04:21] Response: ```python
['AGENTS', 'MULTIMODAL']
```
[13.06.2025 04:21] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A paradigm shift in web agent research is proposed, advocating for the development of Agentic Web Interfaces (AWIs) to optimize interaction for AI agents within web environments.  					AI-generated summary 				 Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community."

[13.06.2025 04:21] Response: ```python
["AGI", "OPTIMIZATION"]
```
[13.06.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper proposes a new approach to web agent research by introducing Agentic Web Interfaces (AWIs), which are specifically designed for AI agents to interact with web environments. Current web interfaces are not optimized for the capabilities of AI, leading to inefficiencies and challenges in task completion. The authors outline six guiding principles for designing AWIs, focusing on safety, efficiency, and standardization to benefit all stakeholders involved. This shift aims to enhance the performance and reliability of web agents, encouraging collaboration within the machine learning community.","title":"Redefining Web Interaction for AI Agents with AWIs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper proposes a new approach to web agent research by introducing Agentic Web Interfaces (AWIs), which are specifically designed for AI agents to interact with web environments. Current web interfaces are not optimized for the capabilities of AI, leading to inefficiencies and challenges in task completion. The authors outline six guiding principles for designing AWIs, focusing on safety, efficiency, and standardization to benefit all stakeholders involved. This shift aims to enhance the performance and reliability of web agents, encouraging collaboration within the machine learning community.', title='Redefining Web Interaction for AI Agents with AWIs'))
[13.06.2025 04:21] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了网络代理研究的范式转变，倡导开发代理网络接口（AWI），以优化人工智能代理在网络环境中的交互。随着大型语言模型（LLM）和多模态模型的进步，开发能够自主导航和完成任务的网络代理引起了广泛关注。当前的方法面临着人类设计的界面与LLM能力之间的根本不匹配问题，导致处理复杂网络输入时的困难。本文提出的AWI概念旨在为代理设计专门的交互界面，以提高安全性、效率和标准化，推动更高效、可靠和透明的网络代理设计。","title":"为代理设计优化网络交互界面"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了网络代理研究的范式转变，倡导开发代理网络接口（AWI），以优化人工智能代理在网络环境中的交互。随着大型语言模型（LLM）和多模态模型的进步，开发能够自主导航和完成任务的网络代理引起了广泛关注。当前的方法面临着人类设计的界面与LLM能力之间的根本不匹配问题，导致处理复杂网络输入时的困难。本文提出的AWI概念旨在为代理设计专门的交互界面，以提高安全性、效率和标准化，推动更高效、可靠和透明的网络代理设计。', title='为代理设计优化网络交互界面'))
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#agents", "#rag", "#rl", "#reasoning", "#multimodal", "#games"], "emoji": "🤖", "ru": {"title": "Optimus-3: Универсальный ИИ-агент покоряет Minecraft", "desc": "Статья представляет Optimus-3 - интеллектуального агента для игры Minecraft, использующег
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#audio", "#open_source", "#video", "#cv", "#multimodal"], "emoji": "🤖", "ru": {"title": "Единая модель для всех модальностей: восприятие и генерация в одном", "desc": "Статья представляет Ming-Omni - унифицированную мультимодальную модель, способную обрабатывать изображения, текст, 
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#multimodal", "#interpretability", "#games"], "emoji": "🖼️", "ru": {"title": "Персонализированные подписи к изображениям: мультимодальный подход", "desc": "LaMP-Cap представляет датасет для персонализированной генерации подписей к изображениям с использо
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#security", "#open_source", "#multimodal"], "emoji": "🔐", "ru": {"title": "Единый мультимодальный бенчмарк для оценки безопасности CAPTCHA", "desc": "MCA-Bench - это комплексный набор инструментов для оценки безопасности CAPTCHA, использующий мультимодальный
[13.06.2025 04:21] Using data from previous issue: {"categories": ["#optimization", "#long_context", "#benchmark", "#architecture", "#training", "#inference"], "emoji": "🚀", "ru": {"title": "Ускорение вывода ИИ с помощью умных черновиков", "desc": "Предложена новая система для приближенного вывода в больших языковых моделях с длинным контекстом, исп
[13.06.2025 04:21] Loading Chinese text from previous data.
[13.06.2025 04:21] Renaming data file.
[13.06.2025 04:21] Renaming previous data. hf_papers.json to ./d/2025-06-13.json
[13.06.2025 04:21] Saving new data file.
[13.06.2025 04:21] Generating page.
[13.06.2025 04:21] Renaming previous page.
[13.06.2025 04:21] Renaming previous data. index.html to ./d/2025-06-13.html
[13.06.2025 04:21] [Experimental] Generating Chinese page for reading.
[13.06.2025 04:21] Chinese vocab [{'word': 'Seedance', 'pinyin': 'Sīdànsì', 'trans': 'Seedance'}, {'word': '高性能', 'pinyin': 'gāo xìngnéng', 'trans': 'high performance'}, {'word': '视频生成模型', 'pinyin': 'shìpín shēngchéng móxíng', 'trans': 'video generation model'}, {'word': '结合', 'pinyin': 'jiéhé', 'trans': 'combine'}, {'word': '先进', 'pinyin': 'xiānjìn', 'trans': 'advanced'}, {'word': '数据整理', 'pinyin': 'shùjù zhěnglǐ', 'trans': 'data processing'}, {'word': '高效', 'pinyin': 'gāoxiào', 'trans': 'efficient'}, {'word': '架构设计', 'pinyin': 'jiàgòu shèjì', 'trans': 'architecture design'}, {'word': '训练后优化', 'pinyin': 'xùnliàn hòu yōuhuà', 'trans': 'post-training optimization'}, {'word': '模型加速', 'pinyin': 'móxíng jiāsù', 'trans': 'model acceleration'}, {'word': '1080p分辨率', 'pinyin': '1080p fēnbiānlǜ', 'trans': '1080p resolution'}, {'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'}, {'word': '只需', 'pinyin': 'zhǐ xū', 'trans': 'only need'}, {'word': '顶尖', 'pinyin': 'dǐngjiān', 'trans': 'top-notch'}, {'word': '质量', 'pinyin': 'zhìliàng', 'trans': 'quality'}, {'word': '速度', 'pinyin': 'sùdù', 'trans': 'speed'}, {'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chūsè', 'trans': 'outstanding'}, {'word': '优越', 'pinyin': 'yōuyuè', 'trans': 'superior'}, {'word': '时空流畅性', 'pinyin': 'shíkōng liúchàngxìng', 'trans': 'spatiotemporal smoothness'}, {'word': '结构稳定性', 'pinyin': 'jiégòu wěndìngxìng', 'trans': 'structural stability'}]
[13.06.2025 04:21] Renaming previous Chinese page.
[13.06.2025 04:21] Renaming previous data. zh.html to ./d/2025-06-12_zh_reading_task.html
[13.06.2025 04:21] Writing Chinese reading task.
[13.06.2025 04:21] Writing result.
[13.06.2025 04:21] Renaming log file.
[13.06.2025 04:21] Renaming previous data. log.txt to ./logs/2025-06-13_last_log.txt
