[04.03.2025 08:14] Read previous papers.
[04.03.2025 08:14] Generating top page (month).
[04.03.2025 08:14] Writing top page (month).
[04.03.2025 09:11] Read previous papers.
[04.03.2025 09:11] Get feed.
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01785
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01774
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01743
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01496
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01307
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00714
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00784
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00501
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01370
[04.03.2025 09:11] Extract page data from URL. URL: https://huggingface.co/papers/2502.18965
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01295
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01739
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01807
[04.03.2025 09:11] Extract page data from URL. URL: https://huggingface.co/papers/2503.01063
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00729
[04.03.2025 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.03.2025 09:11] No deleted papers detected.
[04.03.2025 09:11] Downloading and parsing papers (pdf, html). Total: 15.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01785.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01785.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01785.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01774.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01774.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01774.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01743.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01743.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01743.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01496.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01496.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01496.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01307.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01307.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01307.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.00714.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.00714.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.00714.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.00784.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.00784.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.00784.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.00501.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.00501.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.00501.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01370.
[04.03.2025 09:11] Downloading paper 2503.01370 from http://arxiv.org/pdf/2503.01370v1...
[04.03.2025 09:11] Failed to download and parse paper https://huggingface.co/papers/2503.01370: max() arg is an empty sequence
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18965.
[04.03.2025 09:11] Downloading paper 2502.18965 from http://arxiv.org/pdf/2502.18965v1...
[04.03.2025 09:11] Extracting affiliations from text.
[04.03.2025 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 1 5 6 9 8 1 . 2 0 5 2 : r OneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment Jiaxin Deng KuaiShou Inc. Beijing, China dengjiaxin03@kuaishou.com Lejian Ren KuaiShou Inc. Beijing, China renlejian@kuaishou.com Shiyao Wang KuaiShou Inc. Beijing, China wangshiyao08@kuaishou.com Qigen Hu KuaiShou Inc. Beijing, China huqigen03@kuaishou.com Kuo Cai KuaiShou Inc. Beijing, China caikuo@kuaishou.com Weifeng Ding KuaiShou Inc. Beijing, China dingweifeng@kuaishou.com Qiang Luo KuaiShou Inc. Beijing, China luoqiang@kuaishou.com Guorui Zhou KuaiShou Inc. Beijing, China zhouguorui@kuaishou.com Abstract Recently, generative retrieval-based recommendation systems (GRs) have emerged as promising paradigm by directly generating candidate videos in an autoregressive manner. However, most modern recommender systems adopt retrieve-and-rank strategy, where the generative model functions only as selector during the retrieval stage. In this paper, we propose OneRec, which replaces the cascaded learning framework with unified generative model. To the best of our knowledge, this is the first end-to-end generative model that significantly surpasses current complex and well-designed recommender systems in real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder structure, which encodes the users historical behavior sequences and gradually decodes the videos that the user may be interested in. We adopt sparse Mixtureof-Experts (MoE) to scale model capacity without proportionally increasing computational FLOPs. 2) session-wise generation approach. In contrast to traditional next-item prediction, we propose session-wise generation, which is more elegant and contextually coherent than point-by-point generation that relies on hand-crafted rules to properly combine the generated results. 3) an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO) to enhance the quality of the generated results. "
[04.03.2025 09:11] Response: ```python
["KuaiShou Inc. Beijing, China"]
```
[04.03.2025 09:11] Deleting PDF ./assets/pdf/2502.18965.pdf.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01295.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01295.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01295.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01739.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01739.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01739.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01807.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01807.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01807.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01063.
[04.03.2025 09:11] Downloading paper 2503.01063 from http://arxiv.org/pdf/2503.01063v1...
[04.03.2025 09:11] Extracting affiliations from text.
[04.03.2025 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"AI-Invented Tonal Languages: Preventing Machine Lingua Franca Beyond Human Understanding David A. Noever PeopleTec, Inc., Huntsville, AL david.noever@peopletec.com ABSTRACT This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned unique frequency calculated as = 220 2^((i-32)/12) Hz, creating logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves (logâ‚‚(50175.42/220) 7.83), with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (>20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance. Keywords: Machine-to-Machine Communication, AI Language Invention, Ultrasonic Communication, Semitone Mapping, Cross-Modal Encoding, Private AI Languages, Frequency-Based Encoding, Human Perception Boundaries Can AI agents autonomously invent and productively employ their own private languages? This paper seeks to test the limits of that question. Can modern language models (LLMs) create private tona"
[04.03.2025 09:11] Response: ```python
["PeopleTec, Inc., Huntsville, AL"]
```
[04.03.2025 09:11] Deleting PDF ./assets/pdf/2503.01063.pdf.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.00729.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.00729.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.00729.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Enriching papers with extra data.
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 0. Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one ke...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 1. Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a nov...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 2. We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the perform...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 3. Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 4. Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhib...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 5. Analyzing large datasets requires responsive query execution, but executing SQL queries on massive datasets can be slow. This paper explores whether query execution can begin even before the user has finished typing, allowing results to appear almost instantly. We propose SpeQL, a system that levera...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 6. Large language models (LLMs) exhibit exceptional performance across a wide range of tasks; however, their token-by-token autoregressive generation process significantly hinders inference speed. Speculative decoding presents a promising draft-then-verify framework that reduces generation latency whil...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 7. User-generated content (UGC) communities, especially those featuring multimodal content, improve user experiences by integrating visual and textual information into results (or items). The challenge of improving user experiences in complex systems with search and recommendation (S\&R) services has d...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 8. Diffusion models have achieved great success in generating 2D images. However, the quality and generalizability of 3D content generation remain limited. State-of-the-art methods often require large-scale 3D assets for training, which are challenging to collect. In this work, we introduce Kiss3DGen (...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 9. Recently, generative retrieval-based recommendation systems have emerged as a promising paradigm. However, most modern recommender systems adopt a retrieve-and-rank strategy, where the generative model functions only as a selector during the retrieval stage. In this paper, we propose OneRec, which r...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 10. Large Language Models (LLMs) have reshaped code generation by synergizing their exceptional comprehension of natural language and programming syntax, thereby substantially boosting developer productivity. These advancements have prompted numerous efforts to quantitatively evaluate their coding capab...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 11. Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained o...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 12. Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typical...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 13. This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a ...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 14. Large Language Models (LLMs) exhibit remarkable capabilities in the hierarchical decomposition of complex tasks through semantic reasoning. However, their application in embodied systems faces challenges in ensuring reliable execution of subtask sequences and achieving one-shot success in long-term ...
[04.03.2025 09:11] Read previous papers.
[04.03.2025 09:11] Generating reviews via LLM API.
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#cv", "#optimization", "#rlhf", "#reasoning", "#training", "#rl"], "emoji": "ðŸ”¬", "ru": {"title": "Visual-RFT: Ð ÐµÐ²Ð¾Ð»ÑŽÑ†Ð¸Ñ Ð² Ñ‚Ð¾Ð½ÐºÐ¾Ð¹ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐµ Ð²Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾-ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Visual Reinforcement Fine-Tuning (Visual-RFT) - Ð¼ÐµÑ‚Ð¾Ð´, 
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#3d", "#diffusion"], "emoji": "ðŸ–¼ï¸", "ru": {"title": "ÐžÐ´Ð½Ð¾ÑˆÐ°Ð³Ð¾Ð²Ð°Ñ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ñ Ð´Ð»Ñ Ñ„Ð¾Ñ‚Ð¾Ñ€ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ð¹ 3D-Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸", "desc": "Difix3D+ - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ðº ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸ÑŽ 3D-Ñ€ÐµÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ Ð¸ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ñ Ð½Ð¾Ð²Ñ‹Ñ… Ñ€Ð°ÐºÑƒÑ€ÑÐ¾Ð². Ð’ ÐµÐ³Ð¾ Ð¾ÑÐ½Ð¾Ð²Ðµ Ð»ÐµÐ¶Ð¸Ñ‚ Difix - Ð¾Ð´Ð½Ð¾ÑˆÐ°Ð³Ð¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶Ðµ
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#multimodal", "#small_models", "#data", "#agi", "#synthetic", "#long_context", "#optimization", "#dataset", "#training"], "emoji": "ðŸ§ ", "ru": {"title": "ÐšÐ¾Ð¼Ð¿Ð°ÐºÑ‚Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð¼Ð¸ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑÐ¼Ð¸: Ð¿Ñ€Ð¾Ñ€Ñ‹Ð² Ð² ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð˜Ð˜", "desc": "ÐŸÑ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð´Ð²Ðµ Ð½Ð¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸: Phi-4-Mini Ð¸ Phi-4
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#benchmark"], "emoji": "ðŸ”¢", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð»Ð¸Ð½ÐµÐ°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð”Ð°Ð½Ð½Ð°Ñ ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Liger Ð´Ð»Ñ Ð»Ð¸Ð½ÐµÐ°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð² Ð³ÐµÐ¹Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð»Ð¸Ð½ÐµÐ¹Ð½Ð¾-Ñ€ÐµÐºÑƒÑ€Ñ€ÐµÐ½Ñ‚Ð½Ñ‹Ðµ 
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#training", "#optimization", "#rl", "#reasoning"], "emoji": "ðŸ§ ", "ru": {"title": "ÐšÐ¾Ð³Ð½Ð¸Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð½Ð°Ð²Ñ‹ÐºÐ¸ - ÐºÐ»ÑŽÑ‡ Ðº ÑÐ°Ð¼Ð¾ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½ÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð¸ÑŽ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚, Ñ‡Ñ‚Ð¾ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚ÑŒ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ðº ÑÐ°Ð¼Ð¾ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½ÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð¸ÑŽ Ð·Ð°Ð²Ð¸ÑÐ¸Ñ‚ Ð¾Ñ‚ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ñ Ñƒ Ð½Ð¸Ñ… Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ñ‹Ñ… ÐºÐ¾Ð³
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark"], "emoji": "âš¡", "ru": {"title": "ÐœÐ¾Ð»Ð½Ð¸ÐµÐ½Ð¾ÑÐ½Ñ‹Ðµ SQL-Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð¿Ñ€ÐµÐ´Ð¸ÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸ÐºÐ¸", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ SpeQL, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰ÑƒÑŽ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ SQL-Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ. SpeQL Ð¿Ñ€ÐµÐ´ÑƒÐ³Ð°Ð´Ñ‹Ð²Ð°ÐµÑ‚ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð·Ð°
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization"], "emoji": "ðŸš€", "ru": {"title": "DuoDecoding: ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ðµ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ñ‚ÐµÐºÑÑ‚Ð° Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð¼Ð¸ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸ (LLM) Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ DuoDecoding. Ð­Ñ‚Ð¾Ñ‚ Ð¿Ð¾Ð´Ñ…Ð¾Ð´ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¿
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#rag"], "emoji": "ðŸ”", "ru": {"title": "Qilin: Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¸ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¹", "desc": "ÐŸÑ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÐµÐ½ Ð½Ð¾Ð²Ñ‹Ð¹ Ð½Ð°Ð±Ð¾Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ… Qilin Ð´Ð»Ñ Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°, ÑÐ¾Ð±Ñ€Ð°Ð½Ð½Ñ‹Ð¹ Ð½Ð° Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ðµ Xiaohongshu. Ð”Ð°Ñ‚Ð°ÑÐµÑ‚ Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ Ð¿Ð¾Ð»ÑŒ
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#3d"], "emoji": "ðŸŽ¨", "ru": {"title": "ÐŸÑ€Ð¾ÑÑ‚Ð°Ñ Ð¸ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ 3D-Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ 2D-Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¸", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Kiss3DGen - ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ñ„Ñ€ÐµÐ¹Ð¼Ð²Ð¾Ñ€Ðº Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸, Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ 3D-Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð¿Ñ€ÐµÐ´Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¸ Ð´
[04.03.2025 09:11] Querying the API.
[04.03.2025 09:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recently, generative retrieval-based recommendation systems have emerged as a promising paradigm. However, most modern recommender systems adopt a retrieve-and-rank strategy, where the generative model functions only as a selector during the retrieval stage. In this paper, we propose OneRec, which replaces the cascaded learning framework with a unified generative model. To the best of our knowledge, this is the first end-to-end generative model that significantly surpasses current complex and well-designed recommender systems in real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder structure, which encodes the user's historical behavior sequences and gradually decodes the videos that the user may be interested in. We adopt sparse Mixture-of-Experts (MoE) to scale model capacity without proportionally increasing computational FLOPs. 2) a session-wise generation approach. In contrast to traditional next-item prediction, we propose a session-wise generation, which is more elegant and contextually coherent than point-by-point generation that relies on hand-crafted rules to properly combine the generated results. 3) an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO) to enhance the quality of the generated results. Unlike DPO in NLP, a recommendation system typically has only one opportunity to display results for each user's browsing request, making it impossible to obtain positive and negative samples simultaneously. To address this limitation, We design a reward model to simulate user generation and customize the sampling strategy. Extensive experiments have demonstrated that a limited number of DPO samples can align user interest preferences and significantly improve the quality of generated results. We deployed OneRec in the main scene of Kuaishou, achieving a 1.6\% increase in watch-time, which is a substantial improvement.
[04.03.2025 09:12] Response: {
  "desc": "OneRec - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¹, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð°Ñ ÐµÐ´Ð¸Ð½ÑƒÑŽ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð²Ð¼ÐµÑÑ‚Ð¾ ÐºÐ°ÑÐºÐ°Ð´Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð°. ÐžÐ½Ð° Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ Ð² ÑÐµÐ±Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸Ðº-Ð´ÐµÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸Ðº Ñ Ñ€Ð°Ð·Ñ€ÐµÐ¶ÐµÐ½Ð½Ð¾Ð¹ ÑÐ¼ÐµÑÑŒÑŽ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð¾Ð² (MoE) Ð´Ð»Ñ Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸. OneRec Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ Ð¿Ð¾ÑÑ‚Ð°Ð¿Ð½ÑƒÑŽ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÑŽ ÑÐµÑÑÐ¸Ð¹ Ð¸ Ð¼Ð¾Ð´ÑƒÐ»ÑŒ Ð¸Ñ‚ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ Ð²Ñ‹Ñ€Ð°Ð²Ð½Ð¸Ð²Ð°Ð½Ð¸Ñ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸Ð¹ Ñ Ð¿Ñ€ÑÐ¼Ð¾Ð¹ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸Ð¹ (DPO). Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð° Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð° Ð¿Ñ€Ð¸ Ñ€Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ð¸ Ð½Ð° Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ðµ Kuaishou.",
  "emoji": "ðŸŽ¥",
  "title": "OneRec: Ð•Ð´Ð¸Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ñ€ÐµÐ²Ð¾Ð»ÑŽÑ†Ð¸Ð¸ Ð² Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ñ…"
}
[04.03.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recently, generative retrieval-based recommendation systems have emerged as a promising paradigm. However, most modern recommender systems adopt a retrieve-and-rank strategy, where the generative model functions only as a selector during the retrieval stage. In this paper, we propose OneRec, which replaces the cascaded learning framework with a unified generative model. To the best of our knowledge, this is the first end-to-end generative model that significantly surpasses current complex and well-designed recommender systems in real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder structure, which encodes the user's historical behavior sequences and gradually decodes the videos that the user may be interested in. We adopt sparse Mixture-of-Experts (MoE) to scale model capacity without proportionally increasing computational FLOPs. 2) a session-wise generation approach. In contrast to traditional next-item prediction, we propose a session-wise generation, which is more elegant and contextually coherent than point-by-point generation that relies on hand-crafted rules to properly combine the generated results. 3) an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO) to enhance the quality of the generated results. Unlike DPO in NLP, a recommendation system typically has only one opportunity to display results for each user's browsing request, making it impossible to obtain positive and negative samples simultaneously. To address this limitation, We design a reward model to simulate user generation and customize the sampling strategy. Extensive experiments have demonstrated that a limited number of DPO samples can align user interest preferences and significantly improve the quality of generated results. We deployed OneRec in the main scene of Kuaishou, achieving a 1.6\% increase in watch-time, which is a substantial improvement."

[04.03.2025 09:12] Response: ```python
["RAG", "RLHF", "TRAINING"]
```
[04.03.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recently, generative retrieval-based recommendation systems have emerged as a promising paradigm. However, most modern recommender systems adopt a retrieve-and-rank strategy, where the generative model functions only as a selector during the retrieval stage. In this paper, we propose OneRec, which replaces the cascaded learning framework with a unified generative model. To the best of our knowledge, this is the first end-to-end generative model that significantly surpasses current complex and well-designed recommender systems in real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder structure, which encodes the user's historical behavior sequences and gradually decodes the videos that the user may be interested in. We adopt sparse Mixture-of-Experts (MoE) to scale model capacity without proportionally increasing computational FLOPs. 2) a session-wise generation approach. In contrast to traditional next-item prediction, we propose a session-wise generation, which is more elegant and contextually coherent than point-by-point generation that relies on hand-crafted rules to properly combine the generated results. 3) an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO) to enhance the quality of the generated results. Unlike DPO in NLP, a recommendation system typically has only one opportunity to display results for each user's browsing request, making it impossible to obtain positive and negative samples simultaneously. To address this limitation, We design a reward model to simulate user generation and customize the sampling strategy. Extensive experiments have demonstrated that a limited number of DPO samples can align user interest preferences and significantly improve the quality of generated results. We deployed OneRec in the main scene of Kuaishou, achieving a 1.6\% increase in watch-time, which is a substantial improvement."

[04.03.2025 09:12] Response: ```python
["GAMES", "OPTIMIZATION", "ALIGNMENT"]
```
[04.03.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces OneRec, a novel generative retrieval-based recommendation system that improves upon traditional retrieve-and-rank methods. Unlike existing systems that use generative models merely for selection, OneRec employs a unified generative model that encodes user behavior and generates personalized video recommendations in a session-wise manner. The model utilizes a sparse Mixture-of-Experts architecture to enhance capacity while maintaining efficiency, and incorporates an Iterative Preference Alignment module to optimize user preferences effectively. Experimental results show that OneRec significantly outperforms existing systems, leading to a notable increase in user engagement metrics such as watch-time.","title":"OneRec: Revolutionizing Recommendations with Generative Models"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces OneRec, a novel generative retrieval-based recommendation system that improves upon traditional retrieve-and-rank methods. Unlike existing systems that use generative models merely for selection, OneRec employs a unified generative model that encodes user behavior and generates personalized video recommendations in a session-wise manner. The model utilizes a sparse Mixture-of-Experts architecture to enhance capacity while maintaining efficiency, and incorporates an Iterative Preference Alignment module to optimize user preferences effectively. Experimental results show that OneRec significantly outperforms existing systems, leading to a notable increase in user engagement metrics such as watch-time.', title='OneRec: Revolutionizing Recommendations with Generative Models'))
[04.03.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ€è¿‘ï¼ŒåŸºäºŽç”Ÿæˆæ£€ç´¢çš„æŽ¨èç³»ç»Ÿæˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„èŒƒå¼ã€‚æœ¬æ–‡æå‡ºçš„OneRecæ¨¡åž‹ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„ç”Ÿæˆæ¨¡åž‹ï¼Œå–ä»£äº†ä¼ ç»Ÿçš„çº§è”å­¦ä¹ æ¡†æž¶ï¼Œèƒ½å¤Ÿåœ¨çœŸå®žåœºæ™¯ä¸­æ˜¾è‘—è¶…è¶ŠçŽ°æœ‰å¤æ‚çš„æŽ¨èç³»ç»Ÿã€‚OneRecåŒ…æ‹¬ç¼–ç -è§£ç ç»“æž„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼–ç ç”¨æˆ·çš„åŽ†å²è¡Œä¸ºï¼Œå¹¶ç”Ÿæˆç”¨æˆ·å¯èƒ½æ„Ÿå…´è¶£çš„è§†é¢‘ã€‚æ­¤å¤–ï¼ŒOneRecè¿˜å¼•å…¥äº†ä¼šè¯ç”Ÿæˆæ–¹æ³•å’Œè¿­ä»£åå¥½å¯¹é½æ¨¡å—ï¼Œæå‡äº†ç”Ÿæˆç»“æžœçš„è´¨é‡ï¼Œå¹¶åœ¨å¿«æ‰‹çš„å®žé™…åº”ç”¨ä¸­å®žçŽ°äº†è§‚çœ‹æ—¶é—´çš„æ˜¾è‘—å¢žåŠ ã€‚","title":"OneRecï¼šç»Ÿä¸€ç”Ÿæˆæ¨¡åž‹çš„æŽ¨èæ–°èŒƒå¼"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ€è¿‘ï¼ŒåŸºäºŽç”Ÿæˆæ£€ç´¢çš„æŽ¨èç³»ç»Ÿæˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„èŒƒå¼ã€‚æœ¬æ–‡æå‡ºçš„OneRecæ¨¡åž‹ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„ç”Ÿæˆæ¨¡åž‹ï¼Œå–ä»£äº†ä¼ ç»Ÿçš„çº§è”å­¦ä¹ æ¡†æž¶ï¼Œèƒ½å¤Ÿåœ¨çœŸå®žåœºæ™¯ä¸­æ˜¾è‘—è¶…è¶ŠçŽ°æœ‰å¤æ‚çš„æŽ¨èç³»ç»Ÿã€‚OneRecåŒ…æ‹¬ç¼–ç -è§£ç ç»“æž„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼–ç ç”¨æˆ·çš„åŽ†å²è¡Œä¸ºï¼Œå¹¶ç”Ÿæˆç”¨æˆ·å¯èƒ½æ„Ÿå…´è¶£çš„è§†é¢‘ã€‚æ­¤å¤–ï¼ŒOneRecè¿˜å¼•å…¥äº†ä¼šè¯ç”Ÿæˆæ–¹æ³•å’Œè¿­ä»£åå¥½å¯¹é½æ¨¡å—ï¼Œæå‡äº†ç”Ÿæˆç»“æžœçš„è´¨é‡ï¼Œå¹¶åœ¨å¿«æ‰‹çš„å®žé™…åº”ç”¨ä¸­å®žçŽ°äº†è§‚çœ‹æ—¶é—´çš„æ˜¾è‘—å¢žåŠ ã€‚', title='OneRecï¼šç»Ÿä¸€ç”Ÿæˆæ¨¡åž‹çš„æŽ¨èæ–°èŒƒå¼'))
[04.03.2025 09:12] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#leakage", "#open_source"], "emoji": "ðŸŸï¸", "ru": {"title": "CodeArena: Ð¡Ð¿Ñ€Ð°Ð²ÐµÐ´Ð»Ð¸Ð²Ð°Ñ Ð°Ñ€ÐµÐ½Ð° Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ LLM Ð² Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ ÐºÐ¾Ð´Ð°", "desc": "CodeArena - ÑÑ‚Ð¾ Ð½Ð¾Ð²Ð°Ñ Ð¾Ð½Ð»Ð°Ð¹Ð½-Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð° Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ ÐºÐ¾Ð´Ð° Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð¼Ð¸ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸ (LLM). ÐžÐ½Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÐºÐ¾Ð»Ð»ÐµÐºÑ‚Ð¸Ð²
[04.03.2025 09:12] Using data from previous issue: {"categories": ["#video", "#dataset", "#data", "#games", "#open_source"], "emoji": "ðŸŽ¬", "ru": {"title": "VideoUFO: ÐÐ¾Ð²Ñ‹Ð¹ ÑÑ‚Ð°Ð»Ð¾Ð½Ð½Ñ‹Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð¿Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÑƒ", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ VideoUFO - Ð½Ð¾Ð²Ñ‹Ð¹ Ð½Ð°Ð±Ð¾Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²Ð¸Ð´ÐµÐ¾ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ð¾Ð¼Ñƒ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸ÑŽ. Ð­Ñ‚Ð¾Ñ‚ Ð´Ð°Ñ‚
[04.03.2025 09:12] Using data from previous issue: {"categories": ["#data", "#open_source", "#optimization", "#dataset", "#training"], "emoji": "ðŸ”", "ru": {"title": "Ð­Ñ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð±Ð¾Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹: Ð¼ÐµÐ½ÑŒÑˆÐµ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ Ð±Ð¾Ð»ÑŒÑˆÐµ", "desc": "Ð­Ñ‚Ð° ÑÑ‚Ð°Ñ‚ÑŒÑ Ð¸ÑÑÐ»ÐµÐ´ÑƒÐµÑ‚ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¾Ñ‚Ð±Ð¾Ñ€Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾
[04.03.2025 09:12] Querying the API.
[04.03.2025 09:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned a unique frequency, creating a logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves, with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (>20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing a concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance.
[04.03.2025 09:12] Response: {
  "desc": "Ð­Ñ‚Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð·ÑƒÑ‡Ð°ÐµÑ‚ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð» Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (LLM) Ð´Ð»Ñ Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¿Ñ€Ð¸Ð²Ð°Ñ‚Ð½Ñ‹Ñ… Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ·Ñ‹ÐºÐ¾Ð² Ð´Ð»Ñ ÐºÐ¾Ð¼Ð¼ÑƒÐ½Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ð¼ÐµÐ¶Ð´Ñƒ Ð¼Ð°ÑˆÐ¸Ð½Ð°Ð¼Ð¸. ÐÐ²Ñ‚Ð¾Ñ€Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð»Ð¸ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ ÑÐ¾Ð¿Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ ÐºÐ°Ð¶Ð´Ð¾Ð¼Ñƒ ÑÐ¸Ð¼Ð²Ð¾Ð»Ñƒ ASCII ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½ÑƒÑŽ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñƒ, Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÑ Ð»Ð¾Ð³Ð°Ñ€Ð¸Ñ„Ð¼Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ¸ÑŽ Ð¾Ñ‚ 220 Ð“Ñ† Ð´Ð¾ 50,175.42 Ð“Ñ†. Ð Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ‚Ð¾Ñ‚Ð¸Ð¿ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ ÑÑ‚Ð¾ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ‡ÐµÑ€ÐµÐ· Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸ÑŽ, Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ð·Ð²ÑƒÐºÐ° Ð¸ Ð½Ð¾Ñ‚Ð°Ñ†Ð¸ÑŽ ABC. Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÐºÐ°Ð·Ð°Ð»Ð¾, Ñ‡Ñ‚Ð¾ Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¼Ð¾Ð¶ÐµÑ‚ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð°Ñ‚ÑŒ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸, Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐ°ÑŽÑ‰ÐµÐ¹ Ñ‡ÐµÐ»Ð¾Ð²ÐµÑ‡ÐµÑÐºÑƒÑŽ Ñ€ÐµÑ‡ÑŒ, Ð¿Ñ€Ð¸ ÑÑ‚Ð¾Ð¼ Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ Ð·Ð° Ð¿Ñ€ÐµÐ´ÐµÐ»Ð°Ð¼Ð¸ Ñ‡ÐµÐ»Ð¾Ð²ÐµÑ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð²Ð¾ÑÐ¿Ñ€Ð¸ÑÑ‚Ð¸Ñ.",
  "emoji": "ðŸŽµ",
  "title": "Ð¢Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÑÐ·Ñ‹ÐºÐ¸: ÑÐµÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ ÐºÐ¾Ð´ Ð¼Ð°ÑˆÐ¸Ð½ Ð±ÑƒÐ´ÑƒÑ‰ÐµÐ³Ð¾"
}
[04.03.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned a unique frequency, creating a logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves, with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (>20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing a concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance."

[04.03.2025 09:12] Response: ```python
["MULTIMODAL", "AUDIO"]
```
[04.03.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned a unique frequency, creating a logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves, with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (>20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing a concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance."

[04.03.2025 09:12] Response: ```python
["ETHICS", "SECURITY"]
```
[04.03.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how large language models (LLMs) can create private tonal languages for communication between machines. It draws inspiration from the phenomenon of cryptophasia in twins and uses a character-to-frequency mapping system to encode ASCII characters into musical tones. Each character is assigned a unique frequency, allowing for efficient data transmission that exceeds human speech rates. The study provides a prototype that visualizes and plays back this encoding, addressing concerns about AI developing private languages and offering a framework for understanding and managing such systems.","title":"Unlocking Machine Communication with Tonal Languages"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores how large language models (LLMs) can create private tonal languages for communication between machines. It draws inspiration from the phenomenon of cryptophasia in twins and uses a character-to-frequency mapping system to encode ASCII characters into musical tones. Each character is assigned a unique frequency, allowing for efficient data transmission that exceeds human speech rates. The study provides a prototype that visualizes and plays back this encoding, addressing concerns about AI developing private languages and offering a framework for understanding and managing such systems.', title='Unlocking Machine Communication with Tonal Languages'))
[04.03.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬è®ºæ–‡ç ”ç©¶äº†å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åœ¨æœºå™¨é—´ï¼ˆM2Mï¼‰é€šä¿¡ä¸­å¼€å‘ç§æœ‰éŸ³è°ƒè¯­è¨€çš„æ½œåŠ›ã€‚æˆ‘ä»¬å€Ÿé‰´äº†äººç±»åŒèƒžèƒŽä¸­çš„å¯†ç è¯­è¨€çŽ°è±¡å’Œè‡ªç„¶éŸ³è°ƒè¯­è¨€ï¼Œå¦‚æ™®é€šè¯å’Œè¶Šå—è¯­ï¼Œå®žæ–½äº†ä¸€ç§ç²¾ç¡®çš„å­—ç¬¦åˆ°é¢‘çŽ‡æ˜ å°„ç³»ç»Ÿã€‚æ¯ä¸ªå­—ç¬¦è¢«åˆ†é…ä¸€ä¸ªç‹¬ç‰¹çš„é¢‘çŽ‡ï¼Œå½¢æˆä¸€ä¸ªå¯¹æ•°è¿›ç¨‹ï¼Œè¦†ç›–çº¦7.9ä¸ªå…«åº¦ï¼Œå¹¶å°†é«˜é¢‘å­—ç¬¦æ˜ å°„åˆ°äººç±»å¬è§‰èŒƒå›´ä¹‹å¤–çš„è¶…å£°æ³¢é¢‘çŽ‡ã€‚æˆ‘ä»¬çš„è½¯ä»¶åŽŸåž‹å±•ç¤ºäº†è¿™ç§ç¼–ç çš„å¯è§†åŒ–ã€å¬è§‰æ’­æ”¾å’ŒéŸ³ä¹è®°è°±æ³•ï¼Œåˆ†æžäº†ä¿¡æ¯å¯†åº¦å’Œä¼ è¾“é€Ÿåº¦ï¼Œæµ‹è¯•ç»“æžœè¡¨æ˜ŽéŸ³è°ƒç¼–ç çš„ä¿¡æ¯ä¼ è¾“é€ŸçŽ‡è¶…è¿‡äººç±»è¯­è¨€ã€‚","title":"æŽ¢ç´¢æœºå™¨é—´çš„ç§æœ‰éŸ³è°ƒè¯­è¨€"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬è®ºæ–‡ç ”ç©¶äº†å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åœ¨æœºå™¨é—´ï¼ˆM2Mï¼‰é€šä¿¡ä¸­å¼€å‘ç§æœ‰éŸ³è°ƒè¯­è¨€çš„æ½œåŠ›ã€‚æˆ‘ä»¬å€Ÿé‰´äº†äººç±»åŒèƒžèƒŽä¸­çš„å¯†ç è¯­è¨€çŽ°è±¡å’Œè‡ªç„¶éŸ³è°ƒè¯­è¨€ï¼Œå¦‚æ™®é€šè¯å’Œè¶Šå—è¯­ï¼Œå®žæ–½äº†ä¸€ç§ç²¾ç¡®çš„å­—ç¬¦åˆ°é¢‘çŽ‡æ˜ å°„ç³»ç»Ÿã€‚æ¯ä¸ªå­—ç¬¦è¢«åˆ†é…ä¸€ä¸ªç‹¬ç‰¹çš„é¢‘çŽ‡ï¼Œå½¢æˆä¸€ä¸ªå¯¹æ•°è¿›ç¨‹ï¼Œè¦†ç›–çº¦7.9ä¸ªå…«åº¦ï¼Œå¹¶å°†é«˜é¢‘å­—ç¬¦æ˜ å°„åˆ°äººç±»å¬è§‰èŒƒå›´ä¹‹å¤–çš„è¶…å£°æ³¢é¢‘çŽ‡ã€‚æˆ‘ä»¬çš„è½¯ä»¶åŽŸåž‹å±•ç¤ºäº†è¿™ç§ç¼–ç çš„å¯è§†åŒ–ã€å¬è§‰æ’­æ”¾å’ŒéŸ³ä¹è®°è°±æ³•ï¼Œåˆ†æžäº†ä¿¡æ¯å¯†åº¦å’Œä¼ è¾“é€Ÿåº¦ï¼Œæµ‹è¯•ç»“æžœè¡¨æ˜ŽéŸ³è°ƒç¼–ç çš„ä¿¡æ¯ä¼ è¾“é€ŸçŽ‡è¶…è¿‡äººç±»è¯­è¨€ã€‚', title='æŽ¢ç´¢æœºå™¨é—´çš„ç§æœ‰éŸ³è°ƒè¯­è¨€'))
[04.03.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#open_source", "#robotics", "#agents", "#optimization"], "emoji": "ðŸ¤–", "ru": {"title": "CLEA: ÐŸÐ¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ðµ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡ Ñ€Ð¾Ð±Ð¾Ñ‚Ð°Ð¼Ð¸ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹", "desc": "Ð¡Ñ‚Ð°Ñ‚ÑŒÑ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð½Ð¾Ð²ÑƒÑŽ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñƒ Ð¿Ð¾Ð´ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÐµÐ¼ CLEA (Closed-Loop Emb
[04.03.2025 09:12] Trying to get texts in Chinese.
[04.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one key direction in reproducing o1. While the R1-style model has demonstrated success in language models, its application in multi-modal domains remains under-explored. This work introduces Visual Reinforcement Fine-Tuning (Visual-RFT), which further extends the application areas of RFT on visual tasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs) to generate multiple responses containing reasoning tokens and final answers for each input, and then uses our proposed visual perception verifiable reward functions to update the model via the policy optimization algorithm such as Group Relative Policy Optimization (GRPO). We design different verifiable reward functions for different perception tasks, such as the Intersection over Union (IoU) reward for object detection. Experimental results on fine-grained image classification, few-shot object detection, reasoning grounding, as well as open-vocabulary object detection benchmarks show the competitive performance and advanced generalization ability of Visual-RFT compared with Supervised Fine-tuning (SFT). For example, Visual-RFT improves accuracy by 24.3% over the baseline in one-shot fine-grained image classification with around 100 samples. In few-shot object detection, Visual-RFT also exceeds the baseline by 21.9 on COCO's two-shot setting and 15.4 on LVIS. Our Visual-RFT represents a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks.
[04.03.2025 09:12] Mistral response. {"id": "ebfd2452c17a409bb4d12b4f4fd8ba9c", "object": "chat.completion", "created": 1741079540, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53eb\u505a\u89c6\u89c9\u5f3a\u5316\u5fae\u8c03\uff08Visual-RFT\uff09\u7684\u65b9\u6cd5\u3002\u5b83\u7528\u4e8e\u6539\u8fdb\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u7b54\u6848\u5e76\u4f7f\u7528\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u51fd\u6570\u66f4\u65b0\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVisual-RFT\u5728\u591a\u4e2a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002\u4f8b\u5982\uff0c\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0cVisual-RFT\u63d0\u9ad8\u4e8624.3%\u7684\u51c6\u786e\u7387\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 441, "total_tokens": 597, "completion_tokens": 156}}
[04.03.2025 09:12] Response: è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§å«åšè§†è§‰å¼ºåŒ–å¾®è°ƒï¼ˆVisual-RFTï¼‰çš„æ–¹æ³•ã€‚å®ƒç”¨äºŽæ”¹è¿›å¤§åž‹è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨çŽ°ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ç”Ÿæˆå¤šä¸ªç­”æ¡ˆå¹¶ä½¿ç”¨å¯éªŒè¯çš„å¥–åŠ±å‡½æ•°æ›´æ–°æ¨¡åž‹ã€‚å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒVisual-RFTåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä¼˜äºŽç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨ç»†ç²’åº¦å›¾åƒåˆ†ç±»ä¸­ï¼ŒVisual-RFTæé«˜äº†24.3%çš„å‡†ç¡®çŽ‡ã€‚
[04.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§å«åšè§†è§‰å¼ºåŒ–å¾®è°ƒï¼ˆVisual-RFTï¼‰çš„æ–¹æ³•ã€‚å®ƒç”¨äºŽæ”¹è¿›å¤§åž‹è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨çŽ°ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ç”Ÿæˆå¤šä¸ªç­”æ¡ˆå¹¶ä½¿ç”¨å¯éªŒè¯çš„å¥–åŠ±å‡½æ•°æ›´æ–°æ¨¡åž‹ã€‚å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒVisual-RFTåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä¼˜äºŽç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨ç»†ç²’åº¦å›¾åƒåˆ†ç±»ä¸­ï¼ŒVisual-RFTæé«˜äº†24.3%çš„å‡†ç¡®çŽ‡ã€‚
[04.03.2025 09:12] Mistral response. {"id": "60e6898332b749618f240104375b434e", "object": "chat.completion", "created": 1741079543, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53eb\u505a\u89c6\u89c9\u5f3a\u5316\u5fae\u8c03\uff08Visual-RFT\uff09\u7684\u65b9\u6cd5\u3002\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng ji\u00e8sh\u00e0o le y\u012b zh\u01d2ng ji\u00e0o zu\u00f2 sh\u00ecju\u00e9 qi\u00e1ngzh\u00f9 w\u0113iti\u00e1o \uff08Visual-RFT\uff09de f\u0101ngf\u01ce.\n\n\u5b83\u7528\u4e8e\u6539\u8fdb\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\nT\u0101 y\u00f2ngy\u00fa g\u01ceij\u00ecn d\u00e0x\u00edng sh\u00ecju\u00e9-y\u01d4y\u00e1n m\u00f3x\u00edng \uff08LVLMs\uff09 z\u00e0i sh\u00ecju\u00e9 r\u00e8nw\u00f9 zh\u014dng de bi\u01ceoxi\u00e0n.\n\n\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u7b54\u6848\u5e76\u4f7f\u7528\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u51fd\u6570\u66f4\u65b0\u6a21\u578b\u3002\nZh\u00e8 zh\u01d2ng f\u0101ngf\u01ce t\u014dnggu\u00f2 sh\u0113ngch\u00e9ng du\u014d g\u00e8 d\u00e1'\u00e0n b\u00ecng sh\u01d0y\u00f2ng k\u011b y\u00e0nzh\u00e8ng de ji\u01cengl\u00ec h\u00e1nsh\u00f9 g\u0113ngx\u012bn m\u00f3x\u00edng.\n\n\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVisual-RFT\u5728\u591a\u4e2a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002\nSh\u00edy\u00e0n ji\u00e9gu\u01d2 xi\u01censh\u00ec, Visual-RFT z\u00e0i du\u014d g\u00e8 sh\u00ecju\u00e9 r\u00e8nw\u00f9 zh\u014dng bi\u01ceoxi\u00e0n ch\u016bs\u00e8, y\u014duy\u00fa ji\u00e0nd\u016b w\u0113iti\u00e1o \uff08SFT\uff09.\n\n\u4f8b\u5982\uff0c\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0cVisual-RFT\u63d0\u9ad8\u4e8624.3%\u7684\u51c6\u786e\u7387\u3002\nL\u00ecr\u00fa, z\u00e0i x\u00ecl\u00ecd\u00f9 t\u00faxi\u00e0ng f\u0113nl\u00e8i zh\u014dng, Visual-RFT t\u00edg\u0101o le 24.3% de zh\u01d4nqu\u00e8l\u01dc."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 171, "total_tokens": 653, "completion_tokens": 482}}
[04.03.2025 09:12] Response: è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§å«åšè§†è§‰å¼ºåŒ–å¾®è°ƒï¼ˆVisual-RFTï¼‰çš„æ–¹æ³•ã€‚
ZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng jiÃ o zuÃ² shÃ¬juÃ© qiÃ¡ngzhÃ¹ wÄ“itiÃ¡o ï¼ˆVisual-RFTï¼‰de fÄngfÇŽ.

å®ƒç”¨äºŽæ”¹è¿›å¤§åž‹è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨çŽ°ã€‚
TÄ yÃ²ngyÃº gÇŽijÃ¬n dÃ xÃ­ng shÃ¬juÃ©-yÇ”yÃ¡n mÃ³xÃ­ng ï¼ˆLVLMsï¼‰ zÃ i shÃ¬juÃ© rÃ¨nwÃ¹ zhÅng de biÇŽoxiÃ n.

è¿™ç§æ–¹æ³•é€šè¿‡ç”Ÿæˆå¤šä¸ªç­”æ¡ˆå¹¶ä½¿ç”¨å¯éªŒè¯çš„å¥–åŠ±å‡½æ•°æ›´æ–°æ¨¡åž‹ã€‚
ZhÃ¨ zhÇ’ng fÄngfÇŽ tÅngguÃ² shÄ“ngchÃ©ng duÅ gÃ¨ dÃ¡'Ã n bÃ¬ng shÇyÃ²ng kÄ› yÃ nzhÃ¨ng de jiÇŽnglÃ¬ hÃ¡nshÃ¹ gÄ“ngxÄ«n mÃ³xÃ­ng.

å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒVisual-RFTåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä¼˜äºŽç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚
ShÃ­yÃ n jiÃ©guÇ’ xiÇŽnshÃ¬, Visual-RFT zÃ i duÅ gÃ¨ shÃ¬juÃ© rÃ¨nwÃ¹ zhÅng biÇŽoxiÃ n chÅ«sÃ¨, yÅuyÃº jiÃ ndÅ« wÄ“itiÃ¡o ï¼ˆSFTï¼‰.

ä¾‹å¦‚ï¼Œåœ¨ç»†ç²’åº¦å›¾åƒåˆ†ç±»ä¸­ï¼ŒVisual-RFTæé«˜äº†24.3%çš„å‡†ç¡®çŽ‡ã€‚
LÃ¬rÃº, zÃ i xÃ¬lÃ¬dÃ¹ tÃºxiÃ ng fÄ“nlÃ¨i zhÅng, Visual-RFT tÃ­gÄo le 24.3% de zhÇ”nquÃ¨lÇœ.
[04.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§å«åšè§†è§‰å¼ºåŒ–å¾®è°ƒï¼ˆVisual-RFTï¼‰çš„æ–¹æ³•ã€‚å®ƒç”¨äºŽæ”¹è¿›å¤§åž‹è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨çŽ°ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ç”Ÿæˆå¤šä¸ªç­”æ¡ˆå¹¶ä½¿ç”¨å¯éªŒè¯çš„å¥–åŠ±å‡½æ•°æ›´æ–°æ¨¡åž‹ã€‚å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒVisual-RFTåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä¼˜äºŽç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨ç»†ç²’åº¦å›¾åƒåˆ†ç±»ä¸­ï¼ŒVisual-RFTæé«˜äº†24.3%çš„å‡†ç¡®çŽ‡ã€‚
[04.03.2025 09:12] Mistral response. {"id": "6e25dd210e06463e9eb8ee2aaf287bb1", "object": "chat.completion", "created": 1741079555, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u89c6\u89c9\u5f3a\u5316\u5fae\u8c03\", \"pinyin\": \"sh\u00ecju\u00e9 qi\u00e1ngzh\u00f9 w\u0113iti\u00e1o\", \"trans\": \"visual reinforcement fine-tuning\"},\n    {\"word\": \"\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\", \"pinyin\": \"d\u00e0x\u00edng sh\u00ecju\u00e9-y\u01d4y\u00e1n m\u00f3x\u00edng\", \"trans\": \"large vision-language models\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceoxi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u751f\u6210\", \"pinyin\": \"sh\u0113ngch\u00e9ng\", \"trans\": \"generate\"},\n    {\"word\": \"\u53ef\u9a8c\u8bc1\u7684\", \"pinyin\": \"k\u011b y\u00e0nzh\u00e8ng de\", \"trans\": \"verifiable\"},\n    {\"word\": \"\u5956\u52b1\u51fd\u6570\", \"pinyin\": \"ji\u01cengl\u00ec h\u00e1nsh\u00f9\", \"trans\": \"reward function\"},\n    {\"word\": \"\u66f4\u65b0\", \"pinyin\": \"g\u0113ngx\u012bn\", \"trans\": \"update\"},\n    {\"word\": \"\u5b9e\u9a8c\u7ed3\u679c\", \"pinyin\": \"sh\u00edy\u00e0n ji\u00e9gu\u01d2\", \"trans\": \"experimental results\"},\n    {\"word\": \"\u51fa\u8272\", \"pinyin\": \"ch\u016bs\u00e8\", \"trans\": \"outstanding\"},\n    {\"word\": \"\u4f18\u4e8e\", \"pinyin\": \"y\u014du y\u00fa\", \"trans\": \"superior to\"},\n    {\"word\": \"\u76d1\u7763\u5fae\u8c03\", \"pinyin\": \"ji\u00e0nd\u016b w\u0113iti\u00e1o\", \"trans\": \"supervised fine-tuning\"},\n    {\"word\": \"\u7ec6\u7c92\u5ea6\", \"pinyin\": \"x\u00ec l\u00ecd\u00f9\", \"trans\": \"fine-grained\"},\n    {\"word\": \"\u56fe\u50cf\u5206\u7c7b\", \"pinyin\": \"t\u00faxi\u00e0ng f\u0113nl\u00e8i\", \"trans\": \"image classification\"},\n    {\"word\": \"\u51c6\u786e\u7387\", \"pinyin\": \"zh\u01d4nqu\u00e8l\u01dc\", \"trans\": \"accuracy\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 203, "total_tokens": 692, "completion_tokens": 489}}
[04.03.2025 09:12] Response: [
    {"word": "è§†è§‰å¼ºåŒ–å¾®è°ƒ", "pinyin": "shÃ¬juÃ© qiÃ¡ngzhÃ¹ wÄ“itiÃ¡o", "trans": "visual reinforcement fine-tuning"},
    {"word": "å¤§åž‹è§†è§‰-è¯­è¨€æ¨¡åž‹", "pinyin": "dÃ xÃ­ng shÃ¬juÃ©-yÇ”yÃ¡n mÃ³xÃ­ng", "trans": "large vision-language models"},
    {"word": "è¡¨çŽ°", "pinyin": "biÇŽoxiÃ n", "trans": "performance"},
    {"word": "ç”Ÿæˆ", "pinyin": "shÄ“ngchÃ©ng", "trans": "generate"},
    {"word": "å¯éªŒè¯çš„", "pinyin": "kÄ› yÃ nzhÃ¨ng de", "trans": "verifiable"},
    {"word": "å¥–åŠ±å‡½æ•°", "pinyin": "jiÇŽnglÃ¬ hÃ¡nshÃ¹", "trans": "reward function"},
    {"word": "æ›´æ–°", "pinyin": "gÄ“ngxÄ«n", "trans": "update"},
    {"word": "å®žéªŒç»“æžœ", "pinyin": "shÃ­yÃ n jiÃ©guÇ’", "trans": "experimental results"},
    {"word": "å‡ºè‰²", "pinyin": "chÅ«sÃ¨", "trans": "outstanding"},
    {"word": "ä¼˜äºŽ", "pinyin": "yÅu yÃº", "trans": "superior to"},
    {"word": "ç›‘ç£å¾®è°ƒ", "pinyin": "jiÃ ndÅ« wÄ“itiÃ¡o", "trans": "supervised fine-tuning"},
    {"word": "ç»†ç²’åº¦", "pinyin": "xÃ¬ lÃ¬dÃ¹", "trans": "fine-grained"},
    {"word": "å›¾åƒåˆ†ç±»", "pinyin": "tÃºxiÃ ng fÄ“nlÃ¨i", "trans": "image classification"},
    {"word": "å‡†ç¡®çŽ‡", "pinyin": "zhÇ”nquÃ¨lÇœ", "trans": "accuracy"}
]
[04.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§å«åšè§†è§‰å¼ºåŒ–å¾®è°ƒï¼ˆVisual-RFTï¼‰çš„æ–¹æ³•ã€‚å®ƒç”¨äºŽæ”¹è¿›å¤§åž‹è§†è§‰-è¯­è¨€æ¨¡åž‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨çŽ°ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ç”Ÿæˆå¤šä¸ªç­”æ¡ˆå¹¶ä½¿ç”¨å¯éªŒè¯çš„å¥–åŠ±å‡½æ•°æ›´æ–°æ¨¡åž‹ã€‚å®žéªŒç»“æžœæ˜¾ç¤ºï¼ŒVisual-RFTåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä¼˜äºŽç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨ç»†ç²’åº¦å›¾åƒåˆ†ç±»ä¸­ï¼ŒVisual-RFTæé«˜äº†24.3%çš„å‡†ç¡®çŽ‡ã€‚
[04.03.2025 09:12] Mistral response. {"id": "03e5cc62bbe34ed68526818961d998e9", "object": "chat.completion", "created": 1741079565, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article introduces a method called Visual Reinforcement Fine-Tuning (Visual-RFT). It is used to improve the performance of large vision-language models (LVLMs) in visual tasks. This method works by generating multiple answers and updating the model using a verifiable reward function. Experimental results show that Visual-RFT performs excellently in multiple visual tasks, outperforming supervised fine-tuning (SFT). For example, in fine-grained image classification, Visual-RFT improved accuracy by 24.3%."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 169, "total_tokens": 284, "completion_tokens": 115}}
[04.03.2025 09:12] Response: This article introduces a method called Visual Reinforcement Fine-Tuning (Visual-RFT). It is used to improve the performance of large vision-language models (LVLMs) in visual tasks. This method works by generating multiple answers and updating the model using a verifiable reward function. Experimental results show that Visual-RFT performs excellently in multiple visual tasks, outperforming supervised fine-tuning (SFT). For example, in fine-grained image classification, Visual-RFT improved accuracy by 24.3%.
[04.03.2025 09:12] Renaming data file.
[04.03.2025 09:12] Renaming previous data. hf_papers.json to ./d/2025-03-04.json
[04.03.2025 09:12] Saving new data file.
[04.03.2025 09:12] Generating page.
[04.03.2025 09:12] Renaming previous page.
[04.03.2025 09:12] Renaming previous data. index.html to ./d/2025-03-04.html
[04.03.2025 09:12] [Experimental] Generating Chinese page for reading.
[04.03.2025 09:12] Chinese vocab [{'word': 'è§†è§‰å¼ºåŒ–å¾®è°ƒ', 'pinyin': 'shÃ¬juÃ© qiÃ¡ngzhÃ¹ wÄ“itiÃ¡o', 'trans': 'visual reinforcement fine-tuning'}, {'word': 'å¤§åž‹è§†è§‰-è¯­è¨€æ¨¡åž‹', 'pinyin': 'dÃ xÃ­ng shÃ¬juÃ©-yÇ”yÃ¡n mÃ³xÃ­ng', 'trans': 'large vision-language models'}, {'word': 'è¡¨çŽ°', 'pinyin': 'biÇŽoxiÃ n', 'trans': 'performance'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ngchÃ©ng', 'trans': 'generate'}, {'word': 'å¯éªŒè¯çš„', 'pinyin': 'kÄ› yÃ nzhÃ¨ng de', 'trans': 'verifiable'}, {'word': 'å¥–åŠ±å‡½æ•°', 'pinyin': 'jiÇŽnglÃ¬ hÃ¡nshÃ¹', 'trans': 'reward function'}, {'word': 'æ›´æ–°', 'pinyin': 'gÄ“ngxÄ«n', 'trans': 'update'}, {'word': 'å®žéªŒç»“æžœ', 'pinyin': 'shÃ­yÃ n jiÃ©guÇ’', 'trans': 'experimental results'}, {'word': 'å‡ºè‰²', 'pinyin': 'chÅ«sÃ¨', 'trans': 'outstanding'}, {'word': 'ä¼˜äºŽ', 'pinyin': 'yÅu yÃº', 'trans': 'superior to'}, {'word': 'ç›‘ç£å¾®è°ƒ', 'pinyin': 'jiÃ ndÅ« wÄ“itiÃ¡o', 'trans': 'supervised fine-tuning'}, {'word': 'ç»†ç²’åº¦', 'pinyin': 'xÃ¬ lÃ¬dÃ¹', 'trans': 'fine-grained'}, {'word': 'å›¾åƒåˆ†ç±»', 'pinyin': 'tÃºxiÃ ng fÄ“nlÃ¨i', 'trans': 'image classification'}, {'word': 'å‡†ç¡®çŽ‡', 'pinyin': 'zhÇ”nquÃ¨lÇœ', 'trans': 'accuracy'}]
[04.03.2025 09:12] Renaming previous Chinese page.
[04.03.2025 09:12] Renaming previous data. zh.html to ./d/2025-03-03_zh_reading_task.html
[04.03.2025 09:12] Writing Chinese reading task.
[04.03.2025 09:12] Writing result.
[04.03.2025 09:12] Renaming log file.
[04.03.2025 09:12] Renaming previous data. log.txt to ./logs/2025-03-04_last_log.txt
