[04.03.2025 08:14] Read previous papers.
[04.03.2025 08:14] Generating top page (month).
[04.03.2025 08:14] Writing top page (month).
[04.03.2025 09:11] Read previous papers.
[04.03.2025 09:11] Get feed.
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01785
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01774
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01743
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01496
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01307
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00714
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00784
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00501
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01370
[04.03.2025 09:11] Extract page data from URL. URL: https://huggingface.co/papers/2502.18965
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01295
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01739
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.01807
[04.03.2025 09:11] Extract page data from URL. URL: https://huggingface.co/papers/2503.01063
[04.03.2025 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2503.00729
[04.03.2025 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[04.03.2025 09:11] No deleted papers detected.
[04.03.2025 09:11] Downloading and parsing papers (pdf, html). Total: 15.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01785.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01785.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01785.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01774.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01774.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01774.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01743.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01743.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01743.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01496.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01496.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01496.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01307.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01307.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01307.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.00714.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.00714.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.00714.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.00784.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.00784.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.00784.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.00501.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.00501.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.00501.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01370.
[04.03.2025 09:11] Downloading paper 2503.01370 from http://arxiv.org/pdf/2503.01370v1...
[04.03.2025 09:11] Failed to download and parse paper https://huggingface.co/papers/2503.01370: max() arg is an empty sequence
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2502.18965.
[04.03.2025 09:11] Downloading paper 2502.18965 from http://arxiv.org/pdf/2502.18965v1...
[04.03.2025 09:11] Extracting affiliations from text.
[04.03.2025 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 1 5 6 9 8 1 . 2 0 5 2 : r OneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment Jiaxin Deng KuaiShou Inc. Beijing, China dengjiaxin03@kuaishou.com Lejian Ren KuaiShou Inc. Beijing, China renlejian@kuaishou.com Shiyao Wang KuaiShou Inc. Beijing, China wangshiyao08@kuaishou.com Qigen Hu KuaiShou Inc. Beijing, China huqigen03@kuaishou.com Kuo Cai KuaiShou Inc. Beijing, China caikuo@kuaishou.com Weifeng Ding KuaiShou Inc. Beijing, China dingweifeng@kuaishou.com Qiang Luo KuaiShou Inc. Beijing, China luoqiang@kuaishou.com Guorui Zhou KuaiShou Inc. Beijing, China zhouguorui@kuaishou.com Abstract Recently, generative retrieval-based recommendation systems (GRs) have emerged as promising paradigm by directly generating candidate videos in an autoregressive manner. However, most modern recommender systems adopt retrieve-and-rank strategy, where the generative model functions only as selector during the retrieval stage. In this paper, we propose OneRec, which replaces the cascaded learning framework with unified generative model. To the best of our knowledge, this is the first end-to-end generative model that significantly surpasses current complex and well-designed recommender systems in real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder structure, which encodes the users historical behavior sequences and gradually decodes the videos that the user may be interested in. We adopt sparse Mixtureof-Experts (MoE) to scale model capacity without proportionally increasing computational FLOPs. 2) session-wise generation approach. In contrast to traditional next-item prediction, we propose session-wise generation, which is more elegant and contextually coherent than point-by-point generation that relies on hand-crafted rules to properly combine the generated results. 3) an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO) to enhance the quality of the generated results. "
[04.03.2025 09:11] Response: ```python
["KuaiShou Inc. Beijing, China"]
```
[04.03.2025 09:11] Deleting PDF ./assets/pdf/2502.18965.pdf.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01295.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01295.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01295.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01739.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01739.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01739.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01807.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.01807.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.01807.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.01063.
[04.03.2025 09:11] Downloading paper 2503.01063 from http://arxiv.org/pdf/2503.01063v1...
[04.03.2025 09:11] Extracting affiliations from text.
[04.03.2025 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"AI-Invented Tonal Languages: Preventing Machine Lingua Franca Beyond Human Understanding David A. Noever PeopleTec, Inc., Huntsville, AL david.noever@peopletec.com ABSTRACT This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned unique frequency calculated as = 220 2^((i-32)/12) Hz, creating logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves (log₂(50175.42/220) 7.83), with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (>20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance. Keywords: Machine-to-Machine Communication, AI Language Invention, Ultrasonic Communication, Semitone Mapping, Cross-Modal Encoding, Private AI Languages, Frequency-Based Encoding, Human Perception Boundaries Can AI agents autonomously invent and productively employ their own private languages? This paper seeks to test the limits of that question. Can modern language models (LLMs) create private tona"
[04.03.2025 09:11] Response: ```python
["PeopleTec, Inc., Huntsville, AL"]
```
[04.03.2025 09:11] Deleting PDF ./assets/pdf/2503.01063.pdf.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Downloading and parsing paper https://huggingface.co/papers/2503.00729.
[04.03.2025 09:11] Extra JSON file exists (./assets/json/2503.00729.json), skip PDF parsing.
[04.03.2025 09:11] Paper image links file exists (./assets/img_data/2503.00729.json), skip HTML parsing.
[04.03.2025 09:11] Success.
[04.03.2025 09:11] Enriching papers with extra data.
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 0. Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one ke...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 1. Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a nov...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 2. We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the perform...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 3. Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 4. Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhib...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 5. Analyzing large datasets requires responsive query execution, but executing SQL queries on massive datasets can be slow. This paper explores whether query execution can begin even before the user has finished typing, allowing results to appear almost instantly. We propose SpeQL, a system that levera...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 6. Large language models (LLMs) exhibit exceptional performance across a wide range of tasks; however, their token-by-token autoregressive generation process significantly hinders inference speed. Speculative decoding presents a promising draft-then-verify framework that reduces generation latency whil...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 7. User-generated content (UGC) communities, especially those featuring multimodal content, improve user experiences by integrating visual and textual information into results (or items). The challenge of improving user experiences in complex systems with search and recommendation (S\&R) services has d...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 8. Diffusion models have achieved great success in generating 2D images. However, the quality and generalizability of 3D content generation remain limited. State-of-the-art methods often require large-scale 3D assets for training, which are challenging to collect. In this work, we introduce Kiss3DGen (...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 9. Recently, generative retrieval-based recommendation systems have emerged as a promising paradigm. However, most modern recommender systems adopt a retrieve-and-rank strategy, where the generative model functions only as a selector during the retrieval stage. In this paper, we propose OneRec, which r...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 10. Large Language Models (LLMs) have reshaped code generation by synergizing their exceptional comprehension of natural language and programming syntax, thereby substantially boosting developer productivity. These advancements have prompted numerous efforts to quantitatively evaluate their coding capab...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 11. Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained o...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 12. Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typical...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 13. This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a ...
[04.03.2025 09:11] ********************************************************************************
[04.03.2025 09:11] Abstract 14. Large Language Models (LLMs) exhibit remarkable capabilities in the hierarchical decomposition of complex tasks through semantic reasoning. However, their application in embodied systems faces challenges in ensuring reliable execution of subtask sequences and achieving one-shot success in long-term ...
[04.03.2025 09:11] Read previous papers.
[04.03.2025 09:11] Generating reviews via LLM API.
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#cv", "#optimization", "#rlhf", "#reasoning", "#training", "#rl"], "emoji": "🔬", "ru": {"title": "Visual-RFT: Революция в тонкой настройке визуально-языковых моделей", "desc": "Статья представляет Visual Reinforcement Fine-Tuning (Visual-RFT) - метод, 
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#3d", "#diffusion"], "emoji": "🖼️", "ru": {"title": "Одношаговая диффузия для фотореалистичной 3D-реконструкции", "desc": "Difix3D+ - это новый подход к улучшению 3D-реконструкции и синтеза изображений с новых ракурсов. В его основе лежит Difix - одношаговая модель диффузии изображе
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#multimodal", "#small_models", "#data", "#agi", "#synthetic", "#long_context", "#optimization", "#dataset", "#training"], "emoji": "🧠", "ru": {"title": "Компактные модели с большими возможностями: прорыв в эффективности ИИ", "desc": "Представлены две новые модели: Phi-4-Mini и Phi-4
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#benchmark"], "emoji": "🔢", "ru": {"title": "Эффективная линеаризация больших языковых моделей", "desc": "Данная статья представляет новый метод Liger для линеаризации больших языковых моделей (LLM) в гейтированные линейно-рекуррентные 
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#training", "#optimization", "#rl", "#reasoning"], "emoji": "🧠", "ru": {"title": "Когнитивные навыки - ключ к самосовершенствованию языковых моделей", "desc": "Исследование показывает, что способность языковых моделей к самосовершенствованию зависит от наличия у них определенных ког
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark"], "emoji": "⚡", "ru": {"title": "Молниеносные SQL-запросы с помощью предиктивной аналитики", "desc": "Статья представляет систему SpeQL, использующую большие языковые модели для предсказания SQL-запросов пользователя. SpeQL предугадывает структуру за
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization"], "emoji": "🚀", "ru": {"title": "DuoDecoding: Параллельное ускорение языковых моделей", "desc": "Статья представляет новый метод ускорения генерации текста большими языковыми моделями (LLM) под названием DuoDecoding. Этот подход использует п
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#rag"], "emoji": "🔍", "ru": {"title": "Qilin: мультимодальный датасет для улучшения поиска и рекомендаций", "desc": "Представлен новый набор данных Qilin для мультимодального информационного поиска, собранный на платформе Xiaohongshu. Датасет включает поль
[04.03.2025 09:11] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#3d"], "emoji": "🎨", "ru": {"title": "Простая и эффективная 3D-генерация на основе 2D-диффузии", "desc": "Статья представляет Kiss3DGen - эффективный фреймворк для генерации, редактирования и улучшения 3D-объектов с использованием предобученной модели диффузии д
[04.03.2025 09:11] Querying the API.
[04.03.2025 09:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recently, generative retrieval-based recommendation systems have emerged as a promising paradigm. However, most modern recommender systems adopt a retrieve-and-rank strategy, where the generative model functions only as a selector during the retrieval stage. In this paper, we propose OneRec, which replaces the cascaded learning framework with a unified generative model. To the best of our knowledge, this is the first end-to-end generative model that significantly surpasses current complex and well-designed recommender systems in real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder structure, which encodes the user's historical behavior sequences and gradually decodes the videos that the user may be interested in. We adopt sparse Mixture-of-Experts (MoE) to scale model capacity without proportionally increasing computational FLOPs. 2) a session-wise generation approach. In contrast to traditional next-item prediction, we propose a session-wise generation, which is more elegant and contextually coherent than point-by-point generation that relies on hand-crafted rules to properly combine the generated results. 3) an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO) to enhance the quality of the generated results. Unlike DPO in NLP, a recommendation system typically has only one opportunity to display results for each user's browsing request, making it impossible to obtain positive and negative samples simultaneously. To address this limitation, We design a reward model to simulate user generation and customize the sampling strategy. Extensive experiments have demonstrated that a limited number of DPO samples can align user interest preferences and significantly improve the quality of generated results. We deployed OneRec in the main scene of Kuaishou, achieving a 1.6\% increase in watch-time, which is a substantial improvement.
[04.03.2025 09:12] Response: {
  "desc": "OneRec - это новая система рекомендаций, использующая единую генеративную модель вместо каскадного подхода. Она включает в себя структуру кодировщик-декодировщик с разреженной смесью экспертов (MoE) для масштабирования возможностей модели. OneRec применяет поэтапную генерацию сессий и модуль итеративного выравнивания предпочтений с прямой оптимизацией предпочтений (DPO). Система показала значительное улучшение времени просмотра при развертывании на платформе Kuaishou.",
  "emoji": "🎥",
  "title": "OneRec: Единая генеративная модель для революции в рекомендательных системах"
}
[04.03.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recently, generative retrieval-based recommendation systems have emerged as a promising paradigm. However, most modern recommender systems adopt a retrieve-and-rank strategy, where the generative model functions only as a selector during the retrieval stage. In this paper, we propose OneRec, which replaces the cascaded learning framework with a unified generative model. To the best of our knowledge, this is the first end-to-end generative model that significantly surpasses current complex and well-designed recommender systems in real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder structure, which encodes the user's historical behavior sequences and gradually decodes the videos that the user may be interested in. We adopt sparse Mixture-of-Experts (MoE) to scale model capacity without proportionally increasing computational FLOPs. 2) a session-wise generation approach. In contrast to traditional next-item prediction, we propose a session-wise generation, which is more elegant and contextually coherent than point-by-point generation that relies on hand-crafted rules to properly combine the generated results. 3) an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO) to enhance the quality of the generated results. Unlike DPO in NLP, a recommendation system typically has only one opportunity to display results for each user's browsing request, making it impossible to obtain positive and negative samples simultaneously. To address this limitation, We design a reward model to simulate user generation and customize the sampling strategy. Extensive experiments have demonstrated that a limited number of DPO samples can align user interest preferences and significantly improve the quality of generated results. We deployed OneRec in the main scene of Kuaishou, achieving a 1.6\% increase in watch-time, which is a substantial improvement."

[04.03.2025 09:12] Response: ```python
["RAG", "RLHF", "TRAINING"]
```
[04.03.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recently, generative retrieval-based recommendation systems have emerged as a promising paradigm. However, most modern recommender systems adopt a retrieve-and-rank strategy, where the generative model functions only as a selector during the retrieval stage. In this paper, we propose OneRec, which replaces the cascaded learning framework with a unified generative model. To the best of our knowledge, this is the first end-to-end generative model that significantly surpasses current complex and well-designed recommender systems in real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder structure, which encodes the user's historical behavior sequences and gradually decodes the videos that the user may be interested in. We adopt sparse Mixture-of-Experts (MoE) to scale model capacity without proportionally increasing computational FLOPs. 2) a session-wise generation approach. In contrast to traditional next-item prediction, we propose a session-wise generation, which is more elegant and contextually coherent than point-by-point generation that relies on hand-crafted rules to properly combine the generated results. 3) an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO) to enhance the quality of the generated results. Unlike DPO in NLP, a recommendation system typically has only one opportunity to display results for each user's browsing request, making it impossible to obtain positive and negative samples simultaneously. To address this limitation, We design a reward model to simulate user generation and customize the sampling strategy. Extensive experiments have demonstrated that a limited number of DPO samples can align user interest preferences and significantly improve the quality of generated results. We deployed OneRec in the main scene of Kuaishou, achieving a 1.6\% increase in watch-time, which is a substantial improvement."

[04.03.2025 09:12] Response: ```python
["GAMES", "OPTIMIZATION", "ALIGNMENT"]
```
[04.03.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces OneRec, a novel generative retrieval-based recommendation system that improves upon traditional retrieve-and-rank methods. Unlike existing systems that use generative models merely for selection, OneRec employs a unified generative model that encodes user behavior and generates personalized video recommendations in a session-wise manner. The model utilizes a sparse Mixture-of-Experts architecture to enhance capacity while maintaining efficiency, and incorporates an Iterative Preference Alignment module to optimize user preferences effectively. Experimental results show that OneRec significantly outperforms existing systems, leading to a notable increase in user engagement metrics such as watch-time.","title":"OneRec: Revolutionizing Recommendations with Generative Models"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces OneRec, a novel generative retrieval-based recommendation system that improves upon traditional retrieve-and-rank methods. Unlike existing systems that use generative models merely for selection, OneRec employs a unified generative model that encodes user behavior and generates personalized video recommendations in a session-wise manner. The model utilizes a sparse Mixture-of-Experts architecture to enhance capacity while maintaining efficiency, and incorporates an Iterative Preference Alignment module to optimize user preferences effectively. Experimental results show that OneRec significantly outperforms existing systems, leading to a notable increase in user engagement metrics such as watch-time.', title='OneRec: Revolutionizing Recommendations with Generative Models'))
[04.03.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"最近，基于生成检索的推荐系统成为一种有前景的范式。本文提出的OneRec模型，采用统一的生成模型，取代了传统的级联学习框架，能够在真实场景中显著超越现有复杂的推荐系统。OneRec包括编码-解码结构，能够有效编码用户的历史行为，并生成用户可能感兴趣的视频。此外，OneRec还引入了会话生成方法和迭代偏好对齐模块，提升了生成结果的质量，并在快手的实际应用中实现了观看时间的显著增加。","title":"OneRec：统一生成模型的推荐新范式"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='最近，基于生成检索的推荐系统成为一种有前景的范式。本文提出的OneRec模型，采用统一的生成模型，取代了传统的级联学习框架，能够在真实场景中显著超越现有复杂的推荐系统。OneRec包括编码-解码结构，能够有效编码用户的历史行为，并生成用户可能感兴趣的视频。此外，OneRec还引入了会话生成方法和迭代偏好对齐模块，提升了生成结果的质量，并在快手的实际应用中实现了观看时间的显著增加。', title='OneRec：统一生成模型的推荐新范式'))
[04.03.2025 09:12] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#leakage", "#open_source"], "emoji": "🏟️", "ru": {"title": "CodeArena: Справедливая арена для оценки LLM в генерации кода", "desc": "CodeArena - это новая онлайн-платформа для оценки генерации кода большими языковыми моделями (LLM). Она использует коллектив
[04.03.2025 09:12] Using data from previous issue: {"categories": ["#video", "#dataset", "#data", "#games", "#open_source"], "emoji": "🎬", "ru": {"title": "VideoUFO: Новый эталонный датасет для генерации видео по запросу", "desc": "Статья представляет VideoUFO - новый набор данных для обучения моделей генерации видео по текстовому описанию. Этот дат
[04.03.2025 09:12] Using data from previous issue: {"categories": ["#data", "#open_source", "#optimization", "#dataset", "#training"], "emoji": "🔍", "ru": {"title": "Эффективный отбор данных для обучения языковых моделей: меньше значит больше", "desc": "Эта статья исследует методы автоматического отбора данных для инструктивной настройки языковых мо
[04.03.2025 09:12] Querying the API.
[04.03.2025 09:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned a unique frequency, creating a logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves, with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (>20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing a concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance.
[04.03.2025 09:12] Response: {
  "desc": "Это исследование изучает потенциал больших языковых моделей (LLM) для разработки приватных тональных языков для коммуникации между машинами. Авторы создали систему кодирования, которая сопоставляет каждому символу ASCII уникальную частоту, формируя логарифмическую прогрессию от 220 Гц до 50,175.42 Гц. Разработанный программный прототип демонстрирует это кодирование через визуализацию, воспроизведение звука и нотацию ABC. Тестирование показало, что тональное кодирование может достигать скорости передачи информации, превышающей человеческую речь, при этом частично работая за пределами человеческого восприятия.",
  "emoji": "🎵",
  "title": "Тональные языки: секретный код машин будущего"
}
[04.03.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned a unique frequency, creating a logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves, with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (>20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing a concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance."

[04.03.2025 09:12] Response: ```python
["MULTIMODAL", "AUDIO"]
```
[04.03.2025 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned a unique frequency, creating a logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves, with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (>20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing a concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance."

[04.03.2025 09:12] Response: ```python
["ETHICS", "SECURITY"]
```
[04.03.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper explores how large language models (LLMs) can create private tonal languages for communication between machines. It draws inspiration from the phenomenon of cryptophasia in twins and uses a character-to-frequency mapping system to encode ASCII characters into musical tones. Each character is assigned a unique frequency, allowing for efficient data transmission that exceeds human speech rates. The study provides a prototype that visualizes and plays back this encoding, addressing concerns about AI developing private languages and offering a framework for understanding and managing such systems.","title":"Unlocking Machine Communication with Tonal Languages"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper explores how large language models (LLMs) can create private tonal languages for communication between machines. It draws inspiration from the phenomenon of cryptophasia in twins and uses a character-to-frequency mapping system to encode ASCII characters into musical tones. Each character is assigned a unique frequency, allowing for efficient data transmission that exceeds human speech rates. The study provides a prototype that visualizes and plays back this encoding, addressing concerns about AI developing private languages and offering a framework for understanding and managing such systems.', title='Unlocking Machine Communication with Tonal Languages'))
[04.03.2025 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文研究了大型语言模型（LLMs）在机器间（M2M）通信中开发私有音调语言的潜力。我们借鉴了人类双胞胎中的密码语言现象和自然音调语言，如普通话和越南语，实施了一种精确的字符到频率映射系统。每个字符被分配一个独特的频率，形成一个对数进程，覆盖约7.9个八度，并将高频字符映射到人类听觉范围之外的超声波频率。我们的软件原型展示了这种编码的可视化、听觉播放和音乐记谱法，分析了信息密度和传输速度，测试结果表明音调编码的信息传输速率超过人类语言。","title":"探索机器间的私有音调语言"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文研究了大型语言模型（LLMs）在机器间（M2M）通信中开发私有音调语言的潜力。我们借鉴了人类双胞胎中的密码语言现象和自然音调语言，如普通话和越南语，实施了一种精确的字符到频率映射系统。每个字符被分配一个独特的频率，形成一个对数进程，覆盖约7.9个八度，并将高频字符映射到人类听觉范围之外的超声波频率。我们的软件原型展示了这种编码的可视化、听觉播放和音乐记谱法，分析了信息密度和传输速度，测试结果表明音调编码的信息传输速率超过人类语言。', title='探索机器间的私有音调语言'))
[04.03.2025 09:12] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#open_source", "#robotics", "#agents", "#optimization"], "emoji": "🤖", "ru": {"title": "CLEA: Повышение надежности выполнения задач роботами с помощью языковых моделей", "desc": "Статья представляет новую архитектуру под названием CLEA (Closed-Loop Emb
[04.03.2025 09:12] Trying to get texts in Chinese.
[04.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one key direction in reproducing o1. While the R1-style model has demonstrated success in language models, its application in multi-modal domains remains under-explored. This work introduces Visual Reinforcement Fine-Tuning (Visual-RFT), which further extends the application areas of RFT on visual tasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs) to generate multiple responses containing reasoning tokens and final answers for each input, and then uses our proposed visual perception verifiable reward functions to update the model via the policy optimization algorithm such as Group Relative Policy Optimization (GRPO). We design different verifiable reward functions for different perception tasks, such as the Intersection over Union (IoU) reward for object detection. Experimental results on fine-grained image classification, few-shot object detection, reasoning grounding, as well as open-vocabulary object detection benchmarks show the competitive performance and advanced generalization ability of Visual-RFT compared with Supervised Fine-tuning (SFT). For example, Visual-RFT improves accuracy by 24.3% over the baseline in one-shot fine-grained image classification with around 100 samples. In few-shot object detection, Visual-RFT also exceeds the baseline by 21.9 on COCO's two-shot setting and 15.4 on LVIS. Our Visual-RFT represents a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks.
[04.03.2025 09:12] Mistral response. {"id": "ebfd2452c17a409bb4d12b4f4fd8ba9c", "object": "chat.completion", "created": 1741079540, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53eb\u505a\u89c6\u89c9\u5f3a\u5316\u5fae\u8c03\uff08Visual-RFT\uff09\u7684\u65b9\u6cd5\u3002\u5b83\u7528\u4e8e\u6539\u8fdb\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u7b54\u6848\u5e76\u4f7f\u7528\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u51fd\u6570\u66f4\u65b0\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVisual-RFT\u5728\u591a\u4e2a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002\u4f8b\u5982\uff0c\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0cVisual-RFT\u63d0\u9ad8\u4e8624.3%\u7684\u51c6\u786e\u7387\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 441, "total_tokens": 597, "completion_tokens": 156}}
[04.03.2025 09:12] Response: 这篇文章介绍了一种叫做视觉强化微调（Visual-RFT）的方法。它用于改进大型视觉-语言模型（LVLMs）在视觉任务中的表现。这种方法通过生成多个答案并使用可验证的奖励函数更新模型。实验结果显示，Visual-RFT在多个视觉任务中表现出色，优于监督微调（SFT）。例如，在细粒度图像分类中，Visual-RFT提高了24.3%的准确率。
[04.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

这篇文章介绍了一种叫做视觉强化微调（Visual-RFT）的方法。它用于改进大型视觉-语言模型（LVLMs）在视觉任务中的表现。这种方法通过生成多个答案并使用可验证的奖励函数更新模型。实验结果显示，Visual-RFT在多个视觉任务中表现出色，优于监督微调（SFT）。例如，在细粒度图像分类中，Visual-RFT提高了24.3%的准确率。
[04.03.2025 09:12] Mistral response. {"id": "60e6898332b749618f240104375b434e", "object": "chat.completion", "created": 1741079543, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53eb\u505a\u89c6\u89c9\u5f3a\u5316\u5fae\u8c03\uff08Visual-RFT\uff09\u7684\u65b9\u6cd5\u3002\nZh\u00e8 pi\u0101n w\u00e9nzh\u0101ng ji\u00e8sh\u00e0o le y\u012b zh\u01d2ng ji\u00e0o zu\u00f2 sh\u00ecju\u00e9 qi\u00e1ngzh\u00f9 w\u0113iti\u00e1o \uff08Visual-RFT\uff09de f\u0101ngf\u01ce.\n\n\u5b83\u7528\u4e8e\u6539\u8fdb\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\nT\u0101 y\u00f2ngy\u00fa g\u01ceij\u00ecn d\u00e0x\u00edng sh\u00ecju\u00e9-y\u01d4y\u00e1n m\u00f3x\u00edng \uff08LVLMs\uff09 z\u00e0i sh\u00ecju\u00e9 r\u00e8nw\u00f9 zh\u014dng de bi\u01ceoxi\u00e0n.\n\n\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u7b54\u6848\u5e76\u4f7f\u7528\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u51fd\u6570\u66f4\u65b0\u6a21\u578b\u3002\nZh\u00e8 zh\u01d2ng f\u0101ngf\u01ce t\u014dnggu\u00f2 sh\u0113ngch\u00e9ng du\u014d g\u00e8 d\u00e1'\u00e0n b\u00ecng sh\u01d0y\u00f2ng k\u011b y\u00e0nzh\u00e8ng de ji\u01cengl\u00ec h\u00e1nsh\u00f9 g\u0113ngx\u012bn m\u00f3x\u00edng.\n\n\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVisual-RFT\u5728\u591a\u4e2a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002\nSh\u00edy\u00e0n ji\u00e9gu\u01d2 xi\u01censh\u00ec, Visual-RFT z\u00e0i du\u014d g\u00e8 sh\u00ecju\u00e9 r\u00e8nw\u00f9 zh\u014dng bi\u01ceoxi\u00e0n ch\u016bs\u00e8, y\u014duy\u00fa ji\u00e0nd\u016b w\u0113iti\u00e1o \uff08SFT\uff09.\n\n\u4f8b\u5982\uff0c\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0cVisual-RFT\u63d0\u9ad8\u4e8624.3%\u7684\u51c6\u786e\u7387\u3002\nL\u00ecr\u00fa, z\u00e0i x\u00ecl\u00ecd\u00f9 t\u00faxi\u00e0ng f\u0113nl\u00e8i zh\u014dng, Visual-RFT t\u00edg\u0101o le 24.3% de zh\u01d4nqu\u00e8l\u01dc."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 171, "total_tokens": 653, "completion_tokens": 482}}
[04.03.2025 09:12] Response: 这篇文章介绍了一种叫做视觉强化微调（Visual-RFT）的方法。
Zhè piān wénzhāng jièshào le yī zhǒng jiào zuò shìjué qiángzhù wēitiáo （Visual-RFT）de fāngfǎ.

它用于改进大型视觉-语言模型（LVLMs）在视觉任务中的表现。
Tā yòngyú gǎijìn dàxíng shìjué-yǔyán móxíng （LVLMs） zài shìjué rènwù zhōng de biǎoxiàn.

这种方法通过生成多个答案并使用可验证的奖励函数更新模型。
Zhè zhǒng fāngfǎ tōngguò shēngchéng duō gè dá'àn bìng shǐyòng kě yànzhèng de jiǎnglì hánshù gēngxīn móxíng.

实验结果显示，Visual-RFT在多个视觉任务中表现出色，优于监督微调（SFT）。
Shíyàn jiéguǒ xiǎnshì, Visual-RFT zài duō gè shìjué rènwù zhōng biǎoxiàn chūsè, yōuyú jiàndū wēitiáo （SFT）.

例如，在细粒度图像分类中，Visual-RFT提高了24.3%的准确率。
Lìrú, zài xìlìdù túxiàng fēnlèi zhōng, Visual-RFT tígāo le 24.3% de zhǔnquèlǜ.
[04.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

这篇文章介绍了一种叫做视觉强化微调（Visual-RFT）的方法。它用于改进大型视觉-语言模型（LVLMs）在视觉任务中的表现。这种方法通过生成多个答案并使用可验证的奖励函数更新模型。实验结果显示，Visual-RFT在多个视觉任务中表现出色，优于监督微调（SFT）。例如，在细粒度图像分类中，Visual-RFT提高了24.3%的准确率。
[04.03.2025 09:12] Mistral response. {"id": "6e25dd210e06463e9eb8ee2aaf287bb1", "object": "chat.completion", "created": 1741079555, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[\n    {\"word\": \"\u89c6\u89c9\u5f3a\u5316\u5fae\u8c03\", \"pinyin\": \"sh\u00ecju\u00e9 qi\u00e1ngzh\u00f9 w\u0113iti\u00e1o\", \"trans\": \"visual reinforcement fine-tuning\"},\n    {\"word\": \"\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\", \"pinyin\": \"d\u00e0x\u00edng sh\u00ecju\u00e9-y\u01d4y\u00e1n m\u00f3x\u00edng\", \"trans\": \"large vision-language models\"},\n    {\"word\": \"\u8868\u73b0\", \"pinyin\": \"bi\u01ceoxi\u00e0n\", \"trans\": \"performance\"},\n    {\"word\": \"\u751f\u6210\", \"pinyin\": \"sh\u0113ngch\u00e9ng\", \"trans\": \"generate\"},\n    {\"word\": \"\u53ef\u9a8c\u8bc1\u7684\", \"pinyin\": \"k\u011b y\u00e0nzh\u00e8ng de\", \"trans\": \"verifiable\"},\n    {\"word\": \"\u5956\u52b1\u51fd\u6570\", \"pinyin\": \"ji\u01cengl\u00ec h\u00e1nsh\u00f9\", \"trans\": \"reward function\"},\n    {\"word\": \"\u66f4\u65b0\", \"pinyin\": \"g\u0113ngx\u012bn\", \"trans\": \"update\"},\n    {\"word\": \"\u5b9e\u9a8c\u7ed3\u679c\", \"pinyin\": \"sh\u00edy\u00e0n ji\u00e9gu\u01d2\", \"trans\": \"experimental results\"},\n    {\"word\": \"\u51fa\u8272\", \"pinyin\": \"ch\u016bs\u00e8\", \"trans\": \"outstanding\"},\n    {\"word\": \"\u4f18\u4e8e\", \"pinyin\": \"y\u014du y\u00fa\", \"trans\": \"superior to\"},\n    {\"word\": \"\u76d1\u7763\u5fae\u8c03\", \"pinyin\": \"ji\u00e0nd\u016b w\u0113iti\u00e1o\", \"trans\": \"supervised fine-tuning\"},\n    {\"word\": \"\u7ec6\u7c92\u5ea6\", \"pinyin\": \"x\u00ec l\u00ecd\u00f9\", \"trans\": \"fine-grained\"},\n    {\"word\": \"\u56fe\u50cf\u5206\u7c7b\", \"pinyin\": \"t\u00faxi\u00e0ng f\u0113nl\u00e8i\", \"trans\": \"image classification\"},\n    {\"word\": \"\u51c6\u786e\u7387\", \"pinyin\": \"zh\u01d4nqu\u00e8l\u01dc\", \"trans\": \"accuracy\"}\n]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 203, "total_tokens": 692, "completion_tokens": 489}}
[04.03.2025 09:12] Response: [
    {"word": "视觉强化微调", "pinyin": "shìjué qiángzhù wēitiáo", "trans": "visual reinforcement fine-tuning"},
    {"word": "大型视觉-语言模型", "pinyin": "dàxíng shìjué-yǔyán móxíng", "trans": "large vision-language models"},
    {"word": "表现", "pinyin": "biǎoxiàn", "trans": "performance"},
    {"word": "生成", "pinyin": "shēngchéng", "trans": "generate"},
    {"word": "可验证的", "pinyin": "kě yànzhèng de", "trans": "verifiable"},
    {"word": "奖励函数", "pinyin": "jiǎnglì hánshù", "trans": "reward function"},
    {"word": "更新", "pinyin": "gēngxīn", "trans": "update"},
    {"word": "实验结果", "pinyin": "shíyàn jiéguǒ", "trans": "experimental results"},
    {"word": "出色", "pinyin": "chūsè", "trans": "outstanding"},
    {"word": "优于", "pinyin": "yōu yú", "trans": "superior to"},
    {"word": "监督微调", "pinyin": "jiàndū wēitiáo", "trans": "supervised fine-tuning"},
    {"word": "细粒度", "pinyin": "xì lìdù", "trans": "fine-grained"},
    {"word": "图像分类", "pinyin": "túxiàng fēnlèi", "trans": "image classification"},
    {"word": "准确率", "pinyin": "zhǔnquèlǜ", "trans": "accuracy"}
]
[04.03.2025 09:12] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

这篇文章介绍了一种叫做视觉强化微调（Visual-RFT）的方法。它用于改进大型视觉-语言模型（LVLMs）在视觉任务中的表现。这种方法通过生成多个答案并使用可验证的奖励函数更新模型。实验结果显示，Visual-RFT在多个视觉任务中表现出色，优于监督微调（SFT）。例如，在细粒度图像分类中，Visual-RFT提高了24.3%的准确率。
[04.03.2025 09:12] Mistral response. {"id": "03e5cc62bbe34ed68526818961d998e9", "object": "chat.completion", "created": 1741079565, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "This article introduces a method called Visual Reinforcement Fine-Tuning (Visual-RFT). It is used to improve the performance of large vision-language models (LVLMs) in visual tasks. This method works by generating multiple answers and updating the model using a verifiable reward function. Experimental results show that Visual-RFT performs excellently in multiple visual tasks, outperforming supervised fine-tuning (SFT). For example, in fine-grained image classification, Visual-RFT improved accuracy by 24.3%."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 169, "total_tokens": 284, "completion_tokens": 115}}
[04.03.2025 09:12] Response: This article introduces a method called Visual Reinforcement Fine-Tuning (Visual-RFT). It is used to improve the performance of large vision-language models (LVLMs) in visual tasks. This method works by generating multiple answers and updating the model using a verifiable reward function. Experimental results show that Visual-RFT performs excellently in multiple visual tasks, outperforming supervised fine-tuning (SFT). For example, in fine-grained image classification, Visual-RFT improved accuracy by 24.3%.
[04.03.2025 09:12] Renaming data file.
[04.03.2025 09:12] Renaming previous data. hf_papers.json to ./d/2025-03-04.json
[04.03.2025 09:12] Saving new data file.
[04.03.2025 09:12] Generating page.
[04.03.2025 09:12] Renaming previous page.
[04.03.2025 09:12] Renaming previous data. index.html to ./d/2025-03-04.html
[04.03.2025 09:12] [Experimental] Generating Chinese page for reading.
[04.03.2025 09:12] Chinese vocab [{'word': '视觉强化微调', 'pinyin': 'shìjué qiángzhù wēitiáo', 'trans': 'visual reinforcement fine-tuning'}, {'word': '大型视觉-语言模型', 'pinyin': 'dàxíng shìjué-yǔyán móxíng', 'trans': 'large vision-language models'}, {'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'}, {'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'}, {'word': '可验证的', 'pinyin': 'kě yànzhèng de', 'trans': 'verifiable'}, {'word': '奖励函数', 'pinyin': 'jiǎnglì hánshù', 'trans': 'reward function'}, {'word': '更新', 'pinyin': 'gēngxīn', 'trans': 'update'}, {'word': '实验结果', 'pinyin': 'shíyàn jiéguǒ', 'trans': 'experimental results'}, {'word': '出色', 'pinyin': 'chūsè', 'trans': 'outstanding'}, {'word': '优于', 'pinyin': 'yōu yú', 'trans': 'superior to'}, {'word': '监督微调', 'pinyin': 'jiàndū wēitiáo', 'trans': 'supervised fine-tuning'}, {'word': '细粒度', 'pinyin': 'xì lìdù', 'trans': 'fine-grained'}, {'word': '图像分类', 'pinyin': 'túxiàng fēnlèi', 'trans': 'image classification'}, {'word': '准确率', 'pinyin': 'zhǔnquèlǜ', 'trans': 'accuracy'}]
[04.03.2025 09:12] Renaming previous Chinese page.
[04.03.2025 09:12] Renaming previous data. zh.html to ./d/2025-03-03_zh_reading_task.html
[04.03.2025 09:12] Writing Chinese reading task.
[04.03.2025 09:12] Writing result.
[04.03.2025 09:12] Renaming log file.
[04.03.2025 09:12] Renaming previous data. log.txt to ./logs/2025-03-04_last_log.txt
