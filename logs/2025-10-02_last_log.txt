[02.10.2025 22:11] Read previous papers.
[02.10.2025 22:11] Generating top page (month).
[02.10.2025 22:11] Writing top page (month).
[02.10.2025 23:10] Read previous papers.
[02.10.2025 23:10] Get feed.
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25454
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00406
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01051
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25849
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25455
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22944
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01174
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00615
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00977
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00184
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00232
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26346
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00967
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25301
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01180
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00931
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00526
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00553
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.22887
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00536
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25531
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23250
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01152
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00510
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01037
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00777
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.19185
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01070
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01061
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00438
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.26514
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25916
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25411
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25045
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00225
[02.10.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2509.25162
[02.10.2025 23:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[02.10.2025 23:10] No deleted papers detected.
[02.10.2025 23:10] Downloading and parsing papers (pdf, html). Total: 36.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.25454.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.25454.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.25454.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.00406.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.00406.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.00406.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.01051.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.01051.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.01051.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.25849.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.25849.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.25849.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.25455.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.25455.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.25455.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.22944.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.22944.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.22944.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.01174.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.01174.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.01174.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.00615.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.00615.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.00615.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.00977.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.00977.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.00977.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.00184.
[02.10.2025 23:10] Downloading paper 2510.00184 from http://arxiv.org/pdf/2510.00184v1...
[02.10.2025 23:10] Failed to download and parse paper https://huggingface.co/papers/2510.00184: 'LTChar' object is not iterable
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.00232.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.00232.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.00232.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.26346.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.26346.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.26346.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.00967.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.00967.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.00967.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.25301.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.25301.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.25301.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.01180.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.01180.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.01180.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.00931.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.00931.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.00931.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.00526.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.00526.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.00526.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.00553.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.00553.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.00553.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.22887.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.22887.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.22887.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.00536.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.00536.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.00536.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.25531.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.25531.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.25531.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.23250.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.23250.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.23250.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.01152.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.01152.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.01152.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.00510.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.00510.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.00510.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.01037.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.01037.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.01037.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.00777.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.00777.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.00777.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.19185.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.19185.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.19185.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.01070.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.01070.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.01070.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.01061.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.01061.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.01061.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.00438.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.00438.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.00438.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.26514.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.26514.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.26514.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.25916.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.25916.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.25916.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.25411.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.25411.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.25411.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.25045.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.25045.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.25045.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2510.00225.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2510.00225.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2510.00225.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2509.25162.
[02.10.2025 23:10] Extra JSON file exists (./assets/json/2509.25162.json), skip PDF parsing.
[02.10.2025 23:10] Paper image links file exists (./assets/img_data/2509.25162.json), skip HTML parsing.
[02.10.2025 23:10] Success.
[02.10.2025 23:10] Enriching papers with extra data.
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 0. DeepSearch integrates Monte Carlo Tree Search into RLVR training to enhance exploration and credit assignment, achieving state-of-the-art performance with reduced computational cost.  					AI-generated summary 				 Although RLVR has become an essential component for developing advanced reasoning ski...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 1. VLA-RFT uses a data-driven world model to fine-tune VLA models efficiently, reducing sample requirements and improving robustness under perturbations.  					AI-generated summary 				 Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading t...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 2. GEM, an open-source environment simulator, facilitates experience-based learning for large language models by providing a standardized framework and diverse environments for training and benchmarking reinforcement learning algorithms.  					AI-generated summary 				 The training paradigm for large l...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 3. An adaptive exploration budget allocation method for reinforcement learning in Large Language Models improves training efficiency and performance on mathematical reasoning benchmarks.  					AI-generated summary 				 Large Language Models (LLMs) can self-improve through reinforcement learning, where ...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 4. A specialized model combining supervised fine-tuning and Reinforcement Learning with Verifiable Rewards achieves competitive performance in automated environment setup tasks.  					AI-generated summary 				 Environment setup-the process of configuring the system to work with a specific software proj...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 5. SINQ enhances post-training quantization by introducing a second-axis scale factor and Sinkhorn-Knopp-style algorithm to minimize matrix imbalance, improving perplexity on large language models.  					AI-generated summary 				 Post-training quantization has emerged as the most widely used strategy f...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 6. Code2Video generates educational videos using a code-centric agent framework, improving coherence and interpretability compared to direct code generation.  					AI-generated summary 				 While recent generative models advance pixel-space video synthesis, they remain limited in producing professional...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 7. Agent Context Optimization (ACON) compresses context in large language models for efficient long-horizon tasks by analyzing failure cases and distilling the compressor into smaller models.  					AI-generated summary 				 Large language models (LLMs) are increasingly deployed as agents in dynamic, re...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 8. Reframing Group Relative Policy Optimization as contrastive learning reveals its connection to Direct Preference Optimization, enabling minimal two-rollout GRPO to achieve performance comparable to larger group sizes with reduced computational cost.  					AI-generated summary 				 Group Relative Pol...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 9. Reverse-engineering a model that learns multi-digit multiplication via implicit chain-of-thought reveals that it uses attention to encode long-range dependencies and represents partial products efficiently, insights that help address limitations in standard fine-tuning.  					AI-generated summary 		...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 10. BiasFreeBench evaluates bias mitigation techniques in large language models using a unified benchmark and response-level metric to ensure fair and safe outputs in real-world scenarios.  					AI-generated summary 				 Existing studies on bias mitigation methods for large language models (LLMs) use di...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 11. A new reward model, trained with a large-scale human preference dataset, improves instruction-guided image editing by selecting high-quality training data and achieving state-of-the-art performance on benchmarks.  					AI-generated summary 				 Recently, we have witnessed great progress in image edi...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 12. QUASAR, an RL framework using tool-augmented LLMs, improves quantum circuit generation and optimization through verification and hierarchical rewards, achieving high validity compared to industrial LLMs.  					AI-generated summary 				 Designing and optimizing task-specific quantum circuits are cruc...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 13. Flash-Searcher, a parallel agent reasoning framework using directed acyclic graphs, enhances efficiency and performance in complex reasoning tasks by enabling concurrent execution and dynamic workflow optimization.  					AI-generated summary 				 Large language models (LLMs) have demonstrated remark...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 14. BroRL enhances reinforcement learning by increasing rollouts per example, overcoming performance plateaus and achieving state-of-the-art results in large language models.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key ingredient for unlocki...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 15. Fusion-of-N (FusioN) method improves LLM generation quality by synthesizing elements from multiple samples, outperforming Best-of-N in various settings and tasks.  					AI-generated summary 				 Obtaining high-quality generations in modern LLMs has largely been framed as a selection problem: identif...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 16. Research identifies probability-based objectives that outperform negative log likelihood for fine-tuning large language models, depending on model capability.  					AI-generated summary 				 Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it ...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 17. Two fundamental properties of reinforcement learning-induced parameter updates in large language models are identified, leading to a plug-in acceleration framework that significantly speeds up training without sacrificing performance.  					AI-generated summary 				 Recent advances in reasoning capa...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 18. Integrating Theory of Mind into LLMs improves dialogue effectiveness and goal achievement by enabling strategic reasoning and better partner relationships.  					AI-generated summary 				 Theory of Mind (ToM)-an understanding of the mental states of others-is a key aspect of human social intelligenc...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 19. GUI-KV, a KV cache compression method for GUI agents, improves efficiency by exploiting spatial and temporal redundancies, reducing computational cost while maintaining accuracy.  					AI-generated summary 				 Graphical user interface (GUI) agents built on vision-language models have emerged as a p...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 20. MixtureVitae is a pretraining corpus that combines public-domain and permissively licensed text with low-risk additions, achieving strong model performance across benchmarks while minimizing legal risk.  					AI-generated summary 				 We present MixtureVitae, an open-access pretraining corpus built ...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 21. Hybrid data synthesis and perception-focused supervision improve the reliability of Vision-Language Process Reward Models (VL-PRMs) in guiding VLMs across diverse multimodal benchmarks.  					AI-generated summary 				 Process Reward Models (PRMs) provide step-level supervision that improves the reli...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 22. MASH, a reinforcement learning framework, improves LLMs' selective help-seeking and abstention capabilities without pre-determined knowledge boundaries.  					AI-generated summary 				 LLMs cannot reliably recognize their parametric knowledge boundaries and often hallucinate answers to outside-of-bo...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 23. A generalist agent architecture combining multi-agent planning, hierarchical memory, and a refined tool suite outperforms existing systems in diverse tasks.  					AI-generated summary 				 Large Language Models are increasingly deployed as autonomous agents for complex real-world tasks, yet existing...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 24. CurES, a reinforcement learning-based method, improves the training efficiency of large language models by optimizing prompt selection and rollout allocation, leading to faster convergence and reduced computational overhead.  					AI-generated summary 				 Curriculum learning plays a crucial role in...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 25. In-place feedback allows users to directly edit LLM responses, improving performance and reducing token usage in multi-turn reasoning tasks.  					AI-generated summary 				 Large language models (LLMs) are increasingly studied in the context of multi-turn reasoning, where models iteratively refine t...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 26. The study identifies testing practices in AI agent frameworks and applications, highlighting a focus on deterministic components and a neglect of the Trigger component, suggesting improvements for robustness.  					AI-generated summary 				 Foundation model (FM)-based AI agents are rapidly gaining a...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 27. Researchers develop and evaluate techniques to uncover hidden knowledge in large language models through black-box and white-box methods, with prefill attacks and logit lens being particularly effective.  					AI-generated summary 				 We study secret elicitation: discovering knowledge that an AI po...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 28. Reservoir SWD reduces variance in Sliced Wasserstein Distance, improving gradient stability and performance in vision and graphics tasks.  					AI-generated summary 				 Distribution matching is central to many vision and graphics tasks, where the widely used Wasserstein distance is too costly to co...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 29. BindWeave, a unified framework using MLLM-DiT, enhances subject-consistent video generation by integrating deep cross-modal reasoning with diffusion transformers, achieving superior performance on OpenS2V.  					AI-generated summary 				 Diffusion Transformer has shown remarkable abilities in genera...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 30. BatonVoice framework decouples instruction understanding from speech generation, using an LLM to create vocal feature plans and a specialized TTS model to produce speech, achieving strong performance in controllable and emotional speech synthesis with zero-shot cross-lingual generalization.  					AI...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 31. VLM-FO1 enhances vision-language models with a hybrid fine-grained region encoder to improve object localization and region understanding without sacrificing general visual capabilities.  					AI-generated summary 				 Vision-Language Models (VLMs) excel at high-level scene understanding but falter ...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 32. ImitSAT, a branching policy for CDCL solvers using imitation learning from expert traces, reduces propagation counts and runtime by providing dense decision-level supervision.  					AI-generated summary 				 We propose ImitSAT, a branching policy for conflict-driven clause learning (CDCL) solvers ba...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 33. A novel Hyperdimensional Probe method decodes information from LLM vector spaces using Vector Symbolic Architectures, providing interpretable insights into model states and failures.  					AI-generated summary 				 Despite their capabilities, Large Language Models (LLMs) remain opaque with limited u...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 34. TGPO, a Temporal Grounded Policy Optimization framework, decomposes STL tasks into subgoals and uses a hierarchical approach with dense rewards to improve task success rates in complex, long-horizon robotics tasks.  					AI-generated summary 				 Learning control policies for complex, long-horizon t...
[02.10.2025 23:10] ********************************************************************************
[02.10.2025 23:10] Abstract 35. Pretrained visual encoders are aligned as tokenizers for latent diffusion models, improving image generation quality and convergence speed.  					AI-generated summary 				 In this work, we propose aligning pretrained visual encoders to serve as tokenizers for latent diffusion models in image generat...
[02.10.2025 23:10] Read previous papers.
[02.10.2025 23:10] Generating reviews via LLM API.
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#math", "#reasoning", "#optimization", "#rl", "#training"], "emoji": "üå≥", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –≤–º–µ—Å—Ç–æ –≥—Ä—É–±–æ–π —Å–∏–ª—ã: –¥—Ä–µ–≤–æ–≤–∏–¥–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM", "desc": "DeepSearch –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç Monte Carlo Tree Search –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#training", "#optimization", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ –º–∏—Ä-—Å–∏–º—É–ª—è—Ç–æ—Ä: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏ –Ω–∞–¥—ë–∂–Ω–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VLA-RFT ‚Äî –º–µ—Ç–æ–¥ –¥–æ–æ–±—É—á–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é reinforcement learning. –í–º–µ—Å—Ç–æ –¥–æ
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#rl", "#open_source", "#benchmark", "#training", "#games", "#agents"], "emoji": "üéÆ", "ru": {"title": "GEM: —Å–ø–æ—Ä—Ç–∑–∞–ª –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ reinforcement learning", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω GEM (General Experience Maker) ‚Äî –æ—Ç–∫—Ä—ã—Ç–∞—è —Å—Ä–µ–¥–∞-—Å–∏–º—É–ª—è—Ç–æ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö 
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#math", "#reasoning", "#optimization", "#rl", "#training"], "emoji": "üéí", "ru": {"title": "–£–º–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: –±–æ–ª—å—à–µ –≤–Ω–∏–º–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã–º –∑–∞–¥–∞—á–∞–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –±—é–¥–∂–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –í–º–µ
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#rl", "#training", "#optimization", "#open_source", "#small_models"], "emoji": "‚öôÔ∏è", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è - 
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization"], "emoji": "‚öñÔ∏è", "ru": {"title": "–î–≤—É—Ö–æ—Å–µ–≤–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ SINQ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è ‚Äî –¥–æ–±–∞–≤–∏—Ç—å –≤—Ç
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#video", "#games", "#optimization", "#agents", "#dataset"], "emoji": "üéì", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—É—á–∞—é—â–∏—Ö –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–π –∫–æ–¥", "desc": "Code2Video ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π 
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#agents", "#small_models", "#optimization", "#long_context", "#inference", "#dataset"], "emoji": "üóúÔ∏è", "ru": {"title": "ACON: –°–∂–∞—Ç–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Agent Context Optimization (ACON) ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –∫–æ–Ω—Ç
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#rlhf", "#rl"], "emoji": "‚ö°", "ru": {"title": "–î–≤–∞ —Ä–æ–ª–ª–∞—É—Ç–∞ –≤–º–µ—Å—Ç–æ —à–µ—Å—Ç–Ω–∞–¥—Ü–∞—Ç–∏: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è LLM", "desc": "–°—Ç–∞—Ç—å—è –ø–µ—Ä–µ—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º Group Relative Policy Optimization (GRPO), –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#training", "#architecture", "#long_context", "#reasoning"], "emoji": "üî¢", "ru": {"title": "–ö–∞–∫ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —É—á–∞—Ç—Å—è —É–º–Ω–æ–∂–∞—Ç—å: —Ä–∞–∑–≥–∞–¥–∫–∞ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–≤–µ–ª–∏ –æ–±—Ä–∞—Ç–Ω—É—é –∏–Ω–∂–µ–Ω–µ—Ä–∏—é –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞—É—á–∏–ª–∞—Å—å –º–Ω–æ–≥–æ–∑–Ω–∞—á–Ω–æ–º—É —É–º–Ω–æ–∂–µ–Ω–∏—é —á–µ—Ä–µ–∑ –Ω–µ—è–≤–Ω—É—é —Ü–µ–ø–æ
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#ethics", "#dataset", "#benchmark"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ï–¥–∏–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –±–æ—Ä—å–±—ã —Å –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å—é –≤ LLM", "desc": "–°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ä–∞–∑–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∏ –º–µ—Ç—Ä–∏–∫–∏, —á
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#alignment", "#rlhf", "#data", "#open_source"], "emoji": "üé®", "ru": {"title": "Reward-–º–æ–¥–µ–ª—å –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—É—é reward-–º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#agi", "#training", "#rl", "#agents", "#optimization"], "emoji": "‚öõÔ∏è", "ru": {"title": "–ö–≤–∞–Ω—Ç–æ–≤—ã–µ —Å—Ö–µ–º—ã –ø–æ–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º RL-–∞–≥–µ–Ω—Ç–∞ —Å LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç QUASAR ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ reinforcement learning –∏ LLM —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#reasoning", "#benchmark", "#agents"], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Flash-Searcher ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è AI-–∞–≥–µ–Ω—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –Ω–∞–ø
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#reasoning", "#optimization", "#training"], "emoji": "üåä", "ru": {"title": "–ë–æ–ª—å—à–µ —Ä–æ–ª–ª–∞—É—Ç–æ–≤ ‚Äî –ª—É—á—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ RL —á–µ—Ä–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç BroRL ‚Äî –º–µ—Ç–æ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è reinforcement learning –¥–ª—è LLM —á–µ—Ä–µ–∑ —É–≤–µ–ª
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#rag", "#benchmark", "#optimization", "#synthetic", "#multilingual"], "emoji": "üîÄ", "ru": {"title": "–°–∏–ª–∞ —Å–ª–∏—è–Ω–∏—è: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ª—É—á—à–µ–µ –∏–∑ –º–Ω–æ–≥–∏—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –≤–º–µ—Å—Ç–æ –≤—ã–±–æ—Ä–∞ –æ–¥–Ω–æ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Fusion-of-N (FusioN), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM –ø—É—Ç—ë–º —Å–∏–Ω
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#optimization", "#training"], "emoji": "üéØ", "ru": {"title": "–í—ã–±–æ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–∏–ª—ã –º–æ–¥–µ–ª–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å negative log likelihood (NLL) –Ω–µ –≤—Å–µ–≥–¥–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞ –¥–ª—è —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∏–ª–∏ —Å–µ
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#reasoning"], "emoji": "üöÄ", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è LLM —á–µ—Ä–µ–∑ —Ä–∞–Ω–≥–æ–≤—É—é —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –¥–≤–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—Ä–∏ reinforcement learning: –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#alignment", "#rl", "#reasoning", "#benchmark", "#agents"], "emoji": "üß†", "ru": {"title": "–ê–≥–µ–Ω—Ç—ã —Å —Ç–µ–æ—Ä–∏–µ–π —Ä–∞–∑—É–º–∞ –¥–ª—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–ª–∏ —Ç–µ–æ—Ä–∏—é —Ä–∞–∑—É–º–∞ (ToM) ‚Äî —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–æ–Ω–∏–º–∞—Ç—å –º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥—Ä—É–≥–∏—Ö –ª—é–¥–µ–π ‚Äî –≤ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#optimization", "#inference", "#agents", "#benchmark"], "emoji": "üñ•Ô∏è", "ru": {"title": "–°–∂–∞—Ç–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ GUI-KV ‚Äî –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è KV-–∫—ç—à–∞ –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#synthetic", "#open_source", "#benchmark", "#low_resource", "#data", "#dataset"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ LLM –±–µ–∑ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ä–∏—Å–∫–æ–≤", "desc": "MixtureVitae ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π –∫–æ—Ä–ø—É—Å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è LLM, —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Å –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–µ–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ä–∏—Å–∫
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#training", "#games", "#multimodal", "#data", "#dataset"], "emoji": "üëÅÔ∏è", "ru": {"title": "–£—á–∏–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Å–≤–æ–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ—à–∞–≥–æ–≤–æ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ Vision-Language Process Reward Models (VL-PRMs), –∫–æ—Ç–æ—Ä—ã–µ –æ
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#training", "#alignment", "#reasoning", "#rl", "#hallucinations"], "emoji": "üîç", "ru": {"title": "–ù–∞—É—á–∏—Ç—å LLM –≥–æ–≤–æ—Ä–∏—Ç—å ¬´–Ω–µ –∑–Ω–∞—é¬ª —á–µ—Ä–µ–∑ —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–∏—Å–∫", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MASH ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ reinforcement learning, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–¥–µ—Ä–∂–∏–≤–∞—Ç—å—Å—è
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#optimization", "#architecture", "#multimodal", "#agents", "#agi"], "emoji": "ü§ñ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π AI-–∞–≥–µ–Ω—Ç —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –ø–∞–º—è—Ç–∏, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AI-–∞–≥–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è 
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#training", "#optimization", "#reasoning", "#rl"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω—ã–π –≤—ã–±–æ—Ä –ø—Ä–æ–º–ø—Ç–æ–≤ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "CurES ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ reinforcement learning, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º —É–º–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#training", "#rlhf", "#interpretability", "#reasoning"], "emoji": "‚úèÔ∏è", "ru": {"title": "–†–µ–¥–∞–∫—Ç–∏—Ä—É–π –ø—Ä—è–º–æ –∑–¥–µ—Å—å: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –¥–ª—è LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º in-place feedback, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º –ø–æ–ª—å–∑
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#security", "#open_source", "#training"], "emoji": "üîç", "ru": {"title": "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AI-–∞–≥–µ–Ω—Ç–æ–≤: —Ñ–æ–∫—É—Å –Ω–µ —Ç–∞–º, –≥–¥–µ –Ω—É–∂–Ω–æ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–∞–∫—Ç–∏–∫–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ 39 —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞—Ö AI-–∞–≥–µ–Ω—Ç–æ–≤ –∏ 439 –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö, –≤—ã—è–≤–ª—è—è –¥–µ—Å—è—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ç–µ—Å—Ç–∏—Ä
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#interpretability", "#multimodal", "#hallucinations", "#open_source"], "emoji": "üîç", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Ç–∞–π–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π: –∫–∞–∫ –∑–∞—Å—Ç–∞–≤–∏—Ç—å LLM –ø—Ä–∏–∑–Ω–∞—Ç—å—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–∞ —Å–∫—Ä—ã–≤–∞–µ—Ç", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∞—é—Ç –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –∑–Ω–∞–Ω–∏–π –∏–∑ –±–æ–ª—å—à–∏—Ö 
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#synthetic", "#diffusion", "#training", "#cv"], "emoji": "üéØ", "ru": {"title": "–°—Ç–∞–±–∏–ª—å–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ —Ä–µ–∑–µ—Ä–≤—É–∞—Ä–Ω—É—é –≤—ã–±–æ—Ä–∫—É", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Reservoir SWD –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#video", "#diffusion", "#open_source", "#multimodal", "#reasoning", "#benchmark"], "emoji": "üé¨", "ru": {"title": "BindWeave: —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ —Å—É–±—ä–µ–∫—Ç–æ–≤ –∏ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –≥–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤", "desc": "BindWeave ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–º–∏ —Å—É–±—ä–µ–∫—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–π —Ä
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#long_context", "#multimodal", "#open_source", "#audio", "#games"], "emoji": "üéº", "ru": {"title": "–î–∏—Ä–∏–∂—ë—Ä –∏ –æ—Ä–∫–µ—Å—Ç—Ä: LLM –ø–ª–∞–Ω–∏—Ä—É–µ—Ç, TTS –∏—Å–ø–æ–ª–Ω—è–µ—Ç", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ BatonVoice, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–µ—á–∏: LLM –≤—ã—Å—Ç—É–ø–∞–µ—Ç –∫–∞–∫ \"–¥–∏—Ä–∏–∂—ë
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#training", "#architecture", "#multimodal", "#reasoning", "#agi", "#cv", "#interpretability", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–û—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∫ –ø—Ä–∏–∑–Ω–∞–∫–∞–º: —Ç–æ—á–Ω–∞—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –≤ vision-language –º–æ–¥–µ–ª—è—Ö", "desc": "VLM-FO1 —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ç–æ—á–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ vision-
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#rl", "#open_source", "#training", "#optimization", "#math"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö —Ç—Ä–∞—Å—Å–∞—Ö –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è SAT-—Å–æ–ª–≤–µ—Ä–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω ImitSAT - –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –≤–µ—Ç–≤–ª–µ–Ω–∏—è –¥–ª—è CDCL —Å–æ–ª–≤–µ—Ä–æ–≤ SAT-–∑–∞–¥–∞—á, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ imitation learning. –ú
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#data", "#architecture", "#interpretability"], "emoji": "üîç", "ru": {"title": "–ì–∏–ø–µ—Ä–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º—ã—Å–ª–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Hyperdimensional Probe –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#training", "#robotics", "#rl", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç TGPO - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏ –≤ —Å–ª–æ–∂–Ω—ã—Ö –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∑–∞–¥–∞–Ω–Ω—ã
[02.10.2025 23:10] Using data from previous issue: {"categories": ["#optimization", "#rag", "#diffusion", "#cv"], "emoji": "üîó", "ru": {"title": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –∏–∑ –≥–æ—Ç–æ–≤—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è latent diffusion –º–æ–¥–µ–ª–µ–π
[02.10.2025 23:10] Renaming data file.
[02.10.2025 23:10] Renaming previous data. hf_papers.json to ./d/2025-10-02.json
[02.10.2025 23:10] Saving new data file.
[02.10.2025 23:10] Generating page.
[02.10.2025 23:10] Renaming previous page.
[02.10.2025 23:10] Renaming previous data. index.html to ./d/2025-10-02.html
[02.10.2025 23:10] Writing result.
[02.10.2025 23:10] Renaming log file.
[02.10.2025 23:10] Renaming previous data. log.txt to ./logs/2025-10-02_last_log.txt
