[02.10.2025 02:24] Read previous papers.
[02.10.2025 02:24] Generating top page (month).
[02.10.2025 02:24] Writing top page (month).
[02.10.2025 03:24] Read previous papers.
[02.10.2025 03:24] Get feed.
[02.10.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01051
[02.10.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01174
[02.10.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00526
[02.10.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00232
[02.10.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00184
[02.10.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00777
[02.10.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00553
[02.10.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00536
[02.10.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00510
[02.10.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2510.00406
[02.10.2025 03:24] Extract page data from URL. URL: https://huggingface.co/papers/2509.25411
[02.10.2025 03:24] Get page data from previous paper. URL: https://huggingface.co/papers/2509.23250
[02.10.2025 03:24] Extract page data from URL. URL: https://huggingface.co/papers/2509.19185
[02.10.2025 03:24] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[02.10.2025 03:24] No deleted papers detected.
[02.10.2025 03:24] Downloading and parsing papers (pdf, html). Total: 13.
[02.10.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2510.01051.
[02.10.2025 03:24] Extra JSON file exists (./assets/json/2510.01051.json), skip PDF parsing.
[02.10.2025 03:24] Paper image links file exists (./assets/img_data/2510.01051.json), skip HTML parsing.
[02.10.2025 03:24] Success.
[02.10.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2510.01174.
[02.10.2025 03:24] Extra JSON file exists (./assets/json/2510.01174.json), skip PDF parsing.
[02.10.2025 03:24] Paper image links file exists (./assets/img_data/2510.01174.json), skip HTML parsing.
[02.10.2025 03:24] Success.
[02.10.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2510.00526.
[02.10.2025 03:24] Extra JSON file exists (./assets/json/2510.00526.json), skip PDF parsing.
[02.10.2025 03:24] Paper image links file exists (./assets/img_data/2510.00526.json), skip HTML parsing.
[02.10.2025 03:24] Success.
[02.10.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2510.00232.
[02.10.2025 03:24] Extra JSON file exists (./assets/json/2510.00232.json), skip PDF parsing.
[02.10.2025 03:24] Paper image links file exists (./assets/img_data/2510.00232.json), skip HTML parsing.
[02.10.2025 03:24] Success.
[02.10.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2510.00184.
[02.10.2025 03:24] Downloading paper 2510.00184 from http://arxiv.org/pdf/2510.00184v1...
[02.10.2025 03:24] Failed to download and parse paper https://huggingface.co/papers/2510.00184: 'LTChar' object is not iterable
[02.10.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2510.00777.
[02.10.2025 03:24] Extra JSON file exists (./assets/json/2510.00777.json), skip PDF parsing.
[02.10.2025 03:24] Paper image links file exists (./assets/img_data/2510.00777.json), skip HTML parsing.
[02.10.2025 03:24] Success.
[02.10.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2510.00553.
[02.10.2025 03:24] Extra JSON file exists (./assets/json/2510.00553.json), skip PDF parsing.
[02.10.2025 03:24] Paper image links file exists (./assets/img_data/2510.00553.json), skip HTML parsing.
[02.10.2025 03:24] Success.
[02.10.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2510.00536.
[02.10.2025 03:24] Extra JSON file exists (./assets/json/2510.00536.json), skip PDF parsing.
[02.10.2025 03:24] Paper image links file exists (./assets/img_data/2510.00536.json), skip HTML parsing.
[02.10.2025 03:24] Success.
[02.10.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2510.00510.
[02.10.2025 03:24] Extra JSON file exists (./assets/json/2510.00510.json), skip PDF parsing.
[02.10.2025 03:24] Paper image links file exists (./assets/img_data/2510.00510.json), skip HTML parsing.
[02.10.2025 03:24] Success.
[02.10.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2510.00406.
[02.10.2025 03:24] Extra JSON file exists (./assets/json/2510.00406.json), skip PDF parsing.
[02.10.2025 03:24] Paper image links file exists (./assets/img_data/2510.00406.json), skip HTML parsing.
[02.10.2025 03:24] Success.
[02.10.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2509.25411.
[02.10.2025 03:24] Downloading paper 2509.25411 from http://arxiv.org/pdf/2509.25411v1...
[02.10.2025 03:24] Extracting affiliations from text.
[02.10.2025 03:24] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 1 1 4 5 2 . 9 0 5 2 : r a Zewei Zhang1 Huan Liu1 Yuanhao Yu1 1 McMaster University 2 Xian Jiaotong University Jun Chen1 Xiangyu Xu "
[02.10.2025 03:24] Response: ```python
["McMaster University", "Xian Jiaotong University"]
```
[02.10.2025 03:24] Deleting PDF ./assets/pdf/2509.25411.pdf.
[02.10.2025 03:24] Success.
[02.10.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2509.23250.
[02.10.2025 03:24] Extra JSON file exists (./assets/json/2509.23250.json), skip PDF parsing.
[02.10.2025 03:24] Paper image links file exists (./assets/img_data/2509.23250.json), skip HTML parsing.
[02.10.2025 03:24] Success.
[02.10.2025 03:24] Downloading and parsing paper https://huggingface.co/papers/2509.19185.
[02.10.2025 03:24] Downloading paper 2509.19185 from http://arxiv.org/pdf/2509.19185v2...
[02.10.2025 03:24] Extracting affiliations from text.
[02.10.2025 03:24] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Empirical Software Engineering manuscript No. (will be inserted by the editor) Mohammed Mehedi Hasan Hao Li Emad Fallahzadeh Gopi Krishnan Rajbahadur Bram Adams Ahmed E. Hassan Received: date / Accepted: date Abstract Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide tasklevel evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development. To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests. Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents. Keywords AI Agent LLM Agent Mining So"
[02.10.2025 03:24] Response: ```python
[]
```
[02.10.2025 03:24] Extracting affiliations from text.
[02.10.2025 03:24] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Empirical Software Engineering manuscript No. (will be inserted by the editor)Mohammed Mehedi Hasan Hao Li Emad Fallahzadeh Gopi Krishnan Rajbahadur Bram Adams Ahmed E. Hassan Received: date / Accepted: date Abstract Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide tasklevel evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development. To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests. Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents. Keywords AI Agent LLM Agent Mining Software Repositories Agentic Application Testing 5 2 0 2 4 2 ] . [ 2 5 8 1 9 1 . 9 0 5 2 : r Mohammed Mehedi Hasan and Hao Li and Emad Fallahzadeh and Gopi Krishnan Rajbahadur and Bram Adams and Ahmed E. Hassan School of Computing, Queens University, Kingston, ON, Canada {mohammedmehedi.hasan, E-mail: bram.adams@queensu.ca, ahmed@cs.queensu.ca hao.li,cj79}@queensu.ca, grajbahadur@acm.org,Mohammed Mehedi Hasan et al. Autonomous agents have long been foundational concept in AI, typically defined as systems capable of decision-making and action execution in an environment to fulfill given goals and objectives (Liu et al., 2023a). This classical idea has been supercharged by the advent of foundation models (FMs), giving rise to new paradigm: FM-based agentic applications. These agentic applications augment an FM brain with capabilities such as tool use, planning, and memory, enabling them to tackle complex, high-level goals with minimal human intervention (Hettiarachchi, 2025; Park et al., 2023). Modern agent frameworks further accelerate this trend, enabling developers to build sophisticated single and multi-agent applications that can reason, collaborate, and dynamically adapt their behavior (Wu et al., 2024a). With the proliferation of agent frameworks and agentic applications, along with the increasingly critical tasks agents are asked to perform, evaluating their effectiveness has become essential. The dominant evaluation culture, inherited from traditional AI, relies on standardized benchmarks that measure task performance (Liu et al., 2023a). For example, AgentBench introduced multi-dimensional benchmark with eight distinct interactive environments to systematically assess an LLM-asagents reasoning and decision-making abilities (Liu et al., 2023a). similar approach is followed by other popular benchmarks, e.g., GAIA (Mialon et al., 2023), WebArena (Zhou et al., 2023), and MINT (Wang et al., 2023). While valuable for objective comparison, this approach creates dangerous gap between perceived performance and real-world reliability. Benchmarks primarily test if an agent can succeed on set of pre-defined tasks, but not whether it is robust, safe, or dependable for an end users specific needs. For instance, an agent that tops leaderboard might still perform poorly when faced with edge cases, get stuck in buggy loops, produce hallucinations, or succumb to unhandled faults that benchmarks rarely account for (Weidinger et al., 2025). To address this reliability gap, practitioners increasingly adopt traditional software testing methods, e.g., unit testing, to verify agent behavior in controlled scenarios. Unlike benchmarks, unit tests not only verify that the software behaves as intended, but also expose edge cases and uncover newly introduced faults in code chunks that previously worked properly (Niedermayr et al., 2016). However, adapting traditional testing practices to FM-based agent frameworks and agentic applications is tricky due to the inherent non-determinism and non-reproducibility of the underlying models (Hassan et al., 2024). Surprisingly, this critical area remains almost entirely unexplored. While research has focused extensively on agent architectures, capabilities, and benchmarks, there is clear lack of empirical evidence on how practitioners actually test agent frameworks and agentic applications. We found no comprehensive studies examining the state-of-the-art testing practices for open-source agents: how developers structure tests, verify non-deterministic outputs, and which parts of the agent they prioritize while testing. These gaps are concerning, given that the potential impact of untested or unverified agents can be poor in real-world use cases. For example, previous studies have already reported that the performance of FM-based systems can deteriorate during model upgrades, and without testing the prompts and managing the versions, it is difficult to identify such silent performance degradation (Ma et al., 2024). Similarly, prior studies have reported at least seven error patterns while invoking tools in FM-based systems (Kokane et al., 2025), which can also lead to production issues if Testing Practices in AI Agent Frameworks and Agentic Applications 3 not appropriately tested during the development phase. To complement the findings from researchers, we have also observed that different components (e.g., prompts, tools) behave differently in different FMs or even on different versions of the same F"
[02.10.2025 03:24] Mistral response. {"id": "2264a4680ef747ea8940c8eaf6e252f2", "created": 1759375463, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1415, "total_tokens": 1437, "completion_tokens": 22}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"School of Computing, Queens University, Kingston, ON, Canada\"\n]\n```"}}]}
[02.10.2025 03:24] Response: ```python
[
    "School of Computing, Queens University, Kingston, ON, Canada"
]
```
[02.10.2025 03:24] Deleting PDF ./assets/pdf/2509.19185.pdf.
[02.10.2025 03:24] Success.
[02.10.2025 03:24] Enriching papers with extra data.
[02.10.2025 03:24] ********************************************************************************
[02.10.2025 03:24] Abstract 0. GEM, an open-source environment simulator, facilitates experience-based learning for large language models by providing a standardized framework and diverse environments for training and benchmarking reinforcement learning algorithms.  					AI-generated summary 				 The training paradigm for large l...
[02.10.2025 03:24] ********************************************************************************
[02.10.2025 03:24] Abstract 1. Code2Video generates educational videos using a code-centric agent framework, improving coherence and interpretability compared to direct code generation.  					AI-generated summary 				 While recent generative models advance pixel-space video synthesis, they remain limited in producing professional...
[02.10.2025 03:24] ********************************************************************************
[02.10.2025 03:24] Abstract 2. Research identifies probability-based objectives that outperform negative log likelihood for fine-tuning large language models, depending on model capability.  					AI-generated summary 				 Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it ...
[02.10.2025 03:24] ********************************************************************************
[02.10.2025 03:24] Abstract 3. BiasFreeBench evaluates bias mitigation techniques in large language models using a unified benchmark and response-level metric to ensure fair and safe outputs in real-world scenarios.  					AI-generated summary 				 Existing studies on bias mitigation methods for large language models (LLMs) use di...
[02.10.2025 03:24] ********************************************************************************
[02.10.2025 03:24] Abstract 4. Reverse-engineering a model that learns multi-digit multiplication via implicit chain-of-thought reveals that it uses attention to encode long-range dependencies and represents partial products efficiently, insights that help address limitations in standard fine-tuning.  					AI-generated summary 		...
[02.10.2025 03:24] ********************************************************************************
[02.10.2025 03:24] Abstract 5. In-place feedback allows users to directly edit LLM responses, improving performance and reducing token usage in multi-turn reasoning tasks.  					AI-generated summary 				 Large language models (LLMs) are increasingly studied in the context of multi-turn reasoning, where models iteratively refine t...
[02.10.2025 03:24] ********************************************************************************
[02.10.2025 03:24] Abstract 6. Two fundamental properties of reinforcement learning-induced parameter updates in large language models are identified, leading to a plug-in acceleration framework that significantly speeds up training without sacrificing performance.  					AI-generated summary 				 Recent advances in reasoning capa...
[02.10.2025 03:24] ********************************************************************************
[02.10.2025 03:24] Abstract 7. GUI-KV, a KV cache compression method for GUI agents, improves efficiency by exploiting spatial and temporal redundancies, reducing computational cost while maintaining accuracy.  					AI-generated summary 				 Graphical user interface (GUI) agents built on vision-language models have emerged as a p...
[02.10.2025 03:24] ********************************************************************************
[02.10.2025 03:24] Abstract 8. A generalist agent architecture combining multi-agent planning, hierarchical memory, and a refined tool suite outperforms existing systems in diverse tasks.  					AI-generated summary 				 Large Language Models are increasingly deployed as autonomous agents for complex real-world tasks, yet existing...
[02.10.2025 03:24] ********************************************************************************
[02.10.2025 03:24] Abstract 9. VLA-RFT uses a data-driven world model to fine-tune VLA models efficiently, reducing sample requirements and improving robustness under perturbations.  					AI-generated summary 				 Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading t...
[02.10.2025 03:24] ********************************************************************************
[02.10.2025 03:24] Abstract 10. ImitSAT, a branching policy for CDCL solvers using imitation learning from expert traces, reduces propagation counts and runtime by providing dense decision-level supervision.  					AI-generated summary 				 We propose ImitSAT, a branching policy for conflict-driven clause learning (CDCL) solvers ba...
[02.10.2025 03:24] ********************************************************************************
[02.10.2025 03:24] Abstract 11. Hybrid data synthesis and perception-focused supervision improve the reliability of Vision-Language Process Reward Models (VL-PRMs) in guiding VLMs across diverse multimodal benchmarks.  					AI-generated summary 				 Process Reward Models (PRMs) provide step-level supervision that improves the reli...
[02.10.2025 03:24] ********************************************************************************
[02.10.2025 03:24] Abstract 12. The study identifies testing practices in AI agent frameworks and applications, highlighting a focus on deterministic components and a neglect of the Trigger component, suggesting improvements for robustness.  					AI-generated summary 				 Foundation model (FM)-based AI agents are rapidly gaining a...
[02.10.2025 03:24] Read previous papers.
[02.10.2025 03:24] Generating reviews via LLM API.
[02.10.2025 03:24] Using data from previous issue: {"categories": ["#rl", "#open_source", "#benchmark", "#training", "#games", "#agents"], "emoji": "🎮", "ru": {"title": "GEM: спортзал для тренировки LLM-агентов через reinforcement learning", "desc": "В статье представлен GEM (General Experience Maker) — открытая среда-симулятор для обучения больших 
[02.10.2025 03:24] Using data from previous issue: {"categories": ["#interpretability", "#benchmark", "#video", "#games", "#optimization", "#agents", "#dataset"], "emoji": "🎓", "ru": {"title": "Генерация обучающих видео через программный код", "desc": "Code2Video — это фреймворк на основе агентов для создания образовательных видео через исполняемый 
[02.10.2025 03:24] Using data from previous issue: {"categories": ["#optimization", "#training"], "emoji": "🎯", "ru": {"title": "Выбор функции потерь зависит от силы модели", "desc": "Исследование показывает, что стандартная функция потерь negative log likelihood (NLL) не всегда оптимальна для файн-тюнинга больших языковых моделей. Авторы изучили се
[02.10.2025 03:24] Using data from previous issue: {"categories": ["#ethics", "#dataset", "#benchmark"], "emoji": "⚖️", "ru": {"title": "Единый бенчмарк для честной оценки методов борьбы с предвзятостью в LLM", "desc": "Существующие исследования методов устранения предвзятости в больших языковых моделях используют разные базовые подходы и метрики, ч
[02.10.2025 03:24] Using data from previous issue: {"categories": ["#training", "#architecture", "#long_context", "#reasoning"], "emoji": "🔢", "ru": {"title": "Как нейросети учатся умножать: разгадка механизма длинных зависимостей", "desc": "Исследователи провели обратную инженерию модели, которая научилась многозначному умножению через неявную цепо
[02.10.2025 03:24] Using data from previous issue: {"categories": ["#training", "#rlhf", "#interpretability", "#reasoning"], "emoji": "✏️", "ru": {"title": "Редактируй прямо здесь: эффективная обратная связь для LLM", "desc": "Исследователи предложили новый подход взаимодействия с языковыми моделями под названием in-place feedback, при котором польз
[02.10.2025 03:24] Using data from previous issue: {"categories": ["#training", "#rl", "#optimization", "#reasoning"], "emoji": "🚀", "ru": {"title": "Ускорение обучения LLM через ранговую экстраполяцию", "desc": "Исследование выявляет два фундаментальных свойства обновлений параметров в больших языковых моделях при reinforcement learning: доминирова
[02.10.2025 03:24] Using data from previous issue: {"categories": ["#optimization", "#inference", "#agents", "#benchmark"], "emoji": "🖥️", "ru": {"title": "Сжатие памяти для AI-агентов в графических интерфейсах", "desc": "Исследователи представили GUI-KV — метод сжатия KV-кэша для AI-агентов, работающих с графическими интерфейсами. Метод использует 
[02.10.2025 03:24] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#optimization", "#architecture", "#multimodal", "#agents", "#agi"], "emoji": "🤖", "ru": {"title": "Универсальный AI-агент через интеграцию памяти, планирования и инструментов", "desc": "В статье предлагается универсальная архитектура AI-агента, которая 
[02.10.2025 03:24] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#training", "#optimization", "#agents"], "emoji": "🤖", "ru": {"title": "Обучение робота через мир-симулятор: эффективно и надёжно", "desc": "Статья представляет VLA-RFT — метод дообучения Vision-Language-Action моделей с помощью reinforcement learning. Вместо до
[02.10.2025 03:24] Querying the API.
[02.10.2025 03:24] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ImitSAT, a branching policy for CDCL solvers using imitation learning from expert traces, reduces propagation counts and runtime by providing dense decision-level supervision.  					AI-generated summary 				 We propose ImitSAT, a branching policy for conflict-driven clause learning (CDCL) solvers based on imitation learning for the Boolean satisfiability problem (SAT). Unlike previous methods that predict instance-level signals to improve CDCL branching indirectly, or rely on reinforcement learning and insufficient CDCL information to enhance branching, ImitSAT learns from expert KeyTrace that collapses a full run into the sequence of surviving decisions. Replaying a KeyTrace on the same instance is nearly conflict-free, providing dense decision-level supervision and directly reducing propagations -- the dominant contributor to wall-clock time. This prefix-conditioned supervision enables ImitSAT to reproduce high-quality branches without exploration, yielding faster convergence, stable training, and seamless integration into CDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts and runtime, outperforming state-of-the-art learned approaches. We released the source code and trained model at https://github.com/zewei-Zhang/ImitSAT
[02.10.2025 03:24] Response: ```json
{
  "desc": "В статье представлен ImitSAT - новая стратегия ветвления для CDCL солверов SAT-задач, основанная на imitation learning. Метод обучается на экспертных трассах KeyTrace, которые представляют полный процесс решения как последовательность ключевых решений без конфликтов. Такой подход обеспечивает плотный supervised сигнал на уровне каждого решения и напрямую сокращает количество propagations - основной фактор времени выполнения. Эксперименты показывают, что ImitSAT превосходит современные методы на основе ML, сокращая время работы и количество propagations.",
  "emoji": "🎯",
  "title": "Обучение на экспертных трассах для ускорения SAT-солверов"
}
```
[02.10.2025 03:24] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ImitSAT, a branching policy for CDCL solvers using imitation learning from expert traces, reduces propagation counts and runtime by providing dense decision-level supervision.  					AI-generated summary 				 We propose ImitSAT, a branching policy for conflict-driven clause learning (CDCL) solvers based on imitation learning for the Boolean satisfiability problem (SAT). Unlike previous methods that predict instance-level signals to improve CDCL branching indirectly, or rely on reinforcement learning and insufficient CDCL information to enhance branching, ImitSAT learns from expert KeyTrace that collapses a full run into the sequence of surviving decisions. Replaying a KeyTrace on the same instance is nearly conflict-free, providing dense decision-level supervision and directly reducing propagations -- the dominant contributor to wall-clock time. This prefix-conditioned supervision enables ImitSAT to reproduce high-quality branches without exploration, yielding faster convergence, stable training, and seamless integration into CDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts and runtime, outperforming state-of-the-art learned approaches. We released the source code and trained model at https://github.com/zewei-Zhang/ImitSAT"

[02.10.2025 03:24] Response: ```python
['RL', 'TRAINING', 'MATH']
```
[02.10.2025 03:24] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ImitSAT, a branching policy for CDCL solvers using imitation learning from expert traces, reduces propagation counts and runtime by providing dense decision-level supervision.  					AI-generated summary 				 We propose ImitSAT, a branching policy for conflict-driven clause learning (CDCL) solvers based on imitation learning for the Boolean satisfiability problem (SAT). Unlike previous methods that predict instance-level signals to improve CDCL branching indirectly, or rely on reinforcement learning and insufficient CDCL information to enhance branching, ImitSAT learns from expert KeyTrace that collapses a full run into the sequence of surviving decisions. Replaying a KeyTrace on the same instance is nearly conflict-free, providing dense decision-level supervision and directly reducing propagations -- the dominant contributor to wall-clock time. This prefix-conditioned supervision enables ImitSAT to reproduce high-quality branches without exploration, yielding faster convergence, stable training, and seamless integration into CDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts and runtime, outperforming state-of-the-art learned approaches. We released the source code and trained model at https://github.com/zewei-Zhang/ImitSAT"

[02.10.2025 03:24] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```
[02.10.2025 03:24] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ImitSAT is a new branching policy designed for conflict-driven clause learning (CDCL) solvers that uses imitation learning from expert traces to improve performance on the Boolean satisfiability problem (SAT). Unlike traditional methods that rely on indirect signals or reinforcement learning, ImitSAT directly learns from a sequence of expert decisions, known as KeyTrace, which simplifies the decision-making process. This approach minimizes conflicts during execution, leading to fewer propagation counts and reduced runtime. The results show that ImitSAT significantly outperforms existing learned methods, providing a more efficient and effective solution for SAT problems.","title":"ImitSAT: Learning from Experts for Faster SAT Solving"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ImitSAT is a new branching policy designed for conflict-driven clause learning (CDCL) solvers that uses imitation learning from expert traces to improve performance on the Boolean satisfiability problem (SAT). Unlike traditional methods that rely on indirect signals or reinforcement learning, ImitSAT directly learns from a sequence of expert decisions, known as KeyTrace, which simplifies the decision-making process. This approach minimizes conflicts during execution, leading to fewer propagation counts and reduced runtime. The results show that ImitSAT significantly outperforms existing learned methods, providing a more efficient and effective solution for SAT problems.', title='ImitSAT: Learning from Experts for Faster SAT Solving'))
[02.10.2025 03:24] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ImitSAT是一种基于模仿学习的分支策略，专为冲突驱动子句学习（CDCL）求解器设计，旨在解决布尔可满足性问题（SAT）。与以往方法不同，ImitSAT通过学习专家的KeyTrace，直接提供决策级的监督，从而减少传播次数和运行时间。通过在同一实例上重放KeyTrace，ImitSAT几乎没有冲突，显著提高了分支的质量和收敛速度。大量实验表明，ImitSAT在传播次数和运行时间上优于现有的最先进学习方法。","title":"ImitSAT：高效的CDCL求解器分支策略"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ImitSAT是一种基于模仿学习的分支策略，专为冲突驱动子句学习（CDCL）求解器设计，旨在解决布尔可满足性问题（SAT）。与以往方法不同，ImitSAT通过学习专家的KeyTrace，直接提供决策级的监督，从而减少传播次数和运行时间。通过在同一实例上重放KeyTrace，ImitSAT几乎没有冲突，显著提高了分支的质量和收敛速度。大量实验表明，ImitSAT在传播次数和运行时间上优于现有的最先进学习方法。', title='ImitSAT：高效的CDCL求解器分支策略'))
[02.10.2025 03:24] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#training", "#games", "#multimodal", "#data", "#dataset"], "emoji": "👁️", "ru": {"title": "Учим мультимодальные модели проверять свои рассуждения пошагово", "desc": "Статья представляет улучшенные Vision-Language Process Reward Models (VL-PRMs), которые о
[02.10.2025 03:24] Querying the API.
[02.10.2025 03:24] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The study identifies testing practices in AI agent frameworks and applications, highlighting a focus on deterministic components and a neglect of the Trigger component, suggesting improvements for robustness.  					AI-generated summary 				 Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide task-level evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development.   To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover a fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals a critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests.   Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing a rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents.
[02.10.2025 03:24] Response: ```json
{
  "title": "Тестирование AI-агентов: фокус не там, где нужно",
  "desc": "Исследование анализирует практики тестирования в 39 фреймворках AI-агентов и 439 приложениях, выявляя десять паттернов тестирования. Обнаружена критическая проблема: более 70% усилий по тестированию направлено на детерминированные компоненты (инструменты и воркфлоу), в то время как компоненты на основе foundation models получают менее 5% внимания. Особенно проблематично, что промпты (Trigger component) практически не тестируются, появляясь лишь в 1% тестов. Авторы призывают разработчиков фреймворков улучшить поддержку новых методов тестирования, а разработчиков приложений — внедрить regression-тестирование промптов для повышения надёжности AI-агентов.",
  "emoji": "🔍",
  "desc_en": ""
}
```
[02.10.2025 03:24] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The study identifies testing practices in AI agent frameworks and applications, highlighting a focus on deterministic components and a neglect of the Trigger component, suggesting improvements for robustness.  					AI-generated summary 				 Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide task-level evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development.   To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover a fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals a critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests.   Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing a rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents."

[02.10.2025 03:24] Response: ```python
['AGENTS', 'BENCHMARK', 'TRAINING']
```
[02.10.2025 03:24] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The study identifies testing practices in AI agent frameworks and applications, highlighting a focus on deterministic components and a neglect of the Trigger component, suggesting improvements for robustness.  					AI-generated summary 				 Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide task-level evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development.   To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover a fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals a critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests.   Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing a rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents."

[02.10.2025 03:24] Response: ```python
["SECURITY", "OPEN_SOURCE"]
```
[02.10.2025 03:24] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates the testing practices used in AI agent frameworks and applications, revealing a significant focus on deterministic components while largely neglecting the Trigger component. The study analyzes 39 open-source frameworks and 439 applications, identifying ten distinct testing patterns, with traditional methods dominating the landscape. It highlights that over 70% of testing effort is spent on deterministic components, while less than 5% is allocated to the FM-based Plan Body, indicating a critical oversight. The authors suggest that improving testing methods and incorporating prompt regression testing are essential for enhancing the robustness of AI agents.","title":"Enhancing AI Agent Testing: Bridging the Gap in Robustness"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates the testing practices used in AI agent frameworks and applications, revealing a significant focus on deterministic components while largely neglecting the Trigger component. The study analyzes 39 open-source frameworks and 439 applications, identifying ten distinct testing patterns, with traditional methods dominating the landscape. It highlights that over 70% of testing effort is spent on deterministic components, while less than 5% is allocated to the FM-based Plan Body, indicating a critical oversight. The authors suggest that improving testing methods and incorporating prompt regression testing are essential for enhancing the robustness of AI agents.', title='Enhancing AI Agent Testing: Bridging the Gap in Robustness'))
[02.10.2025 03:24] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本研究分析了人工智能代理框架和应用中的测试实践，发现目前的测试主要集中在确定性组件上，而触发组件却被忽视。我们对39个开源代理框架和439个代理应用进行了大规模实证研究，识别出十种不同的测试模式。结果显示，像DeepEval这样的新型代理特定方法使用率极低，而传统的负面测试和成员测试被广泛应用以应对基础模型的不确定性。为了提高AI代理的鲁棒性，开发者需要改进对新测试方法的支持，并在应用中采用提示回归测试。","title":"提升AI代理测试的鲁棒性"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本研究分析了人工智能代理框架和应用中的测试实践，发现目前的测试主要集中在确定性组件上，而触发组件却被忽视。我们对39个开源代理框架和439个代理应用进行了大规模实证研究，识别出十种不同的测试模式。结果显示，像DeepEval这样的新型代理特定方法使用率极低，而传统的负面测试和成员测试被广泛应用以应对基础模型的不确定性。为了提高AI代理的鲁棒性，开发者需要改进对新测试方法的支持，并在应用中采用提示回归测试。', title='提升AI代理测试的鲁棒性'))
[02.10.2025 03:24] Renaming data file.
[02.10.2025 03:24] Renaming previous data. hf_papers.json to ./d/2025-10-02.json
[02.10.2025 03:24] Saving new data file.
[02.10.2025 03:24] Generating page.
[02.10.2025 03:24] Renaming previous page.
[02.10.2025 03:24] Renaming previous data. index.html to ./d/2025-10-02.html
[02.10.2025 03:24] Writing result.
[02.10.2025 03:24] Renaming log file.
[02.10.2025 03:24] Renaming previous data. log.txt to ./logs/2025-10-02_last_log.txt
