[16.07.2025 18:17] Read previous papers.
[16.07.2025 18:17] Generating top page (month).
[16.07.2025 18:17] Writing top page (month).
[16.07.2025 19:12] Read previous papers.
[16.07.2025 19:12] Get feed.
[16.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07104
[16.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11407
[16.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09404
[16.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.10787
[16.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09075
[16.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08616
[16.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08333
[16.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09411
[16.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11336
[16.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07186
[16.07.2025 19:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.10571
[16.07.2025 19:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.07.2025 19:12] No deleted papers detected.
[16.07.2025 19:12] Downloading and parsing papers (pdf, html). Total: 11.
[16.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.07104.
[16.07.2025 19:12] Extra JSON file exists (./assets/json/2507.07104.json), skip PDF parsing.
[16.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.07104.json), skip HTML parsing.
[16.07.2025 19:12] Success.
[16.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.11407.
[16.07.2025 19:12] Extra JSON file exists (./assets/json/2507.11407.json), skip PDF parsing.
[16.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.11407.json), skip HTML parsing.
[16.07.2025 19:12] Success.
[16.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.09404.
[16.07.2025 19:12] Downloading paper 2507.09404 from http://arxiv.org/pdf/2507.09404v1...
[16.07.2025 19:12] Failed to download and parse paper https://huggingface.co/papers/2507.09404: 'LTChar' object is not iterable
[16.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.10787.
[16.07.2025 19:12] Extra JSON file exists (./assets/json/2507.10787.json), skip PDF parsing.
[16.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.10787.json), skip HTML parsing.
[16.07.2025 19:12] Success.
[16.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.09075.
[16.07.2025 19:12] Extra JSON file exists (./assets/json/2507.09075.json), skip PDF parsing.
[16.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.09075.json), skip HTML parsing.
[16.07.2025 19:12] Success.
[16.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.08616.
[16.07.2025 19:12] Extra JSON file exists (./assets/json/2507.08616.json), skip PDF parsing.
[16.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.08616.json), skip HTML parsing.
[16.07.2025 19:12] Success.
[16.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.08333.
[16.07.2025 19:12] Extra JSON file exists (./assets/json/2507.08333.json), skip PDF parsing.
[16.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.08333.json), skip HTML parsing.
[16.07.2025 19:12] Success.
[16.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.09411.
[16.07.2025 19:12] Extra JSON file exists (./assets/json/2507.09411.json), skip PDF parsing.
[16.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.09411.json), skip HTML parsing.
[16.07.2025 19:12] Success.
[16.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.11336.
[16.07.2025 19:12] Extra JSON file exists (./assets/json/2507.11336.json), skip PDF parsing.
[16.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.11336.json), skip HTML parsing.
[16.07.2025 19:12] Success.
[16.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.07186.
[16.07.2025 19:12] Extra JSON file exists (./assets/json/2507.07186.json), skip PDF parsing.
[16.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.07186.json), skip HTML parsing.
[16.07.2025 19:12] Success.
[16.07.2025 19:12] Downloading and parsing paper https://huggingface.co/papers/2507.10571.
[16.07.2025 19:12] Extra JSON file exists (./assets/json/2507.10571.json), skip PDF parsing.
[16.07.2025 19:12] Paper image links file exists (./assets/img_data/2507.10571.json), skip HTML parsing.
[16.07.2025 19:12] Success.
[16.07.2025 19:12] Enriching papers with extra data.
[16.07.2025 19:12] ********************************************************************************
[16.07.2025 19:12] Abstract 0. The VLV auto-encoder framework uses pretrained vision and text models to create a cost-effective and data-efficient captioning system.  					AI-generated summary 				 Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billio...
[16.07.2025 19:12] ********************************************************************************
[16.07.2025 19:12] Abstract 1. EXAONE 4.0 integrates non-reasoning and reasoning modes, supports multilingualism, and offers models optimized for high performance and on-device use, demonstrating superior performance compared to open-weight models.  					AI-generated summary 				 This technical report introduces EXAONE 4.0, which...
[16.07.2025 19:12] ********************************************************************************
[16.07.2025 19:12] Abstract 2. Scaling laws predict optimal data mixtures for large foundation models, improving performance across different domains and scales.  					AI-generated summary 				 Large foundation models are typically trained on data from multiple domains, with the data mixture--the proportion of each domain used--p...
[16.07.2025 19:12] ********************************************************************************
[16.07.2025 19:12] Abstract 3. A benchmark evaluates multimodal models' ability to interpret scientific schematic diagrams and answer related questions, revealing performance gaps and insights for improvement.  					AI-generated summary 				 This paper introduces MISS-QA, the first benchmark specifically designed to evaluate the ...
[16.07.2025 19:12] ********************************************************************************
[16.07.2025 19:12] Abstract 4. Recent advancements in reasoning-based Large Language Models (LLMs), particularly their potential through test-time scaling, have created significant opportunities for distillation in code generation and critique. However, progress in both areas fundamentally depends on large-scale, high-quality dat...
[16.07.2025 19:12] ********************************************************************************
[16.07.2025 19:12] Abstract 5. AgentsNet is a new benchmark for evaluating multi-agent systems' ability to self-organize, communicate, and solve problems collaboratively across varying network sizes.  					AI-generated summary 				 Large-language models (LLMs) have demonstrated powerful problem-solving capabilities, in particular...
[16.07.2025 19:12] ********************************************************************************
[16.07.2025 19:12] Abstract 6. A discrete diffusion model for audio inpainting using tokenized audio representations achieves competitive performance for reconstructing long gaps in corrupted audio recordings.  					AI-generated summary 				 Audio inpainting refers to the task of reconstructing missing segments in corrupted audio...
[16.07.2025 19:12] ********************************************************************************
[16.07.2025 19:12] Abstract 7. A semi-automated framework uses Large Language Models to generate malware variants, demonstrating reduced detection rates and notable attack success against ML classifiers.  					AI-generated summary 				 Large Language Models (LLMs) have transformed software development and automated code generatio...
[16.07.2025 19:12] ********************************************************************************
[16.07.2025 19:12] Abstract 8. UGC-VideoCap introduces a new benchmark and model for detailed omnimodal captioning of user-generated videos, emphasizing audio-visual integration and using a novel training strategy.  					AI-generated summary 				 Real-world user-generated videos, especially on platforms like TikTok, often feature...
[16.07.2025 19:12] ********************************************************************************
[16.07.2025 19:12] Abstract 9. Research identifies pretraining as the primary source of cognitive biases in large language models, distinguishing its influence from finetuning and training randomness.  					AI-generated summary 				 Large language models (LLMs) exhibit cognitive biases -- systematic tendencies of irrational decis...
[16.07.2025 19:12] ********************************************************************************
[16.07.2025 19:12] Abstract 10. A modular Agentic AI framework integrates multimodal agents with a reasoning orchestrator and RAG module to improve trust and accuracy in zero-shot visual classification tasks like apple leaf disease diagnosis.  					AI-generated summary 				 Modern Artificial Intelligence (AI) increasingly relies o...
[16.07.2025 19:12] Read previous papers.
[16.07.2025 19:12] Generating reviews via LLM API.
[16.07.2025 19:12] Using data from previous issue: {"categories": ["#data", "#optimization", "#architecture", "#multimodal", "#transfer_learning", "#dataset", "#training"], "emoji": "üî¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –±–µ–∑ –æ–≥—Ä–æ–º–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ VLV auto-encoder –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç—Ñ
[16.07.2025 19:12] Using data from previous issue: {"categories": ["#open_source", "#small_models", "#agents", "#multilingual", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "EXAONE 4.0: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏ –≤ –ò–ò –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "EXAONE 4.0 - —ç—Ç–æ –Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —Ä–µ–∂–∏–º—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –Ω–µ—Ä–∞—Å—Å—É–∂–¥–µ–Ω
[16.07.2025 19:12] Using data from previous issue: {"categories": ["#data", "#optimization", "#training", "#multimodal"], "emoji": "üî¨", "ru": {"title": "–ó–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥
[16.07.2025 19:12] Using data from previous issue: {"categories": ["#multimodal", "#science", "#interpretability", "#benchmark"], "emoji": "üî¨", "ru": {"title": "MISS-QA: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MISS-QA - –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ö–µ–º–∞—Ç–∏—á
[16.07.2025 19:12] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#reasoning", "#training", "#optimization", "#plp"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–º–Ω—ã—Ö –∫–æ–¥–µ—Ä–æ–≤: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –ò–ò –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OpenCodeReasoning-II - –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö 
[16.07.2025 19:12] Using data from previous issue: {"categories": ["#benchmark", "#graphs", "#agents", "#reasoning", "#games"], "emoji": "üï∏Ô∏è", "ru": {"title": "AgentsNet: –û—Ü–µ–Ω–∫–∞ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "AgentsNet - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∫ —Å–∞–º–æ–æ—Ä–≥–∞–Ω–∏–∑–∞
[16.07.2025 19:12] Using data from previous issue: {"categories": ["#audio", "#diffusion"], "emoji": "üéµ", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –∞—É–¥–∏–æ–∑–∞–ø–∏—Å—è—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ
[16.07.2025 19:12] Using data from previous issue: {"categories": ["#dataset", "#security", "#agents", "#data"], "emoji": "ü¶†", "ru": {"title": "LLM –Ω–∞ —Å–ª—É–∂–±–µ –∫–∏–±–µ—Ä–ø—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –ü–û", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –ø–æ–ª—É–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É LLMalMorph, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏
[16.07.2025 19:12] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#multimodal", "#video", "#training", "#transfer_learning", "#benchmark"], "emoji": "üé•", "ru": {"title": "–û–º–Ω–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –≤–∏–¥–µ–æ: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "UGC-VideoCap –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π
[16.07.2025 19:12] Using data from previous issue: {"categories": ["#hallucinations", "#training", "#ethics"], "emoji": "üß†", "ru": {"title": "–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ - –∫–ª—é—á –∫ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–º –∏—Å–∫–∞–∂–µ–Ω–∏—è–º –≤ –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –∏—Å–∫–∞–∂–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∞ –Ω–µ 
[16.07.2025 19:12] Using data from previous issue: {"categories": ["#reasoning", "#rag", "#open_source", "#multimodal", "#interpretability", "#agents", "#benchmark"], "emoji": "üçé", "ru": {"title": "–î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–∞—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥—É–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ò–ò –¥–ª
[16.07.2025 19:12] Renaming data file.
[16.07.2025 19:12] Renaming previous data. hf_papers.json to ./d/2025-07-16.json
[16.07.2025 19:12] Saving new data file.
[16.07.2025 19:12] Generating page.
[16.07.2025 19:12] Renaming previous page.
[16.07.2025 19:12] Renaming previous data. index.html to ./d/2025-07-16.html
[16.07.2025 19:12] Writing result.
[16.07.2025 19:12] Renaming log file.
[16.07.2025 19:12] Renaming previous data. log.txt to ./logs/2025-07-16_last_log.txt
