[16.07.2025 10:13] Read previous papers.
[16.07.2025 10:13] Generating top page (month).
[16.07.2025 10:13] Writing top page (month).
[16.07.2025 11:11] Read previous papers.
[16.07.2025 11:11] Get feed.
[16.07.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07104
[16.07.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11407
[16.07.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09404
[16.07.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.10787
[16.07.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09411
[16.07.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09075
[16.07.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08616
[16.07.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07186
[16.07.2025 11:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.07.2025 11:11] No deleted papers detected.
[16.07.2025 11:11] Downloading and parsing papers (pdf, html). Total: 8.
[16.07.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2507.07104.
[16.07.2025 11:11] Extra JSON file exists (./assets/json/2507.07104.json), skip PDF parsing.
[16.07.2025 11:11] Paper image links file exists (./assets/img_data/2507.07104.json), skip HTML parsing.
[16.07.2025 11:11] Success.
[16.07.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2507.11407.
[16.07.2025 11:11] Extra JSON file exists (./assets/json/2507.11407.json), skip PDF parsing.
[16.07.2025 11:11] Paper image links file exists (./assets/img_data/2507.11407.json), skip HTML parsing.
[16.07.2025 11:11] Success.
[16.07.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2507.09404.
[16.07.2025 11:11] Downloading paper 2507.09404 from http://arxiv.org/pdf/2507.09404v1...
[16.07.2025 11:11] Failed to download and parse paper https://huggingface.co/papers/2507.09404: 'LTChar' object is not iterable
[16.07.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2507.10787.
[16.07.2025 11:11] Extra JSON file exists (./assets/json/2507.10787.json), skip PDF parsing.
[16.07.2025 11:11] Paper image links file exists (./assets/img_data/2507.10787.json), skip HTML parsing.
[16.07.2025 11:11] Success.
[16.07.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2507.09411.
[16.07.2025 11:11] Extra JSON file exists (./assets/json/2507.09411.json), skip PDF parsing.
[16.07.2025 11:11] Paper image links file exists (./assets/img_data/2507.09411.json), skip HTML parsing.
[16.07.2025 11:11] Success.
[16.07.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2507.09075.
[16.07.2025 11:11] Extra JSON file exists (./assets/json/2507.09075.json), skip PDF parsing.
[16.07.2025 11:11] Paper image links file exists (./assets/img_data/2507.09075.json), skip HTML parsing.
[16.07.2025 11:11] Success.
[16.07.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2507.08616.
[16.07.2025 11:11] Extra JSON file exists (./assets/json/2507.08616.json), skip PDF parsing.
[16.07.2025 11:11] Paper image links file exists (./assets/img_data/2507.08616.json), skip HTML parsing.
[16.07.2025 11:11] Success.
[16.07.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2507.07186.
[16.07.2025 11:11] Extra JSON file exists (./assets/json/2507.07186.json), skip PDF parsing.
[16.07.2025 11:11] Paper image links file exists (./assets/img_data/2507.07186.json), skip HTML parsing.
[16.07.2025 11:11] Success.
[16.07.2025 11:11] Enriching papers with extra data.
[16.07.2025 11:11] ********************************************************************************
[16.07.2025 11:11] Abstract 0. The VLV auto-encoder framework uses pretrained vision and text models to create a cost-effective and data-efficient captioning system.  					AI-generated summary 				 Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billio...
[16.07.2025 11:11] ********************************************************************************
[16.07.2025 11:11] Abstract 1. EXAONE 4.0 integrates non-reasoning and reasoning modes, supports multilingualism, and offers models optimized for high performance and on-device use, demonstrating superior performance compared to open-weight models.  					AI-generated summary 				 This technical report introduces EXAONE 4.0, which...
[16.07.2025 11:11] ********************************************************************************
[16.07.2025 11:11] Abstract 2. Scaling laws predict optimal data mixtures for large foundation models, improving performance across different domains and scales.  					AI-generated summary 				 Large foundation models are typically trained on data from multiple domains, with the data mixture--the proportion of each domain used--p...
[16.07.2025 11:11] ********************************************************************************
[16.07.2025 11:11] Abstract 3. A benchmark evaluates multimodal models' ability to interpret scientific schematic diagrams and answer related questions, revealing performance gaps and insights for improvement.  					AI-generated summary 				 This paper introduces MISS-QA, the first benchmark specifically designed to evaluate the ...
[16.07.2025 11:11] ********************************************************************************
[16.07.2025 11:11] Abstract 4. A semi-automated framework uses Large Language Models to generate malware variants, demonstrating reduced detection rates and notable attack success against ML classifiers.  					AI-generated summary 				 Large Language Models (LLMs) have transformed software development and automated code generatio...
[16.07.2025 11:11] ********************************************************************************
[16.07.2025 11:11] Abstract 5. Recent advancements in reasoning-based Large Language Models (LLMs), particularly their potential through test-time scaling, have created significant opportunities for distillation in code generation and critique. However, progress in both areas fundamentally depends on large-scale, high-quality dat...
[16.07.2025 11:11] ********************************************************************************
[16.07.2025 11:11] Abstract 6. AgentsNet is a new benchmark for evaluating multi-agent systems' ability to self-organize, communicate, and solve problems collaboratively across varying network sizes.  					AI-generated summary 				 Large-language models (LLMs) have demonstrated powerful problem-solving capabilities, in particular...
[16.07.2025 11:11] ********************************************************************************
[16.07.2025 11:11] Abstract 7. Research identifies pretraining as the primary source of cognitive biases in large language models, distinguishing its influence from finetuning and training randomness.  					AI-generated summary 				 Large language models (LLMs) exhibit cognitive biases -- systematic tendencies of irrational decis...
[16.07.2025 11:11] Read previous papers.
[16.07.2025 11:11] Generating reviews via LLM API.
[16.07.2025 11:11] Using data from previous issue: {"categories": ["#data", "#optimization", "#architecture", "#multimodal", "#transfer_learning", "#dataset", "#training"], "emoji": "üî¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –±–µ–∑ –æ–≥—Ä–æ–º–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ VLV auto-encoder –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç—Ñ
[16.07.2025 11:11] Using data from previous issue: {"categories": ["#open_source", "#small_models", "#agents", "#multilingual", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "EXAONE 4.0: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏ –≤ –ò–ò –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "EXAONE 4.0 - —ç—Ç–æ –Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —Ä–µ–∂–∏–º—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –Ω–µ—Ä–∞—Å—Å—É–∂–¥–µ–Ω
[16.07.2025 11:11] Using data from previous issue: {"categories": ["#data", "#optimization", "#training", "#multimodal"], "emoji": "üî¨", "ru": {"title": "–ó–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥
[16.07.2025 11:11] Using data from previous issue: {"categories": ["#multimodal", "#science", "#interpretability", "#benchmark"], "emoji": "üî¨", "ru": {"title": "MISS-QA: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MISS-QA - –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ö–µ–º–∞—Ç–∏—á
[16.07.2025 11:11] Using data from previous issue: {"categories": ["#dataset", "#security", "#agents", "#data"], "emoji": "ü¶†", "ru": {"title": "LLM –Ω–∞ —Å–ª—É–∂–±–µ –∫–∏–±–µ—Ä–ø—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –ü–û", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –ø–æ–ª—É–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É LLMalMorph, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏
[16.07.2025 11:11] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#reasoning", "#training", "#optimization", "#plp"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–º–Ω—ã—Ö –∫–æ–¥–µ—Ä–æ–≤: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –ò–ò –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OpenCodeReasoning-II - –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö 
[16.07.2025 11:11] Using data from previous issue: {"categories": ["#benchmark", "#graphs", "#agents", "#reasoning", "#games"], "emoji": "üï∏Ô∏è", "ru": {"title": "AgentsNet: –û—Ü–µ–Ω–∫–∞ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "AgentsNet - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∫ —Å–∞–º–æ–æ—Ä–≥–∞–Ω–∏–∑–∞
[16.07.2025 11:11] Using data from previous issue: {"categories": ["#hallucinations", "#training", "#ethics"], "emoji": "üß†", "ru": {"title": "–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ - –∫–ª—é—á –∫ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–º –∏—Å–∫–∞–∂–µ–Ω–∏—è–º –≤ –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –∏—Å–∫–∞–∂–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∞ –Ω–µ 
[16.07.2025 11:11] Renaming data file.
[16.07.2025 11:11] Renaming previous data. hf_papers.json to ./d/2025-07-16.json
[16.07.2025 11:11] Saving new data file.
[16.07.2025 11:11] Generating page.
[16.07.2025 11:11] Renaming previous page.
[16.07.2025 11:11] Renaming previous data. index.html to ./d/2025-07-16.html
[16.07.2025 11:11] Writing result.
[16.07.2025 11:11] Renaming log file.
[16.07.2025 11:11] Renaming previous data. log.txt to ./logs/2025-07-16_last_log.txt
