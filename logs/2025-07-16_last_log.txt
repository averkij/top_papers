[16.07.2025 22:12] Read previous papers.
[16.07.2025 22:12] Generating top page (month).
[16.07.2025 22:12] Writing top page (month).
[16.07.2025 23:12] Read previous papers.
[16.07.2025 23:12] Get feed.
[16.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07104
[16.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11407
[16.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09404
[16.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.10787
[16.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09075
[16.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08616
[16.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08333
[16.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09411
[16.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11336
[16.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07186
[16.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.10571
[16.07.2025 23:12] Extract page data from URL. URL: https://huggingface.co/papers/2507.09082
[16.07.2025 23:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.04127
[16.07.2025 23:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.07.2025 23:12] No deleted papers detected.
[16.07.2025 23:12] Downloading and parsing papers (pdf, html). Total: 13.
[16.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.07104.
[16.07.2025 23:12] Extra JSON file exists (./assets/json/2507.07104.json), skip PDF parsing.
[16.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.07104.json), skip HTML parsing.
[16.07.2025 23:12] Success.
[16.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.11407.
[16.07.2025 23:12] Extra JSON file exists (./assets/json/2507.11407.json), skip PDF parsing.
[16.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.11407.json), skip HTML parsing.
[16.07.2025 23:12] Success.
[16.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.09404.
[16.07.2025 23:12] Downloading paper 2507.09404 from http://arxiv.org/pdf/2507.09404v1...
[16.07.2025 23:12] Failed to download and parse paper https://huggingface.co/papers/2507.09404: 'LTChar' object is not iterable
[16.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.10787.
[16.07.2025 23:12] Extra JSON file exists (./assets/json/2507.10787.json), skip PDF parsing.
[16.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.10787.json), skip HTML parsing.
[16.07.2025 23:12] Success.
[16.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.09075.
[16.07.2025 23:12] Extra JSON file exists (./assets/json/2507.09075.json), skip PDF parsing.
[16.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.09075.json), skip HTML parsing.
[16.07.2025 23:12] Success.
[16.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.08616.
[16.07.2025 23:12] Extra JSON file exists (./assets/json/2507.08616.json), skip PDF parsing.
[16.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.08616.json), skip HTML parsing.
[16.07.2025 23:12] Success.
[16.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.08333.
[16.07.2025 23:12] Extra JSON file exists (./assets/json/2507.08333.json), skip PDF parsing.
[16.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.08333.json), skip HTML parsing.
[16.07.2025 23:12] Success.
[16.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.09411.
[16.07.2025 23:12] Extra JSON file exists (./assets/json/2507.09411.json), skip PDF parsing.
[16.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.09411.json), skip HTML parsing.
[16.07.2025 23:12] Success.
[16.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.11336.
[16.07.2025 23:12] Extra JSON file exists (./assets/json/2507.11336.json), skip PDF parsing.
[16.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.11336.json), skip HTML parsing.
[16.07.2025 23:12] Success.
[16.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.07186.
[16.07.2025 23:12] Extra JSON file exists (./assets/json/2507.07186.json), skip PDF parsing.
[16.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.07186.json), skip HTML parsing.
[16.07.2025 23:12] Success.
[16.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.10571.
[16.07.2025 23:12] Extra JSON file exists (./assets/json/2507.10571.json), skip PDF parsing.
[16.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.10571.json), skip HTML parsing.
[16.07.2025 23:12] Success.
[16.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.09082.
[16.07.2025 23:12] Downloading paper 2507.09082 from http://arxiv.org/pdf/2507.09082v1...
[16.07.2025 23:12] Extracting affiliations from text.
[16.07.2025 23:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Taming generative video models for zero-shot optical flow extraction Seungwoo Kim* Khai Loong Aw* Klemen Kotar* Cristobal Eyzaguirre Wanhee Lee Yunong Liu Juan Carlos Niebles Jiajun Wu Daniel L.K. Yamins Jared Watrous Stefan Stojanov 5 2 0 2 1 1 ] . [ 1 2 8 0 9 0 . 7 0 5 2 : r a Figure 1: We introduce zero-shot test-time inference procedure called KL-tracing, which extracts robust optical flow and point tracking from generative world model on challenging in-the-wild videos. In every column, the green line links the query location in the first frame (top) to the position predicted by our method in the second frame (bottom). All clips are real-world internet videos and contain phenomena that classical, appearance-based optical flow methods find challenging: (A) Newtons cradle, where both frames have four balls in the middle, but the balls are different; the example involves physical reasoning. (B) Globe has challenging in-place object rotation and the query point is in the textureless ocean. (C) Dog weaving through occluding poles with large, rapid motion, including depth changes and motion blur. (D) Soccer tackle with fast, diagonal motion with motion blur and partial occlusion. (E) Windmill rotation where the repetitive blades and uniform sky make local matching challenging. These examples highlight the benefits of leveraging powerful world model to extract optical flow for challenging real-world scene dynamics. "
[16.07.2025 23:12] Response: ```python
[]
```
[16.07.2025 23:12] Extracting affiliations from text.
[16.07.2025 23:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Taming generative video models for zero-shot optical flow extraction Seungwoo Kim* Khai Loong Aw* Klemen Kotar* Cristobal Eyzaguirre Wanhee Lee Yunong Liu Juan Carlos Niebles Jiajun Wu Daniel L.K. Yamins Jared Watrous Stefan Stojanov 5 2 0 2 1 1 ] . [ 1 2 8 0 9 0 . 7 0 5 2 : r aFigure 1: We introduce zero-shot test-time inference procedure called KL-tracing, which extracts robust optical flow and point tracking from generative world model on challenging in-the-wild videos. In every column, the green line links the query location in the first frame (top) to the position predicted by our method in the second frame (bottom). All clips are real-world internet videos and contain phenomena that classical, appearance-based optical flow methods find challenging: (A) Newtons cradle, where both frames have four balls in the middle, but the balls are different; the example involves physical reasoning. (B) Globe has challenging in-place object rotation and the query point is in the textureless ocean. (C) Dog weaving through occluding poles with large, rapid motion, including depth changes and motion blur. (D) Soccer tackle with fast, diagonal motion with motion blur and partial occlusion. (E) Windmill rotation where the repetitive blades and uniform sky make local matching challenging. These examples highlight the benefits of leveraging powerful world model to extract optical flow for challenging real-world scene dynamics.Extracting optical flow from videos remains core computer vision problem. Motivated by the recent success of large general-purpose models, we ask whether frozen self-supervised video models trained only to predict future frames can be prompted, without fine-tuning, to output flow. Prior attempts to read out depth or illumination from video generators required fine-tuning; that strategy is ill-suited for flow, where labeled data is scarce and synthetic datasets suffer from sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm, which can obtain point-wise correspondences by injecting small tracer perturbation into next-frame predictor and tracking its propagation, we extend this idea to generative video models for zero-shot flow extraction. We explore several popular architectures and find that successful zero-shot flow extraction in this manner is aided by three model properties: (1) distributional prediction of future frames (avoiding blurry or noisy outputs); (2) factorized latents that treat each spatio-temporal patch independently; *Equal contribution 1 and (3) random-access decoding that can condition on any subset of future pixels. These properties are uniquely present in the recently introduced Local Random Access Sequence (LRAS) architecture. Building on LRAS, we propose KL-tracing: novel test-time inference procedure that injects localized perturbation into the first frame, rolls out the model one step, and computes the KullbackLeibler divergence between perturbed and unperturbed predictive distributions. Without any flow-specific fine-tuning, our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid Kubric (4.7% relative improvement), despite being trained on real-world videos. Our results indicate that counterfactual prompting of controllable generative video models is scalable and effective alternative to supervised or photometric-loss approaches for high-quality optical flow.Extracting motion information (optical flow) from videos is fundamental yet open challenge in computer vision with many applications. Due to the intractable cost of obtaining ground-truth labels from real-world videos, most supervised baselines [37, 39] are trained on synthetic datasets, e.g., FlyingChairs [12], FlyingThings [27] and Sintel [6]. While invaluable for progress, these datasets cover narrow slice of motion statisticspredominantly rigid objects, limited lighting variation, and short temporal horizons. As result, they under-represent the long-tail of real-world phenomena such as non-rigid deformation, atmospheric effects, rapid camera shake and textureless regions. Self-supervised models [21, 36] that can be trained on real videos attempt to bridge this gap, but they often rely on task-specific heuristics, such as photometric consistency or smoothness which fail under complex lighting, occlusion, or long-range motion. Consequently, both supervised and self-supervised optical flow baselines struggle to generalize to challenging in-the-wild videos. Inspired by successes across vision and language where large general-purpose models outperform smaller task-specific ones, we explore large-scale video models as possible solution. Trained on massive repositories of real-world data, modern video models already demonstrate strong scene understanding [5, 29, 1, 26], suggesting an implicit grasp of optical flow [3, 38]. However, prior work extracting visual intermediates such as depth and illumination from these models still required supervised fine-tuning [33, 11]. Extending that recipe to optical flow would again depend on synthetic labels, facing the same sim-to-real domain gap. This motivates our search for zero-shot procedure that can extract accurate optical flow from off-the-shelf video models without any additional training. In fact, such procedure has been proposed in the Counterfactual World Model (CWM) framework [3]. Zero-shot optical flow is obtained by adding small tracer perturbation to the source frame and tracking how pretrained next-frame predictor propagates the tracer to the target frame (Section 3). In practice, however, because deterministic video models like CWM can only predict single future state, it encourages predictions that average over future possibilities, yielding perceptually blurry frames that wash out the injected tracer, leading to less precise motion estimates (Section 4.1). To overcome this, we implement the perturb-and-track method with generative video models, which generate crisp predictions as they sample from distribution instead of regressing to mean. This is insufficient, however, as each state-of-the-art generative video model faces its own flow extraction challenges. Stable Video Diffusion (SVD) [5] produces photorealistic frames, but its conditioning is weak and non-localized: generation is guided by single global latent inverted from the target frame, so pixel-level edits introduce noisy differences, corrupting the extracted flo"
[16.07.2025 23:12] Mistral response. {"id": "2339386a70a046a1bd15af579ab5ffb4", "object": "chat.completion", "created": 1752707538, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1563, "total_tokens": 1565, "completion_tokens": 2}}
[16.07.2025 23:12] Response: []
[16.07.2025 23:12] Deleting PDF ./assets/pdf/2507.09082.pdf.
[16.07.2025 23:12] Success.
[16.07.2025 23:12] Downloading and parsing paper https://huggingface.co/papers/2507.04127.
[16.07.2025 23:12] Extra JSON file exists (./assets/json/2507.04127.json), skip PDF parsing.
[16.07.2025 23:12] Paper image links file exists (./assets/img_data/2507.04127.json), skip HTML parsing.
[16.07.2025 23:12] Success.
[16.07.2025 23:12] Enriching papers with extra data.
[16.07.2025 23:12] ********************************************************************************
[16.07.2025 23:12] Abstract 0. The VLV auto-encoder framework uses pretrained vision and text models to create a cost-effective and data-efficient captioning system.  					AI-generated summary 				 Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billio...
[16.07.2025 23:12] ********************************************************************************
[16.07.2025 23:12] Abstract 1. EXAONE 4.0 integrates non-reasoning and reasoning modes, supports multilingualism, and offers models optimized for high performance and on-device use, demonstrating superior performance compared to open-weight models.  					AI-generated summary 				 This technical report introduces EXAONE 4.0, which...
[16.07.2025 23:12] ********************************************************************************
[16.07.2025 23:12] Abstract 2. Scaling laws predict optimal data mixtures for large foundation models, improving performance across different domains and scales.  					AI-generated summary 				 Large foundation models are typically trained on data from multiple domains, with the data mixture--the proportion of each domain used--p...
[16.07.2025 23:12] ********************************************************************************
[16.07.2025 23:12] Abstract 3. A benchmark evaluates multimodal models' ability to interpret scientific schematic diagrams and answer related questions, revealing performance gaps and insights for improvement.  					AI-generated summary 				 This paper introduces MISS-QA, the first benchmark specifically designed to evaluate the ...
[16.07.2025 23:12] ********************************************************************************
[16.07.2025 23:12] Abstract 4. Recent advancements in reasoning-based Large Language Models (LLMs), particularly their potential through test-time scaling, have created significant opportunities for distillation in code generation and critique. However, progress in both areas fundamentally depends on large-scale, high-quality dat...
[16.07.2025 23:12] ********************************************************************************
[16.07.2025 23:12] Abstract 5. AgentsNet is a new benchmark for evaluating multi-agent systems' ability to self-organize, communicate, and solve problems collaboratively across varying network sizes.  					AI-generated summary 				 Large-language models (LLMs) have demonstrated powerful problem-solving capabilities, in particular...
[16.07.2025 23:12] ********************************************************************************
[16.07.2025 23:12] Abstract 6. A discrete diffusion model for audio inpainting using tokenized audio representations achieves competitive performance for reconstructing long gaps in corrupted audio recordings.  					AI-generated summary 				 Audio inpainting refers to the task of reconstructing missing segments in corrupted audio...
[16.07.2025 23:12] ********************************************************************************
[16.07.2025 23:12] Abstract 7. A semi-automated framework uses Large Language Models to generate malware variants, demonstrating reduced detection rates and notable attack success against ML classifiers.  					AI-generated summary 				 Large Language Models (LLMs) have transformed software development and automated code generatio...
[16.07.2025 23:12] ********************************************************************************
[16.07.2025 23:12] Abstract 8. UGC-VideoCap introduces a new benchmark and model for detailed omnimodal captioning of user-generated videos, emphasizing audio-visual integration and using a novel training strategy.  					AI-generated summary 				 Real-world user-generated videos, especially on platforms like TikTok, often feature...
[16.07.2025 23:12] ********************************************************************************
[16.07.2025 23:12] Abstract 9. Research identifies pretraining as the primary source of cognitive biases in large language models, distinguishing its influence from finetuning and training randomness.  					AI-generated summary 				 Large language models (LLMs) exhibit cognitive biases -- systematic tendencies of irrational decis...
[16.07.2025 23:12] ********************************************************************************
[16.07.2025 23:12] Abstract 10. A modular Agentic AI framework integrates multimodal agents with a reasoning orchestrator and RAG module to improve trust and accuracy in zero-shot visual classification tasks like apple leaf disease diagnosis.  					AI-generated summary 				 Modern Artificial Intelligence (AI) increasingly relies o...
[16.07.2025 23:12] ********************************************************************************
[16.07.2025 23:12] Abstract 11. Counterfactual prompting of generative video models, using KL-tracing, extracts optical flow without fine-tuning, outperforming state-of-the-art methods on real and synthetic datasets.  					AI-generated summary 				 Extracting optical flow from videos remains a core computer vision problem. Motivat...
[16.07.2025 23:12] ********************************************************************************
[16.07.2025 23:12] Abstract 12. BYOKG-RAG combines LLMs with specialized graph retrieval tools to enhance KGQA, improving generalization and performance over custom knowledge graphs.  					AI-generated summary 				 Knowledge graph question answering (KGQA) presents significant challenges due to the structural and semantic variatio...
[16.07.2025 23:12] Read previous papers.
[16.07.2025 23:12] Generating reviews via LLM API.
[16.07.2025 23:12] Using data from previous issue: {"categories": ["#data", "#optimization", "#architecture", "#multimodal", "#transfer_learning", "#dataset", "#training"], "emoji": "üî¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –±–µ–∑ –æ–≥—Ä–æ–º–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ VLV auto-encoder –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç—Ñ
[16.07.2025 23:12] Using data from previous issue: {"categories": ["#open_source", "#small_models", "#agents", "#multilingual", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "EXAONE 4.0: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏ –≤ –ò–ò –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "EXAONE 4.0 - —ç—Ç–æ –Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è —Ä–µ–∂–∏–º—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –Ω–µ—Ä–∞—Å—Å—É–∂–¥–µ–Ω
[16.07.2025 23:12] Using data from previous issue: {"categories": ["#data", "#optimization", "#training", "#multimodal"], "emoji": "üî¨", "ru": {"title": "–ó–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥
[16.07.2025 23:12] Using data from previous issue: {"categories": ["#multimodal", "#science", "#interpretability", "#benchmark"], "emoji": "üî¨", "ru": {"title": "MISS-QA: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MISS-QA - –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ö–µ–º–∞—Ç–∏—á
[16.07.2025 23:12] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#reasoning", "#training", "#optimization", "#plp"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–º–Ω—ã—Ö –∫–æ–¥–µ—Ä–æ–≤: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –ò–ò –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OpenCodeReasoning-II - –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö 
[16.07.2025 23:12] Using data from previous issue: {"categories": ["#benchmark", "#graphs", "#agents", "#reasoning", "#games"], "emoji": "üï∏Ô∏è", "ru": {"title": "AgentsNet: –û—Ü–µ–Ω–∫–∞ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö", "desc": "AgentsNet - —ç—Ç–æ –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∫ —Å–∞–º–æ–æ—Ä–≥–∞–Ω–∏–∑–∞
[16.07.2025 23:12] Using data from previous issue: {"categories": ["#audio", "#diffusion"], "emoji": "üéµ", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –∞—É–¥–∏–æ–∑–∞–ø–∏—Å—è—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ
[16.07.2025 23:12] Using data from previous issue: {"categories": ["#dataset", "#security", "#agents", "#data"], "emoji": "ü¶†", "ru": {"title": "LLM –Ω–∞ —Å–ª—É–∂–±–µ –∫–∏–±–µ—Ä–ø—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –ü–û", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –ø–æ–ª—É–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É LLMalMorph, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏
[16.07.2025 23:12] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#multimodal", "#video", "#training", "#transfer_learning", "#benchmark"], "emoji": "üé•", "ru": {"title": "–û–º–Ω–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –≤–∏–¥–µ–æ: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "UGC-VideoCap –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π
[16.07.2025 23:12] Using data from previous issue: {"categories": ["#hallucinations", "#training", "#ethics"], "emoji": "üß†", "ru": {"title": "–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ - –∫–ª—é—á –∫ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–º –∏—Å–∫–∞–∂–µ–Ω–∏—è–º –≤ –ò–ò", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –∏—Å–∫–∞–∂–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∞ –Ω–µ 
[16.07.2025 23:12] Using data from previous issue: {"categories": ["#reasoning", "#rag", "#open_source", "#multimodal", "#interpretability", "#agents", "#benchmark"], "emoji": "üçé", "ru": {"title": "–î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–∞—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥—É–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ò–ò –¥–ª
[16.07.2025 23:12] Querying the API.
[16.07.2025 23:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Counterfactual prompting of generative video models, using KL-tracing, extracts optical flow without fine-tuning, outperforming state-of-the-art methods on real and synthetic datasets.  					AI-generated summary 				 Extracting optical flow from videos remains a core computer vision problem. Motivated by the success of large general-purpose models, we ask whether frozen self-supervised video models trained only for future frame prediction can be prompted, without fine-tuning, to output flow. Prior work reading out depth or illumination from video generators required fine-tuning, which is impractical for flow where labels are scarce and synthetic datasets suffer from a sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm, which can obtain point-wise correspondences by injecting a small tracer perturbation into a next-frame predictor and tracking its propagation, we extend this idea to generative video models. We explore several popular architectures and find that successful zero-shot flow extraction in this manner is aided by three model properties: (1) distributional prediction of future frames (avoiding blurry or noisy outputs); (2) factorized latents that treat each spatio-temporal patch independently; and (3) random-access decoding that can condition on any subset of future pixels. These properties are uniquely present in the recent Local Random Access Sequence (LRAS) architecture. Building on LRAS, we propose KL-tracing: a novel test-time procedure that injects a localized perturbation into the first frame, rolls out the model one step, and computes the Kullback-Leibler divergence between perturbed and unperturbed predictive distributions. Without any flow-specific fine-tuning, our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid Kubric (4.7% relative improvement). Our results indicate that counterfactual prompting of controllable generative video models is a scalable and effective alternative to supervised or photometric-loss approaches for high-quality flow.
[16.07.2025 23:12] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –∏–∑ –≤–∏–¥–µ–æ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∫–æ–Ω—Ç—Ä—Ñ–∞–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—è–º, –ø—Ä–∏–º–µ–Ω—è—è —Ç–µ—Ö–Ω–∏–∫—É KL-—Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏. –ú–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö. –ö–ª—é—á–µ–≤—ã–º–∏ —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏ —É—Å–ø–µ—Ö–∞ —è–≤–ª—è—é—Ç—Å—è –¥–∏—Å—Ç—Ä–∏–±—É—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞–¥—Ä–æ–≤, —Ñ–∞–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º –¥–æ—Å—Ç—É–ø–æ–º.",
  "emoji": "üé•",
  "title": "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏"
}
[16.07.2025 23:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Counterfactual prompting of generative video models, using KL-tracing, extracts optical flow without fine-tuning, outperforming state-of-the-art methods on real and synthetic datasets.  					AI-generated summary 				 Extracting optical flow from videos remains a core computer vision problem. Motivated by the success of large general-purpose models, we ask whether frozen self-supervised video models trained only for future frame prediction can be prompted, without fine-tuning, to output flow. Prior work reading out depth or illumination from video generators required fine-tuning, which is impractical for flow where labels are scarce and synthetic datasets suffer from a sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm, which can obtain point-wise correspondences by injecting a small tracer perturbation into a next-frame predictor and tracking its propagation, we extend this idea to generative video models. We explore several popular architectures and find that successful zero-shot flow extraction in this manner is aided by three model properties: (1) distributional prediction of future frames (avoiding blurry or noisy outputs); (2) factorized latents that treat each spatio-temporal patch independently; and (3) random-access decoding that can condition on any subset of future pixels. These properties are uniquely present in the recent Local Random Access Sequence (LRAS) architecture. Building on LRAS, we propose KL-tracing: a novel test-time procedure that injects a localized perturbation into the first frame, rolls out the model one step, and computes the Kullback-Leibler divergence between perturbed and unperturbed predictive distributions. Without any flow-specific fine-tuning, our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid Kubric (4.7% relative improvement). Our results indicate that counterfactual prompting of controllable generative video models is a scalable and effective alternative to supervised or photometric-loss approaches for high-quality flow."

[16.07.2025 23:12] Response: ```python
["VIDEO", "CV", "ARCHITECTURE"]
```
[16.07.2025 23:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Counterfactual prompting of generative video models, using KL-tracing, extracts optical flow without fine-tuning, outperforming state-of-the-art methods on real and synthetic datasets.  					AI-generated summary 				 Extracting optical flow from videos remains a core computer vision problem. Motivated by the success of large general-purpose models, we ask whether frozen self-supervised video models trained only for future frame prediction can be prompted, without fine-tuning, to output flow. Prior work reading out depth or illumination from video generators required fine-tuning, which is impractical for flow where labels are scarce and synthetic datasets suffer from a sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm, which can obtain point-wise correspondences by injecting a small tracer perturbation into a next-frame predictor and tracking its propagation, we extend this idea to generative video models. We explore several popular architectures and find that successful zero-shot flow extraction in this manner is aided by three model properties: (1) distributional prediction of future frames (avoiding blurry or noisy outputs); (2) factorized latents that treat each spatio-temporal patch independently; and (3) random-access decoding that can condition on any subset of future pixels. These properties are uniquely present in the recent Local Random Access Sequence (LRAS) architecture. Building on LRAS, we propose KL-tracing: a novel test-time procedure that injects a localized perturbation into the first frame, rolls out the model one step, and computes the Kullback-Leibler divergence between perturbed and unperturbed predictive distributions. Without any flow-specific fine-tuning, our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid Kubric (4.7% relative improvement). Our results indicate that counterfactual prompting of controllable generative video models is a scalable and effective alternative to supervised or photometric-loss approaches for high-quality flow."

[16.07.2025 23:12] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[16.07.2025 23:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a method called KL-tracing for extracting optical flow from videos using generative video models without the need for fine-tuning. The approach leverages the properties of large self-supervised models that predict future frames, allowing for effective flow extraction even in scenarios with limited labeled data. By injecting a small perturbation into the model\'s predictions, the method computes the Kullback-Leibler divergence to track changes in the output, leading to improved performance on both real and synthetic datasets. The results demonstrate that this counterfactual prompting technique can outperform existing state-of-the-art methods, making it a promising alternative for optical flow extraction.","title":"Revolutionizing Optical Flow Extraction with KL-Tracing"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents a method called KL-tracing for extracting optical flow from videos using generative video models without the need for fine-tuning. The approach leverages the properties of large self-supervised models that predict future frames, allowing for effective flow extraction even in scenarios with limited labeled data. By injecting a small perturbation into the model's predictions, the method computes the Kullback-Leibler divergence to track changes in the output, leading to improved performance on both real and synthetic datasets. The results demonstrate that this counterfactual prompting technique can outperform existing state-of-the-art methods, making it a promising alternative for optical flow extraction.", title='Revolutionizing Optical Flow Extraction with KL-Tracing'))
[16.07.2025 23:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÈÄöËøáÂèç‰∫ãÂÆûÊèêÁ§∫Êù•ÊèêÂèñËßÜÈ¢ë‰∏≠ÁöÑÂÖâÊµÅÔºåËÄåÊó†ÈúÄÂØπÁîüÊàêËßÜÈ¢ëÊ®°ÂûãËøõË°åÂæÆË∞É„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑKL-tracingÊñπÊ≥ïÂà©Áî®‰∫ÜËá™ÁõëÁù£ËßÜÈ¢ëÊ®°ÂûãÁöÑÁâπÊÄßÔºåËÉΩÂ§üÂú®‰∏çÈúÄË¶ÅÂ§ßÈáèÊ†áÁ≠æÁöÑÊÉÖÂÜµ‰∏ãÔºåÊàêÂäüÊèêÂèñÂÖâÊµÅ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÁúüÂÆûÂíåÂêàÊàêÊï∞ÊçÆÈõÜ‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊäÄÊúØ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂèç‰∫ãÂÆûÊèêÁ§∫ÊòØ‰∏ÄÁßçÂèØÊâ©Â±ï‰∏îÊúâÊïàÁöÑÊõø‰ª£ÊñπÊ°àÔºåÈÄÇÁî®‰∫éÈ´òË¥®ÈáèÂÖâÊµÅÊèêÂèñ„ÄÇ","title":"Âèç‰∫ãÂÆûÊèêÁ§∫ÔºöÈ´òÊïàÊèêÂèñËßÜÈ¢ëÂÖâÊµÅÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÈÄöËøáÂèç‰∫ãÂÆûÊèêÁ§∫Êù•ÊèêÂèñËßÜÈ¢ë‰∏≠ÁöÑÂÖâÊµÅÔºåËÄåÊó†ÈúÄÂØπÁîüÊàêËßÜÈ¢ëÊ®°ÂûãËøõË°åÂæÆË∞É„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑKL-tracingÊñπÊ≥ïÂà©Áî®‰∫ÜËá™ÁõëÁù£ËßÜÈ¢ëÊ®°ÂûãÁöÑÁâπÊÄßÔºåËÉΩÂ§üÂú®‰∏çÈúÄË¶ÅÂ§ßÈáèÊ†áÁ≠æÁöÑÊÉÖÂÜµ‰∏ãÔºåÊàêÂäüÊèêÂèñÂÖâÊµÅ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÁúüÂÆûÂíåÂêàÊàêÊï∞ÊçÆÈõÜ‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊäÄÊúØ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂèç‰∫ãÂÆûÊèêÁ§∫ÊòØ‰∏ÄÁßçÂèØÊâ©Â±ï‰∏îÊúâÊïàÁöÑÊõø‰ª£ÊñπÊ°àÔºåÈÄÇÁî®‰∫éÈ´òË¥®ÈáèÂÖâÊµÅÊèêÂèñ„ÄÇ', title='Âèç‰∫ãÂÆûÊèêÁ§∫ÔºöÈ´òÊïàÊèêÂèñËßÜÈ¢ëÂÖâÊµÅÁöÑÊñ∞ÊñπÊ≥ï'))
[16.07.2025 23:12] Using data from previous issue: {"categories": ["#rag", "#multimodal", "#graphs", "#benchmark", "#open_source"], "emoji": "üï∏Ô∏è", "ru": {"title": "BYOKG-RAG: –°–∏–Ω–µ—Ä–≥–∏—è LLM –∏ –≥—Ä–∞—Ñ–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è KGQA", "desc": "BYOKG-RAG - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π (KGQA). –û–Ω –æ–±—ä
[16.07.2025 23:12] Renaming data file.
[16.07.2025 23:12] Renaming previous data. hf_papers.json to ./d/2025-07-16.json
[16.07.2025 23:12] Saving new data file.
[16.07.2025 23:12] Generating page.
[16.07.2025 23:12] Renaming previous page.
[16.07.2025 23:12] Renaming previous data. index.html to ./d/2025-07-16.html
[16.07.2025 23:12] Writing result.
[16.07.2025 23:12] Renaming log file.
[16.07.2025 23:12] Renaming previous data. log.txt to ./logs/2025-07-16_last_log.txt
