[16.07.2025 13:32] Read previous papers.
[16.07.2025 13:32] Generating top page (month).
[16.07.2025 13:32] Writing top page (month).
[16.07.2025 14:12] Read previous papers.
[16.07.2025 14:12] Get feed.
[16.07.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07104
[16.07.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.11407
[16.07.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09404
[16.07.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.10787
[16.07.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09075
[16.07.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.08616
[16.07.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.09411
[16.07.2025 14:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.07186
[16.07.2025 14:12] Extract page data from URL. URL: https://huggingface.co/papers/2507.10571
[16.07.2025 14:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.07.2025 14:12] No deleted papers detected.
[16.07.2025 14:12] Downloading and parsing papers (pdf, html). Total: 9.
[16.07.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2507.07104.
[16.07.2025 14:12] Extra JSON file exists (./assets/json/2507.07104.json), skip PDF parsing.
[16.07.2025 14:12] Paper image links file exists (./assets/img_data/2507.07104.json), skip HTML parsing.
[16.07.2025 14:12] Success.
[16.07.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2507.11407.
[16.07.2025 14:12] Extra JSON file exists (./assets/json/2507.11407.json), skip PDF parsing.
[16.07.2025 14:12] Paper image links file exists (./assets/img_data/2507.11407.json), skip HTML parsing.
[16.07.2025 14:12] Success.
[16.07.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2507.09404.
[16.07.2025 14:12] Downloading paper 2507.09404 from http://arxiv.org/pdf/2507.09404v1...
[16.07.2025 14:12] Failed to download and parse paper https://huggingface.co/papers/2507.09404: 'LTChar' object is not iterable
[16.07.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2507.10787.
[16.07.2025 14:12] Extra JSON file exists (./assets/json/2507.10787.json), skip PDF parsing.
[16.07.2025 14:12] Paper image links file exists (./assets/img_data/2507.10787.json), skip HTML parsing.
[16.07.2025 14:12] Success.
[16.07.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2507.09075.
[16.07.2025 14:12] Extra JSON file exists (./assets/json/2507.09075.json), skip PDF parsing.
[16.07.2025 14:12] Paper image links file exists (./assets/img_data/2507.09075.json), skip HTML parsing.
[16.07.2025 14:12] Success.
[16.07.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2507.08616.
[16.07.2025 14:12] Extra JSON file exists (./assets/json/2507.08616.json), skip PDF parsing.
[16.07.2025 14:12] Paper image links file exists (./assets/img_data/2507.08616.json), skip HTML parsing.
[16.07.2025 14:12] Success.
[16.07.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2507.09411.
[16.07.2025 14:12] Extra JSON file exists (./assets/json/2507.09411.json), skip PDF parsing.
[16.07.2025 14:12] Paper image links file exists (./assets/img_data/2507.09411.json), skip HTML parsing.
[16.07.2025 14:12] Success.
[16.07.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2507.07186.
[16.07.2025 14:12] Extra JSON file exists (./assets/json/2507.07186.json), skip PDF parsing.
[16.07.2025 14:12] Paper image links file exists (./assets/img_data/2507.07186.json), skip HTML parsing.
[16.07.2025 14:12] Success.
[16.07.2025 14:12] Downloading and parsing paper https://huggingface.co/papers/2507.10571.
[16.07.2025 14:12] Downloading paper 2507.10571 from http://arxiv.org/pdf/2507.10571v1...
[16.07.2025 14:12] Extracting affiliations from text.
[16.07.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 ] . [ 1 1 7 5 0 1 . 7 0 5 2 : r Orchestrator-Agent Trust: Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning Konstantinos I. Roumeliotisa,*, Ranjan Sapkotab,*, Manoj Karkeeb, Nikolaos D. Tselikasa aUniversity of the Peloponnese, Department of Informatics and Telecommunications, Tripoli, 22131, Greece. bCornell University, Department of Biological and Environmental Engineering, Ithaca, 14850, NY, USA. *Equal contribution. Corresponding author. Abstract Modern Artificial Intelligence (AI) increasingly relies on multi-agent architectures that blend visual and language understanding. Yet, pressing challenge remains: How can we trust these agents especially in zero-shot settings with no fine-tuning? We introduce novel modular Agentic AI visual classification framework that integrates generalist multimodal agents with non-visual reasoning orchestrator and Retrieval-Augmented Generation (RAG) module. Applied to apple leaf disease diagnosis, we benchmark three configurations: (I) zero-shot with confidence-based orchestration, (II) fine-tuned agents with improved performance, and (III) trust-calibrated orchestration enhanced by CLIP-based image retrieval and re-evaluation loops. Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator modulates trust across agents. Our results demonstrate 77.94% accuracy improvement in the zeroshot setting using trust-aware orchestration and RAG, achieving 85.63% overall. GPT-4o showed better calibration, while Qwen-2.5-VL displayed overconfidence. Furthermore, imageRAG grounded predictions with visually similar cases, enabling correction of agent overconfidence via iterative re-evaluation. The proposed system separates perception (vision agents) from metareasoning (orchestrator), enabling scalable and interpretable multi-agent AI. This blueprint is extensible to diagnostics, biology, and other trust-critical domains. All models, prompts, results, and system componen"
[16.07.2025 14:12] Response: ```python
[
    "University of the Peloponnese, Department of Informatics and Telecommunications, Tripoli, 22131, Greece",
    "Cornell University, Department of Biological and Environmental Engineering, Ithaca, 14850, NY, USA"
]
```
[16.07.2025 14:12] Deleting PDF ./assets/pdf/2507.10571.pdf.
[16.07.2025 14:12] Success.
[16.07.2025 14:12] Enriching papers with extra data.
[16.07.2025 14:12] ********************************************************************************
[16.07.2025 14:12] Abstract 0. The VLV auto-encoder framework uses pretrained vision and text models to create a cost-effective and data-efficient captioning system.  					AI-generated summary 				 Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billio...
[16.07.2025 14:12] ********************************************************************************
[16.07.2025 14:12] Abstract 1. EXAONE 4.0 integrates non-reasoning and reasoning modes, supports multilingualism, and offers models optimized for high performance and on-device use, demonstrating superior performance compared to open-weight models.  					AI-generated summary 				 This technical report introduces EXAONE 4.0, which...
[16.07.2025 14:12] ********************************************************************************
[16.07.2025 14:12] Abstract 2. Scaling laws predict optimal data mixtures for large foundation models, improving performance across different domains and scales.  					AI-generated summary 				 Large foundation models are typically trained on data from multiple domains, with the data mixture--the proportion of each domain used--p...
[16.07.2025 14:12] ********************************************************************************
[16.07.2025 14:12] Abstract 3. A benchmark evaluates multimodal models' ability to interpret scientific schematic diagrams and answer related questions, revealing performance gaps and insights for improvement.  					AI-generated summary 				 This paper introduces MISS-QA, the first benchmark specifically designed to evaluate the ...
[16.07.2025 14:12] ********************************************************************************
[16.07.2025 14:12] Abstract 4. Recent advancements in reasoning-based Large Language Models (LLMs), particularly their potential through test-time scaling, have created significant opportunities for distillation in code generation and critique. However, progress in both areas fundamentally depends on large-scale, high-quality dat...
[16.07.2025 14:12] ********************************************************************************
[16.07.2025 14:12] Abstract 5. AgentsNet is a new benchmark for evaluating multi-agent systems' ability to self-organize, communicate, and solve problems collaboratively across varying network sizes.  					AI-generated summary 				 Large-language models (LLMs) have demonstrated powerful problem-solving capabilities, in particular...
[16.07.2025 14:12] ********************************************************************************
[16.07.2025 14:12] Abstract 6. A semi-automated framework uses Large Language Models to generate malware variants, demonstrating reduced detection rates and notable attack success against ML classifiers.  					AI-generated summary 				 Large Language Models (LLMs) have transformed software development and automated code generatio...
[16.07.2025 14:12] ********************************************************************************
[16.07.2025 14:12] Abstract 7. Research identifies pretraining as the primary source of cognitive biases in large language models, distinguishing its influence from finetuning and training randomness.  					AI-generated summary 				 Large language models (LLMs) exhibit cognitive biases -- systematic tendencies of irrational decis...
[16.07.2025 14:12] ********************************************************************************
[16.07.2025 14:12] Abstract 8. A modular Agentic AI framework integrates multimodal agents with a reasoning orchestrator and RAG module to improve trust and accuracy in zero-shot visual classification tasks like apple leaf disease diagnosis.  					AI-generated summary 				 Modern Artificial Intelligence (AI) increasingly relies o...
[16.07.2025 14:12] Read previous papers.
[16.07.2025 14:12] Generating reviews via LLM API.
[16.07.2025 14:12] Using data from previous issue: {"categories": ["#data", "#optimization", "#architecture", "#multimodal", "#transfer_learning", "#dataset", "#training"], "emoji": "🔬", "ru": {"title": "Эффективное создание подписей к изображениям без огромных датасетов", "desc": "Статья представляет новый фреймворк VLV auto-encoder для создания эф
[16.07.2025 14:12] Using data from previous issue: {"categories": ["#open_source", "#small_models", "#agents", "#multilingual", "#reasoning"], "emoji": "🤖", "ru": {"title": "EXAONE 4.0: Интеграция рассуждений и многоязычности в ИИ нового поколения", "desc": "EXAONE 4.0 - это новая версия языковой модели, объединяющая режимы рассуждения и нерассужден
[16.07.2025 14:12] Using data from previous issue: {"categories": ["#data", "#optimization", "#training", "#multimodal"], "emoji": "🔬", "ru": {"title": "Законы масштабирования для оптимизации смеси данных в больших моделях", "desc": "Статья предлагает систематический метод определения оптимальной смеси данных для обучения больших фундаментальных мод
[16.07.2025 14:12] Using data from previous issue: {"categories": ["#multimodal", "#science", "#interpretability", "#benchmark"], "emoji": "🔬", "ru": {"title": "MISS-QA: новый рубеж в оценке мультимодальных моделей для научной литературы", "desc": "Статья представляет MISS-QA - первый бенчмарк для оценки способности моделей интерпретировать схематич
[16.07.2025 14:12] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#reasoning", "#training", "#optimization", "#plp"], "emoji": "🖥️", "ru": {"title": "Большие данные для умных кодеров: новый подход к обучению ИИ программированию", "desc": "Статья представляет OpenCodeReasoning-II - новый набор данных для обучения языковых 
[16.07.2025 14:12] Using data from previous issue: {"categories": ["#benchmark", "#graphs", "#agents", "#reasoning", "#games"], "emoji": "🕸️", "ru": {"title": "AgentsNet: Оценка коллективного интеллекта в масштабируемых мультиагентных системах", "desc": "AgentsNet - это новый эталонный тест для оценки способности мультиагентных систем к самоорганиза
[16.07.2025 14:12] Using data from previous issue: {"categories": ["#dataset", "#security", "#agents", "#data"], "emoji": "🦠", "ru": {"title": "LLM на службе киберпреступности: новый подход к созданию вредоносного ПО", "desc": "Исследователи разработали полуавтоматизированную систему LLMalMorph, использующую большие языковые модели (LLM) для создани
[16.07.2025 14:12] Using data from previous issue: {"categories": ["#hallucinations", "#training", "#ethics"], "emoji": "🧠", "ru": {"title": "Предобучение - ключ к когнитивным искажениям в ИИ", "desc": "Исследование показывает, что когнитивные искажения в больших языковых моделях (LLM) в основном формируются на этапе предварительного обучения, а не 
[16.07.2025 14:12] Querying the API.
[16.07.2025 14:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A modular Agentic AI framework integrates multimodal agents with a reasoning orchestrator and RAG module to improve trust and accuracy in zero-shot visual classification tasks like apple leaf disease diagnosis.  					AI-generated summary 				 Modern Artificial Intelligence (AI) increasingly relies on multi-agent architectures that blend visual and language understanding. Yet, a pressing challenge remains: How can we trust these agents especially in zero-shot settings with no fine-tuning? We introduce a novel modular Agentic AI visual classification framework that integrates generalist multimodal agents with a non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG) module. Applied to apple leaf disease diagnosis, we benchmark three configurations: (I) zero-shot with confidence-based orchestration, (II) fine-tuned agents with improved performance, and (III) trust-calibrated orchestration enhanced by CLIP-based image retrieval and re-evaluation loops. Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator modulates trust across agents. Our results demonstrate a 77.94\% accuracy improvement in the zero-shot setting using trust-aware orchestration and RAG, achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL displayed overconfidence. Furthermore, image-RAG grounded predictions with visually similar cases, enabling correction of agent overconfidence via iterative re-evaluation. The proposed system separates perception (vision agents) from meta-reasoning (orchestrator), enabling scalable and interpretable multi-agent AI. This blueprint is extensible to diagnostics, biology, and other trust-critical domains. All models, prompts, results, and system components including the complete software source code are openly released to support reproducibility, transparency, and community benchmarking at Github: https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust
[16.07.2025 14:12] Response: {
  "desc": "Статья представляет новую модульную архитектуру агентного ИИ для задач визуальной классификации без предварительного обучения. Система интегрирует мультимодальных агентов с оркестратором рассуждений и модулем RAG для повышения доверия и точности. Применение к диагностике болезней листьев яблони показало значительное улучшение точности до 85,63% в режиме zero-shot. Архитектура разделяет восприятие и мета-рассуждения, что делает ее масштабируемой и интерпретируемой.",
  "emoji": "🍎",
  "title": "Доверительная мультиагентная система для визуальной классификации без обучения"
}
[16.07.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A modular Agentic AI framework integrates multimodal agents with a reasoning orchestrator and RAG module to improve trust and accuracy in zero-shot visual classification tasks like apple leaf disease diagnosis.  					AI-generated summary 				 Modern Artificial Intelligence (AI) increasingly relies on multi-agent architectures that blend visual and language understanding. Yet, a pressing challenge remains: How can we trust these agents especially in zero-shot settings with no fine-tuning? We introduce a novel modular Agentic AI visual classification framework that integrates generalist multimodal agents with a non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG) module. Applied to apple leaf disease diagnosis, we benchmark three configurations: (I) zero-shot with confidence-based orchestration, (II) fine-tuned agents with improved performance, and (III) trust-calibrated orchestration enhanced by CLIP-based image retrieval and re-evaluation loops. Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator modulates trust across agents. Our results demonstrate a 77.94\% accuracy improvement in the zero-shot setting using trust-aware orchestration and RAG, achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL displayed overconfidence. Furthermore, image-RAG grounded predictions with visually similar cases, enabling correction of agent overconfidence via iterative re-evaluation. The proposed system separates perception (vision agents) from meta-reasoning (orchestrator), enabling scalable and interpretable multi-agent AI. This blueprint is extensible to diagnostics, biology, and other trust-critical domains. All models, prompts, results, and system components including the complete software source code are openly released to support reproducibility, transparency, and community benchmarking at Github: https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust"

[16.07.2025 14:12] Response: ```python
["AGENTS", "RAG", "MULTIMODAL", "BENCHMARK"]
```
[16.07.2025 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A modular Agentic AI framework integrates multimodal agents with a reasoning orchestrator and RAG module to improve trust and accuracy in zero-shot visual classification tasks like apple leaf disease diagnosis.  					AI-generated summary 				 Modern Artificial Intelligence (AI) increasingly relies on multi-agent architectures that blend visual and language understanding. Yet, a pressing challenge remains: How can we trust these agents especially in zero-shot settings with no fine-tuning? We introduce a novel modular Agentic AI visual classification framework that integrates generalist multimodal agents with a non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG) module. Applied to apple leaf disease diagnosis, we benchmark three configurations: (I) zero-shot with confidence-based orchestration, (II) fine-tuned agents with improved performance, and (III) trust-calibrated orchestration enhanced by CLIP-based image retrieval and re-evaluation loops. Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator modulates trust across agents. Our results demonstrate a 77.94\% accuracy improvement in the zero-shot setting using trust-aware orchestration and RAG, achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL displayed overconfidence. Furthermore, image-RAG grounded predictions with visually similar cases, enabling correction of agent overconfidence via iterative re-evaluation. The proposed system separates perception (vision agents) from meta-reasoning (orchestrator), enabling scalable and interpretable multi-agent AI. This blueprint is extensible to diagnostics, biology, and other trust-critical domains. All models, prompts, results, and system components including the complete software source code are openly released to support reproducibility, transparency, and community benchmarking at Github: https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust"

[16.07.2025 14:12] Response: ```python
['REASONING', 'INTERPRETABILITY', 'OPEN_SOURCE']
```
[16.07.2025 14:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a modular Agentic AI framework designed to enhance trust and accuracy in zero-shot visual classification tasks, specifically for diagnosing apple leaf diseases. The framework combines multimodal agents with a reasoning orchestrator and a Retrieval-Augmented Generation (RAG) module to improve performance without the need for fine-tuning. By implementing trust-calibrated orchestration and confidence calibration metrics, the system achieves significant accuracy improvements in zero-shot settings. The proposed architecture separates perception from reasoning, making it scalable and interpretable for various applications in trust-critical fields.","title":"Enhancing Trust and Accuracy in AI with Modular Frameworks"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a modular Agentic AI framework designed to enhance trust and accuracy in zero-shot visual classification tasks, specifically for diagnosing apple leaf diseases. The framework combines multimodal agents with a reasoning orchestrator and a Retrieval-Augmented Generation (RAG) module to improve performance without the need for fine-tuning. By implementing trust-calibrated orchestration and confidence calibration metrics, the system achieves significant accuracy improvements in zero-shot settings. The proposed architecture separates perception from reasoning, making it scalable and interpretable for various applications in trust-critical fields.', title='Enhancing Trust and Accuracy in AI with Modular Frameworks'))
[16.07.2025 14:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文提出了一种模块化的Agentic AI框架，旨在提高零-shot视觉分类任务中的信任度和准确性，例如苹果叶病的诊断。该框架结合了多模态智能体、推理协调器和增强检索生成（RAG）模块，以应对在没有微调的情况下如何信任这些智能体的挑战。通过信心校准指标，协调器调节不同智能体之间的信任，最终在零-shot设置中实现了77.94%的准确率提升。该系统的设计使得视觉感知与元推理分离，具有可扩展性和可解释性，适用于诊断、生物学等信任关键领域。","title":"模块化Agentic AI框架：提升视觉分类信任与准确性"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文提出了一种模块化的Agentic AI框架，旨在提高零-shot视觉分类任务中的信任度和准确性，例如苹果叶病的诊断。该框架结合了多模态智能体、推理协调器和增强检索生成（RAG）模块，以应对在没有微调的情况下如何信任这些智能体的挑战。通过信心校准指标，协调器调节不同智能体之间的信任，最终在零-shot设置中实现了77.94%的准确率提升。该系统的设计使得视觉感知与元推理分离，具有可扩展性和可解释性，适用于诊断、生物学等信任关键领域。', title='模块化Agentic AI框架：提升视觉分类信任与准确性'))
[16.07.2025 14:12] Renaming data file.
[16.07.2025 14:12] Renaming previous data. hf_papers.json to ./d/2025-07-16.json
[16.07.2025 14:12] Saving new data file.
[16.07.2025 14:12] Generating page.
[16.07.2025 14:12] Renaming previous page.
[16.07.2025 14:12] Renaming previous data. index.html to ./d/2025-07-16.html
[16.07.2025 14:12] Writing result.
[16.07.2025 14:12] Renaming log file.
[16.07.2025 14:12] Renaming previous data. log.txt to ./logs/2025-07-16_last_log.txt
