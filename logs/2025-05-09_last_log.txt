[09.05.2025 09:14] Read previous papers.
[09.05.2025 09:14] Generating top page (month).
[09.05.2025 09:14] Writing top page (month).
[09.05.2025 10:12] Read previous papers.
[09.05.2025 10:12] Get feed.
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04620
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.05470
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04921
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02847
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.05315
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.05071
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.05474
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.05327
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03981
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.05467
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.05469
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.05408
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2504.19314
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.05288
[09.05.2025 10:12] Extract page data from URL. URL: https://huggingface.co/papers/2505.03422
[09.05.2025 10:12] Get page data from previous paper. URL: https://huggingface.co/papers/2505.04955
[09.05.2025 10:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.05.2025 10:12] No deleted papers detected.
[09.05.2025 10:12] Downloading and parsing papers (pdf, html). Total: 16.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.04620.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2505.04620.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2505.04620.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.05470.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2505.05470.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2505.05470.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.04921.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2505.04921.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2505.04921.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.02847.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2505.02847.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2505.02847.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.05315.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2505.05315.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2505.05315.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.05071.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2505.05071.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2505.05071.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.05474.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2505.05474.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2505.05474.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.05327.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2505.05327.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2505.05327.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.03981.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2505.03981.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2505.03981.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.05467.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2505.05467.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2505.05467.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.05469.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2505.05469.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2505.05469.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.05408.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2505.05408.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2505.05408.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2504.19314.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2504.19314.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2504.19314.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.05288.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2505.05288.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2505.05288.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.03422.
[09.05.2025 10:12] Downloading paper 2505.03422 from http://arxiv.org/pdf/2505.03422v1...
[09.05.2025 10:12] Extracting affiliations from text.
[09.05.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LiftFeat: 3D Geometry-Aware Local Feature Matching Yepeng Liu1, Wenpeng Lai2, Zhou Zhao3, Yuxuan Xiong1, Jinchi Zhu1, Jun Cheng4, and Yongchao Xu1 5 2 0 2 6 ] . [ 1 2 2 4 3 0 . 5 0 5 2 : r it Abstract Robust and efficient local feature matching plays crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose new lightweight network called LiftFeat, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design 3D geometryaware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat. I. INTRODUCTION Local feature matching between images is critical for many core robotic tasks, including Structure from Motion (SfM) [1], [2], [3], Simultaneous Localization and Mapping (SLAM) [4], [5], [6], [7], and visual localization [8], [9], [10], [11]. In practical applications, there are some scenes with extreme conditions, such as significant variation of illumination, and the presence of textureless or repetitive patterns. In these extreme conditions, achieving reliable feature matching still remains challenging task. Traditional local feature matching methods typically involve three stages: keypo"
[09.05.2025 10:12] Response: ```python
[]
```
[09.05.2025 10:12] Extracting affiliations from text.
[09.05.2025 10:12] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"LiftFeat: 3D Geometry-Aware Local Feature Matching Yepeng Liu1, Wenpeng Lai2, Zhou Zhao3, Yuxuan Xiong1, Jinchi Zhu1, Jun Cheng4, and Yongchao Xu1 5 2 0 2 6 ] . [ 1 2 2 4 3 0 . 5 0 5 2 : r it Abstract Robust and efficient local feature matching plays crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose new lightweight network called LiftFeat, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design 3D geometryaware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat. I. INTRODUCTION Local feature matching between images is critical for many core robotic tasks, including Structure from Motion (SfM) [1], [2], [3], Simultaneous Localization and Mapping (SLAM) [4], [5], [6], [7], and visual localization [8], [9], [10], [11]. In practical applications, there are some scenes with extreme conditions, such as significant variation of illumination, and the presence of textureless or repetitive patterns. In these extreme conditions, achieving reliable feature matching still remains challenging task. Traditional local feature matching methods typically involve three stages: keypoint detection, descriptor extraction, and feature matching. Early methods such as SIFT [12] and SURF [13] propose well-designed handcrafted descriptors. During the feature matching stage, nearest neighbor matching is commonly employed to obtain the matching results. In recent years, deep learning-based feature matching methods have significantly improved the performance of traditional algorithms [14], [15]. Some studies have jointly trained keypoint prediction and descriptor extraction [16], [17], [18], which not only increases processing speed but also further optimizes matching performance. Additionally, other studies have introduced graph neural networks [19], [20], framing the feature matching task as an optimal transport problem, thereby effectively improving matching accuracy. 1School of Computer Science, Wuhan University, Wuhan, China; 2SF Technology, Shenzhen, China; 3School of Computer Science, Central China Normal University and the Hubei Engineering Research Center for Intelligent Detection and Identification of Complex Parts, Wuhan, China; 4Institute for Infocomm Research, A*STAR, Singapore. (: Equal contribution.) (Corresponding author: Yongchao Xu, yongchao.xu@whu.edu.cn) Fig. 1: Feature matching of applying 2D visual cues and integrating 3D geometric cues in low texture scene. Green lines: correct matches; Red lines: incorrect matches. Histograms: distribution of descriptor features. (a) Result of using SuperPoint [14]. (b) 3D geometric Normal Map. (c) Result of using our LiftFeat. Incorporating 3D information enhances the distinctiveness of the raw 2D descriptors. Despite the advanced performance of current methods in most scenarios, 2D visual cues can cause confusion in feature matching for scenes with extreme conditions, including significant illumination variation, low texture, or repetitive patterns. As shown in Fig. 1, in textureless scenes, raw 2D descriptors may lead to incorrect matches due to insufficient discriminative visual information. An intuitive idea is to leverage the additional information from 3D data to enhance the robustness of feature matching. However, the precision and cost of using 3D data introduce new challenges, particularly in scenarios like robotics, where computational power is limited. In this paper, we focus on designing lightweight model that integrates 2D and 3D cues for local feature matching. Depth maps is one of the most accessible 3D cue. Yet, depth maps exhibit scale ambiguity, making them unsuitable for direct use in local feature matching. In contrast, surface normal possesses both translation and scale invariance, which is suitable for feature matching. Therefore, we incorporate surface normal estimation head into the network to learn 3D geometric knowledge. Notably, the pseudo surface normal labels are derived from depth maps predicted by Depth Anything v2 [21], which eliminates the need for additional annotation costs during training. Subsequently, we propose 3D Geometry-aware Feature Lifting (3D-GFL) module to fuse the raw 2D description with the 3D normal feature, lifting the discriminative ability of raw 2D descriptors in challenging scenarios. Experimental results demonstrate that our proposed method termed LiftFeat achieves state-of-theart performance across multiple tasks: relative pose estimation, homography estimation, and visual localization. The main contributions of this work are as follows: 1) We propose lightweight network named LiftFeat, which innovatively introduces 3D geometry for local feature matching. 2) We design 3D Geometry-aware Feature Lifting (3DGFL) module that fuses 2D description with 3D normal feature, significantly improving the discriminative ability of raw 2D descriptors in challenging scenarios. 3) Experiments on different tasks confirm that our method achieves high accuracy and robustness across various scenarios. Additional runtime tests confirm that our method can achieve inference latency of 7.4 ms on edge devices. II. RELATED WORK A. Local Feature Matching Local feature matching is fundamental module in downstream applications such as visual localization [11], [22], simultaneous localization and mapping (SLAM) [6], [23]. It typically involves three key steps: feature detection, feature description, and feature matching. Traditional algorithms like SIFT [12] and ORB [24] focus on designing features that are invariant to scale, rotation, and illumination changes. Due to their ease of deployment, these methods are still wid"
[09.05.2025 10:12] Mistral response. {"id": "5815b7c8906445faad9d57f8c1ad6632", "object": "chat.completion", "created": 1746785545, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\n    \"School of Computer Science, Wuhan University, Wuhan, China\",\n    \"SF Technology, Shenzhen, China\",\n    \"School of Computer Science, Central China Normal University and the Hubei Engineering Research Center for Intelligent Detection and Identification of Complex Parts, Wuhan, China\",\n    \"Institute for Infocomm Research, A*STAR, Singapore\"\n]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1572, "total_tokens": 1671, "completion_tokens": 99}}
[09.05.2025 10:12] Response: ```python
[
    "School of Computer Science, Wuhan University, Wuhan, China",
    "SF Technology, Shenzhen, China",
    "School of Computer Science, Central China Normal University and the Hubei Engineering Research Center for Intelligent Detection and Identification of Complex Parts, Wuhan, China",
    "Institute for Infocomm Research, A*STAR, Singapore"
]
```
[09.05.2025 10:12] Deleting PDF ./assets/pdf/2505.03422.pdf.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Downloading and parsing paper https://huggingface.co/papers/2505.04955.
[09.05.2025 10:12] Extra JSON file exists (./assets/json/2505.04955.json), skip PDF parsing.
[09.05.2025 10:12] Paper image links file exists (./assets/img_data/2505.04955.json), skip HTML parsing.
[09.05.2025 10:12] Success.
[09.05.2025 10:12] Enriching papers with extra data.
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 0. The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have ...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 1. We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 2. Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adapti...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 3. Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentien...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 4. Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly ...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 5. Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 6. 3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent ad...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 7. Data selection for instruction tuning is essential for improving the performance of Large Language Models (LLMs) and reducing training cost. However, existing automated selection methods either depend on computationally expensive gradient-based measures or manually designed heuristics, which may fai...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 8. Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclea...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 9. We present StreamBridge, a simple yet effective framework that seamlessly transforms offline Video-LLMs into streaming-capable models. It addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lac...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 10. We introduce LegoGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 11. Reasoning capabilities of large language models are primarily studied for English, even when pretrained models are multilingual. In this work, we investigate to what extent English reasoning finetuning with long chain-of-thoughts (CoTs) can generalize across languages. First, we find that scaling up...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 12. As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 13. We introduce the novel task of Language-Guided Object Placement in Real 3D Scenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual prompt broadly describing where the 3D asset should be placed. The task here is to find a valid placement for the 3D asset that respects the promp...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 14. Robust and efficient local feature matching plays a crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas,...
[09.05.2025 10:12] ********************************************************************************
[09.05.2025 10:12] Abstract 15. Chain-of-thoughts (CoT) requires large language models (LLMs) to generate intermediate steps before reaching the final answer, and has been proven effective to help LLMs solve complex reasoning tasks. However, the inner mechanism of CoT still remains largely unclear. In this paper, we empirically st...
[09.05.2025 10:12] Read previous papers.
[09.05.2025 10:12] Generating reviews via LLM API.
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#agi"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-—Å–∏—Å—Ç–µ–º –Ω–∞ –ø—É—Ç–∏ –∫ AGI", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º General-Level. –≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç 5 —É—Ä
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#rl", "#cv", "#rlhf", "#alignment", "#games", "#multimodal"], "emoji": "üé®", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: RL –≤—Å—Ç—Ä–µ—á–∞–µ—Ç –ø–æ—Ç–æ–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "Flow-GRPO ‚Äì —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∏ –º–æ–¥–µ–ª–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤. –û–Ω –∏
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#rl", "#multimodal", "#benchmark", "#survey", "#agents", "#reasoning"], "emoji": "üß†", "ru": {"title": "–û—Ç –º–æ–¥—É–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LMRMs). –ê
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#alignment", "#agents", "#benchmark"], "emoji": "üß†", "ru": {"title": "SAGE: –ò–∑–º–µ—Ä–µ–Ω–∏–µ —ç–º–ø–∞—Ç–∏–∏ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SAGE - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–Ω–∏–º–∞—Ç—å 
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#plp", "#training", "#reasoning", "#optimization", "#benchmark", "#math"], "emoji": "üß†", "ru": {"title": "–≠–ª–∞—Å—Ç–∏—á–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–∏ –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–≠–ª–∞—Å—Ç–∏—á–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ' –¥–ª—è –∫—Ä—É
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#data", "#open_source", "#optimization", "#multimodal", "#training", "#dataset"], "emoji": "üîç", "ru": {"title": "FG-CLIP: –¢–æ—á–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –Ω–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Fine-Grained CLIP (FG-CLIP) - —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏ CLIP –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#3d", "#robotics", "#multimodal", "#synthetic", "#survey"], "emoji": "üåê", "ru": {"title": "–ù–æ–≤—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω: –æ—Ç –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –∫ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º —Å–µ—Ç—è–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã—Ö —Å—Ü–µ–Ω. –ê–≤—Ç–æ—Ä—ã —Å–∏—Å—Ç–µ
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#benchmark", "#data"], "emoji": "üß†", "ru": {"title": "ICon: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ICon. –≠—Ç
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#multimodal", "#training", "#reasoning", "#transfer_learning", "#healthcare"], "emoji": "üß†", "ru": {"title": "–û–±–æ–±—â–∞–µ–º—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–æ–º–µ–Ω–∞–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±–æ–±—â–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –Ω–∞ —Ä–∞–∑–ª–∏—á
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#benchmark", "#video", "#dataset", "#long_context"], "emoji": "üé•", "ru": {"title": "StreamBridge: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ", "desc": "StreamBridge - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –æ—Ñ–ª–∞–π–Ω-–º–æ–¥–µ–ª–∏ Video-LLM –≤ –ø–æ—Ç–æ–∫–æ–≤—ã–µ. –û–Ω —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ —Ä–µ–∞–ª—å
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#robotics", "#dataset", "#multimodal", "#3d", "#open_source"], "emoji": "üß±", "ru": {"title": "LegoGPT: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º LEGO", "desc": "LegoGPT - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ –∫—É–±–∏–∫–æ–≤ LEGO –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å–æ
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#math", "#reasoning", "#low_resource", "#multilingual"], "emoji": "üåê", "ru": {"title": "–ê–Ω–≥–ª–∏–π—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö LLM", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#multilingual", "#reasoning", "#dataset", "#benchmark", "#low_resource"], "emoji": "üåê", "ru": {"title": "BrowseComp-ZH: –∏—Å–ø—ã—Ç–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∫–∏—Ç–∞–π—Å–∫–æ–º –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç BrowseComp-ZH - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–Ø–ú) —Ä–∞–±–æ—Ç–∞
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#3d", "#survey", "#dataset"], "emoji": "üßä", "ru": {"title": "–Ø–∑—ã–∫–æ–≤–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ–º –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö 3D-—Å—Ü–µ–Ω–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö 3D-—Å—Ü–µ–Ω–∞—Ö —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ú–æ–¥–µ–ª—å –ø–æ–ª—É—á–∞–µ—Ç 
[09.05.2025 10:12] Querying the API.
[09.05.2025 10:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Robust and efficient local feature matching plays a crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose a new lightweight network called LiftFeat, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design a 3D geometry-aware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat.
[09.05.2025 10:12] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ª–µ–≥–∫–æ–≤–µ—Å–Ω—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å LiftFeat –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. LiftFeat –ø–æ–≤—ã—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä–æ–≤ –ø—É—Ç–µ–º –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ 3D –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã. –°–µ—Ç—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª—å, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π 3D –≥–µ–æ–º–µ—Ç—Ä–∏—é, –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–æ—Ä–º–∞–ª–µ–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ 2D –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ LiftFeat –Ω–∞–¥ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ü–µ–Ω–∫–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–∑—ã, –æ—Ü–µ–Ω–∫–∏ –≥–æ–º–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏.",
  "emoji": "ü§ñ",
  "title": "LiftFeat: –ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é 3D –≥–µ–æ–º–µ—Ç—Ä–∏–∏"
}
[09.05.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Robust and efficient local feature matching plays a crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose a new lightweight network called LiftFeat, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design a 3D geometry-aware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat."

[09.05.2025 10:12] Response: ```python
['3D', 'CV', 'ROBOTICS']
```
[09.05.2025 10:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Robust and efficient local feature matching plays a crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose a new lightweight network called LiftFeat, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design a 3D geometry-aware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat."

[09.05.2025 10:12] Response: []
[09.05.2025 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces LiftFeat, a lightweight neural network designed to improve local feature matching in challenging visual conditions. It addresses issues like drastic lighting changes and low texture areas by incorporating 3D geometric features into the feature extraction process. The network uses a pre-trained monocular depth estimation model to create pseudo surface normal labels, which guide the extraction of 3D features. Experimental results show that LiftFeat significantly enhances the performance of visual localization and pose estimation tasks compared to existing lightweight methods.","title":"LiftFeat: Enhancing Feature Matching with 3D Geometry"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces LiftFeat, a lightweight neural network designed to improve local feature matching in challenging visual conditions. It addresses issues like drastic lighting changes and low texture areas by incorporating 3D geometric features into the feature extraction process. The network uses a pre-trained monocular depth estimation model to create pseudo surface normal labels, which guide the extraction of 3D features. Experimental results show that LiftFeat significantly enhances the performance of visual localization and pose estimation tasks compared to existing lightweight methods.', title='LiftFeat: Enhancing Feature Matching with 3D Geometry'))
[09.05.2025 10:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËΩªÈáèÁ∫ßÁΩëÁªúLiftFeatÔºåÊó®Âú®ÊèêÈ´òÂú®ÊûÅÁ´ØÊù°‰ª∂‰∏ãÁöÑËßÜËßâÁâπÂæÅÂåπÈÖçËÉΩÂäõ„ÄÇÈÄöËøáËÅöÂêà‰∏âÁª¥Âá†‰ΩïÁâπÂæÅÔºåLiftFeatÂ¢ûÂº∫‰∫ÜÂéüÂßãÊèèËø∞Á¨¶ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÊàë‰ª¨ÈááÁî®È¢ÑËÆ≠ÁªÉÁöÑÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Ê®°ÂûãÁîüÊàê‰º™Ë°®Èù¢Ê≥ïÁ∫øÊ†áÁ≠æÔºå‰ª•ÊåáÂØº‰∏âÁª¥Âá†‰ΩïÁâπÂæÅÁöÑÊèêÂèñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLiftFeatÂú®Áõ∏ÂØπÂßøÊÄÅ‰º∞ËÆ°„ÄÅÂçïÂ∫îÊÄß‰º∞ËÆ°ÂíåËßÜËßâÂÆö‰Ωç‰ªªÂä°‰∏≠‰ºò‰∫é‰∏Ä‰∫õËΩªÈáèÁ∫ßÁöÑÊúÄÊñ∞ÊñπÊ≥ï„ÄÇ","title":"ÊèêÂçáËßÜËßâÁâπÂæÅÂåπÈÖçÁöÑÈ≤ÅÊ£íÊÄß‰∏éÊïàÁéá"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËΩªÈáèÁ∫ßÁΩëÁªúLiftFeatÔºåÊó®Âú®ÊèêÈ´òÂú®ÊûÅÁ´ØÊù°‰ª∂‰∏ãÁöÑËßÜËßâÁâπÂæÅÂåπÈÖçËÉΩÂäõ„ÄÇÈÄöËøáËÅöÂêà‰∏âÁª¥Âá†‰ΩïÁâπÂæÅÔºåLiftFeatÂ¢ûÂº∫‰∫ÜÂéüÂßãÊèèËø∞Á¨¶ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÊàë‰ª¨ÈááÁî®È¢ÑËÆ≠ÁªÉÁöÑÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Ê®°ÂûãÁîüÊàê‰º™Ë°®Èù¢Ê≥ïÁ∫øÊ†áÁ≠æÔºå‰ª•ÊåáÂØº‰∏âÁª¥Âá†‰ΩïÁâπÂæÅÁöÑÊèêÂèñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLiftFeatÂú®Áõ∏ÂØπÂßøÊÄÅ‰º∞ËÆ°„ÄÅÂçïÂ∫îÊÄß‰º∞ËÆ°ÂíåËßÜËßâÂÆö‰Ωç‰ªªÂä°‰∏≠‰ºò‰∫é‰∏Ä‰∫õËΩªÈáèÁ∫ßÁöÑÊúÄÊñ∞ÊñπÊ≥ï„ÄÇ', title='ÊèêÂçáËßÜËßâÁâπÂæÅÂåπÈÖçÁöÑÈ≤ÅÊ£íÊÄß‰∏éÊïàÁéá'))
[09.05.2025 10:12] Using data from previous issue: {"categories": ["#training", "#interpretability", "#data", "#architecture", "#reasoning"], "emoji": "üß†", "ru": {"title": "–¢–æ–∫–µ–Ω—ã CoT –≤ LLM: –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∏–∑—É—á–µ–Ω–∏—é –º–µ—Ö–∞–Ω–∏–∑–º–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–µ–π (Chain-of-Thought, CoT) –≤
[09.05.2025 10:12] Loading Chinese text from previous data.
[09.05.2025 10:12] Renaming data file.
[09.05.2025 10:12] Renaming previous data. hf_papers.json to ./d/2025-05-09.json
[09.05.2025 10:12] Saving new data file.
[09.05.2025 10:12] Generating page.
[09.05.2025 10:12] Renaming previous page.
[09.05.2025 10:12] Renaming previous data. index.html to ./d/2025-05-09.html
[09.05.2025 10:12] [Experimental] Generating Chinese page for reading.
[09.05.2025 10:12] Chinese vocab [{'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«î y√°n m√≥ x√≠ng', 'trans': 'large language model'}, {'word': 'ËøÖÈÄü', 'pinyin': 'x√πn s√π', 'trans': 'rapid'}, {'word': 'ÂèëÂ±ï', 'pinyin': 'fƒÅ zh«én', 'trans': 'develop'}, {'word': 'ÁêÜËß£', 'pinyin': 'l«ê jiƒõ', 'trans': 'understand'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generate'}, {'word': 'Ê®°ÊÄÅ', 'pinyin': 'm√≥ t√†i', 'trans': 'modality'}, {'word': 'ÂÅáËÆæ', 'pinyin': 'ji«é sh√®', 'trans': 'assume'}, {'word': 'È´òÊÄßËÉΩ', 'pinyin': 'gƒÅo x√¨ng n√©ng', 'trans': 'high performance'}, {'word': 'ÊÑèÂë≥ÁùÄ', 'pinyin': 'y√¨ w√®i zhe', 'trans': 'mean'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': 'È°πÁõÆ', 'pinyin': 'xi√†ng m√π', 'trans': 'project'}, {'word': '‰ªãÁªç', 'pinyin': 'ji√® sh√†o', 'trans': 'introduce'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ng ji√†', 'trans': 'framework'}, {'word': 'Ë°°Èáè', 'pinyin': 'h√©ng li√°ng', 'trans': 'measure'}, {'word': '‰∏ÄËá¥ÊÄß', 'pinyin': 'yƒ´ zh√¨ x√¨ng', 'trans': 'consistency'}, {'word': 'ËøõÂ±ï', 'pinyin': 'j√¨n zh«én', 'trans': 'progress'}]
[09.05.2025 10:12] Renaming previous Chinese page.
[09.05.2025 10:12] Renaming previous data. zh.html to ./d/2025-05-08_zh_reading_task.html
[09.05.2025 10:12] Writing Chinese reading task.
[09.05.2025 10:12] Writing result.
[09.05.2025 10:12] Renaming log file.
[09.05.2025 10:12] Renaming previous data. log.txt to ./logs/2025-05-09_last_log.txt
