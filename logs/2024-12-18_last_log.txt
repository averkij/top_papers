[18.12.2024 18:14] Read previous papers.
[18.12.2024 18:14] Generating top page (month).
[18.12.2024 18:14] Writing top page (month).
[18.12.2024 19:08] Read previous papers.
[18.12.2024 19:08] Get feed.
[18.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.13147
[18.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.12606
[18.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.13018
[18.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.13171
[18.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.12276
[18.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.13180
[18.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.13194
[18.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.10704
[18.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.11713
[18.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.12527
[18.12.2024 19:08] Get page data from previous paper. URL: https://huggingface.co/papers/2412.12877
[18.12.2024 19:08] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[18.12.2024 19:08] No deleted papers detected.
[18.12.2024 19:08] Downloading and parsing papers (pdf, html). Total: 11.
[18.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.13147.
[18.12.2024 19:08] Extra JSON file exists (./assets/json/2412.13147.json), skip PDF parsing.
[18.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.13147.json), skip HTML parsing.
[18.12.2024 19:08] Success.
[18.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.12606.
[18.12.2024 19:08] Extra JSON file exists (./assets/json/2412.12606.json), skip PDF parsing.
[18.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.12606.json), skip HTML parsing.
[18.12.2024 19:08] Success.
[18.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.13018.
[18.12.2024 19:08] Extra JSON file exists (./assets/json/2412.13018.json), skip PDF parsing.
[18.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.13018.json), skip HTML parsing.
[18.12.2024 19:08] Success.
[18.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.13171.
[18.12.2024 19:08] Extra JSON file exists (./assets/json/2412.13171.json), skip PDF parsing.
[18.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.13171.json), skip HTML parsing.
[18.12.2024 19:08] Success.
[18.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.12276.
[18.12.2024 19:08] Extra JSON file exists (./assets/json/2412.12276.json), skip PDF parsing.
[18.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.12276.json), skip HTML parsing.
[18.12.2024 19:08] Success.
[18.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.13180.
[18.12.2024 19:08] Extra JSON file exists (./assets/json/2412.13180.json), skip PDF parsing.
[18.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.13180.json), skip HTML parsing.
[18.12.2024 19:08] Success.
[18.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.13194.
[18.12.2024 19:08] Extra JSON file exists (./assets/json/2412.13194.json), skip PDF parsing.
[18.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.13194.json), skip HTML parsing.
[18.12.2024 19:08] Success.
[18.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.10704.
[18.12.2024 19:08] Extra JSON file exists (./assets/json/2412.10704.json), skip PDF parsing.
[18.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.10704.json), skip HTML parsing.
[18.12.2024 19:08] Success.
[18.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.11713.
[18.12.2024 19:08] Extra JSON file exists (./assets/json/2412.11713.json), skip PDF parsing.
[18.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.11713.json), skip HTML parsing.
[18.12.2024 19:08] Success.
[18.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.12527.
[18.12.2024 19:08] Extra JSON file exists (./assets/json/2412.12527.json), skip PDF parsing.
[18.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.12527.json), skip HTML parsing.
[18.12.2024 19:08] Success.
[18.12.2024 19:08] Downloading and parsing paper https://huggingface.co/papers/2412.12877.
[18.12.2024 19:08] Extra JSON file exists (./assets/json/2412.12877.json), skip PDF parsing.
[18.12.2024 19:08] Paper image links file exists (./assets/img_data/2412.12877.json), skip HTML parsing.
[18.12.2024 19:08] Success.
[18.12.2024 19:08] Enriching papers with extra data.
[18.12.2024 19:08] ********************************************************************************
[18.12.2024 19:08] Abstract 0. The rapid advancement of Large Language Models (LLMs) has demonstrated remarkable progress in complex reasoning tasks. However, a significant discrepancy persists between benchmark performances and real-world applications. We identify this gap as primarily stemming from current evaluation protocols ...
[18.12.2024 19:08] ********************************************************************************
[18.12.2024 19:08] Abstract 1. The rapidly developing field of large multimodal models (LMMs) has led to the emergence of diverse models with remarkable capabilities. However, existing benchmarks fail to comprehensively, objectively and accurately evaluate whether LMMs align with the diverse needs of humans in real-world scenario...
[18.12.2024 19:08] ********************************************************************************
[18.12.2024 19:08] Abstract 2. As a typical and practical application of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) techniques have gained extensive attention, particularly in vertical domains where LLMs may lack domain-specific knowledge. In this paper, we introduce an omnidirectional and automatic RAG be...
[18.12.2024 19:08] ********************************************************************************
[18.12.2024 19:08] Abstract 3. Chain-of-thought (CoT) decoding enables language models to improve reasoning performance at the cost of high generation latency in decoding. Recent proposals have explored variants of contemplation tokens, a term we introduce that refers to special tokens used during inference to allow for extra com...
[18.12.2024 19:08] ********************************************************************************
[18.12.2024 19:08] Abstract 4. Humans distill complex experiences into fundamental abstractions that enable rapid learning and adaptation. Similarly, autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. In this paper, we propose concept encoding-decoding mechanis...
[18.12.2024 19:08] ********************************************************************************
[18.12.2024 19:08] Abstract 5. Recent works on accelerating Vision-Language Models show that strong performance can be maintained across a variety of vision-language tasks despite highly compressing visual information. In this work, we examine the popular acceleration approach of early pruning of visual tokens inside the language...
[18.12.2024 19:08] ********************************************************************************
[18.12.2024 19:08] Abstract 6. The vision of a broadly capable and goal-directed agent, such as an Internet-browsing agent in the digital world and a household humanoid in the physical world, has rapidly advanced, thanks to the generalization capability of foundation models. Such a generalist agent needs to have a large and diver...
[18.12.2024 19:08] ********************************************************************************
[18.12.2024 19:08] Abstract 7. Understanding information from a collection of multiple documents, particularly those with visually rich elements, is important for document-grounded question answering. This paper introduces VisDoMBench, the first comprehensive benchmark designed to evaluate QA systems in multi-document settings wi...
[18.12.2024 19:08] ********************************************************************************
[18.12.2024 19:08] Abstract 8. In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks...
[18.12.2024 19:08] ********************************************************************************
[18.12.2024 19:08] Abstract 9. Large Language Models (LLMs) demonstrate exceptional performance across diverse tasks by leveraging both pre-trained knowledge (i.e., parametric knowledge) and external knowledge (i.e., contextual knowledge). While substantial efforts have been made to leverage both forms of knowledge, scenarios in ...
[18.12.2024 19:08] ********************************************************************************
[18.12.2024 19:08] Abstract 10. Recent AI-based video editing has enabled users to edit videos through simple text prompts, significantly simplifying the editing process. However, recent zero-shot video editing techniques primarily focus on global or single-object edits, which can lead to unintended changes in other parts of the v...
[18.12.2024 19:08] Read previous papers.
[18.12.2024 19:08] Generating reviews via LLM API.
[18.12.2024 19:08] Using data from previous issue: {"categories": ["#math", "#benchmark", "#evaluation", "#reasoning", "#leakage"], "emoji": "üßÆ", "ru": {"title": "–ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∑–∞–¥–∞—á–∞—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å
[18.12.2024 19:08] Using data from previous issue: {"categories": ["#reasoning", "#alignment", "#benchmark", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–ú–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Multi-Dimensional Insights (MDI) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. MDI –≤–∫–ª—é—á–∞–µ—Ç –±–æ–ª–µ–µ 500 –∏–∑–æ–±—Ä
[18.12.2024 19:08] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#rag", "#science"], "emoji": "üìä", "ru": {"title": "OmniEval: –í—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –æ—Ü–µ–Ω–∫–∞ RAG-—Å–∏—Å—Ç–µ–º –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OmniEval - –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ Retrieval-Augmented Generation (RAG) –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ. –ê–≤—Ç
[18.12.2024 19:08] Using data from previous issue: {"categories": ["#training", "#architecture", "#reasoning", "#inference"], "emoji": "üß†", "ru": {"title": "–°–∂–∞—Ç—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Compressed Chain-of-Thought (CCoT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º
[18.12.2024 19:08] Using data from previous issue: {"categories": ["#synthetic", "#interpretability", "#transfer_learning", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Ç–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ: –∫–∞–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º—ã –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (ICL) —É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ
[18.12.2024 19:08] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#inference", "#training", "#optimization"], "emoji": "ü™∂", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —É—Å–∫–æ—Ä–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (Vision-Language
[18.12.2024 19:08] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#rl", "#agents", "#agi"], "emoji": "ü§ñ", "ru": {"title": "–ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤: –æ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∑–∞–¥–∞—á –¥–æ –∏—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Proposer-Agent-Evaluator (PAE) –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ
[18.12.2024 19:08] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#rag", "#reasoning", "#optimization", "#games", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π RAG –¥–ª—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VisDoMBench - –ø–µ—Ä–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º –≤–æ–ø—Ä
[18.12.2024 19:08] Using data from previous issue: {"categories": ["#agents", "#open_source", "#science", "#plp"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞: LLM –Ω–∞ —Å—Ç—Ä–∞–∂–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –≤ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–º –∫–æ–¥–µ. –ê–≤—Ç–æ—Ä—ã 
[18.12.2024 19:08] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#alignment", "#hallucinations", "#training"], "emoji": "üß†", "ru": {"title": "CDA: —É–º–Ω–æ–µ –≤–æ–∑–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–ª—è –Ω–∞–¥–µ–∂–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Contrastive Deco
[18.12.2024 19:08] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#video", "#optimization", "#leakage"], "emoji": "üé¨", "ru": {"title": "MIVE: –¢–æ—á–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π MIVE
[18.12.2024 19:08] Loading Chinese text from previous data.
[18.12.2024 19:08] Renaming data file.
[18.12.2024 19:08] Renaming previous data. hf_papers.json to ./d/2024-12-18.json
[18.12.2024 19:08] Saving new data file.
[18.12.2024 19:08] Generating page.
[18.12.2024 19:08] Renaming previous page.
[18.12.2024 19:08] Renaming previous data. index.html to ./d/2024-12-18.html
[18.12.2024 19:08] [Experimental] Generating Chinese page for reading.
[18.12.2024 19:08] Chinese vocab [{'word': 'Â§ßÂûã', 'pinyin': 'd√† x√≠ng', 'trans': 'large-scale'}, {'word': 'ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'y«î y√°n m√≥ x√≠ng', 'trans': 'language model'}, {'word': 'Â§çÊùÇ', 'pinyin': 'f√π z√°', 'trans': 'complex'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´ l«ê', 'trans': 'reasoning'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®n wu', 'trans': 'task'}, {'word': 'ÂèñÂæó', 'pinyin': 'q«î d√©', 'trans': 'achieve'}, {'word': 'ËøõÂ±ï', 'pinyin': 'j√¨n zh«én', 'trans': 'progress'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ÊµãËØï', 'pinyin': 'c√® sh√¨', 'trans': 'test'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'ÂÆûÈôÖ', 'pinyin': 'sh√≠ j√¨', 'trans': 'actual'}, {'word': 'Â∫îÁî®', 'pinyin': 'y√¨ng y√≤ng', 'trans': 'application'}, {'word': 'Â∑ÆË∑ù', 'pinyin': 'chƒÅ j√π', 'trans': 'gap'}, {'word': 'Â≠òÂú®', 'pinyin': 'c√∫n z√†i', 'trans': 'exist'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluation'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'ÊåáÊ†á', 'pinyin': 'zh«ê biƒÅo', 'trans': 'metric'}, {'word': 'ÊçïÊçâ', 'pinyin': 'b«î zhu≈ç', 'trans': 'capture'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'capability'}, {'word': 'ÂáÜÁ°ÆÊÄß', 'pinyin': 'zh«în qu√® x√¨ng', 'trans': 'accuracy'}, {'word': '‰∏ÄËá¥ÊÄß', 'pinyin': 'yƒ´ zh√¨ x√¨ng', 'trans': 'consistency'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'ÈááÊ†∑', 'pinyin': 'c«éi y√†ng', 'trans': 'sampling'}, {'word': 'ËøûÁª≠', 'pinyin': 'li√°n x√π', 'trans': 'continuous'}, {'word': 'ÈáèÂåñ', 'pinyin': 'li√†ng hu√†', 'trans': 'quantify'}, {'word': 'ÊΩúÂäõ', 'pinyin': 'qi√°n l√¨', 'trans': 'potential'}, {'word': 'Á®≥ÂÆöÊÄß', 'pinyin': 'wƒõn d√¨ng x√¨ng', 'trans': 'stability'}, {'word': 'Êé®Âá∫', 'pinyin': 'tuƒ´ ch≈´', 'trans': 'launch'}, {'word': 'Âä®ÊÄÅ', 'pinyin': 'd√≤ng t√†i', 'trans': 'dynamic'}, {'word': 'ÊåëÊàòÊÄß', 'pinyin': 'ti«éo zh√†n x√¨ng', 'trans': 'challenging'}, {'word': 'ÂΩì‰ª£', 'pinyin': 'dƒÅng d√†i', 'trans': 'contemporary'}, {'word': 'Êï∞Â≠¶', 'pinyin': 'sh√π xu√©', 'trans': 'mathematics'}, {'word': 'ÈóÆÈ¢ò', 'pinyin': 'w√®n t√≠', 'trans': 'problem'}, {'word': 'Êó®Âú®', 'pinyin': 'zh«ê z√†i', 'trans': 'aim to'}, {'word': 'ÂáèÂ∞ë', 'pinyin': 'ji«én sh«éo', 'trans': 'reduce'}, {'word': 'Ê≥ÑÈú≤', 'pinyin': 'xi√® l√≤u', 'trans': 'leak'}, {'word': 'È£éÈô©', 'pinyin': 'fƒìng xi«én', 'trans': 'risk'}, {'word': 'ËØ¶ÁªÜ', 'pinyin': 'xi√°ng x√¨', 'trans': 'detailed'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}, {'word': 'ËÆøÈóÆ', 'pinyin': 'f«éng w√®n', 'trans': 'access'}]
[18.12.2024 19:08] Renaming previous Chinese page.
[18.12.2024 19:08] Renaming previous data. zh.html to ./d/2024-12-17_zh_reading_task.html
[18.12.2024 19:08] Writing Chinese reading task.
[18.12.2024 19:08] Writing result.
[18.12.2024 19:08] Renaming log file.
[18.12.2024 19:08] Renaming previous data. log.txt to ./logs/2024-12-18_last_log.txt
