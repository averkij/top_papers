[25.12.2025 05:25] Read previous papers.
[25.12.2025 05:25] Generating top page (month).
[25.12.2025 05:25] Writing top page (month).
[25.12.2025 06:35] Read previous papers.
[25.12.2025 06:35] Get feed.
[25.12.2025 06:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20557
[25.12.2025 06:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.16093
[25.12.2025 06:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21094
[25.12.2025 06:35] Extract page data from URL. URL: https://huggingface.co/papers/2512.21337
[25.12.2025 06:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21338
[25.12.2025 06:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20856
[25.12.2025 06:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.20848
[25.12.2025 06:35] Extract page data from URL. URL: https://huggingface.co/papers/2512.21252
[25.12.2025 06:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21334
[25.12.2025 06:35] Extract page data from URL. URL: https://huggingface.co/papers/2512.21004
[25.12.2025 06:35] Extract page data from URL. URL: https://huggingface.co/papers/2512.20757
[25.12.2025 06:35] Get page data from previous paper. URL: https://huggingface.co/papers/2512.21010
[25.12.2025 06:35] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.12.2025 06:35] No deleted papers detected.
[25.12.2025 06:35] Downloading and parsing papers (pdf, html). Total: 12.
[25.12.2025 06:35] Downloading and parsing paper https://huggingface.co/papers/2512.20557.
[25.12.2025 06:35] Extra JSON file exists (./assets/json/2512.20557.json), skip PDF parsing.
[25.12.2025 06:35] Paper image links file exists (./assets/img_data/2512.20557.json), skip HTML parsing.
[25.12.2025 06:35] Success.
[25.12.2025 06:35] Downloading and parsing paper https://huggingface.co/papers/2512.16093.
[25.12.2025 06:35] Extra JSON file exists (./assets/json/2512.16093.json), skip PDF parsing.
[25.12.2025 06:35] Paper image links file exists (./assets/img_data/2512.16093.json), skip HTML parsing.
[25.12.2025 06:35] Success.
[25.12.2025 06:35] Downloading and parsing paper https://huggingface.co/papers/2512.21094.
[25.12.2025 06:35] Extra JSON file exists (./assets/json/2512.21094.json), skip PDF parsing.
[25.12.2025 06:35] Paper image links file exists (./assets/img_data/2512.21094.json), skip HTML parsing.
[25.12.2025 06:35] Success.
[25.12.2025 06:35] Downloading and parsing paper https://huggingface.co/papers/2512.21337.
[25.12.2025 06:35] Downloading paper 2512.21337 from https://arxiv.org/pdf/2512.21337v1...
[25.12.2025 06:35] Extracting affiliations from text.
[25.12.2025 06:35] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 7 3 3 1 2 . 2 1 5 2 : r Beyond Memorization: Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models Li-Zhong Szu-Tu Ting-Lin Wu Chia-Jui Chang He Syu Yu-Lun Liu Figure 1. Overview of YEARGUESSR and YearCLIP. (a) Global distribution of the 55k Wikipedia-sourced building images. (b) Log-scale histogram of construction years spanning 10012024 CE. (c) Display all the attributes with an example in YEARGUESSR (d) Given an image and optional GPS coordinates, YearCLIP returns the estimated construction year together with an architectural rationale. "
[25.12.2025 06:35] Response: ```python
[]
```
[25.12.2025 06:35] Extracting affiliations from text.
[25.12.2025 06:35] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 7 3 3 1 2 . 2 1 5 2 : r Beyond Memorization: Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models Li-Zhong Szu-Tu Ting-Lin Wu Chia-Jui Chang He Syu Yu-Lun LiuFigure 1. Overview of YEARGUESSR and YearCLIP. (a) Global distribution of the 55k Wikipedia-sourced building images. (b) Log-scale histogram of construction years spanning 10012024 CE. (c) Display all the attributes with an example in YEARGUESSR (d) Given an image and optional GPS coordinates, YearCLIP returns the estimated construction year together with an architectural rationale.We expose significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (10012024), GPS data, and page-view counts as proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, Equal contribution. memorized items but struggle significantly with unrecognized subjects, exposing critical flaw in their reasoning capabilities. Project page: https://sytwu.github. io/BeyondMemo/ 1. Introduction The construction year of building is essential for sustainability audits, heritage preservation, and post-disaster assessment. Yet, unlike style or material, the age of most of the worlds 1.5 trillion mapped buildings is unknown. Automated building age estimation could support continent-scale retrofitting and fine-grained historical queries, but progress is hindered by the lack of global, large-scale, open benchmark. Figure 1 previews YearGuessr, our answer to this gap. As shown in Table 1, existing datasets are either geographically narrow or temporally shallow. MyCD dataset [14] covers only Western Europe with coarse epochs (<1930, 1 Table 1. Building-age datasets. Earlier datasets are either regional, post-1900, have no images, or are closed source. Our YEARGUESSR will be the first CC BY-SA 4.0 image set with continuous labels, global (157 countries) coverage, and 10012024 CE span. Dataset Region(s) Year span Size Modalities Continuous labels Images Open? MyCD [14] CMAB [79] MTBF-33 [60] ResBldgAge [49] 3D-GIS Age [8] UrbanFormAge [45] GPT-4V London [74] PhotoAge [75] StreetViewAge [57] WikiChurches [5] YEARGUESSR (ours) Europe China USA (33 counties) UK NL NL, FR, ES London Austria NL (AMS) Europe 157 countries < 1930 > 2006 1985 2018 1900 2015 < 1915 > 1980 1860 2017 < 1945 2010 < 1700 > 2020 1960s 2010s 1300 2000 401 2011 1001 32k img 29M bldg 6.2M img 2.5k img 35k bldg 25.3M img OSM, 2-D urban form SVI, VHR, MSI RS, SVI, GIS, HS Footprints (SHP) Maps, census LOD1 3-D GIS 131 img 11.1k img 39k img 9.5k img 55.5k img Facade + attrs real estate, web SVI + BAG Wikipedia facades Wikipedia facades 19301959, ...), CMAB [79] is restricted to modern Chinese facades and narrow temporal range (19852018), MTBF33 [74] lacks photographs, and others remain geographically or temporally narrow. Prior works also cast age prediction as classification, ignoring temporal ordinality, while licensing restrictions impede reproducibility. Beyond data gaps, fundamental question remains: do vision-language models (VLMs) truly learn architecture, or simply memorize landmarks? We show that VLMs like Gemini-2.0 gain +34.18% accuracy on high-popularity buildings, whereas other general models degrade (Section 5). It depicts the evidence of popularity memorization rather than genuine architectural understanding. To address these issues, we present YearGuessr, the first open benchmark for building-age estimation with 55,546 images across 157 countries, spanning 10012024 CE in Figure 1(a) and (b), along with our proposed model, YearCLIP. Each entry includes facades, GPS, captions, and Wikipedia page views to probe memorization bias. We frame the task as ordinal regression and evaluate with MAE, Interval Accuracy, and new popularity-aware metric, demonstrating that VLMs performance might stem from memorization rather than architectural understanding potentially. Our YearCLIP (Section 4) is CLIP-based model that integrates ordinal training using coarse-to-fine strategy from NumCLIP [16], and GPS priors via fused location encoder with zero-convolution. It is also notably enhanced with several pre-defined reasoning prompts (e.g., Roof (spire, dome), Wall (brick, stone)) that enrich the feature space. This allows the model to provide human-verifiable rationales by highlighting the most relevant architectural cues, adding crucial layer of explainability to its final year prediction. Our contributions are: YearGuessr dataset: will be the first open, CC BY-SA 4.0 corpus with global coverage, large-scale ordinal labels, GPS, and rich textual descriptions. YearCLIP model: We propose YearCLIP, our baseline model, which is enhanced with reasoning prompts to provide explainability for the age prediction task. Evaluation protocol: an original regression benchmark with Interval Accuracy and new popularity-based MAE metric to quantify memorization bias. Baseline: We provide comprehensive benchmark study of 30+ CNN-, Transformer-, CLIP-based, and LLM/VLM models. Insights: We reveal that VLMs memorize popular landmarks, achieving dramatically different performance based on building fame rather than architectural features. 2. Related Work Datasets for building-age estimation. Prior corpora remain limited in scope. Building Age Estimation [14] fuses multiple modalities but covers only Western Europe and coarse epochs. CMAB adds Chinese fa√ßades yet spans 19852018 [79]. MTBF-33 lists U.S. footprints without imagery [60]. Other efforts are small-scale (131 London photos [74]) or style-focused (WikiChurches [5]). Early work on architectural style recognition [4042] and heritage analysis [38] established foundations for visual classification. Broader surveys [17] situate AI in preservation. Earlier datasets often rely on real-estate photos or GIS metadata [8, 32, 57]. In contrast, YEARGUE"
[25.12.2025 06:35] Mistral response. {"id": "5d389a55fbf149c5899908870e4cc900", "created": 1766644557, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1747, "total_tokens": 1753, "completion_tokens": 6}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[]\n```"}}]}
[25.12.2025 06:35] Response: ```python
[]
```
[25.12.2025 06:35] Deleting PDF ./assets/pdf/2512.21337.pdf.
[25.12.2025 06:35] Success.
[25.12.2025 06:35] Downloading and parsing paper https://huggingface.co/papers/2512.21338.
[25.12.2025 06:35] Extra JSON file exists (./assets/json/2512.21338.json), skip PDF parsing.
[25.12.2025 06:35] Paper image links file exists (./assets/img_data/2512.21338.json), skip HTML parsing.
[25.12.2025 06:35] Success.
[25.12.2025 06:35] Downloading and parsing paper https://huggingface.co/papers/2512.20856.
[25.12.2025 06:35] Extra JSON file exists (./assets/json/2512.20856.json), skip PDF parsing.
[25.12.2025 06:35] Paper image links file exists (./assets/img_data/2512.20856.json), skip HTML parsing.
[25.12.2025 06:35] Success.
[25.12.2025 06:35] Downloading and parsing paper https://huggingface.co/papers/2512.20848.
[25.12.2025 06:35] Extra JSON file exists (./assets/json/2512.20848.json), skip PDF parsing.
[25.12.2025 06:35] Paper image links file exists (./assets/img_data/2512.20848.json), skip HTML parsing.
[25.12.2025 06:35] Success.
[25.12.2025 06:35] Downloading and parsing paper https://huggingface.co/papers/2512.21252.
[25.12.2025 06:35] Downloading paper 2512.21252 from https://arxiv.org/pdf/2512.21252v1...
[25.12.2025 06:36] Extracting affiliations from text.
[25.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 2 5 2 1 2 . 2 1 5 2 : r DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation Jiawei Liu, Junqiao Li, Jiangfan Deng, Gen Li, Siyu Zhou, Zetao Fang, Shanshan Lao, Zengde Deng, Jianing Zhu, Tingting Ma, Jiayi Li, Yunqiu Wang, Qian He, Xinglong Wu Intelligence Creation Team, ByteDance "
[25.12.2025 06:36] Response: ```python
["Intelligence Creation Team, ByteDance"]
```
[25.12.2025 06:36] Deleting PDF ./assets/pdf/2512.21252.pdf.
[25.12.2025 06:36] Success.
[25.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.21334.
[25.12.2025 06:36] Extra JSON file exists (./assets/json/2512.21334.json), skip PDF parsing.
[25.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.21334.json), skip HTML parsing.
[25.12.2025 06:36] Success.
[25.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.21004.
[25.12.2025 06:36] Downloading paper 2512.21004 from https://arxiv.org/pdf/2512.21004v1...
[25.12.2025 06:36] Extracting affiliations from text.
[25.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations Jinghan Li Yang Jin* Hao Jiang Yadong Mu Yang Song Kun Xu* Peking University {li.jh, jiny}@stu.pku.edu.cn, syxu828@gmail.com https://github.com/Singularity0104/NExT-Vid 5 2 0 2 4 2 ] . [ 1 4 0 0 1 2 . 2 1 5 2 : r a "
[25.12.2025 06:36] Response: ```python
["Peking University"]
```
[25.12.2025 06:36] Deleting PDF ./assets/pdf/2512.21004.pdf.
[25.12.2025 06:36] Success.
[25.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.20757.
[25.12.2025 06:36] Downloading paper 2512.20757 from https://arxiv.org/pdf/2512.20757v1...
[25.12.2025 06:36] Extracting affiliations from text.
[25.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Preprint. TokSuite: MEASURING THE IMPACT OF TOKENIZER CHOICE ON LANGUAGE MODEL BEHAVIOR ul Sena Altƒ±ntas1,2 Malikeh Ehghaghi1,2 Brian Lester1,2,3 Fengyuan Liu4,5 Wanru Zhao6 Marco Ciccone2 Colin Raffel1,2,7 1University of Toronto 5Mila - Quebec AI Institute *Equal contribution 6University of Cambridge 3Google DeepMind 4McGill University 2Vector Institute 7Hugging Face 5 2 0 2 3 2 ] . [ 1 7 5 7 0 2 . 2 1 5 2 : r a "
[25.12.2025 06:36] Response: ```python
[
    "University of Toronto",
    "Vector Institute",
    "Google DeepMind",
    "McGill University",
    "Mila - Quebec AI Institute",
    "University of Cambridge",
    "Hugging Face"
]
```
[25.12.2025 06:36] Deleting PDF ./assets/pdf/2512.20757.pdf.
[25.12.2025 06:36] Success.
[25.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.21010.
[25.12.2025 06:36] Extra JSON file exists (./assets/json/2512.21010.json), skip PDF parsing.
[25.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.21010.json), skip HTML parsing.
[25.12.2025 06:36] Success.
[25.12.2025 06:36] Enriching papers with extra data.
[25.12.2025 06:36] ********************************************************************************
[25.12.2025 06:36] Abstract 0. DSR Suite enhances vision-language models with dynamic spatial reasoning through automated data generation and a geometry selection module that integrates geometric priors.  					AI-generated summary 				 Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial ...
[25.12.2025 06:36] ********************************************************************************
[25.12.2025 06:36] Abstract 1. TurboDiffusion accelerates video generation by 100-200x using attention acceleration, step distillation, and quantization, while maintaining video quality.  					AI-generated summary 				 We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion ge...
[25.12.2025 06:36] ********************************************************************************
[25.12.2025 06:36] Abstract 2. Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction...
[25.12.2025 06:36] ********************************************************************************
[25.12.2025 06:36] Abstract 3. We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce t...
[25.12.2025 06:36] ********************************************************************************
[25.12.2025 06:36] Abstract 4. High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduce...
[25.12.2025 06:36] ********************************************************************************
[25.12.2025 06:36] Abstract 5. We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up ...
[25.12.2025 06:36] ********************************************************************************
[25.12.2025 06:36] Abstract 6. We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments...
[25.12.2025 06:36] ********************************************************************************
[25.12.2025 06:36] Abstract 7. The "one-shot" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically...
[25.12.2025 06:36] ********************************************************************************
[25.12.2025 06:36] Abstract 8. We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action u...
[25.12.2025 06:36] ********************************************************************************
[25.12.2025 06:36] Abstract 9. Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which o...
[25.12.2025 06:36] ********************************************************************************
[25.12.2025 06:36] Abstract 10. Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To addres...
[25.12.2025 06:36] ********************************************************************************
[25.12.2025 06:36] Abstract 11. The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring,...
[25.12.2025 06:36] Read previous papers.
[25.12.2025 06:36] Generating reviews via LLM API.
[25.12.2025 06:36] Using data from previous issue: {"categories": ["#dataset", "#architecture", "#video", "#multimodal", "#benchmark", "#3d"], "emoji": "üé¨", "ru": {"title": "–ì–µ–æ–º–µ—Ç—Ä–∏—è –≤ –¥–≤–∏–∂–µ–Ω–∏–∏: –æ–±—É—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ–ø–æ–Ω–∏–º–∞–Ω–∏—é —á–µ—Ä–µ–∑ 4D —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ DSR Suite –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è vision-language –º–æ–¥–µ–ª–µ–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —Ä–∞—Å—Å—É–∂–¥–∞
[25.12.2025 06:36] Using data from previous issue: {"categories": ["#architecture", "#video", "#open_source", "#diffusion", "#inference", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ú–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ: 100-200x —É—Å–∫–æ—Ä–µ–Ω–∏–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "TurboDiffusion ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ—É–∑
[25.12.2025 06:36] Using data from previous issue: {"categories": ["#dataset", "#video", "#multimodal", "#benchmark", "#audio", "#survey"], "emoji": "üé¨", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏ –∑–≤—É–∫–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç T2AV-Compass ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–∏—Å—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ –∏
[25.12.2025 06:36] Querying the API.
[25.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/
[25.12.2025 06:36] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –∞–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Å–µ—Ä—å—ë–∑–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ (VLM): –æ–Ω–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ 34% –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∑–¥–∞–Ω–∏–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—ã—á–Ω—ã–º–∏, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è. –î–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è —ç—Ç–æ–≥–æ —è–≤–ª–µ–Ω–∏—è –æ–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ YearGuessr ‚Äî —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å 55,546 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∑–¥–∞–Ω–∏–π –∏–∑ 157 —Å—Ç—Ä–∞–Ω —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –æ –≥–æ–¥–µ –ø–æ—Å—Ç—Ä–æ–π–∫–∏, –≥–µ–æ–ª–æ–∫–∞—Ü–∏–µ–π –∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å—é. –ê–≤—Ç–æ—Ä—ã —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–ª–∏ –∑–∞–¥–∞—á—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≥–æ–¥–∞ –ø–æ—Å—Ç—Ä–æ–π–∫–∏ –∫–∞–∫ –æ—Ä–¥–∏–Ω–∞–ª—å–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –∏ –≤–≤–µ–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 30+ –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è –Ω–æ–≤—É—é YearCLIP, –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–æ, —á—Ç–æ VLM —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö, –∑–∞–ø–æ–º–Ω–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–∞—Ö –∏ –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏, —á—Ç–æ –≤—ã—è–≤–ª—è–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –≤ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±–æ–±—â–µ–Ω–∏—é.",
  "emoji": "üèõÔ∏è",
  "title": "–ö–æ–≥–¥–∞ –º–æ–¥–µ–ª–∏ –∑–∞–ø–æ–º–∏–Ω–∞—é—Ç –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–æ–Ω–∏–º–∞—Ç—å: —Ä–∞–∑–æ–±–ª–∞—á–µ–Ω–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–Ω–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö"
}
```
[25.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/"

[25.12.2025 06:36] Response: ```python
['DATASET', 'BENCHMARK', 'MULTIMODAL', 'CV']
```
[25.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/"

[25.12.2025 06:36] Response: ```python
['BIAS', 'BENCHMARK', 'VISION_LANGUAGE_MODELS']
```

Wait, let me reconsider based on the provided topic list only:

```python
['OPEN_SOURCE']
```

The paper releases the YearGuessr dataset and appears to make resources publicly available (indicated by "Project page" URL), which aligns with the OPEN_SOURCE topic of contributing datasets to the public.
[25.12.2025 06:36] Error. Failed to parse JSON from LLM. ["BIAS", "BENCHMARK", "VISION_LANGUAGE_MODELS"]


Wait, let me reconsider based on the provided topic list only:


["OPEN_SOURCE"]


The paper releases the YearGuessr dataset and appears to make resources publicly available (indicated by "Project page" URL), which aligns with the OPEN_SOURCE topic of contributing datasets to the public.
[25.12.2025 06:36] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper reveals a notable popularity bias in vision-language models (VLMs), showing they perform significantly better on well-known buildings than on lesser-known ones. The authors present the YearGuessr dataset, which includes over 55,000 images of buildings from around the world, annotated with their construction years and popularity metrics. They reformulate the task of predicting construction years as ordinal regression and introduce new metrics to measure the impact of popularity on model performance. The findings indicate that while VLMs can memorize popular items effectively, they struggle with less recognized subjects, highlighting a major limitation in their generalization abilities.","title":"Unveiling Popularity Bias in Vision-Language Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper reveals a notable popularity bias in vision-language models (VLMs), showing they perform significantly better on well-known buildings than on lesser-known ones. The authors present the YearGuessr dataset, which includes over 55,000 images of buildings from around the world, annotated with their construction years and popularity metrics. They reformulate the task of predicting construction years as ordinal regression and introduce new metrics to measure the impact of popularity on model performance. The findings indicate that while VLMs can memorize popular items effectively, they struggle with less recognized subjects, highlighting a major limitation in their generalization abilities.', title='Unveiling Popularity Bias in Vision-Language Models'))
[25.12.2025 06:36] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Á†îÁ©∂Êè≠Á§∫‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂ≠òÂú®ÊòæËëóÁöÑÊµÅË°åÊÄßÂÅèÂ∑ÆÔºåËøô‰∫õÊ®°ÂûãÂú®ËëóÂêçÂª∫Á≠ë‰∏äÁöÑÂáÜÁ°ÆÁéáÊØîÊôÆÈÄöÂª∫Á≠ëÈ´òÂá∫34%„ÄÇ‰∏∫‰∫ÜÁ≥ªÁªüÂú∞Á†îÁ©∂Ëøô‰∏ÄÁé∞Ë±°ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜYearGuessrÊï∞ÊçÆÈõÜÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´55,546Âº†Âª∫Á≠ëÂõæÂÉèÁöÑÂºÄÊîæÂü∫ÂáÜÔºåÊ∂µÁõñ157‰∏™ÂõΩÂÆ∂ÔºåÂπ∂ÈôÑÊúâÂª∫Á≠ëÂπ¥‰ªΩ„ÄÅGPSÊï∞ÊçÆÂíåÈ°µÈù¢ÊµèËßàÈáèÁ≠âÂ§öÊ®°ÊÄÅÂ±ûÊÄß„ÄÇÊàë‰ª¨Â∞ÜÂª∫Á≠ëÂπ¥‰ªΩÈ¢ÑÊµã‰ªªÂä°Ê°ÜÂÆö‰∏∫Â∫èÊï∞ÂõûÂΩíÔºåÂπ∂ÂºïÂÖ•‰∫ÜËÄÉËôëÊµÅË°åÊÄßÁöÑÂå∫Èó¥ÂáÜÁ°ÆÁéáÊåáÊ†áÊù•ÈáèÂåñËøôÁßçÂÅèÂ∑Æ„ÄÇÊàë‰ª¨ÁöÑÂü∫ÂáÜÊµãËØïÁªìÊûúÊòæÁ§∫ÔºåVLMsÂú®ÊµÅË°åÁöÑ„ÄÅË¢´ËÆ∞ÂøÜÁöÑÈ°πÁõÆ‰∏äË°®Áé∞‰ºòÂºÇÔºå‰ΩÜÂú®‰∏çË¢´ËØÜÂà´ÁöÑÂØπË±°‰∏äÂàôÊòæËëóÊå£ÊâéÔºåÊö¥Èú≤‰∫ÜÂÆÉ‰ª¨Êé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆÁº∫Èô∑„ÄÇ","title":"Êè≠Á§∫ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊµÅË°åÊÄßÂÅèÂ∑Æ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Á†îÁ©∂Êè≠Á§∫‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂ≠òÂú®ÊòæËëóÁöÑÊµÅË°åÊÄßÂÅèÂ∑ÆÔºåËøô‰∫õÊ®°ÂûãÂú®ËëóÂêçÂª∫Á≠ë‰∏äÁöÑÂáÜÁ°ÆÁéáÊØîÊôÆÈÄöÂª∫Á≠ëÈ´òÂá∫34%„ÄÇ‰∏∫‰∫ÜÁ≥ªÁªüÂú∞Á†îÁ©∂Ëøô‰∏ÄÁé∞Ë±°ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜYearGuessrÊï∞ÊçÆÈõÜÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´55,546Âº†Âª∫Á≠ëÂõæÂÉèÁöÑÂºÄÊîæÂü∫ÂáÜÔºåÊ∂µÁõñ157‰∏™ÂõΩÂÆ∂ÔºåÂπ∂ÈôÑÊúâÂª∫Á≠ëÂπ¥‰ªΩ„ÄÅGPSÊï∞ÊçÆÂíåÈ°µÈù¢ÊµèËßàÈáèÁ≠âÂ§öÊ®°ÊÄÅÂ±ûÊÄß„ÄÇÊàë‰ª¨Â∞ÜÂª∫Á≠ëÂπ¥‰ªΩÈ¢ÑÊµã‰ªªÂä°Ê°ÜÂÆö‰∏∫Â∫èÊï∞ÂõûÂΩíÔºåÂπ∂ÂºïÂÖ•‰∫ÜËÄÉËôëÊµÅË°åÊÄßÁöÑÂå∫Èó¥ÂáÜÁ°ÆÁéáÊåáÊ†áÊù•ÈáèÂåñËøôÁßçÂÅèÂ∑Æ„ÄÇÊàë‰ª¨ÁöÑÂü∫ÂáÜÊµãËØïÁªìÊûúÊòæÁ§∫ÔºåVLMsÂú®ÊµÅË°åÁöÑ„ÄÅË¢´ËÆ∞ÂøÜÁöÑÈ°πÁõÆ‰∏äË°®Áé∞‰ºòÂºÇÔºå‰ΩÜÂú®‰∏çË¢´ËØÜÂà´ÁöÑÂØπË±°‰∏äÂàôÊòæËëóÊå£ÊâéÔºåÊö¥Èú≤‰∫ÜÂÆÉ‰ª¨Êé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆÁº∫Èô∑„ÄÇ', title='Êè≠Á§∫ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊµÅË°åÊÄßÂÅèÂ∑Æ'))
[25.12.2025 06:36] Using data from previous issue: {"categories": ["#diffusion", "#video", "#optimization", "#inference"], "emoji": "üé¨", "ru": {"title": "–£—Å–∫–æ—Ä–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –∫–æ–º–ø—Ä–µ—Å—Å–∏—é –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è HiStream ‚Äî —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à–∞—é—â–∏—Ö—Å—è –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±
[25.12.2025 06:36] Using data from previous issue: {"categories": ["#architecture", "#agents", "#small_models", "#open_source", "#reasoning", "#inference", "#long_context", "#training", "#rl", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –º–∏–ª–ª–∏–æ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ–º —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ
[25.12.2025 06:36] Using data from previous issue: {"categories": ["#architecture", "#agents", "#benchmark", "#open_source", "#reasoning", "#inference", "#long_context", "#training", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–≠–∫–æ–Ω–æ–º–Ω–∞—è –º–æ—â—å: –≥–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å —Å –ø–æ–ª–æ–≤–∏–Ω–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Nemotron 3 Nano, –≥–∏–±
[25.12.2025 06:36] Querying the API.
[25.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

The "one-shot" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.
[25.12.2025 06:36] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞Ê°ÜÊû∂ DreaMontage –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ –≤ —Å—Ç–∏–ª–µ –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –ø–ª–∞–Ω–∞-—Å—Ü–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç –º–µ—Ö–∞–Ω–∏–∑–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É DiT —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤–∏–¥–µ–æ. –û–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç —Ç–µ—Ö–Ω–∏–∫—É Visual Expression SFT —Å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ö–µ–º–æ–π DPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–ª–∞–≤–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –∏ –ª–æ–≥–∏—á–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏–π –æ–±—ä–µ–∫—Ç–æ–≤. –î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è Segment-wise Auto-Regressive inference, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∞—è –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é.",
  "emoji": "üé¨",
  "title": "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤ –∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –≤–∏–¥–µ–æ-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
}
```
[25.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The "one-shot" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences."

[25.12.2025 06:36] Response: ```python
["VIDEO", "ARCHITECTURE", "TRAINING", "RLHF", "DATASET"]
```
[25.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"The "one-shot" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences."

[25.12.2025 06:36] Response: ```python
['DIFFUSION', 'OPTIMIZATION']
```

**Justification:**

- **DIFFUSION**: The paper explicitly discusses diffusion-based video generation models (mentions "DiT architecture" which is Diffusion Transformer) and focuses on generating videos using diffusion-based approaches.

- **OPTIMIZATION**: The paper addresses optimization of the generation process through multiple techniques including Adaptive Tuning strategy, Visual Expression SFT (Supervised Fine-Tuning), Tailored DPO (Direct Preference Optimization) scheme, and Segment-wise Auto-Regressive inference strategy designed for memory efficiency.
[25.12.2025 06:36] Error. Failed to parse JSON from LLM. ["DIFFUSION", "OPTIMIZATION"]


**Justification:**

- **DIFFUSION**: The paper explicitly discusses diffusion-based video generation models (mentions "DiT architecture" which is Diffusion Transformer) and focuses on generating videos using diffusion-based approaches.

- **OPTIMIZATION**: The paper addresses optimization of the generation process through multiple techniques including Adaptive Tuning strategy, Visual Expression SFT (Supervised Fine-Tuning), Tailored DPO (Direct Preference Optimization) scheme, and Segment-wise Auto-Regressive inference strategy designed for memory efficiency.
[25.12.2025 06:36] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents DreaMontage, a novel framework for generating one-shot videos that are visually smooth and coherent. It addresses the limitations of existing video generation methods by integrating a lightweight conditioning mechanism and an Adaptive Tuning strategy for better frame control. The authors enhance visual quality through a curated dataset and a Visual Expression SFT stage, while also improving motion rationality and transition smoothness with a Tailored DPO scheme. Finally, they introduce a Segment-wise Auto-Regressive inference strategy to efficiently produce long video sequences, resulting in impressive cinematic outputs from user inputs.","title":"Seamless One-Shot Video Generation with DreaMontage"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents DreaMontage, a novel framework for generating one-shot videos that are visually smooth and coherent. It addresses the limitations of existing video generation methods by integrating a lightweight conditioning mechanism and an Adaptive Tuning strategy for better frame control. The authors enhance visual quality through a curated dataset and a Visual Expression SFT stage, while also improving motion rationality and transition smoothness with a Tailored DPO scheme. Finally, they introduce a Segment-wise Auto-Regressive inference strategy to efficiently produce long video sequences, resulting in impressive cinematic outputs from user inputs.', title='Seamless One-Shot Video Generation with DreaMontage'))
[25.12.2025 06:36] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫DreaMontageÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÁîüÊàêÊó†Áºù‰∏îÂØåÊúâË°®Áé∞ÂäõÁöÑÈïøÊó∂ÊÆµ‰∏ÄÈïúÂà∞Â∫ïËßÜÈ¢ë„ÄÇÊàë‰ª¨ÈÄöËøáÈõÜÊàêËΩªÈáèÁ∫ßÁöÑ‰∏≠Èó¥Êù°‰ª∂Êú∫Âà∂ÂíåËá™ÈÄÇÂ∫îË∞É‰ºòÁ≠ñÁï•ÔºåÂ¢ûÂº∫‰∫ÜÂØπ‰ªªÊÑèÂ∏ßÁöÑÊéßÂà∂ËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜÊèêÈ´òËßÜËßâË¥®ÈáèÂíåÁîµÂΩ±Ë°®Áé∞ÂäõÔºåÊàë‰ª¨ÊûÑÂª∫‰∫ÜÈ´òË¥®ÈáèÁöÑÊï∞ÊçÆÈõÜÔºåÂπ∂ÂÆûÊñΩ‰∫ÜËßÜËßâË°®ËææÁöÑÂæÆË∞ÉÈò∂ÊÆµ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ËÆæËÆ°‰∫ÜÊÆµÂºèËá™ÂõûÂΩíÊé®ÁêÜÁ≠ñÁï•Ôºå‰ª•È´òÊïàÁöÑÊñπÂºèÁîüÊàêÊâ©Â±ïÂ∫èÂàóÔºåÁ°Æ‰øùÁîüÊàêÂÜÖÂÆπÁöÑËøûË¥ØÊÄßÂíåËßÜËßâÂÜ≤ÂáªÂäõ„ÄÇ","title":"Êó†ÁºùÁîüÊàê‰∏ÄÈïúÂà∞Â∫ïËßÜÈ¢ëÁöÑÂàõÊñ∞Ê°ÜÊû∂"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫DreaMontageÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÁîüÊàêÊó†Áºù‰∏îÂØåÊúâË°®Áé∞ÂäõÁöÑÈïøÊó∂ÊÆµ‰∏ÄÈïúÂà∞Â∫ïËßÜÈ¢ë„ÄÇÊàë‰ª¨ÈÄöËøáÈõÜÊàêËΩªÈáèÁ∫ßÁöÑ‰∏≠Èó¥Êù°‰ª∂Êú∫Âà∂ÂíåËá™ÈÄÇÂ∫îË∞É‰ºòÁ≠ñÁï•ÔºåÂ¢ûÂº∫‰∫ÜÂØπ‰ªªÊÑèÂ∏ßÁöÑÊéßÂà∂ËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜÊèêÈ´òËßÜËßâË¥®ÈáèÂíåÁîµÂΩ±Ë°®Áé∞ÂäõÔºåÊàë‰ª¨ÊûÑÂª∫‰∫ÜÈ´òË¥®ÈáèÁöÑÊï∞ÊçÆÈõÜÔºåÂπ∂ÂÆûÊñΩ‰∫ÜËßÜËßâË°®ËææÁöÑÂæÆË∞ÉÈò∂ÊÆµ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ËÆæËÆ°‰∫ÜÊÆµÂºèËá™ÂõûÂΩíÊé®ÁêÜÁ≠ñÁï•Ôºå‰ª•È´òÊïàÁöÑÊñπÂºèÁîüÊàêÊâ©Â±ïÂ∫èÂàóÔºåÁ°Æ‰øùÁîüÊàêÂÜÖÂÆπÁöÑËøûË¥ØÊÄßÂíåËßÜËßâÂÜ≤ÂáªÂäõ„ÄÇ', title='Êó†ÁºùÁîüÊàê‰∏ÄÈïúÂà∞Â∫ïËßÜÈ¢ëÁöÑÂàõÊñ∞Ê°ÜÊû∂'))
[25.12.2025 06:36] Using data from previous issue: {"categories": ["#dataset", "#video", "#multimodal", "#benchmark"], "emoji": "üìπ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Streamo ‚Äî –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±
[25.12.2025 06:36] Querying the API.
[25.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.
[25.12.2025 06:36] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω NExT-Vid ‚Äî –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–∏–ø–∞ BERT, –º–µ—Ç–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π –∫–∞–¥—Ä, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ —É—á–∏—Ç—ã–≤–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç–µ. –ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ –≤ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ flow-matching –¥–µ–∫–æ–¥–µ—Ä–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.",
  "emoji": "üé¨",
  "title": "–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –≤–∏–¥–µ–æ–∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–∞–¥—Ä–∞"
}
```
[25.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification."

[25.12.2025 06:36] Response: ```python
["VIDEO", "ARCHITECTURE", "TRAINING", "MULTIMODAL"]
```
[25.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification."

[25.12.2025 06:36] Response: ```python
["OPTIMIZATION"]
```

The paper focuses on advancing pretraining methods and optimization techniques for visual generative models. It proposes NExT-Vid, a novel framework that improves upon existing pretraining approaches through architectural innovations (context-isolated autoregressive predictor and conditioned flow-matching decoder) to optimize visual representation learning. The emphasis on improving training methodology and model performance through better pretraining strategies aligns with the OPTIMIZATION topic.
[25.12.2025 06:36] Error. Failed to parse JSON from LLM. ["OPTIMIZATION"]


The paper focuses on advancing pretraining methods and optimization techniques for visual generative models. It proposes NExT-Vid, a novel framework that improves upon existing pretraining approaches through architectural innovations (context-isolated autoregressive predictor and conditioned flow-matching decoder) to optimize visual representation learning. The emphasis on improving training methodology and model performance through better pretraining strategies aligns with the OPTIMIZATION topic.
[25.12.2025 06:36] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces NExT-Vid, a new framework for autoregressive visual generative pretraining that focuses on improving video analysis. Unlike traditional methods that use BERT-style masked modeling, NExT-Vid employs masked next-frame prediction to better capture the temporal dynamics of both images and videos. The framework features a context-isolated autoregressive predictor to separate semantic representation from the decoding process, along with a conditioned flow-matching decoder to improve the quality and diversity of generated outputs. Experimental results show that NExT-Vid outperforms existing visual pretraining methods, leading to better performance in downstream classification tasks.","title":"NExT-Vid: Revolutionizing Visual Pretraining with Autoregressive Modeling"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces NExT-Vid, a new framework for autoregressive visual generative pretraining that focuses on improving video analysis. Unlike traditional methods that use BERT-style masked modeling, NExT-Vid employs masked next-frame prediction to better capture the temporal dynamics of both images and videos. The framework features a context-isolated autoregressive predictor to separate semantic representation from the decoding process, along with a conditioned flow-matching decoder to improve the quality and diversity of generated outputs. Experimental results show that NExT-Vid outperforms existing visual pretraining methods, leading to better performance in downstream classification tasks.', title='NExT-Vid: Revolutionizing Visual Pretraining with Autoregressive Modeling'))
[25.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ÊúÄËøëÔºåÈÄöÁî®Âü∫Á°ÄÊ®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉËøõÂ±ïÊòæËëóÊèêÂçá‰∫ÜÂ§öÁßç‰∏ãÊ∏∏‰ªªÂä°ÁöÑË°®Áé∞„ÄÇÂ∞ΩÁÆ°Ëá™ÂõûÂΩíÁîüÊàêÊ®°ÂûãÂ¶ÇGPTÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÂèñÂæó‰∫ÜÈù©ÂëΩÊÄßËøõÂ±ïÔºå‰ΩÜÂ§ßÂ§öÊï∞ËßÜËßâÁîüÊàêÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ï‰ªç‰æùËµñ‰∫éBERTÈ£éÊ†ºÁöÑÊé©Á†ÅÂª∫Ê®°ÔºåÂ∏∏Â∏∏ÂøΩËßÜËßÜÈ¢ëÂàÜÊûê‰∏≠ÈáçË¶ÅÁöÑÊó∂Èó¥‰ø°ÊÅØ„ÄÇÁé∞ÊúâÁöÑËá™ÂõûÂΩíËßÜËßâÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ïÂ≠òÂú®ËØ≠‰πâÂÆö‰Ωç‰∏çÂáÜÁ°ÆÂíåÁîüÊàêË¥®ÈáèÂ∑ÆÁ≠âÈóÆÈ¢òÔºåÂØºËá¥ËØ≠‰πâÊïàÊûú‰∏ç‰Ω≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜNExT-VidÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑËá™ÂõûÂΩíËßÜËßâÁîüÊàêÈ¢ÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÈÄöËøáÊé©Á†Å‰∏ã‰∏ÄÂ∏ßÈ¢ÑÊµãÊù•ËÅîÂêàÂª∫Ê®°ÂõæÂÉèÂíåËßÜÈ¢ëÔºåÊòæËëóÊèêÂçá‰∫ÜÁîüÊàêË¥®ÈáèÂíåÂ§öÊ†∑ÊÄß„ÄÇ","title":"NExT-VidÔºöÊèêÂçáËßÜËßâÁîüÊàêÈ¢ÑËÆ≠ÁªÉÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ÊúÄËøëÔºåÈÄöÁî®Âü∫Á°ÄÊ®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉËøõÂ±ïÊòæËëóÊèêÂçá‰∫ÜÂ§öÁßç‰∏ãÊ∏∏‰ªªÂä°ÁöÑË°®Áé∞„ÄÇÂ∞ΩÁÆ°Ëá™ÂõûÂΩíÁîüÊàêÊ®°ÂûãÂ¶ÇGPTÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÂèñÂæó‰∫ÜÈù©ÂëΩÊÄßËøõÂ±ïÔºå‰ΩÜÂ§ßÂ§öÊï∞ËßÜËßâÁîüÊàêÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ï‰ªç‰æùËµñ‰∫éBERTÈ£éÊ†ºÁöÑÊé©Á†ÅÂª∫Ê®°ÔºåÂ∏∏Â∏∏ÂøΩËßÜËßÜÈ¢ëÂàÜÊûê‰∏≠ÈáçË¶ÅÁöÑÊó∂Èó¥‰ø°ÊÅØ„ÄÇÁé∞ÊúâÁöÑËá™ÂõûÂΩíËßÜËßâÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ïÂ≠òÂú®ËØ≠‰πâÂÆö‰Ωç‰∏çÂáÜÁ°ÆÂíåÁîüÊàêË¥®ÈáèÂ∑ÆÁ≠âÈóÆÈ¢òÔºåÂØºËá¥ËØ≠‰πâÊïàÊûú‰∏ç‰Ω≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜNExT-VidÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑËá™ÂõûÂΩíËßÜËßâÁîüÊàêÈ¢ÑËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÈÄöËøáÊé©Á†Å‰∏ã‰∏ÄÂ∏ßÈ¢ÑÊµãÊù•ËÅîÂêàÂª∫Ê®°ÂõæÂÉèÂíåËßÜÈ¢ëÔºåÊòæËëóÊèêÂçá‰∫ÜÁîüÊàêË¥®ÈáèÂíåÂ§öÊ†∑ÊÄß„ÄÇ', title='NExT-VidÔºöÊèêÂçáËßÜËßâÁîüÊàêÈ¢ÑËÆ≠ÁªÉÁöÑÊñ∞ÊñπÊ≥ï'))
[25.12.2025 06:37] Querying the API.
[25.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.
[25.12.2025 06:37] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ TokSuite ‚Äî –Ω–∞–±–æ—Ä –º–æ–¥–µ–ª–µ–π –∏ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –≤–ª–∏—è–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –Ω–∞ –ø–æ–≤–µ–¥–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±—É—á–∏–ª–∏ —á–µ—Ç—ã—Ä–Ω–∞–¥—Ü–∞—Ç—å –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏ –ø—Ä–∏ –Ω–µ–∏–∑–º–µ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ, –¥–∞–Ω–Ω—ã—Ö –∏ –±—é–¥–∂–µ—Ç–µ –æ–±—É—á–µ–Ω–∏—è. –ë—ã–ª —Å–æ–∑–¥–∞–Ω —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–º–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–æ–∑–º—É—â–µ–Ω–∏—è—Ö, –≤–ª–∏—è—é—â–∏—Ö –Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –∏–∑–æ–ª–∏—Ä–æ–≤–∞—Ç—å –∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –∏ –ø–æ–≤–µ–¥–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
  "emoji": "üî§",
  "title": "–†–∞—Å–∫—Ä—ã–≤–∞—è —Å–∫—Ä—ã—Ç–æ–µ –≤–ª–∏—è–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –Ω–∞ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏"
}
```
[25.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers."

[25.12.2025 06:37] Response: ```python
["DATASET", "BENCHMARK", "TRAINING"]
```
[25.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers."

[25.12.2025 06:37] Response: ```python
["INTERPRETABILITY", "OPEN_SOURCE"]
```

**Justification:**

- **INTERPRETABILITY**: The paper focuses on understanding and analyzing how tokenization influences language model performance and behavior. It aims to decouple and measure the impact of tokenizers in isolation, which directly relates to analyzing model behavior and explanations.

- **OPEN_SOURCE**: The paper explicitly states they "curate and release a new benchmark," indicating they are contributing resources (benchmark and models) to the public research community.
[25.12.2025 06:37] Error. Failed to parse JSON from LLM. ["INTERPRETABILITY", "OPEN_SOURCE"]


**Justification:**

- **INTERPRETABILITY**: The paper focuses on understanding and analyzing how tokenization influences language model performance and behavior. It aims to decouple and measure the impact of tokenizers in isolation, which directly relates to analyzing model behavior and explanations.

- **OPEN_SOURCE**: The paper explicitly states they "curate and release a new benchmark," indicating they are contributing resources (benchmark and models) to the public research community.
[25.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces TokSuite, a comprehensive framework designed to study the impact of tokenization on language models (LMs). By training fourteen identical models with different tokenizers, the authors isolate the effects of tokenization on model performance. They also present a new benchmark that evaluates how real-world variations affect tokenization outcomes. The findings from TokSuite reveal important insights into the strengths and weaknesses of various popular tokenizers, enhancing our understanding of their role in LM behavior.","title":"Unpacking Tokenization: Insights from TokSuite"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces TokSuite, a comprehensive framework designed to study the impact of tokenization on language models (LMs). By training fourteen identical models with different tokenizers, the authors isolate the effects of tokenization on model performance. They also present a new benchmark that evaluates how real-world variations affect tokenization outcomes. The findings from TokSuite reveal important insights into the strengths and weaknesses of various popular tokenizers, enhancing our understanding of their role in LM behavior.', title='Unpacking Tokenization: Insights from TokSuite'))
[25.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫ÜTokSuiteÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éÁ†îÁ©∂ÂàÜËØçÂØπËØ≠Ë®ÄÊ®°ÂûãÔºàLMÔºâÂΩ±ÂìçÁöÑÊ®°ÂûãÈõÜÂêàÂíåÂü∫ÂáÜÊµãËØï„ÄÇÂ∞ΩÁÆ°ÂàÜËØçÂú®ÊñáÊú¨Ë°®Á§∫ÂíåÂ§ÑÁêÜ‰∏≠ÁöÑÈáçË¶ÅÊÄß‰∏çË®ÄËÄåÂñªÔºå‰ΩÜÂÖ∂ÂØπLMÊÄßËÉΩÁöÑÂÖ∑‰ΩìÂΩ±ÂìçÂ∞ö‰∏çÊòéÁ°Æ„ÄÇÊàë‰ª¨ËÆ≠ÁªÉ‰∫ÜÂçÅÂõõ‰∏™‰ΩøÁî®‰∏çÂêåÂàÜËØçÂô®ÁöÑÊ®°ÂûãÔºåÁ°Æ‰øùÂÆÉ‰ª¨Âú®Êû∂ÊûÑ„ÄÅÊï∞ÊçÆÈõÜ„ÄÅËÆ≠ÁªÉÈ¢ÑÁÆóÂíåÂàùÂßãÂåñ‰∏äÂÆåÂÖ®Áõ∏Âêå„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÂàõÂª∫Âπ∂ÂèëÂ∏É‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÔºå‰∏ìÈó®ÊµãÈáèÂú®Áé∞ÂÆû‰∏ñÁïåÊâ∞Âä®‰∏ãÊ®°ÂûãÁöÑÊÄßËÉΩÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÁêÜËß£ÂàÜËØçÁöÑÂΩ±Âìç„ÄÇ","title":"ÂàÜËØçÂØπËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩÁöÑÊ∑±ËøúÂΩ±Âìç"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫ÜTokSuiteÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éÁ†îÁ©∂ÂàÜËØçÂØπËØ≠Ë®ÄÊ®°ÂûãÔºàLMÔºâÂΩ±ÂìçÁöÑÊ®°ÂûãÈõÜÂêàÂíåÂü∫ÂáÜÊµãËØï„ÄÇÂ∞ΩÁÆ°ÂàÜËØçÂú®ÊñáÊú¨Ë°®Á§∫ÂíåÂ§ÑÁêÜ‰∏≠ÁöÑÈáçË¶ÅÊÄß‰∏çË®ÄËÄåÂñªÔºå‰ΩÜÂÖ∂ÂØπLMÊÄßËÉΩÁöÑÂÖ∑‰ΩìÂΩ±ÂìçÂ∞ö‰∏çÊòéÁ°Æ„ÄÇÊàë‰ª¨ËÆ≠ÁªÉ‰∫ÜÂçÅÂõõ‰∏™‰ΩøÁî®‰∏çÂêåÂàÜËØçÂô®ÁöÑÊ®°ÂûãÔºåÁ°Æ‰øùÂÆÉ‰ª¨Âú®Êû∂ÊûÑ„ÄÅÊï∞ÊçÆÈõÜ„ÄÅËÆ≠ÁªÉÈ¢ÑÁÆóÂíåÂàùÂßãÂåñ‰∏äÂÆåÂÖ®Áõ∏Âêå„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÂàõÂª∫Âπ∂ÂèëÂ∏É‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÔºå‰∏ìÈó®ÊµãÈáèÂú®Áé∞ÂÆû‰∏ñÁïåÊâ∞Âä®‰∏ãÊ®°ÂûãÁöÑÊÄßËÉΩÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞ÁêÜËß£ÂàÜËØçÁöÑÂΩ±Âìç„ÄÇ', title='ÂàÜËØçÂØπËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩÁöÑÊ∑±ËøúÂΩ±Âìç'))
[25.12.2025 06:37] Using data from previous issue: {"categories": ["#benchmark"], "emoji": "üèÜ", "ru": {"title": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ç—É—Ä–Ω–∏—Ä–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥ –≤–º–µ—Å—Ç–æ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Competitive Swiss-System Dynamics (CSD), –∫–æ—Ç–æ—Ä–∞—è –∑–∞–º–µ–Ω—è–µ—Ç —Ç—Ä–∞–¥–∏—Ü–∏
[25.12.2025 06:37] Renaming data file.
[25.12.2025 06:37] Renaming previous data. hf_papers.json to ./d/2025-12-25.json
[25.12.2025 06:37] Saving new data file.
[25.12.2025 06:37] Generating page.
[25.12.2025 06:37] Renaming previous page.
[25.12.2025 06:37] Renaming previous data. index.html to ./d/2025-12-25.html
[25.12.2025 06:37] Writing result.
[25.12.2025 06:37] Renaming log file.
[25.12.2025 06:37] Renaming previous data. log.txt to ./logs/2025-12-25_last_log.txt
