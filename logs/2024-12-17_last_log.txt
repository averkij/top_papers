[17.12.2024 08:15] Read previous papers.
[17.12.2024 08:15] Generating top page (month).
[17.12.2024 08:15] Writing top page (month).
[17.12.2024 09:11] Read previous papers.
[17.12.2024 09:11] Get feed.
[17.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.11919
[17.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.09645
[17.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.11815
[17.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.12095
[17.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.11231
[17.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.12083
[17.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.10316
[17.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.11605
[17.12.2024 09:11] Extract page data from URL. URL: https://huggingface.co/papers/2412.09871
[17.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.12091
[17.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.11258
[17.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.11834
[17.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.11586
[17.12.2024 09:11] Get page data from previous paper. URL: https://huggingface.co/papers/2412.11457
[17.12.2024 09:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[17.12.2024 09:11] No deleted papers detected.
[17.12.2024 09:11] Downloading and parsing papers (pdf, html). Total: 14.
[17.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.11919.
[17.12.2024 09:11] Extra JSON file exists (./assets/json/2412.11919.json), skip PDF parsing.
[17.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.11919.json), skip HTML parsing.
[17.12.2024 09:11] Success.
[17.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09645.
[17.12.2024 09:11] Extra JSON file exists (./assets/json/2412.09645.json), skip PDF parsing.
[17.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.09645.json), skip HTML parsing.
[17.12.2024 09:11] Success.
[17.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.11815.
[17.12.2024 09:11] Extra JSON file exists (./assets/json/2412.11815.json), skip PDF parsing.
[17.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.11815.json), skip HTML parsing.
[17.12.2024 09:11] Success.
[17.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.12095.
[17.12.2024 09:11] Extra JSON file exists (./assets/json/2412.12095.json), skip PDF parsing.
[17.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.12095.json), skip HTML parsing.
[17.12.2024 09:11] Success.
[17.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.11231.
[17.12.2024 09:11] Extra JSON file exists (./assets/json/2412.11231.json), skip PDF parsing.
[17.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.11231.json), skip HTML parsing.
[17.12.2024 09:11] Success.
[17.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.12083.
[17.12.2024 09:11] Extra JSON file exists (./assets/json/2412.12083.json), skip PDF parsing.
[17.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.12083.json), skip HTML parsing.
[17.12.2024 09:11] Success.
[17.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.10316.
[17.12.2024 09:11] Extra JSON file exists (./assets/json/2412.10316.json), skip PDF parsing.
[17.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.10316.json), skip HTML parsing.
[17.12.2024 09:11] Success.
[17.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.11605.
[17.12.2024 09:11] Extra JSON file exists (./assets/json/2412.11605.json), skip PDF parsing.
[17.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.11605.json), skip HTML parsing.
[17.12.2024 09:11] Success.
[17.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.09871.
[17.12.2024 09:11] Downloading paper 2412.09871 from http://arxiv.org/pdf/2412.09871v1...
[17.12.2024 09:11] Extracting affiliations from text.
[17.12.2024 09:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"4 2 0 2 3 1 ] . [ 1 1 7 8 9 0 . 2 1 4 2 : r Byte Latent Transformer: Patches Scale Better Than Tokens Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li1,, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman,2,, Srinivasan Iyer FAIR at Meta, 1Paul G. Allen School of Computer Science & Engineering, University of Washington, 2University of Chicago Joint second author, Joint last author, Work done at Meta We introduce the Byte Latent Transformer (BLT), new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first flop controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size. Date: December 16, 2024 Correspondence: artidoro at cs.washington.edu, sviyer at meta.com Code: https://github.com/facebookresearch/blt We introduce the Byte Latent Transformer (BLT), tokenizer-free architecture that learns from raw byte data and, for the first time, matches the performance of tokenization-based models at scale, with significant improvements in efficiency and robustness (6). Existing large language models (llms) are trained almost entirely en"
[17.12.2024 09:11] Response: ```python
[
    "FAIR at Meta",
    "Paul G. Allen School of Computer Science & Engineering, University of Washington",
    "University of Chicago"
]
```
[17.12.2024 09:11] Deleting PDF ./assets/pdf/2412.09871.pdf.
[17.12.2024 09:11] Success.
[17.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.12091.
[17.12.2024 09:11] Extra JSON file exists (./assets/json/2412.12091.json), skip PDF parsing.
[17.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.12091.json), skip HTML parsing.
[17.12.2024 09:11] Success.
[17.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.11258.
[17.12.2024 09:11] Extra JSON file exists (./assets/json/2412.11258.json), skip PDF parsing.
[17.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.11258.json), skip HTML parsing.
[17.12.2024 09:11] Success.
[17.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.11834.
[17.12.2024 09:11] Extra JSON file exists (./assets/json/2412.11834.json), skip PDF parsing.
[17.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.11834.json), skip HTML parsing.
[17.12.2024 09:11] Success.
[17.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.11586.
[17.12.2024 09:11] Extra JSON file exists (./assets/json/2412.11586.json), skip PDF parsing.
[17.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.11586.json), skip HTML parsing.
[17.12.2024 09:11] Success.
[17.12.2024 09:11] Downloading and parsing paper https://huggingface.co/papers/2412.11457.
[17.12.2024 09:11] Extra JSON file exists (./assets/json/2412.11457.json), skip PDF parsing.
[17.12.2024 09:11] Paper image links file exists (./assets/img_data/2412.11457.json), skip HTML parsing.
[17.12.2024 09:11] Success.
[17.12.2024 09:11] Enriching papers with extra data.
[17.12.2024 09:11] ********************************************************************************
[17.12.2024 09:11] Abstract 0. Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations. Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several limitations: additional deployment costs of sep...
[17.12.2024 09:11] ********************************************************************************
[17.12.2024 09:11] Abstract 1. Recent advancements in visual generative models have enabled high-quality image and video generation, opening diverse applications. However, evaluating these models often demands sampling hundreds or thousands of images or videos, making the process computationally expensive, especially for diffusio...
[17.12.2024 09:11] ********************************************************************************
[17.12.2024 09:11] Abstract 2. Automatic black-and-white image sequence colorization while preserving character and object identity (ID) is a complex task with significant market demand, such as in cartoon or comic series colorization. Despite advancements in visual colorization using large-scale generative models like diffusion ...
[17.12.2024 09:11] ********************************************************************************
[17.12.2024 09:11] Abstract 3. We introduce Causal Diffusion as the autoregressive (AR) counterpart of Diffusion models. It is a next-token(s) forecasting framework that is friendly to both discrete and continuous modalities and compatible with existing next-token prediction models like LLaMA and GPT. While recent works attempt t...
[17.12.2024 09:11] ********************************************************************************
[17.12.2024 09:11] Abstract 4. Instruction tuning has been widely used to unleash the complete potential of large language models. Notably, complex and diverse instructions are of significant importance as they can effectively align models with various downstream tasks. However, current approaches to constructing large-scale inst...
[17.12.2024 09:11] ********************************************************************************
[17.12.2024 09:11] Abstract 5. Capturing geometric and material information from images remains a fundamental challenge in computer vision and graphics. Traditional optimization-based methods often require hours of computational time to reconstruct geometry, material properties, and environmental lighting from dense multi-view in...
[17.12.2024 09:11] ********************************************************************************
[17.12.2024 09:11] Abstract 6. Image editing has advanced significantly with the development of diffusion models using both inversion-based and instruction-based methods. However, current inversion-based approaches struggle with big modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, ...
[17.12.2024 09:11] ********************************************************************************
[17.12.2024 09:11] Abstract 7. Instruction-following is a fundamental capability of language models, requiring the model to recognize even the most subtle requirements in the instructions and accurately reflect them in its output. Such an ability is well-suited for and often optimized by preference learning. However, existing met...
[17.12.2024 09:11] ********************************************************************************
[17.12.2024 09:11] Abstract 8. We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the p...
[17.12.2024 09:11] ********************************************************************************
[17.12.2024 09:11] Abstract 9. This paper addresses a challenging question: How can we efficiently create high-quality, wide-scope 3D scenes from a single arbitrary image? Existing methods face several constraints, such as requiring multi-view data, time-consuming per-scene optimization, low visual quality in backgrounds, and dis...
[17.12.2024 09:11] ********************************************************************************
[17.12.2024 09:11] Abstract 10. Estimating physical properties for visual data is a crucial task in computer vision, graphics, and robotics, underpinning applications such as augmented reality, physical simulation, and robotic grasping. However, this area remains under-explored due to the inherent ambiguities in physical property ...
[17.12.2024 09:11] ********************************************************************************
[17.12.2024 09:11] Abstract 11. In order to make the foundation model more efficient and effective, our idea is combining sequence transformation and state transformation. First, we prove the availability of rotary position embedding in the state space duality algorithm, which reduces the perplexity of the hybrid quadratic causal ...
[17.12.2024 09:11] ********************************************************************************
[17.12.2024 09:11] Abstract 12. While haircut indicates distinct personality, existing avatar generation methods fail to model practical hair due to the general or entangled representation. We propose StrandHead, a novel text to 3D head avatar generation method capable of generating disentangled 3D hair with strand representation....
[17.12.2024 09:11] ********************************************************************************
[17.12.2024 09:11] Abstract 13. Repurposing pre-trained diffusion models has been proven to be effective for NVS. However, these methods are mostly limited to a single object; directly applying such methods to compositional multi-object scenarios yields inferior results, especially incorrect object placement and inconsistent shape...
[17.12.2024 09:11] Read previous papers.
[17.12.2024 09:11] Generating reviews via LLM API.
[17.12.2024 09:11] Using data from previous issue: {"categories": ["#optimization", "#hallucinations", "#rag"], "emoji": "ğŸ”", "ru": {"title": "RetroLLM: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸", "desc": "RetroLLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°
[17.12.2024 09:11] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#cv", "#open_source", "#interpretability"], "emoji": "ğŸ”", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Evaluation Agent. 
[17.12.2024 09:11] Using data from previous issue: {"categories": ["#diffusion", "#benchmark", "#rag", "#cv", "#open_source"], "emoji": "ğŸ¨", "ru": {"title": "ColorFlow: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸", "desc": "ColorFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğº
[17.12.2024 09:11] Using data from previous issue: {"categories": ["#diffusion", "#architecture", "#dataset", "#benchmark", "#training", "#multimodal"], "emoji": "ğŸ”®", "ru": {"title": "CausalFusion: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Causal Diffusion - Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³ Ğ¼Ğ¾Ğ´
[17.12.2024 09:11] Using data from previous issue: {"categories": ["#small_models", "#training", "#open_source", "#alignment", "#optimization"], "emoji": "ğŸ”¬", "ru": {"title": "ĞœĞ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (SLM) Ğ² ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… 
[17.12.2024 09:11] Using data from previous issue: {"categories": ["#optimization", "#3d", "#cv", "#dataset", "#diffusion"], "emoji": "ğŸ–¼ï¸", "ru": {"title": "Ğ”ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸", "desc": "IDArb - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ°
[17.12.2024 09:11] Using data from previous issue: {"categories": ["#interpretability", "#cv", "#diffusion", "#multimodal", "#agents"], "emoji": "ğŸ–Œï¸", "ru": {"title": "BrushEdit: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ°", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BrushEdit - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾
[17.12.2024 09:11] Using data from previous issue: {"categories": ["#transfer_learning", "#benchmark", "#optimization", "#training", "#open_source", "#rlhf", "#alignment", "#architecture"], "emoji": "ğŸ¯", "ru": {"title": "SPaR: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¸Ğ³Ñ€Ñƒ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SPaR Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ…
[17.12.2024 09:11] Querying the API.
[17.12.2024 09:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first FLOP controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.
[17.12.2024 09:12] Response: {
  "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ°Ğ¹Ñ‚Ğ¾Ğ² - Byte Latent Transformer (BLT). BLT ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ°Ğ¹Ñ‚Ñ‹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑĞµĞ¼Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼Ğ¸ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ±Ğ°Ğ¹Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ñ‚Ğ°Ğ¼, Ğ³Ğ´Ğµ ÑÑ‚Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ BLT Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ¾ 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ 4 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ±Ğ°Ğ¹Ñ‚Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.",

  "emoji": "ğŸ§ ",

  "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ°Ğ¹Ñ‚Ğ¾Ğ²"
}
[17.12.2024 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first FLOP controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size."

[17.12.2024 09:12] Response: ```python
['ARCHITECTURE', 'INFERENCE', 'TRAINING']
```
[17.12.2024 09:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first FLOP controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size."

[17.12.2024 09:12] Response: ```python
["OPTIMIZATION", "REASONING", "LONG_CONTEXT"]
```
[17.12.2024 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The Byte Latent Transformer (BLT) is a novel architecture for large language models (LLMs) that operates at the byte level, achieving performance comparable to traditional tokenization methods while enhancing efficiency and robustness. It utilizes dynamically sized patches to encode bytes, adjusting the size based on the complexity of the data, which allows for more effective use of computational resources. The study showcases the scalability of byte-level models, demonstrating significant improvements in both training and inference efficiency, particularly in reasoning and generalization tasks. Overall, BLT outperforms tokenization-based models by optimizing patch and model size simultaneously, leading to better performance at fixed inference costs.","title":"Revolutionizing Efficiency with Byte-Level Transformers"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='The Byte Latent Transformer (BLT) is a novel architecture for large language models (LLMs) that operates at the byte level, achieving performance comparable to traditional tokenization methods while enhancing efficiency and robustness. It utilizes dynamically sized patches to encode bytes, adjusting the size based on the complexity of the data, which allows for more effective use of computational resources. The study showcases the scalability of byte-level models, demonstrating significant improvements in both training and inference efficiency, particularly in reasoning and generalization tasks. Overall, BLT outperforms tokenization-based models by optimizing patch and model size simultaneously, leading to better performance at fixed inference costs.', title='Revolutionizing Efficiency with Byte-Level Transformers'))
[17.12.2024 09:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å­—èŠ‚çº§å¤§è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œç§°ä¸ºå­—èŠ‚æ½œåœ¨å˜æ¢å™¨ï¼ˆBLTï¼‰ã€‚BLTé€šè¿‡åŠ¨æ€å¤§å°çš„è¡¥ä¸ç¼–ç å­—èŠ‚ï¼Œä½œä¸ºè®¡ç®—çš„ä¸»è¦å•ä½ï¼Œä»è€Œåœ¨æ¨ç†æ•ˆç‡å’Œé²æ£’æ€§ä¸Šæ˜¾è‘—æå‡ã€‚è¡¥ä¸çš„åˆ†å‰²åŸºäºä¸‹ä¸€ä¸ªå­—èŠ‚çš„ç†µï¼Œèƒ½å¤Ÿåœ¨æ•°æ®å¤æ‚æ€§å¢åŠ æ—¶åˆ†é…æ›´å¤šçš„è®¡ç®—èµ„æºã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒBLTåœ¨å›ºå®šæ¨ç†æˆæœ¬ä¸‹ï¼Œèƒ½å¤Ÿæ¯”åŸºäºæ ‡è®°åŒ–çš„æ¨¡å‹å®ç°æ›´å¥½çš„æ‰©å±•æ€§ï¼ŒåŒæ—¶æé«˜äº†è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚","title":"å­—èŠ‚æ½œåœ¨å˜æ¢å™¨ï¼šé«˜æ•ˆæ‰©å±•çš„æ–°é€‰æ‹©"}', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=Article(desc='æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å­—èŠ‚çº§å¤§è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œç§°ä¸ºå­—èŠ‚æ½œåœ¨å˜æ¢å™¨ï¼ˆBLTï¼‰ã€‚BLTé€šè¿‡åŠ¨æ€å¤§å°çš„è¡¥ä¸ç¼–ç å­—èŠ‚ï¼Œä½œä¸ºè®¡ç®—çš„ä¸»è¦å•ä½ï¼Œä»è€Œåœ¨æ¨ç†æ•ˆç‡å’Œé²æ£’æ€§ä¸Šæ˜¾è‘—æå‡ã€‚è¡¥ä¸çš„åˆ†å‰²åŸºäºä¸‹ä¸€ä¸ªå­—èŠ‚çš„ç†µï¼Œèƒ½å¤Ÿåœ¨æ•°æ®å¤æ‚æ€§å¢åŠ æ—¶åˆ†é…æ›´å¤šçš„è®¡ç®—èµ„æºã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒBLTåœ¨å›ºå®šæ¨ç†æˆæœ¬ä¸‹ï¼Œèƒ½å¤Ÿæ¯”åŸºäºæ ‡è®°åŒ–çš„æ¨¡å‹å®ç°æ›´å¥½çš„æ‰©å±•æ€§ï¼ŒåŒæ—¶æé«˜äº†è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚', title='å­—èŠ‚æ½œåœ¨å˜æ¢å™¨ï¼šé«˜æ•ˆæ‰©å±•çš„æ–°é€‰æ‹©'))
[17.12.2024 09:12] Using data from previous issue: {"categories": ["#3d", "#diffusion", "#optimization"], "emoji": "ğŸ¥", "ru": {"title": "ĞÑ‚ 2D Ğº 3D: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ
[17.12.2024 09:12] Using data from previous issue: {"categories": ["#3d", "#cv", "#robotics"], "emoji": "ğŸ§ª", "ru": {"title": "GaussianProperty: Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ² 3D Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "GaussianProperty - ÑÑ‚Ğ¾ Ğ±ĞµĞ·Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² 3D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ°Ğ¼. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ SAM Ğ¸ 
[17.12.2024 09:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#architecture"], "emoji": "ğŸ§ ", "ru": {"title": "Ğ§ÑƒĞ´ĞµÑĞ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚Ğµ
[17.12.2024 09:12] Using data from previous issue: {"categories": ["#3d", "#open_source", "#diffusion", "#optimization"], "emoji": "ğŸ’‡", "ru": {"title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D-Ğ¿Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ StrandHead - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ñ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ»Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚
[17.12.2024 09:12] Using data from previous issue: {"categories": ["#optimization", "#3d", "#training", "#dataset", "#diffusion"], "emoji": "ğŸ¥", "ru": {"title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² ÑÑ†ĞµĞ½ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ MOVIS Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½
[17.12.2024 09:12] Trying to get texts in Chinese.
[17.12.2024 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations. Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several limitations: additional deployment costs of separate retrievers, redundant input tokens from retrieved text chunks, and the lack of joint optimization of retrieval and generation. To address these issues, we propose RetroLLM, a unified framework that integrates retrieval and generation into a single, cohesive process, enabling LLMs to directly generate fine-grained evidence from the corpus with constrained decoding. Moreover, to mitigate false pruning in the process of constrained evidence generation, we introduce (1) hierarchical FM-Index constraints, which generate corpus-constrained clues to identify a subset of relevant documents before evidence generation, reducing irrelevant decoding space; and (2) a forward-looking constrained decoding strategy, which considers the relevance of future sequences to improve evidence accuracy. Extensive experiments on five open-domain QA datasets demonstrate RetroLLM's superior performance across both in-domain and out-of-domain tasks. The code is available at https://github.com/sunnynexus/RetroLLM.
[17.12.2024 09:12] Mistral response. {"id": "0c92c38cd329452e93f554aa9a20c01e", "object": "chat.completion", "created": 1734426738, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c55\u793a\u4e86\u663e\u8457\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5e38\u5e38\u51fa\u73b0\u5e7b\u89c9\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u77e5\u8bc6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u9762\u4e34\u4e00\u4e9b\u9650\u5236\uff1a\u989d\u5916\u7684\u90e8\u7f72\u6210\u672c\u3001\u68c0\u7d22\u6587\u672c\u5757\u4e2d\u7684\u5197\u4f59\u8f93\u5165\u6807\u8bb0\u4ee5\u53ca\u68c0\u7d22\u548c\u751f\u6210\u7684\u8054\u5408\u4f18\u5316\u7f3a\u4e4f\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86RetroLLM\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5c06\u68c0\u7d22\u548c\u751f\u6210\u6574\u5408\u4e3a\u5355\u4e00\u7684\u8fc7\u7a0b\uff0c\u4f7fLLMs\u80fd\u591f\u76f4\u63a5\u4ece\u8bed\u6599\u5e93\u4e2d\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u8bc1\u636e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5206\u5c42FM-Index\u7ea6\u675f\u548c\u524d\u77bb\u6027\u7ea6\u675f\u89e3\u7801\u7b56\u7565\uff0c\u4ee5\u51cf\u5c11\u4e0d\u76f8\u5173\u7684\u89e3\u7801\u7a7a\u95f4\u5e76\u63d0\u9ad8\u8bc1\u636e\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRetroLLM\u5728\u4e94\u4e2a\u5f00\u653e\u57df\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u4ee3\u7801\u53ef\u5728https://github.com/sunnynexus/RetroLLM\u83b7\u53d6\u3002"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 301, "total_tokens": 573, "completion_tokens": 272}}
[17.12.2024 09:12] Response: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç¤ºäº†æ˜¾è‘—çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å¸¸å¸¸å‡ºç°å¹»è§‰ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•ä»é¢ä¸´ä¸€äº›é™åˆ¶ï¼šé¢å¤–çš„éƒ¨ç½²æˆæœ¬ã€æ£€ç´¢æ–‡æœ¬å—ä¸­çš„å†—ä½™è¾“å…¥æ ‡è®°ä»¥åŠæ£€ç´¢å’Œç”Ÿæˆçš„è”åˆä¼˜åŒ–ç¼ºä¹ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RetroLLMï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†æ£€ç´¢å’Œç”Ÿæˆæ•´åˆä¸ºå•ä¸€çš„è¿‡ç¨‹ï¼Œä½¿LLMsèƒ½å¤Ÿç›´æ¥ä»è¯­æ–™åº“ä¸­ç”Ÿæˆç»†ç²’åº¦çš„è¯æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†å±‚FM-Indexçº¦æŸå’Œå‰ç»æ€§çº¦æŸè§£ç ç­–ç•¥ï¼Œä»¥å‡å°‘ä¸ç›¸å…³çš„è§£ç ç©ºé—´å¹¶æé«˜è¯æ®å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRetroLLMåœ¨äº”ä¸ªå¼€æ”¾åŸŸé—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä»£ç å¯åœ¨https://github.com/sunnynexus/RetroLLMè·å–ã€‚
[17.12.2024 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç¤ºäº†æ˜¾è‘—çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å¸¸å¸¸å‡ºç°å¹»è§‰ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•ä»é¢ä¸´ä¸€äº›é™åˆ¶ï¼šé¢å¤–çš„éƒ¨ç½²æˆæœ¬ã€æ£€ç´¢æ–‡æœ¬å—ä¸­çš„å†—ä½™è¾“å…¥æ ‡è®°ä»¥åŠæ£€ç´¢å’Œç”Ÿæˆçš„è”åˆä¼˜åŒ–ç¼ºä¹ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RetroLLMï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†æ£€ç´¢å’Œç”Ÿæˆæ•´åˆä¸ºå•ä¸€çš„è¿‡ç¨‹ï¼Œä½¿LLMsèƒ½å¤Ÿç›´æ¥ä»è¯­æ–™åº“ä¸­ç”Ÿæˆç»†ç²’åº¦çš„è¯æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†å±‚FM-Indexçº¦æŸå’Œå‰ç»æ€§çº¦æŸè§£ç ç­–ç•¥ï¼Œä»¥å‡å°‘ä¸ç›¸å…³çš„è§£ç ç©ºé—´å¹¶æé«˜è¯æ®å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRetroLLMåœ¨äº”ä¸ªå¼€æ”¾åŸŸé—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä»£ç å¯åœ¨https://github.com/sunnynexus/RetroLLMè·å–ã€‚
[17.12.2024 09:12] Mistral response. {"id": "ddf9e981d41a4f509f3a1c5e64071ab1", "object": "chat.completion", "created": 1734426743, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c55\u793a\u4e86\u663e\u8457\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5e38\u5e38\u51fa\u73b0\u5e7b\u89c9\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u77e5\u8bc6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u9762\u4e34\u4e00\u4e9b\u9650\u5236\uff1a\u989d\u5916\u7684\u90e8\u7f72\u6210\u672c\u3001\u68c0\u7d22\u6587\u672c\u5757\u4e2d\u7684\u5197\u4f59\u8f93\u5165\u6807\u8bb0\u4ee5\u53ca\u68c0\u7d22\u548c\u751f\u6210\u7684\u8054\u5408\u4f18\u5316\u7f3a\u4e4f\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86RetroLLM\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5c06\u68c0\u7d22\u548c\u751f\u6210\u6574\u5408\u4e3a\u5355\u4e00\u7684\u8fc7\u7a0b\uff0c\u4f7fLLMs\u80fd\u591f\u76f4\u63a5\u4ece\u8bed\u6599\u5e93\u4e2d\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u8bc1\u636e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5206\u5c42FM-Index\u7ea6\u675f\u548c\u524d\u77bb\u6027\u7ea6\u675f\u89e3\u7801\u7b56\u7565\uff0c\u4ee5\u51cf\u5c11\u4e0d\u76f8\u5173\u7684\u89e3\u7801\u7a7a\u95f4\u5e76\u63d0\u9ad8\u8bc1\u636e\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRetroLLM\u5728\u4e94\u4e2a\u5f00\u653e\u57df\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u4ee3\u7801\u53ef\u5728https://github.com/sunnynexus/RetroLLM\u83b7\u53d6\u3002\n\nd\u00e0 x\u00edng y\u01d4 y\u00e1n m\u00f3 x\u00edng (LLMs) zh\u01cen sh\u00ec le xi\u01cen zh\u00f9 de sh\u0113ng ch\u00e9ng n\u00e9ng l\u00ec, d\u00e0n ch\u00e1ng ch\u00e1ng ch\u016b xi\u00e0n hu\u00e0n ju\u00e9. Ji\u01cen su\u01d2 z\u0113ng qi\u00e1ng sh\u0113ng ch\u00e9ng (RAG) t\u014dng gu\u00f2 y\u01d0n r\u00f9 w\u00e0i b\u00f9 zh\u012b shi t\u00ed g\u014dng le y\u01d2u xi\u00e0o ji\u011b ju\u00e9 f\u0101ng \u0101n, d\u00e0n xi\u00e0n y\u01d2u f\u0101ng f\u01ce r\u00e9ng mi\u00e0n l\u00ecng y\u012b xi\u0113 xi\u00e0n zh\u00ec: \u00e9 w\u00e0i de b\u00f9 sh\u00f9 ch\u00e9ng b\u011bn, ji\u01cen su\u01d2 w\u00e9n b\u011bn ku\u00e0i zh\u014dng de r\u00f3ng y\u00f9 sh\u016b r\u00f9 bi\u0101o j\u00ec y\u01d0 j\u00ed ji\u01cen su\u01d2 h\u00e9 sh\u0113ng ch\u00e9ng de li\u00e1n h\u00e9 y\u014du hu\u00e0 qu\u0113 f\u01ce. W\u00e8i ji\u011b ju\u00e9 zh\u00e8 xi\u0113 w\u00e8n t\u00ed, w\u01d2 men t\u00ed ch\u016b le RetroLLM, y\u012b g\u00e8 t\u01d2ng y\u012b de ku\u00e0ng ji\u00e0, ji\u0101ng ji\u01cen su\u01d2 h\u00e9 sh\u0113ng ch\u00e9ng zh\u011bng h\u00e9 w\u00e9i d\u0101n y\u012b de gu\u00f2 ch\u00e9ng, sh\u01d0 LLMs n\u00e9ng g\u00f2u zh\u00ed ji\u0113 k\u00f9 zh\u014dng sh\u0113ng ch\u00e9ng x\u00ec l\u00ec d\u00f9 de zh\u00e8ng j\u00f9. C\u01d0 w\u00e0i, w\u01d2 men y\u01d0n r\u00f9 le f\u0113n c\u00e9ng FM-Index yu\u0113 shu\u014d h\u00e9 qi\u00e1n zh\u0101n x\u00ecng yu\u0113 shu\u014d ji\u011b m\u01ce zhu\u00e0n l\u00fc\u00e8, y\u01d0 ji\u01cen sh\u01ceo b\u00f9 xi\u0101ng gu\u0101n de ji\u011b m\u01ce k\u014dng ji\u0101n y\u01d0ng t\u00ed g\u0101o zh\u00e8ng j\u00f9 zh\u01d4n qu\u00e8 x\u00ecng. Sh\u00ed y\u00e0n ji\u00e9 gu\u01d2 bi\u01ceo m\u00edng, RetroLLM z\u00e0i w\u01d4 g\u00e8 k\u0101i f\u00e0ng y\u00f9 qi\u00fa sh\u00f9 j\u00f9 sh\u00f9 j\u00f9 sh\u00e0ng bi\u01ceo xi\u00e0n y\u014du y\u00e1n. D\u00e0i m\u01ce k\u011b z\u00e0i https://github.com/sunnynexus/RetroLLM hu\u00f2 q\u01d4."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 287, "total_tokens": 1142, "completion_tokens": 855}}
[17.12.2024 09:12] Response: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç¤ºäº†æ˜¾è‘—çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å¸¸å¸¸å‡ºç°å¹»è§‰ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•ä»é¢ä¸´ä¸€äº›é™åˆ¶ï¼šé¢å¤–çš„éƒ¨ç½²æˆæœ¬ã€æ£€ç´¢æ–‡æœ¬å—ä¸­çš„å†—ä½™è¾“å…¥æ ‡è®°ä»¥åŠæ£€ç´¢å’Œç”Ÿæˆçš„è”åˆä¼˜åŒ–ç¼ºä¹ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RetroLLMï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†æ£€ç´¢å’Œç”Ÿæˆæ•´åˆä¸ºå•ä¸€çš„è¿‡ç¨‹ï¼Œä½¿LLMsèƒ½å¤Ÿç›´æ¥ä»è¯­æ–™åº“ä¸­ç”Ÿæˆç»†ç²’åº¦çš„è¯æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†å±‚FM-Indexçº¦æŸå’Œå‰ç»æ€§çº¦æŸè§£ç ç­–ç•¥ï¼Œä»¥å‡å°‘ä¸ç›¸å…³çš„è§£ç ç©ºé—´å¹¶æé«˜è¯æ®å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRetroLLMåœ¨äº”ä¸ªå¼€æ”¾åŸŸé—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä»£ç å¯åœ¨https://github.com/sunnynexus/RetroLLMè·å–ã€‚

dÃ  xÃ­ng yÇ” yÃ¡n mÃ³ xÃ­ng (LLMs) zhÇn shÃ¬ le xiÇn zhÃ¹ de shÄ“ng chÃ©ng nÃ©ng lÃ¬, dÃ n chÃ¡ng chÃ¡ng chÅ« xiÃ n huÃ n juÃ©. JiÇn suÇ’ zÄ“ng qiÃ¡ng shÄ“ng chÃ©ng (RAG) tÅng guÃ² yÇn rÃ¹ wÃ i bÃ¹ zhÄ« shi tÃ­ gÅng le yÇ’u xiÃ o jiÄ› juÃ© fÄng Än, dÃ n xiÃ n yÇ’u fÄng fÇ rÃ©ng miÃ n lÃ¬ng yÄ« xiÄ“ xiÃ n zhÃ¬: Ã© wÃ i de bÃ¹ shÃ¹ chÃ©ng bÄ›n, jiÇn suÇ’ wÃ©n bÄ›n kuÃ i zhÅng de rÃ³ng yÃ¹ shÅ« rÃ¹ biÄo jÃ¬ yÇ jÃ­ jiÇn suÇ’ hÃ© shÄ“ng chÃ©ng de liÃ¡n hÃ© yÅu huÃ  quÄ“ fÇ. WÃ¨i jiÄ› juÃ© zhÃ¨ xiÄ“ wÃ¨n tÃ­, wÇ’ men tÃ­ chÅ« le RetroLLM, yÄ« gÃ¨ tÇ’ng yÄ« de kuÃ ng jiÃ , jiÄng jiÇn suÇ’ hÃ© shÄ“ng chÃ©ng zhÄ›ng hÃ© wÃ©i dÄn yÄ« de guÃ² chÃ©ng, shÇ LLMs nÃ©ng gÃ²u zhÃ­ jiÄ“ kÃ¹ zhÅng shÄ“ng chÃ©ng xÃ¬ lÃ¬ dÃ¹ de zhÃ¨ng jÃ¹. CÇ wÃ i, wÇ’ men yÇn rÃ¹ le fÄ“n cÃ©ng FM-Index yuÄ“ shuÅ hÃ© qiÃ¡n zhÄn xÃ¬ng yuÄ“ shuÅ jiÄ› mÇ zhuÃ n lÃ¼Ã¨, yÇ jiÇn shÇo bÃ¹ xiÄng guÄn de jiÄ› mÇ kÅng jiÄn yÇng tÃ­ gÄo zhÃ¨ng jÃ¹ zhÇ”n quÃ¨ xÃ¬ng. ShÃ­ yÃ n jiÃ© guÇ’ biÇo mÃ­ng, RetroLLM zÃ i wÇ” gÃ¨ kÄi fÃ ng yÃ¹ qiÃº shÃ¹ jÃ¹ shÃ¹ jÃ¹ shÃ ng biÇo xiÃ n yÅu yÃ¡n. DÃ i mÇ kÄ› zÃ i https://github.com/sunnynexus/RetroLLM huÃ² qÇ”.
[17.12.2024 09:12] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç¤ºäº†æ˜¾è‘—çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å¸¸å¸¸å‡ºç°å¹»è§‰ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•ä»é¢ä¸´ä¸€äº›é™åˆ¶ï¼šé¢å¤–çš„éƒ¨ç½²æˆæœ¬ã€æ£€ç´¢æ–‡æœ¬å—ä¸­çš„å†—ä½™è¾“å…¥æ ‡è®°ä»¥åŠæ£€ç´¢å’Œç”Ÿæˆçš„è”åˆä¼˜åŒ–ç¼ºä¹ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RetroLLMï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†æ£€ç´¢å’Œç”Ÿæˆæ•´åˆä¸ºå•ä¸€çš„è¿‡ç¨‹ï¼Œä½¿LLMsèƒ½å¤Ÿç›´æ¥ä»è¯­æ–™åº“ä¸­ç”Ÿæˆç»†ç²’åº¦çš„è¯æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†å±‚FM-Indexçº¦æŸå’Œå‰ç»æ€§çº¦æŸè§£ç ç­–ç•¥ï¼Œä»¥å‡å°‘ä¸ç›¸å…³çš„è§£ç ç©ºé—´å¹¶æé«˜è¯æ®å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRetroLLMåœ¨äº”ä¸ªå¼€æ”¾åŸŸé—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä»£ç å¯åœ¨https://github.com/sunnynexus/RetroLLMè·å–ã€‚
[17.12.2024 09:13] Mistral response. {"id": "9b61ac17bd104b95810ea3edcfd9e945", "object": "chat.completion", "created": 1734426763, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "[{'word': '\u5927\u578b', 'pinyin': 'd\u00e0x\u00edng', 'trans': 'large-scale'},\n{'word': '\u8bed\u8a00\u6a21\u578b', 'pinyin': 'y\u01d4y\u00e1n m\u00f3x\u00edng', 'trans': 'language model'},\n{'word': '\u663e\u8457', 'pinyin': 'xi\u01cenzh\u00f9', 'trans': 'significant'},\n{'word': '\u751f\u6210', 'pinyin': 'sh\u0113ngch\u00e9ng', 'trans': 'generation'},\n{'word': '\u80fd\u529b', 'pinyin': 'n\u00e9ngl\u00ec', 'trans': 'ability'},\n{'word': '\u5e7b\u89c9', 'pinyin': 'hu\u00e0nju\u00e9', 'trans': 'hallucination'},\n{'word': '\u68c0\u7d22', 'pinyin': 'ji\u01censu\u01d2', 'trans': 'retrieval'},\n{'word': '\u589e\u5f3a', 'pinyin': 'z\u0113ngqi\u00e1ng', 'trans': 'enhanced'},\n{'word': '\u5f15\u5165', 'pinyin': 'y\u01d0nr\u00f9', 'trans': 'introduce'},\n{'word': '\u5916\u90e8', 'pinyin': 'w\u00e0ib\u00f9', 'trans': 'external'},\n{'word': '\u77e5\u8bc6', 'pinyin': 'zh\u012bshi', 'trans': 'knowledge'},\n{'word': '\u63d0\u4f9b', 'pinyin': 't\u00edg\u014dng', 'trans': 'provide'},\n{'word': '\u6709\u6548', 'pinyin': 'y\u01d2uxi\u00e0o', 'trans': 'effective'},\n{'word': '\u89e3\u51b3', 'pinyin': 'ji\u011bju\u00e9', 'trans': 'solution'},\n{'word': '\u65b9\u6848', 'pinyin': 'f\u0101ng\u00e0n', 'trans': 'scheme'},\n{'word': '\u9762\u4e34', 'pinyin': 'mi\u00e0nl\u00edn', 'trans': 'face'},\n{'word': '\u9650\u5236', 'pinyin': 'xi\u00e0nzh\u00ec', 'trans': 'limitation'},\n{'word': '\u989d\u5916', 'pinyin': '\u00e9w\u00e0i', 'trans': 'additional'},\n{'word': '\u90e8\u7f72', 'pinyin': 'b\u00f9sh\u01d4', 'trans': 'deployment'},\n{'word': '\u6210\u672c', 'pinyin': 'ch\u00e9ngb\u011bn', 'trans': 'cost'},\n{'word': '\u5197\u4f59', 'pinyin': 'r\u00f3ngy\u00fa', 'trans': 'redundant'},\n{'word': '\u8f93\u5165', 'pinyin': 'sh\u016br\u00f9', 'trans': 'input'},\n{'word': '\u6807\u8bb0', 'pinyin': 'bi\u0101oj\u00ec', 'trans': 'token'},\n{'word': '\u8054\u5408', 'pinyin': 'li\u00e1nh\u00e9', 'trans': 'joint'},\n{'word': '\u4f18\u5316', 'pinyin': 'y\u014duhu\u00e0', 'trans': 'optimization'},\n{'word': '\u7f3a\u4e4f', 'pinyin': 'qu\u0113f\u00e1', 'trans': 'lack'},\n{'word': '\u63d0\u51fa', 'pinyin': 't\u00edch\u016b', 'trans': 'propose'},\n{'word': '\u7edf\u4e00', 'pinyin': 't\u01d2ngy\u012b', 'trans': 'unified'},\n{'word': '\u6846\u67b6', 'pinyin': 'ku\u00e0ngji\u00e0', 'trans': 'framework'},\n{'word': '\u6574\u5408', 'pinyin': 'zh\u011bngh\u00e9', 'trans': 'integrate'},\n{'word': '\u8fc7\u7a0b', 'pinyin': 'gu\u00f2ch\u00e9ng', 'trans': 'process'},\n{'word': '\u4f7f', 'pinyin': 'sh\u01d0', 'trans': 'make'},\n{'word': '\u76f4\u63a5', 'pinyin': 'zh\u00edji\u0113', 'trans': 'directly'},\n{'word': '\u8bed\u6599\u5e93', 'pinyin': 'y\u01d4li\u00e0o k\u00f9', 'trans': 'corpus'},\n{'word': '\u7ec6\u7c92\u5ea6', 'pinyin': 'x\u00ecl\u00ecd\u00f9', 'trans': 'fine-grained'},\n{'word': '\u8bc1\u636e', 'pinyin': 'zh\u00e8ngj\u00f9', 'trans': 'evidence'},\n{'word': '\u5206\u5c42', 'pinyin': 'f\u0113nc\u00e9ng', 'trans': 'hierarchical'},\n{'word': 'FM-Index', 'pinyin': 'FM-Index', 'trans': 'FM-Index'},\n{'word': '\u7ea6\u675f', 'pinyin': 'yu\u0113sh\u00f9', 'trans': 'constraint'},\n{'word': '\u524d\u77bb\u6027', 'pinyin': 'qi\u00e1nzh\u0101nx\u00ecng', 'trans': 'forward-looking'},\n{'word': '\u89e3\u7801', 'pinyin': 'ji\u011bm\u01ce', 'trans': 'decoding'},\n{'word': '\u7b56\u7565', 'pinyin': 'c\u00e8l\u00fc\u00e8', 'trans': 'strategy'},\n{'word': '\u51cf\u5c11', 'pinyin': 'ji\u01censh\u01ceo', 'trans': 'reduce'},\n{'word': '\u4e0d\u76f8\u5173', 'pinyin': 'b\u00f9xi\u0101nggu\u0101n', 'trans': 'irrelevant'},\n{'word': '\u7a7a\u95f4', 'pinyin': 'k\u014dngji\u0101n', 'trans': 'space'},\n{'word': '\u63d0\u9ad8', 'pinyin': 't\u00edg\u0101o', 'trans': 'improve'},\n{'word': '\u51c6\u786e\u6027', 'pinyin': 'zh\u01d4nqu\u00e8x\u00ecng', 'trans': 'accuracy'},\n{'word': '\u5b9e\u9a8c', 'pinyin': 'sh\u00edy\u00e0n', 'trans': 'experiment'},\n{'word': '\u7ed3\u679c', 'pinyin': 'ji\u00e9gu\u01d2', 'trans': 'result'},\n{'word': '\u8868\u660e', 'pinyin': 'bi\u01ceom\u00edng', 'trans': 'indicate'},\n{'word': '\u5f00\u653e\u57df', 'pinyin': 'k\u0101if\u00e0ng y\u00f9', 'trans': 'open-domain'},\n{'word': '\u95ee\u7b54', 'pinyin': 'w\u00e8nd\u00e1', 'trans': 'question-answering'},\n{'word': '\u6570\u636e\u96c6', 'pinyin': 'sh\u00f9j\u00f9j\u00ed', 'trans': 'dataset'},\n{'word': '\u8868\u73b0', 'pinyin': 'bi\u01ceoxi\u00e0n', 'trans': 'performance'},\n{'word': '\u4f18\u5f02', 'pinyin': 'y\u014duy\u00ec', 'trans': 'excellent'},\n{'word': '\u4ee3\u7801', 'pinyin': 'd\u00e0im\u01ce', 'trans': 'code'},\n{'word': '\u83b7\u53d6', 'pinyin': 'hu\u00f2q\u01d4', 'trans': 'obtain'}]"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 319, "total_tokens": 1876, "completion_tokens": 1557}}
[17.12.2024 09:13] Response: [{'word': 'å¤§å‹', 'pinyin': 'dÃ xÃ­ng', 'trans': 'large-scale'},
{'word': 'è¯­è¨€æ¨¡å‹', 'pinyin': 'yÇ”yÃ¡n mÃ³xÃ­ng', 'trans': 'language model'},
{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'},
{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ngchÃ©ng', 'trans': 'generation'},
{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©nglÃ¬', 'trans': 'ability'},
{'word': 'å¹»è§‰', 'pinyin': 'huÃ njuÃ©', 'trans': 'hallucination'},
{'word': 'æ£€ç´¢', 'pinyin': 'jiÇnsuÇ’', 'trans': 'retrieval'},
{'word': 'å¢å¼º', 'pinyin': 'zÄ“ngqiÃ¡ng', 'trans': 'enhanced'},
{'word': 'å¼•å…¥', 'pinyin': 'yÇnrÃ¹', 'trans': 'introduce'},
{'word': 'å¤–éƒ¨', 'pinyin': 'wÃ ibÃ¹', 'trans': 'external'},
{'word': 'çŸ¥è¯†', 'pinyin': 'zhÄ«shi', 'trans': 'knowledge'},
{'word': 'æä¾›', 'pinyin': 'tÃ­gÅng', 'trans': 'provide'},
{'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’uxiÃ o', 'trans': 'effective'},
{'word': 'è§£å†³', 'pinyin': 'jiÄ›juÃ©', 'trans': 'solution'},
{'word': 'æ–¹æ¡ˆ', 'pinyin': 'fÄngÃ n', 'trans': 'scheme'},
{'word': 'é¢ä¸´', 'pinyin': 'miÃ nlÃ­n', 'trans': 'face'},
{'word': 'é™åˆ¶', 'pinyin': 'xiÃ nzhÃ¬', 'trans': 'limitation'},
{'word': 'é¢å¤–', 'pinyin': 'Ã©wÃ i', 'trans': 'additional'},
{'word': 'éƒ¨ç½²', 'pinyin': 'bÃ¹shÇ”', 'trans': 'deployment'},
{'word': 'æˆæœ¬', 'pinyin': 'chÃ©ngbÄ›n', 'trans': 'cost'},
{'word': 'å†—ä½™', 'pinyin': 'rÃ³ngyÃº', 'trans': 'redundant'},
{'word': 'è¾“å…¥', 'pinyin': 'shÅ«rÃ¹', 'trans': 'input'},
{'word': 'æ ‡è®°', 'pinyin': 'biÄojÃ¬', 'trans': 'token'},
{'word': 'è”åˆ', 'pinyin': 'liÃ¡nhÃ©', 'trans': 'joint'},
{'word': 'ä¼˜åŒ–', 'pinyin': 'yÅuhuÃ ', 'trans': 'optimization'},
{'word': 'ç¼ºä¹', 'pinyin': 'quÄ“fÃ¡', 'trans': 'lack'},
{'word': 'æå‡º', 'pinyin': 'tÃ­chÅ«', 'trans': 'propose'},
{'word': 'ç»Ÿä¸€', 'pinyin': 'tÇ’ngyÄ«', 'trans': 'unified'},
{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'},
{'word': 'æ•´åˆ', 'pinyin': 'zhÄ›nghÃ©', 'trans': 'integrate'},
{'word': 'è¿‡ç¨‹', 'pinyin': 'guÃ²chÃ©ng', 'trans': 'process'},
{'word': 'ä½¿', 'pinyin': 'shÇ', 'trans': 'make'},
{'word': 'ç›´æ¥', 'pinyin': 'zhÃ­jiÄ“', 'trans': 'directly'},
{'word': 'è¯­æ–™åº“', 'pinyin': 'yÇ”liÃ o kÃ¹', 'trans': 'corpus'},
{'word': 'ç»†ç²’åº¦', 'pinyin': 'xÃ¬lÃ¬dÃ¹', 'trans': 'fine-grained'},
{'word': 'è¯æ®', 'pinyin': 'zhÃ¨ngjÃ¹', 'trans': 'evidence'},
{'word': 'åˆ†å±‚', 'pinyin': 'fÄ“ncÃ©ng', 'trans': 'hierarchical'},
{'word': 'FM-Index', 'pinyin': 'FM-Index', 'trans': 'FM-Index'},
{'word': 'çº¦æŸ', 'pinyin': 'yuÄ“shÃ¹', 'trans': 'constraint'},
{'word': 'å‰ç»æ€§', 'pinyin': 'qiÃ¡nzhÄnxÃ¬ng', 'trans': 'forward-looking'},
{'word': 'è§£ç ', 'pinyin': 'jiÄ›mÇ', 'trans': 'decoding'},
{'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨lÃ¼Ã¨', 'trans': 'strategy'},
{'word': 'å‡å°‘', 'pinyin': 'jiÇnshÇo', 'trans': 'reduce'},
{'word': 'ä¸ç›¸å…³', 'pinyin': 'bÃ¹xiÄngguÄn', 'trans': 'irrelevant'},
{'word': 'ç©ºé—´', 'pinyin': 'kÅngjiÄn', 'trans': 'space'},
{'word': 'æé«˜', 'pinyin': 'tÃ­gÄo', 'trans': 'improve'},
{'word': 'å‡†ç¡®æ€§', 'pinyin': 'zhÇ”nquÃ¨xÃ¬ng', 'trans': 'accuracy'},
{'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'},
{'word': 'ç»“æœ', 'pinyin': 'jiÃ©guÇ’', 'trans': 'result'},
{'word': 'è¡¨æ˜', 'pinyin': 'biÇomÃ­ng', 'trans': 'indicate'},
{'word': 'å¼€æ”¾åŸŸ', 'pinyin': 'kÄifÃ ng yÃ¹', 'trans': 'open-domain'},
{'word': 'é—®ç­”', 'pinyin': 'wÃ¨ndÃ¡', 'trans': 'question-answering'},
{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹jÃ¹jÃ­', 'trans': 'dataset'},
{'word': 'è¡¨ç°', 'pinyin': 'biÇoxiÃ n', 'trans': 'performance'},
{'word': 'ä¼˜å¼‚', 'pinyin': 'yÅuyÃ¬', 'trans': 'excellent'},
{'word': 'ä»£ç ', 'pinyin': 'dÃ imÇ', 'trans': 'code'},
{'word': 'è·å–', 'pinyin': 'huÃ²qÇ”', 'trans': 'obtain'}]
[17.12.2024 09:13] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç¤ºäº†æ˜¾è‘—çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å¸¸å¸¸å‡ºç°å¹»è§‰ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•ä»é¢ä¸´ä¸€äº›é™åˆ¶ï¼šé¢å¤–çš„éƒ¨ç½²æˆæœ¬ã€æ£€ç´¢æ–‡æœ¬å—ä¸­çš„å†—ä½™è¾“å…¥æ ‡è®°ä»¥åŠæ£€ç´¢å’Œç”Ÿæˆçš„è”åˆä¼˜åŒ–ç¼ºä¹ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RetroLLMï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†æ£€ç´¢å’Œç”Ÿæˆæ•´åˆä¸ºå•ä¸€çš„è¿‡ç¨‹ï¼Œä½¿LLMsèƒ½å¤Ÿç›´æ¥ä»è¯­æ–™åº“ä¸­ç”Ÿæˆç»†ç²’åº¦çš„è¯æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†å±‚FM-Indexçº¦æŸå’Œå‰ç»æ€§çº¦æŸè§£ç ç­–ç•¥ï¼Œä»¥å‡å°‘ä¸ç›¸å…³çš„è§£ç ç©ºé—´å¹¶æé«˜è¯æ®å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRetroLLMåœ¨äº”ä¸ªå¼€æ”¾åŸŸé—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä»£ç å¯åœ¨https://github.com/sunnynexus/RetroLLMè·å–ã€‚
[17.12.2024 09:13] Mistral response. {"id": "46665c3879734d72be5239c8138bdc4c", "object": "chat.completion", "created": 1734426793, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "Large Language Models (LLMs) have demonstrated significant generative capabilities but often suffer from hallucinations. Retrieval-Augmented Generation (RAG) offers an effective solution by introducing external knowledge; however, existing methods still face several limitations: additional deployment costs, redundant input tokens in retrieved text blocks, and a lack of joint optimization for retrieval and generation. To address these issues, we propose RetroLLM, a unified framework that integrates retrieval and generation into a single process, enabling LLMs to generate fine-grained evidence directly from a corpus. Additionally, we introduce hierarchical FM-Index constraints and look-ahead constraint decoding strategies to reduce irrelevant decoding space and enhance evidence accuracy. Experimental results show that RetroLLM performs exceptionally well on five open-domain question-answering datasets. The code is available at https://github.com/sunnynexus/RetroLLM."}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 285, "total_tokens": 485, "completion_tokens": 200}}
[17.12.2024 09:13] Response: Large Language Models (LLMs) have demonstrated significant generative capabilities but often suffer from hallucinations. Retrieval-Augmented Generation (RAG) offers an effective solution by introducing external knowledge; however, existing methods still face several limitations: additional deployment costs, redundant input tokens in retrieved text blocks, and a lack of joint optimization for retrieval and generation. To address these issues, we propose RetroLLM, a unified framework that integrates retrieval and generation into a single process, enabling LLMs to generate fine-grained evidence directly from a corpus. Additionally, we introduce hierarchical FM-Index constraints and look-ahead constraint decoding strategies to reduce irrelevant decoding space and enhance evidence accuracy. Experimental results show that RetroLLM performs exceptionally well on five open-domain question-answering datasets. The code is available at https://github.com/sunnynexus/RetroLLM.
[17.12.2024 09:13] Renaming data file.
[17.12.2024 09:13] Renaming previous data. hf_papers.json to ./d/2024-12-17.json
[17.12.2024 09:13] Saving new data file.
[17.12.2024 09:13] Generating page.
[17.12.2024 09:13] Renaming previous page.
[17.12.2024 09:13] Renaming previous data. index.html to ./d/2024-12-17.html
[17.12.2024 09:13] [Experimental] Generating Chinese page for reading.
[17.12.2024 09:13] Chinese vocab [{'word': 'å¤§å‹', 'pinyin': 'dÃ xÃ­ng', 'trans': 'large-scale'}, {'word': 'è¯­è¨€æ¨¡å‹', 'pinyin': 'yÇ”yÃ¡n mÃ³xÃ­ng', 'trans': 'language model'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ngchÃ©ng', 'trans': 'generation'}, {'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©nglÃ¬', 'trans': 'ability'}, {'word': 'å¹»è§‰', 'pinyin': 'huÃ njuÃ©', 'trans': 'hallucination'}, {'word': 'æ£€ç´¢', 'pinyin': 'jiÇnsuÇ’', 'trans': 'retrieval'}, {'word': 'å¢å¼º', 'pinyin': 'zÄ“ngqiÃ¡ng', 'trans': 'enhanced'}, {'word': 'å¼•å…¥', 'pinyin': 'yÇnrÃ¹', 'trans': 'introduce'}, {'word': 'å¤–éƒ¨', 'pinyin': 'wÃ ibÃ¹', 'trans': 'external'}, {'word': 'çŸ¥è¯†', 'pinyin': 'zhÄ«shi', 'trans': 'knowledge'}, {'word': 'æä¾›', 'pinyin': 'tÃ­gÅng', 'trans': 'provide'}, {'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’uxiÃ o', 'trans': 'effective'}, {'word': 'è§£å†³', 'pinyin': 'jiÄ›juÃ©', 'trans': 'solution'}, {'word': 'æ–¹æ¡ˆ', 'pinyin': 'fÄngÃ n', 'trans': 'scheme'}, {'word': 'é¢ä¸´', 'pinyin': 'miÃ nlÃ­n', 'trans': 'face'}, {'word': 'é™åˆ¶', 'pinyin': 'xiÃ nzhÃ¬', 'trans': 'limitation'}, {'word': 'é¢å¤–', 'pinyin': 'Ã©wÃ i', 'trans': 'additional'}, {'word': 'éƒ¨ç½²', 'pinyin': 'bÃ¹shÇ”', 'trans': 'deployment'}, {'word': 'æˆæœ¬', 'pinyin': 'chÃ©ngbÄ›n', 'trans': 'cost'}, {'word': 'å†—ä½™', 'pinyin': 'rÃ³ngyÃº', 'trans': 'redundant'}, {'word': 'è¾“å…¥', 'pinyin': 'shÅ«rÃ¹', 'trans': 'input'}, {'word': 'æ ‡è®°', 'pinyin': 'biÄojÃ¬', 'trans': 'token'}, {'word': 'è”åˆ', 'pinyin': 'liÃ¡nhÃ©', 'trans': 'joint'}, {'word': 'ä¼˜åŒ–', 'pinyin': 'yÅuhuÃ ', 'trans': 'optimization'}, {'word': 'ç¼ºä¹', 'pinyin': 'quÄ“fÃ¡', 'trans': 'lack'}, {'word': 'æå‡º', 'pinyin': 'tÃ­chÅ«', 'trans': 'propose'}, {'word': 'ç»Ÿä¸€', 'pinyin': 'tÇ’ngyÄ«', 'trans': 'unified'}, {'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'}, {'word': 'æ•´åˆ', 'pinyin': 'zhÄ›nghÃ©', 'trans': 'integrate'}, {'word': 'è¿‡ç¨‹', 'pinyin': 'guÃ²chÃ©ng', 'trans': 'process'}, {'word': 'ä½¿', 'pinyin': 'shÇ', 'trans': 'make'}, {'word': 'ç›´æ¥', 'pinyin': 'zhÃ­jiÄ“', 'trans': 'directly'}, {'word': 'è¯­æ–™åº“', 'pinyin': 'yÇ”liÃ o kÃ¹', 'trans': 'corpus'}, {'word': 'ç»†ç²’åº¦', 'pinyin': 'xÃ¬lÃ¬dÃ¹', 'trans': 'fine-grained'}, {'word': 'è¯æ®', 'pinyin': 'zhÃ¨ngjÃ¹', 'trans': 'evidence'}, {'word': 'åˆ†å±‚', 'pinyin': 'fÄ“ncÃ©ng', 'trans': 'hierarchical'}, {'word': 'FM-Index', 'pinyin': 'FM-Index', 'trans': 'FM-Index'}, {'word': 'çº¦æŸ', 'pinyin': 'yuÄ“shÃ¹', 'trans': 'constraint'}, {'word': 'å‰ç»æ€§', 'pinyin': 'qiÃ¡nzhÄnxÃ¬ng', 'trans': 'forward-looking'}, {'word': 'è§£ç ', 'pinyin': 'jiÄ›mÇ', 'trans': 'decoding'}, {'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨lÃ¼Ã¨', 'trans': 'strategy'}, {'word': 'å‡å°‘', 'pinyin': 'jiÇnshÇo', 'trans': 'reduce'}, {'word': 'ä¸ç›¸å…³', 'pinyin': 'bÃ¹xiÄngguÄn', 'trans': 'irrelevant'}, {'word': 'ç©ºé—´', 'pinyin': 'kÅngjiÄn', 'trans': 'space'}, {'word': 'æé«˜', 'pinyin': 'tÃ­gÄo', 'trans': 'improve'}, {'word': 'å‡†ç¡®æ€§', 'pinyin': 'zhÇ”nquÃ¨xÃ¬ng', 'trans': 'accuracy'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ©guÇ’', 'trans': 'result'}, {'word': 'è¡¨æ˜', 'pinyin': 'biÇomÃ­ng', 'trans': 'indicate'}, {'word': 'å¼€æ”¾åŸŸ', 'pinyin': 'kÄifÃ ng yÃ¹', 'trans': 'open-domain'}, {'word': 'é—®ç­”', 'pinyin': 'wÃ¨ndÃ¡', 'trans': 'question-answering'}, {'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹jÃ¹jÃ­', 'trans': 'dataset'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇoxiÃ n', 'trans': 'performance'}, {'word': 'ä¼˜å¼‚', 'pinyin': 'yÅuyÃ¬', 'trans': 'excellent'}, {'word': 'ä»£ç ', 'pinyin': 'dÃ imÇ', 'trans': 'code'}, {'word': 'è·å–', 'pinyin': 'huÃ²qÇ”', 'trans': 'obtain'}]
[17.12.2024 09:13] Renaming previous Chinese page.
[17.12.2024 09:13] Renaming previous data. zh.html to ./d/2024-12-16_zh_reading_task.html
[17.12.2024 09:13] Writing Chinese reading task.
[17.12.2024 09:13] Writing result.
[17.12.2024 09:13] Renaming log file.
[17.12.2024 09:13] Renaming previous data. log.txt to ./logs/2024-12-17_last_log.txt
