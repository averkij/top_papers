[16.06.2025 10:14] Read previous papers.
[16.06.2025 10:14] Generating top page (month).
[16.06.2025 10:14] Writing top page (month).
[16.06.2025 11:11] Read previous papers.
[16.06.2025 11:11] Get feed.
[16.06.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.11924
[16.06.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09600
[16.06.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10892
[16.06.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.11928
[16.06.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.11997
[16.06.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09427
[16.06.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09366
[16.06.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08477
[16.06.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.07464
[16.06.2025 11:11] Extract page data from URL. URL: https://huggingface.co/papers/2506.11886
[16.06.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.11702
[16.06.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.10128
[16.06.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.11130
[16.06.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08592
[16.06.2025 11:11] Get page data from previous paper. URL: https://huggingface.co/papers/2506.08915
[16.06.2025 11:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[16.06.2025 11:11] No deleted papers detected.
[16.06.2025 11:11] Downloading and parsing papers (pdf, html). Total: 15.
[16.06.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2506.11924.
[16.06.2025 11:11] Extra JSON file exists (./assets/json/2506.11924.json), skip PDF parsing.
[16.06.2025 11:11] Paper image links file exists (./assets/img_data/2506.11924.json), skip HTML parsing.
[16.06.2025 11:11] Success.
[16.06.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2506.09600.
[16.06.2025 11:11] Extra JSON file exists (./assets/json/2506.09600.json), skip PDF parsing.
[16.06.2025 11:11] Paper image links file exists (./assets/img_data/2506.09600.json), skip HTML parsing.
[16.06.2025 11:11] Success.
[16.06.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2506.10892.
[16.06.2025 11:11] Extra JSON file exists (./assets/json/2506.10892.json), skip PDF parsing.
[16.06.2025 11:11] Paper image links file exists (./assets/img_data/2506.10892.json), skip HTML parsing.
[16.06.2025 11:11] Success.
[16.06.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2506.11928.
[16.06.2025 11:11] Extra JSON file exists (./assets/json/2506.11928.json), skip PDF parsing.
[16.06.2025 11:11] Paper image links file exists (./assets/img_data/2506.11928.json), skip HTML parsing.
[16.06.2025 11:11] Success.
[16.06.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2506.11997.
[16.06.2025 11:11] Extra JSON file exists (./assets/json/2506.11997.json), skip PDF parsing.
[16.06.2025 11:11] Paper image links file exists (./assets/img_data/2506.11997.json), skip HTML parsing.
[16.06.2025 11:11] Success.
[16.06.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2506.09427.
[16.06.2025 11:11] Extra JSON file exists (./assets/json/2506.09427.json), skip PDF parsing.
[16.06.2025 11:11] Paper image links file exists (./assets/img_data/2506.09427.json), skip HTML parsing.
[16.06.2025 11:11] Success.
[16.06.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2506.09366.
[16.06.2025 11:11] Extra JSON file exists (./assets/json/2506.09366.json), skip PDF parsing.
[16.06.2025 11:11] Paper image links file exists (./assets/img_data/2506.09366.json), skip HTML parsing.
[16.06.2025 11:11] Success.
[16.06.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2506.08477.
[16.06.2025 11:11] Extra JSON file exists (./assets/json/2506.08477.json), skip PDF parsing.
[16.06.2025 11:11] Paper image links file exists (./assets/img_data/2506.08477.json), skip HTML parsing.
[16.06.2025 11:11] Success.
[16.06.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2506.07464.
[16.06.2025 11:11] Extra JSON file exists (./assets/json/2506.07464.json), skip PDF parsing.
[16.06.2025 11:11] Paper image links file exists (./assets/img_data/2506.07464.json), skip HTML parsing.
[16.06.2025 11:11] Success.
[16.06.2025 11:11] Downloading and parsing paper https://huggingface.co/papers/2506.11886.
[16.06.2025 11:11] Downloading paper 2506.11886 from http://arxiv.org/pdf/2506.11886v1...
[16.06.2025 11:12] Extracting affiliations from text.
[16.06.2025 11:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache Xiaoran Liu1,2*, Siyang He1*, Qiqi Wang1*, Ruixiao Li1,2*, Yuerong Song1,2, Zhigeng Liu1, Mianqiu Huang1, Linlin Li3, Qun Liu3, Zengfeng Huang1,2, Qipeng Guo2,4, Ziwei He2, Xipeng Qiu1,2 1School of Computer Science, Fudan University, 2Shanghai Innovation Institute, 3Huawei Noahs Ark Lab, 4Shanghai AI Lab xrliu24@m.fudan.edu.cn, xpqiu@fudan.edu.cn, ziwei.he@sjtu.edu.cn 5 2 0 2 3 1 ] . [ 1 6 8 8 1 1 . 6 0 5 2 : r a "
[16.06.2025 11:12] Response: ```python
[
    "School of Computer Science, Fudan University",
    "Shanghai Innovation Institute",
    "Huawei Noahs Ark Lab",
    "Shanghai AI Lab"
]
```
[16.06.2025 11:12] Deleting PDF ./assets/pdf/2506.11886.pdf.
[16.06.2025 11:12] Success.
[16.06.2025 11:12] Downloading and parsing paper https://huggingface.co/papers/2506.11702.
[16.06.2025 11:12] Extra JSON file exists (./assets/json/2506.11702.json), skip PDF parsing.
[16.06.2025 11:12] Paper image links file exists (./assets/img_data/2506.11702.json), skip HTML parsing.
[16.06.2025 11:12] Success.
[16.06.2025 11:12] Downloading and parsing paper https://huggingface.co/papers/2506.10128.
[16.06.2025 11:12] Extra JSON file exists (./assets/json/2506.10128.json), skip PDF parsing.
[16.06.2025 11:12] Paper image links file exists (./assets/img_data/2506.10128.json), skip HTML parsing.
[16.06.2025 11:12] Success.
[16.06.2025 11:12] Downloading and parsing paper https://huggingface.co/papers/2506.11130.
[16.06.2025 11:12] Extra JSON file exists (./assets/json/2506.11130.json), skip PDF parsing.
[16.06.2025 11:12] Paper image links file exists (./assets/img_data/2506.11130.json), skip HTML parsing.
[16.06.2025 11:12] Success.
[16.06.2025 11:12] Downloading and parsing paper https://huggingface.co/papers/2506.08592.
[16.06.2025 11:12] Extra JSON file exists (./assets/json/2506.08592.json), skip PDF parsing.
[16.06.2025 11:12] Paper image links file exists (./assets/img_data/2506.08592.json), skip HTML parsing.
[16.06.2025 11:12] Success.
[16.06.2025 11:12] Downloading and parsing paper https://huggingface.co/papers/2506.08915.
[16.06.2025 11:12] Extra JSON file exists (./assets/json/2506.08915.json), skip PDF parsing.
[16.06.2025 11:12] Paper image links file exists (./assets/img_data/2506.08915.json), skip HTML parsing.
[16.06.2025 11:12] Success.
[16.06.2025 11:12] Enriching papers with extra data.
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 0. A diffusion-based framework generates aligned novel views of images and geometry using warping-and-inpainting with cross-modal attention distillation and proximity-based mesh conditioning, achieving high-fidelity synthesis and 3D completion.  					AI-generated summary 				 We introduce a diffusion-b...
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 1. CRAFT, a multi-agent system using policy-aware persuasive strategies, challenges policy-adherent LLM-based agents in customer service to assess and improve their robustness against adversarial attacks.  					AI-generated summary 				 Task-oriented LLM-based agents are increasingly used in domains wi...
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 2. Duo improves uniform-state discrete diffusion models by transferring techniques from Gaussian diffusion, enhancing training speed and enabling fast few-step text generation.  					AI-generated summary 				 Uniform-state discrete diffusion models hold the promise of fast text generation due to their ...
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 3. LLMs perform well on implementation-heavy competitive programming problems but struggle with nuanced algorithmic reasoning, as highlighted by LiveCodeBench Pro.  					AI-generated summary 				 Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programmi...
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 4. pLSTMs are parallelizable linear RNNs designed for DAGs, demonstrating superior performance on long-range tasks and benchmarks compared to Transformers.  					AI-generated summary 				 Modern recurrent architectures, such as xLSTM and Mamba, have recently challenged the Transformer in language model...
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 5. InterSyn, a large-scale dataset with tightly interleaved image-text outputs and automated quality refinement, improves multimodal understanding and generation through the SEIR method and SynJudge, an automatic evaluation tool.  					AI-generated summary 				 Recent advancements in Large Multimodal M...
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 6. SkillBlender is a hierarchical reinforcement learning framework that uses pretrained primitive skills to efficiently solve diverse loco-manipulation tasks for humanoid robots.  					AI-generated summary 				 Humanoid robots hold significant potential in accomplishing daily tasks across diverse envir...
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 7. U-CoT+ is a novel framework for detecting harmful memes by converting them into textual descriptions and using human-crafted guidelines with zero-shot CoT prompting to achieve high flexibility and explainability with small-scale LLMs.  					AI-generated summary 				 Detecting harmful memes is essent...
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 8. DeepVideo-R1 enhances video reasoning performance using Reg-GRPO, a regression-based GRPO approach, and difficulty-aware data augmentation for video large language models.  					AI-generated summary 				 Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-train...
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 9. FourierAttention is a training-free framework that enhances memory efficiency in Large Language Models by compressing long-context-insensitive transformer head dimensions using orthogonal Fourier bases, while maintaining accuracy.  					AI-generated summary 				 Large Language Models struggle with m...
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 10. Configurable Preference Tuning enables language models to dynamically adjust their behavior based on human-interprettable directives, using rubric-guided preference data for fine-tuning and inference-time modulation.  					AI-generated summary 				 Models of human feedback for AI alignment, such as ...
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 11. ViCrit, an RL task for fine-tuning VLMs, improves visual perception by training models to detect subtle hallucinations in image captions, with gains transferable to various visual domains.  					AI-generated summary 				 Reinforcement learning (RL) has shown great effectiveness for fine-tuning large...
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 12. A self-refining framework enhances ASR performance using unlabeled datasets by integrating pseudo-labeling, TTS, and synthesized speech to create a specialized model.  					AI-generated summary 				 We propose a self-refining framework that enhances ASR performance with only unlabeled datasets. The ...
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 13. A new dataset named CapRetrieval is introduced to evaluate the ability of text encoders to recognize fine-grained entities and events, highlighting challenges in dense retrieval tasks.  					AI-generated summary 				 This work focuses on an observed limitation of text encoders: embeddings may not be...
[16.06.2025 11:12] ********************************************************************************
[16.06.2025 11:12] Abstract 14. An attention-based method using learned binary masks improves robustness in object perception by focusing on relevant image regions while filtering out spurious information.  					AI-generated summary 				 We introduce an attention-based method that uses learned binary attention masks to ensure that...
[16.06.2025 11:12] Read previous papers.
[16.06.2025 11:12] Generating reviews via LLM API.
[16.06.2025 11:12] Using data from previous issue: {"categories": ["#cv", "#3d", "#diffusion"], "emoji": "🖼️", "ru": {"title": "Диффузионная модель для согласованной генерации изображений и геометрии с новых ракурсов", "desc": "Эта статья представляет новую систему генерации изображений и геометрии с новых ракурсов, основанную на диффузионных моделя
[16.06.2025 11:12] Using data from previous issue: {"categories": ["#security", "#benchmark", "#agents"], "emoji": "🛡️", "ru": {"title": "Укрепление защиты ЛЛМ-агентов от манипуляций пользователей", "desc": "Статья представляет CRAFT - многоагентную систему, использующую стратегии убеждения с учетом политик для тестирования устойчивости ЛЛМ-агентов 
[16.06.2025 11:12] Using data from previous issue: {"categories": ["#training", "#dataset", "#benchmark", "#optimization", "#open_source", "#diffusion"], "emoji": "🔄", "ru": {"title": "Duo: Ускорение диффузионных языковых моделей с помощью гауссовских техник", "desc": "Метод Duo улучшает дискретные диффузионные модели с равномерным состоянием, перен
[16.06.2025 11:12] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#dataset", "#games"], "emoji": "🤖", "ru": {"title": "LLM в программировании: сила в реализации, слабость в алгоритмах", "desc": "Исследование показывает, что крупные языковые модели (LLM) хорошо справляются с задачами по программированию, требующими сложн
[16.06.2025 11:12] Using data from previous issue: {"categories": ["#benchmark", "#long_context", "#optimization", "#graphs", "#architecture", "#cv", "#open_source", "#synthetic"], "emoji": "🧠", "ru": {"title": "pLSTM: Параллельные линейные RNN для эффективной обработки многомерных данных", "desc": "В статье представлена новая архитектура нейронной 
[16.06.2025 11:12] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#optimization", "#multimodal", "#games"], "emoji": "🔄", "ru": {"title": "InterSyn: новый уровень мультимодального обучения", "desc": "Статья представляет InterSyn - крупномасштабный мультимодальный датасет для обучения языковых моделей. InterSyn создан с ис
[16.06.2025 11:12] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#rl", "#open_source", "#games"], "emoji": "🤖", "ru": {"title": "SkillBlender: умное сочетание навыков для универсальных гуманоидных роботов", "desc": "SkillBlender - это новая иерархическая система обучения с подкреплением для универсального управления гум
[16.06.2025 11:12] Using data from previous issue: {"categories": ["#dataset", "#low_resource", "#ethics", "#interpretability", "#small_models", "#multimodal", "#benchmark"], "emoji": "🕵️", "ru": {"title": "Умный детектив для вредных мемов", "desc": "U-CoT+ - это новый фреймворк для обнаружения вредоносных мемов, который преобразует их в текстовые о
[16.06.2025 11:12] Using data from previous issue: {"categories": ["#rlhf", "#video", "#benchmark", "#optimization", "#reasoning", "#training", "#rl"], "emoji": "🎥", "ru": {"title": "DeepVideo-R1: Улучшение рассуждений на видео с помощью регрессионного GRPO", "desc": "DeepVideo-R1 - это модель для улучшения рассуждений на основе видео, использующая 
[16.06.2025 11:12] Querying the API.
[16.06.2025 11:12] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FourierAttention is a training-free framework that enhances memory efficiency in Large Language Models by compressing long-context-insensitive transformer head dimensions using orthogonal Fourier bases, while maintaining accuracy.  					AI-generated summary 				 Large Language Models struggle with memory demands from the growing Key-Value (KV) cache as context lengths increase. Existing compression methods homogenize head dimensions or rely on attention-guided token pruning, often sacrificing accuracy or introducing computational overhead. We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions: lower dimensions prioritize local context, while upper ones capture long-range dependencies. By projecting the long-context-insensitive dimensions onto orthogonal Fourier bases, FourierAttention approximates their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models show that FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel, FlashFourierAttention, is designed to optimize memory via streamlined read-write operations, enabling efficient deployment without performance compromise.
[16.06.2025 11:12] Response: {
  "desc": "FourierAttention - это фреймворк для повышения эффективности использования памяти в больших языковых моделях (БЯМ). Он сжимает малочувствительные к длинному контексту размерности головок трансформера, используя ортогональные базисы Фурье, сохраняя при этом точность модели. FourierAttention не требует дополнительного обучения и эксплуатирует различные роли размерностей головок трансформера: нижние размерности фокусируются на локальном контексте, а верхние захватывают зависимости на большом расстоянии. Оценки на моделях LLaMA показывают, что FourierAttention достигает лучшей точности для длинного контекста на бенчмарках LongBench и Needle-In-A-Haystack.",
  "emoji": "🧠",
  "title": "Эффективное сжатие памяти БЯМ без потери точности"
}
[16.06.2025 11:12] Renaming some terms.
[16.06.2025 11:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FourierAttention is a training-free framework that enhances memory efficiency in Large Language Models by compressing long-context-insensitive transformer head dimensions using orthogonal Fourier bases, while maintaining accuracy.  					AI-generated summary 				 Large Language Models struggle with memory demands from the growing Key-Value (KV) cache as context lengths increase. Existing compression methods homogenize head dimensions or rely on attention-guided token pruning, often sacrificing accuracy or introducing computational overhead. We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions: lower dimensions prioritize local context, while upper ones capture long-range dependencies. By projecting the long-context-insensitive dimensions onto orthogonal Fourier bases, FourierAttention approximates their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models show that FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel, FlashFourierAttention, is designed to optimize memory via streamlined read-write operations, enabling efficient deployment without performance compromise."

[16.06.2025 11:12] Response: ```python
['INFERENCE', 'TRAINING', 'ARCHITECTURE']
```
[16.06.2025 11:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FourierAttention is a training-free framework that enhances memory efficiency in Large Language Models by compressing long-context-insensitive transformer head dimensions using orthogonal Fourier bases, while maintaining accuracy.  					AI-generated summary 				 Large Language Models struggle with memory demands from the growing Key-Value (KV) cache as context lengths increase. Existing compression methods homogenize head dimensions or rely on attention-guided token pruning, often sacrificing accuracy or introducing computational overhead. We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions: lower dimensions prioritize local context, while upper ones capture long-range dependencies. By projecting the long-context-insensitive dimensions onto orthogonal Fourier bases, FourierAttention approximates their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models show that FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel, FlashFourierAttention, is designed to optimize memory via streamlined read-write operations, enabling efficient deployment without performance compromise."

[16.06.2025 11:12] Response: ```python
["LONG_CONTEXT", "OPTIMIZATION"]
```
[16.06.2025 11:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FourierAttention is a novel framework designed to improve memory efficiency in Large Language Models (LLMs) without the need for training. It addresses the challenge of increasing memory demands from the Key-Value (KV) cache as context lengths grow. By using orthogonal Fourier bases, it compresses transformer head dimensions, allowing lower dimensions to focus on local context while higher dimensions capture long-range dependencies. Evaluations demonstrate that FourierAttention enhances long-context accuracy and includes a custom kernel, FlashFourierAttention, for optimized memory operations during deployment.","title":"Enhancing Memory Efficiency in LLMs with FourierAttention"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FourierAttention is a novel framework designed to improve memory efficiency in Large Language Models (LLMs) without the need for training. It addresses the challenge of increasing memory demands from the Key-Value (KV) cache as context lengths grow. By using orthogonal Fourier bases, it compresses transformer head dimensions, allowing lower dimensions to focus on local context while higher dimensions capture long-range dependencies. Evaluations demonstrate that FourierAttention enhances long-context accuracy and includes a custom kernel, FlashFourierAttention, for optimized memory operations during deployment.', title='Enhancing Memory Efficiency in LLMs with FourierAttention'))
[16.06.2025 11:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FourierAttention是一种无需训练的框架，旨在提高大型语言模型的内存效率。它通过使用正交傅里叶基来压缩长上下文无关的变换头维度，同时保持模型的准确性。该方法利用变换头维度的异质性，低维度优先处理局部上下文，而高维度则捕捉长程依赖。评估结果表明，FourierAttention在长上下文准确性方面表现优异，且通过定制的Triton内核优化内存使用，确保高效部署。","title":"FourierAttention：提升大型语言模型的内存效率"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FourierAttention是一种无需训练的框架，旨在提高大型语言模型的内存效率。它通过使用正交傅里叶基来压缩长上下文无关的变换头维度，同时保持模型的准确性。该方法利用变换头维度的异质性，低维度优先处理局部上下文，而高维度则捕捉长程依赖。评估结果表明，FourierAttention在长上下文准确性方面表现优异，且通过定制的Triton内核优化内存使用，确保高效部署。', title='FourierAttention：提升大型语言模型的内存效率'))
[16.06.2025 11:12] Using data from previous issue: {"categories": ["#rlhf", "#synthetic", "#training", "#dataset", "#alignment", "#open_source"], "emoji": "🎛️", "ru": {"title": "Гибкая настройка языковых моделей под меняющиеся предпочтения пользователей", "desc": "Эта статья представляет новый подход под названием Configurable Preference Tuning (CPT
[16.06.2025 11:12] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#hallucinations", "#cv", "#transfer_learning"], "emoji": "🔍", "ru": {"title": "ViCrit: обучение мультимодальных моделей критическому восприятию визуальной информации", "desc": "Статья представляет ViCrit - задачу обучения с подкреплением для улучшения визуальног
[16.06.2025 11:12] Using data from previous issue: {"categories": ["#data", "#benchmark", "#dataset", "#transfer_learning", "#low_resource", "#audio", "#synthetic"], "emoji": "🔁", "ru": {"title": "Самосовершенствующаяся система АСР: от псевдо-меток к улучшенному распознаванию", "desc": "Предложена самосовершенствующаяся система для улучшения распозн
[16.06.2025 11:12] Using data from previous issue: {"categories": ["#data", "#transfer_learning", "#training", "#open_source", "#dataset"], "emoji": "🔍", "ru": {"title": "Точность в деталях: новый взгляд на возможности текстовых энкодеров", "desc": "Представлен новый датасет CapRetrieval для оценки способности текстовых энкодеров распознавать мелкие
[16.06.2025 11:12] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#optimization", "#architecture", "#interpretability"], "emoji": "👁️", "ru": {"title": "Фокусировка на главном: улучшение восприятия объектов с помощью масок внимания", "desc": "Данная статья представляет метод на основе внимания, использующий обученные бинарные 
[16.06.2025 11:12] Renaming data file.
[16.06.2025 11:12] Renaming previous data. hf_papers.json to ./d/2025-06-16.json
[16.06.2025 11:12] Saving new data file.
[16.06.2025 11:12] Generating page.
[16.06.2025 11:12] Renaming previous page.
[16.06.2025 11:12] Renaming previous data. index.html to ./d/2025-06-16.html
[16.06.2025 11:12] Writing result.
[16.06.2025 11:12] Renaming log file.
[16.06.2025 11:12] Renaming previous data. log.txt to ./logs/2025-06-16_last_log.txt
