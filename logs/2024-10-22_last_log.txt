[22.10.2024 20:13] [Experimental] Generating an image for paper FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors.
[22.10.2024 20:13] [Experimental] Image for paper FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors already exists.
[22.10.2024 20:13] [Experimental] Generating an image for paper CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution.
[22.10.2024 20:13] [Experimental] Image for paper CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution already exists.
[22.10.2024 20:13] [Experimental] Generating an image for paper SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree.
[22.10.2024 20:13] [Experimental] Image for paper SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree already exists.
[22.10.2024 20:13] [Experimental] Generating an image for paper PUMA: Empowering Unified MLLM with Multi-granular Visual Generation.
[22.10.2024 20:13] [Experimental] Image for paper PUMA: Empowering Unified MLLM with Multi-granular Visual Generation already exists.
[22.10.2024 20:13] [Experimental] Generating an image for paper AutoTrain: No-code training for state-of-the-art models.
[22.10.2024 20:13] [Experimental] Image for paper AutoTrain: No-code training for state-of-the-art models already exists.
[22.10.2024 20:13] [Experimental] Generating an image for paper SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation.
[22.10.2024 20:13] [Experimental] Image for paper SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation already exists.
[22.10.2024 20:13] [Experimental] Generating an image for paper Baichuan Alignment Technical Report.
[22.10.2024 20:13] [Experimental] Image for paper Baichuan Alignment Technical Report already exists.
[22.10.2024 20:13] [Experimental] Generating an image for paper Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages.
[22.10.2024 20:13] [Experimental] Image for paper Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages already exists.
[22.10.2024 20:13] [Experimental] Generating an image for paper RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style.
[22.10.2024 20:13] [Experimental] Image for paper RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style already exists.
[22.10.2024 22:12] Read previous papers.
[22.10.2024 22:12] Get feed.
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16271
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16256
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16268
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13861
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.15735
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.14745
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.14940
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16153
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16184
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.12788
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16215
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.15748
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.15316
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.11711
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.15633
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16259
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13218
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.15002
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.14086
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13184
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.15460
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.13394
[22.10.2024 22:12] Get page data from previous paper. URL: https://huggingface.co/papers/2410.15017
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 0. Neural Radiance Fields (NeRF) face significant challenges in few-shot scenarios, primarily due to overfitting and long training times for high-fidelity rendering. Existing methods, such as FreeNeRF and SparseNeRF, use frequency regularization or pre-trained priors but struggle with complex schedulin...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 1. Efficient and accurate evaluation is crucial for the continuous improvement of large language models (LLMs). Among various assessment methods, subjective evaluation has garnered significant attention due to its superior alignment with real-world usage scenarios and human preferences. However, human-...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 2. The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation model for object segmentation in both images and videos, paving the way for various downstream video applications. The crucial design of SAM 2 for video segmentation is its memory module, which prompts object-aware memories fr...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 3. Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed t...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 4. With the advancements in open-source models, training (or finetuning) models on custom datasets has become a crucial part of developing solutions which are tailored to specific industrial or open-source applications. Yet, there is no single tool which simplifies the process of training across differ...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 5. Supervised fine-tuning (SFT) is crucial in adapting large language models (LLMs) to a specific domain or task. However, only a limited amount of labeled data is available in practical applications, which poses a severe challenge for SFT in yielding satisfactory results. Therefore, a data-efficient f...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 6. We introduce Baichuan Alignment, a detailed analysis of the alignment techniques employed in the Baichuan series of models. This represents the industry's first comprehensive account of alignment methodologies, offering valuable insights for advancing AI research. We investigate the critical compone...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 7. Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world's languages and diverse cultural contexts underrepresented. This paper introduces Pangea, a multilingual mult...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 8. Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to disti...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 9. Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline, which impacts the quality of knowledge-intensive tasks. This paper introduces the concept of Meta-Chunking, which refer...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 10. Knowledge distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. Previous work applying KD in the field of large language models (LLMs) typically focused on the post-training phase, where the student LLM learns directly from instructions and corresponding...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 11. Formal proofs are challenging to write even for experienced experts. Recent progress in Neural Theorem Proving (NTP) shows promise in expediting this process. However, the formal corpora available on the Internet are limited compared to the general text, posing a significant data scarcity challenge ...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 12. Large Language Models (LLMs) have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities. This paper introduces Ichigo, a mixed-modal model that seamlessly processes interleaved sequ...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 13. The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. In reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state sp...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 14. The expansion of large language models to effectively handle instructions with extremely long contexts has yet to be fully investigated. The primary obstacle lies in constructing a high-quality long instruction-following dataset devised for long context alignment. Existing studies have attempted to ...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 15. We present Agent-to-Sim (ATS), a framework for learning interactive behavior models of 3D agents from casual longitudinal video collections. Different from prior works that rely on marker-based tracking and multiview cameras, ATS learns natural behaviors of animal and human agents non-invasively thr...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 16. There is a significant gap between patient needs and available mental health support today. In this paper, we aim to thoroughly examine the potential of using Large Language Models (LLMs) to assist professional psychotherapy. To this end, we propose a new benchmark, CBT-BENCH, for the systematic eva...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 17. Text-to-image models are trained using large datasets collected by scraping image-text pairs from the internet. These datasets often include private, copyrighted, and licensed material. Training models on such datasets enables them to generate images with such content, which might violate copyright ...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 18. The goal of machine learning is generalization. While the No Free Lunch Theorem states that we cannot obtain theoretical guarantees for generalization without further assumptions, in practice we observe that simple models which explain the training data generalize best: a principle called Occam's ra...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 19. Traditional transformer models often allocate a fixed amount of computational resources to every input token, leading to inefficient and unnecessary computation. To address this, the Mixture of Depths (MoD) was introduced to dynamically adjust the computational depth by skipping less important layer...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 20. As large language models (LLMs) become increasingly deployed across various industries, concerns regarding their reliability, particularly due to hallucinations-outputs that are factually inaccurate or irrelevant to user input-have grown. Our research investigates the relationship between the traini...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 21. Evaluating machine-generated text remains a significant challenge in NLP, especially for non-English languages. Current methodologies, including automated metrics, human assessments, and LLM-based evaluations, predominantly focus on English, revealing a significant gap in multilingual evaluation fra...
[22.10.2024 22:12] ********************************************************************************
[22.10.2024 22:12] Abstract 22. Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual ...
[22.10.2024 22:12] Read previous papers.
[22.10.2024 22:12] Generating reviews via LLM API.
[22.10.2024 22:12] Using data from previous issue: {"desc": "FrugalNeRF - это новый фреймворк для обучения нейронных полей излучения (NeRF) на малом количестве данных. Он использует разделение весов воксельной структуры на разных масштабах для эффективного представления деталей сцены. Ключевой особенностью является схема геометрической адаптации меж
[22.10.2024 22:12] Using data from previous issue: {"desc": "В статье представлен CompassJudger-1 - первая открытая универсальная модель-оценщик для больших языковых моделей (LLM). Эта модель способна выполнять разнообразные задачи оценки, включая сравнение моделей, генерацию критики и выполнение общих задач как обычная LLM. Авторы также создали бен
[22.10.2024 22:12] Using data from previous issue: {"desc": "SAM2Long - это улучшенная стратегия сегментации видеообъектов, основанная на модели SAM 2. Она решает проблему накопления ошибок при долгосрочной сегментации видео путем использования нескольких путей сегментации и выбора оптимального результата. SAM2Long поддерживает фиксированное количес
[22.10.2024 22:12] Using data from previous issue: {"desc": "Статья представляет PUMA - новую модель многомодального большого языкового моделирования (MLLM) для генерации визуального контента. PUMA объединяет визуальные признаки разной гранулярности как для входных, так и для выходных данных MLLM, что позволяет решать различные задачи генерации изоб
[22.10.2024 22:12] Using data from previous issue: {"desc": "AutoTrain Advanced - это открытый инструмент для обучения моделей машинного обучения без написания кода. Он поддерживает различные задачи, включая дообучение больших языковых моделей, классификацию текста и изображений, и работу с табличными данными. AutoTrain использует лучшие практики дл
[22.10.2024 22:12] Using data from previous issue: {"desc": "Статья представляет новый полу-контролируемый метод тонкой настройки больших языковых моделей под названием SemiEvol. Этот подход эффективно использует как размеченные, так и неразмеченные данные для адаптации модели к конкретной задаче или домену. SemiEvol применяет двухуровневый метод ра
[22.10.2024 22:12] Using data from previous issue: {"desc": "В статье представлен детальный анализ методов выравнивания (alignment), применяемых в серии моделей Baichuan. Описываются три ключевых этапа: система расширения промптов (PAS), контролируемая тонкая настройка (SFT) и выравнивание предпочтений. Исследуются критические компоненты, улучшающие
[22.10.2024 22:12] Using data from previous issue: {"desc": "Статья представляет Pangea - мультиязычную мультимодальную языковую модель, обученную на разнообразном наборе данных PangeaIns, охватывающем 39 языков. Авторы также вводят PangeaBench - комплексный набор для оценки возможностей моделей на 47 языках. Результаты показывают, что Pangea значит
[22.10.2024 22:12] Using data from previous issue: {"desc": "Статья представляет новый бенчмарк RM-Bench для оценки моделей вознаграждения в контексте обучения с подкреплением по обратной связи от человека (RLHF) и законов масштабирования вывода. RM-Bench оценивает чувствительность моделей к тонким различиям в содержании и их устойчивость к стилисти
[22.10.2024 22:12] Using data from previous issue: {"desc": "Статья представляет концепцию Мета-Чанкинга для улучшения Retrieval-Augmented Generation (RAG). Мета-Чанкинг - это метод разбиения текста на фрагменты, учитывающий лингвистические связи между предложениями. Авторы предлагают две стратегии реализации: Margin Sampling Chunking и Perplexity C
[22.10.2024 22:12] Using data from previous issue: {"desc": "Эта статья исследует применение дистилляции знаний (KD) на этапе предобучения больших языковых моделей (LLM). Авторы проводят эксперименты с использованием GLM-4-9B в качестве учителя для дистилляции студента с 1,9 миллиардами параметров. Они систематически изучают пространство дизайна пре
[22.10.2024 22:12] Using data from previous issue: {"desc": "Статья представляет Alchemy - фреймворк для синтеза данных в области нейронного доказательства теорем (NTP). Авторы предлагают метод символьной мутации для создания новых теорем на основе существующих в библиотеке Mathlib. Это позволяет увеличить объем обучающих данных с 110 тысяч до 6 мил
[22.10.2024 22:12] Using data from previous issue: {"desc": "Ичиго - это мультимодальная языковая модель, способная обрабатывать как речь, так и текст. Модель использует ранний подход к слиянию модальностей, квантуя речь в дискретные токены и применяя единую архитектуру трансформера для обеих модальностей. Ичиго демонстрирует передовые результаты в 
[22.10.2024 22:12] Using data from previous issue: {"desc": "В этой статье исследуется применение больших языковых моделей (LLM) для предсказания динамики непрерывных марковских процессов принятия решений. Авторы предлагают метод разделенного обучения в контексте (DICL) для решения проблем обработки многомерных данных и учета управляющего сигнала. М
[22.10.2024 22:12] Using data from previous issue: {"desc": "Этот научный труд представляет GATEAU - новую систему для улучшения способности больших языковых моделей (LLM) работать с длинными контекстами. GATEAU использует два метода: Homologous Models' Guidance (HMG) для измерения сложности генерации ответов, и Contextual Awareness Measurement (CAM
[22.10.2024 22:12] Using data from previous issue: {"desc": "Представлена система Agent-to-Sim (ATS) для обучения интерактивных моделей поведения 3D-агентов на основе обычных видеоколлекций. ATS использует неинвазивный подход, анализируя естественное поведение животных и людей по видеозаписям, сделанным в течение длительного периода в одной среде. Р
[22.10.2024 22:12] Using data from previous issue: {"desc": "Исследователи представили новый бенчмарк CBT-BENCH для оценки потенциала использования больших языковых моделей в когнитивно-поведенческой терапии (КПТ). Бенчмарк включает три уровня задач: базовые знания КПТ, понимание когнитивной модели и генерацию терапевтических ответов. Эксперименты п
[22.10.2024 22:12] Using data from previous issue: {"desc": "Статья исследует проблему имитации в моделях генерации изображений по тексту. Авторы вводят понятие 'порога имитации' - минимального количества примеров, необходимого модели для воспроизведения определенного концепта. Исследование показало, что для современных моделей этот порог составляет
[22.10.2024 22:12] Using data from previous issue: {"desc": "Статья исследует связь между бритвой Оккама и обучением в контексте в машинном обучении. Авторы показывают, что функция потерь для предсказания следующего токена эквивалентна методу сжатия данных под названием 'прекваентиальное кодирование'. Минимизация этой функции потерь одновременно мин
[22.10.2024 22:12] Using data from previous issue: {"desc": "В статье предлагается метод Router-Tuning для оптимизации вычислительных ресурсов в трансформерных моделях. Авторы разработали подход MindSkip, использующий внимание с динамической глубиной, что позволяет сохранить производительность модели при повышении эффективности. Эксперименты показал
[22.10.2024 22:12] Using data from previous issue: {"desc": "Исследование изучает связь между процессом обучения и возникновением галлюцинаций в больших языковых моделях (БЯМ). Авторы анализируют тренды галлюцинаций на протяжении обучения, используя модели из набора Pythia и несколько метрик обнаружения галлюцинаций. Они представляют новый протокол 
[22.10.2024 22:12] Using data from previous issue: {"desc": "Статья представляет новый фреймворк CIA Suite для многоязычной оценки генеративных языковых моделей. Он включает в себя модель-оценщик Hercule и тестовый набор Recon с 500 аннотированными инструкциями на шести языках. Hercule способна оценивать ответы на целевом языке, используя эталонные 
[22.10.2024 22:12] Using data from previous issue: {"desc": "В статье представлен новый подход к токенизации речи, названный DM-Codec. Он объединяет акустическую, семантическую и контекстную информацию для создания более точных речевых представлений. Авторы предлагают два метода дистилляции: с использованием языковой модели и комбинированный метод с
[22.10.2024 22:12] Loading Chinese text from previous data.
[22.10.2024 22:12] Renaming data file.
[22.10.2024 22:12] Renaming previous data. hf_papers.json to ./d/2024-10-22.json
[22.10.2024 22:12] Saving new data file.
[22.10.2024 22:12] Generating page.
[22.10.2024 22:12] Renaming previous page.
[22.10.2024 22:12] Renaming previous data. index.html to ./d/2024-10-22.html
[22.10.2024 22:12] [Experimental] Generating Chinese page for reading.
[22.10.2024 22:12] Chinese vocab [{'word': '改进', 'pinyin': 'gǎi jìn', 'trans': 'improvement'}, {'word': '分割', 'pinyin': 'fēn gē', 'trans': 'segmentation'}, {'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'}, {'word': '称为', 'pinyin': 'chēng wéi', 'trans': 'called'}, {'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'}, {'word': '积累', 'pinyin': 'jī lěi', 'trans': 'accumulation'}, {'word': '不确定性', 'pinyin': 'bù què dìng xìng', 'trans': 'uncertainty'}, {'word': '路径', 'pinyin': 'lù jìng', 'trans': 'path'}, {'word': '最优', 'pinyin': 'zuì yōu', 'trans': 'optimal'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'excellent'}, {'word': '有效', 'pinyin': 'yǒu xiào', 'trans': 'effective'}, {'word': '跟踪', 'pinyin': 'gēn zōng', 'trans': 'tracking'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'}, {'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'}]
[22.10.2024 22:12] Renaming previous Chinese page.
[22.10.2024 22:12] Renaming previous data. zh.html to ./d/2024-10-21_zh_reading_task.html
[22.10.2024 22:12] Writing result.
[22.10.2024 22:12] Writing Chinese reading task.
[22.10.2024 22:12] Renaming log file.
[22.10.2024 22:12] Renaming previous data. log.txt to ./logs/2024-10-22_last_log.txt
