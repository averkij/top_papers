[02.07.2025 00:56] Read previous papers.
[02.07.2025 00:56] Generating top page (month).
[02.07.2025 00:56] Writing top page (month).
[02.07.2025 02:44] Read previous papers.
[02.07.2025 02:44] Get feed.
[02.07.2025 02:44] Extract page data from URL. URL: https://huggingface.co/papers/2506.23115
[02.07.2025 02:44] Extract page data from URL. URL: https://huggingface.co/papers/2507.00432
[02.07.2025 02:44] Extract page data from URL. URL: https://huggingface.co/papers/2506.21277
[02.07.2025 02:44] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[02.07.2025 02:44] Downloading and parsing papers (pdf, html). Total: 3.
[02.07.2025 02:44] Downloading and parsing paper https://huggingface.co/papers/2506.23115.
[02.07.2025 02:44] Downloading paper 2506.23115 from http://arxiv.org/pdf/2506.23115v1...
[02.07.2025 02:44] Extracting affiliations from text.
[02.07.2025 02:44] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 2 ] . [ 1 5 1 1 3 2 . 6 0 5 2 : r MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings Haonan Chen1, Hong Liu2, Yuping Luo, Liang Wang3, Nan Yang3, Furu Wei3, Zhicheng Dou1 1Gaoling School of Artificial Intelligence, Renmin University of China 2Stanford University 3Microsoft Corporation {hnchen,dou}@ruc.edu.cn hliu99@cs.stanford.edu, yupingl@cs.princeton.edu {wangliang,nanya,fuwei}@microsoft.com https://haon-chen.github.io/MoCa/ "
[02.07.2025 02:44] Response: ```python
[
    "Gaoling School of Artificial Intelligence, Renmin University of China",
    "Stanford University",
    "Microsoft Corporation"
]
```
[02.07.2025 02:44] Deleting PDF ./assets/pdf/2506.23115.pdf.
[02.07.2025 02:44] Success.
[02.07.2025 02:44] Downloading and parsing paper https://huggingface.co/papers/2507.00432.
[02.07.2025 02:44] Downloading paper 2507.00432 from http://arxiv.org/pdf/2507.00432v1...
[02.07.2025 02:47] Extracting affiliations from text.
[02.07.2025 02:47] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Preprint. Work in Progress 1 Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning Maggie Huan1,2,*, Yuetai Li3,, Tuney Zheng4,, Xiaoyu Xu5, Seungone Kim1, Minxin Du5, Radha Poovendran3, Graham Neubig1, Xiang Yue1, 1Carnegie Mellon University 2University of Pennsylvania 3University of Washington 4M-A-P 5The Hong Kong Polytechnic University ziyuh@seas.upenn.edu yuetaili@uw.edu xyue2@andrew.cmu.edu Abstract: Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models. github.com/ReasoningTransfer/Transferability-of-LLM-Reasoning huggingface.co/ReasoningTransferability Figure 1 Impact of SFT and RL using math-only training queries on the same base model, Qwen3-14BBase. Performance improvements are measured relative to the bas"
[02.07.2025 02:47] Response: ```python
[
    "Carnegie Mellon University",
    "University of Pennsylvania",
    "University of Washington",
    "M-A-P",
    "The Hong Kong Polytechnic University"
]
```
[02.07.2025 02:47] Deleting PDF ./assets/pdf/2507.00432.pdf.
[02.07.2025 02:47] Success.
[02.07.2025 02:47] Downloading and parsing paper https://huggingface.co/papers/2506.21277.
[02.07.2025 02:47] Downloading paper 2506.21277 from http://arxiv.org/pdf/2506.21277v1...
[02.07.2025 02:47] Extracting affiliations from text.
[02.07.2025 02:47] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 1 7 7 2 1 2 . 6 0 5 2 : r HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context Qize Yang Shimin Yao Weixuan Chen Shenghao Fu Jiaxing Zhao Boyuan Sun Bowen Yin Xihan Wei Tongyi Lab, Alibaba Group qize.yqz@alibaba-inc.com https://github.com/HumanMLLM/HumanOmniV "
[02.07.2025 02:47] Response: ```python
["Tongyi Lab, Alibaba Group"]
```
[02.07.2025 02:47] Deleting PDF ./assets/pdf/2506.21277.pdf.
[02.07.2025 02:47] Success.
[02.07.2025 02:47] Enriching papers with extra data.
[02.07.2025 02:47] ********************************************************************************
[02.07.2025 02:47] Abstract 0. MoCa, a two-stage framework, enhances pre-trained causal vision-language models for multimodal embedding by introducing bidirectional attention, scaling with unlabeled data, and diverse training objectives.  					AI-generated summary 				 Multimodal embedding models, built upon causal Vision Languag...
[02.07.2025 02:47] ********************************************************************************
[02.07.2025 02:47] Abstract 1. Reinforcement learning-tuned models outperform supervised fine-tuned models in generalizing mathematical problem-solving abilities to other domains, indicating a need to re-evaluate training methods for reasoning models.  					AI-generated summary 				 Math reasoning has become the poster child of p...
[02.07.2025 02:47] ********************************************************************************
[02.07.2025 02:47] Abstract 2. A reinforcement learning-based approach enhances multimodal reasoning by addressing context understanding and shortcut problems, using context, format, accuracy, and logical rewards, and achieving superior performance on the IntentBench benchmark.  					AI-generated summary 				 With the rapid evolu...
[02.07.2025 02:47] Read previous papers.
[02.07.2025 02:47] Generating reviews via LLM API.
[02.07.2025 02:47] Querying the API.
[02.07.2025 02:47] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MoCa, a two-stage framework, enhances pre-trained causal vision-language models for multimodal embedding by introducing bidirectional attention, scaling with unlabeled data, and diverse training objectives.  					AI-generated summary 				 Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.
[02.07.2025 02:47] Response: {
  "desc": "MoCa - это двухэтапная система для улучшения предобученных причинно-следственных визуально-языковых моделей для мультимодального встраивания. Она вводит двунаправленное внимание, масштабирование с помощью неразмеченных данных и разнообразные цели обучения. Первый этап включает совместную реконструкцию для улучшения двунаправленных рассуждений с учетом контекста. Второй этап использует разнообразные семантически богатые мультимодальные данные для улучшения обобщения и выравнивания.",
  "emoji": "🧠",
  "title": "MoCa: революция в мультимодальном встраивании"
}
[02.07.2025 02:47] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MoCa, a two-stage framework, enhances pre-trained causal vision-language models for multimodal embedding by introducing bidirectional attention, scaling with unlabeled data, and diverse training objectives.  					AI-generated summary 				 Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB."

[02.07.2025 02:47] Response: ```python
['MULTIMODAL', 'BENCHMARK', 'DATASET', 'TRAINING', 'ARCHITECTURE']
```
[02.07.2025 02:47] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MoCa, a two-stage framework, enhances pre-trained causal vision-language models for multimodal embedding by introducing bidirectional attention, scaling with unlabeled data, and diverse training objectives.  					AI-generated summary 				 Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB."

[02.07.2025 02:47] Response: ```python
["ALIGNMENT", "OPTIMIZATION"]
```
[02.07.2025 02:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MoCa is a two-stage framework designed to improve pre-trained causal vision-language models for better multimodal embedding. It addresses limitations in current models, such as the inefficiency of causal attention and the need for high-quality labeled data. The first stage focuses on modality-aware continual pre-training, which enhances understanding by denoising both text and image inputs. The second stage employs heterogeneous contrastive fine-tuning, using diverse multimodal data to improve model generalization and alignment, leading to state-of-the-art performance in various benchmarks.","title":"MoCa: Enhancing Multimodal Embedding with Bidirectional Attention"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MoCa is a two-stage framework designed to improve pre-trained causal vision-language models for better multimodal embedding. It addresses limitations in current models, such as the inefficiency of causal attention and the need for high-quality labeled data. The first stage focuses on modality-aware continual pre-training, which enhances understanding by denoising both text and image inputs. The second stage employs heterogeneous contrastive fine-tuning, using diverse multimodal data to improve model generalization and alignment, leading to state-of-the-art performance in various benchmarks.', title='MoCa: Enhancing Multimodal Embedding with Bidirectional Attention'))
[02.07.2025 02:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MoCa是一个两阶段框架，旨在增强预训练的因果视觉语言模型在多模态嵌入中的表现。它通过引入双向注意力机制、利用未标记数据进行扩展以及多样化的训练目标来解决现有方法的局限性。第一阶段通过联合重建目标来提高文本和图像输入的去噪能力，增强双向上下文感知推理。第二阶段则利用丰富的多模态数据进行异构对比微调，从而提高模型的泛化能力和对齐效果。","title":"MoCa：双向多模态嵌入的创新框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MoCa是一个两阶段框架，旨在增强预训练的因果视觉语言模型在多模态嵌入中的表现。它通过引入双向注意力机制、利用未标记数据进行扩展以及多样化的训练目标来解决现有方法的局限性。第一阶段通过联合重建目标来提高文本和图像输入的去噪能力，增强双向上下文感知推理。第二阶段则利用丰富的多模态数据进行异构对比微调，从而提高模型的泛化能力和对齐效果。', title='MoCa：双向多模态嵌入的创新框架'))
[02.07.2025 02:47] Querying the API.
[02.07.2025 02:47] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Reinforcement learning-tuned models outperform supervised fine-tuned models in generalizing mathematical problem-solving abilities to other domains, indicating a need to re-evaluate training methods for reasoning models.  					AI-generated summary 				 Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models.
[02.07.2025 02:47] Response: {
  "desc": "Исследование показывает, что модели, обученные с помощью обучения с подкреплением (RL), лучше обобщают навыки решения математических задач на другие области, чем модели, настроенные с помощью обучения с учителем (SFT). Анализ латентного пространства и распределения токенов выявил, что SFT вызывает значительный сдвиг в представлении и выводе, в то время как RL сохраняет общую структуру. Эксперименты проводились на моделях Qwen3-14B с использованием только математических данных, но разных методов обучения. Результаты указывают на необходимость пересмотра стандартных подходов к обучению моделей рассуждения, особенно в отношении использования данных, полученных с помощью SFT.",
  "emoji": "🧠",
  "title": "Обучение с подкреплением превосходит обучение с учителем в обобщении навыков решения задач"
}
[02.07.2025 02:47] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reinforcement learning-tuned models outperform supervised fine-tuned models in generalizing mathematical problem-solving abilities to other domains, indicating a need to re-evaluate training methods for reasoning models.  					AI-generated summary 				 Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models."

[02.07.2025 02:47] Response: ```python
['RL', 'TRAINING', 'MATH']
```
[02.07.2025 02:47] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Reinforcement learning-tuned models outperform supervised fine-tuned models in generalizing mathematical problem-solving abilities to other domains, indicating a need to re-evaluate training methods for reasoning models.  					AI-generated summary 				 Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models."

[02.07.2025 02:47] Response: ```python
['REASONING', 'TRANSFER_LEARNING', 'OPTIMIZATION']
```
[02.07.2025 02:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates the effectiveness of different training methods for reasoning models, particularly in the context of mathematical problem-solving. It finds that models trained with reinforcement learning (RL) outperform those fine-tuned with supervised learning (SFT) when applied to a variety of tasks beyond mathematics. The study reveals that while SFT models excel in math, they struggle to generalize their skills to other domains due to significant representation drift. In contrast, RL-tuned models maintain their general problem-solving abilities, suggesting a need to reconsider current training approaches for reasoning models.","title":"Rethinking Training: Reinforcement Learning for Better Generalization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates the effectiveness of different training methods for reasoning models, particularly in the context of mathematical problem-solving. It finds that models trained with reinforcement learning (RL) outperform those fine-tuned with supervised learning (SFT) when applied to a variety of tasks beyond mathematics. The study reveals that while SFT models excel in math, they struggle to generalize their skills to other domains due to significant representation drift. In contrast, RL-tuned models maintain their general problem-solving abilities, suggesting a need to reconsider current training approaches for reasoning models.', title='Rethinking Training: Reinforcement Learning for Better Generalization'))
[02.07.2025 02:47] Response: ParsedChatCompletionMessage[Article](content='{"desc":"这篇论文探讨了强化学习（RL）调优模型在数学问题解决能力上的表现，发现其在其他领域的泛化能力优于监督微调（SFT）模型。研究表明，虽然许多模型在数学任务上表现出色，但它们在其他任务上的迁移能力却较差。通过对Qwen3-14B模型的实验，发现RL调优模型能够在多个领域中保持良好的泛化能力，而SFT模型则容易遗忘其通用能力。结果提示我们需要重新审视现有的训练方法，尤其是对SFT数据的依赖。","title":"重新思考推理模型的训练方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='这篇论文探讨了强化学习（RL）调优模型在数学问题解决能力上的表现，发现其在其他领域的泛化能力优于监督微调（SFT）模型。研究表明，虽然许多模型在数学任务上表现出色，但它们在其他任务上的迁移能力却较差。通过对Qwen3-14B模型的实验，发现RL调优模型能够在多个领域中保持良好的泛化能力，而SFT模型则容易遗忘其通用能力。结果提示我们需要重新审视现有的训练方法，尤其是对SFT数据的依赖。', title='重新思考推理模型的训练方法'))
[02.07.2025 02:47] Querying the API.
[02.07.2025 02:47] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A reinforcement learning-based approach enhances multimodal reasoning by addressing context understanding and shortcut problems, using context, format, accuracy, and logical rewards, and achieving superior performance on the IntentBench benchmark.  					AI-generated summary 				 With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models.
[02.07.2025 02:48] Response: {
  "desc": "Предложен подход на основе обучения с подкреплением для улучшения мультимодального рассуждения в больших языковых моделях. Метод решает проблемы понимания контекста и упрощения, используя контекстные, форматные, точностные и логические награды. Авторы вводят новый бенчмарк IntentBench для оценки понимания сложных человеческих намерений и эмоций. Предложенный метод показывает превосходную производительность на IntentBench по сравнению с другими открытыми мультимодальными моделями.",
  "emoji": "🧠",
  "title": "Усиление мультимодального рассуждения с помощью обучения с подкреплением"
}
[02.07.2025 02:48] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A reinforcement learning-based approach enhances multimodal reasoning by addressing context understanding and shortcut problems, using context, format, accuracy, and logical rewards, and achieving superior performance on the IntentBench benchmark.  					AI-generated summary 				 With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models."

[02.07.2025 02:48] Response: ```python
['RL', 'MULTIMODAL', 'BENCHMARK']
```
[02.07.2025 02:48] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A reinforcement learning-based approach enhances multimodal reasoning by addressing context understanding and shortcut problems, using context, format, accuracy, and logical rewards, and achieving superior performance on the IntentBench benchmark.  					AI-generated summary 				 With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models."

[02.07.2025 02:48] Response: ```python
['REASONING', 'GAMES', 'SURVEY']
```
[02.07.2025 02:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a reinforcement learning approach to improve multimodal reasoning in large language models. It addresses two main challenges: insufficient understanding of global context and the shortcut problem, where models fail to consider important multimodal cues. By implementing context, format, accuracy, and logical rewards, the model enhances its reasoning capabilities and interprets multimodal inputs more effectively. The proposed method outperforms existing models on the IntentBench benchmark, showcasing its ability to understand complex human intentions and emotions.","title":"Enhancing Multimodal Reasoning with Reinforcement Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a reinforcement learning approach to improve multimodal reasoning in large language models. It addresses two main challenges: insufficient understanding of global context and the shortcut problem, where models fail to consider important multimodal cues. By implementing context, format, accuracy, and logical rewards, the model enhances its reasoning capabilities and interprets multimodal inputs more effectively. The proposed method outperforms existing models on the IntentBench benchmark, showcasing its ability to understand complex human intentions and emotions.', title='Enhancing Multimodal Reasoning with Reinforcement Learning'))
[02.07.2025 02:48] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本论文提出了一种基于强化学习的方法，以增强多模态推理能力，解决上下文理解和捷径问题。我们发现现有多模态推理模型存在全球上下文理解不足和捷径问题，这会导致模型错误解读多模态信息。为了解决这些问题，我们强调模型需要在多模态输入中清晰理解全球上下文，并通过上下文奖励、格式奖励和准确性奖励来确保对多模态信息的准确解读。我们的研究在IntentBench基准测试中表现优异，展示了在理解复杂人类意图和情感方面的先进性能。","title":"强化学习提升多模态推理能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本论文提出了一种基于强化学习的方法，以增强多模态推理能力，解决上下文理解和捷径问题。我们发现现有多模态推理模型存在全球上下文理解不足和捷径问题，这会导致模型错误解读多模态信息。为了解决这些问题，我们强调模型需要在多模态输入中清晰理解全球上下文，并通过上下文奖励、格式奖励和准确性奖励来确保对多模态信息的准确解读。我们的研究在IntentBench基准测试中表现优异，展示了在理解复杂人类意图和情感方面的先进性能。', title='强化学习提升多模态推理能力'))
[02.07.2025 02:48] Renaming data file.
[02.07.2025 02:48] Renaming previous data. hf_papers.json to ./d/2025-07-02.json
[02.07.2025 02:48] Saving new data file.
[02.07.2025 02:48] Generating page.
[02.07.2025 02:48] Renaming previous page.
[02.07.2025 02:48] Renaming previous data. index.html to ./d/2025-07-02.html
[02.07.2025 02:48] Writing result.
[02.07.2025 02:48] Renaming log file.
[02.07.2025 02:48] Renaming previous data. log.txt to ./logs/2025-07-02_last_log.txt
