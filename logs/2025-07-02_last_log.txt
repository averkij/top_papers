[02.07.2025 14:12] Read previous papers.
[02.07.2025 14:12] Generating top page (month).
[02.07.2025 14:12] Writing top page (month).
[02.07.2025 15:12] Read previous papers.
[02.07.2025 15:12] Get feed.
[02.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01006
[02.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23115
[02.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.01001
[02.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.00432
[02.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.19852
[02.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.20639
[02.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21277
[02.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.00951
[02.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.21545
[02.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2507.00162
[02.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.23329
[02.07.2025 15:12] Get page data from previous paper. URL: https://huggingface.co/papers/2506.22960
[02.07.2025 15:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[02.07.2025 15:12] No deleted papers detected.
[02.07.2025 15:12] Downloading and parsing papers (pdf, html). Total: 12.
[02.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2507.01006.
[02.07.2025 15:12] Extra JSON file exists (./assets/json/2507.01006.json), skip PDF parsing.
[02.07.2025 15:12] Paper image links file exists (./assets/img_data/2507.01006.json), skip HTML parsing.
[02.07.2025 15:12] Success.
[02.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.23115.
[02.07.2025 15:12] Extra JSON file exists (./assets/json/2506.23115.json), skip PDF parsing.
[02.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.23115.json), skip HTML parsing.
[02.07.2025 15:12] Success.
[02.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2507.01001.
[02.07.2025 15:12] Extra JSON file exists (./assets/json/2507.01001.json), skip PDF parsing.
[02.07.2025 15:12] Paper image links file exists (./assets/img_data/2507.01001.json), skip HTML parsing.
[02.07.2025 15:12] Success.
[02.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2507.00432.
[02.07.2025 15:12] Extra JSON file exists (./assets/json/2507.00432.json), skip PDF parsing.
[02.07.2025 15:12] Paper image links file exists (./assets/img_data/2507.00432.json), skip HTML parsing.
[02.07.2025 15:12] Success.
[02.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.19852.
[02.07.2025 15:12] Extra JSON file exists (./assets/json/2506.19852.json), skip PDF parsing.
[02.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.19852.json), skip HTML parsing.
[02.07.2025 15:12] Success.
[02.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.20639.
[02.07.2025 15:12] Extra JSON file exists (./assets/json/2506.20639.json), skip PDF parsing.
[02.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.20639.json), skip HTML parsing.
[02.07.2025 15:12] Success.
[02.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.21277.
[02.07.2025 15:12] Extra JSON file exists (./assets/json/2506.21277.json), skip PDF parsing.
[02.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.21277.json), skip HTML parsing.
[02.07.2025 15:12] Success.
[02.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2507.00951.
[02.07.2025 15:12] Extra JSON file exists (./assets/json/2507.00951.json), skip PDF parsing.
[02.07.2025 15:12] Paper image links file exists (./assets/img_data/2507.00951.json), skip HTML parsing.
[02.07.2025 15:12] Success.
[02.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.21545.
[02.07.2025 15:12] Extra JSON file exists (./assets/json/2506.21545.json), skip PDF parsing.
[02.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.21545.json), skip HTML parsing.
[02.07.2025 15:12] Success.
[02.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2507.00162.
[02.07.2025 15:12] Extra JSON file exists (./assets/json/2507.00162.json), skip PDF parsing.
[02.07.2025 15:12] Paper image links file exists (./assets/img_data/2507.00162.json), skip HTML parsing.
[02.07.2025 15:12] Success.
[02.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.23329.
[02.07.2025 15:12] Extra JSON file exists (./assets/json/2506.23329.json), skip PDF parsing.
[02.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.23329.json), skip HTML parsing.
[02.07.2025 15:12] Success.
[02.07.2025 15:12] Downloading and parsing paper https://huggingface.co/papers/2506.22960.
[02.07.2025 15:12] Extra JSON file exists (./assets/json/2506.22960.json), skip PDF parsing.
[02.07.2025 15:12] Paper image links file exists (./assets/img_data/2506.22960.json), skip HTML parsing.
[02.07.2025 15:12] Success.
[02.07.2025 15:12] Enriching papers with extra data.
[02.07.2025 15:12] ********************************************************************************
[02.07.2025 15:12] Abstract 0. A vision-language model, GLM-4.1V-Thinking, enhances general-purpose multimodal reasoning through large-scale pre-training and reinforcement learning, achieving state-of-the-art performance across various tasks.  					AI-generated summary 				 We present GLM-4.1V-Thinking, a vision-language model (V...
[02.07.2025 15:12] ********************************************************************************
[02.07.2025 15:12] Abstract 1. MoCa, a two-stage framework, enhances pre-trained causal vision-language models for multimodal embedding by introducing bidirectional attention, scaling with unlabeled data, and diverse training objectives.  					AI-generated summary 				 Multimodal embedding models, built upon causal Vision Languag...
[02.07.2025 15:12] ********************************************************************************
[02.07.2025 15:12] Abstract 2. SciArena is a community-driven platform for evaluating foundation models on scientific literature tasks, using collective voter judgments to rank models and address the need for reliable automated evaluation.  					AI-generated summary 				 We present SciArena, an open and collaborative platform for...
[02.07.2025 15:12] ********************************************************************************
[02.07.2025 15:12] Abstract 3. Reinforcement learning-tuned models outperform supervised fine-tuned models in generalizing mathematical problem-solving abilities to other domains, indicating a need to re-evaluate training methods for reasoning models.  					AI-generated summary 				 Math reasoning has become the poster child of p...
[02.07.2025 15:12] ********************************************************************************
[02.07.2025 15:12] Abstract 4. Radial Attention, a scalable sparse attention mechanism, improves efficiency and preserves video quality in diffusion models by leveraging spatiotemporal energy decay.  					AI-generated summary 				 Recent advances in diffusion models have enabled high-quality video generation, but the additional t...
[02.07.2025 15:12] ********************************************************************************
[02.07.2025 15:12] Abstract 5. Diffusion large language models are applied to code generation, revealing their unique denoising processes and benefiting from a novel reinforcement learning sampling scheme.  					AI-generated summary 				 Diffusion large language models (dLLMs) are compelling alternatives to autoregressive (AR) mo...
[02.07.2025 15:12] ********************************************************************************
[02.07.2025 15:12] Abstract 6. A reinforcement learning-based approach enhances multimodal reasoning by addressing context understanding and shortcut problems, using context, format, accuracy, and logical rewards, and achieving superior performance on the IntentBench benchmark.  					AI-generated summary 				 With the rapid evolu...
[02.07.2025 15:12] ********************************************************************************
[02.07.2025 15:12] Abstract 7. The paper synthesizes the interdisciplinary approach to achieving Artificial General Intelligence, emphasizing modular reasoning, memory, multi-agent coordination, and the integration of neurosymbolic systems and reinforcement learning to overcome current model limitations.  					AI-generated summar...
[02.07.2025 15:12] ********************************************************************************
[02.07.2025 15:12] Abstract 8. DELT, a paradigm for enhancing language model performance through data efficacy, consists of data scoring, selection, and ordering, demonstrating significant improvements without increasing data scale or model size.  					AI-generated summary 				 Data is fundamental to the training of language mode...
[02.07.2025 15:12] ********************************************************************************
[02.07.2025 15:12] Abstract 9. Recent advances in video generation models have enabled high-quality short video generation from text prompts. However, extending these models to longer videos remains a significant challenge, primarily due to degraded temporal consistency and visual fidelity. Our preliminary observations show that ...
[02.07.2025 15:12] ********************************************************************************
[02.07.2025 15:12] Abstract 10. Vision-language models (VLMs) excel at descriptive tasks, but whether they truly understand scenes from visual observations remains uncertain. We introduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding through active creation rather than passive recognition. Grounded in the a...
[02.07.2025 15:12] ********************************************************************************
[02.07.2025 15:12] Abstract 11. PECCAVI is a robust image watermarking technique that is resistant to visual paraphrase attacks and distortions, utilizing NMPs and multi-channel frequency domain watermarking.  					AI-generated summary 				 A report by the European Union Law Enforcement Agency predicts that by 2026, up to 90 perce...
[02.07.2025 15:12] Read previous papers.
[02.07.2025 15:12] Generating reviews via LLM API.
[02.07.2025 15:12] Using data from previous issue: {"categories": ["#open_source", "#rl", "#architecture", "#reasoning", "#multimodal", "#benchmark", "#training"], "emoji": "üß†", "ru": {"title": "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å GLM-4.1V-Thinking - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω
[02.07.2025 15:12] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#training", "#optimization", "#multimodal", "#alignment", "#architecture"], "emoji": "üß†", "ru": {"title": "MoCa: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–∏", "desc": "MoCa - —ç—Ç–æ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑
[02.07.2025 15:12] Using data from previous issue: {"categories": ["#open_source", "#science", "#benchmark", "#survey", "#dataset"], "emoji": "üß™", "ru": {"title": "–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑—É–º –≤ –æ—Ü–µ–Ω–∫–µ –ò–ò –¥–ª—è –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã", "desc": "SciArena - —ç—Ç–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞–±–æ—Ç—ã —Å –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∫–æ–ª–ª–µ–∫—Ç–∏–≤
[02.07.2025 15:12] Using data from previous issue: {"categories": ["#reasoning", "#training", "#rl", "#transfer_learning", "#optimization", "#math"], "emoji": "üß†", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –≤ –æ–±–æ–±—â–µ–Ω–∏–∏ –Ω–∞–≤—ã–∫–æ–≤ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å 
[02.07.2025 15:12] Using data from previous issue: {"categories": ["#video", "#optimization", "#diffusion", "#architecture", "#inference", "#training"], "emoji": "üé•", "ru": {"title": "–†–∞–¥–∏–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–†–∞–¥–∏–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ' –¥–ª—è 
[02.07.2025 15:12] Using data from previous issue: {"categories": ["#training", "#rl", "#architecture", "#optimization", "#dataset", "#diffusion"], "emoji": "üß†", "ru": {"title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (dLLM) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã
[02.07.2025 15:12] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#games", "#rl", "#multimodal", "#survey"], "emoji": "üß†", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ 
[02.07.2025 15:12] Using data from previous issue: {"categories": ["#agents", "#agi", "#rl", "#architecture", "#multimodal", "#rag", "#ethics", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ü—É—Ç—å –∫ AGI: –æ–±—ä–µ–¥–∏–Ω—è—è –º–æ–¥—É–ª—å–Ω–æ—Å—Ç—å, –ø–∞–º—è—Ç—å –∏ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ—Å—Ç—å", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ–∂–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—â–µ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç
[02.07.2025 15:12] Using data from previous issue: {"categories": ["#optimization", "#training", "#data"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö - –∫–ª—é—á –∫ —É–ª—É—á—à–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "DELT - —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∞ –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ –∫–æ–º
[02.07.2025 15:12] Using data from previous issue: {"categories": ["#games", "#optimization", "#video"], "emoji": "üé¨", "ru": {"title": "FreeLong: –ü—Ä–æ—Ä—ã–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FreeLong –∏ FreeLong++, –Ω–æ–≤—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤—ã—è
[02.07.2025 15:12] Using data from previous issue: {"categories": ["#games", "#benchmark", "#multimodal", "#cv", "#interpretability"], "emoji": "üé®", "ru": {"title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã —á–µ—Ä–µ–∑ –µ—ë –∞–∫—Ç–∏–≤–Ω–æ–µ –≤–æ—Å—Å–æ–∑–¥–∞–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç IR3D-Bench - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å—Ü–µ–Ω –º–æ–¥–µ–ª—è–º–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ (VLM). –í –æ—Ç–ª–∏—á–∏–µ
[02.07.2025 15:12] Using data from previous issue: {"categories": ["#security", "#synthetic", "#data", "#open_source", "#multimodal", "#cv"], "emoji": "üîê", "ru": {"title": "–ù–µ–ø–æ–±–µ–¥–∏–º—ã–µ –≤–æ–¥—è–Ω—ã–µ –∑–Ω–∞–∫–∏ –¥–ª—è —ç–ø–æ—Ö–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò", "desc": "PECCAVI - —ç—Ç–æ —É—Å—Ç–æ–π—á–∏–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –≤–æ–¥—è–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ—Ç–∏–≤–æ—Å—Ç–æ–∏—Ç –∞—Ç–∞–∫–∞–º –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏
[02.07.2025 15:12] Renaming data file.
[02.07.2025 15:12] Renaming previous data. hf_papers.json to ./d/2025-07-02.json
[02.07.2025 15:12] Saving new data file.
[02.07.2025 15:12] Generating page.
[02.07.2025 15:12] Renaming previous page.
[02.07.2025 15:12] Renaming previous data. index.html to ./d/2025-07-02.html
[02.07.2025 15:12] Writing result.
[02.07.2025 15:12] Renaming log file.
[02.07.2025 15:12] Renaming previous data. log.txt to ./logs/2025-07-02_last_log.txt
