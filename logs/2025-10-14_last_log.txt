[14.10.2025 08:14] Read previous papers.
[14.10.2025 08:14] Generating top page (month).
[14.10.2025 08:14] Writing top page (month).
[14.10.2025 09:13] Read previous papers.
[14.10.2025 09:13] Get feed.
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11696
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11690
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10689
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09285
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10395
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10201
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11712
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11701
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09781
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04617
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11652
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11026
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10670
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08886
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11027
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09541
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11718
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11391
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10666
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10197
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09008
[14.10.2025 09:13] Extract page data from URL. URL: https://huggingface.co/papers/2510.11341
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10637
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10023
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11498
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08026
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07841
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10868
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09905
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11650
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10062
[14.10.2025 09:13] Extract page data from URL. URL: https://huggingface.co/papers/2510.09212
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09189
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11512
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10681
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08744
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05213
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11713
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11647
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10493
[14.10.2025 09:13] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04587
[14.10.2025 09:13] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.10.2025 09:13] No deleted papers detected.
[14.10.2025 09:13] Downloading and parsing papers (pdf, html). Total: 41.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.11696.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.11696.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.11696.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.11690.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.11690.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.11690.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.10689.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.10689.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.10689.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.09285.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.09285.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.09285.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.10395.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.10395.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.10395.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.10201.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.10201.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.10201.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.11712.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.11712.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.11712.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.11701.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.11701.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.11701.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.09781.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.09781.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.09781.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.04617.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.04617.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.04617.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.11652.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.11652.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.11652.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.11026.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.11026.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.11026.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.10670.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.10670.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.10670.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.08886.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.08886.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.08886.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.11027.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.11027.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.11027.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.09541.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.09541.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.09541.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.11718.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.11718.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.11718.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.11391.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.11391.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.11391.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.10666.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.10666.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.10666.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.10197.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.10197.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.10197.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.09008.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.09008.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.09008.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.11341.
[14.10.2025 09:13] Downloading paper 2510.11341 from http://arxiv.org/pdf/2510.11341v1...
[14.10.2025 09:13] Extracting affiliations from text.
[14.10.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"INTERNSVG: TOWARDS UNIFIED SVG TASKS WITH MULTIMODAL LARGE LANGUAGE MODELS Haomin Wang1,2 Jinhui Yin3,2 Qi Wei3,2 Wenguang Zeng4 Lixin Gu2 Shenglong Ye2 Zhangwei Gao1,2 Yaohui Wang2 Yanting Zhang4 Yuanqi Li3 Yanwen Guo3 Wenhai Wang5 Kai Chen2 Yu Qiao2 Hongjie Zhang2 1 Shanghai Jiao Tong University 4 Donghua University Project Page: https://hmwang2002.github.io/release/internsvg 5 The Chinese University of Hong Kong 2 Shanghai AI Laboratory 3 Nanjing University 5 2 0 O 3 1 ] . [ 1 1 4 3 1 1 . 0 1 5 2 : r Figure 1: Overview of our InternSVG family. SAgoge provides large-scale and diverse SVG samples across multiple domains. SArena enables comprehensive assessment of existing MLLMs on SVG tasks. InternSVG supports unified modeling for SVG understanding, editing, and generation. "
[14.10.2025 09:13] Response: ```python
[
    "Shanghai Jiao Tong University",
    "Donghua University",
    "The Chinese University of Hong Kong",
    "Shanghai AI Laboratory",
    "Nanjing University"
]
```
[14.10.2025 09:13] Deleting PDF ./assets/pdf/2510.11341.pdf.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.10637.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.10637.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.10637.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.10023.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.10023.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.10023.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.11498.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.11498.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.11498.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.08026.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.08026.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.08026.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.07841.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.07841.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.07841.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.10868.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.10868.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.10868.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.09905.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.09905.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.09905.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.11650.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.11650.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.11650.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.10062.
[14.10.2025 09:13] Extra JSON file exists (./assets/json/2510.10062.json), skip PDF parsing.
[14.10.2025 09:13] Paper image links file exists (./assets/img_data/2510.10062.json), skip HTML parsing.
[14.10.2025 09:13] Success.
[14.10.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2510.09212.
[14.10.2025 09:13] Downloading paper 2510.09212 from http://arxiv.org/pdf/2510.09212v1...
[14.10.2025 09:14] Extracting affiliations from text.
[14.10.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 1 ] . [ 1 2 1 2 9 0 . 0 1 5 2 : r a STABLE VIDEO INFINITY: GENERATION WITH ERROR RECYCLING INFINITE-LENGTH VIDEO Wuyang Li Wentao Pan Po-Chien Luan Yang Gao Alexandre Alahi VITA@EPFL Project Page: https://stable-video-infinity.github.io/homepage/ "
[14.10.2025 09:14] Response: ```python
[]
```
[14.10.2025 09:14] Extracting affiliations from text.
[14.10.2025 09:14] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 1 ] . [ 1 2 1 2 9 0 . 0 1 5 2 : r aSTABLE VIDEO INFINITY: GENERATION WITH ERROR RECYCLING INFINITE-LENGTH VIDEO Wuyang Li Wentao Pan Po-Chien Luan Yang Gao Alexandre Alahi VITA@EPFL Project Page: https://stable-video-infinity.github.io/homepage/We propose Stable Video Infinity (SVI) that is able to generate infinite-length videos with high temporal consistency, plausible scene transitions, and controllable streaming storylines. While existing long-video methods attempt to mitigate accumulated errors via handcrafted anti-drifting (e.g., modified noise scheduler, frame anchoring), they remain limited to single-prompt extrapolation, producing homogeneous scenes with repetitive motions. We identify that the fundamental challenge extends beyond error accumulation to critical discrepancy between the training assumption (seeing clean data) and the test-time autoregressive reality (conditioning on self-generated, error-prone outputs). To bridge this hypothesis gap, SVI incorporates Error-Recycling Fine-Tuning, new type of efficient training that recycles the Diffusion Transformer (DiT)s self-generated errors into supervisory prompts, thereby encouraging DiT to actively identify and correct its own errors. This is achieved by injecting, collecting, and banking errors through closed-loop recycling, autoregressively learning from error-injected feedback. Specifically, we (i) inject historical errors made by DiT to intervene on clean inputs, simulating error-accumulated trajectories in flow matching; (ii) efficiently approximate predictions with one-step bidirectional integration and calculate errors with residuals; (iii) dynamically bank errors into replay memory across discretized timesteps, which are resampled for new input. SVI is able to scale videos from seconds to infinite durations with no additional inference cost, while remaining compatible with diverse conditions (e.g., audio, skeleton, and text streams). We evaluate SVI on three benchmarks, including consistent, creative, and conditional settings, thoroughly verifying its versatility and state-of-the-art role.Failure is simply the opportunity to begin again, this time more intelligently. HENRY FORD With the scaling of models and data, the video Diffusion Transformer (DiT) (Wang et al., 2025a; Kong et al., 2024; Liu et al., 2024; Hong et al., 2023) has made great strides in synthesizing realistic, temporally coherent videos, supporting in-the-wild content creation. While achieving great success, this community suffers from limited video length, typically 5 seconds (Wang et al., 2025a). This is mainly caused by the open challenge of error accumulation, a.k.a., drifting: after autoregressively conditioning on the previously generated, predictive errors will compound over time, leading to progressive degradation in image fidelity, motion stability, and semantic controllability (Fig. 1.a). In this context, existing solutions can be divided into three trends, including (i) noise modification, augmenting and modifying the noise schedule to reduce the past-frame dependency (Chen et al., 2024; Ruhe et al., 2024), (ii) frame anchoring using the error-free reference image (Henschel et al., 2025) as anchors to reduce the dependency of error-included ones, and (iii) improved sampling like masked-noise guidance (Song et al., 2025) and anti-drifting sampling (Zhang & Agrawala, 2025). However, existing methods primarily aim to alleviate rather than correct accumulated errors, which leads to two key limitations: constrained length (generally 10 seconds up to about 1 minute) and the scene homogeneity bias with repetitive motion. Practically, most methods essentially extrapolate the original clips controlled by single prompt, rather than creating truly long-form videosFigure 1: Comparison among (a) video generative DiT, (b) restoration DiT, and (c) our Stable Video Infinity regarding the scheme (row 1), training-test hypothesis gap (row 2), and outcome (row 3). that the prompt stream storylines can easily control. Consequently, current solutions do not satisfy many creative real-world demands, such as short-form filming that requires plausible, frequent scene changes or creation of hour-scale online presentations. To tackle this, we aim to treat the cause rather than the symptom, seeking to fundamentally correct accumulated error itself rather than merely alleviating its effects. By observing the artifacts caused by errors (see Fig. 6), we empirically find that they closely align with common degradation types, such as blur and color shift within the image restoration community. Given the state-of-the-art role of DiT in low-level vision, these degradations should not be difficult for much larger video DiT (e.g., 14B) with more substantial capacity. Surprisingly, the opposite holds in practice: Why are these powerful models highly susceptible to such errors, leading to severe and rapid collapse? We uncover that the fundamental challenge lies in the hypothesis gap between the training and test. In training generative DiT (Fig. 1a), historical trajectories of flow matching are assumed to be error-free. However, this is easily broken in test since the model autoregressively uses previous generations with predictive errors, which is mathematically clarified in Sec. 3. In contrast, for restoration DiT (Fig. 1b), both training and test assume error-injected inputs, ensuring error robustness. Hence, to bridge this gap, we start new perspective: recycling self-generated errors as supervisory prompts, encouraging DiT to autoregressively correct its own mistakes via error feedback. In this work, we propose Stable Video Infinity (SVI) that can generate infinite-length videos with temporally coherent and visually plausible context following long storyline. In Fig. 1c, SVI employs novel Error-Recycling Fine-Tuning that repurposes the DiTs self-generated errors as supervisory signals, thereby enabling the model to iteratively refine its outputs through autoregressive error feedback. Specifically, we (i) intervene on clean input by injecting historical errors to simulate degradation, (ii) approximate the predictions and calculate errors via one-step integration bidirectionally, and (iii) dynamically save and selectively resample errors across discretized timesteps from replay memory. By doing so, we can efficiently unleash the restoration ability in video DiT, actively correcting errors in generation. Additionally, SVI has several emergent advantages ov"
[14.10.2025 09:14] Mistral response. {"id": "c06548e17302420ebeafa0fb1f848f9f", "created": 1760433241, "model": "mistral-large-latest", "usage": {"prompt_tokens": 1426, "total_tokens": 1438, "completion_tokens": 12}, "object": "chat.completion", "choices": [{"index": 0, "finish_reason": "stop", "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"VITA@EPFL\"]\n```"}}]}
[14.10.2025 09:14] Response: ```python
["VITA@EPFL"]
```
[14.10.2025 09:14] Deleting PDF ./assets/pdf/2510.09212.pdf.
[14.10.2025 09:14] Success.
[14.10.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2510.09189.
[14.10.2025 09:14] Extra JSON file exists (./assets/json/2510.09189.json), skip PDF parsing.
[14.10.2025 09:14] Paper image links file exists (./assets/img_data/2510.09189.json), skip HTML parsing.
[14.10.2025 09:14] Success.
[14.10.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2510.11512.
[14.10.2025 09:14] Extra JSON file exists (./assets/json/2510.11512.json), skip PDF parsing.
[14.10.2025 09:14] Paper image links file exists (./assets/img_data/2510.11512.json), skip HTML parsing.
[14.10.2025 09:14] Success.
[14.10.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2510.10681.
[14.10.2025 09:14] Extra JSON file exists (./assets/json/2510.10681.json), skip PDF parsing.
[14.10.2025 09:14] Paper image links file exists (./assets/img_data/2510.10681.json), skip HTML parsing.
[14.10.2025 09:14] Success.
[14.10.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2510.08744.
[14.10.2025 09:14] Extra JSON file exists (./assets/json/2510.08744.json), skip PDF parsing.
[14.10.2025 09:14] Paper image links file exists (./assets/img_data/2510.08744.json), skip HTML parsing.
[14.10.2025 09:14] Success.
[14.10.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2510.05213.
[14.10.2025 09:14] Extra JSON file exists (./assets/json/2510.05213.json), skip PDF parsing.
[14.10.2025 09:14] Paper image links file exists (./assets/img_data/2510.05213.json), skip HTML parsing.
[14.10.2025 09:14] Success.
[14.10.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2510.11713.
[14.10.2025 09:14] Extra JSON file exists (./assets/json/2510.11713.json), skip PDF parsing.
[14.10.2025 09:14] Paper image links file exists (./assets/img_data/2510.11713.json), skip HTML parsing.
[14.10.2025 09:14] Success.
[14.10.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2510.11647.
[14.10.2025 09:14] Extra JSON file exists (./assets/json/2510.11647.json), skip PDF parsing.
[14.10.2025 09:14] Paper image links file exists (./assets/img_data/2510.11647.json), skip HTML parsing.
[14.10.2025 09:14] Success.
[14.10.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2510.10493.
[14.10.2025 09:14] Extra JSON file exists (./assets/json/2510.10493.json), skip PDF parsing.
[14.10.2025 09:14] Paper image links file exists (./assets/img_data/2510.10493.json), skip HTML parsing.
[14.10.2025 09:14] Success.
[14.10.2025 09:14] Downloading and parsing paper https://huggingface.co/papers/2510.04587.
[14.10.2025 09:14] Extra JSON file exists (./assets/json/2510.04587.json), skip PDF parsing.
[14.10.2025 09:14] Paper image links file exists (./assets/img_data/2510.04587.json), skip HTML parsing.
[14.10.2025 09:14] Success.
[14.10.2025 09:14] Enriching papers with extra data.
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 0. QeRL, a quantization-enhanced reinforcement learning framework, accelerates RL training for large language models by combining NVFP4 quantization with Low-Rank Adaptation and an Adaptive Quantization Noise mechanism, achieving significant speedups and improved performance.  					AI-generated summary...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 1. Replacing VAEs with pretrained representation encoders in Diffusion Transformers enhances generative quality and convergence speed without auxiliary losses.  					AI-generated summary 				 Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion pr...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 2. OmniVideoBench is a comprehensive benchmark for evaluating audio-visual reasoning in multimodal large language models, addressing modality complementarity and logical consistency.  					AI-generated summary 				 Recent advances in multimodal large language models (MLLMs) have demonstrated substantia...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 3. VPPO, a novel policy gradient algorithm, enhances multimodal RLVR by leveraging token perception to refine learning signals and improve reasoning capabilities in Large Vision-Language Models.  					AI-generated summary 				 While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 4. AVoCaDO, an audiovisual video captioner, enhances temporal coherence and dialogue accuracy through a two-stage post-training pipeline, outperforming existing models across multiple benchmarks.  					AI-generated summary 				 Audiovisual video captioning aims to generate semantically rich description...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 5. RLFR uses flow rewards derived from latent space to improve reinforcement learning with verifiable rewards, demonstrating reliable reward shaping and efficient context comprehension.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a promi...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 6. DiT360 framework enhances panoramic image generation by hybrid training on perspective and panoramic data, incorporating cross-domain knowledge and hybrid supervision to improve boundary consistency and image fidelity.  					AI-generated summary 				 In this work, we propose DiT360, a DiT-based fram...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 7. Agentic reinforcement learning enhances LLMs' reasoning ability through real datasets, exploration techniques, and a deliberative strategy, achieving strong performance with smaller models.  					AI-generated summary 				 Recently, the emergence of agentic RL has showcased that RL could also effecti...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 8. AuraGen and Safiron address pre-execution safety gaps in LLM agents by synthesizing benign trajectories, injecting risks, and using a cross-planner adapter for robust risk detection and explanation.  					AI-generated summary 				 While LLM agents can plan multi-step tasks, intervening at the planni...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 9. AdaR framework enhances LLMs' robustness and generalization in mathematical reasoning by synthesizing logically equivalent queries and using RLVR to penalize spurious logic.  					AI-generated summary 				 Mathematical reasoning is a primary indicator of large language models (LLMs) intelligence. Ho...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 10. The Acadreason benchmark evaluates LLMs and agents on high-level academic reasoning across multiple domains, revealing significant capability gaps.  					AI-generated summary 				 In recent years, the research focus of large language models (LLMs) and agents has shifted increasingly from demonstrati...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 11. GIR-Bench evaluates unified multimodal models across understanding-generation consistency, reasoning-centric text-to-image generation, and multi-step reasoning in editing, highlighting gaps in their capabilities.  					AI-generated summary 				 Unified multimodal models integrate the reasoning capac...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 12. A two-stage paradigm adapts pre-trained Text-to-Video models for viewpoint prediction in 4D scenes by integrating an adaptive learning branch and a camera extrinsic diffusion branch.  					AI-generated summary 				 Recent Text-to-Video (T2V) models have demonstrated powerful capability in visual sim...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 13. FinAuditing is a benchmark for evaluating LLMs on structured financial auditing tasks, revealing their limitations in handling taxonomy-driven, hierarchical financial documents.  					AI-generated summary 				 The complexity of the Generally Accepted Accounting Principles (GAAP) and the hierarchical...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 14. Vlaser, a Vision-Language-Action Model, integrates high-level reasoning with low-level control for embodied agents, achieving state-of-the-art performance in embodied reasoning tasks and competitive results in robot benchmarks.  					AI-generated summary 				 While significant research has focused o...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 15. The Sandwiched Policy Gradient method improves reinforcement learning for diffusion large language models by using both upper and lower bounds of log-likelihood, outperforming ELBO-based methods.  					AI-generated summary 				 Diffusion large language models (dLLMs) are emerging as an efficient alt...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 16. CodePlot-CoT, a code-driven Chain-of-Thought model, enhances multimodal mathematical reasoning by generating both text and executable plotting code to solve problems requiring visual assistance.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) and Vision Language Model...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 17. DocReward, a document reward model, evaluates and enhances the structural and stylistic quality of generated documents, outperforming GPT-4o and GPT-5 in both accuracy and human-preferred document generation.  					AI-generated summary 				 Recent advances in agentic workflows have enabled the autom...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 18. BrowserAgent, an interactive web agent using human-like browser actions and a two-stage training process, achieves competitive results in Open-QA tasks with less training data and improved reasoning for multi-hop QA.  					AI-generated summary 				 Efficiently solving real-world problems with LLMs i...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 19. Environment Tuning enables LLM agents to learn complex behaviors from problem instances using a structured curriculum, environment augmentation, and progress rewards, achieving competitive in-distribution performance and superior out-of-distribution generalization.  					AI-generated summary 				 La...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 20. A method to reduce object hallucinations in large vision-language models by identifying and masking uncertain visual tokens in the vision encoder.  					AI-generated summary 				 Large vision-language models (LVLMs), which integrate a vision encoder (VE) with a large language model, have achieved re...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 21. A unified multimodal large language model (MLLM) for SVG understanding, editing, and generation leverages a comprehensive dataset and benchmark to achieve superior performance across various tasks.  					AI-generated summary 				 General SVG modeling remains challenging due to fragmented datasets, l...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 22. RoboSimGS, a Real2Sim2Real framework, uses 3D Gaussian Splatting and mesh primitives to create scalable, high-fidelity, and physically interactive simulation environments, enabling successful zero-shot sim-to-real transfer for robotic manipulation tasks.  					AI-generated summary 				 The scalabili...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 23. A new fine-tuning strategy, STAT, uses a teacher model's metacognition to identify and address skill gaps in a student model, leading to improved performance on both in-distribution and out-of-distribution benchmarks.  					AI-generated summary 				 Language models often show little to no improvemen...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 24. ReLook, a vision-grounded reinforcement learning framework, enhances front-end code generation by integrating a multimodal LLM for visual feedback and forced optimization, outperforming existing methods.  					AI-generated summary 				 While Large Language Models (LLMs) excel at algorithmic code gen...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 25. A reward mechanism called Phase Entropy Aware Reward (PEAR) controls the length of reasoning in large models by adjusting entropy at different stages, balancing conciseness and accuracy.  					AI-generated summary 				 Large Reasoning Models (LRMs) have achieved impressive performance on complex rea...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 26. A test-time self-improvement method enhances language models by generating additional training examples from uncertain cases, leading to better performance with fewer samples.  					AI-generated summary 				 One paradigm of language model (LM) fine-tuning relies on creating large training datasets, ...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 27. Two merging strategies and a diffusion-based decoder improve 3D Human Mesh Recovery by reducing computational cost and slightly enhancing performance.  					AI-generated summary 				 Recent transformer-based models for 3D Human Mesh Recovery (HMR) have achieved strong performance but often suffer fr...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 28. LLMs exhibit systematic biases in emotional interpretation and support based on user profiles, potentially reinforcing social hierarchies.  					AI-generated summary 				 When an AI assistant remembers that Sarah is a single mother working two jobs, does it interpret her stress differently than if s...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 29. InfiniHuman framework distills existing models to generate large-scale, richly annotated 3D human data using a diffusion-based generative pipeline, achieving high visual quality, speed, and controllability.  					AI-generated summary 				 Generating realistic and controllable 3D human avatars is a l...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 30. HUME provides human performance baselines for text embedding tasks, enhancing the interpretability of model evaluations and revealing dataset and language-specific challenges.  					AI-generated summary 				 Comparing human and model performance offers a valuable perspective for understanding the st...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 31. Stable Video Infinity generates infinite-length videos with high temporal consistency and controllable storylines by using Error-Recycling Fine-Tuning on the Diffusion Transformer.  					AI-generated summary 				 We propose Stable Video Infinity (SVI) that is able to generate infinite-length videos ...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 32. A novel translation-enhanced recipe using layer-selective tuning on parallel data improves translation performance in both high- and low-resource languages while maintaining reasoning proficiency.  					AI-generated summary 				 General Large Language Models (LLMs) excel in reasoning, but those enha...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 33. LikePhys evaluates intuitive physics in video diffusion models using a denoising objective-based metric, demonstrating better alignment with human preference than existing methods.  					AI-generated summary 				 Intuitive physics understanding in video diffusion models plays an essential role in bu...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 34. RePro, a reinforcement learning-based method, generates high-quality rephrasings of pretraining data to enhance the efficiency and accuracy of large language models.  					AI-generated summary 				 High-quality pretraining data is the fossil fuel of large language models (LLMs), yet its reserves are...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 35. DemoDiff, a demonstration-conditioned diffusion model, uses molecule-score examples to guide a denoising Transformer for molecular design, outperforming larger language models and domain-specific approaches.  					AI-generated summary 				 In-context learning allows large models to adapt to new task...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 36. VER, a Vision Expert Transformer, dynamically selects task-relevant experts from a pretrained vision expert library, achieving state-of-the-art performance across diverse robotic tasks with parameter-efficient fine-tuning.  					AI-generated summary 				 Pretrained vision foundation models (VFMs) ad...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 37. Large Reasoning Models evaluated in dynamic scenarios with interruptions and changing context show significant performance drops compared to static evaluations.  					AI-generated summary 				 Large Reasoning Models (LRMs) excel at complex reasoning but are traditionally evaluated in static, "frozen...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 38. IVEBench is a benchmark suite for instruction-guided video editing that addresses limitations in existing benchmarks through diverse video sources, comprehensive task coverage, and a multi-dimensional evaluation protocol.  					AI-generated summary 				 Instruction-guided video editing has emerged a...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 39. A study on authorship attribution of JavaScript code generated by large language models using a custom dataset and advanced machine learning classifiers demonstrates high accuracy even after code transformations.  					AI-generated summary 				 In this paper, we present the first large-scale study e...
[14.10.2025 09:14] ********************************************************************************
[14.10.2025 09:14] Abstract 40. A framework records and utilizes expert navigation behavior in whole-slide imaging to build an agentic system for pathology diagnosis, achieving high precision and recall in metastasis detection.  					AI-generated summary 				 Diagnosing a whole-slide image is an interactive, multi-stage process in...
[14.10.2025 09:14] Read previous papers.
[14.10.2025 09:14] Generating reviews via LLM API.
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#reasoning", "#math", "#training", "#optimization", "#inference", "#rl"], "emoji": "⚡", "ru": {"title": "Квантизация ускоряет RL-обучение LLM в полтора раза", "desc": "Представлен QeRL — фреймворк для ускорения обучения больших языковых моделей с помощью reinforcement learning. Ключ
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#training", "#optimization", "#cv", "#diffusion", "#architecture"], "emoji": "🎨", "ru": {"title": "RAE: новый стандарт для обучения диффузионных трансформеров", "desc": "В работе предлагается заменить традиционные VAE-энкодеры в Diffusion Transformers на предобученные энкодеры предс
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#multimodal", "#open_source", "#video"], "emoji": "🎬", "ru": {"title": "Проверка настоящего аудио-визуального понимания в AI", "desc": "OmniVideoBench — это комплексный бенчмарк для оценки аудио-визуального reasoning в multimodal LLM, который акцентирует 
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training", "#multimodal", "#rlhf", "#reasoning", "#rl"], "emoji": "👁️", "ru": {"title": "Обучение мультимодальных моделей через визуальное восприятие токенов", "desc": "В статье представлен алгоритм VPPO для улучшения рассуждений в больших визуальн
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#multimodal", "#video", "#benchmark", "#training", "#open_source", "#dataset", "#data"], "emoji": "🎬", "ru": {"title": "Аудиовизуальные описания видео с точной временной синхронизацией", "desc": "AVoCaDO — это модель для генерации описаний видео, которая учитывает как визуальную, та
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#rlhf", "#optimization", "#multimodal", "#rl"], "emoji": "🌊", "ru": {"title": "Обучение через flow-награды в латентном пространстве", "desc": "Статья представляет метод RLFR для улучшения reasoning-способностей LLM через новый подход к формированию наград
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#synthetic", "#optimization", "#dataset", "#cv"], "emoji": "🌐", "ru": {"title": "Гибридное обучение для реалистичных панорамных изображений", "desc": "DiT360 — это фреймворк на базе DiT для генерации панорамных изображений, использующий гибридное обучение на обычных перспективных и 
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#small_models", "#benchmark", "#reasoning", "#training", "#optimization", "#open_source", "#rl", "#dataset"], "emoji": "🤖", "ru": {"title": "Эффективное обучение агентов с подкреплением для улучшения рассуждений LLM", "desc": "Исследователи провели систематический анализ применения 
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#benchmark", "#training", "#interpretability", "#data", "#dataset", "#transfer_learning", "#security", "#agents"], "emoji": "🛡️", "ru": {"title": "Безопасность AI-агентов на этапе планирования: остановить угрозу до действия", "desc": "Статья представляет AuraGen и Safiron — систему 
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#optimization", "#data", "#math"], "emoji": "🔢", "ru": {"title": "Адаптивный reasoning вместо поверхностных решений в математике", "desc": "Статья представляет фреймворк AdaR для улучшения математического reasoning в LLM путём борьбы с ложной логикой (spurious r
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#survey", "#agents", "#benchmark", "#reasoning"], "emoji": "🎓", "ru": {"title": "Академическое мышление остаётся непокорённой вершиной для ИИ", "desc": "Представлен новый бенчмарк Acadreason для оценки способности LLM и AI-агентов к глубокому академическому рассуждению в пяти научны
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#interpretability", "#multimodal"], "emoji": "🔄", "ru": {"title": "Разрыв между пониманием и генерацией в мультимодальных моделях", "desc": "Статья представляет GIR-Bench — новый бенчмарк для оценки унифицированных мультимодальных моделей, которые объедин
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#video", "#games", "#diffusion", "#multimodal"], "emoji": "🎥", "ru": {"title": "Видео-генерация для планирования траектории камеры в 4D сценах", "desc": "Исследователи предлагают двухэтапный метод адаптации предобученных Text-to-Video моделей для предсказания точек обзора в 4D сцена
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#survey", "#benchmark"], "emoji": "📊", "ru": {"title": "LLM провалили экзамен по финансовому аудиту", "desc": "Исследователи создали бенчмарк FinAuditing для проверки способности больших языковых моделей работать со структурированными финансовыми документам
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#training", "#dataset", "#optimization", "#cv", "#agents"], "emoji": "🤖", "ru": {"title": "Объединяя рассуждения и действия роботов", "desc": "Vlaser — это Vision-Language-Action модель, которая соединяет высокоуровневые рассуждения с низкоуровневым управ
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#rlhf", "#training", "#diffusion", "#rl", "#reinforcement_learning"], "emoji": "🥪", "ru": {"title": "Сэндвич из границ для обучения диффузионных языковых моделей", "desc": "Диффузионные языковые модели (dLLM) могут генерировать несколько токенов параллельно, но их сложно обучать с п
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#math", "#reasoning", "#open_source", "#dataset"], "emoji": "📊", "ru": {"title": "Код как визуальное мышление для математических задач", "desc": "Исследователи представили CodePlot-CoT — модель, которая решает сложные математические задачи, генерируя не 
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#dataset", "#data", "#agents", "#alignment"], "emoji": "📄", "ru": {"title": "Научить AI отличать красиво оформленный документ от плохо оформленного", "desc": "DocReward — это reward model для оценки структуры и стиля документов, которая помогает AI-аге
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#optimization", "#training"], "emoji": "🌐", "ru": {"title": "Веб-агент, который работает с браузером как человек", "desc": "BrowserAgent — это интерактивный веб-агент, который взаимодействует с браузером как человек, используя действия вроде скроллинга, клик
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#optimization", "#training", "#agents", "#transfer_learning", "#rl"], "emoji": "🎯", "ru": {"title": "Обучение агентов через взаимодействие со средой вместо готовых примеров", "desc": "Статья представляет Environment Tuning — новый подход к обучению LLM-агентов для сложных многошагов
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#hallucinations"], "emoji": "👁️", "ru": {"title": "Борьба с галлюцинациями через маскировку неуверенных визуальных токенов", "desc": "Статья исследует проблему объектных галлюцинаций в больших vision-language моделях (LVLM), когда модель описывает объекты, отсу
[14.10.2025 09:14] Querying the API.
[14.10.2025 09:14] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A unified multimodal large language model (MLLM) for SVG understanding, editing, and generation leverages a comprehensive dataset and benchmark to achieve superior performance across various tasks.  					AI-generated summary 				 General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated data-benchmark-model suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, a companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, a unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and a two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts.
[14.10.2025 09:14] Response: ```json
{
  "title": "Единая модель для понимания, редактирования и генерации SVG графики",
  "desc": "Исследователи представили InternSVG — семейство мультимодальных LLM для работы с векторной графикой SVG. В основе лежит SAgoge, самый крупный датасет для SVG-задач, включающий иконки, иллюстрации, научные диаграммы и анимации. Модель использует специальные токены для SVG, инициализацию эмбеддингов на основе подслов и двухэтапное обучение от простых статичных изображений к сложным анимациям. InternSVG превосходит конкурентов на бенчмарке SArena благодаря единому подходу, который обеспечивает положительный перенос знаний между разными задачами.",
  "emoji": "🎨",
  "desc": "Исследователи представили InternSVG — семейство мультимодальных LLM для работы с векторной графикой SVG. В основе лежит SAgoge, самый крупный датасет для SVG-задач, включающий иконки, иллюстрации, научные диаграммы и анимации. Модель использует специальные токены для SVG, инициализацию эмбеддингов на основе подслов и двухэтапное обучение от простых статичных изображений к сложным анимациям. InternSVG превосходит конкурентов на бенчмарке SArena благодаря единому подходу, который обеспечивает положительный перенос знаний между разными задачами."
}
```
[14.10.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A unified multimodal large language model (MLLM) for SVG understanding, editing, and generation leverages a comprehensive dataset and benchmark to achieve superior performance across various tasks.  					AI-generated summary 				 General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated data-benchmark-model suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, a companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, a unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and a two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts."

[14.10.2025 09:14] Response: ```python
['DATASET', 'BENCHMARK', 'MULTIMODAL', 'TRAINING']
```
[14.10.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A unified multimodal large language model (MLLM) for SVG understanding, editing, and generation leverages a comprehensive dataset and benchmark to achieve superior performance across various tasks.  					AI-generated summary 				 General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated data-benchmark-model suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, a companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, a unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and a two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts."

[14.10.2025 09:14] Response: ```python
['TRANSFER_LEARNING', 'OPEN_SOURCE']
```
[14.10.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper introduces a unified multimodal large language model (MLLM) called InternSVG, designed specifically for understanding, editing, and generating Scalable Vector Graphics (SVG). It addresses challenges in SVG modeling by utilizing the SAgoge dataset, which is the largest collection of SVG-related data, including static and dynamic graphics. The model employs a two-stage training strategy that enhances its ability to handle both simple and complex SVG tasks. Experimental results demonstrate that InternSVG significantly outperforms existing models, showcasing its effectiveness in various SVG applications.","title":"Revolutionizing SVG with Unified Multimodal Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper introduces a unified multimodal large language model (MLLM) called InternSVG, designed specifically for understanding, editing, and generating Scalable Vector Graphics (SVG). It addresses challenges in SVG modeling by utilizing the SAgoge dataset, which is the largest collection of SVG-related data, including static and dynamic graphics. The model employs a two-stage training strategy that enhances its ability to handle both simple and complex SVG tasks. Experimental results demonstrate that InternSVG significantly outperforms existing models, showcasing its effectiveness in various SVG applications.', title='Revolutionizing SVG with Unified Multimodal Learning'))
[14.10.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"这篇论文提出了一种统一的多模态大语言模型（MLLM），用于SVG的理解、编辑和生成。通过构建一个全面的数据集和基准，模型在各种任务中表现出色。论文介绍了InternSVG系列，核心是SAgoge数据集，涵盖静态图形和动态动画，支持多种难度的任务。基于这些资源，提出了SArena基准，确保任务定义和评估标准化，从而提升了模型的整体性能。","title":"统一的多模态大语言模型，提升SVG处理能力"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='这篇论文提出了一种统一的多模态大语言模型（MLLM），用于SVG的理解、编辑和生成。通过构建一个全面的数据集和基准，模型在各种任务中表现出色。论文介绍了InternSVG系列，核心是SAgoge数据集，涵盖静态图形和动态动画，支持多种难度的任务。基于这些资源，提出了SArena基准，确保任务定义和评估标准化，从而提升了模型的整体性能。', title='统一的多模态大语言模型，提升SVG处理能力'))
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#3d", "#multimodal", "#optimization", "#transfer_learning", "#robotics"], "emoji": "🤖", "ru": {"title": "От фотографий к роботу: автоматическое создание реалистичных симуляций", "desc": "RoboSimGS - это framework Real2Sim2Real, который создаёт высокоточные симуляционные среды для об
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#transfer_learning", "#training", "#optimization", "#open_source", "#synthetic"], "emoji": "🎯", "ru": {"title": "Целевое обучение через диагностику навыков учителем", "desc": "Статья представляет новый метод файнтюнинга STAT, который использует метакогнитивные способности сильной яз
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#rag", "#benchmark", "#training", "#optimization", "#multimodal", "#rl", "#games", "#agents"], "emoji": "👁️", "ru": {"title": "Визуальное обучение с подкреплением для генерации фронтенд-кода", "desc": "ReLook — это фреймворк на основе reinforcement learning, который улучшает генерац
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#training", "#reasoning", "#optimization"], "emoji": "🎯", "ru": {"title": "Контроль длины рассуждений через энтропию на разных фазах", "desc": "Исследователи обнаружили, что энтропия модели коррелирует с длиной ответа на разных этапах рассуждения: высокая энтроп
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#training", "#optimization", "#agents", "#transfer_learning"], "emoji": "🔄", "ru": {"title": "Самообучение на лету: как модели учатся на своих ошибках во время тестирования", "desc": "Статья предлагает метод самосовершенствования языковых моделей во время тестирования (TT-SI). Модел
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#benchmark", "#3d", "#optimization", "#diffusion", "#architecture"], "emoji": "🏃", "ru": {"title": "Быстрое восстановление 3D-сетки человека через умное объединение слоёв и токенов", "desc": "Исследователи предложили два метода оптимизации transformer-моделей для восстановления 3D-с
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#alignment", "#ethics", "#multimodal", "#healthcare"], "emoji": "⚖️", "ru": {"title": "Память LLM усиливает социальное неравенство в понимании эмоций", "desc": "Исследование показывает, что LLM демонстрируют систематические предвзятости при интерпретации эмоций пользователей в завис
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#open_source", "#diffusion", "#dataset", "#synthetic"], "emoji": "👥", "ru": {"title": "Бесконечное разнообразие 3D-людей через дистилляцию AI-моделей", "desc": "InfiniHuman — это фреймворк для генерации реалистичных 3D-аватаров людей с использованием дистилляци
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#benchmark", "#interpretability", "#low_resource"], "emoji": "🤝", "ru": {"title": "Человек против машины: новый стандарт оценки текстовых эмбеддингов", "desc": "Исследователи представили HUME — фреймворк для оценки эмбеддингов текста, который впервые сис
[14.10.2025 09:14] Querying the API.
[14.10.2025 09:14] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Stable Video Infinity generates infinite-length videos with high temporal consistency and controllable storylines by using Error-Recycling Fine-Tuning on the Diffusion Transformer.  					AI-generated summary 				 We propose Stable Video Infinity (SVI) that is able to generate infinite-length videos with high temporal consistency, plausible scene transitions, and controllable streaming storylines. While existing long-video methods attempt to mitigate accumulated errors via handcrafted anti-drifting (e.g., modified noise scheduler, frame anchoring), they remain limited to single-prompt extrapolation, producing homogeneous scenes with repetitive motions. We identify that the fundamental challenge extends beyond error accumulation to a critical discrepancy between the training assumption (seeing clean data) and the test-time autoregressive reality (conditioning on self-generated, error-prone outputs). To bridge this hypothesis gap, SVI incorporates Error-Recycling Fine-Tuning, a new type of efficient training that recycles the Diffusion Transformer (DiT)'s self-generated errors into supervisory prompts, thereby encouraging DiT to actively identify and correct its own errors. This is achieved by injecting, collecting, and banking errors through closed-loop recycling, autoregressively learning from error-injected feedback. Specifically, we (i) inject historical errors made by DiT to intervene on clean inputs, simulating error-accumulated trajectories in flow matching; (ii) efficiently approximate predictions with one-step bidirectional integration and calculate errors with residuals; (iii) dynamically bank errors into replay memory across discretized timesteps, which are resampled for new input. SVI is able to scale videos from seconds to infinite durations with no additional inference cost, while remaining compatible with diverse conditions (e.g., audio, skeleton, and text streams). We evaluate SVI on three benchmarks, including consistent, creative, and conditional settings, thoroughly verifying its versatility and state-of-the-art role.
[14.10.2025 09:14] Response: ```json
{
  "desc": "Stable Video Infinity (SVI) генерирует видео бесконечной длины с высокой временной согласованностью и контролируемым сюжетом. Основная инновация — метод Error-Recycling Fine-Tuning, который обучает Diffusion Transformer исправлять собственные ошибки, возникающие при автоregрессивной генерации. Система намеренно внедряет ошибки в процесс обучения, собирает их в replay memory и переиспользует как обучающие сигналы, устраняя разрыв между обучением на чистых данных и реальной генерацией с накопленными ошибками. В отличие от существующих методов, SVI масштабируется на бесконечную длительность без дополнительных вычислительных затрат и поддерживает различные условия — аудио, скелетную анимацию и текстовые промпты.",
  "emoji": "♾️",
  "title": "Бесконечное видео через переработку собственных ошибок"
}
```
[14.10.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Stable Video Infinity generates infinite-length videos with high temporal consistency and controllable storylines by using Error-Recycling Fine-Tuning on the Diffusion Transformer.  					AI-generated summary 				 We propose Stable Video Infinity (SVI) that is able to generate infinite-length videos with high temporal consistency, plausible scene transitions, and controllable streaming storylines. While existing long-video methods attempt to mitigate accumulated errors via handcrafted anti-drifting (e.g., modified noise scheduler, frame anchoring), they remain limited to single-prompt extrapolation, producing homogeneous scenes with repetitive motions. We identify that the fundamental challenge extends beyond error accumulation to a critical discrepancy between the training assumption (seeing clean data) and the test-time autoregressive reality (conditioning on self-generated, error-prone outputs). To bridge this hypothesis gap, SVI incorporates Error-Recycling Fine-Tuning, a new type of efficient training that recycles the Diffusion Transformer (DiT)'s self-generated errors into supervisory prompts, thereby encouraging DiT to actively identify and correct its own errors. This is achieved by injecting, collecting, and banking errors through closed-loop recycling, autoregressively learning from error-injected feedback. Specifically, we (i) inject historical errors made by DiT to intervene on clean inputs, simulating error-accumulated trajectories in flow matching; (ii) efficiently approximate predictions with one-step bidirectional integration and calculate errors with residuals; (iii) dynamically bank errors into replay memory across discretized timesteps, which are resampled for new input. SVI is able to scale videos from seconds to infinite durations with no additional inference cost, while remaining compatible with diverse conditions (e.g., audio, skeleton, and text streams). We evaluate SVI on three benchmarks, including consistent, creative, and conditional settings, thoroughly verifying its versatility and state-of-the-art role."

[14.10.2025 09:14] Response: ```python
['VIDEO', 'TRAINING', 'BENCHMARK']
```
[14.10.2025 09:14] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Stable Video Infinity generates infinite-length videos with high temporal consistency and controllable storylines by using Error-Recycling Fine-Tuning on the Diffusion Transformer.  					AI-generated summary 				 We propose Stable Video Infinity (SVI) that is able to generate infinite-length videos with high temporal consistency, plausible scene transitions, and controllable streaming storylines. While existing long-video methods attempt to mitigate accumulated errors via handcrafted anti-drifting (e.g., modified noise scheduler, frame anchoring), they remain limited to single-prompt extrapolation, producing homogeneous scenes with repetitive motions. We identify that the fundamental challenge extends beyond error accumulation to a critical discrepancy between the training assumption (seeing clean data) and the test-time autoregressive reality (conditioning on self-generated, error-prone outputs). To bridge this hypothesis gap, SVI incorporates Error-Recycling Fine-Tuning, a new type of efficient training that recycles the Diffusion Transformer (DiT)'s self-generated errors into supervisory prompts, thereby encouraging DiT to actively identify and correct its own errors. This is achieved by injecting, collecting, and banking errors through closed-loop recycling, autoregressively learning from error-injected feedback. Specifically, we (i) inject historical errors made by DiT to intervene on clean inputs, simulating error-accumulated trajectories in flow matching; (ii) efficiently approximate predictions with one-step bidirectional integration and calculate errors with residuals; (iii) dynamically bank errors into replay memory across discretized timesteps, which are resampled for new input. SVI is able to scale videos from seconds to infinite durations with no additional inference cost, while remaining compatible with diverse conditions (e.g., audio, skeleton, and text streams). We evaluate SVI on three benchmarks, including consistent, creative, and conditional settings, thoroughly verifying its versatility and state-of-the-art role."

[14.10.2025 09:14] Response: ```python
["DIFFUSION", "STORY_GENERATION"]
```
[14.10.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Stable Video Infinity (SVI) is a novel approach for generating infinite-length videos that maintain high temporal consistency and allow for controllable storylines. It addresses the limitations of existing long-video methods by introducing Error-Recycling Fine-Tuning, which helps the Diffusion Transformer (DiT) learn from its own errors during video generation. This method involves recycling self-generated errors as supervisory prompts, enabling the model to correct its mistakes and improve the quality of the output. SVI can produce videos of any length without increasing inference costs and works well with various input conditions like audio and text.","title":"Generate Infinite Videos with Consistency and Control!"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Stable Video Infinity (SVI) is a novel approach for generating infinite-length videos that maintain high temporal consistency and allow for controllable storylines. It addresses the limitations of existing long-video methods by introducing Error-Recycling Fine-Tuning, which helps the Diffusion Transformer (DiT) learn from its own errors during video generation. This method involves recycling self-generated errors as supervisory prompts, enabling the model to correct its mistakes and improve the quality of the output. SVI can produce videos of any length without increasing inference costs and works well with various input conditions like audio and text.', title='Generate Infinite Videos with Consistency and Control!'))
[14.10.2025 09:14] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Stable Video Infinity（SVI）是一种生成无限长度视频的新方法，具有高时间一致性和可控的故事情节。该方法通过错误回收微调技术，利用扩散变换器（DiT）自生成的错误来改进视频生成过程。与传统方法不同，SVI能够有效地识别和纠正自身错误，从而避免了重复动作和同质场景的问题。通过这种方式，SVI可以在不增加推理成本的情况下，生成从几秒到无限时长的视频，并兼容多种输入条件。","title":"无限视频生成的稳定性与可控性"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Stable Video Infinity（SVI）是一种生成无限长度视频的新方法，具有高时间一致性和可控的故事情节。该方法通过错误回收微调技术，利用扩散变换器（DiT）自生成的错误来改进视频生成过程。与传统方法不同，SVI能够有效地识别和纠正自身错误，从而避免了重复动作和同质场景的问题。通过这种方式，SVI可以在不增加推理成本的情况下，生成从几秒到无限时长的视频，并兼容多种输入条件。', title='无限视频生成的稳定性与可控性'))
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#low_resource", "#reasoning", "#training", "#machine_translation", "#open_source", "#multilingual"], "emoji": "🌍", "ru": {"title": "Перевод без потери интеллекта: послойная настройка LLM", "desc": "Исследователи предложили новый метод улучшения качества перевода в больших языковых м
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#video", "#benchmark", "#inference", "#diffusion", "#dataset", "#alignment"], "emoji": "🎱", "ru": {"title": "Оценка физической интуиции видео-моделей через правдоподобие", "desc": "Исследователи представили LikePhys — метод оценки понимания интуитивной физики в диффузионных моделях 
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#training", "#data", "#optimization", "#open_source", "#transfer_learning", "#rl"], "emoji": "♻️", "ru": {"title": "Переработка данных для эффективного обучения языковых моделей", "desc": "Статья представляет RePro — метод на основе обучения с подкреплением, который учит небольшую я
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#training", "#optimization", "#open_source", "#diffusion", "#architecture", "#dataset", "#data"], "emoji": "💊", "ru": {"title": "Молекулярный дизайн по примерам: DemoDiff учится создавать молекулы из нескольких демонстраций", "desc": "DemoDiff — это диффузионная модель, которая учит
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#cv", "#training", "#agents", "#robotics", "#games"], "emoji": "🤖", "ru": {"title": "Библиотека визуальных экспертов с умной маршрутизацией для роботов", "desc": "VER — это Vision Expert Transformer для робототехники, который создаёт библиотеку визу
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#reasoning", "#training", "#benchmark", "#hallucinations"], "emoji": "⏱️", "ru": {"title": "Когда AI думает слишком долго: проблема динамического мира для моделей рассуждений", "desc": "Исследование показывает, что большие модели рассуждений (LRM) демонстрируют высокую точность в ст
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#video"], "emoji": "🎬", "ru": {"title": "Комплексная оценка AI-редактирования видео по текстовым инструкциям", "desc": "IVEBench — это новый бенчмарк для оценки методов редактирования видео по текстовым инструкциям. Датасет включает 600 высококачественны
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#security", "#architecture", "#dataset", "#data"], "emoji": "🔍", "ru": {"title": "Каждая LLM оставляет уникальный почерк в JavaScript-коде", "desc": "Исследователи создали первый крупномасштабный датасет LLM-NodeJS из 50,000 программ на JavaScript, сген
[14.10.2025 09:14] Using data from previous issue: {"categories": ["#agents", "#agi", "#reasoning", "#healthcare", "#interpretability", "#science", "#dataset"], "emoji": "🔬", "ru": {"title": "Агентная система патологии учится у экспертов через запись их навигации", "desc": "Исследователи создали AI Session Recorder — систему, которая незаметно запис
[14.10.2025 09:14] Renaming data file.
[14.10.2025 09:14] Renaming previous data. hf_papers.json to ./d/2025-10-14.json
[14.10.2025 09:14] Saving new data file.
[14.10.2025 09:14] Generating page.
[14.10.2025 09:14] Renaming previous page.
[14.10.2025 09:14] Renaming previous data. index.html to ./d/2025-10-14.html
[14.10.2025 09:14] Writing result.
[14.10.2025 09:14] Renaming log file.
[14.10.2025 09:14] Renaming previous data. log.txt to ./logs/2025-10-14_last_log.txt
