[14.10.2025 17:12] Read previous papers.
[14.10.2025 17:12] Generating top page (month).
[14.10.2025 17:12] Writing top page (month).
[14.10.2025 18:17] Read previous papers.
[14.10.2025 18:17] Get feed.
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11696
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11690
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10689
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11052
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10201
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09285
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10395
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11712
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04617
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11701
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11341
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09781
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11652
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10666
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08886
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11391
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10197
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11026
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10670
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11027
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09541
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11718
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09008
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10637
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10023
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11498
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08026
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07841
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10868
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09905
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09212
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11512
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10062
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10047
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07624
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11650
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09189
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04201
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10681
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09474
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08744
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.05213
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.01427
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11713
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11647
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11496
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10606
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.10493
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09871
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.09023
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.06582
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.11218
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.08561
[14.10.2025 18:17] Get page data from previous paper. URL: https://huggingface.co/papers/2510.04587
[14.10.2025 18:17] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[14.10.2025 18:17] No deleted papers detected.
[14.10.2025 18:17] Downloading and parsing papers (pdf, html). Total: 54.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11696.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11696.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11696.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11690.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11690.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11690.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.10689.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.10689.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.10689.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11052.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11052.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11052.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.10201.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.10201.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.10201.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.09285.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.09285.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.09285.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.10395.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.10395.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.10395.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11712.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11712.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11712.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.04617.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.04617.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.04617.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11701.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11701.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11701.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11341.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11341.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11341.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.09781.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.09781.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.09781.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11652.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11652.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11652.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.10666.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.10666.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.10666.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.08886.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.08886.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.08886.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11391.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11391.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11391.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.10197.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.10197.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.10197.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11026.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11026.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11026.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.10670.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.10670.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.10670.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11027.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11027.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11027.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.09541.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.09541.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.09541.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11718.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11718.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11718.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.09008.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.09008.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.09008.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.10637.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.10637.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.10637.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.10023.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.10023.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.10023.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11498.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11498.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11498.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.08026.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.08026.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.08026.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.07841.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.07841.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.07841.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.10868.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.10868.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.10868.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.09905.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.09905.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.09905.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.09212.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.09212.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.09212.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11512.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11512.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11512.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.10062.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.10062.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.10062.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.10047.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.10047.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.10047.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.07624.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.07624.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.07624.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11650.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11650.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11650.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.09189.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.09189.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.09189.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.04201.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.04201.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.04201.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.10681.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.10681.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.10681.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.09474.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.09474.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.09474.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.08744.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.08744.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.08744.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.05213.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.05213.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.05213.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.01427.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.01427.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.01427.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11713.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11713.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11713.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11647.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11647.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11647.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11496.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11496.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11496.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.10606.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.10606.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.10606.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.10493.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.10493.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.10493.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.09871.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.09871.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.09871.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.09023.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.09023.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.09023.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.06582.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.06582.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.06582.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.11218.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.11218.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.11218.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.08561.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.08561.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.08561.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Downloading and parsing paper https://huggingface.co/papers/2510.04587.
[14.10.2025 18:17] Extra JSON file exists (./assets/json/2510.04587.json), skip PDF parsing.
[14.10.2025 18:17] Paper image links file exists (./assets/img_data/2510.04587.json), skip HTML parsing.
[14.10.2025 18:17] Success.
[14.10.2025 18:17] Enriching papers with extra data.
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 0. QeRL, a quantization-enhanced reinforcement learning framework, accelerates RL training for large language models by combining NVFP4 quantization with Low-Rank Adaptation and an Adaptive Quantization Noise mechanism, achieving significant speedups and improved performance.  					AI-generated summary...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 1. Replacing VAEs with pretrained representation encoders in Diffusion Transformers enhances generative quality and convergence speed without auxiliary losses.  					AI-generated summary 				 Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion pr...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 2. OmniVideoBench is a comprehensive benchmark for evaluating audio-visual reasoning in multimodal large language models, addressing modality complementarity and logical consistency.  					AI-generated summary 				 Recent advances in multimodal large language models (MLLMs) have demonstrated substantia...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 3. Latent Refinement Decoding (LRD) improves parallel sequence generation by maintaining global consistency and iterative refinement, enhancing accuracy and reducing latency.  					AI-generated summary 				 Autoregressive (AR) models remain the standard for natural language generation but still suffer ...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 4. RLFR uses flow rewards derived from latent space to improve reinforcement learning with verifiable rewards, demonstrating reliable reward shaping and efficient context comprehension.  					AI-generated summary 				 Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a promi...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 5. VPPO, a novel policy gradient algorithm, enhances multimodal RLVR by leveraging token perception to refine learning signals and improve reasoning capabilities in Large Vision-Language Models.  					AI-generated summary 				 While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 6. AVoCaDO, an audiovisual video captioner, enhances temporal coherence and dialogue accuracy through a two-stage post-training pipeline, outperforming existing models across multiple benchmarks.  					AI-generated summary 				 Audiovisual video captioning aims to generate semantically rich description...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 7. DiT360 framework enhances panoramic image generation by hybrid training on perspective and panoramic data, incorporating cross-domain knowledge and hybrid supervision to improve boundary consistency and image fidelity.  					AI-generated summary 				 In this work, we propose DiT360, a DiT-based fram...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 8. AdaR framework enhances LLMs' robustness and generalization in mathematical reasoning by synthesizing logically equivalent queries and using RLVR to penalize spurious logic.  					AI-generated summary 				 Mathematical reasoning is a primary indicator of large language models (LLMs) intelligence. Ho...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 9. Agentic reinforcement learning enhances LLMs' reasoning ability through real datasets, exploration techniques, and a deliberative strategy, achieving strong performance with smaller models.  					AI-generated summary 				 Recently, the emergence of agentic RL has showcased that RL could also effecti...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 10. A unified multimodal large language model (MLLM) for SVG understanding, editing, and generation leverages a comprehensive dataset and benchmark to achieve superior performance across various tasks.  					AI-generated summary 				 General SVG modeling remains challenging due to fragmented datasets, l...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 11. AuraGen and Safiron address pre-execution safety gaps in LLM agents by synthesizing benign trajectories, injecting risks, and using a cross-planner adapter for robust risk detection and explanation.  					AI-generated summary 				 While LLM agents can plan multi-step tasks, intervening at the planni...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 12. The Acadreason benchmark evaluates LLMs and agents on high-level academic reasoning across multiple domains, revealing significant capability gaps.  					AI-generated summary 				 In recent years, the research focus of large language models (LLMs) and agents has shifted increasingly from demonstrati...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 13. BrowserAgent, an interactive web agent using human-like browser actions and a two-stage training process, achieves competitive results in Open-QA tasks with less training data and improved reasoning for multi-hop QA.  					AI-generated summary 				 Efficiently solving real-world problems with LLMs i...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 14. FinAuditing is a benchmark for evaluating LLMs on structured financial auditing tasks, revealing their limitations in handling taxonomy-driven, hierarchical financial documents.  					AI-generated summary 				 The complexity of the Generally Accepted Accounting Principles (GAAP) and the hierarchical...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 15. DocReward, a document reward model, evaluates and enhances the structural and stylistic quality of generated documents, outperforming GPT-4o and GPT-5 in both accuracy and human-preferred document generation.  					AI-generated summary 				 Recent advances in agentic workflows have enabled the autom...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 16. Environment Tuning enables LLM agents to learn complex behaviors from problem instances using a structured curriculum, environment augmentation, and progress rewards, achieving competitive in-distribution performance and superior out-of-distribution generalization.  					AI-generated summary 				 La...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 17. GIR-Bench evaluates unified multimodal models across understanding-generation consistency, reasoning-centric text-to-image generation, and multi-step reasoning in editing, highlighting gaps in their capabilities.  					AI-generated summary 				 Unified multimodal models integrate the reasoning capac...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 18. A two-stage paradigm adapts pre-trained Text-to-Video models for viewpoint prediction in 4D scenes by integrating an adaptive learning branch and a camera extrinsic diffusion branch.  					AI-generated summary 				 Recent Text-to-Video (T2V) models have demonstrated powerful capability in visual sim...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 19. Vlaser, a Vision-Language-Action Model, integrates high-level reasoning with low-level control for embodied agents, achieving state-of-the-art performance in embodied reasoning tasks and competitive results in robot benchmarks.  					AI-generated summary 				 While significant research has focused o...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 20. The Sandwiched Policy Gradient method improves reinforcement learning for diffusion large language models by using both upper and lower bounds of log-likelihood, outperforming ELBO-based methods.  					AI-generated summary 				 Diffusion large language models (dLLMs) are emerging as an efficient alt...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 21. CodePlot-CoT, a code-driven Chain-of-Thought model, enhances multimodal mathematical reasoning by generating both text and executable plotting code to solve problems requiring visual assistance.  					AI-generated summary 				 Recent advances in Large Language Models (LLMs) and Vision Language Model...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 22. A method to reduce object hallucinations in large vision-language models by identifying and masking uncertain visual tokens in the vision encoder.  					AI-generated summary 				 Large vision-language models (LVLMs), which integrate a vision encoder (VE) with a large language model, have achieved re...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 23. RoboSimGS, a Real2Sim2Real framework, uses 3D Gaussian Splatting and mesh primitives to create scalable, high-fidelity, and physically interactive simulation environments, enabling successful zero-shot sim-to-real transfer for robotic manipulation tasks.  					AI-generated summary 				 The scalabili...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 24. A new fine-tuning strategy, STAT, uses a teacher model's metacognition to identify and address skill gaps in a student model, leading to improved performance on both in-distribution and out-of-distribution benchmarks.  					AI-generated summary 				 Language models often show little to no improvemen...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 25. ReLook, a vision-grounded reinforcement learning framework, enhances front-end code generation by integrating a multimodal LLM for visual feedback and forced optimization, outperforming existing methods.  					AI-generated summary 				 While Large Language Models (LLMs) excel at algorithmic code gen...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 26. A reward mechanism called Phase Entropy Aware Reward (PEAR) controls the length of reasoning in large models by adjusting entropy at different stages, balancing conciseness and accuracy.  					AI-generated summary 				 Large Reasoning Models (LRMs) have achieved impressive performance on complex rea...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 27. A test-time self-improvement method enhances language models by generating additional training examples from uncertain cases, leading to better performance with fewer samples.  					AI-generated summary 				 One paradigm of language model (LM) fine-tuning relies on creating large training datasets, ...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 28. Two merging strategies and a diffusion-based decoder improve 3D Human Mesh Recovery by reducing computational cost and slightly enhancing performance.  					AI-generated summary 				 Recent transformer-based models for 3D Human Mesh Recovery (HMR) have achieved strong performance but often suffer fr...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 29. LLMs exhibit systematic biases in emotional interpretation and support based on user profiles, potentially reinforcing social hierarchies.  					AI-generated summary 				 When an AI assistant remembers that Sarah is a single mother working two jobs, does it interpret her stress differently than if s...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 30. Stable Video Infinity generates infinite-length videos with high temporal consistency and controllable storylines by using Error-Recycling Fine-Tuning on the Diffusion Transformer.  					AI-generated summary 				 We propose Stable Video Infinity (SVI) that is able to generate infinite-length videos ...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 31. LikePhys evaluates intuitive physics in video diffusion models using a denoising objective-based metric, demonstrating better alignment with human preference than existing methods.  					AI-generated summary 				 Intuitive physics understanding in video diffusion models plays an essential role in bu...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 32. HUME provides human performance baselines for text embedding tasks, enhancing the interpretability of model evaluations and revealing dataset and language-specific challenges.  					AI-generated summary 				 Comparing human and model performance offers a valuable perspective for understanding the st...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 33. SwarmSys, a distributed multi-agent framework inspired by swarm intelligence, enhances scalability and adaptability in long-horizon reasoning through specialized roles and self-organizing mechanisms.  					AI-generated summary 				 Large language model (LLM) agents have shown remarkable reasoning ab...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 34. A bilevel optimization framework is used to align generative models with high-quality datasets in the absence of explicit reward signals, with applications in classification and model-based reinforcement learning.  					AI-generated summary 				 Generative models form the backbone of modern machine ...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 35. InfiniHuman framework distills existing models to generate large-scale, richly annotated 3D human data using a diffusion-based generative pipeline, achieving high visual quality, speed, and controllability.  					AI-generated summary 				 Generating realistic and controllable 3D human avatars is a l...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 36. A novel translation-enhanced recipe using layer-selective tuning on parallel data improves translation performance in both high- and low-resource languages while maintaining reasoning proficiency.  					AI-generated summary 				 General Large Language Models (LLMs) excel in reasoning, but those enha...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 37. World-To-Image enhances text-to-image generation by integrating web-based knowledge retrieval and multimodal prompt optimization, improving semantic accuracy and visual quality.  					AI-generated summary 				 While text-to-image (T2I) models can synthesize high-quality images, their performance deg...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 38. RePro, a reinforcement learning-based method, generates high-quality rephrasings of pretraining data to enhance the efficiency and accuracy of large language models.  					AI-generated summary 				 High-quality pretraining data is the fossil fuel of large language models (LLMs), yet its reserves are...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 39. Multimodal Policy Internalization (MPI) internalizes complex multimodal policies into model parameters, enhancing policy adherence and performance in conversational agents.  					AI-generated summary 				 Modern conversational agents like ChatGPT and Alexa+ rely on predefined policies specifying met...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 40. DemoDiff, a demonstration-conditioned diffusion model, uses molecule-score examples to guide a denoising Transformer for molecular design, outperforming larger language models and domain-specific approaches.  					AI-generated summary 				 In-context learning allows large models to adapt to new task...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 41. VER, a Vision Expert Transformer, dynamically selects task-relevant experts from a pretrained vision expert library, achieving state-of-the-art performance across diverse robotic tasks with parameter-efficient fine-tuning.  					AI-generated summary 				 Pretrained vision foundation models (VFMs) ad...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 42. Falconer combines large language models with lightweight proxy models to achieve scalable and efficient knowledge mining, reducing inference costs and accelerating large-scale operations.  					AI-generated summary 				 At the core of Deep Research is knowledge mining, the task of extracting structu...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 43. Large Reasoning Models evaluated in dynamic scenarios with interruptions and changing context show significant performance drops compared to static evaluations.  					AI-generated summary 				 Large Reasoning Models (LRMs) excel at complex reasoning but are traditionally evaluated in static, "frozen...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 44. IVEBench is a benchmark suite for instruction-guided video editing that addresses limitations in existing benchmarks through diverse video sources, comprehensive task coverage, and a multi-dimensional evaluation protocol.  					AI-generated summary 				 Instruction-guided video editing has emerged a...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 45. AndesVL, a suite of mobile-side MLLMs with reduced parameters, achieves top-tier performance across various benchmarks compared to similar-scale models.  					AI-generated summary 				 In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstra...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 46. ViSurf combines Supervised Fine-Tuning and Reinforcement Learning with Verifiable Rewards to enhance Large Vision-and-Language Models, outperforming individual methods and two-stage approaches.  					AI-generated summary 				 Typical post-training paradigms for Large Vision-and-Language Models (LVLM...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 47. A study on authorship attribution of JavaScript code generated by large language models using a custom dataset and advanced machine learning classifiers demonstrates high accuracy even after code transformations.  					AI-generated summary 				 In this paper, we present the first large-scale study e...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 48. CoBia, a suite of adversarial attacks, reveals that LLMs often fail to reject biased follow-up questions, highlighting embedded biases in conversations.  					AI-generated summary 				 Improvements in model construction, including fortified safety guardrails, allow Large language models (LLMs) to in...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 49. Defenses against jailbreaks and prompt injections in language models should be evaluated against adaptive attackers using advanced optimization techniques to ensure robustness.  					AI-generated summary 				 How should we evaluate the robustness of language model defenses? Current defenses against ...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 50. A semi-automated pipeline using spherical projection, feature enrichment, and ensemble learning reduces manual annotation effort for TLS point cloud segmentation while maintaining high accuracy.  					AI-generated summary 				 Accurate semantic segmentation of terrestrial laser scanning (TLS) point ...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 51. LLMs exhibit inconsistent factual knowledge retrieval between simple and complex queries, highlighting a reliability gap that undermines trustworthiness.  					AI-generated summary 				 Large language models (LLMs) can correctly answer "When was Einstein born?" yet fail to provide the same date when...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 52. MultiCOIN, a video inbetweening framework using the Diffusion Transformer, enables multi-modal controls for precise and flexible video interpolation.  					AI-generated summary 				 Video inbetweening creates smooth and natural transitions between two image frames, making it an indispensable tool fo...
[14.10.2025 18:17] ********************************************************************************
[14.10.2025 18:17] Abstract 53. A framework records and utilizes expert navigation behavior in whole-slide imaging to build an agentic system for pathology diagnosis, achieving high precision and recall in metastasis detection.  					AI-generated summary 				 Diagnosing a whole-slide image is an interactive, multi-stage process in...
[14.10.2025 18:17] Read previous papers.
[14.10.2025 18:17] Generating reviews via LLM API.
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#reasoning", "#math", "#training", "#optimization", "#inference", "#rl"], "emoji": "‚ö°", "ru": {"title": "–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è —É—Å–∫–æ—Ä—è–µ—Ç RL-–æ–±—É—á–µ–Ω–∏–µ LLM –≤ –ø–æ–ª—Ç–æ—Ä–∞ —Ä–∞–∑–∞", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω QeRL ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é reinforcement learning. –ö–ª—é—á
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#training", "#optimization", "#cv", "#diffusion", "#architecture"], "emoji": "üé®", "ru": {"title": "RAE: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∑–∞–º–µ–Ω–∏—Ç—å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ VAE-—ç–Ω–∫–æ–¥–µ—Ä—ã –≤ Diffusion Transformers –Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã –ø—Ä–µ–¥—Å
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#multimodal", "#open_source", "#video"], "emoji": "üé¨", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –∞—É–¥–∏–æ-–≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ AI", "desc": "OmniVideoBench ‚Äî —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞—É–¥–∏–æ-–≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ reasoning –≤ multimodal LLM, –∫–æ—Ç–æ—Ä—ã–π –∞–∫—Ü–µ–Ω—Ç–∏—Ä—É–µ—Ç 
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#reasoning", "#optimization", "#math", "#training"], "emoji": "üîÑ", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Latent Refinement Decoding (LRD), –∫–æ—Ç–æ—Ä—ã–π —É—Å–∫–æ—Ä—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –∑–∞ —Å—á—ë—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#rlhf", "#optimization", "#multimodal", "#rl"], "emoji": "üåä", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ flow-–Ω–∞–≥—Ä–∞–¥—ã –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ RLFR –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è reasoning-—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–∞–≥—Ä–∞–¥
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training", "#multimodal", "#rlhf", "#reasoning", "#rl"], "emoji": "üëÅÔ∏è", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ —Ç–æ–∫–µ–Ω–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∞–ª–≥–æ—Ä–∏—Ç–º VPPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#multimodal", "#video", "#benchmark", "#training", "#open_source", "#dataset", "#data"], "emoji": "üé¨", "ru": {"title": "–ê—É–¥–∏–æ–≤–∏–∑—É–∞–ª—å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∏–¥–µ–æ —Å —Ç–æ—á–Ω–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–µ–π", "desc": "AVoCaDO ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–∞–∫ –≤–∏–∑—É–∞–ª—å–Ω—É—é, —Ç–∞
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#synthetic", "#optimization", "#dataset", "#cv"], "emoji": "üåê", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "DiT360 ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –±–∞–∑–µ DiT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞–Ω–æ—Ä–∞–º–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≥–∏–±—Ä–∏–¥–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–±—ã—á–Ω—ã—Ö –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –∏ 
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#rl", "#reasoning", "#optimization", "#data", "#math"], "emoji": "üî¢", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π reasoning –≤–º–µ—Å—Ç–æ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ AdaR –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ reasoning –≤ LLM –ø—É—Ç—ë–º –±–æ—Ä—å–±—ã —Å –ª–æ–∂–Ω–æ–π –ª–æ–≥–∏–∫–æ–π (spurious r
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#small_models", "#benchmark", "#reasoning", "#training", "#optimization", "#open_source", "#rl", "#dataset"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–≤–µ–ª–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è 
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#dataset", "#training", "#transfer_learning", "#multimodal"], "emoji": "üé®", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è, —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SVG –≥—Ä–∞—Ñ–∏–∫–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ InternSVG ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å 
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#benchmark", "#training", "#interpretability", "#data", "#dataset", "#transfer_learning", "#security", "#agents"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å AI-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —ç—Ç–∞–ø–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —É–≥—Ä–æ–∑—É –¥–æ –¥–µ–π—Å—Ç–≤–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AuraGen –∏ Safiron ‚Äî —Å–∏—Å—Ç–µ–º—É 
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#survey", "#agents", "#benchmark", "#reasoning"], "emoji": "üéì", "ru": {"title": "–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –æ—Å—Ç–∞—ë—Ç—Å—è –Ω–µ–ø–æ–∫–æ—Ä—ë–Ω–Ω–æ–π –≤–µ—Ä—à–∏–Ω–æ–π –¥–ª—è –ò–ò", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Acadreason –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∏ AI-–∞–≥–µ–Ω—Ç–æ–≤ –∫ –≥–ª—É–±–æ–∫–æ–º—É –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ –ø—è—Ç–∏ –Ω–∞—É—á–Ω—ã
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#optimization", "#training"], "emoji": "üåê", "ru": {"title": "–í–µ–±-–∞–≥–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –±—Ä–∞—É–∑–µ—Ä–æ–º –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫", "desc": "BrowserAgent ‚Äî —ç—Ç–æ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤–µ–±-–∞–≥–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å –±—Ä–∞—É–∑–µ—Ä–æ–º –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫, –∏—Å–ø–æ–ª—å–∑—É—è –¥–µ–π—Å—Ç–≤–∏—è –≤—Ä–æ–¥–µ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–∞, –∫–ª–∏–∫
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#survey", "#benchmark"], "emoji": "üìä", "ru": {"title": "LLM –ø—Ä–æ–≤–∞–ª–∏–ª–∏ —ç–∫–∑–∞–º–µ–Ω –ø–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º—É –∞—É–¥–∏—Ç—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ FinAuditing –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–±–æ—Ç–∞—Ç—å —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#dataset", "#data", "#agents", "#alignment"], "emoji": "üìÑ", "ru": {"title": "–ù–∞—É—á–∏—Ç—å AI –æ—Ç–ª–∏—á–∞—Ç—å –∫—Ä–∞—Å–∏–≤–æ –æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –æ—Ç –ø–ª–æ—Ö–æ –æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω–æ–≥–æ", "desc": "DocReward ‚Äî —ç—Ç–æ reward model –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Å—Ç–∏–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–≥–∞–µ—Ç AI-–∞–≥–µ
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#optimization", "#training", "#agents", "#transfer_learning", "#rl"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å–æ —Å—Ä–µ–¥–æ–π –≤–º–µ—Å—Ç–æ –≥–æ—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Environment Tuning ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é LLM-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ—à–∞–≥–æ–≤
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#interpretability", "#multimodal"], "emoji": "üîÑ", "ru": {"title": "–†–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GIR-Bench ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ä–µ–¥–∏–Ω
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#video", "#games", "#diffusion", "#multimodal"], "emoji": "üé•", "ru": {"title": "–í–∏–¥–µ–æ-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∫–∞–º–µ—Ä—ã –≤ 4D —Å—Ü–µ–Ω–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö Text-to-Video –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ—á–µ–∫ –æ–±–∑–æ—Ä–∞ –≤ 4D —Å—Ü–µ–Ω–∞
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#training", "#dataset", "#optimization", "#cv", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω—è—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "Vlaser ‚Äî —ç—Ç–æ Vision-Language-Action –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–µ–¥–∏–Ω—è–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–º —É–ø—Ä–∞–≤
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#rlhf", "#training", "#diffusion", "#rl", "#reinforcement_learning"], "emoji": "ü•™", "ru": {"title": "–°—ç–Ω–¥–≤–∏—á –∏–∑ –≥—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (dLLM) –º–æ–≥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, –Ω–æ –∏—Ö —Å–ª–æ–∂–Ω–æ –æ–±—É—á–∞—Ç—å —Å –ø
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#math", "#reasoning", "#open_source", "#dataset"], "emoji": "üìä", "ru": {"title": "–ö–æ–¥ –∫–∞–∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ CodePlot-CoT ‚Äî –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –Ω–µ 
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#hallucinations"], "emoji": "üëÅÔ∏è", "ru": {"title": "–ë–æ—Ä—å–±–∞ —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ —á–µ—Ä–µ–∑ –º–∞—Å–∫–∏—Ä–æ–≤–∫—É –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–±—ä–µ–∫—Ç–Ω—ã—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö vision-language –º–æ–¥–µ–ª—è—Ö (LVLM), –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –æ–ø–∏—Å—ã–≤–∞–µ—Ç –æ–±—ä–µ–∫—Ç—ã, –æ—Ç—Å—É
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#3d", "#multimodal", "#optimization", "#transfer_learning", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –∫ —Ä–æ–±–æ—Ç—É: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å–∏–º—É–ª—è—Ü–∏–π", "desc": "RoboSimGS - —ç—Ç–æ framework Real2Sim2Real, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞—ë—Ç –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã–µ —Å–∏–º—É–ª—è—Ü–∏–æ–Ω–Ω—ã–µ —Å—Ä–µ–¥—ã –¥–ª—è –æ–±
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#transfer_learning", "#training", "#optimization", "#open_source", "#synthetic"], "emoji": "üéØ", "ru": {"title": "–¶–µ–ª–µ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É –Ω–∞–≤—ã–∫–æ–≤ —É—á–∏—Ç–µ–ª–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞ STAT, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–∏–ª—å–Ω–æ–π —è–∑
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#rag", "#benchmark", "#training", "#optimization", "#multimodal", "#rl", "#games", "#agents"], "emoji": "üëÅÔ∏è", "ru": {"title": "–í–∏–∑—É–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥-–∫–æ–¥–∞", "desc": "ReLook ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ reinforcement learning, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#rl", "#benchmark", "#training", "#reasoning", "#optimization"], "emoji": "üéØ", "ru": {"title": "–ö–æ–Ω—Ç—Ä–æ–ª—å –¥–ª–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ —ç–Ω—Ç—Ä–æ–ø–∏—é –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–∞–∑–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —ç–Ω—Ç—Ä–æ–ø–∏—è –º–æ–¥–µ–ª–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –¥–ª–∏–Ω–æ–π –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: –≤—ã—Å–æ–∫–∞—è —ç–Ω—Ç—Ä–æ–ø
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#training", "#optimization", "#agents", "#transfer_learning"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ª–µ—Ç—É: –∫–∞–∫ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç—Å—è –Ω–∞ —Å–≤–æ–∏—Ö –æ—à–∏–±–∫–∞—Ö –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (TT-SI). –ú–æ–¥–µ–ª
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#benchmark", "#3d", "#optimization", "#diffusion", "#architecture"], "emoji": "üèÉ", "ru": {"title": "–ë—ã—Å—Ç—Ä–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ 3D-—Å–µ—Ç–∫–∏ —á–µ–ª–æ–≤–µ–∫–∞ —á–µ—Ä–µ–∑ —É–º–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–ª–æ—ë–≤ –∏ —Ç–æ–∫–µ–Ω–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –¥–≤–∞ –º–µ—Ç–æ–¥–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ transformer-–º–æ–¥–µ–ª–µ–π –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è 3D-—Å
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#alignment", "#ethics", "#multimodal", "#healthcare"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ü–∞–º—è—Ç—å LLM —É—Å–∏–ª–∏–≤–∞–µ—Ç —Å–æ—Ü–∏–∞–ª—å–Ω–æ–µ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —ç–º–æ—Ü–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ LLM –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –ø—Ä–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —ç–º–æ—Ü–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –∑–∞–≤–∏—Å
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#benchmark", "#story_generation", "#video", "#training", "#diffusion"], "emoji": "‚ôæÔ∏è", "ru": {"title": "–ë–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫—É —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫", "desc": "Stable Video Infinity (SVI) –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π –¥–ª–∏–Ω—ã —Å –≤—ã—Å–æ–∫–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#video", "#benchmark", "#inference", "#diffusion", "#dataset", "#alignment"], "emoji": "üé±", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç—É–∏—Ü–∏–∏ –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ LikePhys ‚Äî –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–π —Ñ–∏–∑–∏–∫–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö 
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#benchmark", "#interpretability", "#low_resource"], "emoji": "ü§ù", "ru": {"title": "–ß–µ–ª–æ–≤–µ–∫ –ø—Ä–æ—Ç–∏–≤ –º–∞—à–∏–Ω—ã: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ HUME ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –≤–ø–µ—Ä–≤—ã–µ —Å–∏—Å
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#agi", "#reasoning", "#agents"], "emoji": "üêù", "ru": {"title": "–†–æ–µ–≤–æ–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ SwarmSys ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ä–æ–µ–≤—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#alignment", "#rl", "#dataset", "#optimization", "#training"], "emoji": "üéØ", "ru": {"title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –±–µ–∑ —è–≤–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç framework –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –≤
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#multimodal", "#3d", "#open_source", "#diffusion", "#dataset", "#synthetic"], "emoji": "üë•", "ru": {"title": "–ë–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ 3D-–ª—é–¥–µ–π —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é AI-–º–æ–¥–µ–ª–µ–π", "desc": "InfiniHuman ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D-–∞–≤–∞—Ç–∞—Ä–æ–≤ –ª—é–¥–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#low_resource", "#reasoning", "#training", "#machine_translation", "#open_source", "#multilingual"], "emoji": "üåç", "ru": {"title": "–ü–µ—Ä–µ–≤–æ–¥ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –ø–æ—Å–ª–æ–π–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#synthetic", "#multimodal", "#benchmark", "#rag", "#diffusion"], "emoji": "üåê", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–∏—Å–∫–æ–º –∑–Ω–∞–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç World-To-Image ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è text-to-image –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—É—Ç—ë–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –∏–∑ –∏–Ω—Ç–µ—Ä–Ω
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#training", "#data", "#optimization", "#open_source", "#transfer_learning", "#rl"], "emoji": "‚ôªÔ∏è", "ru": {"title": "–ü–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RePro ‚Äî –º–µ—Ç–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç –Ω–µ–±–æ–ª—å—à—É—é —è
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#optimization", "#rl", "#dataset", "#games", "#synthetic", "#agents", "#training", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–í—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –ø–æ–≤–µ–¥–µ–Ω–∏—è –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Multimodal Policy Internalization (MPI) ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ 
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#training", "#optimization", "#open_source", "#diffusion", "#architecture", "#dataset", "#data"], "emoji": "üíä", "ru": {"title": "–ú–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã–π –¥–∏–∑–∞–π–Ω –ø–æ –ø—Ä–∏–º–µ—Ä–∞–º: DemoDiff —É—á–∏—Ç—Å—è —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–æ–ª–µ–∫—É–ª—ã –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π", "desc": "DemoDiff ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#cv", "#training", "#agents", "#robotics", "#games"], "emoji": "ü§ñ", "ru": {"title": "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ —Å —É–º–Ω–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–µ–π –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "VER ‚Äî —ç—Ç–æ Vision Expert Transformer –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞—ë—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫—É –≤–∏–∑—É
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#optimization", "#data", "#inference", "#benchmark", "#agents", "#reasoning", "#training", "#multimodal"], "emoji": "ü¶Ö", "ru": {"title": "–≠–∫–æ–Ω–æ–º–∏—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π: LLM –æ–±—É—á–∞—é—Ç –ª—ë–≥–∫–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–æ–≤", "desc": "Falconer ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#reasoning", "#training", "#benchmark", "#hallucinations"], "emoji": "‚è±Ô∏è", "ru": {"title": "–ö–æ–≥–¥–∞ AI –¥—É–º–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ: –ø—Ä–æ–±–ª–µ–º–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –º–∏—Ä–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –≤ —Å—Ç
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#video"], "emoji": "üé¨", "ru": {"title": "–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ AI-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º", "desc": "IVEBench ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç–æ–¥–æ–≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –î–∞—Ç–∞—Å–µ—Ç –≤–∫–ª—é—á–∞–µ—Ç 600 –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#hallucinations", "#long_context", "#multilingual", "#benchmark", "#optimization", "#small_models", "#architecture", "#training", "#agi"], "emoji": "üì±", "ru": {"title": "–ú–æ—â–Ω—ã–µ multimodal –º–æ–¥–µ–ª–∏ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω AndesVL ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö m
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#rl", "#training", "#benchmark", "#optimization", "#reasoning"], "emoji": "üéØ", "ru": {"title": "ViSurf: –æ–±—ä–µ–¥–∏–Ω—è—è –ª—É—á—à–µ–µ –∏–∑ supervision –∏ reinforcement –≤ –æ–¥–Ω–æ–º —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ViSurf ‚Äî –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LV
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#benchmark", "#open_source", "#security", "#architecture", "#dataset", "#data"], "emoji": "üîç", "ru": {"title": "–ö–∞–∂–¥–∞—è LLM –æ—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ—á–µ—Ä–∫ –≤ JavaScript-–∫–æ–¥–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç LLM-NodeJS –∏–∑ 50,000 –ø—Ä–æ–≥—Ä–∞–º–º –Ω–∞ JavaScript, —Å–≥–µ–Ω
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#security", "#alignment", "#dataset", "#benchmark", "#multimodal", "#ethics"], "emoji": "üé≠", "ru": {"title": "–†–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ –∞—Ç–∞–∫–∏ –≤—ã—è–≤–ª—è—é—Ç —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ CoBia ‚Äî –Ω–∞–±–æ—Ä –∞–¥–≤–µ—Ä—Å–∞—Ä–Ω—ã—Ö –∞—Ç–∞–∫ –¥–ª—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#optimization", "#rlhf", "#rl", "#security", "#benchmark"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –∞—Ç–∞–∫–∏ –ª–æ–º–∞—é—Ç –∑–∞—â–∏—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞—â–∏—Ç—ã –æ—Ç jailbreak-–∞—Ç–∞–∫ –∏ prompt injection –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ. –í–º
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#dataset", "#3d", "#data", "#optimization", "#open_source"], "emoji": "üåê", "ru": {"title": "–£–º–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –ª–∞–∑–µ—Ä–Ω—ã—Ö –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ —á–µ—Ä–µ–∑ —Å—Ñ–µ—Ä–∏—á–µ—Å–∫—É—é –ø—Ä–æ–µ–∫—Ü–∏—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –ø–æ–ª—É–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π pipeline –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –Ω–∞–∑–µ
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#multimodal", "#hallucinations", "#data", "#alignment", "#benchmark"], "emoji": "üîÄ", "ru": {"title": "–ö–æ–≥–¥–∞ –ø—Ä–æ—Å—Ç–æ–µ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Å–ª–æ–∂–Ω—ã–º: –ø—Ä–æ–±–ª–µ–º–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ñ–∞–∫—Ç–æ–≤ –≤ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ–Ω–∏ –º–æ–≥—É—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–≤–µ—Ç
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#video", "#diffusion", "#games", "#training", "#architecture", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ú–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–π –≤–∏–¥–µ–æ-–∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏", "desc": "MultiCOIN ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–ª–∞–≤–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ Diffusion Transformer
[14.10.2025 18:17] Using data from previous issue: {"categories": ["#agents", "#agi", "#reasoning", "#healthcare", "#interpretability", "#science", "#dataset"], "emoji": "üî¨", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–∞—Ç–æ–ª–æ–≥–∏–∏ —É—á–∏—Ç—Å—è —É —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø–∏—Å—å –∏—Ö –Ω–∞–≤–∏–≥–∞—Ü–∏–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ AI Session Recorder ‚Äî —Å–∏—Å—Ç–µ–º—É, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ–∑–∞–º–µ—Ç–Ω–æ –∑–∞–ø–∏—Å
[14.10.2025 18:17] Renaming data file.
[14.10.2025 18:17] Renaming previous data. hf_papers.json to ./d/2025-10-14.json
[14.10.2025 18:17] Saving new data file.
[14.10.2025 18:17] Generating page.
[14.10.2025 18:17] Renaming previous page.
[14.10.2025 18:17] Renaming previous data. index.html to ./d/2025-10-14.html
[14.10.2025 18:17] Writing result.
[14.10.2025 18:17] Renaming log file.
[14.10.2025 18:17] Renaming previous data. log.txt to ./logs/2025-10-14_last_log.txt
