[21.11.2024 14:10] Read previous papers.
[21.11.2024 14:10] Generating top page (month).
[21.11.2024 14:10] Writing top page (month).
[21.11.2024 15:10] Read previous papers.
[21.11.2024 15:10] Get feed.
[21.11.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.10958
[21.11.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.13503
[21.11.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.13281
[21.11.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.11922
[21.11.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.06559
[21.11.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.12811
[21.11.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.13476
[21.11.2024 15:10] Get page data from previous paper. URL: https://huggingface.co/papers/2411.13025
[21.11.2024 15:10] Downloading and parsing papers (pdf, html). Total: 8.
[21.11.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2411.10958.
[21.11.2024 15:10] Extra JSON file exists (./assets/json/2411.10958.json), skip PDF parsing.
[21.11.2024 15:10] Paper image links file exists (./assets/img_data/2411.10958.json), skip HTML parsing.
[21.11.2024 15:10] Success.
[21.11.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2411.13503.
[21.11.2024 15:10] Extra JSON file exists (./assets/json/2411.13503.json), skip PDF parsing.
[21.11.2024 15:10] Paper image links file exists (./assets/img_data/2411.13503.json), skip HTML parsing.
[21.11.2024 15:10] Success.
[21.11.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2411.13281.
[21.11.2024 15:10] Extra JSON file exists (./assets/json/2411.13281.json), skip PDF parsing.
[21.11.2024 15:10] Paper image links file exists (./assets/img_data/2411.13281.json), skip HTML parsing.
[21.11.2024 15:10] Success.
[21.11.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2411.11922.
[21.11.2024 15:10] Extra JSON file exists (./assets/json/2411.11922.json), skip PDF parsing.
[21.11.2024 15:10] Paper image links file exists (./assets/img_data/2411.11922.json), skip HTML parsing.
[21.11.2024 15:10] Success.
[21.11.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2411.06559.
[21.11.2024 15:10] Extra JSON file exists (./assets/json/2411.06559.json), skip PDF parsing.
[21.11.2024 15:10] Paper image links file exists (./assets/img_data/2411.06559.json), skip HTML parsing.
[21.11.2024 15:10] Success.
[21.11.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2411.12811.
[21.11.2024 15:10] Extra JSON file exists (./assets/json/2411.12811.json), skip PDF parsing.
[21.11.2024 15:10] Paper image links file exists (./assets/img_data/2411.12811.json), skip HTML parsing.
[21.11.2024 15:10] Success.
[21.11.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2411.13476.
[21.11.2024 15:10] Extra JSON file exists (./assets/json/2411.13476.json), skip PDF parsing.
[21.11.2024 15:10] Paper image links file exists (./assets/img_data/2411.13476.json), skip HTML parsing.
[21.11.2024 15:10] Success.
[21.11.2024 15:10] Downloading and parsing paper https://huggingface.co/papers/2411.13025.
[21.11.2024 15:10] Extra JSON file exists (./assets/json/2411.13025.json), skip PDF parsing.
[21.11.2024 15:10] Paper image links file exists (./assets/img_data/2411.13025.json), skip HTML parsing.
[21.11.2024 15:10] Success.
[21.11.2024 15:10] Enriching papers with extra data.
[21.11.2024 15:10] ********************************************************************************
[21.11.2024 15:10] Abstract 0. Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. SageAttention utilizes 8-bit matrix multiplication, 16-bit matrix multiplication with 16-bit accumulator, and precision-enhancing methods, implementing an accurate and 2...
[21.11.2024 15:10] ********************************************************************************
[21.11.2024 15:10] Abstract 1. Video generation has witnessed significant advancements, yet evaluating these models remains a challenge. A comprehensive evaluation benchmark for video generation is indispensable for two reasons: 1) Existing metrics do not fully align with human perceptions; 2) An ideal evaluation system should pr...
[21.11.2024 15:10] ********************************************************************************
[21.11.2024 15:10] Abstract 2. Large multimodal models (LMMs) with advanced video analysis capabilities have recently garnered significant attention. However, most evaluations rely on traditional methods like multiple-choice questions in benchmarks such as VideoMME and LongVideoBench, which are prone to lack the depth needed to c...
[21.11.2024 15:10] ********************************************************************************
[21.11.2024 15:10] Abstract 3. The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when managing crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original mo...
[21.11.2024 15:10] ********************************************************************************
[21.11.2024 15:10] Abstract 4. Language agents have demonstrated promising capabilities in automating web-based tasks, though their current reactive approaches still underperform largely compared to humans. While incorporating advanced planning algorithms, particularly tree search methods, could enhance these agents' performance,...
[21.11.2024 15:10] ********************************************************************************
[21.11.2024 15:10] Abstract 5. Diffusion models excel in image generation, but controlling them remains a challenge. We focus on the problem of style-conditioned image generation. Although example images work, they are cumbersome: srefs (style-reference codes) from MidJourney solve this issue by expressing a specific image style ...
[21.11.2024 15:10] ********************************************************************************
[21.11.2024 15:10] Abstract 6. Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks. Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training. However, we observe...
[21.11.2024 15:10] ********************************************************************************
[21.11.2024 15:10] Abstract 7. The objective of Radiology Report Generation (RRG) is to automatically generate coherent textual analyses of diseases based on radiological images, thereby alleviating the workload of radiologists. Current AI-based methods for RRG primarily focus on modifications to the encoder-decoder model archite...
[21.11.2024 15:10] Read previous papers.
[21.11.2024 15:10] Generating reviews via LLM API.
[21.11.2024 15:10] Using data from previous issue: {"categories": ["#video", "#inference", "#training", "#optimization", "#cv"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä–µ–µ –∏ —Ç–æ—á–Ω–µ–µ: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "SageAttention2 - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 4-–±–∏—Ç–Ω–æ–µ –º–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ –∏ 
[21.11.2024 15:10] Using data from previous issue: {"categories": ["#open_source", "#alignment", "#benchmark", "#video"], "emoji": "üé•", "ru": {"title": "VBench: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "VBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. –û–Ω —Ä–∞–∑–±–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ 16 –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å
[21.11.2024 15:10] Using data from previous issue: {"categories": ["#interpretability", "#multimodal", "#optimization", "#benchmark", "#video", "#games"], "emoji": "üé•", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ—Ü–µ–Ω–∫–µ –≤–∏–¥–µ–æ–∞–Ω–∞–ª–∏–∑–∞: –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä–µ–Ω–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VideoAutoArena - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –º—É–ª—å—Ç–∏–º–æ–¥
[21.11.2024 15:10] Using data from previous issue: {"categories": ["#transfer_learning", "#video", "#benchmark", "#optimization", "#cv"], "emoji": "ü•∑", "ru": {"title": "SAMURAI: –¢–æ—á–Ω–æ–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "SAMURAI - —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ SAM 2, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ–±—ä–µ
[21.11.2024 15:10] Using data from previous issue: {"categories": ["#architecture", "#agents", "#optimization", "#benchmark"], "emoji": "üåê", "ru": {"title": "WebDreamer: LLM –∫–∞–∫ –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ WebDreamer –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –≤–µ–±-–∑–∞–¥–∞—á. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª—å—à–∏–µ —è
[21.11.2024 15:10] Using data from previous issue: {"categories": ["#cv", "#architecture", "#open_source", "#diffusion", "#dataset"], "emoji": "üé®", "ru": {"title": "StyleCodes: –æ—Ç–∫—Ä—ã—Ç—ã–π —Å–ø–æ—Å–æ–± –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ç–∏–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–º —Å—Ç–∏–ª–µ–º –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç
[21.11.2024 15:10] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization", "#long_context"], "emoji": "‚öì", "ru": {"title": "AnchorAttention: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º AnchorAttention –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã
[21.11.2024 15:10] Using data from previous issue: {"categories": ["#multimodal", "#graphs", "#dataset", "#games", "#architecture"], "emoji": "üè•", "ru": {"title": "–£–º–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –æ—Ä–≥–∞–Ω—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π ORID (Organ-Regional
[21.11.2024 15:10] Loading Chinese text from previous data.
[21.11.2024 15:10] Renaming data file.
[21.11.2024 15:10] Renaming previous data. hf_papers.json to ./d/2024-11-21.json
[21.11.2024 15:10] Saving new data file.
[21.11.2024 15:10] Generating page.
[21.11.2024 15:10] Renaming previous page.
[21.11.2024 15:10] Renaming previous data. index.html to ./d/2024-11-21.html
[21.11.2024 15:10] [Experimental] Generating Chinese page for reading.
[21.11.2024 15:10] Chinese vocab [{'word': 'Âä†ÈÄü', 'pinyin': 'jiƒÅ s√π', 'trans': 'accelerate'}, {'word': 'Ê≥®ÊÑèÂäõ', 'pinyin': 'zh√π y√¨ l√¨', 'trans': 'attention'}, {'word': 'ËÆ°ÁÆó', 'pinyin': 'j√¨ su√†n', 'trans': 'calculation'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅng f«é', 'trans': 'method'}, {'word': 'Áß∞‰∏∫', 'pinyin': 'chƒìng w√©i', 'trans': 'called'}, {'word': 'Áü©Èòµ', 'pinyin': 'j«î zh√®n', 'trans': 'matrix'}, {'word': '‰πòÊ≥ï', 'pinyin': 'ch√©n f«é', 'trans': 'multiplication'}, {'word': 'Á≤æÂ∫¶', 'pinyin': 'jƒ´ng d√π', 'trans': 'precision'}, {'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìng qi√°ng', 'trans': 'enhancement'}, {'word': 'ÊäÄÊúØ', 'pinyin': 'j√¨ sh√π', 'trans': 'technology'}, {'word': 'ÈáèÂåñ', 'pinyin': 'li√†ng hu√†', 'trans': 'quantization'}, {'word': 'Âπ≥Êªë', 'pinyin': 'p√≠ng hu√°', 'trans': 'smooth'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠ gƒÅo', 'trans': 'improve'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠ y√†n', 'trans': 'experiment'}, {'word': 'ËØÅÊòé', 'pinyin': 'zh√®ng m√≠ng', 'trans': 'prove'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ng n√©ng', 'trans': 'performance'}, {'word': 'ÊçüÂ§±', 'pinyin': 's«în shƒ´', 'trans': 'loss'}, {'word': 'Êìç‰Ωú', 'pinyin': 'cƒÅo zu√≤', 'trans': 'operation'}, {'word': 'ÈÄüÂ∫¶', 'pinyin': 's√π d√π', 'trans': 'speed'}, {'word': 'ÂÖ¨ÂºÄ', 'pinyin': 'g≈çng kƒÅi', 'trans': 'public'}, {'word': '‰ª£Á†Å', 'pinyin': 'd√†i m«é', 'trans': 'code'}]
[21.11.2024 15:10] Renaming previous Chinese page.
[21.11.2024 15:10] Renaming previous data. zh.html to ./d/2024-11-20_zh_reading_task.html
[21.11.2024 15:10] Writing Chinese reading task.
[21.11.2024 15:10] Writing result.
[21.11.2024 15:10] Renaming log file.
[21.11.2024 15:10] Renaming previous data. log.txt to ./logs/2024-11-21_last_log.txt
