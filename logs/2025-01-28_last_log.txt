[28.01.2025 06:16] Read previous papers.
[28.01.2025 06:16] Generating top page (month).
[28.01.2025 06:16] Writing top page (month).
[28.01.2025 07:09] Read previous papers.
[28.01.2025 07:09] Get feed.
[28.01.2025 07:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15368
[28.01.2025 07:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15383
[28.01.2025 07:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15369
[28.01.2025 07:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.16295
[28.01.2025 07:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.16142
[28.01.2025 07:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.14912
[28.01.2025 07:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.01.2025 07:09] No deleted papers detected.
[28.01.2025 07:09] Downloading and parsing papers (pdf, html). Total: 6.
[28.01.2025 07:09] Downloading and parsing paper https://huggingface.co/papers/2501.15368.
[28.01.2025 07:09] Extra JSON file exists (./assets/json/2501.15368.json), skip PDF parsing.
[28.01.2025 07:09] Paper image links file exists (./assets/img_data/2501.15368.json), skip HTML parsing.
[28.01.2025 07:09] Success.
[28.01.2025 07:09] Downloading and parsing paper https://huggingface.co/papers/2501.15383.
[28.01.2025 07:09] Extra JSON file exists (./assets/json/2501.15383.json), skip PDF parsing.
[28.01.2025 07:09] Paper image links file exists (./assets/img_data/2501.15383.json), skip HTML parsing.
[28.01.2025 07:09] Success.
[28.01.2025 07:09] Downloading and parsing paper https://huggingface.co/papers/2501.15369.
[28.01.2025 07:09] Extra JSON file exists (./assets/json/2501.15369.json), skip PDF parsing.
[28.01.2025 07:09] Paper image links file exists (./assets/img_data/2501.15369.json), skip HTML parsing.
[28.01.2025 07:09] Success.
[28.01.2025 07:09] Downloading and parsing paper https://huggingface.co/papers/2501.16295.
[28.01.2025 07:09] Extra JSON file exists (./assets/json/2501.16295.json), skip PDF parsing.
[28.01.2025 07:09] Paper image links file exists (./assets/img_data/2501.16295.json), skip HTML parsing.
[28.01.2025 07:09] Success.
[28.01.2025 07:09] Downloading and parsing paper https://huggingface.co/papers/2501.16142.
[28.01.2025 07:09] Extra JSON file exists (./assets/json/2501.16142.json), skip PDF parsing.
[28.01.2025 07:09] Paper image links file exists (./assets/img_data/2501.16142.json), skip HTML parsing.
[28.01.2025 07:09] Success.
[28.01.2025 07:09] Downloading and parsing paper https://huggingface.co/papers/2501.14912.
[28.01.2025 07:09] Extra JSON file exists (./assets/json/2501.14912.json), skip PDF parsing.
[28.01.2025 07:09] Paper image links file exists (./assets/img_data/2501.14912.json), skip HTML parsing.
[28.01.2025 07:09] Success.
[28.01.2025 07:09] Enriching papers with extra data.
[28.01.2025 07:09] ********************************************************************************
[28.01.2025 07:09] Abstract 0. We introduce Baichuan-Omni-1.5, an omni-modal model that not only has omni-modal understanding capabilities but also provides end-to-end audio generation capabilities. To achieve fluent and high-quality interaction across modalities without compromising the capabilities of any modality, we prioritiz...
[28.01.2025 07:09] ********************************************************************************
[28.01.2025 07:09] Abstract 1. We introduce Qwen2.5-1M, a series of models that extend the context length to 1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series have significantly enhanced long-context capabilities through long-context pre-training and post-training. Key techniques such as long data syn...
[28.01.2025 07:09] ********************************************************************************
[28.01.2025 07:09] Abstract 2. We present a new family of mobile hybrid vision networks, called iFormer, with a focus on optimizing latency and accuracy on mobile applications. iFormer effectively integrates the fast local representation capacity of convolution with the efficient global modeling ability of self-attention. The loc...
[28.01.2025 07:09] ********************************************************************************
[28.01.2025 07:09] Abstract 3. State Space Models (SSMs) have emerged as efficient alternatives to Transformers for sequential modeling, but their inability to leverage modality-specific features limits their performance in multi-modal pretraining. Here, we propose Mixture-of-Mamba, a novel SSM architecture that introduces modali...
[28.01.2025 07:09] ********************************************************************************
[28.01.2025 07:09] Abstract 4. Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive gener...
[28.01.2025 07:09] ********************************************************************************
[28.01.2025 07:09] Abstract 5. We introduce Feasible Learning (FL), a sample-centric learning paradigm where models are trained by solving a feasibility problem that bounds the loss for each training sample. In contrast to the ubiquitous Empirical Risk Minimization (ERM) framework, which optimizes for average performance, FL dema...
[28.01.2025 07:09] Read previous papers.
[28.01.2025 07:09] Generating reviews via LLM API.
[28.01.2025 07:09] Using data from previous issue: {"categories": ["#data", "#optimization", "#dataset", "#training", "#audio", "#multimodal"], "emoji": "üé≠", "ru": {"title": "Baichuan-Omni-1.5: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò", "desc": "Baichuan-Omni-1.5 - —ç—Ç–æ –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –æ–±–ª–∞–¥–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ. –î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∫–∞—á–µ—Å
[28.01.2025 07:09] Using data from previous issue: {"categories": ["#architecture", "#inference", "#long_context", "#training", "#open_source"], "emoji": "üöÄ", "ru": {"title": "–ú–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π Qwen2.5-1M —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–∫–Ω–æ–º –≤ 1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ —Ç–µ—Ö–Ω–∏–∫–∏ —Å–∏–Ω
[28.01.2025 07:09] Using data from previous issue: {"categories": ["#optimization", "#training", "#cv", "#architecture"], "emoji": "üì±", "ru": {"title": "iFormer: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –º–æ–±–∏–ª—å–Ω–æ–≥–æ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "iFormer - —ç—Ç–æ –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–±–∏–ª—å–Ω—ã—Ö –≥–∏–±—Ä–∏–¥–Ω—ã—Ö —Å–µ—Ç–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π.
[28.01.2025 07:09] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#benchmark"], "emoji": "üß†", "ru": {"title": "Mixture-of-Mamba: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å-—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å—é", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π (SSM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Mixture-of-Mamb
[28.01.2025 07:09] Using data from previous issue: {"categories": ["#optimization", "#rl", "#benchmark", "#training", "#games"], "emoji": "ü§ñ", "ru": {"title": "MR.Q: –ù–∞ –ø—É—Ç–∏ –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º MR.Q. –≠—Ç–æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª—å
[28.01.2025 07:09] Using data from previous issue: {"categories": ["#training", "#optimization"], "emoji": "üéØ", "ru": {"title": "–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–∞–∂–¥–æ–º—É –æ–±—Ä–∞–∑—Ü—É –¥–∞–Ω–Ω—ã—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è - Feasible Learning (FL). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–≥–æ —Ä–∏—Å
[28.01.2025 07:09] Loading Chinese text from previous data.
[28.01.2025 07:09] Renaming data file.
[28.01.2025 07:09] Renaming previous data. hf_papers.json to ./d/2025-01-28.json
[28.01.2025 07:09] Saving new data file.
[28.01.2025 07:09] Generating page.
[28.01.2025 07:09] Renaming previous page.
[28.01.2025 07:09] Renaming previous data. index.html to ./d/2025-01-28.html
[28.01.2025 07:09] [Experimental] Generating Chinese page for reading.
[28.01.2025 07:09] Chinese vocab [{'word': 'ËÆ®ËÆ∫', 'pinyin': 't«éo l√πn', 'trans': 'discuss'}, {'word': 'Â§ßÂûã', 'pinyin': 'd√† x√≠ng', 'trans': 'large-scale'}, {'word': 'ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'y«î y√°n m√≥ x√≠ng', 'trans': 'language model'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluate'}, {'word': 'ÈáçË¶ÅÊÄß', 'pinyin': 'zh√≤ng y√†o x√¨ng', 'trans': 'importance'}, {'word': 'Áé∞Êúâ', 'pinyin': 'xi√†n y«íu', 'trans': 'existing'}, {'word': 'Âü∫ÂáÜÊµãËØï', 'pinyin': 'jƒ´ zh«în c√® sh√¨', 'trans': 'benchmark test'}, {'word': 'Êó†Ê≥ï', 'pinyin': 'w√∫ f«é', 'trans': 'unable'}, {'word': 'Ë∑ü‰∏ä', 'pinyin': 'gƒìn sh√†ng', 'trans': 'keep up with'}, {'word': 'ËøõÊ≠•', 'pinyin': 'j√¨n b√π', 'trans': 'progress'}, {'word': 'ÊµÅË°å', 'pinyin': 'li√∫ x√≠ng', 'trans': 'popular'}, {'word': 'ÂáÜÁ°ÆÁéá', 'pinyin': 'zh«în qu√® l«ú', 'trans': 'accuracy rate'}, {'word': 'Ëß£ÂÜ≥', 'pinyin': 'jiƒõ ju√©', 'trans': 'solve'}, {'word': 'ÂºïÂÖ•', 'pinyin': 'y«ên r√π', 'trans': 'introduce'}, {'word': '‰∫∫Á±ª', 'pinyin': 'r√©n l√®i', 'trans': 'human'}, {'word': 'ÊúÄÂêé', 'pinyin': 'zu√¨ h√≤u', 'trans': 'final'}, {'word': 'ËÄÉËØï', 'pinyin': 'k«éo sh√¨', 'trans': 'exam'}, {'word': 'Ë∑®Â≠¶Áßë', 'pinyin': 'ku√† xu√© kƒì', 'trans': 'interdisciplinary'}, {'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ t√†i', 'trans': 'multimodal'}, {'word': 'ÂåÖÂê´', 'pinyin': 'bƒÅo h√°n', 'trans': 'contain'}, {'word': 'Êï∞Â≠¶', 'pinyin': 'sh√π xu√©', 'trans': 'mathematics'}, {'word': '‰∫∫ÊñáÂ≠¶Áßë', 'pinyin': 'r√©n w√©n xu√© kƒì', 'trans': 'humanities'}, {'word': 'Ëá™ÁÑ∂ÁßëÂ≠¶', 'pinyin': 'z√¨ r√°n kƒì xu√©', 'trans': 'natural sciences'}, {'word': 'È¢ÜÂüü', 'pinyin': 'l«êng y√π', 'trans': 'field'}, {'word': 'ÂºÄÂèë', 'pinyin': 'kƒÅi fƒÅ', 'trans': 'develop'}, {'word': '‰∏ìÂÆ∂', 'pinyin': 'zhuƒÅn jiƒÅ', 'trans': 'expert'}, {'word': 'ÊòéÁ°Æ', 'pinyin': 'm√≠ng qu√®', 'trans': 'clear'}, {'word': '‰∫íËÅîÁΩë', 'pinyin': 'h√π li√°n w«éng', 'trans': 'internet'}]
[28.01.2025 07:09] Renaming previous Chinese page.
[28.01.2025 07:09] Renaming previous data. zh.html to ./d/2025-01-27_zh_reading_task.html
[28.01.2025 07:09] Writing Chinese reading task.
[28.01.2025 07:09] Writing result.
[28.01.2025 07:09] Renaming log file.
[28.01.2025 07:09] Renaming previous data. log.txt to ./logs/2025-01-28_last_log.txt
