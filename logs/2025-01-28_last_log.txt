[28.01.2025 09:11] Read previous papers.
[28.01.2025 09:11] Generating top page (month).
[28.01.2025 09:11] Writing top page (month).
[28.01.2025 10:10] Read previous papers.
[28.01.2025 10:10] Get feed.
[28.01.2025 10:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15368
[28.01.2025 10:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15383
[28.01.2025 10:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15570
[28.01.2025 10:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.16142
[28.01.2025 10:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15369
[28.01.2025 10:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.16295
[28.01.2025 10:10] Get page data from previous paper. URL: https://huggingface.co/papers/2501.14912
[28.01.2025 10:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.01.2025 10:10] No deleted papers detected.
[28.01.2025 10:10] Downloading and parsing papers (pdf, html). Total: 7.
[28.01.2025 10:10] Downloading and parsing paper https://huggingface.co/papers/2501.15368.
[28.01.2025 10:10] Extra JSON file exists (./assets/json/2501.15368.json), skip PDF parsing.
[28.01.2025 10:10] Paper image links file exists (./assets/img_data/2501.15368.json), skip HTML parsing.
[28.01.2025 10:10] Success.
[28.01.2025 10:10] Downloading and parsing paper https://huggingface.co/papers/2501.15383.
[28.01.2025 10:10] Extra JSON file exists (./assets/json/2501.15383.json), skip PDF parsing.
[28.01.2025 10:10] Paper image links file exists (./assets/img_data/2501.15383.json), skip HTML parsing.
[28.01.2025 10:10] Success.
[28.01.2025 10:10] Downloading and parsing paper https://huggingface.co/papers/2501.15570.
[28.01.2025 10:10] Extra JSON file exists (./assets/json/2501.15570.json), skip PDF parsing.
[28.01.2025 10:10] Paper image links file exists (./assets/img_data/2501.15570.json), skip HTML parsing.
[28.01.2025 10:10] Success.
[28.01.2025 10:10] Downloading and parsing paper https://huggingface.co/papers/2501.16142.
[28.01.2025 10:10] Extra JSON file exists (./assets/json/2501.16142.json), skip PDF parsing.
[28.01.2025 10:10] Paper image links file exists (./assets/img_data/2501.16142.json), skip HTML parsing.
[28.01.2025 10:10] Success.
[28.01.2025 10:10] Downloading and parsing paper https://huggingface.co/papers/2501.15369.
[28.01.2025 10:10] Extra JSON file exists (./assets/json/2501.15369.json), skip PDF parsing.
[28.01.2025 10:10] Paper image links file exists (./assets/img_data/2501.15369.json), skip HTML parsing.
[28.01.2025 10:10] Success.
[28.01.2025 10:10] Downloading and parsing paper https://huggingface.co/papers/2501.16295.
[28.01.2025 10:10] Extra JSON file exists (./assets/json/2501.16295.json), skip PDF parsing.
[28.01.2025 10:10] Paper image links file exists (./assets/img_data/2501.16295.json), skip HTML parsing.
[28.01.2025 10:10] Success.
[28.01.2025 10:10] Downloading and parsing paper https://huggingface.co/papers/2501.14912.
[28.01.2025 10:10] Extra JSON file exists (./assets/json/2501.14912.json), skip PDF parsing.
[28.01.2025 10:10] Paper image links file exists (./assets/img_data/2501.14912.json), skip HTML parsing.
[28.01.2025 10:10] Success.
[28.01.2025 10:10] Enriching papers with extra data.
[28.01.2025 10:10] ********************************************************************************
[28.01.2025 10:10] Abstract 0. We introduce Baichuan-Omni-1.5, an omni-modal model that not only has omni-modal understanding capabilities but also provides end-to-end audio generation capabilities. To achieve fluent and high-quality interaction across modalities without compromising the capabilities of any modality, we prioritiz...
[28.01.2025 10:10] ********************************************************************************
[28.01.2025 10:10] Abstract 1. We introduce Qwen2.5-1M, a series of models that extend the context length to 1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series have significantly enhanced long-context capabilities through long-context pre-training and post-training. Key techniques such as long data syn...
[28.01.2025 10:10] ********************************************************************************
[28.01.2025 10:10] Abstract 2. As is known, hybrid quadratic and subquadratic attention models in multi-head architectures have surpassed both Transformer and Linear RNN models , with these works primarily focusing on reducing KV complexity and improving efficiency. For further research on expressiveness, we introduce our series ...
[28.01.2025 10:10] ********************************************************************************
[28.01.2025 10:10] Abstract 3. Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive gener...
[28.01.2025 10:10] ********************************************************************************
[28.01.2025 10:10] Abstract 4. We present a new family of mobile hybrid vision networks, called iFormer, with a focus on optimizing latency and accuracy on mobile applications. iFormer effectively integrates the fast local representation capacity of convolution with the efficient global modeling ability of self-attention. The loc...
[28.01.2025 10:10] ********************************************************************************
[28.01.2025 10:10] Abstract 5. State Space Models (SSMs) have emerged as efficient alternatives to Transformers for sequential modeling, but their inability to leverage modality-specific features limits their performance in multi-modal pretraining. Here, we propose Mixture-of-Mamba, a novel SSM architecture that introduces modali...
[28.01.2025 10:10] ********************************************************************************
[28.01.2025 10:10] Abstract 6. We introduce Feasible Learning (FL), a sample-centric learning paradigm where models are trained by solving a feasibility problem that bounds the loss for each training sample. In contrast to the ubiquitous Empirical Risk Minimization (ERM) framework, which optimizes for average performance, FL dema...
[28.01.2025 10:10] Read previous papers.
[28.01.2025 10:10] Generating reviews via LLM API.
[28.01.2025 10:10] Using data from previous issue: {"categories": ["#data", "#optimization", "#dataset", "#training", "#audio", "#multimodal"], "emoji": "üé≠", "ru": {"title": "Baichuan-Omni-1.5: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò", "desc": "Baichuan-Omni-1.5 - —ç—Ç–æ –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –æ–±–ª–∞–¥–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ. –î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∫–∞—á–µ—Å
[28.01.2025 10:10] Using data from previous issue: {"categories": ["#architecture", "#inference", "#long_context", "#training", "#open_source"], "emoji": "üöÄ", "ru": {"title": "–ú–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π Qwen2.5-1M —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–∫–Ω–æ–º –≤ 1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ —Ç–µ—Ö–Ω–∏–∫–∏ —Å–∏–Ω
[28.01.2025 10:10] Using data from previous issue: {"categories": ["#transfer_learning", "#training", "#architecture", "#small_models", "#optimization", "#open_source"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ RNN —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∑–Ω–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —á–∏—Å—Ç–æ–º –Ω–∞—Ç–∏–≤–Ω–æ–º –≤–Ω–∏–º
[28.01.2025 10:10] Using data from previous issue: {"categories": ["#optimization", "#rl", "#benchmark", "#training", "#games"], "emoji": "ü§ñ", "ru": {"title": "MR.Q: –ù–∞ –ø—É—Ç–∏ –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º MR.Q. –≠—Ç–æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª—å
[28.01.2025 10:10] Using data from previous issue: {"categories": ["#optimization", "#training", "#cv", "#architecture"], "emoji": "üì±", "ru": {"title": "iFormer: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –º–æ–±–∏–ª—å–Ω–æ–≥–æ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "iFormer - —ç—Ç–æ –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–±–∏–ª—å–Ω—ã—Ö –≥–∏–±—Ä–∏–¥–Ω—ã—Ö —Å–µ—Ç–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π.
[28.01.2025 10:10] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#benchmark"], "emoji": "üß†", "ru": {"title": "Mixture-of-Mamba: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å-—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å—é", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π (SSM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Mixture-of-Mamb
[28.01.2025 10:10] Using data from previous issue: {"categories": ["#training", "#optimization"], "emoji": "üéØ", "ru": {"title": "–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–∞–∂–¥–æ–º—É –æ–±—Ä–∞–∑—Ü—É –¥–∞–Ω–Ω—ã—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è - Feasible Learning (FL). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–≥–æ —Ä–∏—Å
[28.01.2025 10:10] Loading Chinese text from previous data.
[28.01.2025 10:10] Renaming data file.
[28.01.2025 10:10] Renaming previous data. hf_papers.json to ./d/2025-01-28.json
[28.01.2025 10:10] Saving new data file.
[28.01.2025 10:10] Generating page.
[28.01.2025 10:10] Renaming previous page.
[28.01.2025 10:10] Renaming previous data. index.html to ./d/2025-01-28.html
[28.01.2025 10:10] [Experimental] Generating Chinese page for reading.
[28.01.2025 10:10] Chinese vocab [{'word': '‰ªãÁªç', 'pinyin': 'ji√® sh√†o', 'trans': 'introduce'}, {'word': 'Baichuan-Omni-1.5', 'pinyin': 'B√†i chuƒÅn-≈åu m√≠-1.5', 'trans': 'Baichuan-Omni-1.5'}, {'word': 'ÂÖ∑Êúâ', 'pinyin': 'j√π y«íu', 'trans': 'have'}, {'word': 'ÂÖ®Ê®°ÊÄÅ', 'pinyin': 'qu√°n m√≥ sh√¨', 'trans': 'full modality'}, {'word': 'ÁêÜËß£', 'pinyin': 'l«ê jiƒõ', 'trans': 'understanding'}, {'word': 'Á´ØÂà∞Á´Ø', 'pinyin': 'duƒÅn d√†o duƒÅn', 'trans': 'end-to-end'}, {'word': 'Èü≥È¢ë', 'pinyin': 'yƒ´n p√≠n', 'trans': 'audio'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generate'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}, {'word': 'ÊµÅÁïÖ', 'pinyin': 'li√∫ ch√†ng', 'trans': 'smooth'}, {'word': 'È´òË¥®Èáè', 'pinyin': 'gƒÅo zh√¨ li√†ng', 'trans': 'high quality'}, {'word': 'Ë∑®Ê®°ÊÄÅ', 'pinyin': 'ku√† m√≥ sh√¨', 'trans': 'cross-modality'}, {'word': '‰∫§‰∫í', 'pinyin': 'jiƒÅo h√π', 'trans': 'interaction'}, {'word': '‰ºòÂåñ', 'pinyin': 'y≈çu hu√†', 'trans': 'optimize'}, {'word': 'ÂÖ≥ÈîÆ', 'pinyin': 'gu«én ji√†n', 'trans': 'key'}, {'word': 'ÊñπÈù¢', 'pinyin': 'fƒÅng mi√†n', 'trans': 'aspect'}, {'word': 'Âª∫Á´ã', 'pinyin': 'ji√†n l√¨', 'trans': 'establish'}, {'word': 'ÂÖ®Èù¢', 'pinyin': 'qu√°n mi√†n', 'trans': 'comprehensive'}, {'word': 'Êï∞ÊçÆ', 'pinyin': 'sh√π j√π', 'trans': 'data'}, {'word': 'Ê∏ÖÊ¥ó', 'pinyin': 'qƒ´ng x«ê', 'trans': 'clean'}, {'word': 'ÂêàÊàê', 'pinyin': 'h√© ch√©ng', 'trans': 'synthesize'}, {'word': 'ÁÆ°ÈÅì', 'pinyin': 'gu«én d√†o', 'trans': 'pipeline'}, {'word': 'Ëé∑Âæó', 'pinyin': 'hu√≤ d√©', 'trans': 'obtain'}, {'word': 'Â§ßÁ∫¶', 'pinyin': 'd√† yuƒì', 'trans': 'about'}, {'word': 'ÊñáÊú¨', 'pinyin': 'w√©n bƒõn', 'trans': 'text'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'visual'}, {'word': 'ËÆæËÆ°', 'pinyin': 'sh√® j√¨', 'trans': 'design'}, {'word': 'ÂàÜËØçÂô®', 'pinyin': 'fƒìn c√≠ q√¨', 'trans': 'tokenizer'}, {'word': 'ÊçïÊçâ', 'pinyin': 'b«î zhu≈ç', 'trans': 'capture'}, {'word': 'ËØ≠‰πâ', 'pinyin': 'y«î y√¨', 'trans': 'semantics'}, {'word': 'Â£∞Â≠¶', 'pinyin': 'shƒìng xu√©', 'trans': 'acoustics'}, {'word': '‰ø°ÊÅØ', 'pinyin': 'x√¨n xƒ´', 'trans': 'information'}, {'word': 'Â§öÈò∂ÊÆµ', 'pinyin': 'du≈ç jiƒì du√†n', 'trans': 'multi-stage'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πn li√†n', 'trans': 'training'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'Á°Æ‰øù', 'pinyin': 'qu√® b«éo', 'trans': 'ensure'}, {'word': 'ÊúâÊïà', 'pinyin': 'y«íu xi√†o', 'trans': 'effective'}, {'word': 'ÂçèÂêå', 'pinyin': 'xi√© t√≥ng', 'trans': 'coordination'}, {'word': 'È¢ÜÂÖà', 'pinyin': 'l«êng xiƒÅn', 'trans': 'lead'}, {'word': 'ÂΩìÂâç', 'pinyin': 'dƒÅng qi√°n', 'trans': 'current'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ÂèñÂæó', 'pinyin': 'q«î d√©', 'trans': 'achieve'}, {'word': 'ÂèØÊØî', 'pinyin': 'kƒõ b«ê', 'trans': 'comparable'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}]
[28.01.2025 10:10] Renaming previous Chinese page.
[28.01.2025 10:10] Renaming previous data. zh.html to ./d/2025-01-27_zh_reading_task.html
[28.01.2025 10:10] Writing Chinese reading task.
[28.01.2025 10:10] Writing result.
[28.01.2025 10:10] Renaming log file.
[28.01.2025 10:10] Renaming previous data. log.txt to ./logs/2025-01-28_last_log.txt
