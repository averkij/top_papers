[28.01.2025 20:11] Read previous papers.
[28.01.2025 20:11] Generating top page (month).
[28.01.2025 20:11] Writing top page (month).
[28.01.2025 21:09] Read previous papers.
[28.01.2025 21:09] Get feed.
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15368
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15383
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.16142
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15907
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15570
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15369
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.16295
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.14723
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2403.09193
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15427
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.12370
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.16273
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.14912
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15420
[28.01.2025 21:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.01.2025 21:09] No deleted papers detected.
[28.01.2025 21:09] Downloading and parsing papers (pdf, html). Total: 14.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.15368.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.15368.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.15368.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.15383.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.15383.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.15383.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.16142.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.16142.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.16142.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.15907.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.15907.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.15907.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.15570.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.15570.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.15570.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.15369.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.15369.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.15369.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.16295.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.16295.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.16295.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.14723.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.14723.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.14723.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2403.09193.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2403.09193.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2403.09193.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.15427.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.15427.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.15427.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.12370.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.12370.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.12370.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.16273.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.16273.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.16273.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.14912.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.14912.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.14912.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.15420.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.15420.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.15420.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Enriching papers with extra data.
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 0. We introduce Baichuan-Omni-1.5, an omni-modal model that not only has omni-modal understanding capabilities but also provides end-to-end audio generation capabilities. To achieve fluent and high-quality interaction across modalities without compromising the capabilities of any modality, we prioritiz...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 1. We introduce Qwen2.5-1M, a series of models that extend the context length to 1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series have significantly enhanced long-context capabilities through long-context pre-training and post-training. Key techniques such as long data syn...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 2. Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive gener...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 3. Recent advancements in speech generation have been driven by the large-scale training datasets. However, current models fall short of capturing the spontaneity and variability inherent in real-world human speech, due to their reliance on audiobook datasets limited to formal read-aloud speech styles....
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 4. As is known, hybrid quadratic and subquadratic attention models in multi-head architectures have surpassed both Transformer and Linear RNN models , with these works primarily focusing on reducing KV complexity and improving efficiency. For further research on expressiveness, we introduce our series ...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 5. We present a new family of mobile hybrid vision networks, called iFormer, with a focus on optimizing latency and accuracy on mobile applications. iFormer effectively integrates the fast local representation capacity of convolution with the efficient global modeling ability of self-attention. The loc...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 6. State Space Models (SSMs) have emerged as efficient alternatives to Transformers for sequential modeling, but their inability to leverage modality-specific features limits their performance in multi-modal pretraining. Here, we propose Mixture-of-Mamba, a novel SSM architecture that introduces modali...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 7. Scaling test-time compute is a promising axis for improving LLM capabilities. However, test-time compute can be scaled in a variety of ways, and effectively combining different approaches remains an active area of research. Here, we explore this problem in the context of solving real-world GitHub is...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 8. Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuiti...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 9. Customizable role-playing in large language models (LLMs), also known as character generalization, is gaining increasing attention for its versatility and cost-efficiency in developing and deploying role-playing dialogue agents. This study explores a large-scale data synthesis approach to equip LLMs...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 10. Scaling the capacity of language models has consistently proven to be a reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the compute per example. While scaling typically involves increa...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 11. The dominance of large decoder-only language models has overshadowed encoder-decoder architectures, despite their fundamental efficiency advantages in sequence processing. For small language models (SLMs) - those with 1 billion parameters or fewer - our systematic analysis across GPU, CPU, and NPU p...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 12. We introduce Feasible Learning (FL), a sample-centric learning paradigm where models are trained by solving a feasibility problem that bounds the loss for each training sample. In contrast to the ubiquitous Empirical Risk Minimization (ERM) framework, which optimizes for average performance, FL dema...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 13. Classifier-Free Guidance (CFG) has been a default technique in various visual generative models, yet it requires inference from both conditional and unconditional models during sampling. We propose to build visual models that are free from guided sampling. The resulting algorithm, Guidance-Free Trai...
[28.01.2025 21:09] Read previous papers.
[28.01.2025 21:09] Generating reviews via LLM API.
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#data", "#optimization", "#dataset", "#training", "#audio", "#multimodal"], "emoji": "üé≠", "ru": {"title": "Baichuan-Omni-1.5: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò", "desc": "Baichuan-Omni-1.5 - —ç—Ç–æ –æ–º–Ω–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –æ–±–ª–∞–¥–∞—é—â–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ. –î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∫–∞—á–µ—Å
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#architecture", "#inference", "#long_context", "#training", "#open_source"], "emoji": "üöÄ", "ru": {"title": "–ú–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π Qwen2.5-1M —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–∫–Ω–æ–º –≤ 1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ —Ç–µ—Ö–Ω–∏–∫–∏ —Å–∏–Ω
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#optimization", "#rl", "#benchmark", "#training", "#games"], "emoji": "ü§ñ", "ru": {"title": "MR.Q: –ù–∞ –ø—É—Ç–∏ –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º MR.Q. –≠—Ç–æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª—å
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#data", "#audio", "#multilingual", "#dataset", "#open_source", "#low_resource"], "emoji": "üó£Ô∏è", "ru": {"title": "Emilia: –Ω–æ–≤—ã–π —ç—Ç–∞–ø –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–µ—á–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Emilia-Pipe - –æ—Ç–∫—Ä—ã—Ç—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#transfer_learning", "#training", "#architecture", "#small_models", "#optimization", "#open_source"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ RNN —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∑–Ω–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —á–∏—Å—Ç–æ–º –Ω–∞—Ç–∏–≤–Ω–æ–º –≤–Ω–∏–º
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#optimization", "#training", "#cv", "#architecture"], "emoji": "üì±", "ru": {"title": "iFormer: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –º–æ–±–∏–ª—å–Ω–æ–≥–æ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "iFormer - —ç—Ç–æ –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–±–∏–ª—å–Ω—ã—Ö –≥–∏–±—Ä–∏–¥–Ω—ã—Ö —Å–µ—Ç–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π.
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#benchmark"], "emoji": "üß†", "ru": {"title": "Mixture-of-Mamba: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å-—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å—é", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π (SSM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Mixture-of-Mamb
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#data", "#optimization", "#training", "#dataset", "#plp", "#open_source"], "emoji": "üêí", "ru": {"title": "CodeMonkeys: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π LLM –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º—É CodeMonkeys –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º GitHub —Å 
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#cv", "#ethics", "#alignment", "#multimodal"], "emoji": "üëÅÔ∏è", "ru": {"title": "–¢–µ–∫—Å—Ç –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –≤–∑–≥–ª—è–¥: –∫–∞–∫ —è–∑—ã–∫–æ–≤—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –≤–ª–∏—è—é—Ç –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏—Ö –∑—Ä–µ–Ω–∏–µ –∏ —è–∑—ã–∫ (VLM). –ê–≤—Ç–æ
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#training", "#data", "#agents", "#dataset", "#open_source", "#synthetic"], "emoji": "üé≠", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –ò–ò –∏—Å–∫—É—Å—Å—Ç–≤—É –ø–µ—Ä–µ–≤–æ–ø–ª–æ—â–µ–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±–æ–±—â–µ–Ω–∏—é —Ö–∞—Ä–∞–∫—Ç–µ—Ä–æ–≤ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π. –ê–≤—Ç–æ—Ä—ã —Å–∏–Ω—Ç–µ–∑–∏—Ä—É—é—Ç 
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å - –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –º–æ—â–Ω–æ—Å—Ç—å—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#architecture", "#training", "#small_models", "#optimization", "#transfer_learning"], "emoji": "ü§ñ", "ru": {"title": "–≠–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —ç–Ω–∫–æ–¥–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä –¥–ª—è –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#training", "#optimization"], "emoji": "üéØ", "ru": {"title": "–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–∞–∂–¥–æ–º—É –æ–±—Ä–∞–∑—Ü—É –¥–∞–Ω–Ω—ã—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è - Feasible Learning (FL). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–≥–æ —Ä–∏—Å
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#training", "#cv", "#open_source"], "emoji": "üñºÔ∏è", "ru": {"title": "GFT: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –Ω–∞–ø—Ä–∞–≤–ª—è—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π - Guidance-Free Training (GFT). GFT 
[28.01.2025 21:09] Loading Chinese text from previous data.
[28.01.2025 21:09] Renaming data file.
[28.01.2025 21:09] Renaming previous data. hf_papers.json to ./d/2025-01-28.json
[28.01.2025 21:09] Saving new data file.
[28.01.2025 21:09] Generating page.
[28.01.2025 21:09] Renaming previous page.
[28.01.2025 21:09] Renaming previous data. index.html to ./d/2025-01-28.html
[28.01.2025 21:09] [Experimental] Generating Chinese page for reading.
[28.01.2025 21:09] Chinese vocab [{'word': '‰ªãÁªç', 'pinyin': 'ji√® sh√†o', 'trans': 'introduce'}, {'word': 'Baichuan-Omni-1.5', 'pinyin': 'B√†i chuƒÅn-≈åu m√≠-1.5', 'trans': 'Baichuan-Omni-1.5'}, {'word': 'ÂÖ∑Êúâ', 'pinyin': 'j√π y«íu', 'trans': 'have'}, {'word': 'ÂÖ®Ê®°ÊÄÅ', 'pinyin': 'qu√°n m√≥ sh√¨', 'trans': 'full modality'}, {'word': 'ÁêÜËß£', 'pinyin': 'l«ê jiƒõ', 'trans': 'understanding'}, {'word': 'Á´ØÂà∞Á´Ø', 'pinyin': 'duƒÅn d√†o duƒÅn', 'trans': 'end-to-end'}, {'word': 'Èü≥È¢ë', 'pinyin': 'yƒ´n p√≠n', 'trans': 'audio'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìng ch√©ng', 'trans': 'generate'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ng l√¨', 'trans': 'ability'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠ xi√†n', 'trans': 'achieve'}, {'word': 'ÊµÅÁïÖ', 'pinyin': 'li√∫ ch√†ng', 'trans': 'smooth'}, {'word': 'È´òË¥®Èáè', 'pinyin': 'gƒÅo zh√¨ li√†ng', 'trans': 'high quality'}, {'word': 'Ë∑®Ê®°ÊÄÅ', 'pinyin': 'ku√† m√≥ sh√¨', 'trans': 'cross-modality'}, {'word': '‰∫§‰∫í', 'pinyin': 'jiƒÅo h√π', 'trans': 'interaction'}, {'word': '‰ºòÂåñ', 'pinyin': 'y≈çu hu√†', 'trans': 'optimize'}, {'word': 'ÂÖ≥ÈîÆ', 'pinyin': 'gu«én ji√†n', 'trans': 'key'}, {'word': 'ÊñπÈù¢', 'pinyin': 'fƒÅng mi√†n', 'trans': 'aspect'}, {'word': 'Âª∫Á´ã', 'pinyin': 'ji√†n l√¨', 'trans': 'establish'}, {'word': 'ÂÖ®Èù¢', 'pinyin': 'qu√°n mi√†n', 'trans': 'comprehensive'}, {'word': 'Êï∞ÊçÆ', 'pinyin': 'sh√π j√π', 'trans': 'data'}, {'word': 'Ê∏ÖÊ¥ó', 'pinyin': 'qƒ´ng x«ê', 'trans': 'clean'}, {'word': 'ÂêàÊàê', 'pinyin': 'h√© ch√©ng', 'trans': 'synthesize'}, {'word': 'ÁÆ°ÈÅì', 'pinyin': 'gu«én d√†o', 'trans': 'pipeline'}, {'word': 'Ëé∑Âæó', 'pinyin': 'hu√≤ d√©', 'trans': 'obtain'}, {'word': 'Â§ßÁ∫¶', 'pinyin': 'd√† yuƒì', 'trans': 'about'}, {'word': 'ÊñáÊú¨', 'pinyin': 'w√©n bƒõn', 'trans': 'text'}, {'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ ju√©', 'trans': 'visual'}, {'word': 'ËÆæËÆ°', 'pinyin': 'sh√® j√¨', 'trans': 'design'}, {'word': 'ÂàÜËØçÂô®', 'pinyin': 'fƒìn c√≠ q√¨', 'trans': 'tokenizer'}, {'word': 'ÊçïÊçâ', 'pinyin': 'b«î zhu≈ç', 'trans': 'capture'}, {'word': 'ËØ≠‰πâ', 'pinyin': 'y«î y√¨', 'trans': 'semantics'}, {'word': 'Â£∞Â≠¶', 'pinyin': 'shƒìng xu√©', 'trans': 'acoustics'}, {'word': '‰ø°ÊÅØ', 'pinyin': 'x√¨n xƒ´', 'trans': 'information'}, {'word': 'Â§öÈò∂ÊÆµ', 'pinyin': 'du≈ç jiƒì du√†n', 'trans': 'multi-stage'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πn li√†n', 'trans': 'training'}, {'word': 'Á≠ñÁï•', 'pinyin': 'c√® l√º√®', 'trans': 'strategy'}, {'word': 'Á°Æ‰øù', 'pinyin': 'qu√® b«éo', 'trans': 'ensure'}, {'word': 'ÊúâÊïà', 'pinyin': 'y«íu xi√†o', 'trans': 'effective'}, {'word': 'ÂçèÂêå', 'pinyin': 'xi√© t√≥ng', 'trans': 'coordination'}, {'word': 'È¢ÜÂÖà', 'pinyin': 'l«êng xiƒÅn', 'trans': 'lead'}, {'word': 'ÂΩìÂâç', 'pinyin': 'dƒÅng qi√°n', 'trans': 'current'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ÂèñÂæó', 'pinyin': 'q«î d√©', 'trans': 'achieve'}, {'word': 'ÂèØÊØî', 'pinyin': 'kƒõ b«ê', 'trans': 'comparable'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√© gu«í', 'trans': 'result'}]
[28.01.2025 21:09] Renaming previous Chinese page.
[28.01.2025 21:09] Renaming previous data. zh.html to ./d/2025-01-27_zh_reading_task.html
[28.01.2025 21:09] Writing Chinese reading task.
[28.01.2025 21:09] Writing result.
[28.01.2025 21:09] Renaming log file.
[28.01.2025 21:09] Renaming previous data. log.txt to ./logs/2025-01-28_last_log.txt
