[28.01.2025 20:11] Read previous papers.
[28.01.2025 20:11] Generating top page (month).
[28.01.2025 20:11] Writing top page (month).
[28.01.2025 21:09] Read previous papers.
[28.01.2025 21:09] Get feed.
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15368
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15383
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.16142
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15907
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15570
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15369
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.16295
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.14723
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2403.09193
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15427
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.12370
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.16273
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.14912
[28.01.2025 21:09] Get page data from previous paper. URL: https://huggingface.co/papers/2501.15420
[28.01.2025 21:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.01.2025 21:09] No deleted papers detected.
[28.01.2025 21:09] Downloading and parsing papers (pdf, html). Total: 14.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.15368.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.15368.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.15368.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.15383.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.15383.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.15383.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.16142.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.16142.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.16142.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.15907.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.15907.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.15907.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.15570.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.15570.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.15570.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.15369.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.15369.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.15369.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.16295.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.16295.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.16295.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.14723.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.14723.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.14723.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2403.09193.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2403.09193.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2403.09193.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.15427.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.15427.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.15427.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.12370.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.12370.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.12370.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.16273.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.16273.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.16273.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.14912.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.14912.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.14912.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Downloading and parsing paper https://huggingface.co/papers/2501.15420.
[28.01.2025 21:09] Extra JSON file exists (./assets/json/2501.15420.json), skip PDF parsing.
[28.01.2025 21:09] Paper image links file exists (./assets/img_data/2501.15420.json), skip HTML parsing.
[28.01.2025 21:09] Success.
[28.01.2025 21:09] Enriching papers with extra data.
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 0. We introduce Baichuan-Omni-1.5, an omni-modal model that not only has omni-modal understanding capabilities but also provides end-to-end audio generation capabilities. To achieve fluent and high-quality interaction across modalities without compromising the capabilities of any modality, we prioritiz...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 1. We introduce Qwen2.5-1M, a series of models that extend the context length to 1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series have significantly enhanced long-context capabilities through long-context pre-training and post-training. Key techniques such as long data syn...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 2. Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive gener...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 3. Recent advancements in speech generation have been driven by the large-scale training datasets. However, current models fall short of capturing the spontaneity and variability inherent in real-world human speech, due to their reliance on audiobook datasets limited to formal read-aloud speech styles....
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 4. As is known, hybrid quadratic and subquadratic attention models in multi-head architectures have surpassed both Transformer and Linear RNN models , with these works primarily focusing on reducing KV complexity and improving efficiency. For further research on expressiveness, we introduce our series ...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 5. We present a new family of mobile hybrid vision networks, called iFormer, with a focus on optimizing latency and accuracy on mobile applications. iFormer effectively integrates the fast local representation capacity of convolution with the efficient global modeling ability of self-attention. The loc...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 6. State Space Models (SSMs) have emerged as efficient alternatives to Transformers for sequential modeling, but their inability to leverage modality-specific features limits their performance in multi-modal pretraining. Here, we propose Mixture-of-Mamba, a novel SSM architecture that introduces modali...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 7. Scaling test-time compute is a promising axis for improving LLM capabilities. However, test-time compute can be scaled in a variety of ways, and effectively combining different approaches remains an active area of research. Here, we explore this problem in the context of solving real-world GitHub is...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 8. Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuiti...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 9. Customizable role-playing in large language models (LLMs), also known as character generalization, is gaining increasing attention for its versatility and cost-efficiency in developing and deploying role-playing dialogue agents. This study explores a large-scale data synthesis approach to equip LLMs...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 10. Scaling the capacity of language models has consistently proven to be a reliable approach for improving performance and unlocking new capabilities. Capacity can be primarily defined by two dimensions: the number of model parameters and the compute per example. While scaling typically involves increa...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 11. The dominance of large decoder-only language models has overshadowed encoder-decoder architectures, despite their fundamental efficiency advantages in sequence processing. For small language models (SLMs) - those with 1 billion parameters or fewer - our systematic analysis across GPU, CPU, and NPU p...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 12. We introduce Feasible Learning (FL), a sample-centric learning paradigm where models are trained by solving a feasibility problem that bounds the loss for each training sample. In contrast to the ubiquitous Empirical Risk Minimization (ERM) framework, which optimizes for average performance, FL dema...
[28.01.2025 21:09] ********************************************************************************
[28.01.2025 21:09] Abstract 13. Classifier-Free Guidance (CFG) has been a default technique in various visual generative models, yet it requires inference from both conditional and unconditional models during sampling. We propose to build visual models that are free from guided sampling. The resulting algorithm, Guidance-Free Trai...
[28.01.2025 21:09] Read previous papers.
[28.01.2025 21:09] Generating reviews via LLM API.
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#data", "#optimization", "#dataset", "#training", "#audio", "#multimodal"], "emoji": "🎭", "ru": {"title": "Baichuan-Omni-1.5: Прорыв в омнимодальном ИИ", "desc": "Baichuan-Omni-1.5 - это омнимодальная модель, обладающая способностями понимания и генерации аудио. Для достижения качес
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#architecture", "#inference", "#long_context", "#training", "#open_source"], "emoji": "🚀", "ru": {"title": "Миллион токенов: новый рубеж для языковых моделей", "desc": "Статья представляет серию моделей Qwen2.5-1M с контекстным окном в 1 миллион токенов. Авторы применили техники син
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#optimization", "#rl", "#benchmark", "#training", "#games"], "emoji": "🤖", "ru": {"title": "MR.Q: На пути к универсальному обучению с подкреплением", "desc": "Статья представляет новый алгоритм обучения с подкреплением под названием MR.Q. Этот алгоритм объединяет преимущества модель
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#data", "#audio", "#multilingual", "#dataset", "#open_source", "#low_resource"], "emoji": "🗣️", "ru": {"title": "Emilia: новый этап в генерации естественной речи", "desc": "Исследователи представили Emilia-Pipe - открытый конвейер предобработки для извлечения высококачественных данн
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#transfer_learning", "#training", "#architecture", "#small_models", "#optimization", "#open_source"], "emoji": "🧠", "ru": {"title": "Повышение эффективности и выразительности RNN через дистилляцию знаний", "desc": "Статья представляет новые модели, основанные на чистом нативном вним
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#optimization", "#training", "#cv", "#architecture"], "emoji": "📱", "ru": {"title": "iFormer: Эффективные нейросети для мобильного компьютерного зрения", "desc": "iFormer - это новое семейство мобильных гибридных сетей компьютерного зрения, оптимизированных для мобильных приложений.
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#multimodal", "#architecture", "#benchmark"], "emoji": "🧠", "ru": {"title": "Mixture-of-Mamba: Эффективное мультимодальное обучение с модальность-специфической разреженностью", "desc": "В этой статье представлена новая архитектура модели состояний (SSM) под названием Mixture-of-Mamb
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#data", "#optimization", "#training", "#dataset", "#plp", "#open_source"], "emoji": "🐒", "ru": {"title": "CodeMonkeys: Масштабирование вычислений LLM для решения реальных задач программирования", "desc": "Статья представляет систему CodeMonkeys для решения реальных проблем GitHub с 
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#cv", "#ethics", "#alignment", "#multimodal"], "emoji": "👁️", "ru": {"title": "Текст направляет взгляд: как языковые подсказки влияют на визуальные предубеждения ИИ", "desc": "Статья исследует визуальные предубеждения в мультимодальных моделях, объединяющих зрение и язык (VLM). Авто
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#training", "#data", "#agents", "#dataset", "#open_source", "#synthetic"], "emoji": "🎭", "ru": {"title": "Обучение ИИ искусству перевоплощения", "desc": "Исследование посвящено обучению больших языковых моделей (LLM) способности к обобщению характеров персонажей. Авторы синтезируют 
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "🧠", "ru": {"title": "Оптимальная разреженность - ключ к эффективному масштабированию языковых моделей", "desc": "Статья исследует взаимосвязь между количеством параметров и вычислительной мощностью в контексте разреженных моде
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#architecture", "#training", "#small_models", "#optimization", "#transfer_learning"], "emoji": "🤖", "ru": {"title": "Энкодер-декодер: эффективное решение для малых языковых моделей", "desc": "Исследование показывает преимущества архитектуры энкодер-декодер для малых языковых моделей
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#training", "#optimization"], "emoji": "🎯", "ru": {"title": "Индивидуальный подход к каждому образцу данных", "desc": "В статье представлена новая парадигма обучения моделей машинного обучения - Feasible Learning (FL). В отличие от традиционного подхода минимизации эмпирического рис
[28.01.2025 21:09] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#training", "#cv", "#open_source"], "emoji": "🖼️", "ru": {"title": "GFT: Эффективная генерация изображений без направляющей выборки", "desc": "Статья представляет новый метод обучения визуальных генеративных моделей - Guidance-Free Training (GFT). GFT 
[28.01.2025 21:09] Loading Chinese text from previous data.
[28.01.2025 21:09] Renaming data file.
[28.01.2025 21:09] Renaming previous data. hf_papers.json to ./d/2025-01-28.json
[28.01.2025 21:09] Saving new data file.
[28.01.2025 21:09] Generating page.
[28.01.2025 21:09] Renaming previous page.
[28.01.2025 21:09] Renaming previous data. index.html to ./d/2025-01-28.html
[28.01.2025 21:09] [Experimental] Generating Chinese page for reading.
[28.01.2025 21:09] Chinese vocab [{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'}, {'word': 'Baichuan-Omni-1.5', 'pinyin': 'Bài chuān-Ōu mí-1.5', 'trans': 'Baichuan-Omni-1.5'}, {'word': '具有', 'pinyin': 'jù yǒu', 'trans': 'have'}, {'word': '全模态', 'pinyin': 'quán mó shì', 'trans': 'full modality'}, {'word': '理解', 'pinyin': 'lǐ jiě', 'trans': 'understanding'}, {'word': '端到端', 'pinyin': 'duān dào duān', 'trans': 'end-to-end'}, {'word': '音频', 'pinyin': 'yīn pín', 'trans': 'audio'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'}, {'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'}, {'word': '流畅', 'pinyin': 'liú chàng', 'trans': 'smooth'}, {'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'}, {'word': '跨模态', 'pinyin': 'kuà mó shì', 'trans': 'cross-modality'}, {'word': '交互', 'pinyin': 'jiāo hù', 'trans': 'interaction'}, {'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimize'}, {'word': '关键', 'pinyin': 'guǎn jiàn', 'trans': 'key'}, {'word': '方面', 'pinyin': 'fāng miàn', 'trans': 'aspect'}, {'word': '建立', 'pinyin': 'jiàn lì', 'trans': 'establish'}, {'word': '全面', 'pinyin': 'quán miàn', 'trans': 'comprehensive'}, {'word': '数据', 'pinyin': 'shù jù', 'trans': 'data'}, {'word': '清洗', 'pinyin': 'qīng xǐ', 'trans': 'clean'}, {'word': '合成', 'pinyin': 'hé chéng', 'trans': 'synthesize'}, {'word': '管道', 'pinyin': 'guǎn dào', 'trans': 'pipeline'}, {'word': '获得', 'pinyin': 'huò dé', 'trans': 'obtain'}, {'word': '大约', 'pinyin': 'dà yuē', 'trans': 'about'}, {'word': '文本', 'pinyin': 'wén běn', 'trans': 'text'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '设计', 'pinyin': 'shè jì', 'trans': 'design'}, {'word': '分词器', 'pinyin': 'fēn cí qì', 'trans': 'tokenizer'}, {'word': '捕捉', 'pinyin': 'bǔ zhuō', 'trans': 'capture'}, {'word': '语义', 'pinyin': 'yǔ yì', 'trans': 'semantics'}, {'word': '声学', 'pinyin': 'shēng xué', 'trans': 'acoustics'}, {'word': '信息', 'pinyin': 'xìn xī', 'trans': 'information'}, {'word': '多阶段', 'pinyin': 'duō jiē duàn', 'trans': 'multi-stage'}, {'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'training'}, {'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'}, {'word': '确保', 'pinyin': 'què bǎo', 'trans': 'ensure'}, {'word': '有效', 'pinyin': 'yǒu xiào', 'trans': 'effective'}, {'word': '协同', 'pinyin': 'xié tóng', 'trans': 'coordination'}, {'word': '领先', 'pinyin': 'lǐng xiān', 'trans': 'lead'}, {'word': '当前', 'pinyin': 'dāng qián', 'trans': 'current'}, {'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'}, {'word': '取得', 'pinyin': 'qǔ dé', 'trans': 'achieve'}, {'word': '可比', 'pinyin': 'kě bǐ', 'trans': 'comparable'}, {'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'}]
[28.01.2025 21:09] Renaming previous Chinese page.
[28.01.2025 21:09] Renaming previous data. zh.html to ./d/2025-01-27_zh_reading_task.html
[28.01.2025 21:09] Writing Chinese reading task.
[28.01.2025 21:09] Writing result.
[28.01.2025 21:09] Renaming log file.
[28.01.2025 21:09] Renaming previous data. log.txt to ./logs/2025-01-28_last_log.txt
