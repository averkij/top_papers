[21.08.2025 08:16] Read previous papers.
[21.08.2025 08:16] Generating top page (month).
[21.08.2025 08:16] Writing top page (month).
[21.08.2025 09:12] Read previous papers.
[21.08.2025 09:12] Get feed.
[21.08.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13491
[21.08.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.11987
[21.08.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14811
[21.08.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14879
[21.08.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14111
[21.08.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14896
[21.08.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14160
[21.08.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14460
[21.08.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14444
[21.08.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14187
[21.08.2025 09:12] Extract page data from URL. URL: https://huggingface.co/papers/2508.13680
[21.08.2025 09:12] Extract page data from URL. URL: https://huggingface.co/papers/2508.11408
[21.08.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.10137
[21.08.2025 09:12] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13745
[21.08.2025 09:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.08.2025 09:12] No deleted papers detected.
[21.08.2025 09:12] Downloading and parsing papers (pdf, html). Total: 14.
[21.08.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2508.13491.
[21.08.2025 09:12] Extra JSON file exists (./assets/json/2508.13491.json), skip PDF parsing.
[21.08.2025 09:12] Paper image links file exists (./assets/img_data/2508.13491.json), skip HTML parsing.
[21.08.2025 09:12] Success.
[21.08.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2508.11987.
[21.08.2025 09:12] Extra JSON file exists (./assets/json/2508.11987.json), skip PDF parsing.
[21.08.2025 09:12] Paper image links file exists (./assets/img_data/2508.11987.json), skip HTML parsing.
[21.08.2025 09:12] Success.
[21.08.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2508.14811.
[21.08.2025 09:12] Extra JSON file exists (./assets/json/2508.14811.json), skip PDF parsing.
[21.08.2025 09:12] Paper image links file exists (./assets/img_data/2508.14811.json), skip HTML parsing.
[21.08.2025 09:12] Success.
[21.08.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2508.14879.
[21.08.2025 09:12] Extra JSON file exists (./assets/json/2508.14879.json), skip PDF parsing.
[21.08.2025 09:12] Paper image links file exists (./assets/img_data/2508.14879.json), skip HTML parsing.
[21.08.2025 09:12] Success.
[21.08.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2508.14111.
[21.08.2025 09:12] Extra JSON file exists (./assets/json/2508.14111.json), skip PDF parsing.
[21.08.2025 09:12] Paper image links file exists (./assets/img_data/2508.14111.json), skip HTML parsing.
[21.08.2025 09:12] Success.
[21.08.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2508.14896.
[21.08.2025 09:12] Extra JSON file exists (./assets/json/2508.14896.json), skip PDF parsing.
[21.08.2025 09:12] Paper image links file exists (./assets/img_data/2508.14896.json), skip HTML parsing.
[21.08.2025 09:12] Success.
[21.08.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2508.14160.
[21.08.2025 09:12] Extra JSON file exists (./assets/json/2508.14160.json), skip PDF parsing.
[21.08.2025 09:12] Paper image links file exists (./assets/img_data/2508.14160.json), skip HTML parsing.
[21.08.2025 09:12] Success.
[21.08.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2508.14460.
[21.08.2025 09:12] Downloading paper 2508.14460 from http://arxiv.org/pdf/2508.14460v1...
[21.08.2025 09:12] Failed to download and parse paper https://huggingface.co/papers/2508.14460: 'LTChar' object is not iterable
[21.08.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2508.14444.
[21.08.2025 09:12] Extra JSON file exists (./assets/json/2508.14444.json), skip PDF parsing.
[21.08.2025 09:12] Paper image links file exists (./assets/img_data/2508.14444.json), skip HTML parsing.
[21.08.2025 09:12] Success.
[21.08.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2508.14187.
[21.08.2025 09:12] Extra JSON file exists (./assets/json/2508.14187.json), skip PDF parsing.
[21.08.2025 09:12] Paper image links file exists (./assets/img_data/2508.14187.json), skip HTML parsing.
[21.08.2025 09:12] Success.
[21.08.2025 09:12] Downloading and parsing paper https://huggingface.co/papers/2508.13680.
[21.08.2025 09:12] Downloading paper 2508.13680 from http://arxiv.org/pdf/2508.13680v1...
[21.08.2025 09:13] Extracting affiliations from text.
[21.08.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 9 1 ] . [ 1 0 8 6 3 1 . 8 0 5 2 : r ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions? Vy Tuong Dang*, An Vo*, Quang Tau, Duc Dm, Daeyoung Kim KAIST {vydang, anvo, quangtau, ducdm200158, kimd}@kaist.ac.kr Abstract VLMs fail to interpret traffic signals and rules Vision language models (VLMs) demonstrate remarkable capabilities on English multimodal tasks, but their performance on low-resource languages with genuinely multimodal educational content remains largely unexplored. In this work, we test how VLMs perform on Vietnamese educational assessments, investigating whether VLMs trained predominantly on English data can handle real-world cross-lingual multimodal reasoning. Our work presents the first comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams through proposing ViExam, benchmark containing 2,548 mulimodal questions. We find that state-of-the-art VLMs achieve only 57.74% while open-source models achieved 27.70% mean accuracy across 7 academic domains, including Mathematics, Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs underperform average human testtakers (66.54%), with only the thinking VLM o3 (74.07%) exceeding human average performance, yet still falling substantially short of human best performance (99.60%). Crosslingual prompting with English instructions while maintaining Vietnamese content fails to improve performance, decreasing accuracy by 1 percentage point for SOTA VLMs. Humanin-the-loop collaboration can partially improve VLM performance by 5 percentage points. Code and data are available at: https://vi-exam.github.io. Vision Language Models (VLMs) have achieved remarkable success on English multimodal benchmarks (e.g., o3 achieving 82.9% on MMMU (Yue et al. 2024)). However, their performance on low-resource languages remains largely unexplored. Vietnamese is the worlds 10th most spoken language with over 100 million native speakers, making i"
[21.08.2025 09:13] Response: ```python
["KAIST"]
```
[21.08.2025 09:13] Deleting PDF ./assets/pdf/2508.13680.pdf.
[21.08.2025 09:13] Success.
[21.08.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2508.11408.
[21.08.2025 09:13] Downloading paper 2508.11408 from http://arxiv.org/pdf/2508.11408v1...
[21.08.2025 09:13] Extracting affiliations from text.
[21.08.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"ON-POLICY RL MEETS OFF-POLICY EXPERTS: HARMONIZING SUPERVISED FINE-TUNING AND REINFORCEMENT LEARNING VIA DYNAMIC WEIGHTING 5 2 0 2 5 1 ] . [ 1 8 0 4 1 1 . 8 0 5 2 : r Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, Jingren Zhou Alibaba Group "
[21.08.2025 09:13] Response: ```python
["Alibaba Group"]
```
[21.08.2025 09:13] Deleting PDF ./assets/pdf/2508.11408.pdf.
[21.08.2025 09:13] Success.
[21.08.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2508.10137.
[21.08.2025 09:13] Extra JSON file exists (./assets/json/2508.10137.json), skip PDF parsing.
[21.08.2025 09:13] Paper image links file exists (./assets/img_data/2508.10137.json), skip HTML parsing.
[21.08.2025 09:13] Success.
[21.08.2025 09:13] Downloading and parsing paper https://huggingface.co/papers/2508.13745.
[21.08.2025 09:13] Extra JSON file exists (./assets/json/2508.13745.json), skip PDF parsing.
[21.08.2025 09:13] Paper image links file exists (./assets/img_data/2508.13745.json), skip HTML parsing.
[21.08.2025 09:13] Success.
[21.08.2025 09:13] Enriching papers with extra data.
[21.08.2025 09:13] ********************************************************************************
[21.08.2025 09:13] Abstract 0. FinCDM, a cognitive diagnosis framework, evaluates financial LLMs at the knowledge-skill level using a comprehensive dataset, revealing hidden knowledge gaps and supporting more trustworthy model development.  					AI-generated summary 				 Large Language Models (LLMs) have shown promise for financi...
[21.08.2025 09:13] ********************************************************************************
[21.08.2025 09:13] Abstract 1. FutureX is a dynamic, live benchmark for evaluating LLM agents in future prediction tasks, addressing challenges in real-time updates and data contamination.  					AI-generated summary 				 Future prediction is a complex task for LLM agents, requiring a high level of analytical thinking, information...
[21.08.2025 09:13] ********************************************************************************
[21.08.2025 09:13] Abstract 2. Tinker is a framework for high-fidelity 3D editing using pretrained diffusion models, enabling multi-view consistency with minimal per-scene training.  					AI-generated summary 				 We introduce Tinker, a versatile framework for high-fidelity 3D editing that operates in both one-shot and few-shot r...
[21.08.2025 09:13] ********************************************************************************
[21.08.2025 09:13] Abstract 3. MeshCoder reconstructs complex 3D objects from point clouds into editable Blender Python scripts, enhancing shape-to-code reconstruction and 3D shape understanding through a multimodal large language model.  					AI-generated summary 				 Reconstructing 3D objects into editable programs is pivotal f...
[21.08.2025 09:13] ********************************************************************************
[21.08.2025 09:13] Abstract 4. Agentic Science leverages large language models, multimodal systems, and integrated platforms to enable autonomous scientific discovery across various domains, encompassing hypothesis generation, experimental design, execution, analysis, and iterative refinement.  					AI-generated summary 				 Arti...
[21.08.2025 09:13] ********************************************************************************
[21.08.2025 09:13] Abstract 5. A systematic study on quantizing diffusion large language models identifies challenges and evaluates state-of-the-art methods across various configurations to improve deployment on edge devices.  					AI-generated summary 				 Recent advances in diffusion large language models (dLLMs) have introduce...
[21.08.2025 09:13] ********************************************************************************
[21.08.2025 09:13] Abstract 6. RynnEC, a video multimodal large language model with a region-centric approach, achieves state-of-the-art performance in object property understanding, segmentation, and spatial reasoning, using an egocentric video pipeline and a region-centered benchmark.  					AI-generated summary 				 We introduc...
[21.08.2025 09:13] ********************************************************************************
[21.08.2025 09:13] Abstract 7. DuPO is a dual learning framework that generates annotation-free feedback using a generalized duality, enhancing performance across various tasks without relying on costly labels.  					AI-generated summary 				 We present DuPO, a dual learning-based preference optimization framework that generates ...
[21.08.2025 09:13] ********************************************************************************
[21.08.2025 09:13] Abstract 8. Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer model, enhances reasoning workload throughput and accuracy by replacing self-attention layers with Mamba-2 layers and using the Minitron strategy for compression.  					AI-generated summary 				 We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transfor...
[21.08.2025 09:13] ********************************************************************************
[21.08.2025 09:13] Abstract 9. A deep equilibrium canonicalizer (DEC) enhances local scale equivariance in deep networks, improving performance and consistency on ImageNet.  					AI-generated summary 				 Scale variation is a fundamental challenge in computer vision. Objects of the same class can have different sizes, and their p...
[21.08.2025 09:13] ********************************************************************************
[21.08.2025 09:13] Abstract 10. VLMs perform poorly on Vietnamese educational assessments, with state-of-the-art models achieving lower accuracy than human test-takers, and cross-lingual prompting does not significantly improve performance.  					AI-generated summary 				 Vision language models (VLMs) demonstrate remarkable capabi...
[21.08.2025 09:13] ********************************************************************************
[21.08.2025 09:13] Abstract 11. CHORD integrates supervised fine-tuning and reinforcement learning by dynamically weighting off-policy and on-policy data to improve model stability and performance.  					AI-generated summary 				 Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two prominent post-training paradigms...
[21.08.2025 09:13] ********************************************************************************
[21.08.2025 09:13] Abstract 12. A multilingual benchmark evaluates the reasoning skills of large language models across different languages and cultures, revealing their limitations in nuanced commonsense understanding.  					AI-generated summary 				 Recent advancements in reasoning-reinforced Large Language Models (LLMs) have sh...
[21.08.2025 09:13] ********************************************************************************
[21.08.2025 09:13] Abstract 13. A novel framework, REARM, enhances multi-modal recommender systems by refining contrastive learning and homography relations, improving feature representation and user-item interaction mining.  					AI-generated summary 				 Multi-modal recommender system focuses on utilizing rich modal information ...
[21.08.2025 09:13] Read previous papers.
[21.08.2025 09:13] Generating reviews via LLM API.
[21.08.2025 09:13] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#dataset", "#interpretability", "#science"], "emoji": "ğŸ’¹", "ru": {"title": "FinCDM: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "FinCDM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². ĞĞ½Ğ° Ğ¸
[21.08.2025 09:13] Using data from previous issue: {"categories": ["#agents", "#survey", "#benchmark", "#reasoning"], "emoji": "ğŸ”®", "ru": {"title": "FutureX: Ğ–Ğ¸Ğ²Ğ¾Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸", "desc": "FutureX - ÑÑ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¶Ğ¸Ğ²Ğ¾Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·
[21.08.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#diffusion", "#3d"], "emoji": "ğŸ¨", "ru": {"title": "Tinker: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ", "desc": "Tinker - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ 3D-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²
[21.08.2025 09:13] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#synthetic", "#3d", "#reasoning"], "emoji": "ğŸ§Š", "ru": {"title": "ĞÑ‚ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğº ĞºĞ¾Ğ´Ñƒ: MeshCoder Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸", "desc": "MeshCoder - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸Ğ· Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼
[21.08.2025 09:13] Using data from previous issue: {"categories": ["#healthcare", "#survey", "#science", "#multimodal", "#agents"], "emoji": "ğŸ§ª", "ru": {"title": "Ğ˜Ğ˜ ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒĞºĞ¸', ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ°
[21.08.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#training", "#inference", "#diffusion", "#open_source"], "emoji": "ğŸ”¬", "ru": {"title": "ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¯Ğœ: Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ", "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (dLLMs) Ğ´Ğ»Ñ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… 
[21.08.2025 09:13] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#multimodal", "#agents", "#video", "#3d", "#games", "#agi", "#reasoning"], "emoji": "ğŸ¤–", "ru": {"title": "RynnEC: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜", "desc": "RynnEC - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿
[21.08.2025 09:13] Using data from previous issue: {"categories": ["#machine_translation", "#rlhf", "#reasoning", "#optimization", "#training", "#rl"], "emoji": "ğŸ”„", "ru": {"title": "DuPO: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğ¾Ğ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ", "desc": "DuPO - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ±ĞµĞ· Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰
[21.08.2025 09:13] Using data from previous issue: {"categories": ["#dataset", "#reasoning", "#optimization", "#training", "#inference", "#open_source", "#architecture"], "emoji": "ğŸ§ ", "ru": {"title": "Nemotron-Nano-9B-v2: Ğ‘Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ´ÑƒĞ¼Ğ°Ğ¹, Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ğ¹", "desc": "Nemotron-Nano-9B-v2 - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ·Ñ‹ĞºĞ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Mamba Ğ¸ Trans
[21.08.2025 09:13] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#architecture"], "emoji": "ğŸ”", "ru": {"title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ ÑĞºĞ²Ğ¸Ğ»Ğ¸Ğ±Ñ€Ğ¸ÑƒĞ¼Ğ½Ñ‹Ğ¹ ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ (DEC) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¹ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚
[21.08.2025 09:13] Querying the API.
[21.08.2025 09:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

VLMs perform poorly on Vietnamese educational assessments, with state-of-the-art models achieving lower accuracy than human test-takers, and cross-lingual prompting does not significantly improve performance.  					AI-generated summary 				 Vision language models (VLMs) demonstrate remarkable capabilities on English multimodal tasks, but their performance on low-resource languages with genuinely multimodal educational content remains largely unexplored. In this work, we test how VLMs perform on Vietnamese educational assessments, investigating whether VLMs trained predominantly on English data can handle real-world cross-lingual multimodal reasoning. Our work presents the first comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams through proposing ViExam, a benchmark containing 2,548 multimodal questions. We find that state-of-the-art VLMs achieve only 57.74% while open-source models achieve 27.70% mean accuracy across 7 academic domains, including Mathematics, Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs underperform average human test-takers (66.54%), with only the thinking VLM o3 (74.07%) exceeding human average performance, yet still falling substantially short of human best performance (99.60%). Cross-lingual prompting with English instructions while maintaining Vietnamese content fails to improve performance, decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop collaboration can partially improve VLM performance by 5 percentage points. Code and data are available at: https://vi-exam.github.io.
[21.08.2025 09:13] Response: {
  "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ½Ğ° Ğ²ÑŒĞµÑ‚Ğ½Ğ°Ğ¼ÑĞºĞ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ViExam, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 2,548 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ 7 Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 57.74%, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸Ğ¶Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (66.54%). ĞšÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.",
  "emoji": "ğŸ‡»ğŸ‡³",
  "title": "VLM Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğ° Ğ²ÑŒĞµÑ‚Ğ½Ğ°Ğ¼ÑĞºĞ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…"
}
[21.08.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VLMs perform poorly on Vietnamese educational assessments, with state-of-the-art models achieving lower accuracy than human test-takers, and cross-lingual prompting does not significantly improve performance.  					AI-generated summary 				 Vision language models (VLMs) demonstrate remarkable capabilities on English multimodal tasks, but their performance on low-resource languages with genuinely multimodal educational content remains largely unexplored. In this work, we test how VLMs perform on Vietnamese educational assessments, investigating whether VLMs trained predominantly on English data can handle real-world cross-lingual multimodal reasoning. Our work presents the first comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams through proposing ViExam, a benchmark containing 2,548 multimodal questions. We find that state-of-the-art VLMs achieve only 57.74% while open-source models achieve 27.70% mean accuracy across 7 academic domains, including Mathematics, Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs underperform average human test-takers (66.54%), with only the thinking VLM o3 (74.07%) exceeding human average performance, yet still falling substantially short of human best performance (99.60%). Cross-lingual prompting with English instructions while maintaining Vietnamese content fails to improve performance, decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop collaboration can partially improve VLM performance by 5 percentage points. Code and data are available at: https://vi-exam.github.io."

[21.08.2025 09:13] Response: ```python
['DATASET', 'BENCHMARK', 'MULTILINGUAL']
```
[21.08.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"VLMs perform poorly on Vietnamese educational assessments, with state-of-the-art models achieving lower accuracy than human test-takers, and cross-lingual prompting does not significantly improve performance.  					AI-generated summary 				 Vision language models (VLMs) demonstrate remarkable capabilities on English multimodal tasks, but their performance on low-resource languages with genuinely multimodal educational content remains largely unexplored. In this work, we test how VLMs perform on Vietnamese educational assessments, investigating whether VLMs trained predominantly on English data can handle real-world cross-lingual multimodal reasoning. Our work presents the first comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams through proposing ViExam, a benchmark containing 2,548 multimodal questions. We find that state-of-the-art VLMs achieve only 57.74% while open-source models achieve 27.70% mean accuracy across 7 academic domains, including Mathematics, Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs underperform average human test-takers (66.54%), with only the thinking VLM o3 (74.07%) exceeding human average performance, yet still falling substantially short of human best performance (99.60%). Cross-lingual prompting with English instructions while maintaining Vietnamese content fails to improve performance, decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop collaboration can partially improve VLM performance by 5 percentage points. Code and data are available at: https://vi-exam.github.io."

[21.08.2025 09:13] Response: ```python
["LOW_RESOURCE", "OPEN_SOURCE"]
```
[21.08.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper evaluates the performance of Vision Language Models (VLMs) on Vietnamese educational assessments, revealing that these models struggle significantly compared to human test-takers. The study introduces ViExam, a benchmark with 2,548 multimodal questions across various academic domains, highlighting that state-of-the-art VLMs achieve only 57.74% accuracy. Despite attempts to enhance performance through cross-lingual prompting, results show a decline in accuracy, indicating that VLMs trained on English data are not effectively handling Vietnamese content. The findings suggest that while human-in-the-loop strategies can improve outcomes, VLMs still fall short of human performance, emphasizing the need for better adaptation to low-resource languages.","title":"Bridging the Gap: VLMs Struggle with Vietnamese Education"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper evaluates the performance of Vision Language Models (VLMs) on Vietnamese educational assessments, revealing that these models struggle significantly compared to human test-takers. The study introduces ViExam, a benchmark with 2,548 multimodal questions across various academic domains, highlighting that state-of-the-art VLMs achieve only 57.74% accuracy. Despite attempts to enhance performance through cross-lingual prompting, results show a decline in accuracy, indicating that VLMs trained on English data are not effectively handling Vietnamese content. The findings suggest that while human-in-the-loop strategies can improve outcomes, VLMs still fall short of human performance, emphasizing the need for better adaptation to low-resource languages.', title='Bridging the Gap: VLMs Struggle with Vietnamese Education'))
[21.08.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¶Šå—æ•™è‚²è¯„ä¼°ä¸­çš„è¡¨ç°ï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨å¤„ç†ä½èµ„æºè¯­è¨€çš„å¤šæ¨¡æ€å†…å®¹æ—¶æ•ˆæœä¸ä½³ã€‚å°½ç®¡åœ¨è‹±è¯­å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è¶Šå—çš„å¤šæ¨¡æ€è€ƒè¯•ä¸­ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹ä»…è¾¾åˆ°57.74%çš„å‡†ç¡®ç‡ï¼Œè¿œä½äºäººç±»è€ƒç”Ÿçš„66.54%ã€‚æˆ‘ä»¬æå‡ºäº†ViExamåŸºå‡†ï¼ŒåŒ…å«2548ä¸ªå¤šæ¨¡æ€é—®é¢˜ï¼Œä»¥å…¨é¢è¯„ä¼°VLMåœ¨è¶Šå—æ•™è‚²ä¸­çš„èƒ½åŠ›ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œè·¨è¯­è¨€æç¤ºå¹¶æœªæ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œåè€Œä½¿å‡†ç¡®ç‡ä¸‹é™ã€‚","title":"è¶Šå—æ•™è‚²è¯„ä¼°ä¸­çš„è§†è§‰è¯­è¨€æ¨¡å‹è¡¨ç°ä¸ä½³"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¶Šå—æ•™è‚²è¯„ä¼°ä¸­çš„è¡¨ç°ï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨å¤„ç†ä½èµ„æºè¯­è¨€çš„å¤šæ¨¡æ€å†…å®¹æ—¶æ•ˆæœä¸ä½³ã€‚å°½ç®¡åœ¨è‹±è¯­å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è¶Šå—çš„å¤šæ¨¡æ€è€ƒè¯•ä¸­ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹ä»…è¾¾åˆ°57.74%çš„å‡†ç¡®ç‡ï¼Œè¿œä½äºäººç±»è€ƒç”Ÿçš„66.54%ã€‚æˆ‘ä»¬æå‡ºäº†ViExamåŸºå‡†ï¼ŒåŒ…å«2548ä¸ªå¤šæ¨¡æ€é—®é¢˜ï¼Œä»¥å…¨é¢è¯„ä¼°VLMåœ¨è¶Šå—æ•™è‚²ä¸­çš„èƒ½åŠ›ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œè·¨è¯­è¨€æç¤ºå¹¶æœªæ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œåè€Œä½¿å‡†ç¡®ç‡ä¸‹é™ã€‚', title='è¶Šå—æ•™è‚²è¯„ä¼°ä¸­çš„è§†è§‰è¯­è¨€æ¨¡å‹è¡¨ç°ä¸ä½³'))
[21.08.2025 09:13] Querying the API.
[21.08.2025 09:13] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

CHORD integrates supervised fine-tuning and reinforcement learning by dynamically weighting off-policy and on-policy data to improve model stability and performance.  					AI-generated summary 				 Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two prominent post-training paradigms for refining the capabilities and aligning the behavior of Large Language Models (LLMs). Existing approaches that integrate SFT and RL often face the risk of disrupting established model patterns and inducing overfitting to expert data. To address this, we present a novel investigation into the unified view of SFT and RL through an off-policy versus on-policy lens. We propose CHORD, a framework for the Controllable Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic Weighting, which reframes SFT not as a separate stage but as a dynamically weighted auxiliary objective within the on-policy RL process. Based on an analysis of off-policy expert data's influence at both holistic and granular levels, we incorporate a dual-control mechanism in CHORD. Specifically, the framework first employs a global coefficient to holistically guide the transition from off-policy imitation to on-policy exploration, and then applies a token-wise weighting function that enables granular learning from expert tokens, which preserves on-policy exploration and mitigates disruption from off-policy data. We conduct extensive experiments on widely used benchmarks, providing empirical evidence that CHORD achieves a stable and efficient learning process. By effectively harmonizing off-policy expert data with on-policy exploration, CHORD demonstrates significant improvements over baselines. We release the implementation at https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to inspire further research.
[21.08.2025 09:13] Response: {
  "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ CHORD Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. CHORD Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ„Ñ„-Ğ¿Ğ¾Ğ»Ğ¸ÑĞ¸ Ğ¸ Ğ¾Ğ½-Ğ¿Ğ¾Ğ»Ğ¸ÑĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğ¾Ñ‚ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‚Ğ¾ĞºĞµĞ½-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CHORD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.",
  "emoji": "ğŸ¼",
  "title": "Ğ“Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"
}
[21.08.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CHORD integrates supervised fine-tuning and reinforcement learning by dynamically weighting off-policy and on-policy data to improve model stability and performance.  					AI-generated summary 				 Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two prominent post-training paradigms for refining the capabilities and aligning the behavior of Large Language Models (LLMs). Existing approaches that integrate SFT and RL often face the risk of disrupting established model patterns and inducing overfitting to expert data. To address this, we present a novel investigation into the unified view of SFT and RL through an off-policy versus on-policy lens. We propose CHORD, a framework for the Controllable Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic Weighting, which reframes SFT not as a separate stage but as a dynamically weighted auxiliary objective within the on-policy RL process. Based on an analysis of off-policy expert data's influence at both holistic and granular levels, we incorporate a dual-control mechanism in CHORD. Specifically, the framework first employs a global coefficient to holistically guide the transition from off-policy imitation to on-policy exploration, and then applies a token-wise weighting function that enables granular learning from expert tokens, which preserves on-policy exploration and mitigates disruption from off-policy data. We conduct extensive experiments on widely used benchmarks, providing empirical evidence that CHORD achieves a stable and efficient learning process. By effectively harmonizing off-policy expert data with on-policy exploration, CHORD demonstrates significant improvements over baselines. We release the implementation at https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to inspire further research."

[21.08.2025 09:13] Response: ```python
["RL", "RLHF", "TRAINING", "BENCHMARK"]
```
[21.08.2025 09:13] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"CHORD integrates supervised fine-tuning and reinforcement learning by dynamically weighting off-policy and on-policy data to improve model stability and performance.  					AI-generated summary 				 Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two prominent post-training paradigms for refining the capabilities and aligning the behavior of Large Language Models (LLMs). Existing approaches that integrate SFT and RL often face the risk of disrupting established model patterns and inducing overfitting to expert data. To address this, we present a novel investigation into the unified view of SFT and RL through an off-policy versus on-policy lens. We propose CHORD, a framework for the Controllable Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic Weighting, which reframes SFT not as a separate stage but as a dynamically weighted auxiliary objective within the on-policy RL process. Based on an analysis of off-policy expert data's influence at both holistic and granular levels, we incorporate a dual-control mechanism in CHORD. Specifically, the framework first employs a global coefficient to holistically guide the transition from off-policy imitation to on-policy exploration, and then applies a token-wise weighting function that enables granular learning from expert tokens, which preserves on-policy exploration and mitigates disruption from off-policy data. We conduct extensive experiments on widely used benchmarks, providing empirical evidence that CHORD achieves a stable and efficient learning process. By effectively harmonizing off-policy expert data with on-policy exploration, CHORD demonstrates significant improvements over baselines. We release the implementation at https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to inspire further research."

[21.08.2025 09:13] Response: ```python
["OPTIMIZATION"]
```
[21.08.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CHORD is a new framework that combines supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the performance of large language models. It addresses the common issue of model instability and overfitting by dynamically adjusting the influence of off-policy and on-policy data during training. By treating SFT as a flexible part of the RL process, CHORD allows for better integration of expert data while maintaining effective exploration. The framework\'s dual-control mechanism ensures that learning is both stable and efficient, leading to improved outcomes in various benchmarks.","title":"Harmonizing Learning: CHORD\'s Dynamic Integration of SFT and RL"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="CHORD is a new framework that combines supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the performance of large language models. It addresses the common issue of model instability and overfitting by dynamically adjusting the influence of off-policy and on-policy data during training. By treating SFT as a flexible part of the RL process, CHORD allows for better integration of expert data while maintaining effective exploration. The framework's dual-control mechanism ensures that learning is both stable and efficient, leading to improved outcomes in various benchmarks.", title="Harmonizing Learning: CHORD's Dynamic Integration of SFT and RL"))
[21.08.2025 09:13] Response: ParsedChatCompletionMessage[Article](content='{"desc":"CHORDæ¡†æ¶é€šè¿‡åŠ¨æ€åŠ æƒçš„æ–¹å¼ï¼Œå°†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»“åˆèµ·æ¥ï¼Œä»¥æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚è¯¥æ–¹æ³•å°†SFTè§†ä¸ºåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­åŠ¨æ€åŠ æƒçš„è¾…åŠ©ç›®æ ‡ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„é˜¶æ®µã€‚CHORDé‡‡ç”¨åŒé‡æ§åˆ¶æœºåˆ¶ï¼Œé¦–å…ˆé€šè¿‡å…¨å±€ç³»æ•°å¼•å¯¼ä»ç¦»ç­–ç•¥æ¨¡ä»¿åˆ°åœ¨ç­–ç•¥æ¢ç´¢çš„è¿‡æ¸¡ï¼Œç„¶åé€šè¿‡é€æ ‡è®°åŠ æƒå‡½æ•°å®ç°å¯¹ä¸“å®¶æ ‡è®°çš„ç»†ç²’åº¦å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCHORDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„å­¦ä¹ ç¨³å®šæ€§å’Œæ•ˆç‡æå‡ã€‚","title":"åŠ¨æ€åŠ æƒï¼Œæå‡æ¨¡å‹ç¨³å®šæ€§ä¸æ€§èƒ½"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='CHORDæ¡†æ¶é€šè¿‡åŠ¨æ€åŠ æƒçš„æ–¹å¼ï¼Œå°†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»“åˆèµ·æ¥ï¼Œä»¥æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚è¯¥æ–¹æ³•å°†SFTè§†ä¸ºåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­åŠ¨æ€åŠ æƒçš„è¾…åŠ©ç›®æ ‡ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„é˜¶æ®µã€‚CHORDé‡‡ç”¨åŒé‡æ§åˆ¶æœºåˆ¶ï¼Œé¦–å…ˆé€šè¿‡å…¨å±€ç³»æ•°å¼•å¯¼ä»ç¦»ç­–ç•¥æ¨¡ä»¿åˆ°åœ¨ç­–ç•¥æ¢ç´¢çš„è¿‡æ¸¡ï¼Œç„¶åé€šè¿‡é€æ ‡è®°åŠ æƒå‡½æ•°å®ç°å¯¹ä¸“å®¶æ ‡è®°çš„ç»†ç²’åº¦å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCHORDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„å­¦ä¹ ç¨³å®šæ€§å’Œæ•ˆç‡æå‡ã€‚', title='åŠ¨æ€åŠ æƒï¼Œæå‡æ¨¡å‹ç¨³å®šæ€§ä¸æ€§èƒ½'))
[21.08.2025 09:13] Using data from previous issue: {"categories": ["#survey", "#benchmark", "#multilingual", "#reasoning"], "emoji": "ğŸŒ", "ru": {"title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¼ ÑĞ¼Ñ‹ÑĞ»Ğµ Ğ˜Ğ˜", "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ mSCoRe Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ ĞºÑƒĞ»
[21.08.2025 09:13] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#graphs", "#optimization", "#games"], "emoji": "ğŸ¯", "ru": {"title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ", "desc": "REARM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾
[21.08.2025 09:13] Renaming data file.
[21.08.2025 09:13] Renaming previous data. hf_papers.json to ./d/2025-08-21.json
[21.08.2025 09:13] Saving new data file.
[21.08.2025 09:13] Generating page.
[21.08.2025 09:13] Renaming previous page.
[21.08.2025 09:13] Renaming previous data. index.html to ./d/2025-08-21.html
[21.08.2025 09:13] Writing result.
[21.08.2025 09:13] Renaming log file.
[21.08.2025 09:13] Renaming previous data. log.txt to ./logs/2025-08-21_last_log.txt
