[21.08.2025 03:37] Read previous papers.
[21.08.2025 03:37] Generating top page (month).
[21.08.2025 03:37] Writing top page (month).
[21.08.2025 04:14] Read previous papers.
[21.08.2025 04:14] Get feed.
[21.08.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.13491
[21.08.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.11987
[21.08.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14811
[21.08.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14879
[21.08.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2508.14160
[21.08.2025 04:14] Extract page data from URL. URL: https://huggingface.co/papers/2508.14460
[21.08.2025 04:14] Extract page data from URL. URL: https://huggingface.co/papers/2508.14444
[21.08.2025 04:14] Extract page data from URL. URL: https://huggingface.co/papers/2508.14896
[21.08.2025 04:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[21.08.2025 04:14] No deleted papers detected.
[21.08.2025 04:14] Downloading and parsing papers (pdf, html). Total: 8.
[21.08.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2508.13491.
[21.08.2025 04:14] Extra JSON file exists (./assets/json/2508.13491.json), skip PDF parsing.
[21.08.2025 04:14] Paper image links file exists (./assets/img_data/2508.13491.json), skip HTML parsing.
[21.08.2025 04:14] Success.
[21.08.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2508.11987.
[21.08.2025 04:14] Extra JSON file exists (./assets/json/2508.11987.json), skip PDF parsing.
[21.08.2025 04:14] Paper image links file exists (./assets/img_data/2508.11987.json), skip HTML parsing.
[21.08.2025 04:14] Success.
[21.08.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2508.14811.
[21.08.2025 04:14] Extra JSON file exists (./assets/json/2508.14811.json), skip PDF parsing.
[21.08.2025 04:14] Paper image links file exists (./assets/img_data/2508.14811.json), skip HTML parsing.
[21.08.2025 04:14] Success.
[21.08.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2508.14879.
[21.08.2025 04:14] Extra JSON file exists (./assets/json/2508.14879.json), skip PDF parsing.
[21.08.2025 04:14] Paper image links file exists (./assets/img_data/2508.14879.json), skip HTML parsing.
[21.08.2025 04:14] Success.
[21.08.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2508.14160.
[21.08.2025 04:14] Extra JSON file exists (./assets/json/2508.14160.json), skip PDF parsing.
[21.08.2025 04:14] Paper image links file exists (./assets/img_data/2508.14160.json), skip HTML parsing.
[21.08.2025 04:14] Success.
[21.08.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2508.14460.
[21.08.2025 04:15] Downloading paper 2508.14460 from http://arxiv.org/pdf/2508.14460v1...
[21.08.2025 04:15] Failed to download and parse paper https://huggingface.co/papers/2508.14460: 'LTChar' object is not iterable
[21.08.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2508.14444.
[21.08.2025 04:15] Downloading paper 2508.14444 from http://arxiv.org/pdf/2508.14444v1...
[21.08.2025 04:15] Extracting affiliations from text.
[21.08.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 2 ] . [ 1 4 4 4 4 1 . 8 0 5 2 : r 2025-8-21 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Abstract. We introduce Nemotron-Nano-9B-v2, hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6 higher inference throughput in reasoning settings like 8k input and 16k output tokens (Figure 1). We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our preand post-training datasets on Hugging Face. 1. Introduction We introduce NVIDIA Nemotron Nano 2, hybrid Mamba-Transformer reasoning model (Waleffe et al., 2024; Lieber et al., 2024; DeepMind, 2025; NVIDIA, 2025) that achieves on-par or better benchmark accuracies at 36 higher throughput than Qwen3-8B (Yang et al., 2025) for generationheavy scenarios like 1k input / 8k output or 8k input / 16k output tokens (Figure 1). Nemotron Nano 2 builds on the architecture of Nemotron-H (NVIDIA, 2025), but utilizes key new datasets and recipes for pre-traini"
[21.08.2025 04:15] Response: ```python
[]
```
[21.08.2025 04:15] Extracting affiliations from text.
[21.08.2025 04:15] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 2 ] . [ 1 4 4 4 4 1 . 8 0 5 2 : r 2025-8-21 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning ModelAbstract. We introduce Nemotron-Nano-9B-v2, hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6 higher inference throughput in reasoning settings like 8k input and 16k output tokens (Figure 1). We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our preand post-training datasets on Hugging Face. 1. Introduction We introduce NVIDIA Nemotron Nano 2, hybrid Mamba-Transformer reasoning model (Waleffe et al., 2024; Lieber et al., 2024; DeepMind, 2025; NVIDIA, 2025) that achieves on-par or better benchmark accuracies at 36 higher throughput than Qwen3-8B (Yang et al., 2025) for generationheavy scenarios like 1k input / 8k output or 8k input / 16k output tokens (Figure 1). Nemotron Nano 2 builds on the architecture of Nemotron-H (NVIDIA, 2025), but utilizes key new datasets and recipes for pre-training, alignment, pruning and distillation. We share these recipes, the checkpoints, as well as the majority of the preand post-training datasets. The initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (2.4) over 20 trillion tokens using Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (2.5). It then underwent continuous pre-training long-context extension phase to become 128k-capable without degrading other benchmarks (2.6). Overall, new and improved datasets led to significant accuracy improvements over Nemotron-H-8B on math, multilingual, MMLU-Pro and other benchmarks (2.2). Nemotron Nano 2 was then post-trained through combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017). We applied multiple SFT stages across various domains, followed by targeted SFT on key areas such as tool use, long-context performance, and truncated (budgeted) training. GRPO and RLHF sharpened instruction-following and conversational ability, while additional DPO stages further strengthened tool use. Overall, post-training was performed on roughly 90 billion tokens, the majority in single-turn promptresponse format with reasoning 2025 NVIDIA. All rights reserved. NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Figure 1 Comparison of Nemotron Nano 2 and Qwen3-8B in terms of accuracy and throughput. Nemotron Nano 2 achieves comparable or better accuracies on complex reasoning benchmarks, while achieving up to 6.3 higher throughput for such workloads. We abbreviate input sequence length to ISL and output sequence length to OSL and measure throughput on single A10G GPU in bfloat16. traces. About 5% of the data contained deliberately truncated reasoning traces, enabling fine-grained thinking budget control at inference time (3.4). Finally, both the base model and aligned model were compressed so as to enable inference over context lengths of 128k tokens on single NVIDIA A10G GPU (22 GiB of memory, bfloat16 precision). This was done by extending compression strategy based on Minitron (Muralidharan et al., 2024; Sreenivas et al., 2024; Taghibakhshi et al., 2025) to compress reasoning models subject to constraints. We are releasing the following models on Hugging Face: NVIDIA-Nemotron-Nano-9B-v2: the aligned and pruned reasoning model, NVIDIA-Nemotron-Nano-9B-v2-Base: pruned base model, NVIDIA-Nemotron-Nano-12B-v2-Base: the base model before alignment or pruning. Additionally, we are releasing the majority of our pre-training dataset in the Nemotron-PreTraining-Dataset-v1 collection of more than 6 trillion tokens: Nemotron-CC-v2: Follow-up to Nemotron-CC (Su et al., 2025) with eight additional Common Crawl snapshots (20242025), synthetic rephrasing, deduplication, and synthetic Q&A data translated into 15 languages. Nemotron-CC-Math-v1: 133B-token math dataset from Common Crawl using Lynx + LLM pipeline (Karimi Mahabadi et al., 2025a). Preserves equations, standardizes to LaTeX, outperforms previous math datasets on benchmarks. Nemotron-Pretraining-Code-v1: Curated GitHub code references with multi-stage filtering, deduplication, and quality filters. Includes code Q&A data in 11 programming languages. Nemotron-Pretraining-SFT-v1: Synthetic SFT-style dataset covering STEM, multilingual, academic, and reasoning domains. Finally, we are releasing an updated post-training dataset: 2 NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Figure 2 Nemotron-Nano-12B-v2-Base layer pattern. As in Nemotron-H models, roughly 8% of the total layers in the model are self-attention layers which are evenly dispersed throughout the model. Model Number of layers Model dimension FFN dimension heads KV heads State dimension Mamba groups Nemotron-Nano-12B-v2-Base 62 5120 20480 40 8 8 Table 1 Summary of Nemotron-Nano-12B-v2-Base architecture. Nemotron-Post-Training-Dataset-v2 (link coming soon): Adds to NVIDIAs post-training dataset releases with an extension of SFT and RL data into five target languages: Spanish, French, German, Italian and Japanese. The data supports improvements of math, code, general reasoning, and instruction following capabilities. The rest of this technical report is organized as follows: In 2, we discuss the Nemotro"
[21.08.2025 04:15] Mistral response. {"object": "error", "message": "Service tier capacity exceeded for this model.", "type": "service_tier_capacity_exceeded", "param": null, "code": "3505"}
[21.08.2025 04:15] Failed to download and parse paper https://huggingface.co/papers/2508.14444: 'choices'
[21.08.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2508.14896.
[21.08.2025 04:15] Downloading paper 2508.14896 from http://arxiv.org/pdf/2508.14896v1...
[21.08.2025 04:15] Extracting affiliations from text.
[21.08.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 2 ] . [ 1 6 9 8 4 1 . 8 0 5 2 : r Quantization Meets dLLMs: Systematic Study of Post-training Quantization for Diffusion LLMs Haokun Lin 1,3, Haobo Xu 2, Yichen Wu3,4, Ziyu Guo5, Renrui Zhang5 , Zhichao Lu3, Ying Wei6, Qingfu Zhang3, Zhenan Sun1 Equal Contribution 1 NLPR & MAIS, Institute of Automation, CAS 3 City University of Hong Kong 5 The Chinese University of Hong Kong 2 Tsinghua University 6 Zhejiang University 4 Harvard University "
[21.08.2025 04:15] Response: ```python
[
    "NLPR & MAIS, Institute of Automation, CAS",
    "City University of Hong Kong",
    "The Chinese University of Hong Kong",
    "Tsinghua University",
    "Zhejiang University",
    "Harvard University"
]
```
[21.08.2025 04:15] Deleting PDF ./assets/pdf/2508.14896.pdf.
[21.08.2025 04:15] Success.
[21.08.2025 04:15] Enriching papers with extra data.
[21.08.2025 04:15] ********************************************************************************
[21.08.2025 04:15] Abstract 0. FinCDM, a cognitive diagnosis framework, evaluates financial LLMs at the knowledge-skill level using a comprehensive dataset, revealing hidden knowledge gaps and supporting more trustworthy model development.  					AI-generated summary 				 Large Language Models (LLMs) have shown promise for financi...
[21.08.2025 04:15] ********************************************************************************
[21.08.2025 04:15] Abstract 1. FutureX is a dynamic, live benchmark for evaluating LLM agents in future prediction tasks, addressing challenges in real-time updates and data contamination.  					AI-generated summary 				 Future prediction is a complex task for LLM agents, requiring a high level of analytical thinking, information...
[21.08.2025 04:15] ********************************************************************************
[21.08.2025 04:15] Abstract 2. Tinker is a framework for high-fidelity 3D editing using pretrained diffusion models, enabling multi-view consistency with minimal per-scene training.  					AI-generated summary 				 We introduce Tinker, a versatile framework for high-fidelity 3D editing that operates in both one-shot and few-shot r...
[21.08.2025 04:15] ********************************************************************************
[21.08.2025 04:15] Abstract 3. MeshCoder reconstructs complex 3D objects from point clouds into editable Blender Python scripts, enhancing shape-to-code reconstruction and 3D shape understanding through a multimodal large language model.  					AI-generated summary 				 Reconstructing 3D objects into editable programs is pivotal f...
[21.08.2025 04:15] ********************************************************************************
[21.08.2025 04:15] Abstract 4. RynnEC, a video multimodal large language model with a region-centric approach, achieves state-of-the-art performance in object property understanding, segmentation, and spatial reasoning, using an egocentric video pipeline and a region-centered benchmark.  					AI-generated summary 				 We introduc...
[21.08.2025 04:15] ********************************************************************************
[21.08.2025 04:15] Abstract 5. DuPO is a dual learning framework that generates annotation-free feedback using a generalized duality, enhancing performance across various tasks without relying on costly labels.  					AI-generated summary 				 We present DuPO, a dual learning-based preference optimization framework that generates ...
[21.08.2025 04:15] ********************************************************************************
[21.08.2025 04:15] Abstract 6. Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer model, enhances reasoning workload throughput and accuracy by replacing self-attention layers with Mamba-2 layers and using the Minitron strategy for compression.  					AI-generated summary 				 We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transfor...
[21.08.2025 04:15] ********************************************************************************
[21.08.2025 04:15] Abstract 7. A systematic study on quantizing diffusion large language models identifies challenges and evaluates state-of-the-art methods across various configurations to improve deployment on edge devices.  					AI-generated summary 				 Recent advances in diffusion large language models (dLLMs) have introduce...
[21.08.2025 04:15] Read previous papers.
[21.08.2025 04:15] Generating reviews via LLM API.
[21.08.2025 04:15] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#dataset", "#interpretability", "#science"], "emoji": "💹", "ru": {"title": "FinCDM: Точная диагностика финансовых ИИ-моделей", "desc": "FinCDM - это новая система оценки больших языковых моделей (LLM) в финансовой сфере на уровне знаний и навыков. Она и
[21.08.2025 04:15] Using data from previous issue: {"categories": ["#agents", "#survey", "#benchmark", "#reasoning"], "emoji": "🔮", "ru": {"title": "FutureX: Живой бенчмарк для оценки прогнозирования будущего ИИ-агентами", "desc": "FutureX - это динамический живой бенчмарк для оценки агентов на основе больших языковых моделей (LLM) в задачах прогноз
[21.08.2025 04:15] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#diffusion", "#3d"], "emoji": "🎨", "ru": {"title": "Tinker: Революция в 3D-редактировании без дополнительного обучения", "desc": "Tinker - это фреймворк для высокоточного 3D-редактирования с использованием предобученных диффузионных моделей. Он обеспечив
[21.08.2025 04:15] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#synthetic", "#3d", "#reasoning"], "emoji": "🧊", "ru": {"title": "От облака точек к коду: MeshCoder открывает новые возможности 3D-реконструкции", "desc": "MeshCoder - это новая система, которая преобразует сложные 3D-объекты из облаков точек в редактируем
[21.08.2025 04:15] Using data from previous issue: {"categories": ["#optimization", "#benchmark", "#multimodal", "#agents", "#video", "#3d", "#games", "#agi", "#reasoning"], "emoji": "🤖", "ru": {"title": "RynnEC: Новый уровень понимания видео для воплощенного ИИ", "desc": "RynnEC - это мультимодальная языковая модель большого масштаба для видео, исп
[21.08.2025 04:15] Querying the API.
[21.08.2025 04:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

DuPO is a dual learning framework that generates annotation-free feedback using a generalized duality, enhancing performance across various tasks without relying on costly labels.  					AI-generated summary 				 We present DuPO, a dual learning-based preference optimization framework that generates annotation-free feedback via a generalized duality. DuPO addresses two key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s reliance on costly labels and applicability restricted to verifiable tasks, and traditional dual learning's restriction to strictly dual task pairs (e.g., translation and back-translation). Specifically, DuPO decomposes a primal task's input into known and unknown components, then constructs its dual task to reconstruct the unknown part using the primal output and known information (e.g., reversing math solutions to recover hidden variables), broadening applicability to non-invertible tasks. The quality of this reconstruction serves as a self-supervised reward to optimize the primal task, synergizing with LLMs' ability to instantiate both tasks via a single model. Empirically, DuPO achieves substantial gains across diverse tasks: it enhances the average translation quality by 2.13 COMET over 756 directions, boosts the mathematical reasoning accuracy by an average of 6.4 points on three challenge benchmarks, and enhances performance by 9.3 points as an inference-time reranker (trading computation for accuracy). These results position DuPO as a scalable, general, and annotation-free paradigm for LLM optimization.
[21.08.2025 04:15] Response: {
  "desc": "DuPO - это фреймворк двойного обучения, который генерирует обратную связь без аннотаций, используя обобщенную двойственность. Он преодолевает ограничения предыдущих подходов, расширяя применимость на неинвертируемые задачи. DuPO разбивает входные данные основной задачи на известные и неизвестные компоненты, а затем конструирует двойственную задачу для восстановления неизвестной части. Качество этой реконструкции служит самоконтролируемым вознаграждением для оптимизации основной задачи, что позволяет достичь значительных улучшений в различных задачах, включая перевод и математические рассуждения.",

  "emoji": "🔄",

  "title": "DuPO: Оптимизация без аннотаций через двойственность"
}
[21.08.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DuPO is a dual learning framework that generates annotation-free feedback using a generalized duality, enhancing performance across various tasks without relying on costly labels.  					AI-generated summary 				 We present DuPO, a dual learning-based preference optimization framework that generates annotation-free feedback via a generalized duality. DuPO addresses two key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s reliance on costly labels and applicability restricted to verifiable tasks, and traditional dual learning's restriction to strictly dual task pairs (e.g., translation and back-translation). Specifically, DuPO decomposes a primal task's input into known and unknown components, then constructs its dual task to reconstruct the unknown part using the primal output and known information (e.g., reversing math solutions to recover hidden variables), broadening applicability to non-invertible tasks. The quality of this reconstruction serves as a self-supervised reward to optimize the primal task, synergizing with LLMs' ability to instantiate both tasks via a single model. Empirically, DuPO achieves substantial gains across diverse tasks: it enhances the average translation quality by 2.13 COMET over 756 directions, boosts the mathematical reasoning accuracy by an average of 6.4 points on three challenge benchmarks, and enhances performance by 9.3 points as an inference-time reranker (trading computation for accuracy). These results position DuPO as a scalable, general, and annotation-free paradigm for LLM optimization."

[21.08.2025 04:15] Response: ```python
['RL', 'RLHF', 'TRAINING']
```
[21.08.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"DuPO is a dual learning framework that generates annotation-free feedback using a generalized duality, enhancing performance across various tasks without relying on costly labels.  					AI-generated summary 				 We present DuPO, a dual learning-based preference optimization framework that generates annotation-free feedback via a generalized duality. DuPO addresses two key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s reliance on costly labels and applicability restricted to verifiable tasks, and traditional dual learning's restriction to strictly dual task pairs (e.g., translation and back-translation). Specifically, DuPO decomposes a primal task's input into known and unknown components, then constructs its dual task to reconstruct the unknown part using the primal output and known information (e.g., reversing math solutions to recover hidden variables), broadening applicability to non-invertible tasks. The quality of this reconstruction serves as a self-supervised reward to optimize the primal task, synergizing with LLMs' ability to instantiate both tasks via a single model. Empirically, DuPO achieves substantial gains across diverse tasks: it enhances the average translation quality by 2.13 COMET over 756 directions, boosts the mathematical reasoning accuracy by an average of 6.4 points on three challenge benchmarks, and enhances performance by 9.3 points as an inference-time reranker (trading computation for accuracy). These results position DuPO as a scalable, general, and annotation-free paradigm for LLM optimization."

[21.08.2025 04:15] Response: ```python
["OPTIMIZATION", "REASONING", "TRANSLATION"]
```
[21.08.2025 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DuPO is a novel dual learning framework that generates feedback without the need for expensive annotations, leveraging a generalized duality approach. It overcomes limitations of existing methods by allowing for the decomposition of tasks into known and unknown components, enabling the reconstruction of unknown parts from known information. This self-supervised feedback acts as a reward to optimize the primary task, making it applicable to a wider range of tasks beyond traditional dual pairs. Empirical results show that DuPO significantly improves performance in translation and mathematical reasoning tasks, demonstrating its effectiveness as a scalable solution for optimizing large language models.","title":"Annotation-Free Learning with DuPO: A New Era in Task Optimization"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DuPO is a novel dual learning framework that generates feedback without the need for expensive annotations, leveraging a generalized duality approach. It overcomes limitations of existing methods by allowing for the decomposition of tasks into known and unknown components, enabling the reconstruction of unknown parts from known information. This self-supervised feedback acts as a reward to optimize the primary task, making it applicable to a wider range of tasks beyond traditional dual pairs. Empirical results show that DuPO significantly improves performance in translation and mathematical reasoning tasks, demonstrating its effectiveness as a scalable solution for optimizing large language models.', title='Annotation-Free Learning with DuPO: A New Era in Task Optimization'))
[21.08.2025 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"DuPO是一种基于双重学习的框架，能够生成无注释的反馈，利用广义对偶性来提升多种任务的性能，而无需依赖昂贵的标签。它解决了强化学习与可验证奖励（RLVR）对标签的依赖和传统双重学习仅限于严格的双任务对的局限性。DuPO通过将原始任务的输入分解为已知和未知部分，构建其对偶任务以重建未知部分，从而扩展到不可逆任务。通过这种自监督奖励，DuPO在多个任务上实现了显著的性能提升，展示了其作为无注释的LLM优化范式的潜力。","title":"DuPO：无注释反馈的双重学习框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='DuPO是一种基于双重学习的框架，能够生成无注释的反馈，利用广义对偶性来提升多种任务的性能，而无需依赖昂贵的标签。它解决了强化学习与可验证奖励（RLVR）对标签的依赖和传统双重学习仅限于严格的双任务对的局限性。DuPO通过将原始任务的输入分解为已知和未知部分，构建其对偶任务以重建未知部分，从而扩展到不可逆任务。通过这种自监督奖励，DuPO在多个任务上实现了显著的性能提升，展示了其作为无注释的LLM优化范式的潜力。', title='DuPO：无注释反馈的双重学习框架'))
[21.08.2025 04:15] Querying the API.
[21.08.2025 04:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer model, enhances reasoning workload throughput and accuracy by replacing self-attention layers with Mamba-2 layers and using the Minitron strategy for compression.  					AI-generated summary 				 We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face.
[21.08.2025 04:15] Response: {
  "desc": "Nemotron-Nano-9B-v2 - это гибридная модель языка, сочетающая архитектуры Mamba и Transformer. Она заменяет слои самовнимания на слои Mamba-2 для повышения скорости вывода при генерации длинных цепочек рассуждений. Модель была предобучена на 20 триллионах токенов, а затем сжата с использованием стратегии Minitron. Nemotron-Nano-9B-v2 демонстрирует сопоставимую или лучшую точность на тестах рассуждений по сравнению с аналогичными моделями, обеспечивая при этом до 6 раз более высокую производительность.",
  "emoji": "🧠",
  "title": "Nemotron-Nano-9B-v2: Быстрее думай, лучше рассуждай"
}
[21.08.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer model, enhances reasoning workload throughput and accuracy by replacing self-attention layers with Mamba-2 layers and using the Minitron strategy for compression.  					AI-generated summary 				 We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face."

[21.08.2025 04:15] Response: ```python
['ARCHITECTURE', 'INFERENCE', 'TRAINING', 'DATASET']
```
[21.08.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer model, enhances reasoning workload throughput and accuracy by replacing self-attention layers with Mamba-2 layers and using the Minitron strategy for compression.  					AI-generated summary 				 We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face."

[21.08.2025 04:15] Response: ```python
["REASONING", "OPTIMIZATION", "OPEN_SOURCE"]
```
[21.08.2025 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Nemotron-Nano-9B-v2 is a new hybrid Mamba-Transformer model that improves the speed and accuracy of reasoning tasks. It replaces traditional self-attention layers with Mamba-2 layers, which enhances inference speed for generating complex reasoning outputs. The model is pre-trained on a massive dataset and uses the Minitron strategy for effective compression, allowing it to handle large input sizes efficiently. As a result, Nemotron-Nano-9B-v2 achieves superior performance compared to similar models, offering up to 6 times higher throughput in reasoning scenarios.","title":"Boosting Reasoning Speed and Accuracy with Nemotron-Nano-9B-v2"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Nemotron-Nano-9B-v2 is a new hybrid Mamba-Transformer model that improves the speed and accuracy of reasoning tasks. It replaces traditional self-attention layers with Mamba-2 layers, which enhances inference speed for generating complex reasoning outputs. The model is pre-trained on a massive dataset and uses the Minitron strategy for effective compression, allowing it to handle large input sizes efficiently. As a result, Nemotron-Nano-9B-v2 achieves superior performance compared to similar models, offering up to 6 times higher throughput in reasoning scenarios.', title='Boosting Reasoning Speed and Accuracy with Nemotron-Nano-9B-v2'))
[21.08.2025 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Nemotron-Nano-9B-v2是一种混合型Mamba-Transformer模型，旨在提高推理工作负载的吞吐量和准确性。该模型通过用Mamba-2层替换传统Transformer架构中的自注意力层，显著提升了推理速度。我们首先在20万亿个标记上预训练了一个120亿参数的模型，然后使用Minitron策略对其进行压缩和蒸馏，以支持在单个NVIDIA A10G GPU上进行128k标记的推理。与同类模型相比，Nemotron-Nano-9B-v2在推理基准测试中表现出相当或更好的准确性，同时在推理设置中实现了高达6倍的吞吐量提升。","title":"提升推理效率与准确性的混合模型"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Nemotron-Nano-9B-v2是一种混合型Mamba-Transformer模型，旨在提高推理工作负载的吞吐量和准确性。该模型通过用Mamba-2层替换传统Transformer架构中的自注意力层，显著提升了推理速度。我们首先在20万亿个标记上预训练了一个120亿参数的模型，然后使用Minitron策略对其进行压缩和蒸馏，以支持在单个NVIDIA A10G GPU上进行128k标记的推理。与同类模型相比，Nemotron-Nano-9B-v2在推理基准测试中表现出相当或更好的准确性，同时在推理设置中实现了高达6倍的吞吐量提升。', title='提升推理效率与准确性的混合模型'))
[21.08.2025 04:15] Querying the API.
[21.08.2025 04:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A systematic study on quantizing diffusion large language models identifies challenges and evaluates state-of-the-art methods across various configurations to improve deployment on edge devices.  					AI-generated summary 				 Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community.
[21.08.2025 04:15] Response: {
  "desc": "Это исследование посвящено квантованию диффузионных больших языковых моделей (dLLMs) для их развертывания на устройствах с ограниченными ресурсами. Авторы выявили проблему выбросов активации, которые затрудняют низкобитное квантование. Они провели систематическую оценку современных методов пост-тренировочного квантования (PTQ) для dLLMs по различным параметрам. Результаты анализа предоставляют практические рекомендации по квантованию dLLMs в разных конфигурациях.",
  "emoji": "🔬",
  "title": "Квантование диффузионных ЯМ: вызовы и решения"
}
[21.08.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A systematic study on quantizing diffusion large language models identifies challenges and evaluates state-of-the-art methods across various configurations to improve deployment on edge devices.  					AI-generated summary 				 Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community."

[21.08.2025 04:15] Response: ```python
["INFERENCE", "TRAINING"]
```
[21.08.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A systematic study on quantizing diffusion large language models identifies challenges and evaluates state-of-the-art methods across various configurations to improve deployment on edge devices.  					AI-generated summary 				 Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community."

[21.08.2025 04:15] Response: ```python
['DIFFUSION', 'OPTIMIZATION', 'OPEN_SOURCE']
```
[21.08.2025 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper investigates the challenges of quantizing diffusion large language models (dLLMs) to make them suitable for deployment on edge devices. It highlights the issue of activation outliers that complicate low-bit quantization, affecting the precision of model performance. The authors evaluate various state-of-the-art post-training quantization (PTQ) methods across different configurations, including bit-width and task types. Their findings aim to guide future research and improve the efficiency of dLLMs in practical applications.","title":"Optimizing Diffusion Models for Edge Deployment"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper investigates the challenges of quantizing diffusion large language models (dLLMs) to make them suitable for deployment on edge devices. It highlights the issue of activation outliers that complicate low-bit quantization, affecting the precision of model performance. The authors evaluate various state-of-the-art post-training quantization (PTQ) methods across different configurations, including bit-width and task types. Their findings aim to guide future research and improve the efficiency of dLLMs in practical applications.', title='Optimizing Diffusion Models for Edge Deployment'))
[21.08.2025 04:15] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本研究系统地探讨了扩散大型语言模型（dLLMs）的量化问题，识别了在边缘设备上部署时面临的挑战。我们发现激活异常值的存在，这些异常值的激活值异常大，影响了低位量化的精度。通过实施最先进的后训练量化（PTQ）方法，我们对多种任务类型和模型变体进行了全面评估。我们的研究为未来高效部署dLLMs提供了基础，并将发布所有代码和实验设置以支持社区。","title":"高效量化扩散语言模型的探索"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本研究系统地探讨了扩散大型语言模型（dLLMs）的量化问题，识别了在边缘设备上部署时面临的挑战。我们发现激活异常值的存在，这些异常值的激活值异常大，影响了低位量化的精度。通过实施最先进的后训练量化（PTQ）方法，我们对多种任务类型和模型变体进行了全面评估。我们的研究为未来高效部署dLLMs提供了基础，并将发布所有代码和实验设置以支持社区。', title='高效量化扩散语言模型的探索'))
[21.08.2025 04:15] Renaming data file.
[21.08.2025 04:15] Renaming previous data. hf_papers.json to ./d/2025-08-21.json
[21.08.2025 04:15] Saving new data file.
[21.08.2025 04:15] Generating page.
[21.08.2025 04:15] Renaming previous page.
[21.08.2025 04:15] Renaming previous data. index.html to ./d/2025-08-21.html
[21.08.2025 04:15] Writing result.
[21.08.2025 04:15] Renaming log file.
[21.08.2025 04:15] Renaming previous data. log.txt to ./logs/2025-08-21_last_log.txt
