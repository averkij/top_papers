[15.01.2026 05:28] Read previous papers.
[15.01.2026 05:28] Generating top page (month).
[15.01.2026 05:28] Writing top page (month).
[15.01.2026 06:36] Read previous papers.
[15.01.2026 06:36] Get feed.
[15.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07348
[15.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09688
[15.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09708
[15.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09136
[15.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09259
[15.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09274
[15.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09575
[15.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.06596
[15.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08605
[15.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09465
[15.01.2026 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2601.08955
[15.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09012
[15.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09697
[15.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09113
[15.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03928
[15.01.2026 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2601.07287
[15.01.2026 06:36] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[15.01.2026 06:36] No deleted papers detected.
[15.01.2026 06:36] Downloading and parsing papers (pdf, html). Total: 16.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.07348.
[15.01.2026 06:36] Extra JSON file exists (./assets/json/2601.07348.json), skip PDF parsing.
[15.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.07348.json), skip HTML parsing.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.09688.
[15.01.2026 06:36] Extra JSON file exists (./assets/json/2601.09688.json), skip PDF parsing.
[15.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.09688.json), skip HTML parsing.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.09708.
[15.01.2026 06:36] Extra JSON file exists (./assets/json/2601.09708.json), skip PDF parsing.
[15.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.09708.json), skip HTML parsing.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.09136.
[15.01.2026 06:36] Extra JSON file exists (./assets/json/2601.09136.json), skip PDF parsing.
[15.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.09136.json), skip HTML parsing.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.09259.
[15.01.2026 06:36] Extra JSON file exists (./assets/json/2601.09259.json), skip PDF parsing.
[15.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.09259.json), skip HTML parsing.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.09274.
[15.01.2026 06:36] Extra JSON file exists (./assets/json/2601.09274.json), skip PDF parsing.
[15.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.09274.json), skip HTML parsing.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.09575.
[15.01.2026 06:36] Extra JSON file exists (./assets/json/2601.09575.json), skip PDF parsing.
[15.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.09575.json), skip HTML parsing.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.06596.
[15.01.2026 06:36] Extra JSON file exists (./assets/json/2601.06596.json), skip PDF parsing.
[15.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.06596.json), skip HTML parsing.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.08605.
[15.01.2026 06:36] Extra JSON file exists (./assets/json/2601.08605.json), skip PDF parsing.
[15.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.08605.json), skip HTML parsing.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.09465.
[15.01.2026 06:36] Extra JSON file exists (./assets/json/2601.09465.json), skip PDF parsing.
[15.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.09465.json), skip HTML parsing.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.08955.
[15.01.2026 06:36] Downloading paper 2601.08955 from https://arxiv.org/pdf/2601.08955v1...
[15.01.2026 06:36] Extracting affiliations from text.
[15.01.2026 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models Youwei Liu2,1*, Jian Wang1, Hanlin Wang1, Beichen Guo1, Wenjie Li1 1 The Hong Kong Polytechnic University 2 Central South University loyiv5477@gmail.com jian51.wang@polyu.edu.hk {hanlin-henry.wang,beichen.guo}@connect.polyu.hk cswjli@comp.polyu.edu.hk 6 2 0 J 3 1 ] . [ 1 5 5 9 8 0 . 1 0 6 2 : r a "
[15.01.2026 06:36] Response: ```python
[
    "The Hong Kong Polytechnic University",
    "Central South University"
]
```
[15.01.2026 06:36] Deleting PDF ./assets/pdf/2601.08955.pdf.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.09012.
[15.01.2026 06:36] Extra JSON file exists (./assets/json/2601.09012.json), skip PDF parsing.
[15.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.09012.json), skip HTML parsing.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.09697.
[15.01.2026 06:36] Extra JSON file exists (./assets/json/2601.09697.json), skip PDF parsing.
[15.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.09697.json), skip HTML parsing.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.09113.
[15.01.2026 06:36] Extra JSON file exists (./assets/json/2601.09113.json), skip PDF parsing.
[15.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.09113.json), skip HTML parsing.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.03928.
[15.01.2026 06:36] Extra JSON file exists (./assets/json/2601.03928.json), skip PDF parsing.
[15.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.03928.json), skip HTML parsing.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.07287.
[15.01.2026 06:36] Downloading paper 2601.07287 from https://arxiv.org/pdf/2601.07287v1...
[15.01.2026 06:36] Extracting affiliations from text.
[15.01.2026 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 2 1 ] . [ 1 7 8 2 7 0 . 1 0 6 2 : r Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models Yuanyang Yin1,2,3 Yufan Deng3 Kaipeng Zhang2 Xiao Yang3 Shenghai Yuan3 Feng Zhao1* 1MoE Key Lab of BIPC, USTC 2Shanghai Innovation Institute 3ByteDance China "
[15.01.2026 06:36] Response: ```python
[
    "MoE Key Lab of BIPC, USTC",
    "Shanghai Innovation Institute",
    "ByteDance"
]
```
[15.01.2026 06:36] Deleting PDF ./assets/pdf/2601.07287.pdf.
[15.01.2026 06:36] Success.
[15.01.2026 06:36] Enriching papers with extra data.
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 0. Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.  					AI-generated summary 				 Self-evolution methods enhance code generation through iter...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 1. DeepResearchEval presents an automated framework for creating complex research tasks and evaluating them through agent-based methods that adapt to task specifics and verify facts without relying on citations.  					AI-generated summary 				 Deep research systems are widely used for multi-step web re...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 2. Fast-ThinkAct is an efficient vision-language-action framework that reduces inference latency by 89.3% through compact latent reasoning while maintaining long-horizon planning and few-shot adaptation capabilities.  					AI-generated summary 				 Vision-Language-Action (VLA) tasks require reasoning o...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 3. SkinFlow introduces a novel framework for dermatological vision-language modeling that improves diagnostic accuracy through optimized visual information transmission efficiency rather than parameter scaling alone.  					AI-generated summary 				 General-purpose Large Vision-Language Models (LVLMs), ...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 4. MAXS is a meta-adaptive reasoning framework for LLM agents that improves multi-tool reasoning through lookahead strategies and trajectory convergence mechanisms, balancing global effectiveness and computational efficiency.  					AI-generated summary 				 Large Language Model (LLM) Agents exhibit inh...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 5. Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, ...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 6. OpenVoxel enables open-vocabulary 3D scene understanding through training-free grouping and captioning of sparse voxels using Vision Language Models and Multi-modal Large Language Models.  					AI-generated summary 				 We propose OpenVoxel, a training-free algorithm for grouping and captioning spar...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 7. Research examines how large language models can be manipulated through preference-undermining attacks that exploit alignment objectives, revealing model vulnerabilities and proposing a factorial evaluation method for diagnosing alignment risks.  					AI-generated summary 				 Large Language Model (L...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 8. ExpSeek enables web agents to proactively seek experience during interaction by using entropy-based timing and tailored content, achieving significant performance improvements across multiple benchmarks.  					AI-generated summary 				 Experience intervention in web agents emerges as a promising tec...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 9. EvoFSM is a structured self-evolving framework for LLM agents that uses finite state machines to improve adaptability while maintaining control through constrained optimization and memory mechanisms.  					AI-generated summary 				 While LLM-based agents have shown promise for deep research, most ex...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 10. Imagine-then-Plan framework enables agent learning through adaptive lookahead imagination, combining imagined trajectories with current observations to guide policy learning in complex task scenarios.  					AI-generated summary 				 Recent advances in world models have shown promise for modeling fut...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 11. TranslateGemma enhances Gemma 3's multilingual capabilities through two-stage fine-tuning with synthetic and human-translated data, achieving superior translation quality with improved efficiency.  					AI-generated summary 				 We present TranslateGemma, a suite of open machine translation models b...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 12. Diffusion-based video generation is made more efficient through keyframe-based 3D reconstruction and rendering, enabling faster synthesis with maintained visual quality.  					AI-generated summary 				 Modern video generative models based on diffusion models can produce very realistic clips, but the...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 13. Memory mechanisms in large language models and multi-modal language models are categorized into implicit, explicit, and agentic paradigms, supporting enhanced reasoning, adaptability, and contextual fidelity through internal parameters, external knowledge storage, and persistent agent memory structu...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 14. FocusUI is an efficient UI grounding framework that reduces computational overhead by selecting relevant visual tokens while preserving positional continuity through a novel PosPad strategy.  					AI-generated summary 				 Vision-Language Models (VLMs) have shown remarkable performance in User Inter...
[15.01.2026 06:36] ********************************************************************************
[15.01.2026 06:36] Abstract 15. Diffusion Transformer-based image-to-video models suffer from condition isolation where visual attention becomes detached from text guidance; focal guidance addresses this through fine-grained semantic guidance and attention cache mechanisms.  					AI-generated summary 				 The task of Image-to-Vide...
[15.01.2026 06:36] Read previous papers.
[15.01.2026 06:36] Generating reviews via LLM API.
[15.01.2026 06:36] Using data from previous issue: {"categories": [], "emoji": "üß¨", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–µ–º–∞—è —ç–≤–æ–ª—é—Ü–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ Controlled Self-Evolution –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —á–µ—Ä–µ–∑ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç—Ä—ë—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö: —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —à
[15.01.2026 06:36] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agents", "#science"], "emoji": "üî¨", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–∏—Å—Ç–µ–º –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã —Å –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π —Ñ–∞–∫—Ç-–ø—Ä–æ–≤–µ—Ä–∫–æ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DeepResearchEval ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö 
[15.01.2026 06:36] Using data from previous issue: {"categories": ["#optimization", "#training", "#cv", "#transfer_learning", "#inference", "#robotics", "#reasoning", "#multimodal"], "emoji": "‚ö°", "ru": {"title": "–ë—ã—Å—Ç—Ä—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è —Ç–æ—á–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π: –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ", "desc": "Fast-ThinkAct ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ñ—Ä
[15.01.2026 06:36] Using data from previous issue: {"categories": ["#optimization", "#science", "#small_models", "#cv", "#healthcare", "#rl", "#architecture", "#benchmark", "#multimodal"], "emoji": "üî¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–¥–∞—á–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤–∞–∂–Ω–µ–µ –º–∞—Å—à—Ç–∞–±–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –¥–µ—Ä–º–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ", "desc": "SkinFlow –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω–Ω–æ–≤
[15.01.2026 06:36] Using data from previous issue: {"categories": ["#reasoning", "#optimization"], "emoji": "üß†", "ru": {"title": "–ü—Ä–µ–¥—É—Å–º–æ—Ç—Ä–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π", "desc": "MAXS ‚Äî —ç—Ç–æ –º–µ—Ç–∞-–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –º–Ω–æ–≥–æ—Ç–æ–ª–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Å
[15.01.2026 06:36] Using data from previous issue: {"categories": ["#reasoning", "#dataset", "#science", "#benchmark"], "emoji": "üß†", "ru": {"title": "–ü–∞–º—è—Ç—å –∫–∞–∫ –æ—Å–Ω–æ–≤–∞ –Ω–∞—É—á–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è: –∏–∑–º–µ—Ä–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —è–∫–æ—Ä–µ–π –∏ –∞—Ç—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞—É—á–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –ø–∞–º—è—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏
[15.01.2026 06:36] Using data from previous issue: {"categories": ["#cv", "#3d", "#multimodal", "#open_source"], "emoji": "üéØ", "ru": {"title": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ 3D —Å—Ü–µ–Ω —á–µ—Ä–µ–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è", "desc": "OpenVoxel ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è 3D —Å—Ü–µ–Ω –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –≤–æ–∫—Å–µ–ª–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–ª—è –Ω–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞
[15.01.2026 06:36] Using data from previous issue: {"categories": ["#alignment", "#security", "#interpretability", "#rlhf", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–í—ã—è–≤–ª–µ–Ω–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è: —Ñ–∞–∫—Ç–æ—Ä–∏–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –º–∞–Ω–∏–ø—É–ª—è—Ç–∏–≤–Ω—ã—Ö –∞—Ç–∞–∫ –Ω–∞ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —É—è–∑–≤–∏–º—ã –¥–ª—è –∞—Ç–∞–∫
[15.01.2026 06:36] Using data from previous issue: {"categories": ["#training", "#agents", "#benchmark", "#optimization"], "emoji": "üß†", "ru": {"title": "–ê–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –æ–ø—ã—Ç–∞ –∞–≥–µ–Ω—Ç–æ–º –≤ –Ω—É–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ ExpSeek, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –≤–µ–±-–∞–≥–µ–Ω—Ç–∞–º –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å –æ–ø—ã—Ç –≤–æ –≤—Ä–µ–º—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º. –í–º–µ—Å—Ç–æ –ø–∞—Å—Å–∏
[15.01.2026 06:36] Using data from previous issue: {"categories": ["#benchmark", "#hallucinations", "#reasoning", "#optimization", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–µ–º–∞—è —ç–≤–æ–ª—é—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω–µ—á–Ω—ã–µ –∞–≤—Ç–æ–º–∞—Ç—ã", "desc": "EvoFSM –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–∏ –¥–ª—è LLM-–∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª
[15.01.2026 06:36] Querying the API.
[15.01.2026 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Imagine-then-Plan framework enables agent learning through adaptive lookahead imagination, combining imagined trajectories with current observations to guide policy learning in complex task scenarios.  					AI-generated summary 				 Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (ITP), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially observable and imaginable Markov decision process to guide policy learning. We instantiate ITP with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that ITP significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.
[15.01.2026 06:36] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ Imagine-then-Plan (ITP), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞–º —É—á–∏—Ç—å—Å—è —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—É–¥—É—â–∏—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Å –ø–æ–º–æ—â—å—é –º–∏—Ä–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –ê–≥–µ–Ω—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é –æ–∫—Ä—É–∂–µ–Ω–∏—è, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–µ –≤–æ–æ–±—Ä–∞–∂–∞–µ–º—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, –≥–æ—Ä–∏–∑–æ–Ω—Ç –∫–æ—Ç–æ—Ä—ã—Ö –∞–¥–∞–ø—Ç–∏–≤–Ω–æ —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏ –∏ —ç—Ç–∞–ø–∞ –æ–±—É—á–µ–Ω–∏—è. –í–æ–æ–±—Ä–∞–∂–∞–µ–º—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –±–æ–≥–∞—Ç—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –±—É–¥—É—â–∏—Ö –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è—Ö –¥–µ–π—Å—Ç–≤–∏–π –∏ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è —Å —Ç–µ–∫—É—â–∏–º–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è–º–∏ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —á–∞—Å—Ç–∏—á–Ω–æ –Ω–∞–±–ª—é–¥–∞–µ–º–æ–≥–æ –ú–∞—Ä–∫–æ–≤—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ITP –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∏ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–æ–≤ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á.",
  "emoji": "üß†",
  "title": "–£—á–∏–º—Å—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ: –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–≤–∏–¥–µ–Ω–∏–µ –¥–ª—è —É–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤"
}
```
[15.01.2026 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Imagine-then-Plan framework enables agent learning through adaptive lookahead imagination, combining imagined trajectories with current observations to guide policy learning in complex task scenarios.  					AI-generated summary 				 Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (ITP), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially observable and imaginable Markov decision process to guide policy learning. We instantiate ITP with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that ITP significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks."

[15.01.2026 06:36] Response: ```python
["AGENTS", "RL", "BENCHMARK"]
```
[15.01.2026 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Imagine-then-Plan framework enables agent learning through adaptive lookahead imagination, combining imagined trajectories with current observations to guide policy learning in complex task scenarios.  					AI-generated summary 				 Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (ITP), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially observable and imaginable Markov decision process to guide policy learning. We instantiate ITP with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that ITP significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks."

[15.01.2026 06:36] Response: ```python
['REASONING', 'OPTIMIZATION']
```

**Justification:**

- **REASONING**: The paper explicitly discusses enhancing agents' "reasoning capability" through lookahead imagination and multi-step planning to guide policy learning in complex task scenarios. The framework enables agents to reason about future consequences and task progress.

- **OPTIMIZATION**: The paper addresses training optimization through the proposed Imagine-then-Plan framework, which optimizes policy learning by combining imagined trajectories with observations. The adaptive lookahead mechanism represents an optimization strategy for agent learning.
[15.01.2026 06:36] Error. Failed to parse JSON from LLM. ["REASONING", "OPTIMIZATION"]


**Justification:**

- **REASONING**: The paper explicitly discusses enhancing agents" "reasoning capability" through lookahead imagination and multi-step planning to guide policy learning in complex task scenarios. The framework enables agents to reason about future consequences and task progress.

- **OPTIMIZATION**: The paper addresses training optimization through the proposed Imagine-then-Plan framework, which optimizes policy learning by combining imagined trajectories with observations. The adaptive lookahead mechanism represents an optimization strategy for agent learning.
[15.01.2026 06:36] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The Imagine-then-Plan (ITP) framework enhances agent learning by using adaptive lookahead imagination to create multi-step imagined trajectories. This approach allows agents to combine their current observations with predictions about future states, improving their decision-making in complex tasks. By introducing an adaptive mechanism that adjusts the imagination horizon based on task requirements, ITP effectively balances immediate goals with overall task progress. Experimental results show that ITP outperforms existing methods, demonstrating its ability to improve reasoning and planning capabilities in agents.","title":"Imagine-then-Plan: Enhancing Agent Learning through Adaptive Lookahead Imagination"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The Imagine-then-Plan (ITP) framework enhances agent learning by using adaptive lookahead imagination to create multi-step imagined trajectories. This approach allows agents to combine their current observations with predictions about future states, improving their decision-making in complex tasks. By introducing an adaptive mechanism that adjusts the imagination horizon based on task requirements, ITP effectively balances immediate goals with overall task progress. Experimental results show that ITP outperforms existing methods, demonstrating its ability to improve reasoning and planning capabilities in agents.', title='Imagine-then-Plan: Enhancing Agent Learning through Adaptive Lookahead Imagination'))
[15.01.2026 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Imagine-then-PlanÔºàITPÔºâÊ°ÜÊû∂ÈÄöËøáËá™ÈÄÇÂ∫îÁöÑÂâçÁûªÊÄßÊÉ≥Ë±°ÔºåÂ∏ÆÂä©Êô∫ËÉΩ‰ΩìÂú®Â§çÊùÇ‰ªªÂä°Âú∫ÊôØ‰∏≠Â≠¶‰π†„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫ÜÊÉ≥Ë±°ÁöÑËΩ®ËøπÂíåÂΩìÂâçËßÇÂØüÔºåÊåáÂØºÁ≠ñÁï•Â≠¶‰π†„ÄÇITPÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËá™ÈÄÇÂ∫îÂâçÁûªÊú∫Âà∂ÔºåÊ†πÊçÆ‰ªªÂä°ÂíåÈò∂ÊÆµÁöÑ‰∏çÂêåË∞ÉÊï¥ÊÉ≥Ë±°ÁöÑËåÉÂõ¥Ôºå‰ªéËÄåÊèê‰æõÂÖ≥‰∫éÊú™Êù•ÁªìÊûúÁöÑ‰∏∞ÂØå‰ø°Âè∑„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåITPÂú®Â§ö‰∏™Êô∫ËÉΩ‰ΩìÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÂÖ∂‰ªñÁ´û‰∫âÊñπÊ≥ïÔºåÂ¢ûÂº∫‰∫ÜÊô∫ËÉΩ‰ΩìÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ","title":"ÊÉ≥Ë±°ÂÖàË°åÔºåËßÑÂàíÊú™Êù•"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Imagine-then-PlanÔºàITPÔºâÊ°ÜÊû∂ÈÄöËøáËá™ÈÄÇÂ∫îÁöÑÂâçÁûªÊÄßÊÉ≥Ë±°ÔºåÂ∏ÆÂä©Êô∫ËÉΩ‰ΩìÂú®Â§çÊùÇ‰ªªÂä°Âú∫ÊôØ‰∏≠Â≠¶‰π†„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫ÜÊÉ≥Ë±°ÁöÑËΩ®ËøπÂíåÂΩìÂâçËßÇÂØüÔºåÊåáÂØºÁ≠ñÁï•Â≠¶‰π†„ÄÇITPÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËá™ÈÄÇÂ∫îÂâçÁûªÊú∫Âà∂ÔºåÊ†πÊçÆ‰ªªÂä°ÂíåÈò∂ÊÆµÁöÑ‰∏çÂêåË∞ÉÊï¥ÊÉ≥Ë±°ÁöÑËåÉÂõ¥Ôºå‰ªéËÄåÊèê‰æõÂÖ≥‰∫éÊú™Êù•ÁªìÊûúÁöÑ‰∏∞ÂØå‰ø°Âè∑„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåITPÂú®Â§ö‰∏™Êô∫ËÉΩ‰ΩìÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÂÖ∂‰ªñÁ´û‰∫âÊñπÊ≥ïÔºåÂ¢ûÂº∫‰∫ÜÊô∫ËÉΩ‰ΩìÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ', title='ÊÉ≥Ë±°ÂÖàË°åÔºåËßÑÂàíÊú™Êù•'))
[15.01.2026 06:37] Using data from previous issue: {"categories": ["#training", "#open_source", "#multilingual", "#benchmark", "#synthetic", "#machine_translation", "#multimodal", "#optimization", "#rl"], "emoji": "üåç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "TranslateGemma –ø
[15.01.2026 06:37] Using data from previous issue: {"categories": ["#3d", "#inference", "#diffusion", "#optimization", "#robotics", "#video"], "emoji": "üé¨", "ru": {"title": "–ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ –∫–∞–¥—Ä—ã –∏ 3D-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –∏ —Ç—Ä—ë
[15.01.2026 06:37] Using data from previous issue: {"categories": ["#rag", "#alignment", "#benchmark", "#reasoning", "#survey", "#multimodal", "#interpretability", "#long_context", "#agents", "#architecture"], "emoji": "üß†", "ru": {"title": "–¢—Ä–∏ —É—Ä–æ–≤–Ω—è –ø–∞–º—è—Ç–∏: –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫ –∞–≥–µ–Ω—Ç–∞–º", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏
[15.01.2026 06:37] Using data from previous issue: {"categories": ["#cv", "#inference", "#multimodal"], "emoji": "üéØ", "ru": {"title": "–£–º–Ω–∞—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–∏–≤–Ω—ã–π –æ—Ç–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤", "desc": "FocusUI ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ª–æ–∫–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º, –∫–æ—Ç–æ—Ä–∞—è
[15.01.2026 06:37] Querying the API.
[15.01.2026 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Diffusion Transformer-based image-to-video models suffer from condition isolation where visual attention becomes detached from text guidance; focal guidance addresses this through fine-grained semantic guidance and attention cache mechanisms.  					AI-generated summary 				 The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model's learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\%).
[15.01.2026 06:37] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –∏–∑–æ–ª—è—Ü–∏–∏ —É—Å–ª–æ–≤–∏–π –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç–∞, –∫–æ–≥–¥–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –æ—Ç—Ä—ã–≤–∞–µ—Ç—Å—è –æ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Focal Guidance, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ CLIP –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–∞–¥—Ä–µ –∏ –º–µ—Ö–∞–Ω–∏–∑–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –º–µ–∂–¥—É —Å–ª–æ—è–º–∏ –º–æ–¥–µ–ª–∏. –ü–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –¥–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: –¥–µ—Ç–∞–ª—å–Ω–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ä—Ç –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —É—Å–∏–ª–∏–≤–∞—é—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ—Å—Ç—å —Å–ª–∞–±—ã—Ö –ø–æ —Å–µ–º–∞–Ω—Ç–∏–∫–µ —Å–ª–æ–µ–≤ –∏ –ø–æ–≤—ã—à–∞—é—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ –º–æ–¥–µ–ª—è—Ö Image-to-Video –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª—è—Ö.",
  "emoji": "üé¨",
  "title": "–§–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞ —Ç–µ–∫—Å—Ç–µ: –∫–∞–∫ –∑–∞—Å—Ç–∞–≤–∏—Ç—å –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—å —Å–ª—É—à–∞—Ç—å —Å–ª–æ–≤–µ—Å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"
}
```
[15.01.2026 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Diffusion Transformer-based image-to-video models suffer from condition isolation where visual attention becomes detached from text guidance; focal guidance addresses this through fine-grained semantic guidance and attention cache mechanisms.  					AI-generated summary 				 The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model's learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\%)."

[15.01.2026 06:37] Response: ```python
["VIDEO", "MULTIMODAL", "BENCHMARK"]
```
[15.01.2026 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Diffusion Transformer-based image-to-video models suffer from condition isolation where visual attention becomes detached from text guidance; focal guidance addresses this through fine-grained semantic guidance and attention cache mechanisms.  					AI-generated summary 				 The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model's learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\%)."

[15.01.2026 06:37] Response: ```python
['DIFFUSION']
```
[15.01.2026 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper addresses the challenge of condition isolation in Diffusion Transformer-based image-to-video (I2V) models, where visual attention can become disconnected from text prompts. The authors introduce Focal Guidance (FG), which enhances the model\'s ability to follow textual instructions by improving the performance of Semantic-Weak Layers. FG employs Fine-grained Semantic Guidance (FSG) to pinpoint important areas in the reference image and uses Attention Cache to transfer attention from more responsive layers to those that are weak. The proposed methods significantly improve adherence to text prompts, as demonstrated by their benchmark results, showing notable performance gains in I2V tasks.","title":"Enhancing Text-Visual Alignment in Image-to-Video Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper addresses the challenge of condition isolation in Diffusion Transformer-based image-to-video (I2V) models, where visual attention can become disconnected from text prompts. The authors introduce Focal Guidance (FG), which enhances the model's ability to follow textual instructions by improving the performance of Semantic-Weak Layers. FG employs Fine-grained Semantic Guidance (FSG) to pinpoint important areas in the reference image and uses Attention Cache to transfer attention from more responsive layers to those that are weak. The proposed methods significantly improve adherence to text prompts, as demonstrated by their benchmark results, showing notable performance gains in I2V tasks.", title='Enhancing Text-Visual Alignment in Image-to-Video Generation'))
[15.01.2026 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂü∫‰∫éÊâ©Êï£ÂèòÊç¢Âô®ÁöÑÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàêÊ®°Âûã‰∏≠ÁöÑÊù°‰ª∂ÈöîÁ¶ªÈóÆÈ¢òÔºåÂØºËá¥ËßÜËßâÊ≥®ÊÑèÂäõ‰∏éÊñáÊú¨ÊåáÂØºËÑ±ËäÇ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊèêÂá∫‰∫ÜÁÑ¶ÁÇπÂºïÂØºÔºàFocal GuidanceÔºâÊñπÊ≥ïÔºåÈÄöËøáÁªÜÁ≤íÂ∫¶ËØ≠‰πâÂºïÂØºÂíåÊ≥®ÊÑèÂäõÁºìÂ≠òÊú∫Âà∂Êù•Â¢ûÂº∫ËØ≠‰πâÂº±Â±ÇÁöÑÊéßÂà∂ËÉΩÂäõ„ÄÇÁªÜÁ≤íÂ∫¶ËØ≠‰πâÂºïÂØºÂà©Áî®CLIPËØÜÂà´ÂèÇËÄÉÂ∏ß‰∏≠ÁöÑÂÖ≥ÈîÆÂå∫ÂüüÔºå‰Ωú‰∏∫ÂºïÂØºËØ≠‰πâÂº±Â±ÇÁöÑÈîöÁÇπ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁÑ¶ÁÇπÂºïÂØºÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÂØπÊñáÊú¨Êåá‰ª§ÁöÑÈÅµÂæ™ËÉΩÂäõ„ÄÇ","title":"ÊèêÂçáÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàêÁöÑÊñáÊú¨ÈÅµÂæ™ËÉΩÂäõ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂü∫‰∫éÊâ©Êï£ÂèòÊç¢Âô®ÁöÑÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàêÊ®°Âûã‰∏≠ÁöÑÊù°‰ª∂ÈöîÁ¶ªÈóÆÈ¢òÔºåÂØºËá¥ËßÜËßâÊ≥®ÊÑèÂäõ‰∏éÊñáÊú¨ÊåáÂØºËÑ±ËäÇ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊèêÂá∫‰∫ÜÁÑ¶ÁÇπÂºïÂØºÔºàFocal GuidanceÔºâÊñπÊ≥ïÔºåÈÄöËøáÁªÜÁ≤íÂ∫¶ËØ≠‰πâÂºïÂØºÂíåÊ≥®ÊÑèÂäõÁºìÂ≠òÊú∫Âà∂Êù•Â¢ûÂº∫ËØ≠‰πâÂº±Â±ÇÁöÑÊéßÂà∂ËÉΩÂäõ„ÄÇÁªÜÁ≤íÂ∫¶ËØ≠‰πâÂºïÂØºÂà©Áî®CLIPËØÜÂà´ÂèÇËÄÉÂ∏ß‰∏≠ÁöÑÂÖ≥ÈîÆÂå∫ÂüüÔºå‰Ωú‰∏∫ÂºïÂØºËØ≠‰πâÂº±Â±ÇÁöÑÈîöÁÇπ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁÑ¶ÁÇπÂºïÂØºÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÂØπÊñáÊú¨Êåá‰ª§ÁöÑÈÅµÂæ™ËÉΩÂäõ„ÄÇ', title='ÊèêÂçáÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàêÁöÑÊñáÊú¨ÈÅµÂæ™ËÉΩÂäõ'))
[15.01.2026 06:37] Renaming data file.
[15.01.2026 06:37] Renaming previous data. hf_papers.json to ./d/2026-01-15.json
[15.01.2026 06:37] Saving new data file.
[15.01.2026 06:37] Generating page.
[15.01.2026 06:37] Renaming previous page.
[15.01.2026 06:37] Renaming previous data. index.html to ./d/2026-01-15.html
[15.01.2026 06:37] Writing result.
[15.01.2026 06:37] Renaming log file.
[15.01.2026 06:37] Renaming previous data. log.txt to ./logs/2026-01-15_last_log.txt
