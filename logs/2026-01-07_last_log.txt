[07.01.2026 05:27] Read previous papers.
[07.01.2026 05:27] Generating top page (month).
[07.01.2026 05:27] Writing top page (month).
[07.01.2026 06:36] Read previous papers.
[07.01.2026 06:36] Get feed.
[07.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03252
[07.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.01554
[07.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03193
[07.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.01874
[07.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03233
[07.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02780
[07.01.2026 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2601.03044
[07.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02427
[07.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02785
[07.01.2026 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2601.03227
[07.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03194
[07.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03153
[07.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02439
[07.01.2026 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.01720
[07.01.2026 06:36] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.01.2026 06:36] No deleted papers detected.
[07.01.2026 06:36] Downloading and parsing papers (pdf, html). Total: 14.
[07.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.03252.
[07.01.2026 06:36] Extra JSON file exists (./assets/json/2601.03252.json), skip PDF parsing.
[07.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.03252.json), skip HTML parsing.
[07.01.2026 06:36] Success.
[07.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.01554.
[07.01.2026 06:36] Extra JSON file exists (./assets/json/2601.01554.json), skip PDF parsing.
[07.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.01554.json), skip HTML parsing.
[07.01.2026 06:36] Success.
[07.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.03193.
[07.01.2026 06:36] Extra JSON file exists (./assets/json/2601.03193.json), skip PDF parsing.
[07.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.03193.json), skip HTML parsing.
[07.01.2026 06:36] Success.
[07.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.01874.
[07.01.2026 06:36] Extra JSON file exists (./assets/json/2601.01874.json), skip PDF parsing.
[07.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.01874.json), skip HTML parsing.
[07.01.2026 06:36] Success.
[07.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.03233.
[07.01.2026 06:36] Extra JSON file exists (./assets/json/2601.03233.json), skip PDF parsing.
[07.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.03233.json), skip HTML parsing.
[07.01.2026 06:36] Success.
[07.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.02780.
[07.01.2026 06:36] Extra JSON file exists (./assets/json/2601.02780.json), skip PDF parsing.
[07.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.02780.json), skip HTML parsing.
[07.01.2026 06:36] Success.
[07.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.03044.
[07.01.2026 06:36] Downloading paper 2601.03044 from https://arxiv.org/pdf/2601.03044v1...
[07.01.2026 06:36] Extracting affiliations from text.
[07.01.2026 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"SOP: Scalable Online Post-Training System for Vision-Language-Action Models Mingjie Pan*1 Siyuan Feng*1 Qinglin Zhang1 Xinchen Li1 Jianheng Song1 Chendi Qu1 Yi Wang1,2 Chuankang Li Ziyu Xiong1 Zhi Chen1 Yi Liu1 Jianlan Luo1,2 1Agibot Research. 2Shanghai Innovation Institute. https://www.agibot.com/research/sop 6 2 0 J 6 ] . [ 1 4 4 0 3 0 . 1 0 6 2 : r AbstractVision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from realworld interaction. We introduce Scalable Online Post-training (SOP) system that enables online, distributed, multi-task posttraining of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through closed-loop architecture in which fleet of robots continuously streams onpolicy experience and human intervention signals to centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental"
[07.01.2026 06:36] Response: ```python
[
    "Agibot Research",
    "Shanghai Innovation Institute"
]
```
[07.01.2026 06:36] Deleting PDF ./assets/pdf/2601.03044.pdf.
[07.01.2026 06:36] Success.
[07.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.02427.
[07.01.2026 06:36] Extra JSON file exists (./assets/json/2601.02427.json), skip PDF parsing.
[07.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.02427.json), skip HTML parsing.
[07.01.2026 06:36] Success.
[07.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.02785.
[07.01.2026 06:36] Extra JSON file exists (./assets/json/2601.02785.json), skip PDF parsing.
[07.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.02785.json), skip HTML parsing.
[07.01.2026 06:36] Success.
[07.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.03227.
[07.01.2026 06:36] Downloading paper 2601.03227 from https://arxiv.org/pdf/2601.03227v1...
[07.01.2026 06:36] Extracting affiliations from text.
[07.01.2026 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization Ruixing Zhang1, Zihan Liu1,2, Leilei Sun1,3, Tongyu Zhu1,3, Weifeng Lv1 1The State Key Laboratory of Complex and Critical Software Environment, Beihang University 2Shanghai AI Laboratory, 3The Key Laboratory of Data Science and Intelligent Computing, International Innovation Institute, Beihang University 6 2 0 2 6 ] . [ 1 7 2 2 3 0 . 1 0 6 2 : r a "
[07.01.2026 06:36] Response: ```python
[
    "The State Key Laboratory of Complex and Critical Software Environment, Beihang University",
    "Shanghai AI Laboratory",
    "The Key Laboratory of Data Science and Intelligent Computing, International Innovation Institute, Beihang University"
]
```
[07.01.2026 06:36] Deleting PDF ./assets/pdf/2601.03227.pdf.
[07.01.2026 06:36] Success.
[07.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.03194.
[07.01.2026 06:36] Extra JSON file exists (./assets/json/2601.03194.json), skip PDF parsing.
[07.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.03194.json), skip HTML parsing.
[07.01.2026 06:36] Success.
[07.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.03153.
[07.01.2026 06:36] Extra JSON file exists (./assets/json/2601.03153.json), skip PDF parsing.
[07.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.03153.json), skip HTML parsing.
[07.01.2026 06:36] Success.
[07.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.02439.
[07.01.2026 06:36] Extra JSON file exists (./assets/json/2601.02439.json), skip PDF parsing.
[07.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.02439.json), skip HTML parsing.
[07.01.2026 06:36] Success.
[07.01.2026 06:36] Downloading and parsing paper https://huggingface.co/papers/2601.01720.
[07.01.2026 06:36] Extra JSON file exists (./assets/json/2601.01720.json), skip PDF parsing.
[07.01.2026 06:36] Paper image links file exists (./assets/img_data/2601.01720.json), skip HTML parsing.
[07.01.2026 06:36] Success.
[07.01.2026 06:36] Enriching papers with extra data.
[07.01.2026 06:36] ********************************************************************************
[07.01.2026 06:36] Abstract 0. InfiniDepth represents depth as neural implicit fields using a local implicit decoder, enabling continuous 2D coordinate querying for arbitrary-resolution depth estimation and superior performance in fine-detail regions.  					AI-generated summary 				 Existing depth estimation methods are fundament...
[07.01.2026 06:36] ********************************************************************************
[07.01.2026 06:36] Abstract 1. A unified multimodal large language model for end-to-end speaker-attributed, time-stamped transcription with extended context window and strong generalization across benchmarks.  					AI-generated summary 				 Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and ...
[07.01.2026 06:36] ********************************************************************************
[07.01.2026 06:36] Abstract 2. UniCorn, a self-improvement framework for unified multimodal models, addresses generation gaps through self-play and cognitive pattern reconstruction, achieving state-of-the-art results in text-to-image generation.  					AI-generated summary 				 While Unified Multimodal Models (UMMs) have achieved ...
[07.01.2026 06:36] ********************************************************************************
[07.01.2026 06:36] Abstract 3. Visual mathematical problem solving remains challenging for multimodal large language models, prompting the development of CogFlow, a cognitive-inspired three-stage framework that enhances perception, internalization, and reasoning through synergistic rewards and visual-gated policy optimization.  	...
[07.01.2026 06:36] ********************************************************************************
[07.01.2026 06:36] Abstract 4. LTX-2 is an open-source audiovisual diffusion model that generates synchronized video and audio content using a dual-stream transformer architecture with cross-modal attention and classifier-free guidance.  					AI-generated summary 				 Recent text-to-video diffusion models can generate compelling ...
[07.01.2026 06:36] ********************************************************************************
[07.01.2026 06:36] Abstract 5. MiMo-V2-Flash is a sparse Mixture-of-Experts model with hybrid attention architecture and efficient distillation technique that achieves strong performance with reduced parameters and improved inference speed.  					AI-generated summary 				 We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model...
[07.01.2026 06:36] ********************************************************************************
[07.01.2026 06:36] Abstract 6. A scalable online post-training system enables real-world robot policy adaptation through distributed, multi-task learning that maintains generality while improving task proficiency.  					AI-generated summary 				 Vision-language-action (VLA) models achieve strong generalization through large-scale...
[07.01.2026 06:36] ********************************************************************************
[07.01.2026 06:36] Abstract 7. NitroGen is a vision-action foundation model trained on extensive gameplay data that demonstrates strong cross-game generalization and effective transfer learning capabilities.  					AI-generated summary 				 We introduce NitroGen, a vision-action foundation model for generalist gaming agents that i...
[07.01.2026 06:36] ********************************************************************************
[07.01.2026 06:36] Abstract 8. DreamStyle is a unified video stylization framework that supports multiple style conditions while addressing style inconsistency and temporal flicker through a specialized data curation pipeline and LoRA training approach.  					AI-generated summary 				 Video stylization, an important downstream ta...
[07.01.2026 06:36] ********************************************************************************
[07.01.2026 06:36] Abstract 9. Audio geo-localization benchmark AGL1K is introduced to advance audio language models' geospatial reasoning capabilities through curated audio clips and evaluation across multiple models.  					AI-generated summary 				 Geo-localization aims to infer the geographic origin of a given signal. In compu...
[07.01.2026 06:36] ********************************************************************************
[07.01.2026 06:36] Abstract 10. A novel explainability-guided training framework for hate speech detection in Indic languages that combines large language models with attention-enhancing techniques and provides human-annotated rationales for improved performance and interpretability.  					AI-generated summary 				 Hate speech det...
[07.01.2026 06:36] ********************************************************************************
[07.01.2026 06:36] Abstract 11. Parallel Latent Reasoning framework improves sequential recommendation by exploring multiple diverse reasoning trajectories simultaneously through learnable trigger tokens and adaptive aggregation.  					AI-generated summary 				 Capturing complex user preferences from sparse behavioral sequences re...
[07.01.2026 06:36] ********************************************************************************
[07.01.2026 06:36] Abstract 12. WebGym presents a large-scale open-source environment for training visual web agents using reinforcement learning with high-throughput asynchronous sampling, achieving superior performance on unseen websites compared to proprietary models.  					AI-generated summary 				 We present WebGym, the large...
[07.01.2026 06:36] ********************************************************************************
[07.01.2026 06:36] Abstract 13. A new large-scale video dataset and framework are presented that enable effective first-frame propagation without runtime guidance through adaptive spatio-temporal positional encoding and self-distillation techniques.  					AI-generated summary 				 First-Frame Propagation (FFP) offers a promising p...
[07.01.2026 06:36] Read previous papers.
[07.01.2026 06:36] Generating reviews via LLM API.
[07.01.2026 06:36] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#cv"], "emoji": "ğŸŒŠ", "ru": {"title": "ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ", "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ InfiniDepth â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑÑ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¾Ğº
[07.01.2026 06:36] Using data from previous issue: {"categories": ["#multimodal", "#audio", "#benchmark"], "emoji": "ğŸ™ï¸", "ru": {"title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸ĞµĞ¹ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MOSS Transcribe Diarize, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑĞºĞ²Ğ¾Ğ·
[07.01.2026 06:36] Using data from previous issue: {"categories": ["#training", "#benchmark", "#multimodal"], "emoji": "ğŸ¦„", "ru": {"title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¸Ğ³Ñ€Ñƒ Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ", "desc": "UniCorn â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ñ‚ĞµĞºÑ
[07.01.2026 06:36] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#reasoning", "#optimization", "#math", "#rlhf", "#benchmark"], "emoji": "ğŸ§ ", "ru": {"title": "ĞšĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº: Ğ¾Ñ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğº Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° CogFlow â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿ÑĞ¸Ñ…Ğ¾
[07.01.2026 06:36] Using data from previous issue: {"categories": ["#video", "#multimodal", "#audio", "#open_source", "#architecture", "#diffusion", "#training"], "emoji": "ğŸ¬", "ru": {"title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ·Ğ²ÑƒĞºĞ° Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ", "desc": "LTX-2 â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ 
[07.01.2026 06:36] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#reasoning", "#open_source", "#architecture", "#long_context", "#inference", "#agents", "#training"], "emoji": "âš¡", "ru": {"title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "MiMo-V2-Flash â€” ÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½
[07.01.2026 06:36] Querying the API.
[07.01.2026 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A scalable online post-training system enables real-world robot policy adaptation through distributed, multi-task learning that maintains generality while improving task proficiency.  					AI-generated summary 				 Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.
[07.01.2026 06:36] Response: ```json
{
  "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Scalable Online Post-training (SOP) Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ…-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹Ğ½Ñ‹Ñ… (VLA) Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ³Ğ´Ğµ Ñ„Ğ»Ğ¾Ñ‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ on-policy Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ñ†ĞµĞ½Ñ‚Ñ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ SOP Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° Ñ‡Ğ°ÑÑ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ¼, Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ.",
  "emoji": "ğŸ¤–",
  "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ"
}
```
[07.01.2026 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A scalable online post-training system enables real-world robot policy adaptation through distributed, multi-task learning that maintains generality while improving task proficiency.  					AI-generated summary 				 Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world."

[07.01.2026 06:37] Response: ```python
["ROBOTICS", "MULTIMODAL", "TRAINING", "RL"]
```
[07.01.2026 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A scalable online post-training system enables real-world robot policy adaptation through distributed, multi-task learning that maintains generality while improving task proficiency.  					AI-generated summary 				 Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world."

[07.01.2026 06:37] Response: ```python
["OPTIMIZATION", "TRANSFER_LEARNING"]
```

**Justification:**

- **OPTIMIZATION**: The paper discusses post-training optimization methods (HG-DAgger and RECAP) for improving model performance on real-world tasks, and presents a system designed to efficiently optimize VLA models through on-policy learning.

- **TRANSFER_LEARNING**: The paper explicitly addresses knowledge transfer from large-scale pre-trained generalist VLA models to specific real-world tasks while maintaining generality across multiple tasks, which is a core transfer learning concept.
[07.01.2026 06:37] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "TRANSFER_LEARNING"]


**Justification:**

- **OPTIMIZATION**: The paper discusses post-training optimization methods (HG-DAgger and RECAP) for improving model performance on real-world tasks, and presents a system designed to efficiently optimize VLA models through on-policy learning.

- **TRANSFER_LEARNING**: The paper explicitly addresses knowledge transfer from large-scale pre-trained generalist VLA models to specific real-world tasks while maintaining generality across multiple tasks, which is a core transfer learning concept.
[07.01.2026 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a Scalable Online Post-training (SOP) system designed for improving robot policy adaptation in real-world scenarios. SOP allows multiple robots to learn and adapt their tasks simultaneously by continuously sharing their experiences and receiving updates from a centralized cloud learner. The system maintains the generality of the models while enhancing their proficiency in specific tasks through a closed-loop architecture. By utilizing both interactive imitation learning and reinforcement learning, SOP demonstrates significant performance improvements across various manipulation tasks, achieving effective post-training in just hours of real-world interaction.","title":"Scalable Learning for Real-World Robot Adaptation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a Scalable Online Post-training (SOP) system designed for improving robot policy adaptation in real-world scenarios. SOP allows multiple robots to learn and adapt their tasks simultaneously by continuously sharing their experiences and receiving updates from a centralized cloud learner. The system maintains the generality of the models while enhancing their proficiency in specific tasks through a closed-loop architecture. By utilizing both interactive imitation learning and reinforcement learning, SOP demonstrates significant performance improvements across various manipulation tasks, achieving effective post-training in just hours of real-world interaction.', title='Scalable Learning for Real-World Robot Adaptation'))
[07.01.2026 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¯æ‰©å±•çš„åœ¨çº¿åè®­ç»ƒç³»ç»Ÿï¼ˆSOPï¼‰ï¼Œæ—¨åœ¨é€šè¿‡åˆ†å¸ƒå¼å¤šä»»åŠ¡å­¦ä¹ å®ç°æœºå™¨äººç­–ç•¥çš„é€‚åº”ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­ç›´æ¥è¿›è¡Œåœ¨çº¿åè®­ç»ƒï¼Œç»“åˆæ‰§è¡Œä¸å­¦ä¹ ï¼Œæ”¯æŒæœºå™¨äººå®æ—¶æ”¶é›†ç»éªŒå¹¶è¿›è¡Œç­–ç•¥æ›´æ–°ã€‚SOPåœ¨å¤šä¸ªçœŸå®æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¦‚æŠ˜å è¡£ç‰©ã€ç»„è£…ç›’å­å’Œè¡¥è´§ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡é—´çš„å…±äº«ç­–ç•¥ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨çº¿å­¦ä¹ ä¸å¤§è§„æ¨¡éƒ¨ç½²çš„ç´§å¯†ç»“åˆå¯¹äºæé«˜é€šç”¨æœºå™¨äººç­–ç•¥çš„æ•ˆç‡å’Œå¯é æ€§è‡³å…³é‡è¦ã€‚","title":"å¯æ‰©å±•çš„åœ¨çº¿åè®­ç»ƒç³»ç»Ÿæå‡æœºå™¨äººç­–ç•¥é€‚åº”èƒ½åŠ›"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¯æ‰©å±•çš„åœ¨çº¿åè®­ç»ƒç³»ç»Ÿï¼ˆSOPï¼‰ï¼Œæ—¨åœ¨é€šè¿‡åˆ†å¸ƒå¼å¤šä»»åŠ¡å­¦ä¹ å®ç°æœºå™¨äººç­–ç•¥çš„é€‚åº”ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­ç›´æ¥è¿›è¡Œåœ¨çº¿åè®­ç»ƒï¼Œç»“åˆæ‰§è¡Œä¸å­¦ä¹ ï¼Œæ”¯æŒæœºå™¨äººå®æ—¶æ”¶é›†ç»éªŒå¹¶è¿›è¡Œç­–ç•¥æ›´æ–°ã€‚SOPåœ¨å¤šä¸ªçœŸå®æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¦‚æŠ˜å è¡£ç‰©ã€ç»„è£…ç›’å­å’Œè¡¥è´§ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡é—´çš„å…±äº«ç­–ç•¥ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨çº¿å­¦ä¹ ä¸å¤§è§„æ¨¡éƒ¨ç½²çš„ç´§å¯†ç»“åˆå¯¹äºæé«˜é€šç”¨æœºå™¨äººç­–ç•¥çš„æ•ˆç‡å’Œå¯é æ€§è‡³å…³é‡è¦ã€‚', title='å¯æ‰©å±•çš„åœ¨çº¿åè®­ç»ƒç³»ç»Ÿæå‡æœºå™¨äººç­–ç•¥é€‚åº”èƒ½åŠ›'))
[07.01.2026 06:37] Using data from previous issue: {"categories": ["#cv", "#transfer_learning", "#dataset", "#open_source", "#robotics", "#benchmark", "#games", "#agents", "#training"], "emoji": "ğŸ®", "ru": {"title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¸Ñ€Ğ°Ğ¼Ğ¸", "desc": "NitroGen â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´
[07.01.2026 06:37] Using data from previous issue: {"categories": ["#video", "#data", "#dataset", "#optimization", "#training"], "emoji": "ğŸ¨", "ru": {"title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ ÑÑ‚Ğ¸Ğ»Ñ", "desc": "DreamStyle â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ ÑÑ‚Ğ¸Ğ»Ñ
[07.01.2026 06:37] Querying the API.
[07.01.2026 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Audio geo-localization benchmark AGL1K is introduced to advance audio language models' geospatial reasoning capabilities through curated audio clips and evaluation across multiple models.  					AI-generated summary 				 Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.
[07.01.2026 06:37] Response: ```json
{
  "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AGL1K Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ 1444 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ğ¸Ğ· 72 ÑÑ‚Ñ€Ğ°Ğ½, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Audio Localizability Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ° 16 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ALM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ğ³Ñ€Ğ°ÑÑ‚ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑÑ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸.",
  "emoji": "ğŸŒ",
  "title": "ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"
}
```
[07.01.2026 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Audio geo-localization benchmark AGL1K is introduced to advance audio language models' geospatial reasoning capabilities through curated audio clips and evaluation across multiple models.  					AI-generated summary 				 Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability."

[07.01.2026 06:37] Response: ```python
["DATASET", "BENCHMARK", "AUDIO", "MULTIMODAL"]
```
[07.01.2026 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Audio geo-localization benchmark AGL1K is introduced to advance audio language models' geospatial reasoning capabilities through curated audio clips and evaluation across multiple models.  					AI-generated summary 				 Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability."

[07.01.2026 06:37] Response: ```python
['INTERPRETABILITY', 'OPEN_SOURCE', 'REASONING']
```
[07.01.2026 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces AGL1K, a new benchmark designed to enhance the geospatial reasoning abilities of audio language models (ALMs) using curated audio clips. It addresses the challenge of audio geo-localization, which involves determining the geographic origin of audio signals, a task previously limited by the lack of quality audio-location pairs. AGL1K includes 1,444 carefully selected audio clips from 72 countries, utilizing a novel Audio Localizability metric to ensure the recordings are informative. The study evaluates 16 ALMs, revealing that closed-source models outperform open-source ones and highlighting the importance of linguistic cues in making predictions.","title":"AGL1K: Advancing Audio Geo-Localization for Better Spatial Reasoning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper introduces AGL1K, a new benchmark designed to enhance the geospatial reasoning abilities of audio language models (ALMs) using curated audio clips. It addresses the challenge of audio geo-localization, which involves determining the geographic origin of audio signals, a task previously limited by the lack of quality audio-location pairs. AGL1K includes 1,444 carefully selected audio clips from 72 countries, utilizing a novel Audio Localizability metric to ensure the recordings are informative. The study evaluates 16 ALMs, revealing that closed-source models outperform open-source ones and highlighting the importance of linguistic cues in making predictions.', title='AGL1K: Advancing Audio Geo-Localization for Better Spatial Reasoning'))
[07.01.2026 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"æœ¬æ–‡ä»‹ç»äº†éŸ³é¢‘åœ°ç†å®šä½åŸºå‡†AGL1Kï¼Œæ—¨åœ¨é€šè¿‡ç²¾å¿ƒæŒ‘é€‰çš„éŸ³é¢‘ç‰‡æ®µæå‡éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„åœ°ç†æ¨ç†èƒ½åŠ›ã€‚åœ°ç†å®šä½çš„ç›®æ ‡æ˜¯æ¨æ–­ç»™å®šä¿¡å·çš„åœ°ç†æ¥æºï¼Œè€ŒéŸ³é¢‘åœ°ç†å®šä½çš„è¿›å±•å—åˆ°é«˜è´¨é‡éŸ³é¢‘-ä½ç½®å¯¹ç¼ºä¹çš„é™åˆ¶ã€‚AGL1Kæ˜¯é¦–ä¸ªé’ˆå¯¹éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„éŸ³é¢‘åœ°ç†å®šä½åŸºå‡†ï¼Œæ¶µç›–72ä¸ªå›½å®¶å’Œåœ°åŒºï¼Œå¹¶æå‡ºäº†éŸ³é¢‘å¯å®šä½æ€§æŒ‡æ ‡æ¥é‡åŒ–æ¯ä¸ªå½•éŸ³çš„ä¿¡æ¯é‡ã€‚é€šè¿‡å¯¹16ä¸ªéŸ³é¢‘è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°é—­æºæ¨¡å‹åœ¨éŸ³é¢‘åœ°ç†å®šä½èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºå¼€æºæ¨¡å‹ï¼Œè¯­è¨€çº¿ç´¢åœ¨é¢„æµ‹ä¸­èµ·åˆ°äº†ä¸»å¯¼ä½œç”¨ã€‚","title":"éŸ³é¢‘åœ°ç†å®šä½æ–°åŸºå‡†AGL1K"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='æœ¬æ–‡ä»‹ç»äº†éŸ³é¢‘åœ°ç†å®šä½åŸºå‡†AGL1Kï¼Œæ—¨åœ¨é€šè¿‡ç²¾å¿ƒæŒ‘é€‰çš„éŸ³é¢‘ç‰‡æ®µæå‡éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„åœ°ç†æ¨ç†èƒ½åŠ›ã€‚åœ°ç†å®šä½çš„ç›®æ ‡æ˜¯æ¨æ–­ç»™å®šä¿¡å·çš„åœ°ç†æ¥æºï¼Œè€ŒéŸ³é¢‘åœ°ç†å®šä½çš„è¿›å±•å—åˆ°é«˜è´¨é‡éŸ³é¢‘-ä½ç½®å¯¹ç¼ºä¹çš„é™åˆ¶ã€‚AGL1Kæ˜¯é¦–ä¸ªé’ˆå¯¹éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„éŸ³é¢‘åœ°ç†å®šä½åŸºå‡†ï¼Œæ¶µç›–72ä¸ªå›½å®¶å’Œåœ°åŒºï¼Œå¹¶æå‡ºäº†éŸ³é¢‘å¯å®šä½æ€§æŒ‡æ ‡æ¥é‡åŒ–æ¯ä¸ªå½•éŸ³çš„ä¿¡æ¯é‡ã€‚é€šè¿‡å¯¹16ä¸ªéŸ³é¢‘è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°é—­æºæ¨¡å‹åœ¨éŸ³é¢‘åœ°ç†å®šä½èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºå¼€æºæ¨¡å‹ï¼Œè¯­è¨€çº¿ç´¢åœ¨é¢„æµ‹ä¸­èµ·åˆ°äº†ä¸»å¯¼ä½œç”¨ã€‚', title='éŸ³é¢‘åœ°ç†å®šä½æ–°åŸºå‡†AGL1K'))
[07.01.2026 06:37] Using data from previous issue: {"categories": ["#interpretability", "#low_resource", "#dataset", "#open_source", "#multilingual", "#benchmark", "#training"], "emoji": "ğŸš¨", "ru": {"title": "ĞĞ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ½ĞµĞ½Ğ°Ğ²Ğ¸ÑÑ‚Ğ¸ Ğ² Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° X-MuT
[07.01.2026 06:37] Using data from previous issue: {"categories": [], "emoji": "ğŸŒ³", "ru": {"title": "ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹", "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ â€” Parallel Latent Reasoning (PLR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ 
[07.01.2026 06:37] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#optimization", "#benchmark", "#rl", "#agents"], "emoji": "ğŸ•·ï¸", "ru": {"title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ñ‘Ğ½Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ RL", "desc": "WebGym Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ°
[07.01.2026 06:37] Using data from previous issue: {"categories": ["#dataset", "#architecture", "#video", "#benchmark"], "emoji": "ğŸ¬", "ru": {"title": "Ğ¡Ğ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¾Ñ‚ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ", "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… FFP-300K, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 300,000 Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ°Ñ€ Ğ²Ñ‹ÑĞ¾ĞºĞ¾
[07.01.2026 06:37] Renaming data file.
[07.01.2026 06:37] Renaming previous data. hf_papers.json to ./d/2026-01-07.json
[07.01.2026 06:37] Saving new data file.
[07.01.2026 06:37] Generating page.
[07.01.2026 06:37] Renaming previous page.
[07.01.2026 06:37] Renaming previous data. index.html to ./d/2026-01-07.html
[07.01.2026 06:37] Writing result.
[07.01.2026 06:37] Renaming log file.
[07.01.2026 06:37] Renaming previous data. log.txt to ./logs/2026-01-07_last_log.txt
