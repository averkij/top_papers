[07.01.2026 03:44] Read previous papers.
[07.01.2026 03:44] Generating top page (month).
[07.01.2026 03:44] Writing top page (month).
[07.01.2026 04:42] Read previous papers.
[07.01.2026 04:42] Get feed.
[07.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.01554
[07.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03252
[07.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.01874
[07.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03233
[07.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02427
[07.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02785
[07.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03194
[07.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.03153
[07.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02780
[07.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.02439
[07.01.2026 04:42] Get page data from previous paper. URL: https://huggingface.co/papers/2601.01720
[07.01.2026 04:42] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.01.2026 04:42] No deleted papers detected.
[07.01.2026 04:42] Downloading and parsing papers (pdf, html). Total: 11.
[07.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.01554.
[07.01.2026 04:42] Extra JSON file exists (./assets/json/2601.01554.json), skip PDF parsing.
[07.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.01554.json), skip HTML parsing.
[07.01.2026 04:42] Success.
[07.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.03252.
[07.01.2026 04:42] Extra JSON file exists (./assets/json/2601.03252.json), skip PDF parsing.
[07.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.03252.json), skip HTML parsing.
[07.01.2026 04:42] Success.
[07.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.01874.
[07.01.2026 04:42] Extra JSON file exists (./assets/json/2601.01874.json), skip PDF parsing.
[07.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.01874.json), skip HTML parsing.
[07.01.2026 04:42] Success.
[07.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.03233.
[07.01.2026 04:42] Extra JSON file exists (./assets/json/2601.03233.json), skip PDF parsing.
[07.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.03233.json), skip HTML parsing.
[07.01.2026 04:42] Success.
[07.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.02427.
[07.01.2026 04:42] Extra JSON file exists (./assets/json/2601.02427.json), skip PDF parsing.
[07.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.02427.json), skip HTML parsing.
[07.01.2026 04:42] Success.
[07.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.02785.
[07.01.2026 04:42] Extra JSON file exists (./assets/json/2601.02785.json), skip PDF parsing.
[07.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.02785.json), skip HTML parsing.
[07.01.2026 04:42] Success.
[07.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.03194.
[07.01.2026 04:42] Extra JSON file exists (./assets/json/2601.03194.json), skip PDF parsing.
[07.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.03194.json), skip HTML parsing.
[07.01.2026 04:42] Success.
[07.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.03153.
[07.01.2026 04:42] Extra JSON file exists (./assets/json/2601.03153.json), skip PDF parsing.
[07.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.03153.json), skip HTML parsing.
[07.01.2026 04:42] Success.
[07.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.02780.
[07.01.2026 04:42] Extra JSON file exists (./assets/json/2601.02780.json), skip PDF parsing.
[07.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.02780.json), skip HTML parsing.
[07.01.2026 04:42] Success.
[07.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.02439.
[07.01.2026 04:42] Extra JSON file exists (./assets/json/2601.02439.json), skip PDF parsing.
[07.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.02439.json), skip HTML parsing.
[07.01.2026 04:42] Success.
[07.01.2026 04:42] Downloading and parsing paper https://huggingface.co/papers/2601.01720.
[07.01.2026 04:42] Extra JSON file exists (./assets/json/2601.01720.json), skip PDF parsing.
[07.01.2026 04:42] Paper image links file exists (./assets/img_data/2601.01720.json), skip HTML parsing.
[07.01.2026 04:42] Success.
[07.01.2026 04:42] Enriching papers with extra data.
[07.01.2026 04:42] ********************************************************************************
[07.01.2026 04:42] Abstract 0. A unified multimodal large language model for end-to-end speaker-attributed, time-stamped transcription with extended context window and strong generalization across benchmarks.  					AI-generated summary 				 Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and ...
[07.01.2026 04:42] ********************************************************************************
[07.01.2026 04:42] Abstract 1. InfiniDepth represents depth as neural implicit fields using a local implicit decoder, enabling continuous 2D coordinate querying for arbitrary-resolution depth estimation and superior performance in fine-detail regions.  					AI-generated summary 				 Existing depth estimation methods are fundament...
[07.01.2026 04:42] ********************************************************************************
[07.01.2026 04:42] Abstract 2. Visual mathematical problem solving remains challenging for multimodal large language models, prompting the development of CogFlow, a cognitive-inspired three-stage framework that enhances perception, internalization, and reasoning through synergistic rewards and visual-gated policy optimization.  	...
[07.01.2026 04:42] ********************************************************************************
[07.01.2026 04:42] Abstract 3. LTX-2 is an open-source audiovisual diffusion model that generates synchronized video and audio content using a dual-stream transformer architecture with cross-modal attention and classifier-free guidance.  					AI-generated summary 				 Recent text-to-video diffusion models can generate compelling ...
[07.01.2026 04:42] ********************************************************************************
[07.01.2026 04:42] Abstract 4. NitroGen is a vision-action foundation model trained on extensive gameplay data that demonstrates strong cross-game generalization and effective transfer learning capabilities.  					AI-generated summary 				 We introduce NitroGen, a vision-action foundation model for generalist gaming agents that i...
[07.01.2026 04:42] ********************************************************************************
[07.01.2026 04:42] Abstract 5. DreamStyle is a unified video stylization framework that supports multiple style conditions while addressing style inconsistency and temporal flicker through a specialized data curation pipeline and LoRA training approach.  					AI-generated summary 				 Video stylization, an important downstream ta...
[07.01.2026 04:42] ********************************************************************************
[07.01.2026 04:42] Abstract 6. A novel explainability-guided training framework for hate speech detection in Indic languages that combines large language models with attention-enhancing techniques and provides human-annotated rationales for improved performance and interpretability.  					AI-generated summary 				 Hate speech det...
[07.01.2026 04:42] ********************************************************************************
[07.01.2026 04:42] Abstract 7. Parallel Latent Reasoning framework improves sequential recommendation by exploring multiple diverse reasoning trajectories simultaneously through learnable trigger tokens and adaptive aggregation.  					AI-generated summary 				 Capturing complex user preferences from sparse behavioral sequences re...
[07.01.2026 04:42] ********************************************************************************
[07.01.2026 04:42] Abstract 8. MiMo-V2-Flash is a sparse Mixture-of-Experts model with hybrid attention architecture and efficient distillation technique that achieves strong performance with reduced parameters and improved inference speed.  					AI-generated summary 				 We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model...
[07.01.2026 04:42] ********************************************************************************
[07.01.2026 04:42] Abstract 9. WebGym presents a large-scale open-source environment for training visual web agents using reinforcement learning with high-throughput asynchronous sampling, achieving superior performance on unseen websites compared to proprietary models.  					AI-generated summary 				 We present WebGym, the large...
[07.01.2026 04:42] ********************************************************************************
[07.01.2026 04:42] Abstract 10. A new large-scale video dataset and framework are presented that enable effective first-frame propagation without runtime guidance through adaptive spatio-temporal positional encoding and self-distillation techniques.  					AI-generated summary 				 First-Frame Propagation (FFP) offers a promising p...
[07.01.2026 04:42] Read previous papers.
[07.01.2026 04:42] Generating reviews via LLM API.
[07.01.2026 04:42] Using data from previous issue: {"categories": ["#multimodal", "#audio", "#benchmark"], "emoji": "üéôÔ∏è", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–∫–≤–æ–∑–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Ä–µ—á–∏ —Å –∞—Ç—Ä–∏–±—É—Ü–∏–µ–π –≥–æ–≤–æ—Ä—è—â–∏—Ö –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –æ—Ç–º–µ—Ç–∫–∞–º–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å MOSS Transcribe Diarize, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–∫–≤–æ–∑
[07.01.2026 04:42] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#cv"], "emoji": "üåä", "ru": {"title": "–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –ø–æ–ª—è –¥–ª—è –≥–ª—É–±–∏–Ω—ã –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç InfiniDepth ‚Äî –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –≥–ª—É–±–∏–Ω—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –Ω–µ—è–≤–Ω—ã—Ö –ø–æ–ª—è—Ö –≤–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö —Å–µ—Ç–æ–∫. –ò—Å–ø–æ–ª—å–∑—É—è –ª–æ–∫
[07.01.2026 04:42] Using data from previous issue: {"categories": ["#multimodal", "#dataset", "#reasoning", "#optimization", "#math", "#rlhf", "#benchmark"], "emoji": "üß†", "ru": {"title": "–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –ø–æ—Ç–æ–∫: –æ—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∫ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–º—É –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ CogFlow ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–π –ø—Å–∏—Ö–æ
[07.01.2026 04:42] Using data from previous issue: {"categories": ["#video", "#multimodal", "#audio", "#open_source", "#architecture", "#diffusion", "#training"], "emoji": "üé¨", "ru": {"title": "–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –∏ –∑–≤—É–∫–∞ –≤ –µ–¥–∏–Ω–æ–º –ø–æ—Ç–æ–∫–µ", "desc": "LTX-2 ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è 
[07.01.2026 04:42] Using data from previous issue: {"categories": ["#cv", "#transfer_learning", "#dataset", "#open_source", "#robotics", "#benchmark", "#games", "#agents", "#training"], "emoji": "üéÆ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–≥—Ä–æ–≤–æ–π –∞–≥–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—Å—è –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –∑–Ω–∞–Ω–∏—è –º–µ–∂–¥—É –∏–≥—Ä–æ–≤—ã–º–∏ –º–∏—Ä–∞–º–∏", "desc": "NitroGen ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å-–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –¥–ª—è –≤–∏–¥
[07.01.2026 04:42] Using data from previous issue: {"categories": ["#video", "#data", "#dataset", "#optimization", "#training"], "emoji": "üé®", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π —Å—Ç–∏–ª—è", "desc": "DreamStyle ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∞—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–∏–ø–æ–≤ —É—Å–ª–æ–≤–∏–π —Å—Ç–∏–ª—è
[07.01.2026 04:42] Using data from previous issue: {"categories": ["#interpretability", "#low_resource", "#dataset", "#open_source", "#multilingual", "#benchmark", "#training"], "emoji": "üö®", "ru": {"title": "–û–±—ä—è—Å–Ω–∏–º–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ä–µ—á–∏ –Ω–µ–Ω–∞–≤–∏—Å—Ç–∏ –≤ –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–∞—Ö —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ X-MuT
[07.01.2026 04:42] Using data from previous issue: {"categories": [], "emoji": "üå≥", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –≥–ª—É–±–∏–Ω—ã: –Ω–æ–≤—ã–π –º–∞—Å—à—Ç–∞–± –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π ‚Äî Parallel Latent Reasoning (PLR), –∫–æ—Ç–æ—Ä—ã–π –≤–º–µ—Å—Ç–æ —É–≥–ª—É–±–ª–µ–Ω–∏—è –æ–¥–Ω–æ–π —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏—Å—Å–ª–µ–¥—É–µ—Ç 
[07.01.2026 04:42] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#reasoning", "#open_source", "#architecture", "#long_context", "#inference", "#agents", "#training"], "emoji": "‚ö°", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–Ω–æ–≥–æ—É—á–∏—Ç–µ–ª—å—Å–∫–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "MiMo-V2-Flash ‚Äî —ç—Ç–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω
[07.01.2026 04:42] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#optimization", "#benchmark", "#rl", "#agents"], "emoji": "üï∑Ô∏è", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–±–æ–≥–∞—â—ë–Ω–Ω—É—é —Å—Ä–µ–¥—É –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RL", "desc": "WebGym –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—É—é –æ—Ç–∫—Ä—ã—Ç—É—é —Å—Ä–µ–¥—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤–µ–±-–∞
[07.01.2026 04:42] Using data from previous issue: {"categories": ["#dataset", "#architecture", "#video", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–°–≤–æ–±–æ–¥–Ω–æ–µ –æ—Ç —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –∫–∞–¥—Ä–∞ —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö FFP-300K, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 300,000 –≤–∏–¥–µ–æ–ø–∞—Ä –≤—ã—Å–æ–∫–æ
[07.01.2026 04:42] Renaming data file.
[07.01.2026 04:42] Renaming previous data. hf_papers.json to ./d/2026-01-07.json
[07.01.2026 04:42] Saving new data file.
[07.01.2026 04:42] Generating page.
[07.01.2026 04:42] Renaming previous page.
[07.01.2026 04:42] Renaming previous data. index.html to ./d/2026-01-07.html
[07.01.2026 04:42] Writing result.
[07.01.2026 04:42] Renaming log file.
[07.01.2026 04:42] Renaming previous data. log.txt to ./logs/2026-01-07_last_log.txt
