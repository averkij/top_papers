[28.10.2025 03:43] Read previous papers.
[28.10.2025 03:43] Generating top page (month).
[28.10.2025 03:43] Writing top page (month).
[28.10.2025 04:14] Read previous papers.
[28.10.2025 04:14] Get feed.
[28.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23607
[28.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.21817
[28.10.2025 04:14] Extract page data from URL. URL: https://huggingface.co/papers/2510.23588
[28.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22201
[28.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23451
[28.10.2025 04:14] Extract page data from URL. URL: https://huggingface.co/papers/2510.22706
[28.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22733
[28.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23603
[28.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23544
[28.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23571
[28.10.2025 04:14] Extract page data from URL. URL: https://huggingface.co/papers/2510.23052
[28.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22907
[28.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.21003
[28.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23594
[28.10.2025 04:14] Extract page data from URL. URL: https://huggingface.co/papers/2510.23564
[28.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22946
[28.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22200
[28.10.2025 04:14] Get page data from previous paper. URL: https://huggingface.co/papers/2510.21800
[28.10.2025 04:14] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.10.2025 04:14] No deleted papers detected.
[28.10.2025 04:14] Downloading and parsing papers (pdf, html). Total: 18.
[28.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.23607.
[28.10.2025 04:14] Extra JSON file exists (./assets/json/2510.23607.json), skip PDF parsing.
[28.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.23607.json), skip HTML parsing.
[28.10.2025 04:14] Success.
[28.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.21817.
[28.10.2025 04:14] Extra JSON file exists (./assets/json/2510.21817.json), skip PDF parsing.
[28.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.21817.json), skip HTML parsing.
[28.10.2025 04:14] Success.
[28.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.23588.
[28.10.2025 04:14] Downloading paper 2510.23588 from http://arxiv.org/pdf/2510.23588v1...
[28.10.2025 04:14] Extracting affiliations from text.
[28.10.2025 04:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 8 8 5 3 2 . 0 1 5 2 : r FARMER: Flow AutoRegressive Transformer over Pixels Guangting Zheng1,3, Qinyu Zhao1,4, Tao Yang1, Fei Xiao1, Zhijie Lin2, Jie Wu1, Jiajun Deng5, Yanyong Zhang3, Rui Zhu1 1ByteDance Seed China, 2ByteDance Seed Singapore, 3USTC, 4ANU, 5NUS Project lead "
[28.10.2025 04:14] Response: ```python
["ByteDance Seed China", "ByteDance Seed Singapore", "USTC", "ANU", "NUS"]
```
[28.10.2025 04:14] Deleting PDF ./assets/pdf/2510.23588.pdf.
[28.10.2025 04:14] Success.
[28.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.22201.
[28.10.2025 04:14] Extra JSON file exists (./assets/json/2510.22201.json), skip PDF parsing.
[28.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.22201.json), skip HTML parsing.
[28.10.2025 04:14] Success.
[28.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.23451.
[28.10.2025 04:14] Extra JSON file exists (./assets/json/2510.23451.json), skip PDF parsing.
[28.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.23451.json), skip HTML parsing.
[28.10.2025 04:14] Success.
[28.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.22706.
[28.10.2025 04:14] Downloading paper 2510.22706 from http://arxiv.org/pdf/2510.22706v1...
[28.10.2025 04:14] Extracting affiliations from text.
[28.10.2025 04:14] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 2 ] . [ 1 6 0 7 2 2 . 0 1 5 2 : r Under review as conference paper IGGT: FORMER FOR SEMANTIC 3D RECONSTRUCTION INSTANCE-GROUNDED GEOMETRY TRANSHao Li1,2,3, Zhengyu Zou1, Fangfu Liu4, Xuanyang Zhang3, Fangzhou Hong2, Yukang Cao2, Yushi Lan2, Manyuan Zhang4, Gang Yu3, Dingwen Zhang2(cid:66), Ziwei Liu2 1NWPU 2S-Lab, NTU 3StepFun, Inc. 4THU 5MMLab, CUHK Figure 1: IGGT: building upon our curated large-scale dataset InsScene-15K, we propose novel end-to-end framework that enables geometric reconstruction and contextual understanding in unified representation. This paradigm facilitates wide range of applications, including spatial tracking, 2D / 3D open-vocabulary segmentation, and scene grounding. "
[28.10.2025 04:14] Response: ```python
["NWPU", "S-Lab, NTU", "StepFun, Inc.", "THU", "MMLab, CUHK"]
```
[28.10.2025 04:14] Deleting PDF ./assets/pdf/2510.22706.pdf.
[28.10.2025 04:14] Success.
[28.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.22733.
[28.10.2025 04:14] Extra JSON file exists (./assets/json/2510.22733.json), skip PDF parsing.
[28.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.22733.json), skip HTML parsing.
[28.10.2025 04:14] Success.
[28.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.23603.
[28.10.2025 04:14] Extra JSON file exists (./assets/json/2510.23603.json), skip PDF parsing.
[28.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.23603.json), skip HTML parsing.
[28.10.2025 04:14] Success.
[28.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.23544.
[28.10.2025 04:14] Extra JSON file exists (./assets/json/2510.23544.json), skip PDF parsing.
[28.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.23544.json), skip HTML parsing.
[28.10.2025 04:14] Success.
[28.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.23571.
[28.10.2025 04:14] Extra JSON file exists (./assets/json/2510.23571.json), skip PDF parsing.
[28.10.2025 04:14] Paper image links file exists (./assets/img_data/2510.23571.json), skip HTML parsing.
[28.10.2025 04:14] Success.
[28.10.2025 04:14] Downloading and parsing paper https://huggingface.co/papers/2510.23052.
[28.10.2025 04:14] Downloading paper 2510.23052 from http://arxiv.org/pdf/2510.23052v1...
[28.10.2025 04:15] Extracting affiliations from text.
[28.10.2025 04:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Knocking-Heads Attention Zhanchao Zhou1,2,3, Xiaodong Chen1,4, Haoxing Chen1, Zhenzhong Lan1,3, Jianguo Li1 1 Ant Group 2 Zhejiang University 3 Westlake University 4 Renmin University of China "
[28.10.2025 04:15] Response: ```python
["Ant Group", "Zhejiang University", "Westlake University", "Renmin University of China"]
```
[28.10.2025 04:15] Deleting PDF ./assets/pdf/2510.23052.pdf.
[28.10.2025 04:15] Success.
[28.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.22907.
[28.10.2025 04:15] Extra JSON file exists (./assets/json/2510.22907.json), skip PDF parsing.
[28.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.22907.json), skip HTML parsing.
[28.10.2025 04:15] Success.
[28.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.21003.
[28.10.2025 04:15] Extra JSON file exists (./assets/json/2510.21003.json), skip PDF parsing.
[28.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.21003.json), skip HTML parsing.
[28.10.2025 04:15] Success.
[28.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.23594.
[28.10.2025 04:15] Extra JSON file exists (./assets/json/2510.23594.json), skip PDF parsing.
[28.10.2025 04:15] Paper image links file exists (./assets/img_data/2510.23594.json), skip HTML parsing.
[28.10.2025 04:15] Success.
[28.10.2025 04:15] Downloading and parsing paper https://huggingface.co/papers/2510.23564.
[28.10.2025 04:15] Downloading paper 2510.23564 from http://arxiv.org/pdf/2510.23564v1...
[28.10.2025 04:16] Extracting affiliations from text.
[28.10.2025 04:16] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 7 2 ] . [ 1 4 6 5 3 2 . 0 1 5 2 : r RECODE: UNIFY PLAN AND ACTION FOR UNIVERSAL GRANULARITY CONTROL Zhaoyang Yu1, Jiayi Zhang1,2, Huixue Su3, Yufan Zhao3, Yifan Wu1,2, Mingyi Deng1, Jinyu Xiang, Yizhang Lin1, Lingxiao Tang4, Yingchao Li1, Yuyu Luo2, Bang Liu5, Chenglin Wu1 1DeepWisdom, 2The Hong Kong University of Science and Technology (Guangzhou), 3Renmin University of China, 4Zhejiang University, 5Université de Montréal & Mila zhaoyangyu713@gmail.com, alexanderwu@deepwisdom.ai "
[28.10.2025 04:16] Response: ```python
[
    "DeepWisdom",
    "The Hong Kong University of Science and Technology (Guangzhou)",
    "Renmin University of China",
    "Zhejiang University",
    "Université de Montréal & Mila"
]
```
[28.10.2025 04:16] Deleting PDF ./assets/pdf/2510.23564.pdf.
[28.10.2025 04:16] Success.
[28.10.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2510.22946.
[28.10.2025 04:16] Extra JSON file exists (./assets/json/2510.22946.json), skip PDF parsing.
[28.10.2025 04:16] Paper image links file exists (./assets/img_data/2510.22946.json), skip HTML parsing.
[28.10.2025 04:16] Success.
[28.10.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2510.22200.
[28.10.2025 04:16] Extra JSON file exists (./assets/json/2510.22200.json), skip PDF parsing.
[28.10.2025 04:16] Paper image links file exists (./assets/img_data/2510.22200.json), skip HTML parsing.
[28.10.2025 04:16] Success.
[28.10.2025 04:16] Downloading and parsing paper https://huggingface.co/papers/2510.21800.
[28.10.2025 04:16] Extra JSON file exists (./assets/json/2510.21800.json), skip PDF parsing.
[28.10.2025 04:16] Paper image links file exists (./assets/img_data/2510.21800.json), skip HTML parsing.
[28.10.2025 04:16] Success.
[28.10.2025 04:16] Enriching papers with extra data.
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 0. Concerto, a minimalist model combining 3D self-distillation and 2D-3D joint embedding, achieves superior spatial feature learning and outperforms existing models in scene understanding and open-world perception.  					AI-generated summary 				 Humans learn abstract concepts through multisensory syne...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 1. VITA-E, a dual-model embodied interaction framework, enables concurrent and interruptible vision-language-action capabilities, enhancing real-time user interaction and multitasking.  					AI-generated summary 				 Current Vision-Language-Action (VLA) models are often constrained by a rigid, static i...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 2. FARMER, a generative framework combining Normalizing Flows and Autoregressive models, achieves competitive image synthesis from raw pixels with exact likelihoods and scalable training.  					AI-generated summary 				 Directly modeling the explicit likelihood of the raw data distribution is key topic...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 3. Action Coherence Guidance (ACG) improves action coherence in Vision-Language-Action (VLA) models during test time, enhancing performance in diverse manipulation tasks.  					AI-generated summary 				 Diffusion and flow matching models have emerged as powerful robot policies, enabling Vision-Language...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 4. Omni-Reward addresses modality imbalance and preference rigidity in reward models by introducing a benchmark, dataset, and model that support multiple modalities and free-form preferences.  					AI-generated summary 				 Reward models (RMs) play a critical role in aligning AI behaviors with human pr...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 5. InstanceGrounded Geometry Transformer (IGGT) unifies 3D reconstruction and instance-level understanding using a unified transformer and 3D-Consistent Contrastive Learning, supported by a new dataset InsScene-15K.  					AI-generated summary 				 Humans naturally perceive the geometric structure and s...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 6. A unified framework extends a single text embedding model to perform both retrieval and listwise reranking, achieving state-of-the-art results with low latency.  					AI-generated summary 				 Text embedding models serve as a fundamental component in real-world search applications. By mapping querie...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 7. PixelRefer is a unified region-level MLLM framework that enhances fine-grained object-centric understanding using a Scale-Adaptive Object Tokenizer and Object-Centric Infusion module, achieving high performance and efficiency.  					AI-generated summary 				 Multimodal large language models (MLLMs) ...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 8. LIMRANK-SYNTHESIZER generates synthetic data to fine-tune LIMRANK, achieving competitive performance with minimal supervision on information reranking tasks.  					AI-generated summary 				 Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, w...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 9. A new benchmarking framework uses large-scale simulated environments with human feedback to evaluate robot policies, addressing limitations in real-world testing and existing simulation benchmarks.  					AI-generated summary 				 The pursuit of robot generalists - instructable agents capable of perf...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 10. Knocking-heads attention (KHA) enhances multi-head attention by enabling cross-head interactions, improving training dynamics and performance in large language models.  					AI-generated summary 				 Multi-head attention (MHA) has become the cornerstone of modern large language models, enhancing rep...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 11. Lanser-CLI orchestrates Language Server Protocol servers for coding agents and CI, providing deterministic workflows and actionable process rewards based on verified code facts.  					AI-generated summary 				 Large language models routinely hallucinate APIs and mislocalize edits, while language ser...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 12. A new method, Distilled Decoding 2 (DD2), enables one-step sampling for image auto-regressive models with minimal performance degradation and significant speed-up compared to previous methods.  					AI-generated summary 				 Image Auto-regressive (AR) models have emerged as a powerful paradigm of vi...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 13. PRISM-Bench evaluates models' reasoning processes by identifying errors in step-by-step solutions to visual puzzles, highlighting gaps between fluent generation and logical consistency.  					AI-generated summary 				 We introduce PRISM-Bench, a benchmark of puzzle-based visual challenges designed t...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 14. ReCode, a recursive code generation paradigm, unifies high-level planning and low-level action in a single representation, enhancing decision granularity and data efficiency in LLM-based agents.  					AI-generated summary 				 Real-world tasks require decisions at varying granularities, and humans e...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 15. A unified multimodal model achieves competitive performance with efficient fusion of generation and understanding models, using interleaved multimodal self-attention blocks and minimal training resources.  					AI-generated summary 				 Unified multimodal models have recently shown remarkable gains ...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 16. LongCat-Video, a 13.6B parameter video generation model based on the Diffusion Transformer framework, excels in efficient and high-quality long video generation across multiple tasks using unified architecture, coarse-to-fine generation, and block sparse attention.  					AI-generated summary 				 Vi...
[28.10.2025 04:16] ********************************************************************************
[28.10.2025 04:16] Abstract 17. MARS-M, a new optimizer combining Muon and MARS techniques, achieves faster convergence and better performance in large-scale neural network training.  					AI-generated summary 				 Matrix-based preconditioned optimizers, such as Muon, have recently been shown to be more efficient than scalar-based...
[28.10.2025 04:16] Read previous papers.
[28.10.2025 04:16] Generating reviews via LLM API.
[28.10.2025 04:16] Using data from previous issue: {"categories": ["#transfer_learning", "#multimodal", "#benchmark", "#3d", "#optimization", "#training"], "emoji": "🎼", "ru": {"title": "Обучение пространственному восприятию через синергию 2D и 3D", "desc": "Concerto — это модель для изучения пространственных концептов, вдохновлённая тем, как люди у
[28.10.2025 04:16] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#agi", "#interpretability", "#reasoning", "#architecture"], "emoji": "🤖", "ru": {"title": "Двухмодельная система для прерываемого взаимодействия роботов", "desc": "Современные Vision-Language-Action модели работают по жёсткому сценарию и не могут одновремен
[28.10.2025 04:16] Querying the API.
[28.10.2025 04:16] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FARMER, a generative framework combining Normalizing Flows and Autoregressive models, achieves competitive image synthesis from raw pixels with exact likelihoods and scalable training.  					AI-generated summary 				 Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training.
[28.10.2025 04:16] Response: ```json
{
  "title": "FARMER: объединение потоков и авторегрессии для генерации изображений",
  "desc": "FARMER — это новая генеративная модель, которая объединяет Normalizing Flows и авторегрессионные модели для генерации изображений напрямую из пикселей с точной оценкой правдоподобия. Для решения проблемы избыточности при моделировании на уровне пикселей авторы предлагают схему самообучаемого снижения размерности, разделяя латентные каналы на информативные и избыточные. Модель использует обратимый авторегрессионный поток для преобразования изображений в латентные последовательности, распределение которых моделируется AR-моделью. Дополнительно представлены методы однократной дистилляции для ускорения инференса и алгоритм classifier-free guidance для улучшения качества генерации.",
  "emoji": "🌾",
  "desc": "FARMER — это новая генеративная модель, которая объединяет Normalizing Flows и авторегрессионные модели для генерации изображений напрямую из пикселей с точной оценкой правдоподобия. Для решения проблемы избыточности при моделировании на уровне пикселей авторы предлагают схему самообучаемого снижения размерности, разделяя латентные каналы на информативные и избыточные. Модель использует обратимый авторегрессионный поток для преобразования изображений в латентные последовательности, распределение которых моделируется AR-моделью. Дополнительно представлены методы однократной дистилляции для ускорения инференса и алгоритм classifier-free guidance для улучшения качества генер
[28.10.2025 04:16] Error. Failed to parse JSON from LLM. {
  "title": "FARMER: объединение потоков и авторегрессии для генерации изображений",
  "desc": "FARMER — это новая генеративная модель, которая объединяет Normalizing Flows и авторегрессионные модели для генерации изображений напрямую из пикселей с точной оценкой правдоподобия. Для решения проблемы избыточности при моделировании на уровне пикселей авторы предлагают схему самообучаемого снижения размерности, разделяя латентные каналы на информативные и избыточные. Модель использует обратимый авторегрессионный поток для преобразования изображений в латентные последовательности, распределение которых моделируется AR-моделью. Дополнительно представлены методы однократной дистилляции для ускорения инференса и алгоритм classifier-free guidance для улучшения качества генерации.",
  "emoji": "🌾",
  "desc": "FARMER — это новая генеративная модель, которая объединяет Normalizing Flows и авторегрессионные модели для генерации изображений напрямую из пикселей с точной оценкой правдоподобия. Для решения проблемы избыточности при моделировании на уровне пикселей авторы предлагают схему самообучаемого снижения размерности, разделяя латентные каналы на информативные и избыточные. Модель использует обратимый авторегрессионный поток для преобразования изображений в латентные последовательности, распределение которых моделируется AR-моделью. Дополнительно представлены методы однократной дистилляции для ускорения инференса и алгоритм classifier-free guidance для улучшения качества генер
[28.10.2025 04:16] Fallback to OpenAI.
[28.10.2025 04:16] Response: ParsedChatCompletionMessage[ArticleFull](content='{"desc":"В статье представлена новая генеративная структура FARMER, которая объединяет Normalizing Flows и Autoregressive модели для синтеза изображений с точными вероятностями. FARMER преобразует изображения в латентные последовательности, которые моделируются авторегрессионной моделью. Для уменьшения сложности моделирования пикселей используется самосупервизируемая схема уменьшения размерности. Эксперименты показывают, что FARMER достигает конкурентоспособных результатов по сравнению с существующими моделями, обеспечивая точные вероятности и масштабируемое обучение.","emoji":"🖼️","title":"FARMER: Новая эра генерации изображений"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=ArticleFull(desc='В статье представлена новая генеративная структура FARMER, которая объединяет Normalizing Flows и Autoregressive модели для синтеза изображений с точными вероятностями. FARMER преобразует изображения в латентные последовательности, которые моделируются авторегрессионной моделью. Для уменьшения сложности моделирования пикселей используется самосупервизируемая схема уменьшения размерности. Эксперименты показывают, что FARMER достигает конкурентоспособных результатов по сравнению с существующими моделями, обеспечивая точные вероятности и масштабируемое обучение.', emoji='🖼️', title='FARMER: Новая эра генерации изображений'))
[28.10.2025 04:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FARMER, a generative framework combining Normalizing Flows and Autoregressive models, achieves competitive image synthesis from raw pixels with exact likelihoods and scalable training.  					AI-generated summary 				 Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training."

[28.10.2025 04:16] Response: ```python
['CV', 'ARCHITECTURE', 'TRAINING', 'INFERENCE']
```
[28.10.2025 04:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FARMER, a generative framework combining Normalizing Flows and Autoregressive models, achieves competitive image synthesis from raw pixels with exact likelihoods and scalable training.  					AI-generated summary 				 Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training."

[28.10.2025 04:16] Response: ```python
["DIFFUSION", "OPTIMIZATION"]
```
[28.10.2025 04:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FARMER is a new generative framework that combines Normalizing Flows and Autoregressive models to improve image synthesis from raw pixels. It directly models the likelihood of data distribution, which is crucial for scaling in machine learning. The framework uses an invertible autoregressive flow to convert images into latent sequences, allowing for better likelihood estimation. Additionally, FARMER includes a self-supervised dimension reduction method and a fast inference technique to enhance image generation quality and efficiency.","title":"FARMER: Unifying Flows and Autoregressive Models for Superior Image Synthesis"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FARMER is a new generative framework that combines Normalizing Flows and Autoregressive models to improve image synthesis from raw pixels. It directly models the likelihood of data distribution, which is crucial for scaling in machine learning. The framework uses an invertible autoregressive flow to convert images into latent sequences, allowing for better likelihood estimation. Additionally, FARMER includes a self-supervised dimension reduction method and a fast inference technique to enhance image generation quality and efficiency.', title='FARMER: Unifying Flows and Autoregressive Models for Superior Image Synthesis'))
[28.10.2025 04:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FARMER是一种结合了归一化流和自回归模型的生成框架，能够从原始像素中实现高质量的图像合成。该框架通过可逆的自回归流将图像转换为潜在序列，并利用自回归模型隐式建模其分布。为了解决像素级建模中的冗余和复杂性，FARMER提出了一种自监督的维度减少方案，将潜在通道分为信息性和冗余组，从而提高自回归建模的效率。此外，FARMER还设计了一种一步蒸馏方案，加快推理速度，并引入无分类器引导算法以提升图像生成质量。","title":"FARMER：高效的图像生成新框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FARMER是一种结合了归一化流和自回归模型的生成框架，能够从原始像素中实现高质量的图像合成。该框架通过可逆的自回归流将图像转换为潜在序列，并利用自回归模型隐式建模其分布。为了解决像素级建模中的冗余和复杂性，FARMER提出了一种自监督的维度减少方案，将潜在通道分为信息性和冗余组，从而提高自回归建模的效率。此外，FARMER还设计了一种一步蒸馏方案，加快推理速度，并引入无分类器引导算法以提升图像生成质量。', title='FARMER：高效的图像生成新框架'))
[28.10.2025 04:16] Using data from previous issue: {"categories": ["#robotics", "#games", "#agents", "#optimization"], "emoji": "🤖", "ru": {"title": "Плавные движения роботов без дополнительного обучения", "desc": "Статья представляет метод Action Coherence Guidance (ACG) для улучшения согласованности действий в Vision-Language-Action (VLA) моделях 
[28.10.2025 04:16] Using data from previous issue: {"categories": ["#data", "#multimodal", "#benchmark", "#dataset", "#alignment", "#architecture"], "emoji": "🎁", "ru": {"title": "Универсальная модель вознаграждения для всех модальностей и персонализированных предпочтений", "desc": "Статья представляет Omni-Reward — систему для обучения reward model
[28.10.2025 04:16] Querying the API.
[28.10.2025 04:16] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

InstanceGrounded Geometry Transformer (IGGT) unifies 3D reconstruction and instance-level understanding using a unified transformer and 3D-Consistent Contrastive Learning, supported by a new dataset InsScene-15K.  					AI-generated summary 				 Humans naturally perceive the geometric structure and semantic content of a 3D world as intertwined dimensions, enabling coherent and accurate understanding of complex scenes. However, most prior approaches prioritize training large geometry models for low-level 3D reconstruction and treat high-level spatial understanding in isolation, overlooking the crucial interplay between these two fundamental aspects of 3D-scene analysis, thereby limiting generalization and leading to poor performance in downstream 3D understanding tasks. Recent attempts have mitigated this issue by simply aligning 3D models with specific language models, thus restricting perception to the aligned model's capacity and limiting adaptability to downstream tasks. In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an end-to-end large unified transformer to unify the knowledge for both spatial reconstruction and instance-level contextual understanding. Specifically, we design a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode a unified representation with geometric structures and instance-grounded clustering through only 2D visual inputs. This representation supports consistent lifting of 2D visual inputs into a coherent 3D scene with explicitly distinct object instances. To facilitate this task, we further construct InsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth maps, and 3D-consistent instance-level mask annotations with a novel data curation pipeline.
[28.10.2025 04:16] Response: ```json
{
  "title": "Единая модель для 3D-реконструкции и понимания объектов в сценах",
  "desc": "Исследователи представили IGGT — трансформер, который одновременно восстанавливает геометрию 3D-сцены и распознаёт отдельные объекты в ней. Модель использует специальную стратегию контрастного обучения, которая позволяет создавать единое представление геометрии и семантики, работая только с 2D-изображениями. Для обучения был создан датасет InsScene-15K с высококачественными RGB-изображениями, картами глубины и разметкой объектов, согласованной в 3D. Такой подход преодолевает ограничения предыдущих методов, которые решали задачи низкоуровневой реконструкции и высокоуровневого понимания сцен раздельно.",
  "emoji": "🎯",
  "title_en": "Unified model for 3D reconstruction and instance understanding"
}
```
[28.10.2025 04:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"InstanceGrounded Geometry Transformer (IGGT) unifies 3D reconstruction and instance-level understanding using a unified transformer and 3D-Consistent Contrastive Learning, supported by a new dataset InsScene-15K.  					AI-generated summary 				 Humans naturally perceive the geometric structure and semantic content of a 3D world as intertwined dimensions, enabling coherent and accurate understanding of complex scenes. However, most prior approaches prioritize training large geometry models for low-level 3D reconstruction and treat high-level spatial understanding in isolation, overlooking the crucial interplay between these two fundamental aspects of 3D-scene analysis, thereby limiting generalization and leading to poor performance in downstream 3D understanding tasks. Recent attempts have mitigated this issue by simply aligning 3D models with specific language models, thus restricting perception to the aligned model's capacity and limiting adaptability to downstream tasks. In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an end-to-end large unified transformer to unify the knowledge for both spatial reconstruction and instance-level contextual understanding. Specifically, we design a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode a unified representation with geometric structures and instance-grounded clustering through only 2D visual inputs. This representation supports consistent lifting of 2D visual inputs into a coherent 3D scene with explicitly distinct object instances. To facilitate this task, we further construct InsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth maps, and 3D-consistent instance-level mask annotations with a novel data curation pipeline."

[28.10.2025 04:16] Response: ```python
['DATASET', '3D']
```
[28.10.2025 04:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"InstanceGrounded Geometry Transformer (IGGT) unifies 3D reconstruction and instance-level understanding using a unified transformer and 3D-Consistent Contrastive Learning, supported by a new dataset InsScene-15K.  					AI-generated summary 				 Humans naturally perceive the geometric structure and semantic content of a 3D world as intertwined dimensions, enabling coherent and accurate understanding of complex scenes. However, most prior approaches prioritize training large geometry models for low-level 3D reconstruction and treat high-level spatial understanding in isolation, overlooking the crucial interplay between these two fundamental aspects of 3D-scene analysis, thereby limiting generalization and leading to poor performance in downstream 3D understanding tasks. Recent attempts have mitigated this issue by simply aligning 3D models with specific language models, thus restricting perception to the aligned model's capacity and limiting adaptability to downstream tasks. In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an end-to-end large unified transformer to unify the knowledge for both spatial reconstruction and instance-level contextual understanding. Specifically, we design a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode a unified representation with geometric structures and instance-grounded clustering through only 2D visual inputs. This representation supports consistent lifting of 2D visual inputs into a coherent 3D scene with explicitly distinct object instances. To facilitate this task, we further construct InsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth maps, and 3D-consistent instance-level mask annotations with a novel data curation pipeline."

[28.10.2025 04:16] Response: ```python
["GAMES", "OPTIMIZATION"]
```
[28.10.2025 04:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The InstanceGrounded Geometry Transformer (IGGT) is a novel approach that integrates 3D reconstruction with instance-level understanding using a unified transformer architecture. It employs 3D-Consistent Contrastive Learning to create a cohesive representation that captures both geometric structures and distinct object instances from 2D visual inputs. This method addresses the limitations of previous models that treated spatial understanding and geometry separately, enhancing generalization for downstream 3D tasks. Additionally, the introduction of the InsScene-15K dataset provides high-quality data necessary for training and evaluating the model\'s performance in complex scene analysis.","title":"Unifying 3D Reconstruction and Instance Understanding with IGGT"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="The InstanceGrounded Geometry Transformer (IGGT) is a novel approach that integrates 3D reconstruction with instance-level understanding using a unified transformer architecture. It employs 3D-Consistent Contrastive Learning to create a cohesive representation that captures both geometric structures and distinct object instances from 2D visual inputs. This method addresses the limitations of previous models that treated spatial understanding and geometry separately, enhancing generalization for downstream 3D tasks. Additionally, the introduction of the InsScene-15K dataset provides high-quality data necessary for training and evaluating the model's performance in complex scene analysis.", title='Unifying 3D Reconstruction and Instance Understanding with IGGT'))
[28.10.2025 04:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了实例基础几何变换器（IGGT），旨在将3D重建与实例级理解统一起来。通过3D一致性对比学习，IGGT能够从2D视觉输入中编码出几何结构和实例聚类的统一表示。该方法克服了以往方法中低级3D重建与高级空间理解相互孤立的问题，提升了3D场景分析的性能。为支持这一研究，作者还构建了一个新的数据集InsScene-15K，包含高质量的RGB图像、姿态、深度图和3D一致的实例级掩码注释。","title":"统一3D重建与实例理解的创新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了实例基础几何变换器（IGGT），旨在将3D重建与实例级理解统一起来。通过3D一致性对比学习，IGGT能够从2D视觉输入中编码出几何结构和实例聚类的统一表示。该方法克服了以往方法中低级3D重建与高级空间理解相互孤立的问题，提升了3D场景分析的性能。为支持这一研究，作者还构建了一个新的数据集InsScene-15K，包含高质量的RGB图像、姿态、深度图和3D一致的实例级掩码注释。', title='统一3D重建与实例理解的创新方法'))
[28.10.2025 04:16] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#optimization", "#rag"], "emoji": "🔄", "ru": {"title": "Одна embedding-модель для поиска и ранжирования", "desc": "Статья представляет E^2Rank — единую框架, которая расширяет возможности одной текстовой embedding-модели для выполнения как поиска, так и list
[28.10.2025 04:16] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#games", "#dataset", "#optimization", "#reasoning", "#training"], "emoji": "🎯", "ru": {"title": "Точное понимание объектов в мультимодальных LLM", "desc": "PixelRefer — это фреймворк для мультимодальных LLM, который фокусируется на детальном понимании отдельных
[28.10.2025 04:16] Using data from previous issue: {"categories": ["#data", "#rag", "#open_source", "#benchmark", "#dataset", "#synthetic", "#reasoning", "#training"], "emoji": "🎯", "ru": {"title": "Эффективное ранжирование информации с минимальным количеством данных", "desc": "Статья представляет LIMRANK-SYNTHESIZER — pipeline для генерации синтети
[28.10.2025 04:16] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#agents", "#games", "#optimization"], "emoji": "🤖", "ru": {"title": "Симуляция с человеческой обратной связью для оценки роботических политик", "desc": "Статья представляет новый фреймворк для тестирования роботических политик, который переносит оценку VLA
[28.10.2025 04:16] Querying the API.
[28.10.2025 04:16] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Knocking-heads attention (KHA) enhances multi-head attention by enabling cross-head interactions, improving training dynamics and performance in large language models.  					AI-generated summary 				 Multi-head attention (MHA) has become the cornerstone of modern large language models, enhancing representational capacity through parallel attention heads. However, increasing the number of heads inherently weakens individual head capacity, and existing attention mechanisms - whether standard MHA or its variants like grouped-query attention (GQA) and grouped-tied attention (GTA) - simply concatenate outputs from isolated heads without strong interaction. To address this limitation, we propose knocking-heads attention (KHA), which enables attention heads to "knock" on each other - facilitating cross-head feature-level interactions before the scaled dot-product attention. This is achieved by applying a shared, diagonally-initialized projection matrix across all heads. The diagonal initialization preserves head-specific specialization at the start of training while allowing the model to progressively learn integrated cross-head representations. KHA adds only minimal parameters and FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention variants. We validate KHA by training a 6.1B parameter MoE model (1.01B activated) on 1T high-quality tokens. Compared to baseline attention mechanisms, KHA brings superior and more stable training dynamics, achieving better performance across downstream tasks.
[28.10.2025 04:16] Response: ```json
{
  "title": "Головы внимания учатся взаимодействовать друг с другом",
  "desc": "В статье предлагается механизм knocking-heads attention (KHA), который улучшает классический multi-head attention, позволяя головам внимания взаимодействовать между собой на уровне признаков. В отличие от стандартного подхода, где выходы голов просто конкатенируются, KHA использует общую проекционную матрицу с диагональной инициализацией для обмена информацией между головами. Механизм добавляет минимальное количество параметров и вычислений, легко интегрируется в существующие варианты attention (MHA, GQA, GTA). Эксперименты на MoE модели с 6.1B параметров показали улучшение динамики обучения и качества на downstream задачах.",
  "emoji": "🤝",
  "desc": "В статье предлагается механизм knocking-heads attention (KHA), который улучшает классический multi-head attention, позволяя головам внимания взаимодействовать между собой на уровне признаков. В отличие от стандартного подхода, где выходы голов просто конкатенируются, KHA использует общую проекционную матрицу с диагональной инициализацией для обмена информацией между головами. Механизм добавляет минимальное количество параметров и вычислений, легко интегрируется в существующие варианты attention (MHA, GQA, GTA). Эксперименты на MoE модели с 6.1B параметров показали улучшение динамики обучения и качества на downstream задачах."
}
```
[28.10.2025 04:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Knocking-heads attention (KHA) enhances multi-head attention by enabling cross-head interactions, improving training dynamics and performance in large language models.  					AI-generated summary 				 Multi-head attention (MHA) has become the cornerstone of modern large language models, enhancing representational capacity through parallel attention heads. However, increasing the number of heads inherently weakens individual head capacity, and existing attention mechanisms - whether standard MHA or its variants like grouped-query attention (GQA) and grouped-tied attention (GTA) - simply concatenate outputs from isolated heads without strong interaction. To address this limitation, we propose knocking-heads attention (KHA), which enables attention heads to "knock" on each other - facilitating cross-head feature-level interactions before the scaled dot-product attention. This is achieved by applying a shared, diagonally-initialized projection matrix across all heads. The diagonal initialization preserves head-specific specialization at the start of training while allowing the model to progressively learn integrated cross-head representations. KHA adds only minimal parameters and FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention variants. We validate KHA by training a 6.1B parameter MoE model (1.01B activated) on 1T high-quality tokens. Compared to baseline attention mechanisms, KHA brings superior and more stable training dynamics, achieving better performance across downstream tasks."

[28.10.2025 04:16] Response: ```python
['ARCHITECTURE', 'TRAINING']
```
[28.10.2025 04:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Knocking-heads attention (KHA) enhances multi-head attention by enabling cross-head interactions, improving training dynamics and performance in large language models.  					AI-generated summary 				 Multi-head attention (MHA) has become the cornerstone of modern large language models, enhancing representational capacity through parallel attention heads. However, increasing the number of heads inherently weakens individual head capacity, and existing attention mechanisms - whether standard MHA or its variants like grouped-query attention (GQA) and grouped-tied attention (GTA) - simply concatenate outputs from isolated heads without strong interaction. To address this limitation, we propose knocking-heads attention (KHA), which enables attention heads to "knock" on each other - facilitating cross-head feature-level interactions before the scaled dot-product attention. This is achieved by applying a shared, diagonally-initialized projection matrix across all heads. The diagonal initialization preserves head-specific specialization at the start of training while allowing the model to progressively learn integrated cross-head representations. KHA adds only minimal parameters and FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention variants. We validate KHA by training a 6.1B parameter MoE model (1.01B activated) on 1T high-quality tokens. Compared to baseline attention mechanisms, KHA brings superior and more stable training dynamics, achieving better performance across downstream tasks."

[28.10.2025 04:16] Response: ```python
["OPTIMIZATION"]
```
[28.10.2025 04:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Knocking-heads attention (KHA) improves multi-head attention (MHA) by allowing attention heads to interact with each other, enhancing the model\'s ability to learn complex representations. Traditional MHA limits head interactions, which can weaken the overall performance as more heads are added. KHA introduces a shared projection matrix that maintains individual head specialization while enabling cross-head feature interactions. This method not only adds minimal computational overhead but also leads to better training stability and performance in large language models.","title":"Enhancing Attention with Cross-Head Interactions"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="Knocking-heads attention (KHA) improves multi-head attention (MHA) by allowing attention heads to interact with each other, enhancing the model's ability to learn complex representations. Traditional MHA limits head interactions, which can weaken the overall performance as more heads are added. KHA introduces a shared projection matrix that maintains individual head specialization while enabling cross-head feature interactions. This method not only adds minimal computational overhead but also leads to better training stability and performance in large language models.", title='Enhancing Attention with Cross-Head Interactions'))
[28.10.2025 04:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"击头注意力（KHA）通过允许多头之间的交互，增强了多头注意力的效果，从而改善了大型语言模型的训练动态和性能。传统的多头注意力机制在增加头数时，往往会削弱每个头的能力，而KHA通过让头部之间进行特征级的交互，解决了这一问题。KHA使用共享的对角初始化投影矩阵，保持了头部的专业化，同时允许模型逐步学习综合的跨头表示。实验表明，KHA在训练动态和下游任务性能上优于传统的注意力机制。","title":"击头注意力：提升多头注意力的新方法"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='击头注意力（KHA）通过允许多头之间的交互，增强了多头注意力的效果，从而改善了大型语言模型的训练动态和性能。传统的多头注意力机制在增加头数时，往往会削弱每个头的能力，而KHA通过让头部之间进行特征级的交互，解决了这一问题。KHA使用共享的对角初始化投影矩阵，保持了头部的专业化，同时允许模型逐步学习综合的跨头表示。实验表明，KHA在训练动态和下游任务性能上优于传统的注意力机制。', title='击头注意力：提升多头注意力的新方法'))
[28.10.2025 04:16] Using data from previous issue: {"categories": ["#hallucinations", "#agents", "#optimization", "#plp", "#training"], "emoji": "🔧", "ru": {"title": "Проверенные факты вместо галлюцинаций: language servers как награда для coding-агентов", "desc": "Статья представляет Lanser-CLI — инструмент командной строки для управления Language S
[28.10.2025 04:16] Using data from previous issue: {"categories": ["#optimization", "#cv", "#training"], "emoji": "⚡", "ru": {"title": "Одношаговая генерация изображений через дистилляцию score-функций", "desc": "Статья представляет новый метод Distilled Decoding 2 (DD2) для ускорения генерации изображений в авторегрессионных моделях. В отличие от п
[28.10.2025 04:16] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#benchmark", "#interpretability", "#reasoning"], "emoji": "🔍", "ru": {"title": "Проверка логики, а не только ответов: новый подход к оценке рассуждений AI", "desc": "В статье представлен PRISM-Bench — бенчмарк для оценки процесса рассуждений моделей на основе в
[28.10.2025 04:16] Querying the API.
[28.10.2025 04:16] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

ReCode, a recursive code generation paradigm, unifies high-level planning and low-level action in a single representation, enhancing decision granularity and data efficiency in LLM-based agents.  					AI-generated summary 				 Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.
[28.10.2025 04:17] Response: ```json
{
  "desc": "ReCode — это новая парадигма для LLM-агентов, которая объединяет планирование высокого уровня и действия низкого уровня в единое представление через рекурсивную генерацию кода. Система рассматривает планы как абстрактные функции-заглушки, которые рекурсивно декомпозируются на более детальные подфункции вплоть до примитивных действий. Такой подход устраняет жёсткую границу между планированием и действием, позволяя агенту динамически контролировать гранулярность принятия решений. Эксперименты показывают значительное превосходство над существующими методами как в качестве inference, так и в эффективности обучения благодаря автоматической генерации иерархических обучающих данных.",
  "emoji": "🔄",
  "title": "Рекурсивный код для универсального управления гранулярностью решений"
}
```
[28.10.2025 04:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ReCode, a recursive code generation paradigm, unifies high-level planning and low-level action in a single representation, enhancing decision granularity and data efficiency in LLM-based agents.  					AI-generated summary 				 Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode."

[28.10.2025 04:17] Response: ```python
['AGENTS', 'TRAINING']
```
[28.10.2025 04:17] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"ReCode, a recursive code generation paradigm, unifies high-level planning and low-level action in a single representation, enhancing decision granularity and data efficiency in LLM-based agents.  					AI-generated summary 				 Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode."

[28.10.2025 04:17] Response: ```python
["AGI", "OPTIMIZATION"]
```
[28.10.2025 04:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ReCode introduces a new way for Large Language Model (LLM)-based agents to make decisions by combining high-level planning and low-level actions into one unified representation. This approach allows agents to adapt their decision-making process dynamically, breaking down complex tasks into simpler actions recursively. By treating high-level plans as abstract functions that can be decomposed, ReCode enhances the model\'s ability to learn from diverse training data and improves its performance on various tasks. The results show that ReCode outperforms existing methods in both efficiency and effectiveness, demonstrating the benefits of integrating planning and action.","title":"Unifying Planning and Action for Smarter Decision-Making"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="ReCode introduces a new way for Large Language Model (LLM)-based agents to make decisions by combining high-level planning and low-level actions into one unified representation. This approach allows agents to adapt their decision-making process dynamically, breaking down complex tasks into simpler actions recursively. By treating high-level plans as abstract functions that can be decomposed, ReCode enhances the model's ability to learn from diverse training data and improves its performance on various tasks. The results show that ReCode outperforms existing methods in both efficiency and effectiveness, demonstrating the benefits of integrating planning and action.", title='Unifying Planning and Action for Smarter Decision-Making'))
[28.10.2025 04:17] Response: ParsedChatCompletionMessage[Article](content='{"desc":"ReCode是一种递归代码生成范式，它将高层次的规划和低层次的行动统一在一个表示中，从而增强了基于大型语言模型（LLM）代理的决策粒度和数据效率。当前的LLM代理在决策粒度上存在局限，无法灵活地在高层次规划和低层次行动之间切换。ReCode通过将高层次计划视为抽象占位符函数，并递归地分解为更细粒度的子函数，解决了这一问题。实验结果表明，ReCode在推理性能上显著超越了先进的基线，并在训练中展现出卓越的数据效率。","title":"统一规划与行动的递归代码生成"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='ReCode是一种递归代码生成范式，它将高层次的规划和低层次的行动统一在一个表示中，从而增强了基于大型语言模型（LLM）代理的决策粒度和数据效率。当前的LLM代理在决策粒度上存在局限，无法灵活地在高层次规划和低层次行动之间切换。ReCode通过将高层次计划视为抽象占位符函数，并递归地分解为更细粒度的子函数，解决了这一问题。实验结果表明，ReCode在推理性能上显著超越了先进的基线，并在训练中展现出卓越的数据效率。', title='统一规划与行动的递归代码生成'))
[28.10.2025 04:17] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#dataset", "#multimodal"], "emoji": "🔗", "ru": {"title": "Объединяй и властвуй: эффективная мультимодальная модель из готовых компонентов", "desc": "Исследователи предложили эффективный способ создания мультимодальной модели путём объединения уже сущест
[28.10.2025 04:17] Using data from previous issue: {"categories": ["#open_source", "#video", "#rlhf", "#diffusion"], "emoji": "🎬", "ru": {"title": "Генерация длинных видео высокого качества с единой архитектурой", "desc": "LongCat-Video — это модель генерации видео на основе Diffusion Transformer с 13.6 миллиардами параметров, специализирующаяся на 
[28.10.2025 04:17] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#optimization", "#training"], "emoji": "🚀", "ru": {"title": "MARS-M: ускоренная оптимизация через матричное предобусловливание и уменьшение дисперсии", "desc": "В статье представлен MARS-M — новый оптимизатор для обучения больших нейронных сетей, котор
[28.10.2025 04:17] Renaming data file.
[28.10.2025 04:17] Renaming previous data. hf_papers.json to ./d/2025-10-28.json
[28.10.2025 04:17] Saving new data file.
[28.10.2025 04:17] Generating page.
[28.10.2025 04:17] Renaming previous page.
[28.10.2025 04:17] Renaming previous data. index.html to ./d/2025-10-28.html
[28.10.2025 04:17] Writing result.
[28.10.2025 04:17] Renaming log file.
[28.10.2025 04:17] Renaming previous data. log.txt to ./logs/2025-10-28_last_log.txt
