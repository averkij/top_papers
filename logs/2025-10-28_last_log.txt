[28.10.2025 21:12] Read previous papers.
[28.10.2025 21:12] Generating top page (month).
[28.10.2025 21:12] Writing top page (month).
[28.10.2025 22:11] Read previous papers.
[28.10.2025 22:11] Get feed.
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23607
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23564
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23587
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23588
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23581
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.21817
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22201
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22521
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22733
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22706
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23451
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23052
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23603
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23393
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22946
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22200
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23544
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23272
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.21003
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23571
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23479
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23594
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22975
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22907
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22849
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22236
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20512
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16320
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07723
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23605
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22603
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22317
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22010
[28.10.2025 22:11] Extract page data from URL. URL: https://huggingface.co/papers/2510.21986
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.21800
[28.10.2025 22:11] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23595
[28.10.2025 22:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.10.2025 22:11] No deleted papers detected.
[28.10.2025 22:11] Downloading and parsing papers (pdf, html). Total: 36.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23607.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23607.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23607.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23564.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23564.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23564.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23587.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23587.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23587.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23588.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23588.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23588.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23581.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23581.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23581.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.21817.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.21817.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.21817.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.22201.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.22201.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.22201.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.22521.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.22521.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.22521.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.22733.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.22733.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.22733.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.22706.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.22706.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.22706.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23451.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23451.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23451.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23052.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23052.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23052.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23603.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23603.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23603.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23393.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23393.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23393.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.22946.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.22946.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.22946.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.22200.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.22200.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.22200.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23544.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23544.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23544.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23272.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23272.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23272.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.21003.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.21003.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.21003.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23571.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23571.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23571.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23479.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23479.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23479.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23594.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23594.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23594.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.22975.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.22975.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.22975.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.22907.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.22907.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.22907.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.22849.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.22849.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.22849.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.22236.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.22236.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.22236.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.20512.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.20512.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.20512.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.16320.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.16320.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.16320.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.07723.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.07723.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.07723.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.23605.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.23605.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.23605.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.22603.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.22603.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.22603.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.22317.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.22317.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.22317.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.22010.
[28.10.2025 22:11] Extra JSON file exists (./assets/json/2510.22010.json), skip PDF parsing.
[28.10.2025 22:11] Paper image links file exists (./assets/img_data/2510.22010.json), skip HTML parsing.
[28.10.2025 22:11] Success.
[28.10.2025 22:11] Downloading and parsing paper https://huggingface.co/papers/2510.21986.
[28.10.2025 22:11] Downloading paper 2510.21986 from http://arxiv.org/pdf/2510.21986v1...
[28.10.2025 22:12] Extracting affiliations from text.
[28.10.2025 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 4 2 ] . [ 1 6 8 9 1 2 . 0 1 5 2 : r a SPRINT: SPARSE-DENSE RESIDUAL FUSION FOR EFFICIENT DIFFUSION TRANSFORMERS Dogyun Park1,2 Moayed Haji-Ali1 Yanyu Li1 Willi Menapace1 Sergey Tulyakov1 Hyunwoo J. Kim3 Aliaksandr Siarohin1 Anil Kag1 1Snap Inc. 2Korea University 3KAIST "
[28.10.2025 22:12] Response: ```python
["Snap Inc.", "Korea University", "KAIST"]
```
[28.10.2025 22:12] Deleting PDF ./assets/pdf/2510.21986.pdf.
[28.10.2025 22:12] Success.
[28.10.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2510.21800.
[28.10.2025 22:12] Extra JSON file exists (./assets/json/2510.21800.json), skip PDF parsing.
[28.10.2025 22:12] Paper image links file exists (./assets/img_data/2510.21800.json), skip HTML parsing.
[28.10.2025 22:12] Success.
[28.10.2025 22:12] Downloading and parsing paper https://huggingface.co/papers/2510.23595.
[28.10.2025 22:12] Extra JSON file exists (./assets/json/2510.23595.json), skip PDF parsing.
[28.10.2025 22:12] Paper image links file exists (./assets/img_data/2510.23595.json), skip HTML parsing.
[28.10.2025 22:12] Success.
[28.10.2025 22:12] Enriching papers with extra data.
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 0. Concerto, a minimalist model combining 3D self-distillation and 2D-3D joint embedding, achieves superior spatial feature learning and outperforms existing models in scene understanding and open-world perception.  					AI-generated summary 				 Humans learn abstract concepts through multisensory syne...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 1. ReCode, a recursive code generation paradigm, unifies high-level planning and low-level action in a single representation, enhancing decision granularity and data efficiency in LLM-based agents.  					AI-generated summary 				 Real-world tasks require decisions at varying granularities, and humans e...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 2. A systematic taxonomy for data agents is introduced to clarify their autonomy levels and capabilities, addressing terminological ambiguity and guiding future research and development.  					AI-generated summary 				 The rapid advancement of large language models (LLMs) has spurred the emergence of d...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 3. FARMER, a generative framework combining Normalizing Flows and Autoregressive models, achieves competitive image synthesis from raw pixels with exact likelihoods and scalable training.  					AI-generated summary 				 Directly modeling the explicit likelihood of the raw data distribution is key topic...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 4. Lookahead Anchoring improves audio-driven human animation by using future keyframes as dynamic guides, enhancing lip synchronization, identity preservation, and visual quality.  					AI-generated summary 				 Audio-driven human animation models often suffer from identity drift during temporal autore...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 5. VITA-E, a dual-model embodied interaction framework, enables concurrent and interruptible vision-language-action capabilities, enhancing real-time user interaction and multitasking.  					AI-generated summary 				 Current Vision-Language-Action (VLA) models are often constrained by a rigid, static i...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 6. Action Coherence Guidance (ACG) improves action coherence in Vision-Language-Action (VLA) models during test time, enhancing performance in diverse manipulation tasks.  					AI-generated summary 				 Diffusion and flow matching models have emerged as powerful robot policies, enabling Vision-Language...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 7. ORIG, an agentic open multimodal retrieval-augmented framework, enhances factual consistency and image quality in factual image generation by iteratively integrating refined web-based evidence into prompts.  					AI-generated summary 				 Large Multimodal Models (LMMs) have achieved remarkable progr...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 8. A unified framework extends a single text embedding model to perform both retrieval and listwise reranking, achieving state-of-the-art results with low latency.  					AI-generated summary 				 Text embedding models serve as a fundamental component in real-world search applications. By mapping querie...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 9. InstanceGrounded Geometry Transformer (IGGT) unifies 3D reconstruction and instance-level understanding using a unified transformer and 3D-Consistent Contrastive Learning, supported by a new dataset InsScene-15K.  					AI-generated summary 				 Humans naturally perceive the geometric structure and s...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 10. Omni-Reward addresses modality imbalance and preference rigidity in reward models by introducing a benchmark, dataset, and model that support multiple modalities and free-form preferences.  					AI-generated summary 				 Reward models (RMs) play a critical role in aligning AI behaviors with human pr...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 11. Knocking-heads attention (KHA) enhances multi-head attention by enabling cross-head interactions, improving training dynamics and performance in large language models.  					AI-generated summary 				 Multi-head attention (MHA) has become the cornerstone of modern large language models, enhancing rep...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 12. PixelRefer is a unified region-level MLLM framework that enhances fine-grained object-centric understanding using a Scale-Adaptive Object Tokenizer and Object-Centric Infusion module, achieving high performance and efficiency.  					AI-generated summary 				 Multimodal large language models (MLLMs) ...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 13. Optimizing the max@k metric through unbiased gradient estimates in RLVR improves the diversity and performance of Large Language Models in Best-of-N sampling scenarios.  					AI-generated summary 				 The application of Reinforcement Learning with Verifiable Rewards (RLVR) to mathematical and coding...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 14. A unified multimodal model achieves competitive performance with efficient fusion of generation and understanding models, using interleaved multimodal self-attention blocks and minimal training resources.  					AI-generated summary 				 Unified multimodal models have recently shown remarkable gains ...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 15. LongCat-Video, a 13.6B parameter video generation model based on the Diffusion Transformer framework, excels in efficient and high-quality long video generation across multiple tasks using unified architecture, coarse-to-fine generation, and block sparse attention.  					AI-generated summary 				 Vi...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 16. LIMRANK-SYNTHESIZER generates synthetic data to fine-tune LIMRANK, achieving competitive performance with minimal supervision on information reranking tasks.  					AI-generated summary 				 Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, w...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 17. A new pipeline enhances the aesthetic quality of LLM-generated code through instruction-tuning, agentic reward feedback, and joint optimization, outperforming existing models.  					AI-generated summary 				 Large Language Models (LLMs) have become valuable assistants for developers in code-related ...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 18. A new method, Distilled Decoding 2 (DD2), enables one-step sampling for image auto-regressive models with minimal performance degradation and significant speed-up compared to previous methods.  					AI-generated summary 				 Image Auto-regressive (AR) models have emerged as a powerful paradigm of vi...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 19. A new benchmarking framework uses large-scale simulated environments with human feedback to evaluate robot policies, addressing limitations in real-world testing and existing simulation benchmarks.  					AI-generated summary 				 The pursuit of robot generalists - instructable agents capable of perf...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 20. MergeMix, a training-time augmentation method, combines attention-aware image mixing and preference-driven training to improve vision-language alignment in multi-modal large language models with enhanced efficiency and accuracy.  					AI-generated summary 				 Vision-language alignment in multi-moda...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 21. PRISM-Bench evaluates models' reasoning processes by identifying errors in step-by-step solutions to visual puzzles, highlighting gaps between fluent generation and logical consistency.  					AI-generated summary 				 We introduce PRISM-Bench, a benchmark of puzzle-based visual challenges designed t...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 22. VoMP uses a feed-forward method with a Geometry Transformer to predict accurate volumetric material properties from 3D objects, outperforming existing methods in both accuracy and speed.  					AI-generated summary 				 Physical simulation relies on spatially-varying mechanical properties, often labo...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 23. Lanser-CLI orchestrates Language Server Protocol servers for coding agents and CI, providing deterministic workflows and actionable process rewards based on verified code facts.  					AI-generated summary 				 Large language models routinely hallucinate APIs and mislocalize edits, while language ser...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 24. Per-Instance Program Synthesis (PIPS) enhances LLM performance by generating and refining instance-level programs with structural feedback, improving accuracy and reducing undesirable solutions.  					AI-generated summary 				 Large language models (LLMs) excel at zero-shot inference but continue to...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 25. DiffusionLane, a diffusion-based model, enhances lane detection by refining noisy lane anchors through a hybrid diffusion decoder and auxiliary head, achieving superior performance across multiple benchmarks.  					AI-generated summary 				 In this paper, we present a novel diffusion-based model for...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 26. A bidirectional concept distillation framework enhances one-step text-to-image diffusion models by leveraging a multi-step model, improving personalization and generative quality.  					AI-generated summary 				 Recent advances in accelerating text-to-image (T2I) diffusion models have enabled the sy...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 27. Research on scaling laws in deepfake detection using the largest dataset to date reveals power-law scaling similar to large language models, enabling performance forecasting and data-centric countermeasures against evolving deepfake technology.  					AI-generated summary 				 This paper presents a s...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 28. SyncHuman combines 2D multiview and 3D native generative models with pixel-aligned synchronization and feature injection to achieve high-quality, photorealistic 3D human reconstruction from single images.  					AI-generated summary 				 Photorealistic 3D full-body human reconstruction from a single ...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 29. TIRE method enhances identity preservation in 3D/4D generation by tracking, inpainting, and resplatting regions of a 3D asset using video tracking and a subject-driven 2D inpainting model.  					AI-generated summary 				 Current 3D/4D generation methods are usually optimized for photorealism, effici...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 30. Attention sinks and massive activations in multimodal speech recognition are identified and mitigated using a decorrelation loss, improving word error rate under high feature downsampling.  					AI-generated summary 				 Large language models (LLMs) have recently advanced auditory speech recognition...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 31. Memory-based language modeling provides an efficient, eco-friendly alternative to deep neural networks, offering scalable performance and strong memorization with low ecological impact.  					AI-generated summary 				 We present memory-based language modeling as an efficient, eco-friendly alternativ...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 32. FlowOpt, a zero-order optimization framework, enables efficient control of diffusion and flow-matching models for image editing tasks without backpropagation.  					AI-generated summary 				 The remarkable success of diffusion and flow-matching models has ignited a surge of works on adapting them at...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 33. SPRINT, a method for efficient training of Diffusion Transformers, achieves significant training cost reduction and maintains quality through aggressive token dropping and residual fusion.  					AI-generated summary 				 Diffusion Transformers (DiTs) deliver state-of-the-art generative performance b...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 34. MARS-M, a new optimizer combining Muon and MARS techniques, achieves faster convergence and better performance in large-scale neural network training.  					AI-generated summary 				 Matrix-based preconditioned optimizers, such as Muon, have recently been shown to be more efficient than scalar-based...
[28.10.2025 22:12] ********************************************************************************
[28.10.2025 22:12] Abstract 35. Multi-Agent Evolve (MAE) framework uses reinforcement learning to enhance LLM reasoning across diverse tasks with minimal human supervision.  					AI-generated summary 				 Reinforcement Learning (RL) has demonstrated significant potential in enhancing the reasoning capabilities of large language mo...
[28.10.2025 22:12] Read previous papers.
[28.10.2025 22:12] Generating reviews via LLM API.
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#transfer_learning", "#multimodal", "#benchmark", "#3d", "#optimization", "#training"], "emoji": "üéº", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é —á–µ—Ä–µ–∑ —Å–∏–Ω–µ—Ä–≥–∏—é 2D –∏ 3D", "desc": "Concerto ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω–∞—è —Ç–µ–º, –∫–∞–∫ –ª—é–¥–∏ —É
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#agi", "#training", "#agents", "#optimization"], "emoji": "üîÑ", "ru": {"title": "–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –∫–æ–¥ –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å—é —Ä–µ—à–µ–Ω–∏–π", "desc": "ReCode ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –¥–ª—è LLM-–∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –∏ –¥–µ–π—Å—Ç–≤–∏—è –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –≤ –µ
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#survey", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –ø–æ–ª–Ω–æ–π –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏: —Ç–∞–∫—Å–æ–Ω–æ–º–∏—è data-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ç–∞–∫—Å–æ–Ω–æ–º–∏—è –¥–ª—è data-–∞–≥–µ–Ω—Ç–æ–≤ ‚Äî –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∫–æ—Ç–æ—Ä—ã–µ —É–ø—Ä–∞–≤–ª—è—é—Ç —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –∏ AI –¥–ª—è —Ä–µ—à–µ
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#architecture", "#training", "#inference", "#optimization"], "emoji": "üñºÔ∏è", "ru": {"title": "FARMER: –ù–æ–≤–∞—è —ç—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ FARMER, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç Normalizing Flows –∏ Autoregressive –º–æ–¥–µ–ª
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#architecture", "#audio", "#video", "#multimodal", "#games"], "emoji": "‚öì", "ru": {"title": "–Ø–∫–æ—Ä—å –∏–∑ –±—É–¥—É—â–µ–≥–æ: —É–ª—É—á—à–µ–Ω–∏–µ –∞–Ω–∏–º–∞—Ü–∏–∏ –ª—é–¥–µ–π —á–µ—Ä–µ–∑ –æ–ø–µ—Ä–µ–∂–∞—é—â–µ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Lookahead Anchoring –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–Ω–∏–º–∞—Ü–∏–∏ –ª—é–¥–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ. –í–º–µ—Å—Ç–æ –≥–µ
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#agi", "#interpretability", "#reasoning", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–î–≤—É—Ö–º–æ–¥–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ—Ä—ã–≤–∞–µ–º–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ Vision-Language-Action –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –ø–æ –∂—ë—Å—Ç–∫–æ–º—É —Å—Ü–µ–Ω–∞—Ä–∏—é –∏ –Ω–µ –º–æ–≥—É—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#robotics", "#games", "#agents", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ü–ª–∞–≤–Ω—ã–µ –¥–≤–∏–∂–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Action Coherence Guidance (ACG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –≤ Vision-Language-Action (VLA) –º–æ–¥–µ–ª—è—Ö 
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#benchmark", "#games", "#interpretability", "#optimization"], "emoji": "üîç", "ru": {"title": "–§–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Ç–æ—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç–Ω—ã–π –≤–µ–±-–ø–æ–∏—Å–∫", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ORIG ‚Äî –∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Ç–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#optimization", "#rag"], "emoji": "üîÑ", "ru": {"title": "–û–¥–Ω–∞ embedding-–º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç E^2Rank ‚Äî –µ–¥–∏–Ω—É—éÊ°ÜÊû∂, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç–æ–≤–æ–π embedding-–º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–∞–∫ –ø–æ–∏—Å–∫–∞, —Ç–∞–∫ –∏ list
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#dataset", "#games", "#optimization", "#3d"], "emoji": "üéØ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Å—Ü–µ–Ω–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ IGGT ‚Äî —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—é 3D-—Å—Ü–µ–Ω—ã –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞—ë—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –æ–±—ä–µ–∫
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#data", "#multimodal", "#benchmark", "#dataset", "#alignment", "#architecture"], "emoji": "üéÅ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Omni-Reward ‚Äî —Å–∏—Å—Ç–µ–º—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è reward model
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#training"], "emoji": "ü§ù", "ru": {"title": "–ì–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è —É—á–∞—Ç—Å—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º knocking-heads attention (KHA), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π multi-head attention, –ø–æ–∑–≤–æ–ª—è—è –≥–æ–ª–æ–≤–∞–º –≤–Ω–∏–º–∞–Ω–∏
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#games", "#dataset", "#optimization", "#reasoning", "#training"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM", "desc": "PixelRefer ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#rlhf", "#training", "#reasoning", "#rl", "#optimization"], "emoji": "üéØ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è LLM –¥–ª—è Best-of-N —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RLVR) —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#dataset", "#multimodal"], "emoji": "üîó", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∏–∑ –≥–æ—Ç–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± —Å–æ–∑–¥–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—É—Ç—ë–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#open_source", "#video", "#rlhf", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å –µ–¥–∏–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π", "desc": "LongCat-Video ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ Diffusion Transformer —Å 13.6 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∞—è—Å—è –Ω–∞ 
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#data", "#rag", "#open_source", "#benchmark", "#dataset", "#synthetic", "#reasoning", "#training"], "emoji": "üéØ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LIMRANK-SYNTHESIZER ‚Äî pipeline –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#training", "#rlhf", "#optimization", "#architecture", "#open_source"], "emoji": "üé®", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ LLM —Å–æ–∑–¥–∞–≤–∞—Ç—å –∫—Ä–∞—Å–∏–≤—ã–π –∫–æ–¥ —Å –ø–æ–º–æ—â—å—é –∞–≥–µ–Ω—Ç–æ–≤-–æ—Ü–µ–Ω—â–∏–∫–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Å—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#optimization", "#cv", "#training"], "emoji": "‚ö°", "ru": {"title": "–û–¥–Ω–æ—à–∞–≥–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é score-—Ñ—É–Ω–∫—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Distilled Decoding 2 (DD2) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#agents", "#games", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–°–∏–º—É–ª—è—Ü–∏—è —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–æ–±–æ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–∏—Ç–∏–∫", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–æ–±–æ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–∏—Ç–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –æ—Ü–µ–Ω–∫—É VLA
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#training", "#multimodal", "#optimization", "#alignment"], "emoji": "üé®", "ru": {"title": "MergeMix: —É–º–Ω–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MergeMix ‚Äî –º–µ—Ç–æ–¥ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#benchmark", "#interpretability", "#reasoning"], "emoji": "üîç", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–∏–∫–∏, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–æ–≤: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π AI", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω PRISM-Bench ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#science", "#dataset", "#benchmark", "#optimization", "#3d"], "emoji": "üßä", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏–∑ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞", "desc": "VoMP ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ–±—ä—ë–º–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ (–º–æ–¥—É
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#hallucinations", "#agents", "#optimization", "#plp", "#training"], "emoji": "üîß", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã –≤–º–µ—Å—Ç–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π: language servers –∫–∞–∫ –Ω–∞–≥—Ä–∞–¥–∞ –¥–ª—è coding-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Lanser-CLI ‚Äî –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è Language S
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#training", "#optimization", "#benchmark", "#math"], "emoji": "üéØ", "ru": {"title": "–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –º–µ—Ç–æ–¥ PIPS, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#cv", "#benchmark", "#architecture"], "emoji": "üõ£Ô∏è", "ru": {"title": "–î–µ—Ç–µ–∫—Ü–∏—è –¥–æ—Ä–æ–∂–Ω—ã—Ö –ø–æ–ª–æ—Å —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ —à—É–º–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤", "desc": "DiffusionLane - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–æ—Ä–æ–∂–Ω—ã—Ö –ø–æ–ª–æ—Å, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#training", "#diffusion", "#optimization"], "emoji": "üîÑ", "ru": {"title": "–î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑–∞ –æ–¥–∏–Ω —à–∞–≥", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ EchoDistill –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#security", "#data", "#optimization", "#dataset", "#benchmark"], "emoji": "üîç", "ru": {"title": "–°—Ç–µ–ø–µ–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–∏–ø—Ñ–µ–π–∫–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∑–∞–¥–∞—á–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–∏–ø—Ñ–µ–π–∫–æ–≤, —Å–æ–∑–¥–∞–≤ –∫—Ä—É–ø–Ω–µ–π—à–∏–π –¥–∞—Ç–∞—Å–µ—Ç ScaleDF —Å 5.8 –º–ª–Ω —Ä–µ–∞–ª
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#games", "#3d"], "emoji": "üßç", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ 2D –∏ 3D –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞", "desc": "SyncHuman - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π —á–µ–ª–æ–≤–µ–∫–∞ –ø–æ –æ–¥–Ω–æ–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –∫–∏–Ω–æ –∏ –∏–≥—Ä. –ú–µ—Ç
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#optimization", "#3d"], "emoji": "üéØ", "ru": {"title": "TIRE: –¢—Ä–µ–∫–∏–Ω–≥, –∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥ –∏ resplatting –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –≤ 3D", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ TIRE –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D/4D –æ–±—ä–µ–∫—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ —Å—É–±—ä–µ–∫—Ç–∞. –ú–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞: –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –Ω—É–∂–Ω—ã—Ö
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#training", "#optimization", "#multimodal", "#audio", "#interpretability"], "emoji": "üé§", "ru": {"title": "–£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ ¬´—Ç–æ–∫–µ–Ω–æ–≤-–º–∞–≥–Ω–∏—Ç–æ–≤¬ª –≤–Ω–∏–º–∞–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ —Ä–µ—á–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤–ø–µ—Ä–≤—ã–µ –∏–∑—É—á–∏–ª–∏ —Ñ–µ–Ω–æ–º–µ–Ω attention sinks (—Ç–æ–∫–µ–Ω–æ–≤, –ø—Ä–∏—Ç—è–≥–∏–≤–∞—é—â–∏—Ö –Ω–µ–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#optimization", "#interpretability", "#training", "#inference", "#data", "#architecture"], "emoji": "üå±", "ru": {"title": "–Ø–∑—ã–∫–æ–≤–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞–º—è—Ç–∏ –∫–∞–∫ —ç–∫–æ–ª–æ–≥–∏—á–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —è–∑—ã–∫–æ–≤–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞–º—è—Ç–∏ –∫–∞–∫ —ç
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#training", "#diffusion", "#cv", "#optimization"], "emoji": "üéØ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤", "desc": "FlowOpt ‚Äî —ç—Ç–æ framework –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ flow-matching –º–æ–¥–µ–ª–µ–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è backpropagation. –ú–µ—Ç–æ–¥ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≤–µ
[28.10.2025 22:12] Querying the API.
[28.10.2025 22:12] Claude request. Model: claude-sonnet-4-5-20250929. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

SPRINT, a method for efficient training of Diffusion Transformers, achieves significant training cost reduction and maintains quality through aggressive token dropping and residual fusion.  					AI-generated summary 				 Diffusion Transformers (DiTs) deliver state-of-the-art generative performance but their quadratic training cost with sequence length makes large-scale pretraining prohibitively expensive. Token dropping can reduce training cost, yet na\"ive strategies degrade representations, and existing methods are either parameter-heavy or fail at high drop ratios. We present SPRINT, Sparse--Dense Residual Fusion for Efficient Diffusion Transformers, a simple method that enables aggressive token dropping (up to 75%) while preserving quality. SPRINT leverages the complementary roles of shallow and deep layers: early layers process all tokens to capture local detail, deeper layers operate on a sparse subset to cut computation, and their outputs are fused through residual connections. Training follows a two-stage schedule: long masked pre-training for efficiency followed by short full-token fine-tuning to close the train--inference gap. On ImageNet-1K 256x256, SPRINT achieves 9.8x training savings with comparable FID/FDD, and at inference, its Path-Drop Guidance (PDG) nearly halves FLOPs while improving quality. These results establish SPRINT as a simple, effective, and general solution for efficient DiT training.
[28.10.2025 22:12] Response: ```json
{
  "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ —É–º–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤",
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ SPRINT –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è Diffusion Transformers (DiT), –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –¥–æ 75% —Ç–æ–∫–µ–Ω–æ–≤: —Ä–∞–Ω–Ω–∏–µ —Å–ª–æ–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –¥–µ—Ç–∞–ª–µ–π, –≥–ª—É–±–æ–∫–∏–µ —Å–ª–æ–∏ —Ä–∞–±–æ—Ç–∞—é—Ç —Å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ–º, –∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è —á–µ—Ä–µ–∑ residual connections. –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Ö–æ–¥–∏—Ç –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞: –¥–ª–∏—Ç–µ–ª—å–Ω—ã–π pre-training —Å –º–∞—Å–∫–∞–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏–π fine-tuning –Ω–∞ –ø–æ–ª–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö. –ù–∞ ImageNet-1K –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 9.8x —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∞ —Ç–µ—Ö–Ω–∏–∫–∞ Path-Drop Guidance —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è inference –ø–æ—á—Ç–∏ –≤–¥–≤–æ–µ.",
  "emoji": "‚ö°",
  "desc_length": 4
}
```
[28.10.2025 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SPRINT, a method for efficient training of Diffusion Transformers, achieves significant training cost reduction and maintains quality through aggressive token dropping and residual fusion.  					AI-generated summary 				 Diffusion Transformers (DiTs) deliver state-of-the-art generative performance but their quadratic training cost with sequence length makes large-scale pretraining prohibitively expensive. Token dropping can reduce training cost, yet na\"ive strategies degrade representations, and existing methods are either parameter-heavy or fail at high drop ratios. We present SPRINT, Sparse--Dense Residual Fusion for Efficient Diffusion Transformers, a simple method that enables aggressive token dropping (up to 75%) while preserving quality. SPRINT leverages the complementary roles of shallow and deep layers: early layers process all tokens to capture local detail, deeper layers operate on a sparse subset to cut computation, and their outputs are fused through residual connections. Training follows a two-stage schedule: long masked pre-training for efficiency followed by short full-token fine-tuning to close the train--inference gap. On ImageNet-1K 256x256, SPRINT achieves 9.8x training savings with comparable FID/FDD, and at inference, its Path-Drop Guidance (PDG) nearly halves FLOPs while improving quality. These results establish SPRINT as a simple, effective, and general solution for efficient DiT training."

[28.10.2025 22:12] Response: ```python
['TRAINING', 'ARCHITECTURE']
```
[28.10.2025 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"SPRINT, a method for efficient training of Diffusion Transformers, achieves significant training cost reduction and maintains quality through aggressive token dropping and residual fusion.  					AI-generated summary 				 Diffusion Transformers (DiTs) deliver state-of-the-art generative performance but their quadratic training cost with sequence length makes large-scale pretraining prohibitively expensive. Token dropping can reduce training cost, yet na\"ive strategies degrade representations, and existing methods are either parameter-heavy or fail at high drop ratios. We present SPRINT, Sparse--Dense Residual Fusion for Efficient Diffusion Transformers, a simple method that enables aggressive token dropping (up to 75%) while preserving quality. SPRINT leverages the complementary roles of shallow and deep layers: early layers process all tokens to capture local detail, deeper layers operate on a sparse subset to cut computation, and their outputs are fused through residual connections. Training follows a two-stage schedule: long masked pre-training for efficiency followed by short full-token fine-tuning to close the train--inference gap. On ImageNet-1K 256x256, SPRINT achieves 9.8x training savings with comparable FID/FDD, and at inference, its Path-Drop Guidance (PDG) nearly halves FLOPs while improving quality. These results establish SPRINT as a simple, effective, and general solution for efficient DiT training."

[28.10.2025 22:12] Response: ```python
["OPTIMIZATION", "DIFFUSION"]
```
[28.10.2025 22:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SPRINT is a novel method designed to enhance the training efficiency of Diffusion Transformers (DiTs) by significantly reducing training costs while maintaining high-quality outputs. It employs aggressive token dropping, allowing up to 75% of tokens to be discarded without degrading the model\'s performance. The method utilizes a combination of shallow and deep layers, where early layers process all tokens for detailed information, and deeper layers work with a sparse subset to minimize computation. By implementing a two-stage training schedule, SPRINT achieves substantial training savings and improved inference efficiency, making it a promising solution for large-scale DiT applications.","title":"Efficient Training of Diffusion Transformers with SPRINT"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="SPRINT is a novel method designed to enhance the training efficiency of Diffusion Transformers (DiTs) by significantly reducing training costs while maintaining high-quality outputs. It employs aggressive token dropping, allowing up to 75% of tokens to be discarded without degrading the model's performance. The method utilizes a combination of shallow and deep layers, where early layers process all tokens for detailed information, and deeper layers work with a sparse subset to minimize computation. By implementing a two-stage training schedule, SPRINT achieves substantial training savings and improved inference efficiency, making it a promising solution for large-scale DiT applications.", title='Efficient Training of Diffusion Transformers with SPRINT'))
[28.10.2025 22:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"SPRINTÊòØ‰∏ÄÁßçÈ´òÊïàËÆ≠ÁªÉÊâ©Êï£ÂèòÊç¢Âô®ÔºàDiffusion TransformersÔºâÁöÑÊñπÊ≥ïÔºåÈÄöËøáÊøÄËøõÁöÑ‰ª§Áâå‰∏¢ÂºÉÂíåÊÆãÂ∑ÆËûçÂêàÊòæËëóÈôç‰ΩéËÆ≠ÁªÉÊàêÊú¨ÔºåÂêåÊó∂‰øùÊåÅÊ®°ÂûãË¥®Èáè„ÄÇÊâ©Êï£ÂèòÊç¢Âô®Âú®ÁîüÊàêÊÄßËÉΩ‰∏äË°®Áé∞‰ºòÂºÇÔºå‰ΩÜÁî±‰∫éÂ∫èÂàóÈïøÂ∫¶ÂØºËá¥ÁöÑËÆ≠ÁªÉÊàêÊú¨Âëà‰∫åÊ¨°Â¢ûÈïøÔºå‰ΩøÂæóÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÂèòÂæóÈùûÂ∏∏ÊòÇË¥µ„ÄÇSPRINTÈÄöËøáÁªìÂêàÊµÖÂ±ÇÂíåÊ∑±Â±ÇÁöÑ‰∫íË°•‰ΩúÁî®ÔºåÂÖÅËÆ∏È´òËææ75%ÁöÑ‰ª§Áâå‰∏¢ÂºÉÔºåÂêåÊó∂‰øùÁïôÈáçË¶ÅÁöÑÁâπÂæÅ‰ø°ÊÅØ„ÄÇËØ•ÊñπÊ≥ïÈááÁî®‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ÔºåÈ¶ñÂÖàËøõË°åÈïøÊó∂Èó¥ÁöÑÊé©ËîΩÈ¢ÑËÆ≠ÁªÉ‰ª•ÊèêÈ´òÊïàÁéáÔºåÁÑ∂ÂêéËøõË°åÁü≠Êó∂Èó¥ÁöÑÂÖ®‰ª§ÁâåÂæÆË∞ÉÔºå‰ª•Áº©Â∞èËÆ≠ÁªÉ‰∏éÊé®ÁêÜ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ","title":"È´òÊïàËÆ≠ÁªÉÊâ©Êï£ÂèòÊç¢Âô®ÁöÑSPRINTÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='SPRINTÊòØ‰∏ÄÁßçÈ´òÊïàËÆ≠ÁªÉÊâ©Êï£ÂèòÊç¢Âô®ÔºàDiffusion TransformersÔºâÁöÑÊñπÊ≥ïÔºåÈÄöËøáÊøÄËøõÁöÑ‰ª§Áâå‰∏¢ÂºÉÂíåÊÆãÂ∑ÆËûçÂêàÊòæËëóÈôç‰ΩéËÆ≠ÁªÉÊàêÊú¨ÔºåÂêåÊó∂‰øùÊåÅÊ®°ÂûãË¥®Èáè„ÄÇÊâ©Êï£ÂèòÊç¢Âô®Âú®ÁîüÊàêÊÄßËÉΩ‰∏äË°®Áé∞‰ºòÂºÇÔºå‰ΩÜÁî±‰∫éÂ∫èÂàóÈïøÂ∫¶ÂØºËá¥ÁöÑËÆ≠ÁªÉÊàêÊú¨Âëà‰∫åÊ¨°Â¢ûÈïøÔºå‰ΩøÂæóÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÂèòÂæóÈùûÂ∏∏ÊòÇË¥µ„ÄÇSPRINTÈÄöËøáÁªìÂêàÊµÖÂ±ÇÂíåÊ∑±Â±ÇÁöÑ‰∫íË°•‰ΩúÁî®ÔºåÂÖÅËÆ∏È´òËææ75%ÁöÑ‰ª§Áâå‰∏¢ÂºÉÔºåÂêåÊó∂‰øùÁïôÈáçË¶ÅÁöÑÁâπÂæÅ‰ø°ÊÅØ„ÄÇËØ•ÊñπÊ≥ïÈááÁî®‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ÔºåÈ¶ñÂÖàËøõË°åÈïøÊó∂Èó¥ÁöÑÊé©ËîΩÈ¢ÑËÆ≠ÁªÉ‰ª•ÊèêÈ´òÊïàÁéáÔºåÁÑ∂ÂêéËøõË°åÁü≠Êó∂Èó¥ÁöÑÂÖ®‰ª§ÁâåÂæÆË∞ÉÔºå‰ª•Áº©Â∞èËÆ≠ÁªÉ‰∏éÊé®ÁêÜ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ', title='È´òÊïàËÆ≠ÁªÉÊâ©Êï£ÂèòÊç¢Âô®ÁöÑSPRINTÊñπÊ≥ï'))
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#optimization", "#training"], "emoji": "üöÄ", "ru": {"title": "MARS-M: —É—Å–∫–æ—Ä–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –º–∞—Ç—Ä–∏—á–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞–Ω–∏–µ –∏ —É–º–µ–Ω—å—à–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MARS-M ‚Äî –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –∫–æ—Ç–æ—Ä
[28.10.2025 22:12] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#optimization", "#reasoning", "#rl", "#games"], "emoji": "üîÑ", "ru": {"title": "–¢—Ä–∏ –∞–≥–µ–Ω—Ç–∞ —É—á–∞—Ç –¥—Ä—É–≥ –¥—Ä—É–≥–∞: —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏—è LLM —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –¥–∏–∞–ª–æ–≥", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Multi-Agent Evolve (MAE), –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç reinforcement learning –¥
[28.10.2025 22:12] Renaming data file.
[28.10.2025 22:12] Renaming previous data. hf_papers.json to ./d/2025-10-28.json
[28.10.2025 22:12] Saving new data file.
[28.10.2025 22:12] Generating page.
[28.10.2025 22:12] Renaming previous page.
[28.10.2025 22:12] Renaming previous data. index.html to ./d/2025-10-28.html
[28.10.2025 22:12] Writing result.
[28.10.2025 22:12] Renaming log file.
[28.10.2025 22:12] Renaming previous data. log.txt to ./logs/2025-10-28_last_log.txt
