[28.10.2025 17:12] Read previous papers.
[28.10.2025 17:12] Generating top page (month).
[28.10.2025 17:12] Writing top page (month).
[28.10.2025 18:18] Read previous papers.
[28.10.2025 18:18] Get feed.
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23607
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23564
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23587
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23588
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.21817
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23581
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22201
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22521
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22733
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22706
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23451
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23052
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23603
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23393
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22946
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22200
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23544
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23272
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.21003
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23571
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23479
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22975
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22907
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23594
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22236
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.20512
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.16320
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.07723
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.23605
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22849
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22603
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22317
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.22010
[28.10.2025 18:18] Get page data from previous paper. URL: https://huggingface.co/papers/2510.21800
[28.10.2025 18:18] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[28.10.2025 18:18] No deleted papers detected.
[28.10.2025 18:18] Downloading and parsing papers (pdf, html). Total: 34.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23607.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23607.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23607.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23564.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23564.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23564.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23587.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23587.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23587.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23588.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23588.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23588.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.21817.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.21817.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.21817.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23581.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23581.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23581.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.22201.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.22201.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.22201.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.22521.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.22521.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.22521.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.22733.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.22733.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.22733.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.22706.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.22706.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.22706.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23451.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23451.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23451.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23052.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23052.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23052.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23603.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23603.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23603.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23393.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23393.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23393.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.22946.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.22946.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.22946.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.22200.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.22200.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.22200.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23544.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23544.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23544.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23272.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23272.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23272.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.21003.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.21003.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.21003.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23571.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23571.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23571.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23479.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23479.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23479.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.22975.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.22975.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.22975.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.22907.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.22907.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.22907.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23594.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23594.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23594.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.22236.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.22236.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.22236.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.20512.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.20512.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.20512.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.16320.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.16320.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.16320.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.07723.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.07723.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.07723.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.23605.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.23605.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.23605.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.22849.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.22849.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.22849.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.22603.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.22603.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.22603.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.22317.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.22317.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.22317.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.22010.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.22010.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.22010.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Downloading and parsing paper https://huggingface.co/papers/2510.21800.
[28.10.2025 18:18] Extra JSON file exists (./assets/json/2510.21800.json), skip PDF parsing.
[28.10.2025 18:18] Paper image links file exists (./assets/img_data/2510.21800.json), skip HTML parsing.
[28.10.2025 18:18] Success.
[28.10.2025 18:18] Enriching papers with extra data.
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 0. Concerto, a minimalist model combining 3D self-distillation and 2D-3D joint embedding, achieves superior spatial feature learning and outperforms existing models in scene understanding and open-world perception.  					AI-generated summary 				 Humans learn abstract concepts through multisensory syne...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 1. ReCode, a recursive code generation paradigm, unifies high-level planning and low-level action in a single representation, enhancing decision granularity and data efficiency in LLM-based agents.  					AI-generated summary 				 Real-world tasks require decisions at varying granularities, and humans e...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 2. A systematic taxonomy for data agents is introduced to clarify their autonomy levels and capabilities, addressing terminological ambiguity and guiding future research and development.  					AI-generated summary 				 The rapid advancement of large language models (LLMs) has spurred the emergence of d...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 3. FARMER, a generative framework combining Normalizing Flows and Autoregressive models, achieves competitive image synthesis from raw pixels with exact likelihoods and scalable training.  					AI-generated summary 				 Directly modeling the explicit likelihood of the raw data distribution is key topic...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 4. VITA-E, a dual-model embodied interaction framework, enables concurrent and interruptible vision-language-action capabilities, enhancing real-time user interaction and multitasking.  					AI-generated summary 				 Current Vision-Language-Action (VLA) models are often constrained by a rigid, static i...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 5. Lookahead Anchoring improves audio-driven human animation by using future keyframes as dynamic guides, enhancing lip synchronization, identity preservation, and visual quality.  					AI-generated summary 				 Audio-driven human animation models often suffer from identity drift during temporal autore...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 6. Action Coherence Guidance (ACG) improves action coherence in Vision-Language-Action (VLA) models during test time, enhancing performance in diverse manipulation tasks.  					AI-generated summary 				 Diffusion and flow matching models have emerged as powerful robot policies, enabling Vision-Language...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 7. ORIG, an agentic open multimodal retrieval-augmented framework, enhances factual consistency and image quality in factual image generation by iteratively integrating refined web-based evidence into prompts.  					AI-generated summary 				 Large Multimodal Models (LMMs) have achieved remarkable progr...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 8. A unified framework extends a single text embedding model to perform both retrieval and listwise reranking, achieving state-of-the-art results with low latency.  					AI-generated summary 				 Text embedding models serve as a fundamental component in real-world search applications. By mapping querie...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 9. InstanceGrounded Geometry Transformer (IGGT) unifies 3D reconstruction and instance-level understanding using a unified transformer and 3D-Consistent Contrastive Learning, supported by a new dataset InsScene-15K.  					AI-generated summary 				 Humans naturally perceive the geometric structure and s...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 10. Omni-Reward addresses modality imbalance and preference rigidity in reward models by introducing a benchmark, dataset, and model that support multiple modalities and free-form preferences.  					AI-generated summary 				 Reward models (RMs) play a critical role in aligning AI behaviors with human pr...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 11. Knocking-heads attention (KHA) enhances multi-head attention by enabling cross-head interactions, improving training dynamics and performance in large language models.  					AI-generated summary 				 Multi-head attention (MHA) has become the cornerstone of modern large language models, enhancing rep...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 12. PixelRefer is a unified region-level MLLM framework that enhances fine-grained object-centric understanding using a Scale-Adaptive Object Tokenizer and Object-Centric Infusion module, achieving high performance and efficiency.  					AI-generated summary 				 Multimodal large language models (MLLMs) ...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 13. Optimizing the max@k metric through unbiased gradient estimates in RLVR improves the diversity and performance of Large Language Models in Best-of-N sampling scenarios.  					AI-generated summary 				 The application of Reinforcement Learning with Verifiable Rewards (RLVR) to mathematical and coding...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 14. A unified multimodal model achieves competitive performance with efficient fusion of generation and understanding models, using interleaved multimodal self-attention blocks and minimal training resources.  					AI-generated summary 				 Unified multimodal models have recently shown remarkable gains ...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 15. LongCat-Video, a 13.6B parameter video generation model based on the Diffusion Transformer framework, excels in efficient and high-quality long video generation across multiple tasks using unified architecture, coarse-to-fine generation, and block sparse attention.  					AI-generated summary 				 Vi...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 16. LIMRANK-SYNTHESIZER generates synthetic data to fine-tune LIMRANK, achieving competitive performance with minimal supervision on information reranking tasks.  					AI-generated summary 				 Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, w...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 17. A new pipeline enhances the aesthetic quality of LLM-generated code through instruction-tuning, agentic reward feedback, and joint optimization, outperforming existing models.  					AI-generated summary 				 Large Language Models (LLMs) have become valuable assistants for developers in code-related ...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 18. A new method, Distilled Decoding 2 (DD2), enables one-step sampling for image auto-regressive models with minimal performance degradation and significant speed-up compared to previous methods.  					AI-generated summary 				 Image Auto-regressive (AR) models have emerged as a powerful paradigm of vi...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 19. A new benchmarking framework uses large-scale simulated environments with human feedback to evaluate robot policies, addressing limitations in real-world testing and existing simulation benchmarks.  					AI-generated summary 				 The pursuit of robot generalists - instructable agents capable of perf...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 20. MergeMix, a training-time augmentation method, combines attention-aware image mixing and preference-driven training to improve vision-language alignment in multi-modal large language models with enhanced efficiency and accuracy.  					AI-generated summary 				 Vision-language alignment in multi-moda...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 21. VoMP uses a feed-forward method with a Geometry Transformer to predict accurate volumetric material properties from 3D objects, outperforming existing methods in both accuracy and speed.  					AI-generated summary 				 Physical simulation relies on spatially-varying mechanical properties, often labo...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 22. Lanser-CLI orchestrates Language Server Protocol servers for coding agents and CI, providing deterministic workflows and actionable process rewards based on verified code facts.  					AI-generated summary 				 Large language models routinely hallucinate APIs and mislocalize edits, while language ser...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 23. PRISM-Bench evaluates models' reasoning processes by identifying errors in step-by-step solutions to visual puzzles, highlighting gaps between fluent generation and logical consistency.  					AI-generated summary 				 We introduce PRISM-Bench, a benchmark of puzzle-based visual challenges designed t...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 24. DiffusionLane, a diffusion-based model, enhances lane detection by refining noisy lane anchors through a hybrid diffusion decoder and auxiliary head, achieving superior performance across multiple benchmarks.  					AI-generated summary 				 In this paper, we present a novel diffusion-based model for...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 25. A bidirectional concept distillation framework enhances one-step text-to-image diffusion models by leveraging a multi-step model, improving personalization and generative quality.  					AI-generated summary 				 Recent advances in accelerating text-to-image (T2I) diffusion models have enabled the sy...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 26. Research on scaling laws in deepfake detection using the largest dataset to date reveals power-law scaling similar to large language models, enabling performance forecasting and data-centric countermeasures against evolving deepfake technology.  					AI-generated summary 				 This paper presents a s...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 27. SyncHuman combines 2D multiview and 3D native generative models with pixel-aligned synchronization and feature injection to achieve high-quality, photorealistic 3D human reconstruction from single images.  					AI-generated summary 				 Photorealistic 3D full-body human reconstruction from a single ...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 28. TIRE method enhances identity preservation in 3D/4D generation by tracking, inpainting, and resplatting regions of a 3D asset using video tracking and a subject-driven 2D inpainting model.  					AI-generated summary 				 Current 3D/4D generation methods are usually optimized for photorealism, effici...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 29. Per-Instance Program Synthesis (PIPS) enhances LLM performance by generating and refining instance-level programs with structural feedback, improving accuracy and reducing undesirable solutions.  					AI-generated summary 				 Large language models (LLMs) excel at zero-shot inference but continue to...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 30. Attention sinks and massive activations in multimodal speech recognition are identified and mitigated using a decorrelation loss, improving word error rate under high feature downsampling.  					AI-generated summary 				 Large language models (LLMs) have recently advanced auditory speech recognition...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 31. Memory-based language modeling provides an efficient, eco-friendly alternative to deep neural networks, offering scalable performance and strong memorization with low ecological impact.  					AI-generated summary 				 We present memory-based language modeling as an efficient, eco-friendly alternativ...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 32. FlowOpt, a zero-order optimization framework, enables efficient control of diffusion and flow-matching models for image editing tasks without backpropagation.  					AI-generated summary 				 The remarkable success of diffusion and flow-matching models has ignited a surge of works on adapting them at...
[28.10.2025 18:18] ********************************************************************************
[28.10.2025 18:18] Abstract 33. MARS-M, a new optimizer combining Muon and MARS techniques, achieves faster convergence and better performance in large-scale neural network training.  					AI-generated summary 				 Matrix-based preconditioned optimizers, such as Muon, have recently been shown to be more efficient than scalar-based...
[28.10.2025 18:18] Read previous papers.
[28.10.2025 18:18] Generating reviews via LLM API.
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#transfer_learning", "#multimodal", "#benchmark", "#3d", "#optimization", "#training"], "emoji": "üéº", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é —á–µ—Ä–µ–∑ —Å–∏–Ω–µ—Ä–≥–∏—é 2D –∏ 3D", "desc": "Concerto ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω–∞—è —Ç–µ–º, –∫–∞–∫ –ª—é–¥–∏ —É
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#agi", "#training", "#agents", "#optimization"], "emoji": "üîÑ", "ru": {"title": "–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –∫–æ–¥ –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å—é —Ä–µ—à–µ–Ω–∏–π", "desc": "ReCode ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –¥–ª—è LLM-–∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –∏ –¥–µ–π—Å—Ç–≤–∏—è –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –≤ –µ
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#survey", "#agents"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –ø–æ–ª–Ω–æ–π –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏: —Ç–∞–∫—Å–æ–Ω–æ–º–∏—è data-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ç–∞–∫—Å–æ–Ω–æ–º–∏—è –¥–ª—è data-–∞–≥–µ–Ω—Ç–æ–≤ ‚Äî –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∫–æ—Ç–æ—Ä—ã–µ —É–ø—Ä–∞–≤–ª—è—é—Ç —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –∏ AI –¥–ª—è —Ä–µ—à–µ
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#architecture", "#training", "#inference", "#optimization"], "emoji": "üñºÔ∏è", "ru": {"title": "FARMER: –ù–æ–≤–∞—è —ç—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ FARMER, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç Normalizing Flows –∏ Autoregressive –º–æ–¥–µ–ª
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#multimodal", "#agents", "#agi", "#interpretability", "#reasoning", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "–î–≤—É—Ö–º–æ–¥–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ—Ä—ã–≤–∞–µ–º–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ Vision-Language-Action –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –ø–æ –∂—ë—Å—Ç–∫–æ–º—É —Å—Ü–µ–Ω–∞—Ä–∏—é –∏ –Ω–µ –º–æ–≥—É—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#architecture", "#audio", "#video", "#multimodal", "#games"], "emoji": "‚öì", "ru": {"title": "–Ø–∫–æ—Ä—å –∏–∑ –±—É–¥—É—â–µ–≥–æ: —É–ª—É—á—à–µ–Ω–∏–µ –∞–Ω–∏–º–∞—Ü–∏–∏ –ª—é–¥–µ–π —á–µ—Ä–µ–∑ –æ–ø–µ—Ä–µ–∂–∞—é—â–µ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Lookahead Anchoring –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–Ω–∏–º–∞—Ü–∏–∏ –ª—é–¥–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ. –í–º–µ—Å—Ç–æ –≥–µ
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#robotics", "#games", "#agents", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–ü–ª–∞–≤–Ω—ã–µ –¥–≤–∏–∂–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Action Coherence Guidance (ACG) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –≤ Vision-Language-Action (VLA) –º–æ–¥–µ–ª—è—Ö 
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#benchmark", "#games", "#interpretability", "#optimization"], "emoji": "üîç", "ru": {"title": "–§–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Ç–æ—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç–Ω—ã–π –≤–µ–±-–ø–æ–∏—Å–∫", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ORIG ‚Äî –∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Ç–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#benchmark", "#reasoning", "#optimization", "#rag"], "emoji": "üîÑ", "ru": {"title": "–û–¥–Ω–∞ embedding-–º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç E^2Rank ‚Äî –µ–¥–∏–Ω—É—éÊ°ÜÊû∂, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç–æ–≤–æ–π embedding-–º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–∞–∫ –ø–æ–∏—Å–∫–∞, —Ç–∞–∫ –∏ list
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#dataset", "#games", "#optimization", "#3d"], "emoji": "üéØ", "ru": {"title": "–ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è 3D-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Å—Ü–µ–Ω–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ IGGT ‚Äî —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—é 3D-—Å—Ü–µ–Ω—ã –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞—ë—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –æ–±—ä–µ–∫
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#data", "#multimodal", "#benchmark", "#dataset", "#alignment", "#architecture"], "emoji": "üéÅ", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Omni-Reward ‚Äî —Å–∏—Å—Ç–µ–º—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è reward model
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#architecture", "#optimization", "#training"], "emoji": "ü§ù", "ru": {"title": "–ì–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è —É—á–∞—Ç—Å—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º knocking-heads attention (KHA), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π multi-head attention, –ø–æ–∑–≤–æ–ª—è—è –≥–æ–ª–æ–≤–∞–º –≤–Ω–∏–º–∞–Ω–∏
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#games", "#dataset", "#optimization", "#reasoning", "#training"], "emoji": "üéØ", "ru": {"title": "–¢–æ—á–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM", "desc": "PixelRefer ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#rlhf", "#training", "#reasoning", "#rl", "#optimization"], "emoji": "üéØ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è LLM –¥–ª—è Best-of-N —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RLVR) —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–µ—à–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#dataset", "#multimodal"], "emoji": "üîó", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∏–∑ –≥–æ—Ç–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± —Å–æ–∑–¥–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø—É—Ç—ë–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#open_source", "#video", "#rlhf", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å –µ–¥–∏–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π", "desc": "LongCat-Video ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ Diffusion Transformer —Å 13.6 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∞—è—Å—è –Ω–∞ 
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#data", "#rag", "#open_source", "#benchmark", "#dataset", "#synthetic", "#reasoning", "#training"], "emoji": "üéØ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LIMRANK-SYNTHESIZER ‚Äî pipeline –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#training", "#rlhf", "#optimization", "#architecture", "#open_source"], "emoji": "üé®", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ LLM —Å–æ–∑–¥–∞–≤–∞—Ç—å –∫—Ä–∞—Å–∏–≤—ã–π –∫–æ–¥ —Å –ø–æ–º–æ—â—å—é –∞–≥–µ–Ω—Ç–æ–≤-–æ—Ü–µ–Ω—â–∏–∫–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç—Å—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#optimization", "#cv", "#training"], "emoji": "‚ö°", "ru": {"title": "–û–¥–Ω–æ—à–∞–≥–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é score-—Ñ—É–Ω–∫—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Distilled Decoding 2 (DD2) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#robotics", "#benchmark", "#agents", "#games", "#optimization"], "emoji": "ü§ñ", "ru": {"title": "–°–∏–º—É–ª—è—Ü–∏—è —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–æ–±–æ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–∏—Ç–∏–∫", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–æ–±–æ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–∏—Ç–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –æ—Ü–µ–Ω–∫—É VLA
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#training", "#multimodal", "#optimization", "#alignment"], "emoji": "üé®", "ru": {"title": "MergeMix: —É–º–Ω–æ–µ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MergeMix ‚Äî –º–µ—Ç–æ–¥ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#science", "#dataset", "#benchmark", "#optimization", "#3d"], "emoji": "üßä", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏–∑ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞", "desc": "VoMP ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ–±—ä—ë–º–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ (–º–æ–¥—É
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#hallucinations", "#agents", "#optimization", "#plp", "#training"], "emoji": "üîß", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã –≤–º–µ—Å—Ç–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π: language servers –∫–∞–∫ –Ω–∞–≥—Ä–∞–¥–∞ –¥–ª—è coding-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Lanser-CLI ‚Äî –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è Language S
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#benchmark", "#interpretability", "#reasoning"], "emoji": "üîç", "ru": {"title": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–∏–∫–∏, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–æ–≤: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π AI", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω PRISM-Bench ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#optimization", "#diffusion", "#cv", "#benchmark", "#architecture"], "emoji": "üõ£Ô∏è", "ru": {"title": "–î–µ—Ç–µ–∫—Ü–∏—è –¥–æ—Ä–æ–∂–Ω—ã—Ö –ø–æ–ª–æ—Å —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ —à—É–º–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤", "desc": "DiffusionLane - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–æ—Ä–æ–∂–Ω—ã—Ö –ø–æ–ª–æ—Å, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#training", "#diffusion", "#optimization"], "emoji": "üîÑ", "ru": {"title": "–î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∑–∞ –æ–¥–∏–Ω —à–∞–≥", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ EchoDistill –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#security", "#data", "#optimization", "#dataset", "#benchmark"], "emoji": "üîç", "ru": {"title": "–°—Ç–µ–ø–µ–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–∏–ø—Ñ–µ–π–∫–æ–≤", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑—É—á–∏–ª–∏ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∑–∞–¥–∞—á–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–∏–ø—Ñ–µ–π–∫–æ–≤, —Å–æ–∑–¥–∞–≤ –∫—Ä—É–ø–Ω–µ–π—à–∏–π –¥–∞—Ç–∞—Å–µ—Ç ScaleDF —Å 5.8 –º–ª–Ω —Ä–µ–∞–ª
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#cv", "#diffusion", "#games", "#3d"], "emoji": "üßç", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ 2D –∏ 3D –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞", "desc": "SyncHuman - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π —á–µ–ª–æ–≤–µ–∫–∞ –ø–æ –æ–¥–Ω–æ–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –∫–∏–Ω–æ –∏ –∏–≥—Ä. –ú–µ—Ç
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#optimization", "#3d"], "emoji": "üéØ", "ru": {"title": "TIRE: –¢—Ä–µ–∫–∏–Ω–≥, –∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥ –∏ resplatting –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –≤ 3D", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ TIRE –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D/4D –æ–±—ä–µ–∫—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ —Å—É–±—ä–µ–∫—Ç–∞. –ú–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞: –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –Ω—É–∂–Ω—ã—Ö
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#training", "#optimization", "#benchmark", "#math"], "emoji": "üéØ", "ru": {"title": "–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –º–µ—Ç–æ–¥ PIPS, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#training", "#optimization", "#multimodal", "#audio", "#interpretability"], "emoji": "üé§", "ru": {"title": "–£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ ¬´—Ç–æ–∫–µ–Ω–æ–≤-–º–∞–≥–Ω–∏—Ç–æ–≤¬ª –≤–Ω–∏–º–∞–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ —Ä–µ—á–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –≤–ø–µ—Ä–≤—ã–µ –∏–∑—É—á–∏–ª–∏ —Ñ–µ–Ω–æ–º–µ–Ω attention sinks (—Ç–æ–∫–µ–Ω–æ–≤, –ø—Ä–∏—Ç—è–≥–∏–≤–∞—é—â–∏—Ö –Ω–µ–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#optimization", "#interpretability", "#training", "#inference", "#data", "#architecture"], "emoji": "üå±", "ru": {"title": "–Ø–∑—ã–∫–æ–≤–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞–º—è—Ç–∏ –∫–∞–∫ —ç–∫–æ–ª–æ–≥–∏—á–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —è–∑—ã–∫–æ–≤–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞–º—è—Ç–∏ –∫–∞–∫ —ç
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#training", "#diffusion", "#cv", "#optimization"], "emoji": "üéØ", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤", "desc": "FlowOpt ‚Äî —ç—Ç–æ framework –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ flow-matching –º–æ–¥–µ–ª–µ–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è backpropagation. –ú–µ—Ç–æ–¥ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≤–µ
[28.10.2025 18:18] Using data from previous issue: {"categories": ["#architecture", "#benchmark", "#optimization", "#training"], "emoji": "üöÄ", "ru": {"title": "MARS-M: —É—Å–∫–æ—Ä–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –º–∞—Ç—Ä–∏—á–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞–Ω–∏–µ –∏ —É–º–µ–Ω—å—à–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MARS-M ‚Äî –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –∫–æ—Ç–æ—Ä
[28.10.2025 18:18] Renaming data file.
[28.10.2025 18:18] Renaming previous data. hf_papers.json to ./d/2025-10-28.json
[28.10.2025 18:18] Saving new data file.
[28.10.2025 18:18] Generating page.
[28.10.2025 18:18] Renaming previous page.
[28.10.2025 18:18] Renaming previous data. index.html to ./d/2025-10-28.html
[28.10.2025 18:18] Writing result.
[28.10.2025 18:18] Renaming log file.
[28.10.2025 18:18] Renaming previous data. log.txt to ./logs/2025-10-28_last_log.txt
