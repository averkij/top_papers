[20.06.2025 18:15] Read previous papers.
[20.06.2025 18:15] Generating top page (month).
[20.06.2025 18:15] Writing top page (month).
[20.06.2025 19:09] Read previous papers.
[20.06.2025 19:09] Get feed.
[20.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14965
[20.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.09827
[20.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15154
[20.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.14837
[20.06.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2506.15455
[20.06.2025 19:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[20.06.2025 19:09] No deleted papers detected.
[20.06.2025 19:09] Downloading and parsing papers (pdf, html). Total: 5.
[20.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.14965.
[20.06.2025 19:09] Extra JSON file exists (./assets/json/2506.14965.json), skip PDF parsing.
[20.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.14965.json), skip HTML parsing.
[20.06.2025 19:09] Success.
[20.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.09827.
[20.06.2025 19:09] Extra JSON file exists (./assets/json/2506.09827.json), skip PDF parsing.
[20.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.09827.json), skip HTML parsing.
[20.06.2025 19:09] Success.
[20.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.15154.
[20.06.2025 19:09] Extra JSON file exists (./assets/json/2506.15154.json), skip PDF parsing.
[20.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.15154.json), skip HTML parsing.
[20.06.2025 19:09] Success.
[20.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.14837.
[20.06.2025 19:09] Extra JSON file exists (./assets/json/2506.14837.json), skip PDF parsing.
[20.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.14837.json), skip HTML parsing.
[20.06.2025 19:09] Success.
[20.06.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2506.15455.
[20.06.2025 19:09] Downloading paper 2506.15455 from http://arxiv.org/pdf/2506.15455v1...
[20.06.2025 19:09] Extracting affiliations from text.
[20.06.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 5 5 4 5 1 . 6 0 5 2 : r RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation Xinnuo Xu * 1 Rachel Lawrence * 1 Kshitij Dubey * 2 Atharva Pandey * 2 Risa Ueno 1 Fabian Falck 1 Aditya V. Nori 1 Rahul Sharma 2 Amit Sharma 2 Javier Gonzalez "
[20.06.2025 19:09] Response: ```python
[]
```
[20.06.2025 19:09] Extracting affiliations from text.
[20.06.2025 19:09] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 8 1 ] . [ 1 5 5 4 5 1 . 6 0 5 2 : r RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation Xinnuo Xu * 1 Rachel Lawrence * 1 Kshitij Dubey * 2 Atharva Pandey * 2 Risa Ueno 1 Fabian Falck 1 Aditya V. Nori 1 Rahul Sharma 2 Amit Sharma 2 Javier GonzalezRecent Large Language Models (LLMs) have reported high accuracy on reasoning benchmarks. However, it is still unclear whether the observed results arise from true reasoning or from statistical recall of the training set. Inspired by the ladder of causation (Pearl, 2009) and its three levels (associations, interventions and counterfactuals), this paper introduces RE-IMAGINE: framework to characterize hierarchy of reasoning ability in LLMs, alongside scalable pipeline to generate problem variations across all the levels of the hierarchy. By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone. The framework is general and can work across reasoning domains, including math, code, and logic. We demonstrate the type of insights that RE-IMAGINE can generate on four widely-used benchmarks, which we use to evaluate reasoning on several families of LLMs. We observe reductions in performance when the models are queried with problem variations. These assessments indicate degree of reliance on statistical recall for past performance, and open the door to further research targeting skills across the reasoning hierarchy. 1. Introduction Recent advancements in Artificial Intelligence (AI) have sparked increasing interest in the development of reasoning systems. Central to this goal are Large Language Models (LLMs) models like OpenAIs o1 (Jaech et al., 2024), o3 (OpenAI, 2024), or DeepSeek-R1 (Team, 2025) show complex problem-solving abilities, and demonstrate unprece- *Equal contribution 1Microsoft Research Cambridge, UK 2Microsoft Research India, Correspondence to: Xinnuo Xu <xinnuoxu@microsoft.com>, Rachel Lawrence <rachel.lawrence@microsoft.com>. India. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 dented results on reasoning benchmarks, e.g. FrontierMath (Glazer et al., 2024) and ARC-AGI (Chollet et al., 2024). With growing reliance on LLMs across wide-ranging applications, it is increasingly important to clarify the strengths and limitations of these apparent reasoning abilities. Reasoning is cognitive process. It involves using facts or premises to make inferences about conclusions or judgments (Holyoak & Morrison, 2005). In the realm of LLMs and AI, reasoning is understood to be the ability of model to demonstrate logically correct systematic processes that surpass mere statistical pattern recognition in the training set (Gonzalez & Nori, 2024). Traditionally, the evaluation of reasoning in LLMs has been focused on their performance across fixed benchmarks in domains such as math (Cobbe et al., 2021), programming (Wu et al., 2024; Gu et al., 2024), real-world logic (Jin et al., 2023a) and others (Wang et al., 2019; Hendrycks et al., 2020; Srivastava et al., 2022). However, debate persists on whether the observed results occur from genuine reasoning or from mere statistical recall of training data (Mitchell & Krakauer, 2023) particularly for training data which, in the case of published benchmarks, may have information leakage from the test set (Zhou et al., 2023). Defining principled ways to make this distinction is crucial for advancing AI and controlling potential hazards and risks (Weidinger et al., 2021). Recently, several surveys have explored how to evaluate reasoning beyond memorization in LLMs (Xu et al., 2025; Huang & Chang, 2023). In general, two main approaches have emerged. One aims to develop novel reasoning tasks such as mystery blocksworld (Webb et al., 2024), ARCAGI (Chollet et al., 2025), and others (Zhu et al., 2023). An alternative is to create novel variations of existing benchmarks, e.g. for math (Mirzadeh et al., 2024; Srivastava et al., 2024), analogies (Lewis & Mitchell, 2024), and diverse tasks across code, math, and logic (Wu et al., 2023; Zhang et al., 2024). common strategy for creating such variations involves leveraging symbolic representations of problems such as functional templates (Mirzadeh et al., 2024; Srivastava et al., 2024), reasoning or causal graphs (Gonzalez & Nori, 2024; Huyuk et al., 2024; Yang et al., 2024), planning tasks (Valmeekam et al., 2022) or code (Li et al., 2024). RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation Level Description Examples Evaluation metric(s) 1. Observe Original problem. 2. Mutate 3. Imagine Mutated problem by replacing or adding components. Original problem augmented with an imagine statement, modifying the original statements or assertions before it. Problems in GSM8K, CLadder, CRUXEval Loop Replacing numeric value, changing variable name, modifying operator, irrelevant information Extra logic involving revisions or counterfactual statements. Task performance (original). Task performance (original and mutated). Task performance (original and augmented) PN and PS (only in counterfactuals). References Cobbe et al. (2021) Jin et al. (2023a) Kamath et al. (2024) Gu et al. (2024) Mirzadeh et al. (2024) Srivastava et al. (2024) Wu et al. (2023) Lewis & Mitchell (2024) Gonzalez & Nori (2024) Huyuk et al. (2024) Table 1. Hierarchy of problem variations introduced in the RE-IMAGINE framework to evaluate LLMs. Level-1 (observe) captures the accuracy of LLMs to solve problems in existing benchmarks. These benchmarks may have been observed by the LLMs in current or similar form. Level-2 (mutate) captures the ability to solve problem variations. Level-3 (imagine) captures the ability to correctly incorporate new logic into existing problems, even when this logic contradicts the original components. In certain cases, the new logic can be understood as counterfactual statement (Pearl, 2009). Despite the introduction of various benchmark variations aimed at assessing LLMs reasoning abilities, these variations have been developed in an ad hoc manner, lacking systematic hierarchy. Moreover, most existing approaches rely on significant manual effort and are designed for specific tasks, making them difficult to scale across multiple benchmarks and tasks. For instance, in Mirzadeh et al. (2024); Srivastava et al. (2024), functional templates for simple math pr"
[20.06.2025 19:09] Mistral response. {"id": "e9ee8d372c374bca8105ecda9cc22a0a", "object": "chat.completion", "created": 1750446566, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n[\"Microsoft Research Cambridge, UK\", \"Microsoft Research India\"]\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1850, "total_tokens": 1869, "completion_tokens": 19}}
[20.06.2025 19:09] Response: ```python
["Microsoft Research Cambridge, UK", "Microsoft Research India"]
```
[20.06.2025 19:09] Deleting PDF ./assets/pdf/2506.15455.pdf.
[20.06.2025 19:09] Paper image links file exists (./assets/img_data/2506.15455.json), skip HTML parsing.
[20.06.2025 19:09] Success.
[20.06.2025 19:09] Enriching papers with extra data.
[20.06.2025 19:09] ********************************************************************************
[20.06.2025 19:09] Abstract 0. Guru, a diverse RL reasoning corpus, highlights domain-specific training needs and demonstrates improved performance in complex tasks for RL-enhanced LLMs.  					AI-generated summary 				 Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning,...
[20.06.2025 19:09] ********************************************************************************
[20.06.2025 19:09] Abstract 1. EmoNet-Voice, a new resource with large pre-training and benchmark datasets, advances speech emotion recognition by offering fine-grained emotion evaluation with synthetic, privacy-preserving audio.  					AI-generated summary 				 The advancement of text-to-speech and audio generation models necessi...
[20.06.2025 19:09] ********************************************************************************
[20.06.2025 19:09] Abstract 2. SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.  					AI-generated summary 				 Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases a...
[20.06.2025 19:09] ********************************************************************************
[20.06.2025 19:09] Abstract 3. ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.  					AI-generated summary 				 Recently, multimodal large language models (MLLMs) have attracted increasing research at...
[20.06.2025 19:09] ********************************************************************************
[20.06.2025 19:09] Abstract 4. RE-IMAGINE evaluates the reasoning abilities of Large Language Models by generating variations of problems that cannot be solved by memorization, indicating reliance on statistical recall.  					AI-generated summary 				 Recent Large Language Models (LLMs) have reported high accuracy on reasoning be...
[20.06.2025 19:09] Read previous papers.
[20.06.2025 19:09] Generating reviews via LLM API.
[20.06.2025 19:09] Using data from previous issue: {"categories": ["#training", "#rl", "#open_source", "#reasoning", "#dataset"], "emoji": "üß†", "ru": {"title": "Guru: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∫–æ—Ä–ø—É—Å Guru –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –±–æ–ª—å—à
[20.06.2025 19:09] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#science", "#synthetic", "#data", "#audio"], "emoji": "üé≠", "ru": {"title": "–¢–æ—á–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π –≤ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "EmoNet-Voice - —ç—Ç–æ –Ω–æ–≤—ã–π —Ä–µ—Å—É—Ä—Å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π –≤ —Ä–µ—á–∏, –≤–∫–ª—é—á–∞—é—â–∏–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è
[20.06.2025 19:09] Using data from previous issue: {"categories": ["#audio", "#games", "#multimodal", "#architecture", "#science", "#dataset", "#training"], "emoji": "üéµ", "ru": {"title": "SonicVerse: –ü–æ–Ω–∏–º–∞–Ω–∏–µ –º—É–∑—ã–∫–∏ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "SonicVerse - —ç—Ç–æ –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏–π –º—É–∑—ã–∫–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥–µ–Ω–µ—Ä–∞
[20.06.2025 19:09] Using data from previous issue: {"categories": ["#cv", "#interpretability", "#optimization", "#training", "#multimodal"], "emoji": "üìä", "ru": {"title": "–¢–æ—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É—Ç–æ—á–Ω–µ–Ω–∏—è", "desc": "ChartIR - —ç—Ç–æ –º–µ—Ç–æ–¥ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É—Ç–æ—á–Ω–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å
[20.06.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#training", "#math"], "emoji": "üß†", "ru": {"title": "–û—Ü–µ–Ω–∫–∞ –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∞—Ä–∏–∞—Ü–∏–π –∑–∞–¥–∞—á", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ RE-IMAGINE –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≥–µ–Ω–µ
[20.06.2025 19:09] Renaming data file.
[20.06.2025 19:09] Renaming previous data. hf_papers.json to ./d/2025-06-20.json
[20.06.2025 19:09] Saving new data file.
[20.06.2025 19:09] Generating page.
[20.06.2025 19:09] Renaming previous page.
[20.06.2025 19:09] Renaming previous data. index.html to ./d/2025-06-20.html
[20.06.2025 19:09] Writing result.
[20.06.2025 19:09] Renaming log file.
[20.06.2025 19:09] Renaming previous data. log.txt to ./logs/2025-06-20_last_log.txt
