[12.09.2025 04:14] Read previous papers.
[12.09.2025 04:14] Generating top page (month).
[12.09.2025 04:14] Writing top page (month).
[12.09.2025 05:11] Read previous papers.
[12.09.2025 05:11] Get feed.
[12.09.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.09674
[12.09.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.09174
[12.09.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.09265
[12.09.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.09680
[12.09.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.09595
[12.09.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.09676
[12.09.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.09286
[12.09.2025 05:11] Extract page data from URL. URL: https://huggingface.co/papers/2509.08519
[12.09.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.09118
[12.09.2025 05:11] Extract page data from URL. URL: https://huggingface.co/papers/2509.01964
[12.09.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.09614
[12.09.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.09332
[12.09.2025 05:11] Get page data from previous paper. URL: https://huggingface.co/papers/2509.07430
[12.09.2025 05:11] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[12.09.2025 05:11] No deleted papers detected.
[12.09.2025 05:11] Downloading and parsing papers (pdf, html). Total: 13.
[12.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.09674.
[12.09.2025 05:11] Extra JSON file exists (./assets/json/2509.09674.json), skip PDF parsing.
[12.09.2025 05:11] Paper image links file exists (./assets/img_data/2509.09674.json), skip HTML parsing.
[12.09.2025 05:11] Success.
[12.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.09174.
[12.09.2025 05:11] Extra JSON file exists (./assets/json/2509.09174.json), skip PDF parsing.
[12.09.2025 05:11] Paper image links file exists (./assets/img_data/2509.09174.json), skip HTML parsing.
[12.09.2025 05:11] Success.
[12.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.09265.
[12.09.2025 05:11] Extra JSON file exists (./assets/json/2509.09265.json), skip PDF parsing.
[12.09.2025 05:11] Paper image links file exists (./assets/img_data/2509.09265.json), skip HTML parsing.
[12.09.2025 05:11] Success.
[12.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.09680.
[12.09.2025 05:11] Extra JSON file exists (./assets/json/2509.09680.json), skip PDF parsing.
[12.09.2025 05:11] Paper image links file exists (./assets/img_data/2509.09680.json), skip HTML parsing.
[12.09.2025 05:11] Success.
[12.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.09595.
[12.09.2025 05:11] Extra JSON file exists (./assets/json/2509.09595.json), skip PDF parsing.
[12.09.2025 05:11] Paper image links file exists (./assets/img_data/2509.09595.json), skip HTML parsing.
[12.09.2025 05:11] Success.
[12.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.09676.
[12.09.2025 05:11] Extra JSON file exists (./assets/json/2509.09676.json), skip PDF parsing.
[12.09.2025 05:11] Paper image links file exists (./assets/img_data/2509.09676.json), skip HTML parsing.
[12.09.2025 05:11] Success.
[12.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.09286.
[12.09.2025 05:11] Extra JSON file exists (./assets/json/2509.09286.json), skip PDF parsing.
[12.09.2025 05:11] Paper image links file exists (./assets/img_data/2509.09286.json), skip HTML parsing.
[12.09.2025 05:11] Success.
[12.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.08519.
[12.09.2025 05:11] Downloading paper 2509.08519 from http://arxiv.org/pdf/2509.08519v1...
[12.09.2025 05:11] Extracting affiliations from text.
[12.09.2025 05:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning Liyang Chen1,, Tianxiang Ma2,, Jiawei Liu2, Bingchuan Li2,, Zhuowei Chen2, Lijie Liu2, Xu He1, Gen Li2, Qian He2, Zhiyong Wu1, 1Tsinghua University, 2Intelligent Creation Lab, ByteDance Equal contribution, Project lead, Corresponding author "
[12.09.2025 05:11] Response: ```python
["Tsinghua University", "Intelligent Creation Lab, ByteDance"]
```
[12.09.2025 05:11] Deleting PDF ./assets/pdf/2509.08519.pdf.
[12.09.2025 05:11] Success.
[12.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.09118.
[12.09.2025 05:11] Extra JSON file exists (./assets/json/2509.09118.json), skip PDF parsing.
[12.09.2025 05:11] Paper image links file exists (./assets/img_data/2509.09118.json), skip HTML parsing.
[12.09.2025 05:11] Success.
[12.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.01964.
[12.09.2025 05:11] Downloading paper 2509.01964 from http://arxiv.org/pdf/2509.01964v1...
[12.09.2025 05:11] Extracting affiliations from text.
[12.09.2025 05:11] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2D Gaussian Splatting with Semantic Alignment for Image Inpainting Hongyu Li1, Chaofeng Chen2, Xiaoming Li3, Guangming Lu1 1Harbin Institute of Technology, Shenzhen 2School of Artificial Intelligence, Wuhan University 3Nanyang Technological University https://github.com/hitlhy715/2DGS-inpaint 5 2 0 2 2 ] . [ 1 4 6 9 1 0 . 9 0 5 2 : r Abstract Gaussian Splatting (GS), recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into continuous field of 2D Gaussian splat coefficients and reconstructs the final image via differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from pretrained DINO model. We observe that DINOs global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing new direction for applying Gaussian Splatting to 2D image processing. Human visual perception naturally interprets the world as continuous experience. In contrast, digital images are constrained by hardware and storage limitations, resulting in representations composed of discr"
[12.09.2025 05:11] Response: ```python
[
    "Harbin Institute of Technology, Shenzhen",
    "School of Artificial Intelligence, Wuhan University",
    "Nanyang Technological University"
]
```
[12.09.2025 05:11] Deleting PDF ./assets/pdf/2509.01964.pdf.
[12.09.2025 05:11] Success.
[12.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.09614.
[12.09.2025 05:11] Extra JSON file exists (./assets/json/2509.09614.json), skip PDF parsing.
[12.09.2025 05:11] Paper image links file exists (./assets/img_data/2509.09614.json), skip HTML parsing.
[12.09.2025 05:11] Success.
[12.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.09332.
[12.09.2025 05:11] Extra JSON file exists (./assets/json/2509.09332.json), skip PDF parsing.
[12.09.2025 05:11] Paper image links file exists (./assets/img_data/2509.09332.json), skip HTML parsing.
[12.09.2025 05:11] Success.
[12.09.2025 05:11] Downloading and parsing paper https://huggingface.co/papers/2509.07430.
[12.09.2025 05:11] Extra JSON file exists (./assets/json/2509.07430.json), skip PDF parsing.
[12.09.2025 05:11] Paper image links file exists (./assets/img_data/2509.07430.json), skip HTML parsing.
[12.09.2025 05:11] Success.
[12.09.2025 05:11] Enriching papers with extra data.
[12.09.2025 05:11] ********************************************************************************
[12.09.2025 05:11] Abstract 0. SimpleVLA-RL, an RL framework for VLA models, enhances long-horizon action planning, achieves state-of-the-art performance, and discovers novel patterns during training.  					AI-generated summary 				 Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manip...
[12.09.2025 05:11] ********************************************************************************
[12.09.2025 05:11] Abstract 1. EchoX, a speech-to-speech large language model, addresses the acoustic-semantic gap by integrating semantic representations, preserving reasoning abilities, and achieving advanced performance on knowledge-based benchmarks.  					AI-generated summary 				 Speech-to-speech large language models (SLLMs...
[12.09.2025 05:11] ********************************************************************************
[12.09.2025 05:11] Abstract 2. Entropy-Modulated Policy Gradients (EMPG) addresses learning dynamics issues in LLMs by recalibrating policy gradients based on uncertainty and task outcomes, leading to improved performance in long-horizon tasks.  					AI-generated summary 				 In long-horizon tasks, recent agents based on Large La...
[12.09.2025 05:11] ********************************************************************************
[12.09.2025 05:11] Abstract 3. FLUX-Reason-6M and PRISM-Bench address the lack of reasoning-focused datasets and benchmarks for text-to-image models, providing a large-scale dataset and evaluation standard to improve model performance.  					AI-generated summary 				 The advancement of open-source text-to-image (T2I) models has b...
[12.09.2025 05:11] ********************************************************************************
[12.09.2025 05:11] Abstract 4. Kling-Avatar, a cascaded framework, enhances audio-driven avatar video generation by integrating multimodal instruction understanding with photorealistic portrait generation, resulting in high-fidelity, semantically grounded videos.  					AI-generated summary 				 Recent advances in audio-driven ava...
[12.09.2025 05:11] ********************************************************************************
[12.09.2025 05:11] Abstract 5. SpatialVID, a large-scale dataset with diverse videos and dense 3D annotations, enhances model generalization and performance in video and 3D vision research.  					AI-generated summary 				 Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world ex...
[12.09.2025 05:11] ********************************************************************************
[12.09.2025 05:11] Abstract 6. VLMs are enhanced with an adaptive framework that selects between code-based and direct visual reasoning for chart understanding, improving performance and robustness.  					AI-generated summary 				 Chart understanding presents a critical test to the reasoning capabilities of Vision-Language Models...
[12.09.2025 05:11] ********************************************************************************
[12.09.2025 05:11] Abstract 7. HuMo is a unified framework for human-centric video generation that addresses challenges in multimodal control through a two-stage training paradigm and novel strategies for subject preservation and audio-visual synchronization.  					AI-generated summary 				 Human-Centric Video Generation (HCVG) m...
[12.09.2025 05:11] ********************************************************************************
[12.09.2025 05:11] Abstract 8. GA-DMS framework enhances CLIP for person representation learning by improving data quality and model architecture, achieving state-of-the-art performance.  					AI-generated summary 				 Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks,...
[12.09.2025 05:11] ********************************************************************************
[12.09.2025 05:11] Abstract 9. A novel image inpainting framework using 2D Gaussian Splatting achieves competitive performance by combining continuous field representation with pretrained DINO model features for global semantic consistency.  					AI-generated summary 				 Gaussian Splatting (GS), a recent technique for converting...
[12.09.2025 05:11] ********************************************************************************
[12.09.2025 05:11] Abstract 10. LoCoBench evaluates long-context language models in complex software development scenarios, addressing the gap in understanding entire codebases and maintaining architectural consistency across large-scale systems.  					AI-generated summary 				 The emergence of long-context language models with co...
[12.09.2025 05:11] ********************************************************************************
[12.09.2025 05:11] Abstract 11. OmniEVA addresses spatial and embodiment gaps in multimodal large language models for embodied intelligence through a task-adaptive 3D grounding mechanism and an embodiment-aware reasoning framework, achieving state-of-the-art performance across diverse tasks.  					AI-generated summary 				 Recent ...
[12.09.2025 05:11] ********************************************************************************
[12.09.2025 05:11] Abstract 12. A new framework, DPH-RL, uses mass-covering f-divergences to address Pass@k degradation and catastrophic forgetting in fine-tuning LLMs with RLVR, improving both Pass@1 and Pass@k.  					AI-generated summary 				 A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learni...
[12.09.2025 05:11] Read previous papers.
[12.09.2025 05:11] Generating reviews via LLM API.
[12.09.2025 05:11] Using data from previous issue: {"categories": ["#agents", "#optimization", "#rl", "#robotics", "#reasoning"], "emoji": "🤖", "ru": {"title": "Обучение с подкреплением открывает новые горизонты для роботов", "desc": "SimpleVLA-RL - это фреймворк для обучения с подкреплением, разработанный для моделей типа Vision-Language-Action (VL
[12.09.2025 05:11] Using data from previous issue: {"categories": ["#science", "#reasoning", "#benchmark", "#audio", "#dataset"], "emoji": "🗣️", "ru": {"title": "Преодоление разрыва между звуком и смыслом в речевых ИИ-моделях", "desc": "EchoX - это речевая большая языковая модель, которая решает проблему акустико-семантического разрыва. Она интегрир
[12.09.2025 05:11] Using data from previous issue: {"categories": ["#agents", "#optimization", "#rl", "#training", "#rlhf"], "emoji": "🧠", "ru": {"title": "Улучшение обучения языковых моделей через энтропийную модуляцию градиентов", "desc": "Метод EMPG (Entropy-Modulated Policy Gradients) решает проблемы динамики обучения в больших языковых моделях 
[12.09.2025 05:11] Using data from previous issue: {"categories": ["#multilingual", "#benchmark", "#open_source", "#long_context", "#dataset", "#reasoning"], "emoji": "🧠", "ru": {"title": "Революция в обучении и оценке моделей текст-изображение", "desc": "FLUX-Reason-6M и PRISM-Bench решают проблему отсутствия наборов данных и эталонов для оценки мо
[12.09.2025 05:11] Using data from previous issue: {"categories": ["#multimodal", "#benchmark", "#story_generation", "#video", "#games"], "emoji": "🎭", "ru": {"title": "Семантически обоснованные аватары с высокой детализацией", "desc": "Kling-Avatar - это новая каскадная архитектура для генерации видео с аватарами на основе аудио. Она объединяет пон
[12.09.2025 05:11] Using data from previous issue: {"categories": ["#3d", "#dataset", "#video"], "emoji": "🎥", "ru": {"title": "SpatialVID: Большие данные для прорыва в пространственном интеллекте", "desc": "SpatialVID - это масштабный набор данных, содержащий разнообразные видео с плотными 3D-аннотациями. Он включает более 21 000 часов необработанн
[12.09.2025 05:11] Using data from previous issue: {"categories": ["#reasoning", "#rl", "#benchmark", "#multimodal", "#training", "#hallucinations"], "emoji": "📊", "ru": {"title": "Адаптивное рассуждение VLM для улучшенного понимания графиков", "desc": "Статья представляет адаптивный фреймворк для улучшения понимания графиков визуально-языковыми мод
[12.09.2025 05:11] Querying the API.
[12.09.2025 05:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

HuMo is a unified framework for human-centric video generation that addresses challenges in multimodal control through a two-stage training paradigm and novel strategies for subject preservation and audio-visual synchronization.  					AI-generated summary 				 Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: https://phantom-video.github.io/HuMo.
[12.09.2025 05:11] Response: {
  "desc": "HuMo - это унифицированная система для генерации видео с людьми, учитывающая несколько модальностей входных данных. Она использует двухэтапную парадигму обучения и новые стратегии для сохранения характеристик субъекта и синхронизации аудио с видео. Система решает проблемы нехватки обучающих данных с парными условиями и сложности координации подзадач с мультимодальными входами. HuMo превосходит специализированные современные методы в подзадачах, создавая единую систему для совместной мультимодальной генерации видео с людьми.",
  "emoji": "🎭",
  "title": "HuMo: Революция в генерации видео с людьми через мультимодальное управление"
}
[12.09.2025 05:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HuMo is a unified framework for human-centric video generation that addresses challenges in multimodal control through a two-stage training paradigm and novel strategies for subject preservation and audio-visual synchronization.  					AI-generated summary 				 Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: https://phantom-video.github.io/HuMo."

[12.09.2025 05:11] Response: ```python
['DATASET', 'MULTIMODAL', 'VIDEO', 'TRAINING']
```
[12.09.2025 05:11] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HuMo is a unified framework for human-centric video generation that addresses challenges in multimodal control through a two-stage training paradigm and novel strategies for subject preservation and audio-visual synchronization.  					AI-generated summary 				 Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: https://phantom-video.github.io/HuMo."

[12.09.2025 05:11] Response: ```python
[]
```
[12.09.2025 05:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HuMo is a novel framework designed for generating human-centric videos by effectively integrating multiple input modalities such as text, images, and audio. It tackles the challenges of limited training data and the need for precise coordination between subject preservation and audio-visual synchronization. The framework employs a two-stage training approach, utilizing a high-quality dataset and innovative strategies like minimal-invasive image injection and focus-by-predicting for improved task performance. HuMo demonstrates superior capabilities in multimodal control, outperforming existing methods in generating coherent and synchronized human videos.","title":"HuMo: Unifying Multimodal Control for Human-Centric Video Generation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HuMo is a novel framework designed for generating human-centric videos by effectively integrating multiple input modalities such as text, images, and audio. It tackles the challenges of limited training data and the need for precise coordination between subject preservation and audio-visual synchronization. The framework employs a two-stage training approach, utilizing a high-quality dataset and innovative strategies like minimal-invasive image injection and focus-by-predicting for improved task performance. HuMo demonstrates superior capabilities in multimodal control, outperforming existing methods in generating coherent and synchronized human videos.', title='HuMo: Unifying Multimodal Control for Human-Centric Video Generation'))
[12.09.2025 05:11] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HuMo是一个统一的人本视频生成框架，旨在通过两阶段训练范式和新颖的策略解决多模态控制中的挑战。该框架能够从文本、图像和音频等多种输入中合成视频，克服了训练数据稀缺和多模态输入下的任务协作难题。为了解决这些问题，HuMo构建了一个高质量的数据集，并提出了渐进式的多模态训练方法。实验结果表明，HuMo在子任务上超越了现有的最先进方法，建立了一个协作的多模态条件视频生成框架。","title":"HuMo：人本视频生成的统一框架"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HuMo是一个统一的人本视频生成框架，旨在通过两阶段训练范式和新颖的策略解决多模态控制中的挑战。该框架能够从文本、图像和音频等多种输入中合成视频，克服了训练数据稀缺和多模态输入下的任务协作难题。为了解决这些问题，HuMo构建了一个高质量的数据集，并提出了渐进式的多模态训练方法。实验结果表明，HuMo在子任务上超越了现有的最先进方法，建立了一个协作的多模态条件视频生成框架。', title='HuMo：人本视频生成的统一框架'))
[12.09.2025 05:11] Using data from previous issue: {"categories": ["#data", "#optimization", "#benchmark", "#transfer_learning", "#dataset", "#architecture"], "emoji": "👤", "ru": {"title": "Улучшение CLIP для точного распознавания людей", "desc": "Статья представляет GA-DMS фреймворк для улучшения CLIP в задаче обучения представлений людей. Авторы р
[12.09.2025 05:11] Querying the API.
[12.09.2025 05:11] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel image inpainting framework using 2D Gaussian Splatting achieves competitive performance by combining continuous field representation with pretrained DINO model features for global semantic consistency.  					AI-generated summary 				 Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing.
[12.09.2025 05:12] Response: {
  "desc": "Новый подход к дорисовке изображений использует технологию 2D Gaussian Splatting, которая преобразует дискретные точки в непрерывные пространственные представления. Метод комбинирует непрерывное представление поля с предобученными признаками модели DINO для обеспечения глобальной семантической согласованности. Предложенный фреймворк кодирует неполные изображения в непрерывное поле коэффициентов 2D гауссовых сплатов и реконструирует финальное изображение через дифференцируемый процесс растеризации. Эксперименты показывают, что метод достигает конкурентоспособных результатов как по количественным метрикам, так и по визуальному качеству.",
  "emoji": "🖌️",
  "title": "Дорисовка изображений с помощью 2D Gaussian Splatting и семантических признаков"
}
[12.09.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel image inpainting framework using 2D Gaussian Splatting achieves competitive performance by combining continuous field representation with pretrained DINO model features for global semantic consistency.  					AI-generated summary 				 Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing."

[12.09.2025 05:12] Response: ```python
['CV', 'INFERENCE', 'BENCHMARK']
```
[12.09.2025 05:12] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel image inpainting framework using 2D Gaussian Splatting achieves competitive performance by combining continuous field representation with pretrained DINO model features for global semantic consistency.  					AI-generated summary 				 Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing."

[12.09.2025 05:12] Response: ```python
[]
```
[12.09.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new framework for image inpainting using 2D Gaussian Splatting, which transforms incomplete images into continuous representations. By leveraging pretrained DINO model features, the framework ensures that the inpainted areas maintain global semantic consistency with the surrounding content. The method employs a differentiable rasterization process to reconstruct images, promoting pixel-level coherence in the results. Additionally, a patch-wise rasterization strategy is introduced to enhance efficiency and reduce memory usage, leading to competitive performance in both quantitative and perceptual evaluations.","title":"Revolutionizing Image Inpainting with 2D Gaussian Splatting"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents a new framework for image inpainting using 2D Gaussian Splatting, which transforms incomplete images into continuous representations. By leveraging pretrained DINO model features, the framework ensures that the inpainted areas maintain global semantic consistency with the surrounding content. The method employs a differentiable rasterization process to reconstruct images, promoting pixel-level coherence in the results. Additionally, a patch-wise rasterization strategy is introduced to enhance efficiency and reduce memory usage, leading to competitive performance in both quantitative and perceptual evaluations.', title='Revolutionizing Image Inpainting with 2D Gaussian Splatting'))
[12.09.2025 05:12] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文提出了一种新颖的图像修复框架，利用二维高斯点云（2D Gaussian Splatting）技术，结合预训练的DINO模型特征，以实现全局语义一致性。该框架将不完整的图像编码为二维高斯点系数的连续场，并通过可微分光栅化过程重建最终图像。高斯点云的连续渲染方式自然促进了修复结果的像素级一致性。通过引入基于补丁的光栅化策略，本文在提高效率和可扩展性的同时，确保修复内容与周围场景保持一致。","title":"高效图像修复的新方向：二维高斯点云技术"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文提出了一种新颖的图像修复框架，利用二维高斯点云（2D Gaussian Splatting）技术，结合预训练的DINO模型特征，以实现全局语义一致性。该框架将不完整的图像编码为二维高斯点系数的连续场，并通过可微分光栅化过程重建最终图像。高斯点云的连续渲染方式自然促进了修复结果的像素级一致性。通过引入基于补丁的光栅化策略，本文在提高效率和可扩展性的同时，确保修复内容与周围场景保持一致。', title='高效图像修复的新方向：二维高斯点云技术'))
[12.09.2025 05:12] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#long_context"], "emoji": "🧠", "ru": {"title": "LoCoBench: новый рубеж в оценке ИИ для разработки ПО", "desc": "LoCoBench - это новый бенчмарк для оценки языковых моделей с длинным контекстом в сложных сценариях разработки программного обеспечения. Он включ
[12.09.2025 05:12] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#benchmark", "#multimodal", "#games", "#3d"], "emoji": "🤖", "ru": {"title": "OmniEVA: Универсальный планировщик для воплощенного ИИ", "desc": "OmniEVA - это новая система для воплощенного искусственного интеллекта, которая решает проблемы пространственной ад
[12.09.2025 05:12] Using data from previous issue: {"categories": ["#training", "#reasoning", "#optimization", "#rl"], "emoji": "🧠", "ru": {"title": "Сохранение разнообразия знаний при обучении языковых моделей", "desc": "Статья представляет новый фреймворк DPH-RL для улучшения обучения больших языковых моделей с помощью обучения с подкреплением. DP
[12.09.2025 05:12] Renaming data file.
[12.09.2025 05:12] Renaming previous data. hf_papers.json to ./d/2025-09-12.json
[12.09.2025 05:12] Saving new data file.
[12.09.2025 05:12] Generating page.
[12.09.2025 05:12] Renaming previous page.
[12.09.2025 05:12] Renaming previous data. index.html to ./d/2025-09-12.html
[12.09.2025 05:12] Writing result.
[12.09.2025 05:12] Renaming log file.
[12.09.2025 05:12] Renaming previous data. log.txt to ./logs/2025-09-12_last_log.txt
