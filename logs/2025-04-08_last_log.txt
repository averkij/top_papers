[08.04.2025 05:13] Read previous papers.
[08.04.2025 05:13] Generating top page (month).
[08.04.2025 05:13] Writing top page (month).
[08.04.2025 06:15] Read previous papers.
[08.04.2025 06:15] Get feed.
[08.04.2025 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05298
[08.04.2025 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05305
[08.04.2025 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.02828
[08.04.2025 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05288
[08.04.2025 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.04823
[08.04.2025 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.05304
[08.04.2025 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.04715
[08.04.2025 06:15] Extract page data from URL. URL: https://huggingface.co/papers/2504.02812
[08.04.2025 06:15] Get page data from previous paper. URL: https://huggingface.co/papers/2504.03770
[08.04.2025 06:15] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[08.04.2025 06:15] No deleted papers detected.
[08.04.2025 06:15] Downloading and parsing papers (pdf, html). Total: 9.
[08.04.2025 06:15] Downloading and parsing paper https://huggingface.co/papers/2504.05298.
[08.04.2025 06:15] Extra JSON file exists (./assets/json/2504.05298.json), skip PDF parsing.
[08.04.2025 06:15] Paper image links file exists (./assets/img_data/2504.05298.json), skip HTML parsing.
[08.04.2025 06:15] Success.
[08.04.2025 06:15] Downloading and parsing paper https://huggingface.co/papers/2504.05305.
[08.04.2025 06:15] Extra JSON file exists (./assets/json/2504.05305.json), skip PDF parsing.
[08.04.2025 06:15] Paper image links file exists (./assets/img_data/2504.05305.json), skip HTML parsing.
[08.04.2025 06:15] Success.
[08.04.2025 06:15] Downloading and parsing paper https://huggingface.co/papers/2504.02828.
[08.04.2025 06:15] Extra JSON file exists (./assets/json/2504.02828.json), skip PDF parsing.
[08.04.2025 06:15] Paper image links file exists (./assets/img_data/2504.02828.json), skip HTML parsing.
[08.04.2025 06:15] Success.
[08.04.2025 06:15] Downloading and parsing paper https://huggingface.co/papers/2504.05288.
[08.04.2025 06:15] Extra JSON file exists (./assets/json/2504.05288.json), skip PDF parsing.
[08.04.2025 06:15] Paper image links file exists (./assets/img_data/2504.05288.json), skip HTML parsing.
[08.04.2025 06:15] Success.
[08.04.2025 06:15] Downloading and parsing paper https://huggingface.co/papers/2504.04823.
[08.04.2025 06:15] Extra JSON file exists (./assets/json/2504.04823.json), skip PDF parsing.
[08.04.2025 06:15] Paper image links file exists (./assets/img_data/2504.04823.json), skip HTML parsing.
[08.04.2025 06:15] Success.
[08.04.2025 06:15] Downloading and parsing paper https://huggingface.co/papers/2504.05304.
[08.04.2025 06:15] Extra JSON file exists (./assets/json/2504.05304.json), skip PDF parsing.
[08.04.2025 06:15] Paper image links file exists (./assets/img_data/2504.05304.json), skip HTML parsing.
[08.04.2025 06:15] Success.
[08.04.2025 06:15] Downloading and parsing paper https://huggingface.co/papers/2504.04715.
[08.04.2025 06:15] Extra JSON file exists (./assets/json/2504.04715.json), skip PDF parsing.
[08.04.2025 06:15] Paper image links file exists (./assets/img_data/2504.04715.json), skip HTML parsing.
[08.04.2025 06:15] Success.
[08.04.2025 06:15] Downloading and parsing paper https://huggingface.co/papers/2504.02812.
[08.04.2025 06:15] Downloading paper 2504.02812 from http://arxiv.org/pdf/2504.02812v1...
[08.04.2025 06:15] Extracting affiliations from text.
[08.04.2025 06:15] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 3 ] . [ 1 2 1 8 2 0 . 4 0 5 2 : r BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation Van Nguyen Nguyen1 Stephen Tyree2 Andrew Guo3 Mederic Fourmy4 Anas Gouda5 Taeyeop Lee6 Sungphill Moon7 Hyeontae Son7 Lukas Ranftl8,9 Jonathan Tremblay2 Eric Brachmann10 Bertram Drost8 Vincent Lepetit1 Carsten Rother11 Stan Birchfield2 Jiri Matas4 Yann Labbe13 Martin Sundermeyer12 Tomas Hodan13 1ENPC 2NVIDIA 3University of Toronto 4CTU Prague 5TU Dortmund 6KAIST 7NAVER LABS 8MVTec 9TU Munich 10Niantic 11Heidelberg University 12Google 13Meta HOT3D [1] HOPEv2 [41] HANDAL [12] Static/dynamic onboarding Fig. 1. New BOP-H3 datasets with object-onboarding sequences for model-free tasks. The first three columns show sample images from the new datasets, with the contour of 3D object models in the ground-truth poses drawn in green. The fourth column shows static (top) and dynamic (bottom) onboarding sequences, which are available in BOP-H3 and used for learning objects in the newly introduced model-free tasks. "
[08.04.2025 06:15] Response: ```python
[
    "ENPC",
    "NVIDIA",
    "University of Toronto",
    "CTU Prague",
    "TU Dortmund",
    "KAIST",
    "NAVER LABS",
    "MVTec",
    "TU Munich",
    "Niantic",
    "Heidelberg University",
    "Google",
    "Meta"
]
```
[08.04.2025 06:15] Deleting PDF ./assets/pdf/2504.02812.pdf.
[08.04.2025 06:15] Success.
[08.04.2025 06:15] Downloading and parsing paper https://huggingface.co/papers/2504.03770.
[08.04.2025 06:15] Extra JSON file exists (./assets/json/2504.03770.json), skip PDF parsing.
[08.04.2025 06:15] Paper image links file exists (./assets/img_data/2504.03770.json), skip HTML parsing.
[08.04.2025 06:15] Success.
[08.04.2025 06:15] Enriching papers with extra data.
[08.04.2025 06:15] ********************************************************************************
[08.04.2025 06:15] Abstract 0. Transformers today still struggle to generate one-minute videos because self-attention layers are inefficient for long context. Alternatives such as Mamba layers struggle with complex multi-scene stories because their hidden states are less expressive. We experiment with Test-Time Training (TTT) lay...
[08.04.2025 06:15] ********************************************************************************
[08.04.2025 06:15] Abstract 1. Region-level captioning aims to generate natural language descriptions for specific image regions while highlighting their distinguishing features. However, existing methods struggle to produce unique captions across multi-granularity, limiting their real-world applicability. To address the need for...
[08.04.2025 06:15] ********************************************************************************
[08.04.2025 06:15] Abstract 2. Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual c...
[08.04.2025 06:15] ********************************************************************************
[08.04.2025 06:15] Abstract 3. We introduce LiveVQA, an automatically collected dataset of latest visual knowledge from the Internet with synthesized VQA problems. LiveVQA consists of 3,602 single- and multi-hop visual questions from 6 news websites across 14 news categories, featuring high-quality image-text coherence and authen...
[08.04.2025 06:15] ********************************************************************************
[08.04.2025 06:15] Abstract 4. Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact...
[08.04.2025 06:15] ********************************************************************************
[08.04.2025 06:15] Abstract 5. Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under clas...
[08.04.2025 06:15] ********************************************************************************
[08.04.2025 06:15] Abstract 6. The proliferation of Large Language Models (LLMs) accessed via black-box APIs introduces a significant trust challenge: users pay for services based on advertised model capabilities (e.g., size, performance), but providers may covertly substitute the specified model with a cheaper, lower-quality alt...
[08.04.2025 06:15] ********************************************************************************
[08.04.2025 06:15] Abstract 7. We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world sce...
[08.04.2025 06:15] ********************************************************************************
[08.04.2025 06:15] Abstract 8. Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of in...
[08.04.2025 06:15] Read previous papers.
[08.04.2025 06:15] Generating reviews via LLM API.
[08.04.2025 06:15] Using data from previous issue: {"categories": ["#training", "#video", "#story_generation", "#long_context", "#dataset"], "emoji": "🎬", "ru": {"title": "TTT слои: прорыв в генерации длинных видео трансформерами", "desc": "Эта статья представляет новый подход к генерации длинных видео с использованием слоев Test-Time Training (TTT)
[08.04.2025 06:15] Using data from previous issue: {"categories": ["#data", "#games", "#cv", "#interpretability", "#dataset", "#benchmark", "#multimodal"], "emoji": "🖼️", "ru": {"title": "Точное описание регионов изображений с помощью многоуровневого подхода", "desc": "Статья представляет новый набор данных URECA для многоуровневого описания регионо
[08.04.2025 06:15] Using data from previous issue: {"categories": ["#diffusion", "#dataset", "#cv", "#inference"], "emoji": "✂️", "ru": {"title": "Точное редактирование изображений с помощью концептуального скальпеля", "desc": "Статья представляет новый подход к редактированию изображений с помощью диффузионных моделей, называемый Concept Lancet (Co
[08.04.2025 06:15] Using data from previous issue: {"categories": ["#cv", "#reasoning", "#survey", "#dataset", "#benchmark"], "emoji": "🔍", "ru": {"title": "LiveVQA: Новый рубеж в визуальном вопросно-ответном анализе", "desc": "LiveVQA - это автоматически собранный датасет с последними визуальными знаниями из интернета и синтезированными задачами ви
[08.04.2025 06:15] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#inference", "#reasoning", "#math", "#training", "#benchmark"], "emoji": "🧠", "ru": {"title": "Квантование моделей рассуждений: баланс между эффективностью и точностью", "desc": "Это исследование посвящено изучению влияния квантования на языковые мод
[08.04.2025 06:15] Using data from previous issue: {"categories": ["#training", "#cv", "#diffusion", "#inference"], "emoji": "🌊", "ru": {"title": "GMFlow: мощная генерация изображений с гауссовыми смесями", "desc": "Статья представляет новую модель Gaussian mixture flow matching (GMFlow) для генерации изображений. GMFlow предсказывает параметры дина
[08.04.2025 06:15] Using data from previous issue: {"categories": ["#security", "#inference", "#benchmark", "#ethics"], "emoji": "🕵️", "ru": {"title": "Защита от подмены: обеспечение честности в API языковых моделей", "desc": "Статья рассматривает проблему доверия к API больших языковых моделей (LLM), когда провайдеры могут тайно подменять заявленны
[08.04.2025 06:15] Querying the API.
[08.04.2025 06:15] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined a new, more practical 6D object detection task where identities of objects visible in a test image are not provided as input. Third, we introduced new BOP-H3 datasets recorded with high-resolution sensors and AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks, each defined by a task, object onboarding setup, and dataset group. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for this task is Co-op which takes only 0.8s per image and is 25X faster and 13% more accurate than GenFlow. Methods have a similar ranking on 6D detection as on 6D localization but higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still noticealy (-53%) behind the accuracy for seen objects (GDet2023). The online evaluation system stays open and is available at http://bop.felk.cvut.cz/
[08.04.2025 06:16] Response: {
  "desc": "Статья представляет результаты соревнования BOP Challenge 2024 по оценке 6D позы объектов. Введены новые задачи без использования 3D моделей и более практичные сценарии обнаружения объектов. Представлены новые наборы данных BOP-H3, записанные с помощью высокоточных сенсоров и AR/VR гарнитур. Лучшие методы 2024 года показали значительное улучшение точности по сравнению с методами 2023 года для различных задач, включая локализацию и обнаружение объектов.",
  "emoji": "🤖",
  "title": "Прорыв в компьютерном зрении: новые горизонты оценки 6D позы объектов"
}
[08.04.2025 06:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined a new, more practical 6D object detection task where identities of objects visible in a test image are not provided as input. Third, we introduced new BOP-H3 datasets recorded with high-resolution sensors and AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks, each defined by a task, object onboarding setup, and dataset group. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for this task is Co-op which takes only 0.8s per image and is 25X faster and 13% more accurate than GenFlow. Methods have a similar ranking on 6D detection as on 6D localization but higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still noticealy (-53%) behind the accuracy for seen objects (GDet2023). The online evaluation system stays open and is available at http://bop.felk.cvut.cz/"

[08.04.2025 06:16] Response: ```python
["DATASET", "BENCHMARK", "CV"]
```
[08.04.2025 06:16] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined a new, more practical 6D object detection task where identities of objects visible in a test image are not provided as input. Third, we introduced new BOP-H3 datasets recorded with high-resolution sensors and AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks, each defined by a task, object onboarding setup, and dataset group. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for this task is Co-op which takes only 0.8s per image and is 25X faster and 13% more accurate than GenFlow. Methods have a similar ranking on 6D detection as on 6D localization but higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still noticealy (-53%) behind the accuracy for seen objects (GDet2023). The online evaluation system stays open and is available at http://bop.felk.cvut.cz/"

[08.04.2025 06:16] Response: ```python
[]
```
[08.04.2025 06:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The BOP Challenge 2024 focuses on advancing 6D object pose estimation by transitioning from controlled lab environments to real-world applications. This year, new model-free tasks were introduced, requiring methods to learn from reference videos without 3D models. The challenge also featured a practical 6D object detection task where object identities were not provided, alongside the release of the BOP-H3 datasets that simulate real-world conditions. Notably, the top-performing methods demonstrated significant improvements in accuracy and speed compared to previous years, highlighting the ongoing evolution in this field of machine learning.","title":"Advancing 6D Object Pose Estimation in Real-World Scenarios"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The BOP Challenge 2024 focuses on advancing 6D object pose estimation by transitioning from controlled lab environments to real-world applications. This year, new model-free tasks were introduced, requiring methods to learn from reference videos without 3D models. The challenge also featured a practical 6D object detection task where object identities were not provided, alongside the release of the BOP-H3 datasets that simulate real-world conditions. Notably, the top-performing methods demonstrated significant improvements in accuracy and speed compared to previous years, highlighting the ongoing evolution in this field of machine learning.', title='Advancing 6D Object Pose Estimation in Real-World Scenarios'))
[08.04.2025 06:16] Response: ParsedChatCompletionMessage[Article](content='{"desc":"本文介绍了2024年BOP挑战赛的评估方法、数据集和结果，这是一个旨在捕捉6D物体姿态估计最新技术的公开竞赛。2024年的目标是将BOP从实验室环境转向真实世界场景，推出了新的无模型任务，要求方法仅通过参考视频进行物体识别。我们还定义了一个更实用的6D物体检测任务，测试图像中物体的身份不再作为输入提供。此外，BOP-H3数据集使用高分辨率传感器和AR/VR头显录制，支持模型基础和无模型任务。","title":"BOP挑战赛：从实验室到真实世界的6D物体姿态估计"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='本文介绍了2024年BOP挑战赛的评估方法、数据集和结果，这是一个旨在捕捉6D物体姿态估计最新技术的公开竞赛。2024年的目标是将BOP从实验室环境转向真实世界场景，推出了新的无模型任务，要求方法仅通过参考视频进行物体识别。我们还定义了一个更实用的6D物体检测任务，测试图像中物体的身份不再作为输入提供。此外，BOP-H3数据集使用高分辨率传感器和AR/VR头显录制，支持模型基础和无模型任务。', title='BOP挑战赛：从实验室到真实世界的6D物体姿态估计'))
[08.04.2025 06:16] Using data from previous issue: {"categories": ["#alignment", "#multimodal", "#security", "#dataset", "#benchmark"], "emoji": "🛡️", "ru": {"title": "Адаптивная защита MLLM от атак 'jailbreak' без доступа к вредоносным данным", "desc": "В статье представлен метод JAILDAM для обнаружения атак типа 'jailbreak' на мультимодальные боль
[08.04.2025 06:16] Loading Chinese text from previous data.
[08.04.2025 06:16] Renaming data file.
[08.04.2025 06:16] Renaming previous data. hf_papers.json to ./d/2025-04-08.json
[08.04.2025 06:16] Saving new data file.
[08.04.2025 06:16] Generating page.
[08.04.2025 06:16] Renaming previous page.
[08.04.2025 06:16] Renaming previous data. index.html to ./d/2025-04-08.html
[08.04.2025 06:16] [Experimental] Generating Chinese page for reading.
[08.04.2025 06:16] Chinese vocab [{'word': '涵盖', 'pinyin': 'hángài', 'trans': 'cover'}, {'word': '基准', 'pinyin': 'jīzhǔn', 'trans': 'benchmark'}, {'word': '注释', 'pinyin': 'zhùshì', 'trans': 'annotate'}, {'word': '评估', 'pinyin': 'pínggū', 'trans': 'evaluate'}, {'word': '先进', 'pinyin': 'xiānjìn', 'trans': 'advanced'}, {'word': '详细', 'pinyin': 'xiángxì', 'trans': 'detailed'}, {'word': '分析', 'pinyin': 'fēnxī', 'trans': 'analysis'}, {'word': '宣布', 'pinyin': 'xuānbù', 'trans': 'announce'}, {'word': '成立', 'pinyin': 'chénglì', 'trans': 'establish'}, {'word': '开源', 'pinyin': 'kāiyuán', 'trans': 'open-source'}, {'word': '社区', 'pinyin': 'shèqū', 'trans': 'community'}, {'word': '旨在', 'pinyin': 'zhǐzài', 'trans': 'aim to'}, {'word': '构建', 'pinyin': 'gòujiàn', 'trans': 'build'}, {'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'}, {'word': '强化学习', 'pinyin': 'qiáng huà xuéxí', 'trans': 'reinforcement learning'}, {'word': '训练', 'pinyin': 'xùnliàn', 'trans': 'training'}, {'word': '数据集', 'pinyin': 'shùjùjí', 'trans': 'dataset'}]
[08.04.2025 06:16] Renaming previous Chinese page.
[08.04.2025 06:16] Renaming previous data. zh.html to ./d/2025-04-07_zh_reading_task.html
[08.04.2025 06:16] Writing Chinese reading task.
[08.04.2025 06:16] Writing result.
[08.04.2025 06:16] Renaming log file.
[08.04.2025 06:16] Renaming previous data. log.txt to ./logs/2025-04-08_last_log.txt
