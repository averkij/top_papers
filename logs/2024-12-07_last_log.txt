[07.12.2024 06:30] Read previous papers.
[07.12.2024 06:30] Generating top page (month).
[07.12.2024 06:30] Writing top page (month).
[07.12.2024 12:41] Read previous papers.
[07.12.2024 12:41] Get feed.
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04467
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04424
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04455
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04454
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04468
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03679
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03895
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01506
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01339
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04146
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03632
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04431
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04280
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04315
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04378
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03304
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.02142
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01169
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04062
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.01820
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04462
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04003
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04139
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.03704
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04448
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2411.19574
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04106
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04449
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04403
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04262
[07.12.2024 12:41] Get page data from previous paper. URL: https://huggingface.co/papers/2412.04363
[07.12.2024 12:41] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.12.2024 12:41] No deleted papers detected.
[07.12.2024 12:41] Downloading and parsing papers (pdf, html). Total: 31.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04467.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04467.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04467.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04424.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04424.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04424.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04455.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04455.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04455.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04454.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04454.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04454.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04468.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04468.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04468.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.03679.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.03679.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.03679.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.03895.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.03895.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.03895.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.01506.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.01506.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.01506.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.01339.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.01339.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.01339.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04146.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04146.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04146.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.03632.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.03632.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.03632.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04431.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04431.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04431.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04280.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04280.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04280.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04315.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04315.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04315.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04378.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04378.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04378.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.03304.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.03304.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.03304.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.02142.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.02142.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.02142.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.01169.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.01169.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.01169.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04062.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04062.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04062.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.01820.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.01820.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.01820.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04462.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04462.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04462.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04003.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04003.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04003.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04139.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04139.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04139.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.03704.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.03704.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.03704.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04448.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04448.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04448.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2411.19574.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2411.19574.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2411.19574.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04106.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04106.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04106.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04449.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04449.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04449.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04403.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04403.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04403.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04262.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04262.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04262.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Downloading and parsing paper https://huggingface.co/papers/2412.04363.
[07.12.2024 12:41] Extra JSON file exists (./assets/json/2412.04363.json), skip PDF parsing.
[07.12.2024 12:41] Paper image links file exists (./assets/img_data/2412.04363.json), skip HTML parsing.
[07.12.2024 12:41] Success.
[07.12.2024 12:41] Enriching papers with extra data.
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 0. Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 1. We present Florence-VL, a new family of multimodal large language models (MLLMs) with enriched visual representations produced by Florence-2, a generative vision foundation model. Unlike the widely used CLIP-style vision transformer trained by contrastive learning, Florence-2 can capture different l...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 2. Automatic detection and prevention of open-set failures are crucial in closed-loop robotic systems. Recent studies often struggle to simultaneously identify unexpected failures reactively after they occur and prevent foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a nove...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 3. Graphical User Interfaces (GUIs) are critical to human-computer interaction, yet automating GUI tasks remains challenging due to the complexity and variability of visual environments. Existing approaches often rely on textual representations of GUIs, which introduce limitations in generalization, ef...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 4. Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, we improve its model ar...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 5. Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic ...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 6. Diffusion models excel in generating high-quality images. However, current diffusion models struggle to produce reliable images without guidance methods, such as classifier-free guidance (CFG). Are guidance methods truly necessary? Observing that noise obtained via diffusion inversion can reconstruc...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 7. We introduce a novel 3D generation method for versatile and high-quality 3D asset creation. The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a spa...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 8. Text-based adversarial guidance using a negative prompt has emerged as a widely adopted approach to push the output features away from undesired concepts. While useful, performing adversarial guidance using text alone can be insufficient to capture complex visual concepts and avoid undesired visual ...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 9. Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiti...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 10. Existing multi-view image generation methods often make invasive modifications to pre-trained text-to-image (T2I) models and require full fine-tuning, leading to (1) high computational costs, especially with large base models and high-resolution images, and (2) degradation in image quality due to op...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 11. We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and ...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 12. We present HumanEdit, a high-quality, human-rewarded dataset specifically designed for instruction-guided image editing, enabling precise and diverse image manipulations through open-form language instructions. Previous large-scale editing datasets often incorporate minimal human feedback, leading t...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 13. Large Language Models (LLMs) have emerged as a milestone in artificial intelligence, and their performance can improve as the model size increases. However, this scaling brings great challenges to training and inference efficiency, particularly for deploying LLMs in resource-constrained environments...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 14. Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a "bag of words" behavior. At the same time, Large Vision-Language M...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 15. Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermo...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 16. Multimodal Large Language Models (MLLMs) have become increasingly important due to their state-of-the-art performance and ability to integrate multiple data modalities, such as text, images, and audio, to perform complex tasks with high accuracy. This paper presents a comprehensive survey on persona...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 17. We introduce OmniFlow, a novel generative model designed for any-to-any generation tasks such as text-to-image, text-to-audio, and audio-to-image synthesis. OmniFlow advances the rectified flow (RF) framework used in text-to-image models to handle the joint distribution of multiple modalities. It ou...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 18. In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Giv...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 19. As a globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding. Specifically, we make the following contributions in this paper: (i) we introduce SoccerReplay-1988,...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 20. We propose 4Real-Video, a novel framework for generating 4D videos, organized as a grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. We propose a novel two-stream archit...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 21. Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 22. Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by polysemanticity -- where individual neurons respond to multi...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 23. Despite significant advancements in vision-language models (VLMs), there lacks effective approaches to enhance response quality by scaling inference-time computation. This capability is known to be a core step towards the self-improving models in recent large language model studies. In this paper, w...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 24. Recent advances in video diffusion models have unlocked new potential for realistic audio-driven talking video generation. However, achieving seamless audio-lip synchronization, maintaining long-term identity consistency, and producing natural, audio-aligned expressions in generated talking videos r...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 25. The current large language models are mainly based on decode-only structure transformers, which have great in-context learning (ICL) capabilities. It is generally believed that the important foundation of its ICL capability is the induction heads mechanism, which requires at least two layers attenti...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 26. Medical image segmentation has recently demonstrated impressive progress with deep neural networks, yet the heterogeneous modalities and scarcity of mask annotations limit the development of segmentation models on unannotated modalities. This paper investigates a new paradigm for leveraging generati...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 27. Despite the remarkable performance of multimodal large language models (MLLMs) across diverse tasks, the substantial training and inference costs impede their advancement. The majority of computation stems from the overwhelming volume of vision tokens processed by the transformer decoder. In this pa...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 28. We develop task scaling laws and model ladders to predict the individual task performance of pretrained language models (LMs) in the overtrained setting. Standard power laws for language modeling loss cannot accurately model task performance. Therefore, we leverage a two-step prediction approach: fi...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 29. Table extraction from document images is a challenging AI problem, and labelled data for many content domains is difficult to come by. Existing table extraction datasets often focus on scientific tables due to the vast amount of academic articles that are readily available, along with their source c...
[07.12.2024 12:41] ********************************************************************************
[07.12.2024 12:41] Abstract 30. Open community-driven platforms like Chatbot Arena that collect user preference data from site visitors have gained a reputation as one of the most trustworthy publicly available benchmarks for LLM performance. While now standard, it is tricky to implement effective guardrails to collect high-qualit...
[07.12.2024 12:41] Read previous papers.
[07.12.2024 12:41] Generating reviews via LLM API.
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#inference", "#interpretability", "#multimodal", "#optimization", "#cv"], "emoji": "üóúÔ∏è", "ru": {"title": "VisionZip: –°–∂–∏–º–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, —É—Å–∫–æ—Ä—è–µ–º –ò–ò", "desc": "VisionZip - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º –≤—ã–±–æ—Ä–∞ –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#alignment", "#training", "#benchmark", "#hallucinations", "#open_source", "#architecture", "#multimodal"], "emoji": "üñºÔ∏è", "ru": {"title": "Florence-VL: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Florence-VL –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#agents", "#robotics", "#optimization", "#cv", "#security"], "emoji": "ü§ñ", "ru": {"title": "–ö–æ–¥ –∫–∞–∫ –º–æ–Ω–∏—Ç–æ—Ä: —É–º–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Code-as-Monitor (CaM), –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –æ—à–∏–±
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#open_source", "#games", "#multimodal", "#agents", "#training", "#reasoning", "#dataset"], "emoji": "üñ•Ô∏è", "ru": {"title": "Aguvis: –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π –ì–ò–ü-–∞–≥–µ–Ω—Ç –Ω–∞ —á–∏—Å—Ç–æ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Aguvis - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#architecture", "#training", "#benchmark", "#video", "#open_source", "#optimization", "#cv"], "emoji": "üñºÔ∏è", "ru": {"title": "NVILA: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π NVILA, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#synthetic", "#data"], "emoji": "üß†", "ru": {"title": "AgoraBench: –ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω AgoraBench - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ. –ò—Å—Å–ª
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#cv", "#optimization", "#diffusion", "#inference"], "emoji": "üé®", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –Ω–∞–≤–µ–¥–µ–Ω–∏—è: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ—Ö–Ω–∏–∫ –Ω–∞–≤–µ–¥–µ–Ω–∏—è
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#3d", "#dataset", "#training", "#open_source"], "emoji": "üé®", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞. –û—Å–Ω–æ–≤–æ–π –º–µ—Ç–æ–¥–∞ —è–≤–ª—è–µ—Ç—Å—è —É–Ω–∏—Ñ–∏—Ü
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#training", "#cv", "#ethics", "#multimodal", "#security", "#diffusion"], "emoji": "üé®", "ru": {"title": "NegToMe: –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º NegToMe (negative token merging) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#diffusion", "#games", "#cv", "#multimodal"], "emoji": "üëö", "ru": {"title": "AnyDressing: –í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–∏–º–µ—Ä–∫–∞ –ª—é–±–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –æ–¥–µ–∂–¥—ã —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –¥–µ—Ç–∞–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ AnyDressing –¥–ª—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–º–µ—Ä–∫–∏ –æ–¥–µ–∂–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ —Å
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#training", "#synthetic", "#optimization", "#architecture", "#cv", "#3d"], "emoji": "üñºÔ∏è", "ru": {"title": "MV-Adapter: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MV-Adapter - –ø–µ—Ä–≤–æ–µ –∞–¥–∞–ø—Ç–µ—Ä–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#open_source", "#diffusion", "#multimodal", "#cv", "#benchmark", "#architecture"], "emoji": "üîÑ", "ru": {"title": "Infinity: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–º —Å–ª–æ–≤–∞—Ä–µ–º", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Infinity - –±–∏—Ç–æ–≤–∞—è –≤–∏–∑—É–∞–ª—å–Ω–∞—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#data"], "emoji": "üé®", "ru": {"title": "HumanEdit: –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø–æ–¥—Ö–æ–¥–æ–º", "desc": "HumanEdit - —ç—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Å —É—á–∞—Å—Ç–∏–µ–º –ª—é–¥–µ–π. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç 5751 –∏–∑–æ–±—Ä–∞–∂
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#training", "#benchmark", "#open_source", "#optimization", "#architecture", "#inference"], "emoji": "üìà", "ru": {"title": "–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –º–æ—â–Ω–æ—Å—Ç–∏: –Ω–æ–≤—ã–π –ø—É—Ç—å –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É '–ø–ª–æ—Ç–Ω–æ—Å—Ç—å –º–æ—â–Ω–æ—Å—Ç–∏' –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#architecture", "#cv", "#reasoning", "#benchmark", "#optimization", "#multimodal", "#training"], "emoji": "üîç", "ru": {"title": "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–≥–æ –∏–∑ –¥–≤—É—Ö –º–∏—Ä–æ–≤: –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–µ LVLM —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º —è–∑—ã–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö –≤–∏–∑
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#ethics", "#dataset", "#multilingual", "#low_resource", "#machine_translation"], "emoji": "üåç", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –±–∞—Ä—å–µ—Ä–æ–≤ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –æ—Ü–µ–Ω–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –ø—Ä–µ–¥—É–±–µ–∂–¥–µ–Ω–∏–π –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è 
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#training", "#survey", "#multimodal", "#architecture", "#dataset", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò: –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –æ–±–∑–æ—Ä –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–úLLM).
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#training", "#optimization", "#architecture"], "emoji": "üîÑ", "ru": {"title": "OmniFlow: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", "desc": "OmniFlow - —ç—Ç–æ –Ω–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª—é–±–æ–≥–æ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ª—é–±–æ–π –¥
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#optimization", "#training", "#cv"], "emoji": "üöÄ", "ru": {"title": "ZipAR: –£—Å–∫–æ—Ä—è–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º", "desc": "ZipAR - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –Ω–∞–±–ª—é–¥–µ
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#dataset", "#architecture", "#multimodal", "#video"], "emoji": "‚öΩ", "ru": {"title": "MatchVision: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏ –¥–ª—è —Ñ—É—Ç–±–æ–ª–∞", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å MatchVision –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ñ—É—Ç–±–æ–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –∫—Ä—É–ø–Ω–µ–π—à–∏–π –¥–∞—Ç–∞—Å–µ—Ç Socce
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#video", "#architecture"], "emoji": "üé•", "ru": {"title": "4Real-Video: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 4D-–≤–∏–¥–µ–æ", "desc": "4Real-Video - —ç—Ç–æ –Ω–æ–≤–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 4D-–≤–∏–¥–µ–æ, –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤ –≤–∏–¥–µ —Å–µ—Ç–∫–∏ –∫–∞–¥—Ä–æ–≤ —Å –æ—Å—è–º–∏ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–∞–∫—É—Ä—Å–∞. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –¥–≤—É—Ö–ø–æ—Ç–æ–∫–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: –æ–¥–∏–Ω –ø–æ—Ç–æ–∫ 
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#training", "#machine_translation", "#dataset", "#low_resource", "#multilingual"], "emoji": "üåç", "ru": {"title": "Marco-LLM: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –±–∞—Ä—å–µ—Ä–∞ –≤ –º–∏—Ä–µ –ò–ò", "desc": "Marco-LLM - —ç—Ç–æ –Ω–æ–≤–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#training", "#interpretability", "#optimization", "#open_source", "#architecture", "#alignment"], "emoji": "üß†", "ru": {"title": "–ü—Ä–æ–∑—Ä–∞—á–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –º–æ–Ω–æ—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#optimization", "#inference", "#cv", "#hallucinations", "#training", "#multimodal"], "emoji": "üîç", "ru": {"title": "VisVM: –£–ª—É—á—à–µ–Ω–∏–µ VLM —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≤—ã–≤–æ–¥–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Vision Value Model (VisVM) - –º–æ–¥–µ–ª—å, —É–ª—É—á—à–∞—é—â—É—é –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#long_context", "#multimodal", "#video", "#diffusion"], "emoji": "üé≠", "ru": {"title": "MEMO: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –ø–æ—Ä—Ç—Ä–µ—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–∞–º—è—Ç–∏ –∏ —ç–º–æ—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –≥–æ–≤–æ—Ä—è—â–∏–º–∏ –ø–æ—Ä—Ç—Ä–µ—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º MEMO (Memory-guid
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#training"], "emoji": "üß†", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏–Ω–¥—É–∫—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –∏–Ω–¥—É–∫—Ü–∏–æ–Ω–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–∫ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#diffusion", "#healthcare", "#synthetic", "#cv", "#data", "#dataset"], "emoji": "üß†", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ú–†–¢ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏–Ω—Ç–µ–∑—É –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –Ω–µ–∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#optimization", "#inference", "#training", "#benchmark", "#multimodal"], "emoji": "üöÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏: –º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤, –±–æ–ª—å—à–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–≤—ã—à–µ–Ω–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#small_models", "#optimization", "#dataset", "#training"], "emoji": "üìà", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–µ—Ç–æ–¥—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#open_source", "#data", "#dataset", "#synthetic", "#transfer_learning", "#benchmark"], "emoji": "üìä", "ru": {"title": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ç–∞–±–ª–∏—Ü", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SynFinTabs - –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏
[07.12.2024 12:41] Using data from previous issue: {"categories": ["#ethics", "#rlhf", "#hallucinations", "#alignment", "#benchmark"], "emoji": "üé≠", "ru": {"title": "–ê—Ö–∏–ª–ª–µ—Å–æ–≤–∞ –ø—è—Ç–∞ –∫—Ä–∞—É–¥—Å–æ—Ä—Å–∏–Ω–≥–æ–≤—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –ò–ò", "desc": "–°—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ Chatbot Arena. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç —Ç—Ä–∏ –∏—Å—Ç–æ—á
[07.12.2024 12:41] Trying to get texts in Chinese.
[07.12.2024 12:41] Mistral request. Model: mistral-large-latest. Prompt: Write simple and brief explanation (4-5 sentences) of an article in Chinese. Use short sentences. Text:

Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and SigLIP, contain significant redundancy. To address this, we introduce VisionZip, a simple yet effective method that selects a set of informative tokens for input to the language model, reducing visual token redundancy and improving efficiency while maintaining model performance. The proposed VisionZip can be widely applied to image and video understanding tasks and is well-suited for multi-turn dialogues in real-world scenarios, where previous methods tend to underperform. Experimental results show that VisionZip outperforms the previous state-of-the-art method by at least 5% performance gains across nearly all settings. Moreover, our method significantly enhances model inference speed, improving the prefilling time by 8x and enabling the LLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while achieving better results. Furthermore, we analyze the causes of this redundancy and encourage the community to focus on extracting better visual features rather than merely increasing token length. Our code is available at https://github.com/dvlab-research/VisionZip .
[07.12.2024 12:41] Mistral response. {"id": "80aafe7701ce4ea59c1d56b2267a1d84", "object": "chat.completion", "created": 1733575305, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "\u6700\u8fd1\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5c55\u901a\u8fc7\u589e\u52a0\u89c6\u89c9\u6807\u8bb0\u7684\u957f\u5ea6\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u8fd9\u4e5f\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6d41\u884c\u7684\u89c6\u89c9\u7f16\u7801\u5668\u751f\u6210\u7684\u89c6\u89c9\u6807\u8bb0\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86VisionZip\u65b9\u6cd5\uff0c\u9009\u62e9\u4e00\u7ec4\u6709\u7528\u7684\u89c6\u89c9\u6807\u8bb0\u8f93\u5165\u8bed\u8a00\u6a21\u578b\uff0c\u51cf\u5c11\u5197\u4f59\u5e76\u63d0\u9ad8\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVisionZip\u5728\u51e0\u4e4e\u6240\u6709\u8bbe\u7f6e\u4e2d\u90fd\u6bd4\u4e4b\u524d\u7684\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad8\u4e86\u81f3\u5c115%\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u63a8\u7406\u901f\u5ea6\uff0c\u4f7fLLaVA-Next 13B\u6a21\u578b\u6bd4LLaVA-Next 7B\u6a21\u578b\u63a8\u7406\u66f4\u5feb\uff0c\u5e76\u53d6\u5f97\u66f4\u597d\u7684\u7ed3\u679c\u3002", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 320, "total_tokens": 537, "completion_tokens": 217}}
[07.12.2024 12:41] Response: ÊúÄËøëËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑËøõÂ±ïÈÄöËøáÂ¢ûÂä†ËßÜËßâÊ†áËÆ∞ÁöÑÈïøÂ∫¶Êù•ÊèêÈ´òÊÄßËÉΩÔºå‰ΩÜËøô‰πüÂ¢ûÂä†‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊµÅË°åÁöÑËßÜËßâÁºñÁ†ÅÂô®ÁîüÊàêÁöÑËßÜËßâÊ†áËÆ∞‰∏≠Â≠òÂú®Â§ßÈáèÂÜó‰Ωô„ÄÇ‰∏∫Ëß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÁ†îÁ©∂‰∫∫ÂëòÊèêÂá∫‰∫ÜVisionZipÊñπÊ≥ïÔºåÈÄâÊã©‰∏ÄÁªÑÊúâÁî®ÁöÑËßÜËßâÊ†áËÆ∞ËæìÂÖ•ËØ≠Ë®ÄÊ®°ÂûãÔºåÂáèÂ∞ëÂÜó‰ΩôÂπ∂ÊèêÈ´òÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåVisionZipÂú®Âá†‰πéÊâÄÊúâËÆæÁΩÆ‰∏≠ÈÉΩÊØî‰πãÂâçÁöÑÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÈ´ò‰∫ÜËá≥Â∞ë5%ÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÊé®ÁêÜÈÄüÂ∫¶Ôºå‰ΩøLLaVA-Next 13BÊ®°ÂûãÊØîLLaVA-Next 7BÊ®°ÂûãÊé®ÁêÜÊõ¥Âø´ÔºåÂπ∂ÂèñÂæóÊõ¥Â•ΩÁöÑÁªìÊûú„ÄÇ
[07.12.2024 12:41] Mistral request. Model: mistral-large-latest. Prompt: Write pinyin transcription for text. Text:

ÊúÄËøëËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑËøõÂ±ïÈÄöËøáÂ¢ûÂä†ËßÜËßâÊ†áËÆ∞ÁöÑÈïøÂ∫¶Êù•ÊèêÈ´òÊÄßËÉΩÔºå‰ΩÜËøô‰πüÂ¢ûÂä†‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊµÅË°åÁöÑËßÜËßâÁºñÁ†ÅÂô®ÁîüÊàêÁöÑËßÜËßâÊ†áËÆ∞‰∏≠Â≠òÂú®Â§ßÈáèÂÜó‰Ωô„ÄÇ‰∏∫Ëß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÁ†îÁ©∂‰∫∫ÂëòÊèêÂá∫‰∫ÜVisionZipÊñπÊ≥ïÔºåÈÄâÊã©‰∏ÄÁªÑÊúâÁî®ÁöÑËßÜËßâÊ†áËÆ∞ËæìÂÖ•ËØ≠Ë®ÄÊ®°ÂûãÔºåÂáèÂ∞ëÂÜó‰ΩôÂπ∂ÊèêÈ´òÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåVisionZipÂú®Âá†‰πéÊâÄÊúâËÆæÁΩÆ‰∏≠ÈÉΩÊØî‰πãÂâçÁöÑÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÈ´ò‰∫ÜËá≥Â∞ë5%ÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÊé®ÁêÜÈÄüÂ∫¶Ôºå‰ΩøLLaVA-Next 13BÊ®°ÂûãÊØîLLaVA-Next 7BÊ®°ÂûãÊé®ÁêÜÊõ¥Âø´ÔºåÂπ∂ÂèñÂæóÊõ¥Â•ΩÁöÑÁªìÊûú„ÄÇ
[07.12.2024 12:42] Mistral response. {"id": "2624e25ba16c40f8bdbbf1df768b7bac", "object": "chat.completion", "created": 1733575309, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "\u6700\u8fd1\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5c55\u901a\u8fc7\u589e\u52a0\u89c6\u89c9\u6807\u8bb0\u7684\u957f\u5ea6\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u8fd9\u4e5f\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6d41\u884c\u7684\u89c6\u89c9\u7f16\u7801\u5668\u751f\u6210\u7684\u89c6\u89c9\u6807\u8bb0\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86VisionZip\u65b9\u6cd5\uff0c\u9009\u62e9\u4e00\u7ec4\u6709\u7528\u7684\u89c6\u89c9\u6807\u8bb0\u8f93\u5165\u8bed\u8a00\u6a21\u578b\uff0c\u51cf\u5c11\u5197\u4f59\u5e76\u63d0\u9ad8\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVisionZip\u5728\u51e0\u4e4e\u6240\u6709\u8bbe\u7f6e\u4e2d\u90fd\u6bd4\u4e4b\u524d\u7684\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad8\u4e86\u81f3\u5c115%\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u63a8\u7406\u901f\u5ea6\uff0c\u4f7fLLaVA-Next 13B\u6a21\u578b\u6bd4LLaVA-Next 7B\u6a21\u578b\u63a8\u7406\u66f4\u5feb\uff0c\u5e76\u53d6\u5f97\u66f4\u597d\u7684\u7ed3\u679c\u3002\n\nZu\u00ecj\u00ecn sh\u00ecju\u00e9-y\u01d4y\u00e1n m\u00f3x\u00edng de j\u00ecnzh\u01cen t\u014dnggu\u00f2 z\u0113ngji\u0101 sh\u00ecju\u00e9 bi\u0101oj\u00ec de ch\u00e1ngd\u00f9 l\u00e1i t\u00edg\u0101o x\u00ecngn\u00e9ng, d\u00e0n zh\u00e8 y\u011b z\u0113ngji\u0101le j\u00ecsu\u00e0n ch\u00e9ngb\u011bn. Y\u00e1nji\u016b f\u0101xi\u00e0n, li\u00fax\u00edng de sh\u00ecju\u00e9 bi\u0101nm\u01ceq\u00ec sh\u0113ngch\u00e9ng de sh\u00ecju\u00e9 bi\u0101oj\u00ec zh\u014dng c\u00fanz\u00e0i d\u00e0li\u00e0ng r\u01d2ngy\u00f9. W\u00e8i ji\u011bju\u00e9 zh\u00e8 y\u012b w\u00e8nt\u00ed, y\u00e1nji\u016b r\u00e9nyu\u00e1n t\u00edch\u016ble VisionZip f\u0101ngf\u01ce, xu\u01cenz\u00e9 y\u012b z\u01d4 y\u01d2uy\u00f2ng de sh\u00ecju\u00e9 bi\u0101oj\u00ec sh\u016br\u00f9 y\u01d4y\u00e1n m\u00f3x\u00edng, ji\u01censh\u01ceo r\u01d2ngy\u00f9 b\u00ecng t\u00edg\u0101o xi\u00e0ol\u01dc. Sh\u00edy\u00e0n ji\u00e9gu\u01d2 xi\u01censh\u00ec, VisionZip z\u00e0i j\u012bhu\u0101 su\u01d2y\u01d2u sh\u00e8zh\u00ec zh\u014dng d\u014du b\u01d0 zh\u012bqi\u00e1n de zu\u00ecji\u0101 f\u0101ngf\u01ce t\u00edg\u0101o le zh\u00ecsh\u01ceo 5% de x\u00ecngn\u00e9ng. C\u01d0w\u00e0i, g\u0101i f\u0101ngf\u01ce xi\u01cenzh\u00f9 t\u00edg\u0101o le m\u00f3x\u00edng tu\u012bl\u01d0 s\u00f9d\u00f9, sh\u01d0 LLaVA-Next 13B m\u00f3x\u00edng b\u01d0 LLaVA-Next 7B m\u00f3x\u00edng tu\u012bl\u01d0 g\u00e8ng ku\u00e0i, b\u00ecng q\u01d4d\u00e9 g\u00e8ng h\u01ceo de ji\u00e9gu\u01d2.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 232, "total_tokens": 889, "completion_tokens": 657}}
[07.12.2024 12:42] Response: ÊúÄËøëËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑËøõÂ±ïÈÄöËøáÂ¢ûÂä†ËßÜËßâÊ†áËÆ∞ÁöÑÈïøÂ∫¶Êù•ÊèêÈ´òÊÄßËÉΩÔºå‰ΩÜËøô‰πüÂ¢ûÂä†‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊµÅË°åÁöÑËßÜËßâÁºñÁ†ÅÂô®ÁîüÊàêÁöÑËßÜËßâÊ†áËÆ∞‰∏≠Â≠òÂú®Â§ßÈáèÂÜó‰Ωô„ÄÇ‰∏∫Ëß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÁ†îÁ©∂‰∫∫ÂëòÊèêÂá∫‰∫ÜVisionZipÊñπÊ≥ïÔºåÈÄâÊã©‰∏ÄÁªÑÊúâÁî®ÁöÑËßÜËßâÊ†áËÆ∞ËæìÂÖ•ËØ≠Ë®ÄÊ®°ÂûãÔºåÂáèÂ∞ëÂÜó‰ΩôÂπ∂ÊèêÈ´òÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåVisionZipÂú®Âá†‰πéÊâÄÊúâËÆæÁΩÆ‰∏≠ÈÉΩÊØî‰πãÂâçÁöÑÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÈ´ò‰∫ÜËá≥Â∞ë5%ÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÊé®ÁêÜÈÄüÂ∫¶Ôºå‰ΩøLLaVA-Next 13BÊ®°ÂûãÊØîLLaVA-Next 7BÊ®°ÂûãÊé®ÁêÜÊõ¥Âø´ÔºåÂπ∂ÂèñÂæóÊõ¥Â•ΩÁöÑÁªìÊûú„ÄÇ

Zu√¨j√¨n sh√¨ju√©-y«îy√°n m√≥x√≠ng de j√¨nzh«én t≈çnggu√≤ zƒìngjiƒÅ sh√¨ju√© biƒÅoj√¨ de ch√°ngd√π l√°i t√≠gƒÅo x√¨ngn√©ng, d√†n zh√® yƒõ zƒìngjiƒÅle j√¨su√†n ch√©ngbƒõn. Y√°nji≈´ fƒÅxi√†n, li√∫x√≠ng de sh√¨ju√© biƒÅnm«éq√¨ shƒìngch√©ng de sh√¨ju√© biƒÅoj√¨ zh≈çng c√∫nz√†i d√†li√†ng r«íngy√π. W√®i jiƒõju√© zh√® yƒ´ w√®nt√≠, y√°nji≈´ r√©nyu√°n t√≠ch≈´le VisionZip fƒÅngf«é, xu«énz√© yƒ´ z«î y«íuy√≤ng de sh√¨ju√© biƒÅoj√¨ sh≈´r√π y«îy√°n m√≥x√≠ng, ji«énsh«éo r«íngy√π b√¨ng t√≠gƒÅo xi√†ol«ú. Sh√≠y√†n ji√©gu«í xi«énsh√¨, VisionZip z√†i jƒ´huƒÅ su«íy«íu sh√®zh√¨ zh≈çng d≈çu b«ê zhƒ´qi√°n de zu√¨jiƒÅ fƒÅngf«é t√≠gƒÅo le zh√¨sh«éo 5% de x√¨ngn√©ng. C«êw√†i, gƒÅi fƒÅngf«é xi«énzh√π t√≠gƒÅo le m√≥x√≠ng tuƒ´l«ê s√πd√π, sh«ê LLaVA-Next 13B m√≥x√≠ng b«ê LLaVA-Next 7B m√≥x√≠ng tuƒ´l«ê g√®ng ku√†i, b√¨ng q«îd√© g√®ng h«éo de ji√©gu«í.
[07.12.2024 12:42] Mistral request. Model: mistral-large-latest. Prompt: Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

ÊúÄËøëËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑËøõÂ±ïÈÄöËøáÂ¢ûÂä†ËßÜËßâÊ†áËÆ∞ÁöÑÈïøÂ∫¶Êù•ÊèêÈ´òÊÄßËÉΩÔºå‰ΩÜËøô‰πüÂ¢ûÂä†‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊµÅË°åÁöÑËßÜËßâÁºñÁ†ÅÂô®ÁîüÊàêÁöÑËßÜËßâÊ†áËÆ∞‰∏≠Â≠òÂú®Â§ßÈáèÂÜó‰Ωô„ÄÇ‰∏∫Ëß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÁ†îÁ©∂‰∫∫ÂëòÊèêÂá∫‰∫ÜVisionZipÊñπÊ≥ïÔºåÈÄâÊã©‰∏ÄÁªÑÊúâÁî®ÁöÑËßÜËßâÊ†áËÆ∞ËæìÂÖ•ËØ≠Ë®ÄÊ®°ÂûãÔºåÂáèÂ∞ëÂÜó‰ΩôÂπ∂ÊèêÈ´òÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåVisionZipÂú®Âá†‰πéÊâÄÊúâËÆæÁΩÆ‰∏≠ÈÉΩÊØî‰πãÂâçÁöÑÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÈ´ò‰∫ÜËá≥Â∞ë5%ÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÊé®ÁêÜÈÄüÂ∫¶Ôºå‰ΩøLLaVA-Next 13BÊ®°ÂûãÊØîLLaVA-Next 7BÊ®°ÂûãÊé®ÁêÜÊõ¥Âø´ÔºåÂπ∂ÂèñÂæóÊõ¥Â•ΩÁöÑÁªìÊûú„ÄÇ
[07.12.2024 12:42] Mistral response. {"id": "9b63d792f5b54774b9765d8b1536ee76", "object": "chat.completion", "created": 1733575333, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "[{'word': '\u89c6\u89c9', 'pinyin': 'sh\u00ecju\u00e9', 'trans': 'vision'},\n{'word': '\u8bed\u8a00', 'pinyin': 'y\u01d4y\u00e1n', 'trans': 'language'},\n{'word': '\u6a21\u578b', 'pinyin': 'm\u00f3x\u00edng', 'trans': 'model'},\n{'word': '\u8fdb\u5c55', 'pinyin': 'j\u00ecnzh\u01cen', 'trans': 'progress'},\n{'word': '\u589e\u52a0', 'pinyin': 'z\u0113ngji\u0101', 'trans': 'increase'},\n{'word': '\u6807\u8bb0', 'pinyin': 'bi\u0101oj\u00ec', 'trans': 'marker'},\n{'word': '\u957f\u5ea6', 'pinyin': 'ch\u00e1ngd\u00f9', 'trans': 'length'},\n{'word': '\u6027\u80fd', 'pinyin': 'x\u00ecngn\u00e9ng', 'trans': 'performance'},\n{'word': '\u8ba1\u7b97', 'pinyin': 'j\u00ecsu\u00e0n', 'trans': 'computation'},\n{'word': '\u6210\u672c', 'pinyin': 'ch\u00e9ngb\u011bn', 'trans': 'cost'},\n{'word': '\u7814\u7a76', 'pinyin': 'y\u00e1nji\u016b', 'trans': 'research'},\n{'word': '\u53d1\u73b0', 'pinyin': 'f\u0101xi\u00e0n', 'trans': 'discover'},\n{'word': '\u6d41\u884c', 'pinyin': 'li\u00fax\u00edng', 'trans': 'popular'},\n{'word': '\u7f16\u7801\u5668', 'pinyin': 'bi\u0101nm\u01ceq\u00ec', 'trans': 'encoder'},\n{'word': '\u751f\u6210', 'pinyin': 'sh\u0113ngch\u00e9ng', 'trans': 'generate'},\n{'word': '\u5b58\u5728', 'pinyin': 'c\u00fanz\u00e0i', 'trans': 'exist'},\n{'word': '\u5197\u4f59', 'pinyin': 'r\u00f3ngy\u00fa', 'trans': 'redundancy'},\n{'word': '\u89e3\u51b3', 'pinyin': 'ji\u011bju\u00e9', 'trans': 'solve'},\n{'word': '\u63d0\u51fa', 'pinyin': 't\u00edch\u016b', 'trans': 'propose'},\n{'word': '\u65b9\u6cd5', 'pinyin': 'f\u0101ngf\u01ce', 'trans': 'method'},\n{'word': '\u9009\u62e9', 'pinyin': 'xu\u01cenz\u00e9', 'trans': 'select'},\n{'word': '\u8f93\u5165', 'pinyin': 'sh\u016br\u00f9', 'trans': 'input'},\n{'word': '\u51cf\u5c11', 'pinyin': 'ji\u01censh\u01ceo', 'trans': 'reduce'},\n{'word': '\u6548\u7387', 'pinyin': 'xi\u00e0ol\u01dc', 'trans': 'efficiency'},\n{'word': '\u5b9e\u9a8c', 'pinyin': 'sh\u00edy\u00e0n', 'trans': 'experiment'},\n{'word': '\u7ed3\u679c', 'pinyin': 'ji\u00e9gu\u01d2', 'trans': 'result'},\n{'word': '\u663e\u793a', 'pinyin': 'xi\u01censh\u00ec', 'trans': 'display'},\n{'word': '\u8bbe\u7f6e', 'pinyin': 'sh\u00e8zh\u00ec', 'trans': 'setting'},\n{'word': '\u4e4b\u524d', 'pinyin': 'zh\u012bqi\u00e1n', 'trans': 'before'},\n{'word': '\u6700\u4f73', 'pinyin': 'zu\u00ecji\u0101', 'trans': 'best'},\n{'word': '\u63d0\u9ad8', 'pinyin': 't\u00edg\u0101o', 'trans': 'improve'},\n{'word': '\u63a8\u7406', 'pinyin': 'tu\u012bl\u01d0', 'trans': 'inference'},\n{'word': '\u901f\u5ea6', 'pinyin': 's\u00f9d\u00f9', 'trans': 'speed'},\n{'word': '\u53d6\u5f97', 'pinyin': 'q\u01d4d\u00e9', 'trans': 'achieve'}]", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 264, "total_tokens": 1159, "completion_tokens": 895}}
[07.12.2024 12:42] Response: [{'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ju√©', 'trans': 'vision'},
{'word': 'ËØ≠Ë®Ä', 'pinyin': 'y«îy√°n', 'trans': 'language'},
{'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'},
{'word': 'ËøõÂ±ï', 'pinyin': 'j√¨nzh«én', 'trans': 'progress'},
{'word': 'Â¢ûÂä†', 'pinyin': 'zƒìngjiƒÅ', 'trans': 'increase'},
{'word': 'Ê†áËÆ∞', 'pinyin': 'biƒÅoj√¨', 'trans': 'marker'},
{'word': 'ÈïøÂ∫¶', 'pinyin': 'ch√°ngd√π', 'trans': 'length'},
{'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ngn√©ng', 'trans': 'performance'},
{'word': 'ËÆ°ÁÆó', 'pinyin': 'j√¨su√†n', 'trans': 'computation'},
{'word': 'ÊàêÊú¨', 'pinyin': 'ch√©ngbƒõn', 'trans': 'cost'},
{'word': 'Á†îÁ©∂', 'pinyin': 'y√°nji≈´', 'trans': 'research'},
{'word': 'ÂèëÁé∞', 'pinyin': 'fƒÅxi√†n', 'trans': 'discover'},
{'word': 'ÊµÅË°å', 'pinyin': 'li√∫x√≠ng', 'trans': 'popular'},
{'word': 'ÁºñÁ†ÅÂô®', 'pinyin': 'biƒÅnm«éq√¨', 'trans': 'encoder'},
{'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'},
{'word': 'Â≠òÂú®', 'pinyin': 'c√∫nz√†i', 'trans': 'exist'},
{'word': 'ÂÜó‰Ωô', 'pinyin': 'r√≥ngy√∫', 'trans': 'redundancy'},
{'word': 'Ëß£ÂÜ≥', 'pinyin': 'jiƒõju√©', 'trans': 'solve'},
{'word': 'ÊèêÂá∫', 'pinyin': 't√≠ch≈´', 'trans': 'propose'},
{'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'},
{'word': 'ÈÄâÊã©', 'pinyin': 'xu«énz√©', 'trans': 'select'},
{'word': 'ËæìÂÖ•', 'pinyin': 'sh≈´r√π', 'trans': 'input'},
{'word': 'ÂáèÂ∞ë', 'pinyin': 'ji«énsh«éo', 'trans': 'reduce'},
{'word': 'ÊïàÁéá', 'pinyin': 'xi√†ol«ú', 'trans': 'efficiency'},
{'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'},
{'word': 'ÁªìÊûú', 'pinyin': 'ji√©gu«í', 'trans': 'result'},
{'word': 'ÊòæÁ§∫', 'pinyin': 'xi«énsh√¨', 'trans': 'display'},
{'word': 'ËÆæÁΩÆ', 'pinyin': 'sh√®zh√¨', 'trans': 'setting'},
{'word': '‰πãÂâç', 'pinyin': 'zhƒ´qi√°n', 'trans': 'before'},
{'word': 'ÊúÄ‰Ω≥', 'pinyin': 'zu√¨jiƒÅ', 'trans': 'best'},
{'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'},
{'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'inference'},
{'word': 'ÈÄüÂ∫¶', 'pinyin': 's√πd√π', 'trans': 'speed'},
{'word': 'ÂèñÂæó', 'pinyin': 'q«îd√©', 'trans': 'achieve'}]
[07.12.2024 12:42] Mistral request. Model: mistral-large-latest. Prompt: Translate this text in English. Text:

ÊúÄËøëËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑËøõÂ±ïÈÄöËøáÂ¢ûÂä†ËßÜËßâÊ†áËÆ∞ÁöÑÈïøÂ∫¶Êù•ÊèêÈ´òÊÄßËÉΩÔºå‰ΩÜËøô‰πüÂ¢ûÂä†‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊµÅË°åÁöÑËßÜËßâÁºñÁ†ÅÂô®ÁîüÊàêÁöÑËßÜËßâÊ†áËÆ∞‰∏≠Â≠òÂú®Â§ßÈáèÂÜó‰Ωô„ÄÇ‰∏∫Ëß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÁ†îÁ©∂‰∫∫ÂëòÊèêÂá∫‰∫ÜVisionZipÊñπÊ≥ïÔºåÈÄâÊã©‰∏ÄÁªÑÊúâÁî®ÁöÑËßÜËßâÊ†áËÆ∞ËæìÂÖ•ËØ≠Ë®ÄÊ®°ÂûãÔºåÂáèÂ∞ëÂÜó‰ΩôÂπ∂ÊèêÈ´òÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåVisionZipÂú®Âá†‰πéÊâÄÊúâËÆæÁΩÆ‰∏≠ÈÉΩÊØî‰πãÂâçÁöÑÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÈ´ò‰∫ÜËá≥Â∞ë5%ÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåËØ•ÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÊé®ÁêÜÈÄüÂ∫¶Ôºå‰ΩøLLaVA-Next 13BÊ®°ÂûãÊØîLLaVA-Next 7BÊ®°ÂûãÊé®ÁêÜÊõ¥Âø´ÔºåÂπ∂ÂèñÂæóÊõ¥Â•ΩÁöÑÁªìÊûú„ÄÇ
[07.12.2024 12:42] Mistral response. {"id": "9efa1da55b5e4a31b91c338bce2932e8", "object": "chat.completion", "created": 1733575354, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "Recent advancements in vision-language models have improved performance by increasing the length of visual tokens, but this has also increased computational costs. Research has found that popular visual encoders generate visual tokens with a significant amount of redundancy. To address this issue, researchers have proposed the VisionZip method, which selects a set of useful visual tokens to input into the language model, reducing redundancy and enhancing efficiency. Experimental results show that VisionZip improves performance by at least 5% over previous best methods in almost all settings. Additionally, this method significantly speeds up model inference, making the LLaVA-Next 13B model infer faster than the LLaVA-Next 7B model and achieve better results.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 230, "total_tokens": 385, "completion_tokens": 155}}
[07.12.2024 12:42] Response: Recent advancements in vision-language models have improved performance by increasing the length of visual tokens, but this has also increased computational costs. Research has found that popular visual encoders generate visual tokens with a significant amount of redundancy. To address this issue, researchers have proposed the VisionZip method, which selects a set of useful visual tokens to input into the language model, reducing redundancy and enhancing efficiency. Experimental results show that VisionZip improves performance by at least 5% over previous best methods in almost all settings. Additionally, this method significantly speeds up model inference, making the LLaVA-Next 13B model infer faster than the LLaVA-Next 7B model and achieve better results.
[07.12.2024 12:42] Renaming data file.
[07.12.2024 12:42] Renaming previous data. hf_papers.json to ./d/2024-12-06.json
[07.12.2024 12:42] Saving new data file.
[07.12.2024 12:42] Generating page.
[07.12.2024 12:42] Renaming previous page.
[07.12.2024 12:42] Renaming previous data. index.html to ./d/2024-12-06.html
[07.12.2024 12:42] [Experimental] Generating Chinese page for reading.
[07.12.2024 12:42] Chinese vocab [{'word': 'ËßÜËßâ', 'pinyin': 'sh√¨ju√©', 'trans': 'vision'}, {'word': 'ËØ≠Ë®Ä', 'pinyin': 'y«îy√°n', 'trans': 'language'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}, {'word': 'ËøõÂ±ï', 'pinyin': 'j√¨nzh«én', 'trans': 'progress'}, {'word': 'Â¢ûÂä†', 'pinyin': 'zƒìngjiƒÅ', 'trans': 'increase'}, {'word': 'Ê†áËÆ∞', 'pinyin': 'biƒÅoj√¨', 'trans': 'marker'}, {'word': 'ÈïøÂ∫¶', 'pinyin': 'ch√°ngd√π', 'trans': 'length'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√¨ngn√©ng', 'trans': 'performance'}, {'word': 'ËÆ°ÁÆó', 'pinyin': 'j√¨su√†n', 'trans': 'computation'}, {'word': 'ÊàêÊú¨', 'pinyin': 'ch√©ngbƒõn', 'trans': 'cost'}, {'word': 'Á†îÁ©∂', 'pinyin': 'y√°nji≈´', 'trans': 'research'}, {'word': 'ÂèëÁé∞', 'pinyin': 'fƒÅxi√†n', 'trans': 'discover'}, {'word': 'ÊµÅË°å', 'pinyin': 'li√∫x√≠ng', 'trans': 'popular'}, {'word': 'ÁºñÁ†ÅÂô®', 'pinyin': 'biƒÅnm«éq√¨', 'trans': 'encoder'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'}, {'word': 'Â≠òÂú®', 'pinyin': 'c√∫nz√†i', 'trans': 'exist'}, {'word': 'ÂÜó‰Ωô', 'pinyin': 'r√≥ngy√∫', 'trans': 'redundancy'}, {'word': 'Ëß£ÂÜ≥', 'pinyin': 'jiƒõju√©', 'trans': 'solve'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ch≈´', 'trans': 'propose'}, {'word': 'ÊñπÊ≥ï', 'pinyin': 'fƒÅngf«é', 'trans': 'method'}, {'word': 'ÈÄâÊã©', 'pinyin': 'xu«énz√©', 'trans': 'select'}, {'word': 'ËæìÂÖ•', 'pinyin': 'sh≈´r√π', 'trans': 'input'}, {'word': 'ÂáèÂ∞ë', 'pinyin': 'ji«énsh«éo', 'trans': 'reduce'}, {'word': 'ÊïàÁéá', 'pinyin': 'xi√†ol«ú', 'trans': 'efficiency'}, {'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'}, {'word': 'ÁªìÊûú', 'pinyin': 'ji√©gu«í', 'trans': 'result'}, {'word': 'ÊòæÁ§∫', 'pinyin': 'xi«énsh√¨', 'trans': 'display'}, {'word': 'ËÆæÁΩÆ', 'pinyin': 'sh√®zh√¨', 'trans': 'setting'}, {'word': '‰πãÂâç', 'pinyin': 'zhƒ´qi√°n', 'trans': 'before'}, {'word': 'ÊúÄ‰Ω≥', 'pinyin': 'zu√¨jiƒÅ', 'trans': 'best'}, {'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'inference'}, {'word': 'ÈÄüÂ∫¶', 'pinyin': 's√πd√π', 'trans': 'speed'}, {'word': 'ÂèñÂæó', 'pinyin': 'q«îd√©', 'trans': 'achieve'}]
[07.12.2024 12:42] Renaming previous Chinese page.
[07.12.2024 12:42] Renaming previous data. zh.html to ./d/2024-12-06_zh_reading_task.html
[07.12.2024 12:42] Writing Chinese reading task.
[07.12.2024 12:42] Writing result.
[07.12.2024 12:42] Renaming log file.
[07.12.2024 12:42] Renaming previous data. log.txt to ./logs/2024-12-07_last_log.txt
