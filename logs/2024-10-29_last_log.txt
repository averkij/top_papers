[28.10.2024 22:12] [Experimental] Generating an image for paper ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting.
[28.10.2024 22:12] [Experimental] Image for paper ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting already exists.
[28.10.2024 22:12] [Experimental] Generating an image for paper Continuous Speech Synthesis using per-token Latent Diffusion.
[28.10.2024 22:12] [Experimental] Image for paper Continuous Speech Synthesis using per-token Latent Diffusion already exists.
[28.10.2024 22:12] [Experimental] Generating an image for paper Teach Multimodal LLMs to Comprehend Electrocardiographic Images.
[28.10.2024 22:12] [Experimental] Image for paper Teach Multimodal LLMs to Comprehend Electrocardiographic Images already exists.
[28.10.2024 22:12] [Experimental] Generating an image for paper Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design.
[28.10.2024 22:12] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design' Text: 'The proliferation of large language models (LLMs) has led to the adoption of Mixture-of-Experts (MoE) architectures that dynamically leverage specialized subnetworks for improved efficiency and performance. Despite their benefits, MoE models face significant challenges during inference, including inefficient memory management and suboptimal batching, due to misaligned design choices between the model architecture and the system policies. Furthermore, the conventional approach of training MoEs from scratch is increasingly prohibitive in terms of cost. In this paper, we propose a novel framework Read-ME that transforms pre-trained dense LLMs into smaller MoE models (in contrast to "upcycling" generalist MoEs), avoiding the high costs of ground-up training. Our approach employs activation sparsity to extract experts. To compose experts, we examine the widely-adopted layer-wise router design and show its redundancy, and thus we introduce the pre-gating router decoupled from the MoE backbone that facilitates system-friendly pre-computing and lookahead scheduling, enhancing expert-aware batching and caching. Our codesign therefore addresses critical gaps on both the algorithmic and system fronts, establishing a scalable and efficient alternative for LLM inference in resource-constrained settings. Read-ME outperforms other popular open-source dense models of similar scales, achieving improvements of up to 10.1% on MMLU, and improving mean end-to-end latency up to 6.1%. Codes are available at: https://github.com/VITA-Group/READ-ME.'
[28.10.2024 22:12] Response: **Prompt:** Create a surrealistic linear art piece on a white background depicting a large, intricate machine with flowing gears and cogs that resemble brains, symbolizing the complexity of large language models. Intertwine vibrant lines representing dynamic subnetworks, branching out into various pathways, each leading to a unique small expert figure, illustrating the concept of Mixture-of-Experts. Include abstract representations of memory management and batching, such as floating cubes and spirals that hint at inefficiency. Above the machine, place a translucent banner that reads: **"Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design"**. In the background, faintly show a landscape of cloud-like formations representing pre-trained models, with sparks of light indicating activation sparsity.
[28.10.2024 22:12] Generating image by prompt: **Prompt:** Create a surrealistic linear art piece on a white background depicting a large, intricate machine with flowing gears and cogs that resemble brains, symbolizing the complexity of large language models. Intertwine vibrant lines representing dynamic subnetworks, branching out into various pathways, each leading to a unique small expert figure, illustrating the concept of Mixture-of-Experts. Include abstract representations of memory management and batching, such as floating cubes and spirals that hint at inefficiency. Above the machine, place a translucent banner that reads: **"Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design"**. In the background, faintly show a landscape of cloud-like formations representing pre-trained models, with sparks of light indicating activation sparsity..
[28.10.2024 22:12] Saving generated image from https://fal.media/files/panda/JKpjO_b92j6DQ67H7iCEu.png to 94c8bf1991abe8b0.jpg.
[28.10.2024 22:12] [Experimental] Generating an image for paper FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality.
[28.10.2024 22:12] [Experimental] Image for paper FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality already exists.
[28.10.2024 22:12] [Experimental] Generating an image for paper Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data.
[28.10.2024 22:12] [Experimental] Image for paper Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data already exists.
[28.10.2024 22:12] [Experimental] Generating an image for paper Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance.
[28.10.2024 22:12] [Experimental] Image for paper Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance already exists.
[28.10.2024 22:12] [Experimental] Generating an image for paper MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark.
[28.10.2024 22:12] [Experimental] Image for paper MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark already exists.
[29.10.2024 00:59] Read previous papers.
[29.10.2024 00:59] Get feed.
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.17856
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16048
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19008
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19123
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19355
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18558
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19168
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18889
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19133
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19730
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16090
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19290
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.17655
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.16270
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18076
[29.10.2024 00:59] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18912
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 0. Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. A key issue is the difficulty in smoothly connecting individual entities in low-level observations with abstract concepts required for planni...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 1. The success of autoregressive transformer models with discrete tokens has inspired quantization-based approaches for continuous modalities, though these often limit reconstruction quality. We therefore introduce SALAD, a per-token latent diffusion model for zero-shot text-to-speech, that operates on...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 2. The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for assessing cardiac conditions. Existing automatic interpretation methods suffer from limited generalizability, focusing on a narrow range of cardiac conditions, and typically depend on raw physiological signals, which may no...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 3. The proliferation of large language models (LLMs) has led to the adoption of Mixture-of-Experts (MoE) architectures that dynamically leverage specialized subnetworks for improved efficiency and performance. Despite their benefits, MoE models face significant challenges during inference, including in...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 4. In this paper, we present \textit{FasterCache}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that directly reusing adjacent-step features degrades video quality due to...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 5. Vision-Language Models (VLMs) have recently made significant progress, but the limited scale and quality of open-source instruction data hinder their performance compared to closed-source models. In this work, we address this limitation by introducing Infinity-MM, a large-scale multimodal instructio...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 6. The ability to comprehend audio--which includes speech, non-speech sounds, and music--is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex r...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 7. NLP benchmarks rely on standardized datasets for training and evaluating models and are crucial for advancing the field. Traditionally, expert annotations ensure high-quality labels; however, the cost of expert annotation does not scale well with the growing demand for larger datasets required by mo...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 8. Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, directly collecting human preferences can be expensive, time-consuming, and can have high variance. An appealing alternative is to distill preferences from LMs as a source of synthetic an...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 9. Transformers, the backbone of modern large language models (LLMs), face inherent architectural limitations that impede their reasoning capabilities. Unlike recurrent networks, Transformers lack recurrent connections, confining them to constant-depth computation. This restriction places them in the c...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 10. Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model behaviour, such as reliance on outdated or incorrect infor...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 11. Recent studies have identified one aggravating factor of LLM hallucinations as the knowledge inconsistency between pre-training and fine-tuning, where unfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong outputs. In this paper, we propose a novel fine-tuning strategy called P...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 12. Bias assessment of news sources is paramount for professionals, organizations, and researchers who rely on truthful evidence for information gathering and reporting. While certain bias indicators are discernible from content analysis, descriptors like political bias and fake news pose greater challe...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 13. The ability to adapt beliefs or behaviors in response to unexpected outcomes, reflection, is fundamental to intelligent systems' interaction with the world. From a cognitive science perspective, this serves as a core principle of intelligence applicable to both human and AI systems. To address the d...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 14. Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative sel...
[29.10.2024 00:59] ********************************************************************************
[29.10.2024 00:59] Abstract 15. Videos of robots interacting with objects encode rich information about the objects' dynamics. However, existing video prediction approaches typically do not explicitly account for the 3D information from videos, such as robot actions and objects' 3D states, limiting their use in real-world robotic ...
[29.10.2024 00:59] Read previous papers.
[29.10.2024 00:59] Generating reviews via LLM API.
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#agents", "#cv", "#multimodal", "#rl"], "emoji": "🚀", "ru": {"title": "Визуально-временной контекст открывает новые горизонты для VLM в воплощенном ИИ", "desc": "Статья представляет новый подход к использованию визуально-языковых моделей (VLM) в задачах принятия решений в открытых с
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#audio", "#diffusion", "#benchmark"], "emoji": "🗣️", "ru": {"title": "SALAD: непрерывная диффузия для качественного синтеза речи", "desc": "SALAD - это новая модель латентной диффузии для синтеза речи без предварительного обучения. Она работает с непрерывными представлениями, что по
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#medicine"], "emoji": "❤️", "ru": {"title": "PULSE: Революция в автоматическом анализе ЭКГ с помощью мультимодальных языковых моделей", "desc": "Статья представляет новый подход к автоматической интерпретации ЭКГ с использованием мультимодаль
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#architecture", "#inference", "#training", "#optimization"], "emoji": "🧠", "ru": {"title": "Эффективные MoE модели из предобученных плотных языковых моделей", "desc": "Авторы предлагают новый фреймворк Read-ME для преобразования предобученных плотных языковых моделей в меньшие модел
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#video", "#diffusion", "#inference"], "emoji": "🚀", "ru": {"title": "FasterCache: Ускорение видео-диффузии без потери качества", "desc": "FasterCache - это новая стратегия для ускорения вывода видео-моделей диффузии без дополнительного обучения. Метод анализирует существующие кэш-по
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#dataset", "#data", "#multimodal"], "emoji": "🔍", "ru": {"title": "Большие данные - ключ к улучшению открытых визуально-языковых моделей", "desc": "Исследователи представили Infinity-MM - крупномасштабный мультимодальный набор данных с инструкциями, содержащий 40 миллионов образцов.
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#benchmark", "#audio", "#multimodal"], "emoji": "🎵", "ru": {"title": "MMAU: новый рубеж в понимании аудио для ИИ", "desc": "MMAU - это новый эталонный тест для оценки моделей мультимодального понимания аудио. Он включает 10 тысяч аудиоклипов с вопросами и ответами, охватывающими реч
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark"], "emoji": "🏷️", "ru": {"title": "LLM как судья: повышение качества аннотаций в NLP-датасетах", "desc": "Статья описывает использование больших языковых моделей (LLM) для повышения качества аннотаций в наборах данных для NLP-задач. Авторы сравнивают 
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#rlhf", "#dataset", "#data", "#optimization"], "emoji": "🔀", "ru": {"title": "Оптимальное сочетание человеческого и машинного интеллекта для обучения ИИ", "desc": "Статья представляет новый подход к обучению языковых моделей на основе предпочтений. Авторы предлагают гибридный метод,
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#reasoning", "#architecture"], "emoji": "🧮", "ru": {"title": "Токенизация как ключ к улучшению рассуждений в больших языковых моделях", "desc": "Это исследование посвящено влиянию токенизации на способность больших языковых моделей (LLM) к подсчету. Авторы утверждают, что архитектур
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#interpretability", "#alignment", "#hallucinations"], "emoji": "🧠", "ru": {"title": "Распознавание конфликтов знаний в больших языковых моделях", "desc": "Это исследование посвящено способности больших языковых моделей (LLM) распознавать конфликты между знаниями, хранящимися в их па
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#hallucinations", "#alignment"], "emoji": "🧠", "ru": {"title": "Prereq-Tune: Уменьшение галлюцинаций в языковых моделях путем разделения обучения навыкам и знаниям", "desc": "В статье представлена новая стратегия дообучения языковых моделей под названием Prereq-Tune, направленная на
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#dataset", "#rl", "#benchmark", "#data"], "emoji": "🔍", "ru": {"title": "Анализ новостных источников с помощью ИИ: новый взгляд на медиаландшафт", "desc": "В статье представлен метод оценки надежности новостных источников с использованием обучения с подкреплением. Авторы анализируют
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#benchmark", "#interpretability"], "emoji": "🤔", "ru": {"title": "Reflection-Bench: измеряя способность ИИ к самоанализу", "desc": "Исследователи представили Reflection-Bench - комплексный набор тестов для оценки способности больших языковых моделей (LLM) к рефлексии. Бенчмарк включ
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#rl", "#rlhf"], "emoji": "🤖", "ru": {"title": "Эффективное исследование в RL с помощью непомеченных данных", "desc": "Эта статья представляет метод SUPE для эффективного исследования в обучении с подкреплением, используя непомеченные предварительные данные траекторий. SUPE сначала и
[29.10.2024 00:59] Using data from previous issue: {"categories": ["#3d", "#robots", "#cv"], "emoji": "🤖", "ru": {"title": "3D-динамика объектов из видео для управления роботами", "desc": "Эта статья представляет новый подход к обучению динамики объектов непосредственно из многоракурсных RGB-видео, учитывая траектории действий робота. Авторы использ
[29.10.2024 00:59] Loading Chinese text from previous data.
[29.10.2024 00:59] Renaming data file.
[29.10.2024 00:59] Renaming previous data. hf_papers.json to ./d/2024-10-29.json
[29.10.2024 00:59] Saving new data file.
[29.10.2024 00:59] Generating page.
[29.10.2024 00:59] Renaming previous page.
[29.10.2024 00:59] Renaming previous data. index.html to ./d/2024-10-29.html
[29.10.2024 00:59] [Experimental] Generating Chinese page for reading.
[29.10.2024 00:59] Chinese vocab [{'word': '视觉语言模型', 'pinyin': 'shìjué yǔyán móxíng', 'trans': 'visual language models'}, {'word': '开放环境', 'pinyin': 'kāifàng huánjìng', 'trans': 'open environments'}, {'word': '决策能力', 'pinyin': 'juécè nénglì', 'trans': 'decision-making ability'}, {'word': '多模式任务', 'pinyin': 'duō móshì rènwù', 'trans': 'multimodal tasks'}, {'word': '表现出色', 'pinyin': 'biǎoxiàn chūsè', 'trans': 'perform well'}, {'word': '面临挑战', 'pinyin': 'miànlín tiǎozhàn', 'trans': 'face challenges'}, {'word': '低层次观察', 'pinyin': 'dī céngcì guānchá', 'trans': 'low-level observations'}, {'word': '单个实体', 'pinyin': 'dān gè shítǐ', 'trans': 'individual entities'}, {'word': '规划所需的抽象概念', 'pinyin': 'guīhuà suǒxū de chōuxiàng gàiniàn', 'trans': 'abstract concepts required for planning'}, {'word': '视觉时间上下文提示', 'pinyin': 'shìjué shíjiān shàngxiàwén tíshì', 'trans': 'visual temporal context prompts'}, {'word': '新通信协议', 'pinyin': 'xīn tōngxìn xiéyì', 'trans': 'new communication protocol'}, {'word': '过去和现在的观察', 'pinyin': 'guòqù hé xiànzài de guānchá', 'trans': 'past and present observations'}, {'word': '对象分割', 'pinyin': 'duìxiàng fēngé', 'trans': 'object segmentation'}, {'word': '指导策略与环境的交互', 'pinyin': 'zhǐdǎo cèlüè yǔ huánjìng de jiāohù', 'trans': 'guide the interaction of strategies with the environment'}, {'word': '代理', 'pinyin': 'dàilǐ', 'trans': 'agent'}, {'word': '完成以前无法实现的任务', 'pinyin': 'wánchéng yǐqián wúfǎ shíxiàn de rènwù', 'trans': 'complete tasks that were previously impossible'}, {'word': '依赖空间理解的复杂任务', 'pinyin': 'yīlài kōngjiān lǐjiě de fùzá rènwù', 'trans': 'complex tasks that rely on spatial understanding'}]
[29.10.2024 00:59] Renaming previous Chinese page.
[29.10.2024 00:59] Renaming previous data. zh.html to ./d/2024-10-28_zh_reading_task.html
[29.10.2024 00:59] Writing result.
[29.10.2024 00:59] Writing Chinese reading task.
[29.10.2024 00:59] Renaming log file.
[29.10.2024 00:59] Renaming previous data. log.txt to ./logs/2024-10-29_last_log.txt
