[29.10.2024 09:44] [Experimental] Generating an image for paper Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation.
[29.10.2024 09:44] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation' Text: 'We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for Polish language processing. Trained on curated Polish corpora, this model addresses key challenges in language model development through innovative techniques. These include Weighted Instruction Cross-Entropy Loss, which balances the learning of different instruction types, and Adaptive Learning Rate, which dynamically adjusts the learning rate based on training progress. To evaluate performance, we created the Open PL LLM Leaderboard and Polish MT-Bench, novel frameworks assessing various NLP tasks and conversational abilities. Bielik 7B v0.1 demonstrates significant improvements, achieving a 9 percentage point increase in average score compared to Mistral-7B-v0.1 on the RAG Reader task. It also excels in the Polish MT-Bench, particularly in Reasoning (6.15/10) and Role-playing (7.83/10) categories. This model represents a substantial advancement in Polish language AI, offering a powerful tool for diverse linguistic applications and setting new benchmarks in the field.'
[29.10.2024 09:44] Response: **Image Prompt:** Create a linear art composition on a white background that reflects the themes of language, innovation, and evaluation in a surreal manner. Visualize the concept of a '7-billion-parameter generative text model' as a giant, luminous brain made of intertwined letters and symbols, floating above a vibrant landscape of Polish cultural elements, such as traditional architecture and abstract representations of language. Incorporate surreal elements like clocks melting into books and birds shaped like speech bubbles, signifying the fluidity of time and communication. 

**Label Text:** "Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation"
[29.10.2024 09:44] Generating image by prompt: **Image Prompt:** Create a linear art composition on a white background that reflects the themes of language, innovation, and evaluation in a surreal manner. Visualize the concept of a '7-billion-parameter generative text model' as a giant, luminous brain made of intertwined letters and symbols, floating above a vibrant landscape of Polish cultural elements, such as traditional architecture and abstract representations of language. Incorporate surreal elements like clocks melting into books and birds shaped like speech bubbles, signifying the fluidity of time and communication. 

**Label Text:** "Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation".
[29.10.2024 09:44] Saving generated image from https://fal.media/files/kangaroo/u1ZRBmXusj4JxO4bxfPMo.png to c68d615cedcd651e.jpg.
[29.10.2024 09:44] [Experimental] Generating an image for paper AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant.
[29.10.2024 09:44] [Experimental] Image for paper AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant already exists.
[29.10.2024 09:44] [Experimental] Generating an image for paper GPT-4o System Card.
[29.10.2024 09:44] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'GPT-4o System Card' Text: 'GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.'
[29.10.2024 09:44] Response: **Prompt:** Create a linear art image on a white background featuring a surrealistic landscape where elements of text, audio, image, and video intertwine in abstract forms. Include floating geometric shapes representing neural networks, clock faces symbolizing response times, and fragmented text flowing through the scene. In the foreground, depict a large, surreal object resembling a book or card, labeled with the title: 'GPT-4o System Card'. The surroundings should evoke a sense of modernity and futuristic technology, with distorted representations of communication devices.
[29.10.2024 09:44] Generating image by prompt: **Prompt:** Create a linear art image on a white background featuring a surrealistic landscape where elements of text, audio, image, and video intertwine in abstract forms. Include floating geometric shapes representing neural networks, clock faces symbolizing response times, and fragmented text flowing through the scene. In the foreground, depict a large, surreal object resembling a book or card, labeled with the title: 'GPT-4o System Card'. The surroundings should evoke a sense of modernity and futuristic technology, with distorted representations of communication devices..
[29.10.2024 09:44] Saving generated image from https://fal.media/files/tiger/rE1OLhumdLIe0h8fOe8qm.png to 15d57a2a0852d31e.jpg.
[29.10.2024 09:44] [Experimental] Generating an image for paper LongReward: Improving Long-context Large Language Models with AI Feedback.
[29.10.2024 09:44] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'LongReward: Improving Long-context Large Language Models with AI Feedback' Text: 'Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT models and leads to inherent limitations. In principle, reinforcement learning (RL) with appropriate reward signals can further enhance models' capacities. However, how to obtain reliable rewards in long-context scenarios remains unexplored. To this end, we propose LongReward, a novel method that utilizes an off-the-shelf LLM to provide rewards for long-context model responses from four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness, each with a carefully designed assessment pipeline. By combining LongReward and offline RL algorithm DPO, we are able to effectively improve long-context SFT models. Our experiments indicate that LongReward not only significantly improves models' long-context performance but also enhances their ability to follow short instructions. We also find that long-context DPO with LongReward and conventional short-context DPO can be used together without hurting either one's performance.'
[29.10.2024 09:44] Response: **Prompt:** Create a surrealistic linear art piece on a white background that visually represents the intricate relationship between long-context language models and AI feedback. Include abstract representations of the four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness, depicted as ethereal figures or symbols interacting with elongated, flowing text that spirals into an infinite loop, suggesting the complexity and continuity of thought. Central to the composition, incorporate an oversized, stylized label that reads: ‚ÄúLongReward: Enhancing Long-context Models Through AI Feedback‚Äù to serve as the title of the artwork.
[29.10.2024 09:44] Generating image by prompt: **Prompt:** Create a surrealistic linear art piece on a white background that visually represents the intricate relationship between long-context language models and AI feedback. Include abstract representations of the four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness, depicted as ethereal figures or symbols interacting with elongated, flowing text that spirals into an infinite loop, suggesting the complexity and continuity of thought. Central to the composition, incorporate an oversized, stylized label that reads: ‚ÄúLongReward: Enhancing Long-context Models Through AI Feedback‚Äù to serve as the title of the artwork..
[29.10.2024 09:44] Saving generated image from https://fal.media/files/panda/V31shZGwHV0mXGWILc8gK.png to 0ff5d39896cdfbbe.jpg.
[29.10.2024 09:44] [Experimental] Generating an image for paper DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation.
[29.10.2024 09:44] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation' Text: 'Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets. To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model. GenIR, our pioneering contribution, is a dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only a few thousand images and thus offer limited generalizability for larger models. GenIR streamlines the process into three stages: image-text pair construction, dual-prompt based fine-tuning, and data generation & filtering. This approach circumvents the laborious data crawling process, ensuring copyright compliance and providing a cost-effective, privacy-safe solution for IR dataset construction. The result is a large-scale dataset of one million high-quality images. Our second contribution, DreamClear, is a DiT-based image restoration model. It utilizes the generative priors of text-to-image (T2I) diffusion models and the robust perceptual capabilities of multi-modal large language models (MLLMs) to achieve photorealistic restoration. To boost the model's adaptability to diverse real-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM). It employs token-wise degradation priors to dynamically integrate various restoration experts, thereby expanding the range of degradations the model can address. Our exhaustive experiments confirm DreamClear's superior performance, underlining the efficacy of our dual strategy for real-world image restoration. Code and pre-trained models will be available at: https://github.com/shallowdream204/DreamClear.'
[29.10.2024 09:44] Response: **Prompt:** Create a surrealistic linear art piece on a white background that visually represents the concepts of "high-capacity image restoration" and "privacy-safe dataset curation." Incorporate abstract forms representing dual strategies: a flowing river symbolizing GenIR's data curation pipeline, winding through a forest of pixelated trees that suggest the expansive nature of a million high-quality images. Include dreamlike elements, such as floating clocks and distorted landscapes, to illustrate the notion of adapting to various real-world degradations. Add a prominent label in the center of the image with the title: **'DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation'**
[29.10.2024 09:44] Generating image by prompt: **Prompt:** Create a surrealistic linear art piece on a white background that visually represents the concepts of "high-capacity image restoration" and "privacy-safe dataset curation." Incorporate abstract forms representing dual strategies: a flowing river symbolizing GenIR's data curation pipeline, winding through a forest of pixelated trees that suggest the expansive nature of a million high-quality images. Include dreamlike elements, such as floating clocks and distorted landscapes, to illustrate the notion of adapting to various real-world degradations. Add a prominent label in the center of the image with the title: **'DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation'**.
[29.10.2024 09:44] Saving generated image from https://fal.media/files/tiger/QcCB53c-x9l41z796sPGa.png to ebdde1eca3a0b04c.jpg.
[29.10.2024 10:13] Read previous papers.
[29.10.2024 10:13] Get feed.
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18565
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18603
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21276
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21252
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21169
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18666
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20011
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20474
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20280
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19313
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21220
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21264
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20290
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20220
[29.10.2024 10:13] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20636
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 0. We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for Polish language processing. Trained on curated Polish corpora, this model addresses key challenges in language model development through innovative techniques. These include Weighted Instruction Cross-Entropy Loss, which ba...
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 1. Digital agents capable of automating complex computer tasks have attracted considerable attention due to their immense potential to enhance human-computer interaction. However, existing agent methods exhibit deficiencies in their generalization and specialization capabilities, especially in handling...
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 2. GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural netw...
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 3. Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT models and leads to inherent limitations. In principle, reinf...
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 4. Document parsing is essential for converting unstructured and semi-structured documents-such as contracts, academic papers, and invoices-into structured, machine-readable data. Document parsing extract reliable structured data from unstructured inputs, providing huge convenience for numerous applica...
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 5. Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets. To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (Di...
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 6. Small Language Models (SLMs) have become increasingly important due to their efficiency and performance to perform various language tasks with minimal computational resources, making them ideal for various settings including on-device, mobile, edge devices, among many others. In this article, we pre...
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 7. We introduce a novel training-free spatial grounding technique for text-to-image generation using Diffusion Transformers (DiT). Spatial grounding with bounding boxes has gained attention for its simplicity and versatility, allowing for enhanced user control in image generation. However, prior traini...
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 8. We introduce MarDini, a new family of video diffusion models that integrate the advantages of masked auto-regression (MAR) into a unified diffusion model (DM) framework. Here, MAR handles temporal planning, while DM focuses on spatial generation in an asymmetric network design: i) a MAR-based planni...
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 9. FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces...
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 10. Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This challenge is particularly pronounced for large vision-lang...
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 11. We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization s...
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 12. The safe and effective deployment of Large Language Models (LLMs) involves a critical step called alignment, which ensures that the model's responses are in accordance with human preferences. Prevalent alignment techniques, such as DPO, PPO and their variants, align LLMs by changing the pre-trained ...
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 13. Neural Fields have emerged as a transformative approach for 3D scene representation in computer vision and robotics, enabling accurate inference of geometry, 3D semantics, and dynamics from posed 2D data. Leveraging differentiable rendering, Neural Fields encompass both continuous implicit and expli...
[29.10.2024 10:13] ********************************************************************************
[29.10.2024 10:13] Abstract 14. This research tests the role of Large Language Models (LLMs) as formal second opinion tools in professional decision-making, particularly focusing on complex medical cases where even experienced physicians seek peer consultation. The work analyzed 183 challenging medical cases from Medscape over a 2...
[29.10.2024 10:13] Read previous papers.
[29.10.2024 10:13] Generating reviews via LLM API.
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark", "#multilingual", "#training", "#alignment", "#reasoning"], "emoji": "üáµüá±", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ–ª—å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞: –º–æ—â–Ω–∞—è 7B-–º–æ–¥–µ–ª—å Bielik", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Bielik 7B v0.1 - —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª—å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "AgentStore: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "AgentStore - —ç—Ç–æ –Ω–æ–≤–∞—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∑–∞–¥–∞—á, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –º–∞–≥–∞–∑–∏–Ω–∞ 
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#multimodal", "#alignment", "#medicine", "#interpretability", "#ethics"], "emoji": "ü§ñ", "ru": {"title": "GPT-4o: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò", "desc": "GPT-4o - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –∞—É–¥–∏–æ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤–∏–¥–µ–æ. –û–Ω–∞ –æ–±—É—á–µ–Ω–∞ —Å–∫–≤–æ–∑–Ω
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#rl", "#rlhf", "#long_context"], "emoji": "üìè", "ru": {"title": "LongReward: –£–ª—É—á—à–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ LongReward –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–æ—Ç–æ–≤—É—é –±–æ–ª—å—à—É—é —è
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#dataset", "#data", "#survey", "#multimodal"], "emoji": "üìÑ", "ru": {"title": "–ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: –æ—Ç –º–æ–¥—É–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏, –æ—Ç –º–æ–¥—É–ª—å–Ω
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#dataset", "#data", "#cv", "#diffusion"], "emoji": "üñºÔ∏è", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –æ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç GenIR
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#survey", "#architecture", "#inference", "#benchmark", "#edge_computing"], "emoji": "üß†", "ru": {"title": "–ú–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –æ–±–∑–æ—Ä –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (SLM), –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –≤—Å–µ –±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#benchmark"], "emoji": "üñºÔ∏è", "ru": {"title": "–¢–æ—á–Ω–∞—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –ø—Ä–∏–≤—è–∑–∫–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Diffusion Transf
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#video", "#diffusion"], "emoji": "üé¨", "ru": {"title": "MarDini: –ì–∏–±–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π –∏ –¥–∏—Ñ—Ñ—É–∑–∏–µ–π", "desc": "MarDini - —ç—Ç–æ –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ, –æ–±—ä–µ–¥–∏–Ω—è—é—â–µ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (MAR) –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (DM).
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üöÄ", "ru": {"title": "COAT: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –ø–∞–º—è—Ç–∏", "desc": "COAT - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ FP8, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –≤
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#rag", "#agents", "#cv", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ó—Ä–µ–Ω–∏–µ –ò–ò: –æ—Ç –Ω–µ–∑–Ω–∞–Ω–∏—è –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Vision Search Assistant - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –û–Ω –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#video", "#multimodal", "#benchmark", "#architecture"], "emoji": "üé¨", "ru": {"title": "LARP: –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "LARP - —ç—Ç–æ –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤–∏–¥–µ–æ, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –≤ 
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#rlhf", "#alignment", "#inference"], "emoji": "üöÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ LLM –±–µ–∑ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Speculative Rejection. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#3d", "#robots", "#survey"], "emoji": "ü§ñ", "ru": {"title": "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ –ø–æ–ª—è: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-–≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ —Ä–æ–±–æ—Ç–æ–≤", "desc": "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ –ø–æ–ª—è - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é 3D-—Å—Ü–µ–Ω –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏ –∏ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ. –û–Ω–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç —Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –≥–µ–æ–º–µ—Ç—Ä–∏—é, 3D-—Å–µ–º–∞–Ω—Ç–∏–∫—É –∏ –¥–∏–Ω–∞–º–∏–∫—É
[29.10.2024 10:13] Using data from previous issue: {"categories": ["#medicine", "#benchmark", "#alignment"], "emoji": "ü©∫", "ru": {"title": "LLM –∫–∞–∫ –≤—Ç–æ—Ä–æ–µ –º–Ω–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Ç–æ—Ä–æ–≥
[29.10.2024 10:13] Loading Chinese text from previous data.
[29.10.2024 10:13] Renaming data file.
[29.10.2024 10:13] Renaming previous data. hf_papers.json to ./d/2024-10-29.json
[29.10.2024 10:13] Saving new data file.
[29.10.2024 10:13] Generating page.
[29.10.2024 10:13] Renaming previous page.
[29.10.2024 10:13] Renaming previous data. index.html to ./d/2024-10-29.html
[29.10.2024 10:13] [Experimental] Generating Chinese page for reading.
[29.10.2024 10:13] Chinese vocab [{'word': '‰ªãÁªç', 'pinyin': 'ji√®sh√†o', 'trans': 'introduce'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}, {'word': 'ÂàõÊñ∞', 'pinyin': 'chu√†ngxƒ´n', 'trans': 'innovative'}, {'word': 'ÊäÄÊúØ', 'pinyin': 'j√¨sh√π', 'trans': 'technology'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πnli√†n', 'trans': 'train'}, {'word': 'ÊùÉÈáç', 'pinyin': 'qu√°nzh√≤ng', 'trans': 'weight'}, {'word': 'Êåá‰ª§', 'pinyin': 'zh«êl√¨ng', 'trans': 'instruction'}, {'word': '‰∫§ÂèâÁÜµ', 'pinyin': 'jiƒÅochƒÅ shƒÅng', 'trans': 'cross-entropy'}, {'word': 'ÊçüÂ§±', 'pinyin': 's«înshƒ´', 'trans': 'loss'}, {'word': 'Ëá™ÈÄÇÂ∫î', 'pinyin': 'z√¨sh√¨y√¨ng', 'trans': 'adaptive'}, {'word': 'Â≠¶‰π†Áéá', 'pinyin': 'xu√©x√≠l«ú', 'trans': 'learning rate'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ngg≈´', 'trans': 'evaluate'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√≠ngn√©ng', 'trans': 'performance'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®nw√π', 'trans': 'task'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'}, {'word': 'ÊîπËøõ', 'pinyin': 'g«éij√¨n', 'trans': 'improvement'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': 'ËßíËâ≤', 'pinyin': 'ju√©s√®', 'trans': 'role'}, {'word': 'ÊâÆÊºî', 'pinyin': 'b√†ny«én', 'trans': 'play'}, {'word': 'È¢ÜÂüü', 'pinyin': 'l«êngy√π', 'trans': 'field'}, {'word': 'ÈáçÂ§ß', 'pinyin': 'zh√≤ngd√†', 'trans': 'major'}, {'word': 'ËøõÂ±ï', 'pinyin': 'j√¨nzh«én', 'trans': 'progress'}]
[29.10.2024 10:13] Renaming previous Chinese page.
[29.10.2024 10:13] Renaming previous data. zh.html to ./d/2024-10-28_zh_reading_task.html
[29.10.2024 10:13] Writing result.
[29.10.2024 10:13] Writing Chinese reading task.
[29.10.2024 10:13] Renaming log file.
[29.10.2024 10:13] Renaming previous data. log.txt to ./logs/2024-10-29_last_log.txt
