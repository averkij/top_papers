[29.10.2024 14:12] [Experimental] Generating an image for paper Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation.
[29.10.2024 14:12] [Experimental] Image for paper Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation already exists.
[29.10.2024 14:12] [Experimental] Generating an image for paper AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant.
[29.10.2024 14:12] [Experimental] Image for paper AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant already exists.
[29.10.2024 14:12] [Experimental] Generating an image for paper GPT-4o System Card.
[29.10.2024 14:12] [Experimental] Image for paper GPT-4o System Card already exists.
[29.10.2024 14:12] [Experimental] Generating an image for paper Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction.
[29.10.2024 14:12] [Experimental] Image for paper Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction already exists.
[29.10.2024 14:12] [Experimental] Generating an image for paper LongReward: Improving Long-context Large Language Models with AI Feedback.
[29.10.2024 14:12] [Experimental] Image for paper LongReward: Improving Long-context Large Language Models with AI Feedback already exists.
[29.10.2024 14:12] [Experimental] Generating an image for paper MarDini: Masked Autoregressive Diffusion for Video Generation at Scale.
[29.10.2024 14:12] [Experimental] Image for paper MarDini: Masked Autoregressive Diffusion for Video Generation at Scale already exists.
[29.10.2024 14:12] [Experimental] Generating an image for paper DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation.
[29.10.2024 14:12] [Experimental] Image for paper DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation already exists.
[29.10.2024 14:12] [Experimental] Generating an image for paper GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation.
[29.10.2024 14:12] OpenAI request. Model: gpt-4o-mini. Prompt: Write a text with image prompt in style of surrealism and modern art based on the following paper. Use key themes and elements from it. Add instruction to write a text that reads as brief paper title as a label on some object on an image. Style: linear art on white background. Return only prompt and nothing else. Title: 'GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation' Text: 'We introduce a novel training-free spatial grounding technique for text-to-image generation using Diffusion Transformers (DiT). Spatial grounding with bounding boxes has gained attention for its simplicity and versatility, allowing for enhanced user control in image generation. However, prior training-free approaches often rely on updating the noisy image during the reverse diffusion process via backpropagation from custom loss functions, which frequently struggle to provide precise control over individual bounding boxes. In this work, we leverage the flexibility of the Transformer architecture, demonstrating that DiT can generate noisy patches corresponding to each bounding box, fully encoding the target object and allowing for fine-grained control over each region. Our approach builds on an intriguing property of DiT, which we refer to as semantic sharing. Due to semantic sharing, when a smaller patch is jointly denoised alongside a generatable-size image, the two become "semantic clones". Each patch is denoised in its own branch of the generation process and then transplanted into the corresponding region of the original noisy image at each timestep, resulting in robust spatial grounding for each bounding box. In our experiments on the HRS and DrawBench benchmarks, we achieve state-of-the-art performance compared to previous training-free spatial grounding approaches.'
[29.10.2024 14:12] Response: **Image Prompt:** A linear art piece on a white background depicting a surreal landscape where abstract shapes of bounding boxes float in a dreamy, cloud-like atmosphere. Each bounding box contains fragmented images of objects that seem to merge and blend into one another, illustrating the concept of "semantic sharing." There are patches of color and texture that appear to be "transplanted" onto the canvas, creating a sense of depth and diffusion. The overall composition should evoke a feeling of controlled chaos, with a balance between structure (the bounding boxes) and fluidity (the dreamy patches). 

**Label for Object:** 'GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation'
[29.10.2024 14:12] Generating image by prompt: **Image Prompt:** A linear art piece on a white background depicting a surreal landscape where abstract shapes of bounding boxes float in a dreamy, cloud-like atmosphere. Each bounding box contains fragmented images of objects that seem to merge and blend into one another, illustrating the concept of "semantic sharing." There are patches of color and texture that appear to be "transplanted" onto the canvas, creating a sense of depth and diffusion. The overall composition should evoke a feeling of controlled chaos, with a balance between structure (the bounding boxes) and fluidity (the dreamy patches). 

**Label for Object:** 'GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation'.
[29.10.2024 14:12] Saving generated image from https://fal.media/files/lion/-QrJGoNYg3NYVtT5jhimx.png to 354e97e3a32e2063.jpg.
[29.10.2024 16:15] Read previous papers.
[29.10.2024 16:15] Get feed.
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18565
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21276
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18603
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21169
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21252
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20280
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20474
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18666
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20011
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.19313
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20290
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21220
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.21264
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20220
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.18481
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2406.10615
[29.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.20636
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 0. We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for Polish language processing. Trained on curated Polish corpora, this model addresses key challenges in language model development through innovative techniques. These include Weighted Instruction Cross-Entropy Loss, which ba...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 1. GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural netw...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 2. Digital agents capable of automating complex computer tasks have attracted considerable attention due to their immense potential to enhance human-computer interaction. However, existing agent methods exhibit deficiencies in their generalization and specialization capabilities, especially in handling...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 3. Document parsing is essential for converting unstructured and semi-structured documents-such as contracts, academic papers, and invoices-into structured, machine-readable data. Document parsing extract reliable structured data from unstructured inputs, providing huge convenience for numerous applica...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 4. Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT models and leads to inherent limitations. In principle, reinf...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 5. We introduce MarDini, a new family of video diffusion models that integrate the advantages of masked auto-regression (MAR) into a unified diffusion model (DM) framework. Here, MAR handles temporal planning, while DM focuses on spatial generation in an asymmetric network design: i) a MAR-based planni...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 6. We introduce a novel training-free spatial grounding technique for text-to-image generation using Diffusion Transformers (DiT). Spatial grounding with bounding boxes has gained attention for its simplicity and versatility, allowing for enhanced user control in image generation. However, prior traini...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 7. Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets. To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (Di...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 8. Small Language Models (SLMs) have become increasingly important due to their efficiency and performance to perform various language tasks with minimal computational resources, making them ideal for various settings including on-device, mobile, edge devices, among many others. In this article, we pre...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 9. FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 10. The safe and effective deployment of Large Language Models (LLMs) involves a critical step called alignment, which ensures that the model's responses are in accordance with human preferences. Prevalent alignment techniques, such as DPO, PPO and their variants, align LLMs by changing the pre-trained ...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 11. Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This challenge is particularly pronounced for large vision-lang...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 12. We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization s...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 13. Neural Fields have emerged as a transformative approach for 3D scene representation in computer vision and robotics, enabling accurate inference of geometry, 3D semantics, and dynamics from posed 2D data. Leveraging differentiable rendering, Neural Fields encompass both continuous implicit and expli...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 14. Efficiently deriving structured workflows from unannotated dialogs remains an underexplored and formidable challenge in computational linguistics. Automating this process could significantly accelerate the manual design of workflows in new domains and enable the grounding of large language models in...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 15. Given the high cost of collecting robotic data in the real world, sample efficiency is a consistently compelling pursuit in robotics. In this paper, we introduce SGRv2, an imitation learning framework that enhances sample efficiency through improved visual and action representations. Central to the ...
[29.10.2024 16:15] ********************************************************************************
[29.10.2024 16:15] Abstract 16. This research tests the role of Large Language Models (LLMs) as formal second opinion tools in professional decision-making, particularly focusing on complex medical cases where even experienced physicians seek peer consultation. The work analyzed 183 challenging medical cases from Medscape over a 2...
[29.10.2024 16:15] Read previous papers.
[29.10.2024 16:15] Generating reviews via LLM API.
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark", "#multilingual", "#alignment", "#reasoning"], "emoji": "üáµüá±", "ru": {"title": "–ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ–ª—å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞: –º–æ—â–Ω–∞—è 7B-–º–æ–¥–µ–ª—å Bielik", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Bielik 7B v0.1 - —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª—å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å 
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#multimodal", "#alignment", "#medicine", "#interpretability", "#ethics"], "emoji": "ü§ñ", "ru": {"title": "GPT-4o: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ò–ò", "desc": "GPT-4o - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –∞—É–¥–∏–æ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤–∏–¥–µ–æ. –û–Ω–∞ –æ–±—É—á–µ–Ω–∞ —Å–∫–≤–æ–∑–Ω
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#architecture"], "emoji": "ü§ñ", "ru": {"title": "AgentStore: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "AgentStore - —ç—Ç–æ –Ω–æ–≤–∞—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∑–∞–¥–∞—á, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –º–∞–≥–∞–∑–∏–Ω–∞ 
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#dataset", "#data", "#survey", "#multimodal"], "emoji": "üìÑ", "ru": {"title": "–ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: –æ—Ç –º–æ–¥—É–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±–∑–æ—Ä —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏, –æ—Ç –º–æ–¥—É–ª—å–Ω
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#rl", "#rlhf", "#long_context"], "emoji": "üìè", "ru": {"title": "LongReward: –£–ª—É—á—à–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ LongReward –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–æ—Ç–æ–≤—É—é –±–æ–ª—å—à—É—é —è
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#video", "#diffusion"], "emoji": "üé¨", "ru": {"title": "MarDini: –ì–∏–±–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π –∏ –¥–∏—Ñ—Ñ—É–∑–∏–µ–π", "desc": "MarDini - —ç—Ç–æ –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ, –æ–±—ä–µ–¥–∏–Ω—è—é—â–µ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (MAR) –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (DM).
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#diffusion", "#cv", "#benchmark"], "emoji": "üñºÔ∏è", "ru": {"title": "–¢–æ—á–Ω–∞—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –ø—Ä–∏–≤—è–∑–∫–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Diffusion Transf
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#dataset", "#data", "#cv", "#diffusion"], "emoji": "üñºÔ∏è", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –æ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç GenIR
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#survey", "#architecture", "#inference", "#benchmark", "#edge_computing"], "emoji": "üß†", "ru": {"title": "–ú–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç–∏", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –æ–±–∑–æ—Ä –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (SLM), –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –≤—Å–µ –±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üöÄ", "ru": {"title": "COAT: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –ø–∞–º—è—Ç–∏", "desc": "COAT - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ FP8, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –≤
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#rlhf", "#alignment", "#inference"], "emoji": "üöÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ LLM –±–µ–∑ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Speculative Rejection. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#rag", "#agents", "#cv", "#benchmark"], "emoji": "üîç", "ru": {"title": "–ó—Ä–µ–Ω–∏–µ –ò–ò: –æ—Ç –Ω–µ–∑–Ω–∞–Ω–∏—è –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Vision Search Assistant - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –û–Ω –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#video", "#multimodal", "#benchmark", "#architecture"], "emoji": "üé¨", "ru": {"title": "LARP: –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "LARP - —ç—Ç–æ –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤–∏–¥–µ–æ, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –≤ 
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#3d", "#robots", "#survey"], "emoji": "ü§ñ", "ru": {"title": "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ –ø–æ–ª—è: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ 3D-–≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ —Ä–æ–±–æ—Ç–æ–≤", "desc": "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ –ø–æ–ª—è - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é 3D-—Å—Ü–µ–Ω –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏ –∏ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ. –û–Ω–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç —Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –≥–µ–æ–º–µ—Ç—Ä–∏—é, 3D-—Å–µ–º–∞–Ω—Ç–∏–∫—É –∏ –¥–∏–Ω–∞–º–∏–∫—É
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#dataset", "#data", "#benchmark", "#multimodal", "#training"], "emoji": "üó∫Ô∏è", "ru": {"title": "D2F: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∫–∞—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤ –≤ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏–∑ –Ω–µ–∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ —Å –ø–æ–º
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#rl", "#agents", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–õ–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π - –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Ä–æ–±–æ—Ç–æ–≤", "desc": "SGRv2 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏, –ø–æ–≤—ã—à–∞—é—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –û–Ω –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π
[29.10.2024 16:15] Using data from previous issue: {"categories": ["#medicine", "#benchmark", "#alignment"], "emoji": "ü©∫", "ru": {"title": "LLM –∫–∞–∫ –≤—Ç–æ—Ä–æ–µ –º–Ω–µ–Ω–∏–µ: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Ç–æ—Ä–æ–≥
[29.10.2024 16:15] Loading Chinese text from previous data.
[29.10.2024 16:15] Renaming data file.
[29.10.2024 16:15] Renaming previous data. hf_papers.json to ./d/2024-10-29.json
[29.10.2024 16:15] Saving new data file.
[29.10.2024 16:15] Generating page.
[29.10.2024 16:15] Renaming previous page.
[29.10.2024 16:15] Renaming previous data. index.html to ./d/2024-10-29.html
[29.10.2024 16:15] [Experimental] Generating Chinese page for reading.
[29.10.2024 16:15] Chinese vocab [{'word': '‰ªãÁªç', 'pinyin': 'ji√®sh√†o', 'trans': 'introduce'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generate'}, {'word': 'Ê®°Âûã', 'pinyin': 'm√≥x√≠ng', 'trans': 'model'}, {'word': 'ÂàõÊñ∞', 'pinyin': 'chu√†ngxƒ´n', 'trans': 'innovative'}, {'word': 'ÊäÄÊúØ', 'pinyin': 'j√¨sh√π', 'trans': 'technology'}, {'word': 'ËÆ≠ÁªÉ', 'pinyin': 'x√πnli√†n', 'trans': 'train'}, {'word': 'ÊùÉÈáç', 'pinyin': 'qu√°nzh√≤ng', 'trans': 'weight'}, {'word': 'Êåá‰ª§', 'pinyin': 'zh«êl√¨ng', 'trans': 'instruction'}, {'word': '‰∫§ÂèâÁÜµ', 'pinyin': 'jiƒÅochƒÅ shƒÅng', 'trans': 'cross-entropy'}, {'word': 'ÊçüÂ§±', 'pinyin': 's«înshƒ´', 'trans': 'loss'}, {'word': 'Ëá™ÈÄÇÂ∫î', 'pinyin': 'z√¨sh√¨y√¨ng', 'trans': 'adaptive'}, {'word': 'Â≠¶‰π†Áéá', 'pinyin': 'xu√©x√≠l«ú', 'trans': 'learning rate'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ngg≈´', 'trans': 'evaluate'}, {'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'}, {'word': 'ÊÄßËÉΩ', 'pinyin': 'x√≠ngn√©ng', 'trans': 'performance'}, {'word': '‰ªªÂä°', 'pinyin': 'r√®nw√π', 'trans': 'task'}, {'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'}, {'word': 'ÊîπËøõ', 'pinyin': 'g«éij√¨n', 'trans': 'improvement'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': 'ËßíËâ≤', 'pinyin': 'ju√©s√®', 'trans': 'role'}, {'word': 'ÊâÆÊºî', 'pinyin': 'b√†ny«én', 'trans': 'play'}, {'word': 'È¢ÜÂüü', 'pinyin': 'l«êngy√π', 'trans': 'field'}, {'word': 'ÈáçÂ§ß', 'pinyin': 'zh√≤ngd√†', 'trans': 'major'}, {'word': 'ËøõÂ±ï', 'pinyin': 'j√¨nzh«én', 'trans': 'progress'}]
[29.10.2024 16:15] Renaming previous Chinese page.
[29.10.2024 16:15] Renaming previous data. zh.html to ./d/2024-10-28_zh_reading_task.html
[29.10.2024 16:15] Writing result.
[29.10.2024 16:15] Writing Chinese reading task.
[29.10.2024 16:15] Renaming log file.
[29.10.2024 16:15] Renaming previous data. log.txt to ./logs/2024-10-29_last_log.txt
