[09.09.2025 19:09] Read previous papers.
[09.09.2025 19:09] Generating top page (month).
[09.09.2025 19:09] Writing top page (month).
[09.09.2025 20:12] Read previous papers.
[09.09.2025 20:12] Get feed.
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06160
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06501
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06949
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06467
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01656
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06733
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06461
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06155
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06917
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.03516
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02108
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06945
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06631
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06493
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06861
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06786
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.05668
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06771
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06477
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06809
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06285
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04582
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.03740
[09.09.2025 20:12] Get page data from previous paper. URL: https://huggingface.co/papers/2509.00328
[09.09.2025 20:12] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.09.2025 20:12] No deleted papers detected.
[09.09.2025 20:12] Downloading and parsing papers (pdf, html). Total: 24.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06160.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06160.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06160.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06501.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06501.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06501.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06949.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06949.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06949.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06467.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06467.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06467.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.01656.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.01656.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.01656.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06733.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06733.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06733.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06461.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06461.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06461.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06155.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06155.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06155.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06917.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06917.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06917.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.03516.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.03516.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.03516.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.02108.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.02108.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.02108.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06945.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06945.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06945.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06631.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06631.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06631.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06493.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06493.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06493.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06861.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06861.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06861.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06786.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06786.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06786.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.05668.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.05668.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.05668.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06771.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06771.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06771.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06477.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06477.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06477.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06809.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06809.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06809.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.06285.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.06285.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.06285.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.04582.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.04582.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.04582.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.03740.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.03740.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.03740.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Downloading and parsing paper https://huggingface.co/papers/2509.00328.
[09.09.2025 20:12] Extra JSON file exists (./assets/json/2509.00328.json), skip PDF parsing.
[09.09.2025 20:12] Paper image links file exists (./assets/img_data/2509.00328.json), skip HTML parsing.
[09.09.2025 20:12] Success.
[09.09.2025 20:12] Enriching papers with extra data.
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 0. REER, a new paradigm for deep reasoning, uses reverse engineering to discover step-by-step reasoning processes, enabling a model to perform competitively on open-ended tasks.  					AI-generated summary 				 While the ``deep reasoning'' paradigm has spurred significant advances in verifiable domains ...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 1. WebExplorer, a data-driven approach for developing advanced web agents, achieves state-of-the-art performance in information-seeking tasks through systematic data generation and reinforcement learning.  					AI-generated summary 				 The paradigm of Large Language Models (LLMs) has increasingly shif...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 2. TraceRL enhances diffusion language models with trajectory-aware reinforcement learning, improving reasoning performance on complex tasks and enabling flexible sampling.  					AI-generated summary 				 We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language mod...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 3. DINOv3, a self-supervised vision transformer, demonstrates strong performance across various medical vision tasks without domain-specific pre-training, though it shows limitations in deeply specialized domains and does not consistently follow scaling laws in the medical domain.  					AI-generated su...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 4. ReVPT enhances multi-modal LLMs' visual reasoning capabilities using reinforcement learning, achieving state-of-the-art performance on visual benchmarks.  					AI-generated summary 				 Visual reasoning, a cornerstone of human intelligence, encompasses complex perceptual and logical processes essent...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 5. Reinforcement learning is explored as a foundational approach for training deep research systems, addressing limitations of supervised and preference alignment methods by optimizing policies for tool interaction and exploration.  					AI-generated summary 				 Deep research systems, agentic AI that ...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 6. Contrastive Attention Refinement for Visual Enhancement (CARVE) improves VLM performance by extracting task-relevant visual signals through attention contrasting, addressing issues with visual complexity and attention mechanisms.  					AI-generated summary 				 Vision-Language Models (VLMs) have dem...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 7. UniVerse-1, a unified audio-video generation model, uses a stitching of experts technique to combine pre-trained video and music models, ensuring accurate temporal alignment and producing high-quality audio-visual outputs.  					AI-generated summary 				 We introduce UniVerse-1, a unified, Veo-3-lik...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 8. Paper2Agent converts research papers into interactive AI agents to facilitate knowledge dissemination and enable complex scientific queries through natural language.  					AI-generated summary 				 We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2A...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 9. T2I-CoReBench is a benchmark that evaluates the composition and reasoning capabilities of text-to-image models using a comprehensive and complex set of prompts and checklist questions.  					AI-generated summary 				 Text-to-image (T2I) generation aims to synthesize images from textual prompts, whic...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 10. Multi-task learning (MTL) is often achieved by merging datasets before fine-tuning, but the growing availability of fine-tuned models has led to new approaches such as model merging via task arithmetic. A major challenge in this setting is task interference, which worsens as the number of tasks incr...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 11. Interleaving Reasoning Generation (IRG) framework alternates between text-based thinking and image synthesis to improve Text-to-Image generation, achieving state-of-the-art performance and enhanced visual quality.  					AI-generated summary 				 Unified multimodal understanding and generation models...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 12. Guided decoding methods in Retrieval-Augmented Generation (RAG) systems are evaluated for structured output generation, revealing performance variations across different prompting setups.  					AI-generated summary 				 The integration of Large Language Models (LLMs) into various applications has dr...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 13. BFS-Prover-V2 addresses scaling challenges in automated theorem proving by integrating a multi-turn off-policy RL framework and a planner-enhanced multi-agent search architecture, achieving state-of-the-art results on formal mathematics benchmarks.  					AI-generated summary 				 The integration of ...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 14. Test-time scaling does not consistently improve accuracy or reduce hallucinations in knowledge-intensive tasks, often leading to overconfident errors.  					AI-generated summary 				 Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has ...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 15. A new framework, R¬≤AI, is proposed to enhance AI safety through coevolution, combining resistance to known threats with resilience to unforeseen risks using fast and slow safe models and adversarial simulation.  					AI-generated summary 				 In this position paper, we address the persistent gap bet...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 16. Llama-GENBA-10B, a trilingual foundation model, addresses English-centric bias by balancing English, German, and Bavarian training, achieving strong cross-lingual performance and setting new benchmarks for Bavarian.  					AI-generated summary 				 We present Llama-GENBA-10B, a trilingual foundation ...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 17. A reasoning-augmented framework using a Large Vision-Language Model and a Tri-stream Cross-Reasoning Network achieves superior performance in detecting dark humor, identifying targets, and predicting intensity in multimodal memes.  					AI-generated summary 				 Dark humor in online memes poses uniq...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 18. MAS-Bench evaluates GUI-shortcut hybrid agents on mobile devices, demonstrating their superior performance and efficiency over GUI-only agents through a comprehensive benchmarking framework.  					AI-generated summary 				 To enhance the efficiency of GUI agents on various platforms like smartphones...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 19. A framework generates a large corpus of valid theorems using automated theorem proving to create symbolic training data for improving LLMs' mathematical reasoning.  					AI-generated summary 				 The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematic...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 20. DCReg addresses ill-conditioned LiDAR point cloud registration by detecting, characterizing, and mitigating degeneracies through Schur complement decomposition and a novel preconditioner.  					AI-generated summary 				 LiDAR point cloud registration is fundamental to robotic perception and navigati...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 21. Inpaint4Drag enhances drag-based image editing by decomposing it into pixel-space warping and inpainting, offering real-time performance and superior visual quality.  					AI-generated summary 				 Drag-based image editing has emerged as a powerful paradigm for intuitive image manipulation. However,...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 22. Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Exis...
[09.09.2025 20:12] ********************************************************************************
[09.09.2025 20:12] Abstract 23. A framework for interpreting and steering Vision-Language-Action (VLA) models via internal representations enables real-time behavioral control without fine-tuning or environment interaction.  					AI-generated summary 				 Vision-Language-Action (VLA) models are a promising path to realizing genera...
[09.09.2025 20:12] Read previous papers.
[09.09.2025 20:12] Generating reviews via LLM API.
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#dataset", "#architecture", "#training", "#rl"], "emoji": "üß†", "ru": {"title": "–û–±—Ä–∞—Ç–Ω–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–ª—É–±–æ–∫–æ–º—É –æ–±—É—á–µ–Ω–∏—é", "desc": "REER (Reverse-Engineered Reasoning) - —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –≤ –≥–ª—É–±–æ–∫–æ–º –æ–±—É—á–µ–Ω–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#agents", "#training", "#reasoning", "#agi", "#dataset", "#rl", "#long_context"], "emoji": "üï∏Ô∏è", "ru": {"title": "WebExplorer: –ú–∞–ª–µ–Ω—å–∫–∏–π –∞–≥–µ–Ω—Ç —Å –±–æ–ª—å—à–∏–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏", "desc": "WebExplorer - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∞–Ω–Ω—ã—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#training", "#math", "#rl", "#open_source", "#diffusion"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "TraceRL - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#transfer_learning", "#3d", "#cv", "#healthcare", "#benchmark", "#optimization", "#science"], "emoji": "ü©∫", "ru": {"title": "DINOv3: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏", "desc": "–ú–æ–¥–µ–ª—å DINOv3, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Vision Transformer, –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#optimization", "#benchmark", "#rl", "#cv"], "emoji": "üß†", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –ò–ò —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ ReVPT, —É–ª—É—á—à–∞—é—â–∏–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#rl", "#training", "#long_context", "#benchmark", "#survey", "#optimization", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∫–∞–∫ –æ—Å–Ω–æ–≤–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#cv", "#training", "#multimodal", "#open_source"], "emoji": "üîç", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ CARVE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VL
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#multimodal", "#video", "#audio", "#benchmark", "#open_source"], "emoji": "üé•", "ru": {"title": "UniVerse-1: —Å–∏–Ω–µ—Ä–≥–∏—è –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ –≤ –æ–¥–Ω–æ–º –º–æ–¥–µ–ª–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å UniVerse-1, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–µ—Ö–Ω–∏–∫—É \"stitching of experts\
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#agents", "#science", "#multimodal"], "emoji": "üß¨", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤", "desc": "Paper2Agent - —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤. –û–Ω–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –∫–æ–¥ —Å—Ç–∞—Ç—å–∏, 
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#reasoning", "#games"], "emoji": "üé®", "ru": {"title": "T2I-CoReBench: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "T2I-CoReBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#training", "#dataset", "#architecture", "#optimization", "#transfer_learning"], "emoji": "üîÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤ –µ
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#training", "#multimodal", "#cv", "#optimization", "#reasoning", "#dataset", "#open_source"], "emoji": "üé®", "ru": {"title": "–ß–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è Text-to-Image –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#hallucinations", "#rag", "#alignment"], "emoji": "üß©", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–µ–º–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ RAG: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å", "desc": "–í —ç—Ç–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ —É–ø—Ä–∞–≤–ª—è–µ–º–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º (RAG) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#rl", "#training", "#benchmark", "#optimization"], "emoji": "üß†", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ —Ç–µ–æ—Ä–µ–º —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "BFS-Prover-V2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º, —Ä–µ—à–∞—é—â—É—é –ø—Ä–æ–±–ª–µ–º—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#inference", "#benchmark", "#hallucinations", "#reasoning"], "emoji": "ü§î", "ru": {"title": "–ë–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π - –Ω–µ –≤—Å–µ–≥–¥–∞ –ª—É—á—à–µ: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –º–æ—â–Ω–æ—Å—Ç–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ (test-time scaling)
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#agents", "#security", "#ethics", "#training", "#agi"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ö–æ—ç–≤–æ–ª—é—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é R¬≤AI –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —á–µ—Ä–µ–∑ –∫–æ—ç–≤–æ–ª—é—Ü–∏—é. –ü–æ–¥—Ö–æ–¥ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#dataset", "#machine_translation", "#architecture", "#training", "#low_resource", "#multilingual"], "emoji": "üåç", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –≤ –ò–ò: —Ç—Ä–µ—Ö—ä—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω—ã–µ —è–∑—ã–∫–∏", "desc": "Llama-GENBA-10B - —ç—Ç–æ —Ç—Ä–µ—Ö—ä—è–∑—ã—á–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#dataset", "#games", "#multimodal"], "emoji": "üé≠", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —á–µ—Ä–Ω—ã–π —é–º–æ—Ä –≤ –º–µ–º–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é —á–µ—Ä–Ω–æ–≥–æ —é–º–æ—Ä–∞ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–µ–º–∞—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#games", "#optimization"], "emoji": "üì±", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã - –±—É–¥—É—â–µ–µ –º–æ–±–∏–ª—å–Ω–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏", "desc": "MAS-Bench - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, —Å–æ—á–µ—Ç–∞—é—â–∏—Ö –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∏ —è—Ä–ª—ã–∫–∏, –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤. –û–Ω–∞ –≤–∫–ª—é—á–∞–µ—Ç 1
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#data", "#open_source", "#dataset", "#math", "#reasoning", "#training"], "emoji": "üßÆ", "ru": {"title": "–°–∏–º–≤–æ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–æ–ª—å—à–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–æ—Ä–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#3d", "#robotics"], "emoji": "üöó", "ru": {"title": "DCReg: –ù–∞–¥–µ–∂–Ω–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ LiDAR –≤ —Å–ª–æ–∂–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö", "desc": "DCReg - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º—ã —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ LiDAR –≤ —Å–ª–æ–∂–Ω—ã—Ö –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –®—É—Ä–∞
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#cv", "#video"], "emoji": "üñºÔ∏è", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –º–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏—è –∏ –∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥", "desc": "Inpaint4Drag - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–µ—Ç–æ–¥–æ–º –ø–µ—Ä–µ—Ç–∞—Å–∫–∏–≤–∞–Ω–∏—è. –û–Ω–∞ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞ –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#training", "#interpretability", "#healthcare", "#open_source", "#multimodal", "#transfer_learning"], "emoji": "üî¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è CLIP –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∑–Ω–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CLIP-SVD - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å–∏
[09.09.2025 20:12] Using data from previous issue: {"categories": ["#open_source", "#multimodal", "#agents", "#agi", "#inference", "#interpretability", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ü—Ä–æ–∑—Ä–∞—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–∞–≥–µ–Ω—Ç–∞–º–∏ —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–µ—Ç–æ–¥–∏–∫—É –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
[09.09.2025 20:12] Renaming data file.
[09.09.2025 20:12] Renaming previous data. hf_papers.json to ./d/2025-09-09.json
[09.09.2025 20:12] Saving new data file.
[09.09.2025 20:12] Generating page.
[09.09.2025 20:12] Renaming previous page.
[09.09.2025 20:12] Renaming previous data. index.html to ./d/2025-09-09.html
[09.09.2025 20:12] Writing result.
[09.09.2025 20:12] Renaming log file.
[09.09.2025 20:12] Renaming previous data. log.txt to ./logs/2025-09-09_last_log.txt
