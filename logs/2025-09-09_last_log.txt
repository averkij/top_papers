[09.09.2025 16:15] Read previous papers.
[09.09.2025 16:15] Generating top page (month).
[09.09.2025 16:15] Writing top page (month).
[09.09.2025 17:09] Read previous papers.
[09.09.2025 17:09] Get feed.
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06160
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06501
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06949
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06467
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.01656
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06733
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06461
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06155
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.03516
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.02108
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06945
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06917
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06631
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06493
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06861
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06786
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.05668
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06771
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06477
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06809
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.06285
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.04582
[09.09.2025 17:09] Get page data from previous paper. URL: https://huggingface.co/papers/2509.03740
[09.09.2025 17:09] Extract page data from URL. URL: https://huggingface.co/papers/2509.00328
[09.09.2025 17:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[09.09.2025 17:09] No deleted papers detected.
[09.09.2025 17:09] Downloading and parsing papers (pdf, html). Total: 24.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06160.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06160.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06160.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06501.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06501.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06501.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06949.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06949.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06949.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06467.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06467.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06467.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.01656.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.01656.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.01656.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06733.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06733.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06733.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06461.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06461.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06461.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06155.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06155.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06155.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.03516.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.03516.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.03516.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.02108.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.02108.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.02108.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06945.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06945.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06945.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06917.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06917.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06917.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06631.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06631.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06631.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06493.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06493.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06493.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06861.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06861.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06861.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06786.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06786.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06786.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.05668.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.05668.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.05668.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06771.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06771.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06771.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06477.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06477.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06477.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06809.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06809.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06809.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.06285.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.06285.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.06285.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.04582.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.04582.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.04582.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.03740.
[09.09.2025 17:09] Extra JSON file exists (./assets/json/2509.03740.json), skip PDF parsing.
[09.09.2025 17:09] Paper image links file exists (./assets/img_data/2509.03740.json), skip HTML parsing.
[09.09.2025 17:09] Success.
[09.09.2025 17:09] Downloading and parsing paper https://huggingface.co/papers/2509.00328.
[09.09.2025 17:09] Downloading paper 2509.00328 from http://arxiv.org/pdf/2509.00328v1...
[09.09.2025 17:10] Extracting affiliations from text.
[09.09.2025 17:10] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 0 3 ] . [ 1 8 2 3 0 0 . 9 0 5 2 : r Mechanistic Interpretability for Steering Vision-Language-Action Models Bear Haon Kaylene Stocking Ian Chuang Claire Tomlin Department of Electrical Engineering and Computer Sciences University of California, Berkeley Figure 1: We present framework for steering Vision-Language-Action (VLA) models. We extract FFN vectors, project them to the VLA token space, cluster them by semantic alignment, and inject activations at inference time to modulate behavior. Our experiments demonstrate interpretable zero-shot control in both simulation (OPENVLA in LIBERO) and on physical robot (œÄ0 on UR5). Abstract: Vision-Language-Action (VLA) models are promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce generalpurpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, œÄ0 and OPENVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on physical robot (UR5). This work demonstrates that interpretable"
[09.09.2025 17:10] Response: ```python
["Department of Electrical Engineering and Computer Sciences, University of California, Berkeley"]
```
[09.09.2025 17:10] Deleting PDF ./assets/pdf/2509.00328.pdf.
[09.09.2025 17:10] Success.
[09.09.2025 17:10] Enriching papers with extra data.
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 0. REER, a new paradigm for deep reasoning, uses reverse engineering to discover step-by-step reasoning processes, enabling a model to perform competitively on open-ended tasks.  					AI-generated summary 				 While the ``deep reasoning'' paradigm has spurred significant advances in verifiable domains ...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 1. WebExplorer, a data-driven approach for developing advanced web agents, achieves state-of-the-art performance in information-seeking tasks through systematic data generation and reinforcement learning.  					AI-generated summary 				 The paradigm of Large Language Models (LLMs) has increasingly shif...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 2. TraceRL enhances diffusion language models with trajectory-aware reinforcement learning, improving reasoning performance on complex tasks and enabling flexible sampling.  					AI-generated summary 				 We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language mod...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 3. DINOv3, a self-supervised vision transformer, demonstrates strong performance across various medical vision tasks without domain-specific pre-training, though it shows limitations in deeply specialized domains and does not consistently follow scaling laws in the medical domain.  					AI-generated su...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 4. ReVPT enhances multi-modal LLMs' visual reasoning capabilities using reinforcement learning, achieving state-of-the-art performance on visual benchmarks.  					AI-generated summary 				 Visual reasoning, a cornerstone of human intelligence, encompasses complex perceptual and logical processes essent...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 5. Reinforcement learning is explored as a foundational approach for training deep research systems, addressing limitations of supervised and preference alignment methods by optimizing policies for tool interaction and exploration.  					AI-generated summary 				 Deep research systems, agentic AI that ...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 6. Contrastive Attention Refinement for Visual Enhancement (CARVE) improves VLM performance by extracting task-relevant visual signals through attention contrasting, addressing issues with visual complexity and attention mechanisms.  					AI-generated summary 				 Vision-Language Models (VLMs) have dem...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 7. UniVerse-1, a unified audio-video generation model, uses a stitching of experts technique to combine pre-trained video and music models, ensuring accurate temporal alignment and producing high-quality audio-visual outputs.  					AI-generated summary 				 We introduce UniVerse-1, a unified, Veo-3-lik...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 8. T2I-CoReBench is a benchmark that evaluates the composition and reasoning capabilities of text-to-image models using a comprehensive and complex set of prompts and checklist questions.  					AI-generated summary 				 Text-to-image (T2I) generation aims to synthesize images from textual prompts, whic...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 9. Multi-task learning (MTL) is often achieved by merging datasets before fine-tuning, but the growing availability of fine-tuned models has led to new approaches such as model merging via task arithmetic. A major challenge in this setting is task interference, which worsens as the number of tasks incr...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 10. Interleaving Reasoning Generation (IRG) framework alternates between text-based thinking and image synthesis to improve Text-to-Image generation, achieving state-of-the-art performance and enhanced visual quality.  					AI-generated summary 				 Unified multimodal understanding and generation models...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 11. Paper2Agent converts research papers into interactive AI agents to facilitate knowledge dissemination and enable complex scientific queries through natural language.  					AI-generated summary 				 We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2A...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 12. Guided decoding methods in Retrieval-Augmented Generation (RAG) systems are evaluated for structured output generation, revealing performance variations across different prompting setups.  					AI-generated summary 				 The integration of Large Language Models (LLMs) into various applications has dr...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 13. BFS-Prover-V2 addresses scaling challenges in automated theorem proving by integrating a multi-turn off-policy RL framework and a planner-enhanced multi-agent search architecture, achieving state-of-the-art results on formal mathematics benchmarks.  					AI-generated summary 				 The integration of ...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 14. Test-time scaling does not consistently improve accuracy or reduce hallucinations in knowledge-intensive tasks, often leading to overconfident errors.  					AI-generated summary 				 Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has ...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 15. A new framework, R¬≤AI, is proposed to enhance AI safety through coevolution, combining resistance to known threats with resilience to unforeseen risks using fast and slow safe models and adversarial simulation.  					AI-generated summary 				 In this position paper, we address the persistent gap bet...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 16. Llama-GENBA-10B, a trilingual foundation model, addresses English-centric bias by balancing English, German, and Bavarian training, achieving strong cross-lingual performance and setting new benchmarks for Bavarian.  					AI-generated summary 				 We present Llama-GENBA-10B, a trilingual foundation ...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 17. A reasoning-augmented framework using a Large Vision-Language Model and a Tri-stream Cross-Reasoning Network achieves superior performance in detecting dark humor, identifying targets, and predicting intensity in multimodal memes.  					AI-generated summary 				 Dark humor in online memes poses uniq...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 18. MAS-Bench evaluates GUI-shortcut hybrid agents on mobile devices, demonstrating their superior performance and efficiency over GUI-only agents through a comprehensive benchmarking framework.  					AI-generated summary 				 To enhance the efficiency of GUI agents on various platforms like smartphones...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 19. A framework generates a large corpus of valid theorems using automated theorem proving to create symbolic training data for improving LLMs' mathematical reasoning.  					AI-generated summary 				 The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematic...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 20. DCReg addresses ill-conditioned LiDAR point cloud registration by detecting, characterizing, and mitigating degeneracies through Schur complement decomposition and a novel preconditioner.  					AI-generated summary 				 LiDAR point cloud registration is fundamental to robotic perception and navigati...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 21. Inpaint4Drag enhances drag-based image editing by decomposing it into pixel-space warping and inpainting, offering real-time performance and superior visual quality.  					AI-generated summary 				 Drag-based image editing has emerged as a powerful paradigm for intuitive image manipulation. However,...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 22. Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Exis...
[09.09.2025 17:10] ********************************************************************************
[09.09.2025 17:10] Abstract 23. A framework for interpreting and steering Vision-Language-Action (VLA) models via internal representations enables real-time behavioral control without fine-tuning or environment interaction.  					AI-generated summary 				 Vision-Language-Action (VLA) models are a promising path to realizing genera...
[09.09.2025 17:10] Read previous papers.
[09.09.2025 17:10] Generating reviews via LLM API.
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#dataset", "#architecture", "#training", "#rl"], "emoji": "üß†", "ru": {"title": "–û–±—Ä–∞—Ç–Ω–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–ª—É–±–æ–∫–æ–º—É –æ–±—É—á–µ–Ω–∏—é", "desc": "REER (Reverse-Engineered Reasoning) - —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –≤ –≥–ª—É–±–æ–∫–æ–º –æ–±—É—á–µ–Ω–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#agents", "#training", "#reasoning", "#agi", "#dataset", "#rl", "#long_context"], "emoji": "üï∏Ô∏è", "ru": {"title": "WebExplorer: –ú–∞–ª–µ–Ω—å–∫–∏–π –∞–≥–µ–Ω—Ç —Å –±–æ–ª—å—à–∏–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏", "desc": "WebExplorer - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∞–Ω–Ω—ã—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#training", "#math", "#rl", "#open_source", "#diffusion"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "TraceRL - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#transfer_learning", "#3d", "#cv", "#healthcare", "#benchmark", "#optimization", "#science"], "emoji": "ü©∫", "ru": {"title": "DINOv3: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏", "desc": "–ú–æ–¥–µ–ª—å DINOv3, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Vision Transformer, –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#multimodal", "#reasoning", "#optimization", "#benchmark", "#rl", "#cv"], "emoji": "üß†", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –ò–ò —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ ReVPT, —É–ª—É—á—à–∞—é—â–∏–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#rl", "#training", "#long_context", "#benchmark", "#survey", "#optimization", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∫–∞–∫ –æ—Å–Ω–æ–≤–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#cv", "#training", "#multimodal", "#open_source"], "emoji": "üîç", "ru": {"title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ CARVE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VL
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#multimodal", "#video", "#audio", "#benchmark", "#open_source"], "emoji": "üé•", "ru": {"title": "UniVerse-1: —Å–∏–Ω–µ—Ä–≥–∏—è –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ –≤ –æ–¥–Ω–æ–º –º–æ–¥–µ–ª–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å UniVerse-1, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–µ—Ö–Ω–∏–∫—É \"stitching of experts\
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#cv", "#benchmark", "#reasoning", "#games"], "emoji": "üé®", "ru": {"title": "T2I-CoReBench: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É", "desc": "T2I-CoReBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#training", "#dataset", "#architecture", "#optimization", "#transfer_learning"], "emoji": "üîÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤ –µ
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#training", "#multimodal", "#cv", "#optimization", "#reasoning", "#dataset", "#open_source"], "emoji": "üé®", "ru": {"title": "–ß–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è Text-to-Image –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#agents", "#science", "#multimodal"], "emoji": "üß¨", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤", "desc": "Paper2Agent - —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤. –û–Ω–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –∫–æ–¥ —Å—Ç–∞—Ç—å–∏, 
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#hallucinations", "#rag", "#alignment"], "emoji": "üß©", "ru": {"title": "–£–ø—Ä–∞–≤–ª—è–µ–º–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ RAG: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å", "desc": "–í —ç—Ç–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ —É–ø—Ä–∞–≤–ª—è–µ–º–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º (RAG) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#rl", "#training", "#benchmark", "#optimization"], "emoji": "üß†", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ —Ç–µ–æ—Ä–µ–º —Å –ø–æ–º–æ—â—å—é –ò–ò", "desc": "BFS-Prover-V2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º, —Ä–µ—à–∞—é—â—É—é –ø—Ä–æ–±–ª–µ–º—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#inference", "#benchmark", "#hallucinations", "#reasoning"], "emoji": "ü§î", "ru": {"title": "–ë–æ–ª—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π - –Ω–µ –≤—Å–µ–≥–¥–∞ –ª—É—á—à–µ: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –º–æ—â–Ω–æ—Å—Ç–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ (test-time scaling)
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#agents", "#security", "#ethics", "#training", "#agi"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ö–æ—ç–≤–æ–ª—é—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ò–ò", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∫–æ–Ω—Ü–µ–ø—Ü–∏—é R¬≤AI –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —á–µ—Ä–µ–∑ –∫–æ—ç–≤–æ–ª—é—Ü–∏—é. –ü–æ–¥—Ö–æ–¥ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#dataset", "#machine_translation", "#architecture", "#training", "#low_resource", "#multilingual"], "emoji": "üåç", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –≤ –ò–ò: —Ç—Ä–µ—Ö—ä—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –º–∞–ª–æ—Ä–µ—Å—É—Ä—Å–Ω—ã–µ —è–∑—ã–∫–∏", "desc": "Llama-GENBA-10B - —ç—Ç–æ —Ç—Ä–µ—Ö—ä—è–∑—ã—á–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#dataset", "#games", "#multimodal"], "emoji": "üé≠", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —á–µ—Ä–Ω—ã–π —é–º–æ—Ä –≤ –º–µ–º–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é —á–µ—Ä–Ω–æ–≥–æ —é–º–æ—Ä–∞ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–µ–º–∞—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#agents", "#benchmark", "#games", "#optimization"], "emoji": "üì±", "ru": {"title": "–ì–∏–±—Ä–∏–¥–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã - –±—É–¥—É—â–µ–µ –º–æ–±–∏–ª—å–Ω–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏", "desc": "MAS-Bench - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, —Å–æ—á–µ—Ç–∞—é—â–∏—Ö –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∏ —è—Ä–ª—ã–∫–∏, –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤. –û–Ω–∞ –≤–∫–ª—é—á–∞–µ—Ç 1
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#data", "#open_source", "#dataset", "#math", "#reasoning", "#training"], "emoji": "üßÆ", "ru": {"title": "–°–∏–º–≤–æ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò", "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–æ–ª—å—à–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–æ—Ä–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#3d", "#robotics"], "emoji": "üöó", "ru": {"title": "DCReg: –ù–∞–¥–µ–∂–Ω–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ LiDAR –≤ —Å–ª–æ–∂–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö", "desc": "DCReg - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º—ã —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –æ–±–ª–∞–∫–æ–≤ —Ç–æ—á–µ–∫ LiDAR –≤ —Å–ª–æ–∂–Ω—ã—Ö –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –®—É—Ä–∞
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#cv", "#video"], "emoji": "üñºÔ∏è", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –º–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏—è –∏ –∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥", "desc": "Inpaint4Drag - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–µ—Ç–æ–¥–æ–º –ø–µ—Ä–µ—Ç–∞—Å–∫–∏–≤–∞–Ω–∏—è. –û–Ω–∞ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞ –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤
[09.09.2025 17:10] Using data from previous issue: {"categories": ["#training", "#interpretability", "#healthcare", "#open_source", "#multimodal", "#transfer_learning"], "emoji": "üî¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è CLIP –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∑–Ω–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CLIP-SVD - –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å–∏
[09.09.2025 17:10] Querying the API.
[09.09.2025 17:10] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A framework for interpreting and steering Vision-Language-Action (VLA) models via internal representations enables real-time behavioral control without fine-tuning or environment interaction.  					AI-generated summary 				 Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is a central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce a general-purpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that interpretable components of embodied VLAs can be systematically harnessed for control - establishing a new paradigm for transparent and steerable foundation models in robotics.
[09.09.2025 17:10] Response: {
  "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–µ—Ç–æ–¥–∏–∫—É –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ Vision-Language-Action (VLA) —á–µ—Ä–µ–∑ –∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–ø–æ—Å–æ–± –ø—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞–Ω–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤ —Å–ª–æ—è—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –Ω–∞ –±–∞–∑–∏—Å —Ç–æ–∫–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –≤—ã—è–≤–ª—è—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –≤—ã–±–æ—Ä–æ–º –¥–µ–π—Å—Ç–≤–∏–π. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –º–µ—Ç–æ–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è–º–∏, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –º–æ–¥–µ–ª—è—Ö Pi0 –∏ OpenVLA –≤ —Å–∏–º—É–ª—è—Ü–∏–∏ –∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º —Ä–æ–±–æ—Ç–µ UR5.",
  "emoji": "ü§ñ",
  "title": "–ü—Ä–æ–∑—Ä–∞—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–∞–≥–µ–Ω—Ç–∞–º–∏ —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π"
}
[09.09.2025 17:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework for interpreting and steering Vision-Language-Action (VLA) models via internal representations enables real-time behavioral control without fine-tuning or environment interaction.  					AI-generated summary 				 Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is a central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce a general-purpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that interpretable components of embodied VLAs can be systematically harnessed for control - establishing a new paradigm for transparent and steerable foundation models in robotics."

[09.09.2025 17:10] Response: ```python
["AGENTS", "ROBOTICS", "MULTIMODAL", "INFERENCE"]
```
[09.09.2025 17:10] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A framework for interpreting and steering Vision-Language-Action (VLA) models via internal representations enables real-time behavioral control without fine-tuning or environment interaction.  					AI-generated summary 				 Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is a central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce a general-purpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that interpretable components of embodied VLAs can be systematically harnessed for control - establishing a new paradigm for transparent and steerable foundation models in robotics."

[09.09.2025 17:10] Response: ```python
['INTERPRETABILITY', 'AGI', 'OPEN_SOURCE']
```
[09.09.2025 17:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents a new framework for interpreting and controlling Vision-Language-Action (VLA) models, which are designed to help robots understand and perform tasks. The framework allows for real-time adjustments to the model\'s behavior without needing to retrain it or interact with the environment. By analyzing the internal representations of the VLA models, the authors identify key semantic directions that influence action choices, such as speed and direction. This approach enables effective steering of robot actions in both simulations and real-world applications, paving the way for more transparent and adaptable robotic systems.","title":"Steering Robots with Insight: Real-Time Control of VLA Models"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents a new framework for interpreting and controlling Vision-Language-Action (VLA) models, which are designed to help robots understand and perform tasks. The framework allows for real-time adjustments to the model's behavior without needing to retrain it or interact with the environment. By analyzing the internal representations of the VLA models, the authors identify key semantic directions that influence action choices, such as speed and direction. This approach enables effective steering of robot actions in both simulations and real-world applications, paving the way for more transparent and adaptable robotic systems.", title='Steering Robots with Insight: Real-Time Control of VLA Models'))
[09.09.2025 17:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ°ÜÊû∂ÔºåÁî®‰∫éÈÄöËøáÂÜÖÈÉ®Ë°®Á§∫Ëß£ÈáäÂíåÂºïÂØºËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®ÔºàVLAÔºâÊ®°ÂûãÔºå‰ªéËÄåÂÆûÁé∞ÂÆûÊó∂Ë°å‰∏∫ÊéßÂà∂ÔºåËÄåÊó†ÈúÄÂæÆË∞ÉÊàñ‰∏éÁéØÂ¢É‰∫§‰∫í„ÄÇVLAÊ®°ÂûãÊúâÊΩúÂäõÊàê‰∏∫ÈÄöÁî®ÁöÑÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÔºåËÉΩÂ§üÂø´ÈÄüÈÄÇÂ∫îÊñ∞‰ªªÂä°ÂíåÁéØÂ¢ÉÔºå‰ΩÜÁõÆÂâçÁöÑËß£ÈáäÂíåÂºïÂØºÊñπÊ≥ïËøú‰∏çÂ¶Ç‰º†ÁªüÊú∫Âô®‰∫∫ÁÆ°ÈÅì„ÄÇÊàë‰ª¨ÈÄöËøáÂØπÂèòÊç¢Âô®Â±Ç‰∏≠ÁöÑÂâçÈ¶àÊøÄÊ¥ªËøõË°åÊäïÂΩ±ÔºåËØÜÂà´Âá∫‰∏éÂä®‰ΩúÈÄâÊã©Âõ†ÊûúÁõ∏ÂÖ≥ÁöÑÁ®ÄÁñèËØ≠‰πâÊñπÂêëÔºåÂ¶ÇÈÄüÂ∫¶ÂíåÊñπÂêë„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöÁî®ÁöÑÊøÄÊ¥ªÂºïÂØºÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰∏çÈúÄË¶ÅÂæÆË∞ÉÊàñÂ•ñÂä±‰ø°Âè∑ÁöÑÊÉÖÂÜµ‰∏ãÂÆûÊó∂Ë∞ÉËäÇË°å‰∏∫„ÄÇ","title":"ÂÆûÊó∂ÊéßÂà∂ËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®Ê®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ°ÜÊû∂ÔºåÁî®‰∫éÈÄöËøáÂÜÖÈÉ®Ë°®Á§∫Ëß£ÈáäÂíåÂºïÂØºËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®ÔºàVLAÔºâÊ®°ÂûãÔºå‰ªéËÄåÂÆûÁé∞ÂÆûÊó∂Ë°å‰∏∫ÊéßÂà∂ÔºåËÄåÊó†ÈúÄÂæÆË∞ÉÊàñ‰∏éÁéØÂ¢É‰∫§‰∫í„ÄÇVLAÊ®°ÂûãÊúâÊΩúÂäõÊàê‰∏∫ÈÄöÁî®ÁöÑÂÖ∑Ë∫´Êô∫ËÉΩ‰ΩìÔºåËÉΩÂ§üÂø´ÈÄüÈÄÇÂ∫îÊñ∞‰ªªÂä°ÂíåÁéØÂ¢ÉÔºå‰ΩÜÁõÆÂâçÁöÑËß£ÈáäÂíåÂºïÂØºÊñπÊ≥ïËøú‰∏çÂ¶Ç‰º†ÁªüÊú∫Âô®‰∫∫ÁÆ°ÈÅì„ÄÇÊàë‰ª¨ÈÄöËøáÂØπÂèòÊç¢Âô®Â±Ç‰∏≠ÁöÑÂâçÈ¶àÊøÄÊ¥ªËøõË°åÊäïÂΩ±ÔºåËØÜÂà´Âá∫‰∏éÂä®‰ΩúÈÄâÊã©Âõ†ÊûúÁõ∏ÂÖ≥ÁöÑÁ®ÄÁñèËØ≠‰πâÊñπÂêëÔºåÂ¶ÇÈÄüÂ∫¶ÂíåÊñπÂêë„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöÁî®ÁöÑÊøÄÊ¥ªÂºïÂØºÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰∏çÈúÄË¶ÅÂæÆË∞ÉÊàñÂ•ñÂä±‰ø°Âè∑ÁöÑÊÉÖÂÜµ‰∏ãÂÆûÊó∂Ë∞ÉËäÇË°å‰∏∫„ÄÇ', title='ÂÆûÊó∂ÊéßÂà∂ËßÜËßâ-ËØ≠Ë®Ä-Ë°åÂä®Ê®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï'))
[09.09.2025 17:10] Renaming data file.
[09.09.2025 17:10] Renaming previous data. hf_papers.json to ./d/2025-09-09.json
[09.09.2025 17:10] Saving new data file.
[09.09.2025 17:10] Generating page.
[09.09.2025 17:10] Renaming previous page.
[09.09.2025 17:10] Renaming previous data. index.html to ./d/2025-09-09.html
[09.09.2025 17:10] Writing result.
[09.09.2025 17:10] Renaming log file.
[09.09.2025 17:10] Renaming previous data. log.txt to ./logs/2025-09-09_last_log.txt
