[12.10.2024 18:14] Get feed.
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08196
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07484
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.03450
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05265
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07869
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08164
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08207
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07303
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08159
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05248
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.04751
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08151
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06508
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06154
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07041
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05210
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05603
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08115
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07137
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.08049
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06293
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.04808
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07707
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05269
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.05629
[12.10.2024 18:14] Get page data from previous paper. URL: https://huggingface.co/papers/2410.03437
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 0. Code has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining often include code that utilizes math-related packages, which are primarily designed for field...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 1. Can large language models (LLMs) directly serve as powerful world models for model-based agents? While the gaps between the prior knowledge of LLMs and the specified environment's dynamics do exist, our study reveals that the gaps can be bridged by aligning an LLM with its deployed environment and s...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 2. MLLM agents demonstrate potential for complex embodied tasks by retrieving multimodal task-relevant trajectory data. However, current retrieval methods primarily focus on surface-level similarities of textual or visual cues in trajectories, neglecting their effectiveness for the specific task at han...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 3. Quantization is essential for deploying Large Language Models (LLMs) by enhancing memory efficiency and inference speed. Existing methods for activation quantization mainly address channel-wise outliers, often neglecting token-wise outliers, leading to reliance on costly per-token dynamic quantizati...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 4. Large Language Models (LLMs), with their exceptional ability to handle a wide range of tasks, have driven significant advancements in tackling reasoning and planning tasks, wherein decomposing complex problems into executable workflows is a crucial step in this process. Existing workflow evaluation ...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 5. We present Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S aims to address three key challenges in automating computer tas...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 6. Discrete diffusion models have achieved success in tasks like image generation and masked language modeling but face limitations in controlled content editing. We introduce DICE (Discrete Inversion for Controllable Editing), the first approach to enable precise inversion for discrete diffusion model...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 7. Diffusion models have greatly improved visual generation but are hindered by slow generation speed due to the computationally intensive nature of solving generative ODEs. Rectified flow, a widely recognized solution, improves generation speed by straightening the ODE path. Its key components include...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 8. Diffusion models have become the dominant approach for visual generation. They are trained by denoising a Markovian process that gradually adds noise to the input. We argue that the Markovian property limits the models ability to fully utilize the generation trajectory, leading to inefficiencies dur...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 9. To induce desired behaviors in large language models (LLMs) for interaction-driven tasks, the instruction-tuning stage typically trains LLMs on instruction-response pairs using the next-token prediction (NTP) loss. Previous work aiming to improve instruction-tuning performance often emphasizes the n...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 10. Recently, large language and vision models (LLVMs) have received significant attention and development efforts due to their remarkable generalization performance across a wide range of tasks requiring perception and cognitive abilities. A key factor behind their success is their simple architecture,...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 11. Current frontier video diffusion models have demonstrated remarkable results at generating high-quality videos. However, they can only generate short video clips, normally around 10 seconds or 240 frames, due to computation limitations during training. In this work, we show that existing models can ...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 12. Monte Carlo Tree Search (MCTS) has recently emerged as a powerful technique for enhancing the reasoning capabilities of LLMs. Techniques such as SFT or DPO have enabled LLMs to distill high-quality behaviors from MCTS, improving their reasoning performance. However, existing distillation methods und...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 13. In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task description, querying it for suitable VLM prompts (e.g., for...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 14. We study the performance of transformers as a function of the number of repetitions of training examples with algorithmically generated datasets. On three problems of mathematics: the greatest common divisor, modular multiplication, and matrix eigenvalues, we show that for a fixed number of training...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 15. In this paper, we propose a new method to enhance compositional understanding in pre-trained vision and language models (VLMs) without sacrificing performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches often improve compositional reasoning at the cost of degrading multi-modal...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 16. Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. In this study, we explore a surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during a single inference call, a capability we term "task...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 17. Large Language Model (LLM) based multi-agent systems (MAS) show remarkable potential in collaborative problem-solving, yet they still face critical challenges: low communication efficiency, poor scalability, and a lack of effective parameter-updating optimization methods. We present Optima, a novel ...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 18. Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impac...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 19. This paper proposes the paradigm of large convolutional kernels in designing modern Convolutional Neural Networks (ConvNets). We establish that employing a few large kernels, instead of stacking multiple smaller ones, can be a superior design strategy. Our work introduces a set of architecture desig...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 20. Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal tool for aligning large language models (LLMs) with human preferences. Direct Preference Optimization (DPO), one of the most popular approaches, formulates RLHF as a policy optimization problem without explicitly estimating t...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 21. In spite of the outstanding performance, Neural Architecture Search (NAS) is criticized for massive computation. Recently, Zero-shot NAS has emerged as a promising approach by exploiting Zero-cost (ZC) proxies, which markedly reduce computational demands. Despite this, existing ZC proxies heavily re...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 22. Dynamic scene reconstruction is a long-term challenge in the field of 3D vision. Recently, the emergence of 3D Gaussian Splatting has provided new insights into this problem. Although subsequent efforts rapidly extend static 3D Gaussian to dynamic scenes, they often lack explicit constraints on obje...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 23. Data is a crucial element in large language model (LLM) alignment. Recent studies have explored using LLMs for efficient data collection. However, LLM-generated data often suffers from quality issues, with underrepresented or absent aspects and low-quality datapoints. To address these problems, we p...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 24. Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLM's embedding spa...
[12.10.2024 18:14] ********************************************************************************
[12.10.2024 18:14] Abstract 25. Solving time-dependent parametric partial differential equations (PDEs) is challenging, as models must adapt to variations in parameters such as coefficients, forcing terms, and boundary conditions. Data-driven neural solvers either train on data sampled from the PDE parameters distribution in the h...
[12.10.2024 18:14] Read previous papers.
[12.10.2024 18:14] Generating reviews via LLM API.
[12.10.2024 18:14] Using data from previous issue: {"desc": "Статья представляет новый метод генерации математического кода с соответствующими шагами рассуждений для дальнейшего предобучения языковых моделей. Авторы создали высококачественный датасет MathCode-Pile объемом 19.2 млрд токенов, включающий математические веб-данные, код с использованием 
[12.10.2024 18:14] Using data from previous issue: {"desc": "Статья исследует возможность использования больших языковых моделей (LLM) в качестве мощных моделей мира для агентов на основе модели. Авторы предлагают нейросимволический подход для обучения правил, позволяющих согласовать предсказания LLM с динамикой конкретной среды. Разработанный агент
[12.10.2024 18:14] Using data from previous issue: {"desc": "Статья представляет новый метод MART для улучшения работы воплощенных агентов с использованием мультимодальных языковых моделей (MLLM). MART использует данные взаимодействия для тонкой настройки MLLM-ретривера на основе обучения с предпочтениями, что позволяет эффективно оценивать и приори
[12.10.2024 18:14] Using data from previous issue: {"desc": "PrefixQuant - это новый метод квантования больших языковых моделей (LLM), который изолирует выбросы на уровне токенов офлайн без переобучения. Он префиксирует часто встречающиеся токены-выбросы в KV-кэше, что позволяет применять эффективное статическое квантование на уровне тензоров. Prefi
[12.10.2024 18:14] Using data from previous issue: {"desc": "Статья представляет WorFBench - новый бенчмарк для оценки способности языковых моделей генерировать рабочие процессы. Авторы также предлагают WorFEval - протокол оценки, использующий алгоритмы сопоставления подпоследовательностей и подграфов. Исследование выявило значительный разрыв между 
[12.10.2024 18:14] Using data from previous issue: {"desc": "Agent S - это открытая агентная система, предназначенная для автономного взаимодействия с компьютерами через графический интерфейс пользователя. Система решает три ключевые проблемы автоматизации компьютерных задач: получение специфических знаний, планирование долгосрочных задач и работа с
[12.10.2024 18:14] Using data from previous issue: {"desc": "DICE (Discrete Inversion for Controllable Editing) - это новый подход к точной инверсии для дискретных диффузионных моделей. Он позволяет осуществлять точную реконструкцию и гибкое редактирование дискретных данных без необходимости в предопределенных масках или манипуляциях с вниманием. DI
[12.10.2024 18:14] Using data from previous issue: {"desc": "Статья представляет новый метод под названием Rectified Diffusion, который улучшает скорость генерации изображений в диффузионных моделях. Авторы утверждают, что успех ректификации основан на использовании предобученной диффузионной модели для получения сопоставленных пар шума и образцов. 
[12.10.2024 18:14] Using data from previous issue: {"desc": "DART - это новый подход к генерации изображений, объединяющий авторегрессионные модели и диффузию в рамках немарковского процесса. Модель использует трансформер для итеративного шумоподавления патчей изображения как в пространственном, так и в спектральном измерении. DART не требует кванти
[12.10.2024 18:14] Using data from previous issue: {"desc": "SFTMix - это новый метод для улучшения процесса инструктивной настройки больших языковых моделей (LLM). Он использует динамику обучения для идентификации примеров с разными уровнями уверенности модели. Затем применяется регуляризация на основе Mixup для снижения переобучения на уверенных п
[12.10.2024 18:14] Using data from previous issue: {"desc": "В статье исследуются крупные языковые и визуальные модели (LLVM) с точки зрения их восприятия изображений. Авторы проводят систематический анализ различных аспектов работы LLVM, включая инвариантность к перестановкам, устойчивость, математические рассуждения и сохранение выравнивания. Эксп
[12.10.2024 18:14] Using data from previous issue: {"desc": "Статья представляет новый подход к генерации длинных видео с использованием авторегрессивных моделей диффузии. Авторы предлагают применять прогрессивно увеличивающиеся уровни шума к латентным кадрам, что позволяет создавать более тонкие связи между ними. Этот метод позволяет генерировать в
[12.10.2024 18:14] Using data from previous issue: {"desc": "AlphaLLM-CPL - это новый метод обучения языковых моделей (LLM) с использованием поиска по дереву Монте-Карло (MCTS). Он улучшает рассуждения LLM, эффективно извлекая информацию из траекторий MCTS. AlphaLLM-CPL использует попарное обучение на основе узлов дерева поиска и куррикулярное обуче
[12.10.2024 18:14] Using data from previous issue: {"desc": "Предложен новый метод GLOV, позволяющий использовать большие языковые модели (LLM) в качестве неявных оптимизаторов для визуально-языковых моделей (VLM) с целью улучшения задач компьютерного зрения. GLOV использует мета-промпты для LLM с описанием целевой задачи, запрашивая подходящие пром
[12.10.2024 18:14] Using data from previous issue: {"desc": "Исследование показывает, что трансформеры лучше обучаются на меньших наборах данных с повторяющимися примерами, чем на больших наборах с уникальными примерами. Эксперименты проводились на трех математических задачах: нахождение наибольшего общего делителя, модульное умножение и собственные
[12.10.2024 18:14] Using data from previous issue: {"desc": "В статье предлагается новый метод улучшения композиционного понимания в предобученных моделях зрения и языка (VLM) без ущерба для производительности в мультимодальных задачах с нулевым обучением. Авторы представляют Fine-grained Selective Calibrated CLIP (FSC-CLIP), который интегрирует лок
[12.10.2024 18:14] Using data from previous issue: {"desc": "Исследование демонстрирует удивительную способность больших языковых моделей (LLM) выполнять несколько вычислительно различных задач обучения в контексте одновременно, названную 'суперпозицией задач'. Это явление наблюдается в различных семействах LLM и масштабах, даже если модель обучена 
[12.10.2024 18:14] Using data from previous issue: {"desc": "Optima - новая система, улучшающая эффективность коммуникации и решения задач в многоагентных системах на основе больших языковых моделей. Она использует итеративный подход генерации, ранжирования, отбора и обучения с функцией вознаграждения, балансирующей производительность, токен-эффекти
[12.10.2024 18:14] Using data from previous issue: {"desc": "Статья рассматривает проблему обмана автоматических бенчмарков для оценки языковых моделей. Авторы демонстрируют, как даже 'нулевая модель', всегда выдающая один и тот же ответ, может достичь высоких результатов на популярных бенчмарках. Эксперименты показывают, что такие трюки могут быть 
[12.10.2024 18:14] Using data from previous issue: {"desc": "Статья предлагает новую парадигму использования больших сверточных ядер в современных сверточных нейронных сетях (ConvNets). Авторы утверждают, что применение нескольких больших ядер вместо стека из множества мелких может быть более эффективной стратегией проектирования. Они представляют а
[12.10.2024 18:14] Using data from previous issue: {"desc": "Статья представляет новый метод ускорения обучения языковых моделей с учетом человеческих предпочтений - Accelerated Preference Optimization (APO). APO основан на технике момента Нестерова и позволяет достичь более быстрой сходимости по сравнению с существующими методами итеративной оптими
[12.10.2024 18:14] Using data from previous issue: {"desc": "LPZero - это новый фреймворк для автоматического проектирования прокси с нулевой стоимостью (ZC) для различных задач нейроархитектурного поиска (NAS). Он моделирует ZC-прокси как символическое уравнение и включает единое пространство поиска прокси. LPZero использует генетическое программир
[12.10.2024 18:14] Using data from previous issue: {"desc": "MotionGS - это новый фреймворк для деформируемого 3D-сплаттинга гауссианов, который использует явные ограничения движения объектов для улучшения реконструкции динамических сцен. Система включает модуль декомпозиции оптического потока, разделяющий его на поток камеры и поток движения объект
[12.10.2024 18:14] Using data from previous issue: {"desc": "Data Advisor - это улучшенный метод генерации данных на основе больших языковых моделей (LLM). Он контролирует качество генерируемых данных, выявляет слабые места в текущем наборе данных и дает рекомендации для следующей итерации генерации. Data Advisor легко интегрируется в существующие м
[12.10.2024 18:14] Using data from previous issue: {"desc": "В статье исследуется возможность расширения способностей больших языковых моделей (LLM) к обучению в контексте на векторные данные из различных доменов. Авторы предлагают метод Vector-ICL, который позволяет LLM эффективно обрабатывать и учиться на проецированных векторах. Эксперименты пока
[12.10.2024 18:14] Using data from previous issue: {"desc": "Zebra - это новый генеративный авторегрессионный трансформер для решения параметрических дифференциальных уравнений в частных производных (ДУЧП). В отличие от существующих подходов, Zebra не требует градиентной адаптации во время вывода, а использует обучение в контексте, вдохновленное воз
[12.10.2024 18:14] Renaming data file.
[12.10.2024 18:14] Renaming previous data. hf_papers.json to 2024-10-11_hf_papers.json
[12.10.2024 18:14] Saving new data file.
[12.10.2024 18:14] Generating page.
[12.10.2024 18:14] Renaming previous page.
[12.10.2024 18:14] Renaming previous data. index.html to 2024-10-11_hf_papers.html
[12.10.2024 18:14] Writing result.
[12.10.2024 18:14] Renaming log file.
[12.10.2024 18:14] Renaming previous data. log.txt to 2024-10-11_last_log.txt
