[19.01.2026 14:29] Read previous papers.
[19.01.2026 14:29] Generating top page (month).
[19.01.2026 14:29] Writing top page (month).
[19.01.2026 15:28] Read previous papers.
[19.01.2026 15:28] Get feed.
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08521
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11496
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10355
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08430
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11000
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11404
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11037
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09195
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10909
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.10825
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09636
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11516
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11514
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11087
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09001
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11354
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11227
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.09255
[19.01.2026 15:28] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07812
[19.01.2026 15:28] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[19.01.2026 15:28] No deleted papers detected.
[19.01.2026 15:28] Downloading and parsing papers (pdf, html). Total: 19.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.08521.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.08521.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.08521.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.11496.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.11496.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.11496.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.10355.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.10355.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.10355.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.08430.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.08430.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.08430.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.11000.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.11000.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.11000.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.11404.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.11404.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.11404.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.11037.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.11037.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.11037.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.09195.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.09195.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.09195.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.10909.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.10909.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.10909.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.10825.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.10825.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.10825.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.09636.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.09636.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.09636.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.11516.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.11516.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.11516.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.11514.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.11514.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.11514.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.11087.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.11087.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.11087.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.09001.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.09001.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.09001.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.11354.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.11354.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.11354.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.11227.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.11227.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.11227.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.09255.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.09255.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.09255.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Downloading and parsing paper https://huggingface.co/papers/2601.07812.
[19.01.2026 15:28] Extra JSON file exists (./assets/json/2601.07812.json), skip PDF parsing.
[19.01.2026 15:28] Paper image links file exists (./assets/img_data/2601.07812.json), skip HTML parsing.
[19.01.2026 15:28] Success.
[19.01.2026 15:28] Enriching papers with extra data.
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 0. Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method.  					AI-generated summary 				 Reinforcement Learning f...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 1. The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric ...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 2. A text-based data synthesis approach generates multi-turn tool-use trajectories for large language models, achieving improved performance and reduced computational costs through a specialized trajectory synthesizer.  					AI-generated summary 				 Enabling Large Language Models (LLMs) to effectively...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 3. RLVR has advanced reasoning capabilities but struggles with open-ended generation due to lack of ground truth; this work proposes an automated rubric generation framework and dataset to improve performance in health reasoning benchmarks.  					AI-generated summary 				 Reinforcement Learning with Ve...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 4. Personalized large language models can generate false information aligned with user history instead of factual truth, but a new method called FPPS helps maintain both factual accuracy and personalized responses while preserving existing personalization effects.  					AI-generated summary 				 Person...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 5. Vision-Language-Action models are enhanced by incorporating action-space reasoning through a structured sequence of coarse action intents, improving manipulation task performance in both simulation and real-world environments.  					AI-generated summary 				 Vision-Language-Action (VLA) models have ...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 6. Reinforcement learning framework for agentic search that improves reliability by teaching agents to recognize reasoning limits and respond appropriately when evidence is insufficient.  					AI-generated summary 				 RL-based agentic search enables LLMs to solve complex questions via dynamic planning...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 7. Supervised fine-tuning with multiple references addresses overfitting to non-core expressions by masking low-probability tokens based on their semantic importance.  					AI-generated summary 				 Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLM...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 8. A diffusion-based framework generates human motion from text prompts with fine-grained part-level control using a newly constructed dataset with atomic, temporally-aware annotations.  					AI-generated summary 				 Human motion generation from text prompts has made remarkable progress in recent year...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 9. Reasoning models demonstrate enhanced performance through multi-agent-like interactions that create diverse cognitive perspectives and improve problem-solving through structured social organization.  					AI-generated summary 				 Large language models have achieved remarkable capabilities across do...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 10. PersonalAlign framework addresses GUI agent alignment with implicit user intents through hierarchical memory organization and long-term record reasoning, improving both execution and proactive performance.  					AI-generated summary 				 While GUI agents have shown strong performance under explicit ...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 11. Activation probes for language model misuse mitigation face challenges with long-context generalization, requiring new architectures and diverse training for robust performance across production shifts.  					AI-generated summary 				 Frontier language model capabilities are improving rapidly. We th...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 12. ShapeR generates high-fidelity 3D shapes from casual image sequences using visual-inertial SLAM, 3D detection, and vision-language models with rectified flow transformer conditioning.  					AI-generated summary 				 Recent advances in 3D shape generation have achieved impressive results, but most ex...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 13. A physics-aware reinforcement learning paradigm is introduced for video generation that enforces physical collision rules directly in high-dimensional spaces, ensuring strict application of physics knowledge rather than treating it as conditional constraints.  					AI-generated summary 				 Physical...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 14. Output-entropy profiles computed from final-layer next-token probabilities serve as a scalable signal for monitoring LLM performance and prioritizing data acquisition under domain shifts.  					AI-generated summary 				 Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where ...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 15. Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained rea...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 16. Controlling the language of thought in large language models increases output diversity by leveraging distinct thinking spaces across different languages, with mixed-language sampling providing superior results.  					AI-generated summary 				 Output diversity is crucial for Large Language Models as...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 17. A three-stage pipeline decouples physical reasoning from visual synthesis in video generation, improving physical plausibility and motion controllability through distinct phases of reasoning, planning, and refinement.  					AI-generated summary 				 Recent diffusion-based video generation models can...
[19.01.2026 15:28] ********************************************************************************
[19.01.2026 15:28] Abstract 18. Large Vision Language Models exhibit significant limitations in multi-image understanding and reasoning, which are revealed through a new benchmark and addressed via procedural data generation and attention masking techniques.  					AI-generated summary 				 Large Vision Language Models (LVLMs) have...
[19.01.2026 15:28] Read previous papers.
[19.01.2026 15:28] Generating reviews via LLM API.
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#reasoning", "#training", "#optimization", "#rl", "#rlhf", "#math"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–º–µ—â—ë–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –≤ –≥—Ä—É–ø–ø–æ–≤–æ–º –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –≤–∑–≤–µ—à–∏–≤–∞—é—â—É—é —Ñ—É–Ω–∫—Ü–∏—é —Å–ª–æ–∂–Ω–æ—Å—Ç–∏", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —Å–º–µ—â—ë–Ω–Ω–æ–π 
[19.01.2026 15:28] Using data from previous issue: {"categories": [], "emoji": "üçé", "ru": {"title": "–ú–∞–Ω–∏–ø—É–ª—è—Ü–∏—è —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º —á–µ—Ä–µ–∑ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –≤—ã–ø—É—Å–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –Ω–∞–±–æ—Ä–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö AI —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –Ω–∞ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç—Ä—ë—Ö –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –∏–≥—Ä–∞—Ö: —Ç–æ—Ä–≥–µ, –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–∞—Ö –∏ —É–±–µ–∂–¥–µ–Ω–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞
[19.01.2026 15:28] Using data from previous issue: {"categories": [], "emoji": "üîß", "ru": {"title": "–°–∏–Ω—Ç–µ–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä—è–º–æ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç GEM ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤ —á–µ—Ä–µ–∑ —á–µ—Ç—ã—Ä—ë—Ö—ç—Ç–∞–ø–Ω—ã
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#healthcare", "#benchmark", "#training", "#optimization", "#rl", "#dataset"], "emoji": "üìã", "ru": {"title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä—É–±—Ä–∏–∫ –æ—Ü–µ–Ω–∫–∏ –¥–ª
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#alignment", "#hallucinations"], "emoji": "‚öñÔ∏è", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –ø—Ä–∞–≤–¥–æ–π –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–µ–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ª–æ–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—É—é —Å –∏—Å—Ç–æ—Ä–∏–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–º–µ—Å—Ç–æ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π –∏—Å—Ç–∏–Ω—ã, –∏–∑
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#reasoning", "#architecture", "#multimodal", "#robotics"], "emoji": "ü§ñ", "ru": {"title": "–ú—ã—Å–ª–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏—è–º–∏: —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Action Chain-of-Thought (ACoT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è Vision-Language-Action –º–æ–¥–µ–ª–µ–π ‚Äî –æ–±–æ–±—â
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#agents", "#rl", "#training"], "emoji": "üéØ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–∏–∑–Ω–∞–≤–∞—Ç—å –≥—Ä–∞–Ω–∏—Ü—ã —Å–≤–æ–µ–≥–æ –∑–Ω–∞–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ—Å–æ–∑–Ω–∞–≤–∞—Ç—å –≥—Ä–∞–Ω–∏—Ü—ã —Å–≤–æ–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#benchmark", "#training"], "emoji": "üéØ", "ru": {"title": "–¶–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–∑–Ω–∞—á–∏–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ ProFit –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ª
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#video", "#dataset", "#multimodal"], "emoji": "üï∫", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –Ω–∞–¥ –∫–∞–∂–¥–æ–π —á–∞—Å—Ç—å—é —Ç–µ–ª–∞", "desc": "–ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–≤–∏–∂–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏, –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–º–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —á–∞—Å—Ç–µ–π —Ç–µ–ª
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#interpretability", "#reasoning", "#training", "#agents", "#architecture", "#rlhf"], "emoji": "üß†", "ru": {"title": "–°–∏–ª–∞ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è: –∫–∞–∫ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –æ–±—â–µ—Å—Ç–≤–æ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤ —É–ª—É—á—à–∞–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#multimodal", "#agents"], "emoji": "üß†", "ru": {"title": "–õ–∏—á–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –Ω–∞–º–µ—Ä–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ GUI-–∞–≥–µ–Ω—Ç–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PersonalAlign ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è GUI-–∞–≥–µ–Ω—Ç–æ–≤ —Å –Ω–µ—è–≤–Ω—ã–º–∏ –Ω–∞–º–µ—Ä–µ–Ω–∏—è–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —á–µ—Ä–µ
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#alignment", "#security", "#long_context"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ó–∞—â–∏—Ç–∞ –æ—Ç –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–π: –∑–æ–Ω–¥—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—é –º–µ—Ç–æ–¥–æ–≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —Å –ø–æ–º–æ—â—å—é –∑–æ–Ω–¥–æ–≤ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –ê–≤—Ç
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#3d", "#synthetic", "#training", "#benchmark", "#multimodal"], "emoji": "üé®", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–æ—á–Ω—ã—Ö 3D —Ñ–æ—Ä–º –∏–∑ —Å–ª—É—á–∞–π–Ω–æ —Å–Ω—è—Ç—ã—Ö –≤–∏–¥–µ–æ", "desc": "ShapeR ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö 3D –º–æ–¥–µ–ª–µ–π –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –æ–±—ã—á–Ω—ã—Ö —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π. –ú–µ—Ç–æ–¥ –∫–æ
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#rl", "#video", "#training", "#benchmark"], "emoji": "üé¨", "ru": {"title": "–§–∏–∑–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–æ–Ω—ã –∫–∞–∫ –Ω–µ–æ—Ç—ä–µ–º–ª–µ–º–∞—è —á–∞—Å—Ç—å, –∞ –Ω–µ —É—Å–ª–æ–≤–∏–µ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è –∂—ë—Å—Ç–∫–æ –≤—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –∑–∞–∫
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#benchmark", "#inference", "#training", "#data"], "emoji": "üìä", "ru": {"title": "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ LLM —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Ñ–∏–ª–∏ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤—ã—á–∏—Å–ª—è–µ–º—ã–µ –∏–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –ø–æ—Å–ª–µ
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#open_source", "#reasoning", "#benchmark", "#agents", "#dataset"], "emoji": "üõ∞Ô∏è", "ru": {"title": "–ö–æ–≥–¥–∞ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Å —Ä–µ–∞–ª—å–Ω–æ–π —Ñ–∏–∑–∏–∫–æ–π –∫–æ—Å–º–æ—Å–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ AstroReason-Bench ‚Äî –Ω–æ–≤–∞—è –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç—Å–∫–∏—Ö LLM —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#multilingual", "#alignment", "#open_source", "#training"], "emoji": "üåç", "ru": {"title": "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –∫–∞–∫ –ø—É—Ç—å –∫ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—é –≤—ã—Ö–æ–¥–æ–≤ LLM", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫–æ–Ω—Ç—Ä–æ–ª—å —è–∑—ã–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#architecture", "#reasoning", "#multimodal", "#diffusion", "#video"], "emoji": "üé¨", "ru": {"title": "–§–∏–∑–∏–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ, —Å–∏–Ω—Ç–µ–∑ –æ—Ç–¥–µ–ª—å–Ω–æ: —Ç—Ä—ë—Ö—ç—Ç–∞–ø–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω —Ç—Ä—ë—Ö—ç—Ç–∞–ø–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä PhyRPR –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π
[19.01.2026 15:28] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#dataset", "#reasoning", "#optimization", "#open_source", "#architecture", "#benchmark", "#synthetic", "#training"], "emoji": "üñºÔ∏è", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Å–ª–µ–ø–æ—Ç—ã: –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–∏–¥–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –≤—ã—è–≤–ª–µ–Ω
[19.01.2026 15:28] Renaming data file.
[19.01.2026 15:28] Renaming previous data. hf_papers.json to ./d/2026-01-19.json
[19.01.2026 15:28] Saving new data file.
[19.01.2026 15:28] Generating page.
[19.01.2026 15:28] Renaming previous page.
[19.01.2026 15:28] Renaming previous data. index.html to ./d/2026-01-19.html
[19.01.2026 15:28] Writing result.
[19.01.2026 15:28] Renaming log file.
[19.01.2026 15:28] Renaming previous data. log.txt to ./logs/2026-01-19_last_log.txt
