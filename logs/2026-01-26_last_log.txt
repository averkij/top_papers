[25.01.2026 18:39] Read previous papers.
[25.01.2026 18:39] Generating top page (month).
[25.01.2026 18:39] Writing top page (month).
[26.01.2026 01:59] Read previous papers.
[26.01.2026 01:59] Get feed.
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15876
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14724
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16206
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15165
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15197
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16208
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15892
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16093
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16175
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15621
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.11868
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15727
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15369
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16125
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16163
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16148
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14255
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15224
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15703
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16192
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15778
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15690
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15549
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16134
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16004
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15440
[26.01.2026 01:59] Get page data from previous paper. URL: https://huggingface.co/papers/2601.08118
[26.01.2026 01:59] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.01.2026 01:59] No deleted papers detected.
[26.01.2026 01:59] Downloading and parsing papers (pdf, html). Total: 27.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.15876.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.15876.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.15876.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.14724.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.14724.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.14724.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.16206.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.16206.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.16206.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.15165.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.15165.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.15165.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.15197.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.15197.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.15197.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.16208.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.16208.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.16208.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.15892.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.15892.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.15892.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.16093.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.16093.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.16093.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.16175.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.16175.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.16175.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.15621.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.15621.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.15621.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.11868.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.11868.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.11868.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.15727.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.15727.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.15727.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.15369.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.15369.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.15369.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.16125.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.16125.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.16125.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.16163.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.16163.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.16163.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.16148.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.16148.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.16148.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.14255.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.14255.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.14255.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.15224.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.15224.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.15224.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.15703.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.15703.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.15703.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.16192.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.16192.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.16192.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.15778.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.15778.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.15778.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.15690.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.15690.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.15690.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.15549.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.15549.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.15549.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.16134.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.16134.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.16134.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.16004.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.16004.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.16004.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.15440.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.15440.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.15440.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Downloading and parsing paper https://huggingface.co/papers/2601.08118.
[26.01.2026 01:59] Extra JSON file exists (./assets/json/2601.08118.json), skip PDF parsing.
[26.01.2026 01:59] Paper image links file exists (./assets/img_data/2601.08118.json), skip HTML parsing.
[26.01.2026 01:59] Success.
[26.01.2026 01:59] Enriching papers with extra data.
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 0. EvoCUA introduces an evolutionary approach to computer-use agents that combines autonomous task generation with policy optimization to achieve superior performance in complex, long-horizon tasks.  					AI-generated summary 				 The development of native computer-use agents (CUA) represents a signifi...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 1. HERMES is a training-free architecture that enables real-time video stream understanding by utilizing a hierarchical memory framework based on KV cache reuse, achieving faster response times and maintained accuracy even with reduced video token input.  					AI-generated summary 				 Recent advanceme...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 2. LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.  					AI-generated summary 				 We introduce LLM-in-Sandbox, enabling LLMs to...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 3. Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.  					AI-generated summary 				 Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 4. BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.  					AI-generated summary 				 Vision-Language-Action (VLA) models have shown promise in robot manipulation bu...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 5. Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.  					AI-generated summary 				 Repre...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 6. Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.  					AI-generated summary 				 Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation a...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 7. SAMTok enables pixel-wise capabilities in multi-modal LLMs through discrete mask tokenization and standard training methods, achieving state-of-the-art performance on various vision-language tasks.  					AI-generated summary 				 Pixel-wise capabilities are essential for building interactive intelli...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 8. Test-time training enables AI systems to discover optimal solutions for specific scientific problems through continual learning focused on individual challenges rather than generalization.  					AI-generated summary 				 How can we use AI to discover a new state of the art for a scientific problem? ...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 9. The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.  					AI-generated summary 				 In this report, we p...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 10. Terminal-Bench 2.0 presents a challenging benchmark with 89 terminal-based tasks to evaluate AI agents' capabilities in real-world scenarios.  					AI-generated summary 				 AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmar...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 11. Large language models and agent-based systems are being leveraged to automate kernel generation and optimization, addressing the scalability challenges in hardware-specific code development through structured approaches and systematic benchmarking.  					AI-generated summary 				 The performance of ...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 12. An advanced vision encoder named OpenVision 3 learns a unified visual representation for both image understanding and generation by combining VAE-compressed image latents with ViT architecture and joint optimization of reconstruction and semantic signals.  					AI-generated summary 				 This paper p...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 13. A novel fine-grained composed image retrieval benchmark is introduced through image editing techniques, revealing significant capability gaps in existing multimodal models and exposing limitations of current benchmarks.  					AI-generated summary 				 Composed Image Retrieval (CIR) is a pivotal and ...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 14. A pretrained video model is adapted into a robot policy through single-stage post-training, enabling direct action generation and planning capabilities without architectural modifications.  					AI-generated summary 				 Recent video generation models demonstrate remarkable ability to capture comple...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 15. ActionMesh generates animated 3D meshes by extending 3D diffusion models with a temporal axis, producing high-quality, rig-free animations from various input types while maintaining topological consistency.  					AI-generated summary 				 Generating animated 3D objects is at the heart of many applic...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 16. VideoMaMa converts coarse masks to accurate alpha mattes using pretrained video diffusion models, enabling zero-shot generalization and scalable pseudo-labeling for video matting.  					AI-generated summary 				 Generalizing video matting models to real-world videos remains a significant challenge d...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 17. Vision-Language Models struggle with task progress estimation from partial observations, requiring new approaches like ProgressLM-45K and ProgressLM-3B to improve reasoning capabilities.  					AI-generated summary 				 Estimating task progress requires reasoning over long-horizon dynamics rather tha...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 18. A unified dual-process framework transforms verbalized uncertainty into active control signals for improved reasoning reliability in AI agents.  					AI-generated summary 				 Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hamper...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 19. A geometry-free framework using pre-trained diffusion transformers lifts perspective images and videos to 360¬∞ panoramas without requiring camera metadata, achieving state-of-the-art performance through token sequence processing and addressing seam artifacts via circular latent encoding.  					AI-ge...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 20. Agentic confidence calibration addresses limitations of static calibration methods by introducing a trajectory-based diagnostic framework that improves reliability across diverse AI agent systems.  					AI-generated summary 				 AI agents are rapidly advancing from passive language models to autonom...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 21. Large language models face reliability challenges that are being addressed through uncertainty as an active control signal across advanced reasoning, autonomous agents, and reinforcement learning, supported by theoretical frameworks like Bayesian methods and conformal prediction.  					AI-generated ...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 22. VIOLA is a label-efficient framework that combines minimal expert supervision with abundant unlabeled data to enable effective multimodal large language model adaptation in low-resource video domains through density-uncertainty-weighted sampling and confidence-aware retrieval mechanisms.  					AI-ge...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 23. A systematic evaluation framework using tournament-style testing and Glicko2 rating system demonstrates effective prompt engineering for educational LLM applications, prioritizing metacognitive learning strategies.  					AI-generated summary 				 As large language models (LLMs) become increasingly c...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 24. Implementation and benchmarking of quantum circuits for estimating operational inter-branch communication witnesses on IBM Quantum hardware demonstrates visibility and coherence witness measurements under realistic device conditions.  					AI-generated summary 				 We implement and benchmark on IBM ...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 25. We present dla-ideal-solver, a high-performance framework for simulating two-dimensional Diffusion-Limited Aggregation (DLA) using Numba-accelerated Python. By leveraging just-in-time (JIT) compilation, we achieve computational throughput comparable to legacy static implementations while retaining h...
[26.01.2026 01:59] ********************************************************************************
[26.01.2026 01:59] Abstract 26. Large language models are evaluated as user simulators through a reproducible benchmark framework that assesses their ability to generate human-like conversational responses across diverse tasks.  					AI-generated summary 				 Large language models (LLMs) are increasingly used as human simulators, ...
[26.01.2026 01:59] Read previous papers.
[26.01.2026 01:59] Generating reviews via LLM API.
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#training", "#benchmark", "#rl"], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∞–≤—Ç–æ–Ω–æ–º–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏", "desc": "EvoCUA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#multimodal", "#inference", "#video"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è", "desc": "HERMES ‚Äî —ç—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å–∏—Å—Ç–µ–º–µ –ø–∞–º—è—Ç–∏ —Å –ø–µ—Ä–µ–∏
[26.01.2026 01:59] Using data from previous issue: {"categories": [], "emoji": "üèúÔ∏è", "ru": {"title": "–í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞ –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä —Ä–∞–∑—É–º–∞ –º–æ–¥–µ–ª–∏", "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LLM-in-Sandbox ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å –∫–æ–¥–æ–º –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–µ–¥–º–µ—Ç–Ω—ã—Ö –æ
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#math", "#optimization", "#diffusion", "#reasoning", "#rl", "#rlhf", "#training"], "emoji": "üîÑ", "ru": {"title": "–ì–∏–±–∫–æ—Å—Ç—å, —Å—Ç–∞–≤—à–∞—è –ª–æ–≤—É—à–∫–æ–π: –ø–æ—á–µ–º—É —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ª—É—á—à–µ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –≥–µ
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#multimodal", "#alignment", "#robotics", "#reasoning", "#training"], "emoji": "ü§ñ", "ru": {"title": "–Ø–∑—ã–∫ –≤ –¥–µ–π—Å—Ç–≤–∏–∏: –±–∞–π–µ—Å–æ–≤—Å–∫–æ–µ –∑–∞–∑–µ–º–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω BayesianVLA ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è vision-language-action –º–æ–¥–µ–ª–µ–π —Ä–æ–±–æ—Ç–æ
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#diffusion", "#synthetic", "#open_source", "#architecture", "#training"], "emoji": "üñºÔ∏è", "ru": {"title": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç VAE –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã Representation Autoencoders (RAE) ‚Äî –∞—Ä—Ö–∏—Ç
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#low_resource", "#optimization", "#plp", "#diffusion", "#architecture", "#training"], "emoji": "üß©", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–≤—Ç–æ—Ä¬≠–µ–≥—Ä–µ—Å—Å–∏—é –≤ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –∫–æ–¥–∞", "desc": "Stable-DiffCoder ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#dataset", "#rl", "#training", "#benchmark", "#cv", "#multimodal"], "emoji": "üé≠", "ru": {"title": "–ü–∏–∫—Å–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –¥–∏—Å–∫—Ä–µ—Ç–Ω—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –º–∞—Å–æ–∫", "desc": "SAMTok ‚Äî —ç—Ç–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –º–∞—Å–æ–∫, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –º–∞—Å–∫–∏ —Ä–µ–≥–∏–æ–Ω–æ–≤ –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#math", "#optimization", "#science", "#rl", "#open_source", "#training"], "emoji": "üéØ", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ Test-Time Training to Discover (TTT-Discover), –∫–æ—Ç–æ—Ä—ã
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#multilingual", "#multimodal", "#audio", "#dataset", "#open_source", "#architecture"], "emoji": "üé§", "ru": {"title": "–¢—Ä—ë—Ö—Å–µ–∫—É–Ω–¥–Ω–æ–µ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–∞ —Å —É–ª—å—Ç—Ä–∞–Ω–∏–∑–∫–æ–π –∑–∞–¥–µ—Ä–∂–∫–æ–π", "desc": "Qwen3-TTS ‚Äî —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#agents"], "emoji": "üñ•Ô∏è", "ru": {"title": "–†–µ–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ AI –∞–≥–µ–Ω—Ç–æ–≤ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞–±–æ—Ç", "desc": "Terminal-Bench 2.0 ‚Äî —ç—Ç–æ —Å–ª–æ–∂–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 89 –∑–∞–¥–∞—á –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ä–µ–¥–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π AI –∞
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#survey", "#open_source", "#benchmark", "#agents", "#optimization", "#science", "#plp", "#dataset"], "emoji": "‚öôÔ∏è", "ru": {"title": "–û—Ç —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—ã –∫ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏: LLM-–∞–≥–µ–Ω—Ç—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —è–¥–µ—Ä", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#multimodal", "#open_source", "#architecture", "#cv"], "emoji": "üé®", "ru": {"title": "–ï–¥–∏–Ω–æ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç OpenVision 3 ‚Äî –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –µ–¥–∏–Ω–æ–º—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#dataset", "#cv"], "emoji": "üîç", "ru": {"title": "–¢–æ–Ω–∫–æ–∑–µ—Ä–Ω–∏—Å—Ç—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Å–ª–∞–±–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö EDIR –¥–ª—è –∑–∞–¥–∞—á–∏ –ø–æ–∏—Å–∫–∞ —Å–æ—Å—Ç–∞–≤–Ω
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#benchmark", "#robotics", "#video", "#training"], "emoji": "ü§ñ", "ru": {"title": "–û—Ç –≤–∏–¥–µ–æ–ø—Ä–µ–¥–∏–∫—Ü–∏–∏ –∫ —Ä–æ–±–æ—Ç-–ø–æ–ª–∏—Ç–∏–∫–µ –≤ –æ–¥–∏–Ω —ç—Ç–∞–ø –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Cosmos Policy ‚Äî –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–∏ –≤ –ø–æ–ª–∏—Ç–∏–∫—É —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —ç—Ç–∞–ø –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è 
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#video", "#multimodal", "#architecture", "#diffusion", "#3d"], "emoji": "üé¨", "ru": {"title": "–í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∞–Ω–∏–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-—Å–µ—Ç–æ–∫", "desc": "ActionMesh ‚Äî —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç 3D –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –æ—Å—å—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∏–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö 3D-
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#video", "#open_source", "#diffusion", "#synthetic", "#multimodal", "#dataset"], "emoji": "üé¨", "ru": {"title": "–û—Ç –≥—Ä—É–±—ã—Ö –º–∞—Å–æ–∫ –∫ —Ç–æ—á–Ω—ã–º –º–∞—Ç—Ç–∞–º: –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –≤–∏–¥–µ–æ –∫–æ–º–ø–æ–∑–∏—Ç–∏–Ω–≥ —Å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å VideoMaMa, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≥—Ä—É–±—ã–µ –º–∞
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#dataset", "#multimodal", "#small_models", "#training", "#benchmark"], "emoji": "üìä", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤–∏–¥–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ –ø–æ–Ω–∏–º–∞—Ç—å —Ö–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –≤ Vision-Language Models –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–∏—á–Ω—ã—Ö –Ω–∞–±
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#training", "#benchmark", "#agents"], "emoji": "üéØ", "ru": {"title": "–ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ —Å–æ–º–Ω–µ–Ω–∏–π –≤ –¥–µ–π—Å—Ç–≤–∏—è: –∞–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å—é –¥–ª—è –Ω–∞–¥—ë–∂–Ω—ã—Ö AI –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –µ–¥–∏–Ω—É—é –¥–≤—É—Ö–ø—Ä–æ—Ü–µ—Å—Å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å—é –≤ AI –∞–≥–µ–Ω—Ç–∞—Ö, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#cv", "#video", "#architecture", "#diffusion", "#3d", "#multimodal"], "emoji": "üåê", "ru": {"title": "–û—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã –∫ –ø–∞–Ω–æ—Ä–∞–º–µ: –≥–µ–æ–º–µ—Ç—Ä–∏—è –±–µ–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–∞–º–µ—Ä—ã", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å 360Anything, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ–±—ã—á–Ω—ã–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –≤–∏
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#benchmark", "#agents"], "emoji": "üéØ", "ru": {"title": "–û—Ç —Å—Ç–∞—Ç–∏—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∫ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ: –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –Ω–∞–¥–µ–∂–Ω—ã—Ö AI-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–∞–ª–∏–±—Ä–æ–≤–∫–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–µ –∑–∞–¥–∞—á–∏. –ê–≤—Ç–æ—Ä—ã 
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#survey", "#agents", "#training", "#reasoning", "#rl", "#alignment"], "emoji": "üéØ", "ru": {"title": "–û—Ç –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é: –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å –∫–∞–∫ –∞–∫—Ç–∏–≤–Ω—ã–π —Å–∏–≥–Ω–∞–ª –∫–æ–Ω—Ç—Ä–æ–ª—è –≤ LLM", "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è —ç–≤–æ–ª—é—Ü–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –æ—Ç –ø–∞—Å—Å
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#transfer_learning", "#video", "#data", "#training", "#multimodal", "#low_resource"], "emoji": "üé¨", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –≤–∏–¥–µ–æ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π", "desc": "VIOLA ‚Äî —ç—Ç–æ —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#survey"], "emoji": "üéì", "ru": {"title": "–û—Ç –∏–Ω—Ç—É–∏—Ü–∏–∏ –∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º: –Ω–∞—É—á–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —Ç—É—Ä–Ω–∏—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∏ —Ä–µ
[26.01.2026 01:59] Using data from previous issue: {"categories": [], "emoji": "‚öõÔ∏è", "ru": {"title": "–ö–≤–∞–Ω—Ç–æ–≤—ã–µ —Å–≤–∏–¥–µ—Ç–µ–ª–∏ –º–µ–∂–≤–µ—Ç–≤–µ–≤—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã —Ä–µ–∞–ª–∏–∑—É—é—Ç –∏ —Ç–µ—Å—Ç–∏—Ä—É—é—Ç –Ω–∞ –∫–≤–∞–Ω—Ç–æ–≤–æ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ IBM —Å–µ–º–µ–π—Å—Ç–≤–æ –∫–≤–∞–Ω—Ç–æ–≤—ã—Ö —Å—Ö–µ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–≤–∏–¥–µ—Ç–µ–ª–µ–π –º–µ–∂–≤–µ—Ç–≤–µ–≤—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –≤ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –∏–∑–º–µ—Ä–µ–Ω–∏—è—Ö, –ø–æ
[26.01.2026 01:59] Using data from previous issue: {"categories": [], "emoji": "üåø", "ru": {"title": "–û—Ç —Ö–∞–æ—Å–∞ –∫ –ø–æ—Ä—è–¥–∫—É: —Ñ–∞–∑–æ–≤—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ dla-ideal-solver –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–≤—É–º–µ—Ä–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º JIT-–∫–æ–º–ø–∏–ª—è—Ü–∏–∏ –Ω–∞ Numba. –ê–≤
[26.01.2026 01:59] Using data from previous issue: {"categories": ["#dataset", "#benchmark", "#agents", "#open_source"], "emoji": "ü™û", "ru": {"title": "–ó–µ—Ä–∫–∞–ª—å–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ: –æ—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –¥–∏–∞–ª–æ–≥–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç MIRRORBENCH ‚Äî –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫
[26.01.2026 01:59] Renaming data file.
[26.01.2026 01:59] Renaming previous data. hf_papers.json to ./d/2026-01-26.json
[26.01.2026 01:59] Saving new data file.
[26.01.2026 01:59] Generating page.
[26.01.2026 01:59] Renaming previous page.
[26.01.2026 01:59] Renaming previous data. index.html to ./d/2026-01-26.html
[26.01.2026 01:59] Writing result.
[26.01.2026 01:59] Renaming log file.
[26.01.2026 01:59] Renaming previous data. log.txt to ./logs/2026-01-26_last_log.txt
