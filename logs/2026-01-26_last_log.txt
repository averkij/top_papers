[26.01.2026 07:30] Read previous papers.
[26.01.2026 07:30] Generating top page (month).
[26.01.2026 07:30] Writing top page (month).
[26.01.2026 08:36] Read previous papers.
[26.01.2026 08:36] Get feed.
[26.01.2026 08:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16746
[26.01.2026 08:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14133
[26.01.2026 08:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16725
[26.01.2026 08:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16973
[26.01.2026 08:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.14243
[26.01.2026 08:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.07251
[26.01.2026 08:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16515
[26.01.2026 08:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16296
[26.01.2026 08:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16443
[26.01.2026 08:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16344
[26.01.2026 08:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.15808
[26.01.2026 08:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.13118
[26.01.2026 08:36] Get page data from previous paper. URL: https://huggingface.co/papers/2601.16018
[26.01.2026 08:36] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.01.2026 08:36] No deleted papers detected.
[26.01.2026 08:36] Downloading and parsing papers (pdf, html). Total: 13.
[26.01.2026 08:36] Downloading and parsing paper https://huggingface.co/papers/2601.16746.
[26.01.2026 08:36] Extra JSON file exists (./assets/json/2601.16746.json), skip PDF parsing.
[26.01.2026 08:36] Paper image links file exists (./assets/img_data/2601.16746.json), skip HTML parsing.
[26.01.2026 08:36] Success.
[26.01.2026 08:36] Downloading and parsing paper https://huggingface.co/papers/2601.14133.
[26.01.2026 08:36] Extra JSON file exists (./assets/json/2601.14133.json), skip PDF parsing.
[26.01.2026 08:36] Paper image links file exists (./assets/img_data/2601.14133.json), skip HTML parsing.
[26.01.2026 08:36] Success.
[26.01.2026 08:36] Downloading and parsing paper https://huggingface.co/papers/2601.16725.
[26.01.2026 08:36] Extra JSON file exists (./assets/json/2601.16725.json), skip PDF parsing.
[26.01.2026 08:36] Paper image links file exists (./assets/img_data/2601.16725.json), skip HTML parsing.
[26.01.2026 08:36] Success.
[26.01.2026 08:36] Downloading and parsing paper https://huggingface.co/papers/2601.16973.
[26.01.2026 08:36] Extra JSON file exists (./assets/json/2601.16973.json), skip PDF parsing.
[26.01.2026 08:36] Paper image links file exists (./assets/img_data/2601.16973.json), skip HTML parsing.
[26.01.2026 08:36] Success.
[26.01.2026 08:36] Downloading and parsing paper https://huggingface.co/papers/2601.14243.
[26.01.2026 08:36] Extra JSON file exists (./assets/json/2601.14243.json), skip PDF parsing.
[26.01.2026 08:36] Paper image links file exists (./assets/img_data/2601.14243.json), skip HTML parsing.
[26.01.2026 08:36] Success.
[26.01.2026 08:36] Downloading and parsing paper https://huggingface.co/papers/2601.07251.
[26.01.2026 08:36] Extra JSON file exists (./assets/json/2601.07251.json), skip PDF parsing.
[26.01.2026 08:36] Paper image links file exists (./assets/img_data/2601.07251.json), skip HTML parsing.
[26.01.2026 08:36] Success.
[26.01.2026 08:36] Downloading and parsing paper https://huggingface.co/papers/2601.16515.
[26.01.2026 08:36] Extra JSON file exists (./assets/json/2601.16515.json), skip PDF parsing.
[26.01.2026 08:36] Paper image links file exists (./assets/img_data/2601.16515.json), skip HTML parsing.
[26.01.2026 08:36] Success.
[26.01.2026 08:36] Downloading and parsing paper https://huggingface.co/papers/2601.16296.
[26.01.2026 08:36] Extra JSON file exists (./assets/json/2601.16296.json), skip PDF parsing.
[26.01.2026 08:36] Paper image links file exists (./assets/img_data/2601.16296.json), skip HTML parsing.
[26.01.2026 08:36] Success.
[26.01.2026 08:36] Downloading and parsing paper https://huggingface.co/papers/2601.16443.
[26.01.2026 08:36] Extra JSON file exists (./assets/json/2601.16443.json), skip PDF parsing.
[26.01.2026 08:36] Paper image links file exists (./assets/img_data/2601.16443.json), skip HTML parsing.
[26.01.2026 08:36] Success.
[26.01.2026 08:36] Downloading and parsing paper https://huggingface.co/papers/2601.16344.
[26.01.2026 08:36] Extra JSON file exists (./assets/json/2601.16344.json), skip PDF parsing.
[26.01.2026 08:36] Paper image links file exists (./assets/img_data/2601.16344.json), skip HTML parsing.
[26.01.2026 08:36] Success.
[26.01.2026 08:36] Downloading and parsing paper https://huggingface.co/papers/2601.15808.
[26.01.2026 08:36] Extra JSON file exists (./assets/json/2601.15808.json), skip PDF parsing.
[26.01.2026 08:36] Paper image links file exists (./assets/img_data/2601.15808.json), skip HTML parsing.
[26.01.2026 08:36] Success.
[26.01.2026 08:36] Downloading and parsing paper https://huggingface.co/papers/2601.13118.
[26.01.2026 08:36] Extra JSON file exists (./assets/json/2601.13118.json), skip PDF parsing.
[26.01.2026 08:36] Paper image links file exists (./assets/img_data/2601.13118.json), skip HTML parsing.
[26.01.2026 08:36] Success.
[26.01.2026 08:36] Downloading and parsing paper https://huggingface.co/papers/2601.16018.
[26.01.2026 08:36] Extra JSON file exists (./assets/json/2601.16018.json), skip PDF parsing.
[26.01.2026 08:36] Paper image links file exists (./assets/img_data/2601.16018.json), skip HTML parsing.
[26.01.2026 08:36] Success.
[26.01.2026 08:36] Enriching papers with extra data.
[26.01.2026 08:36] ********************************************************************************
[26.01.2026 08:36] Abstract 0. SWE-Pruner is a self-adaptive context pruning framework for coding agents that uses task-aware pruning to reduce token usage while maintaining performance.  					AI-generated summary 				 LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered ...
[26.01.2026 08:36] ********************************************************************************
[26.01.2026 08:36] Abstract 1. TwinBrainVLA addresses the tension between semantic understanding and motor skills in robot control by coordinating a generalist vision-language model with a specialist model through an asymmetric mixture-of-transformers mechanism.  					AI-generated summary 				 Standard Vision-Language-Action (VLA...
[26.01.2026 08:36] ********************************************************************************
[26.01.2026 08:36] Abstract 2. A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for complex tool interactions and real-world robustness.  					AI...
[26.01.2026 08:36] ********************************************************************************
[26.01.2026 08:36] Abstract 3. Modern vision-language models exhibit significant challenges in multi-step visual interaction tasks, particularly in long-horizon perception-memory-action integration, with performance declining when handling unbounded historical contexts.  					AI-generated summary 				 Modern Vision-Language Model...
[26.01.2026 08:36] ********************************************************************************
[26.01.2026 08:36] Abstract 4. Quantized reinforcement learning training using FP8 precision faces stability issues due to numerical mismatches between training and inference phases, but a unified FP8 framework achieves significant speedups with stable convergence.  					AI-generated summary 				 Reinforcement learning (RL) is es...
[26.01.2026 08:36] ********************************************************************************
[26.01.2026 08:36] Abstract 5. MeepleLM enables human-AI collaboration in board game design by providing constructive critique through persona-specific reasoning patterns that align with player experiences.  					AI-generated summary 				 Recent advancements have expanded the role of Large Language Models in board games from play...
[26.01.2026 08:36] ********************************************************************************
[26.01.2026 08:36] Abstract 6. Diffusion Transformers for video generation are enhanced with SALAD, a method that combines linear and sparse attention branches to achieve high sparsity and speedup while maintaining quality and requiring minimal training data.  					AI-generated summary 				 Diffusion Transformers have recently de...
[26.01.2026 08:36] ********************************************************************************
[26.01.2026 08:36] Abstract 7. Memory-V2V enhances multi-turn video editing by maintaining cross-consistency through explicit memory mechanisms and efficient token compression in video-to-video diffusion models.  					AI-generated summary 				 Recent foundational video-to-video diffusion models have achieved impressive results in...
[26.01.2026 08:36] ********************************************************************************
[26.01.2026 08:36] Abstract 8. Endless Terminals introduces an autonomous pipeline for generating procedural terminal tasks that significantly improves agent performance on both synthetic and human-curated benchmarks through scalable reinforcement learning environments.  					AI-generated summary 				 Environments are the bottlen...
[26.01.2026 08:36] ********************************************************************************
[26.01.2026 08:36] Abstract 9. DSGym presents a standardized framework for evaluating data science agents with comprehensive task suites and execution-verified training capabilities.  					AI-generated summary 				 Data science agents promise to accelerate discovery and insight-generation by turning data into executable analyses ...
[26.01.2026 08:36] ********************************************************************************
[26.01.2026 08:36] Abstract 10. A novel self-evolving framework for Deep Research Agents that enhances performance through iterative verification and rubric-based feedback during inference, achieving significant accuracy improvements without additional training.  					AI-generated summary 				 Recent advances in Deep Research Agen...
[26.01.2026 08:36] ********************************************************************************
[26.01.2026 08:36] Abstract 11. Research derives and evaluates prompt optimization guidelines for code generation tasks in software engineering, identifying 10 specific improvement patterns related to input/output specification, conditions, examples, and clarity.  					AI-generated summary 				 Large Language Models (LLMs) are now...
[26.01.2026 08:36] ********************************************************************************
[26.01.2026 08:36] Abstract 12. A framework for developing specialized Turkish legal language models through domain adaptation, featuring a pre-trained encoder model and decoder models with continual pre-training for enhanced legal text processing.  					AI-generated summary 				 This paper presents Mecellem models, a framework fo...
[26.01.2026 08:36] Read previous papers.
[26.01.2026 08:36] Generating reviews via LLM API.
[26.01.2026 08:36] Using data from previous issue: {"categories": ["#long_context", "#benchmark", "#agents", "#inference", "#plp", "#small_models", "#optimization"], "emoji": "‚úÇÔ∏è", "ru": {"title": "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫–æ–¥–∞ —á–µ—Ä–µ–∑ –∑–∞–¥–∞—á–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ–±—Ä–µ–∑–∫—É", "desc": "SWE-Pruner ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –æ–±—Ä–µ–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å–ø–µ—Ü
[26.01.2026 08:36] Using data from previous issue: {"categories": ["#architecture", "#robotics", "#training", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "–î–≤–æ–π–Ω–æ–π –º–æ–∑–≥ —Ä–æ–±–æ—Ç–∞: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –º—ã—à–ª–µ–Ω–∏–µ–º –∏ –ª–æ–≤–∫–æ—Å—Ç—å—é", "desc": "TwinBrainVLA —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ –º–µ–∂–¥—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –º–æ—Ç–æ—Ä–Ω—ã–º–∏ –Ω–∞–≤—ã–∫–∞–º–∏ –≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ —Ä–æ–±–æ—Ç–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è –¥–≤–∞
[26.01.2026 08:36] Using data from previous issue: {"categories": ["#open_source", "#rl", "#agents", "#architecture", "#reasoning", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç —Å –≥–ª—É–±–æ–∫–∏–º –º—ã—à–ª–µ–Ω–∏–µ–º: MoE-–º–æ–¥–µ–ª—å –¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω LongCat-Flash-Thinking-2601 ‚Äî –æ—Ç–∫—Ä—ã—Ç–∞
[26.01.2026 08:36] Using data from previous issue: {"categories": ["#open_source", "#long_context", "#benchmark", "#dataset", "#cv", "#training", "#multimodal"], "emoji": "üéÆ", "ru": {"title": "–ö–æ–≥–¥–∞ –∑—Ä–µ–Ω–∏–µ –∏ –ø–∞–º—è—Ç—å —Ç–µ—Ä—è—é—Ç –¥—Ä—É–≥ –¥—Ä—É–≥–∞: –ø—Ä–æ–±–ª–µ–º—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç VisGym - –Ω–∞–±–æ—Ä –∏–∑ 17 —Å–∏–º—É–ª
[26.01.2026 08:36] Using data from previous issue: {"categories": ["#optimization", "#rl", "#inference", "#training", "#reasoning"], "emoji": "‚ö°", "ru": {"title": "–ï–¥–∏–Ω–∞—è FP8 —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –∏ –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ FP8 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã
[26.01.2026 08:36] Using data from previous issue: {"categories": ["#alignment", "#reasoning", "#games"], "emoji": "üé≤", "ru": {"title": "–í–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫ –∏–≥—Ä —á–µ—Ä–µ–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –æ –¥–∏–Ω–∞–º–∏–∫–µ –≥–µ–π–º–ø–ª–µ—è", "desc": "MeepleLM ‚Äî —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–≥–∞–µ—Ç –¥–∏–∑–∞–π–Ω–µ—Ä–∞–º –Ω–∞—Å—Ç–æ–ª—å–Ω—ã—Ö –∏–≥—Ä –ø—É—Ç—ë–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∫–æ
[26.01.2026 08:36] Using data from previous issue: {"categories": ["#architecture", "#long_context", "#diffusion", "#inference", "#optimization", "#training", "#video"], "emoji": "‚ö°", "ru": {"title": "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –≥–∏–±—Ä–∏–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SALAD –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è Diffusion Transfor
[26.01.2026 08:36] Using data from previous issue: {"categories": ["#video", "#inference", "#architecture", "#optimization", "#diffusion"], "emoji": "üé¨", "ru": {"title": "–ü–∞–º—è—Ç—å –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ–º–æ–Ω—Ç–∞–∂–∞ —á–µ—Ä–µ–∑ –ø–æ–∫–æ–ª–µ–Ω–∏—è –ø—Ä–∞–≤–æ–∫", "desc": "Memory-V2V ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ 
[26.01.2026 08:36] Using data from previous issue: {"categories": ["#benchmark", "#rl", "#agents", "#dataset", "#synthetic", "#transfer_learning", "#training", "#optimization"], "emoji": "üñ•Ô∏è", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Endless Terminals ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤
[26.01.2026 08:36] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#dataset", "#agents", "#synthetic", "#science", "#training"], "emoji": "üß™", "ru": {"title": "–ï–¥–∏–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö", "desc": "DSGym –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ data science —Å
[26.01.2026 08:36] Using data from previous issue: {"categories": ["#open_source", "#benchmark", "#agents", "#dataset", "#inference", "#reasoning", "#training"], "emoji": "üîç", "ru": {"title": "–°–∞–º–æ—É–ª—É—á—à–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é –∏ —Ä—É–±—Ä–∏–∫–∏ –Ω–∞ —ç—Ç–∞–ø–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞
[26.01.2026 08:36] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üí°", "ru": {"title": "–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã–≤–æ–¥–∏—Ç –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ
[26.01.2026 08:36] Using data from previous issue: {"categories": ["#multilingual", "#low_resource", "#open_source", "#transfer_learning", "#small_models", "#optimization", "#training"], "emoji": "‚öñÔ∏è", "ru": {"title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç—É—Ä–µ—Ü–∫–æ–≥–æ –ø—Ä–∞–≤–∞ —á–µ—Ä–µ–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é –∫ –¥–æ–º–µ–Ω—É", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Mecelle
[26.01.2026 08:36] Renaming data file.
[26.01.2026 08:36] Renaming previous data. hf_papers.json to ./d/2026-01-26.json
[26.01.2026 08:36] Saving new data file.
[26.01.2026 08:36] Generating page.
[26.01.2026 08:36] Renaming previous page.
[26.01.2026 08:36] Renaming previous data. index.html to ./d/2026-01-26.html
[26.01.2026 08:36] Writing result.
[26.01.2026 08:36] Renaming log file.
[26.01.2026 08:36] Renaming previous data. log.txt to ./logs/2026-01-26_last_log.txt
