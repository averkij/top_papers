[18.12.2025 05:24] Read previous papers.
[18.12.2025 05:24] Generating top page (month).
[18.12.2025 05:24] Writing top page (month).
[18.12.2025 06:36] Read previous papers.
[18.12.2025 06:36] Get feed.
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15176
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14681
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15693
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14693
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15687
[18.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.15635
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15182
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15713
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09299
[18.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.13190
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15603
[18.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.10863
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15702
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15649
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15431
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15374
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15110
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13874
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15715
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09851
[18.12.2025 06:36] Extract page data from URL. URL: https://huggingface.co/papers/2512.14719
[18.12.2025 06:36] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13077
[18.12.2025 06:36] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[18.12.2025 06:36] No deleted papers detected.
[18.12.2025 06:36] Downloading and parsing papers (pdf, html). Total: 22.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.15176.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.15176.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.15176.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14681.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.14681.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.14681.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.15693.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.15693.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.15693.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14693.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.14693.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.14693.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.15687.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.15687.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.15687.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.15635.
[18.12.2025 06:36] Downloading paper 2512.15635 from https://arxiv.org/pdf/2512.15635v1...
[18.12.2025 06:36] Extracting affiliations from text.
[18.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning Yuanhang Li1, Yiren Song2, Junzhe Bai1, Xinran Liang1, Hu Yang3, Libiao Jin1, Qi Mao1(cid:66) 1 School of Information and Communication Engineering, Communication University of China 2 Show Lab, National University of Singapore 3 Baidu Inc., Beijing, China https://cuc-mipg.github.io/IC-Effect/ 5 2 0 D 7 1 ] . [ 1 5 3 6 5 1 . 2 1 5 2 : r Figure 1. Video VFX Editing Results of IC-Effect. Our IC-Effect enables precise video VFX editing aligned with textual instructions while preserving the complete spatiotemporal information of the source video. The complete video is available in our supplementary materials. "
[18.12.2025 06:36] Response: ```python
[
    "School of Information and Communication Engineering, Communication University of China",
    "Show Lab, National University of Singapore",
    "Baidu Inc., Beijing, China"
]
```
[18.12.2025 06:36] Deleting PDF ./assets/pdf/2512.15635.pdf.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.15182.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.15182.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.15182.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.15713.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.15713.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.15713.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.09299.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.09299.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.09299.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13190.
[18.12.2025 06:36] Downloading paper 2512.13190 from https://arxiv.org/pdf/2512.13190v1...
[18.12.2025 06:36] Extracting affiliations from text.
[18.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 0 9 1 3 1 . 2 1 5 2 : r WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory JIN SOB KIM HYUN JOON PARK WOOSEOK SHIN Korea University, Seoul 02841, Republic of Korea DONGIL PARK SeaVantage, Seoul 06119, Republic of Korea SUNG WON HAN Korea University, Seoul 02841, Republic of Korea Abstract The Automatic Identification System (AIS) has recorded near-real-time vessel monitoring data over the years, paving the way for data-driven maritime surveillance methods; concurrently, the data suffer from unrefined, reliability issues and irregular intervals. In this paper, we address the problem of vessel destination estimation by exploiting the global-scope AIS data. We propose differentiated data-driven approach recasting long sequence of port-to-port international vessel trajectories as nested sequence structure. Based on spatial grids, this approach mitigates the spatio-temporal bias of AIS data while preserving the detailed resolution of the original. Further, we propose novel deep learning architecture (WAY) that is designed to effectively process the reformulated trajectory and perform the long-term estimation of the vessel destination ahead of arrival with horizon of days to weeks. WAY comprises trajectory representation layer and channel-aggregative sequential processing (CASP) blocks. The representation layer produces the multi-channel vector sequence output based on each kinematic and non-kinematic feature collected from AIS data. Then CASP blocks include multi-headed channeland self-attention architectures, where each processes aggregation information delivery respectively. Then, taskand sequential specialized learning technique, Gradient Dropout (GD), is also suggested for adopting many-to-many training along the trajectory progression on single labels. The technique prevents surge of biased feedback by blocking the gradient flow stochastically using the condition depending on the length of training samples. Experimental results on "
[18.12.2025 06:36] Response: ```python
[
    "Korea University, Seoul 02841, Republic of Korea",
    "SeaVantage, Seoul 06119, Republic of Korea"
]
```
[18.12.2025 06:36] Deleting PDF ./assets/pdf/2512.13190.pdf.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.15603.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.15603.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.15603.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.10863.
[18.12.2025 06:36] Downloading paper 2512.10863 from https://arxiv.org/pdf/2512.10863v1...
[18.12.2025 06:36] Extracting affiliations from text.
[18.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 1 1 ] . [ 1 3 6 8 0 1 . 2 1 5 2 : r MMSI-Video-Bench: Holistic Benchmark for Video-Based Spatial Intelligence Jingli Lin1,2 Runsen Xu1,3 Shaohao Zhu1,4 Sihan Yang1 Peizhou Cao1,5 Yunlong Ran1,4 Miao Hu6 Chenming Zhu1,7 Yiman Xie1,4 Yilin Long1,8 Wenbo Hu1,9 Dahua Lin1,3 Tai Wang1(cid:0) Jiangmiao Pang1(cid:0) 1Shanghai AI Laboratory 2Shanghai Jiaotong University 3The Chinese University of Hong Kong 7University of Hong Kong 8Fudan University 4Zhejiang University 5Beihang University 6Xian Jiaotong University 9University of California, Los Angeles Equal Contribution Project Lead MMSI-Video-Bench Figure 1. MMSI-Video-Bench is diverse, human-annotated, and challenging benchmark, designed to evaluate models video-based spatial intelligence, including their ability to perceive, understand, reason, and make decisions over spatio-temporal information in videos. The bar chart in the top-right corner illustrates the substantial performance gap between state-of-the-art models and human performance. "
[18.12.2025 06:36] Response: ```python
[
    "Shanghai AI Laboratory",
    "Shanghai Jiaotong University",
    "The Chinese University of Hong Kong",
    "Zhejiang University",
    "Beihang University",
    "Xian Jiaotong University",
    "University of Hong Kong",
    "Fudan University",
    "University of California, Los Angeles"
]
```
[18.12.2025 06:36] Deleting PDF ./assets/pdf/2512.10863.pdf.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.15702.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.15702.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.15702.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.15649.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.15649.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.15649.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.15431.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.15431.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.15431.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.15374.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.15374.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.15374.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.15110.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.15110.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.15110.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13874.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13874.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13874.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.15715.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.15715.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.15715.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.09851.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.09851.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.09851.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.14719.
[18.12.2025 06:36] Downloading paper 2512.14719 from https://arxiv.org/pdf/2512.14719v1...
[18.12.2025 06:36] Extracting affiliations from text.
[18.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Preprint. Under review. Feng Zhang1,2 Shangyuan Li1,2 Yang Shi1,3 Zhuoran Zhang1,2 Yuanxing Zhang3 Wei Chen1,2 Tengjiao Wang1,2 Kam-Fai Wong4 1School of Computer Science, Peking University 2Key Lab of High Confidence Software Technologies, Peking University 3Kling Team 4Department of Systems Engineering and Engineering Management, CUHK 5 2 0 2 9 ] . [ 1 9 1 7 4 1 . 2 1 5 2 : r a "
[18.12.2025 06:36] Response: ```python
[
    "School of Computer Science, Peking University",
    "Key Lab of High Confidence Software Technologies, Peking University",
    "Kling Team",
    "Department of Systems Engineering and Engineering Management, CUHK"
]
```
[18.12.2025 06:36] Deleting PDF ./assets/pdf/2512.14719.pdf.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Downloading and parsing paper https://huggingface.co/papers/2512.13077.
[18.12.2025 06:36] Extra JSON file exists (./assets/json/2512.13077.json), skip PDF parsing.
[18.12.2025 06:36] Paper image links file exists (./assets/img_data/2512.13077.json), skip HTML parsing.
[18.12.2025 06:36] Success.
[18.12.2025 06:36] Enriching papers with extra data.
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 0. DEER framework uses diffusion large language models for efficient speculative decoding, overcoming the limitations of autoregressive drafters with better speed and draft quality.  					AI-generated summary 				 Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning system...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 1. Jacobi Forcing is a progressive distillation method that enables efficient parallel decoding of transformer-based models while maintaining performance, significantly reducing inference latency.  					AI-generated summary 				 Multi-token generation has emerged as a promising paradigm for acceleratin...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 2. Skyra, a specialized multimodal large language model, detects and explains visual artifacts in AI-generated videos using a novel dataset and two-stage training strategy, outperforming existing methods.  					AI-generated summary 				 The misuse of AI-driven video generation technologies has raised s...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 3. The Universal Reasoning Model enhances Universal Transformers with short convolution and truncated backpropagation to improve reasoning performance on ARC-AGI tasks.  					AI-generated summary 				 Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sud...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 4. G2RL, a gradient-guided reinforcement learning framework, enhances exploration in large language models by leveraging the model's own update geometry, leading to improved performance on various reasoning benchmarks.  					AI-generated summary 				 Reinforcement learning has become essential for stre...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 5. IC-Effect, an instruction-guided DiT-based framework, synthesizes complex video VFX effects while preserving spatial and temporal consistency using a two-stage training strategy and spatiotemporal sparse tokenization.  					AI-generated summary 				 We propose IC-Effect, an instruction-guided, DiT-b...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 6. A resynthesis framework enhances deepfake detection by verifying authenticity with low false positive rates and robustness against efficient adversaries, supporting multiple modalities.  					AI-generated summary 				 Generative models can synthesize highly realistic content, so-called deepfakes, th...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 7. DiffusionVL, a family of diffusion vision language models derived from autoregressive models through fine-tuning, achieves performance improvements and faster inference speeds compared to existing models.  					AI-generated summary 				 In recent multimodal research, the diffusion paradigm has emerg...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 8. VABench is a benchmark framework for evaluating audio-video generation models, covering text-to-audio-video, image-to-audio-video, and stereo audio-video tasks with 15 evaluation dimensions.  					AI-generated summary 				 Recent advances in video generation have been remarkable, enabling models to ...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 9. A novel deep learning architecture, WAY, uses nested sequence structures and spatial grids for accurate long-term vessel destination estimation from AIS data, incorporating CASP blocks and Gradient Dropout for improved performance.  					AI-generated summary 				 The Automatic Identification System ...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 10. Qwen-Image-Layered decomposes images into semantically disentangled RGBA layers using a diffusion model, enabling independent editing of each layer and improving decomposition quality and consistency.  					AI-generated summary 				 Recent visual generative models often struggle with consistency dur...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 11. MMSI-Video-Bench is a comprehensive benchmark for video-based spatial intelligence in MLLMs, revealing significant gaps between human and AI performance and highlighting challenges in geometric reasoning, motion grounding, and cross-video correspondence.  					AI-generated summary 				 Spatial under...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 12. Resampling Forcing is introduced as a teacher-free framework to train autoregressive video diffusion models with improved temporal consistency using self-resampling and history routing.  					AI-generated summary 				 Autoregressive video diffusion models hold promise for world simulation but are vu...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 13. A benchmark evaluates the performance of vision-language models on understanding long-context information compressed into dense visual representations, revealing significant limitations in capturing long-term dependencies.  					AI-generated summary 				 The computational and memory overheads associ...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 14. A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.  					AI-generated summary 				 Recent advances in multimodal large language models unlock unprecedented opportunities for G...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 15. SCOPE enhances LLM agents' context management through prompt evolution, improving task success rates in dynamic environments without human intervention.  					AI-generated summary 				 Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 16. Nano Banana Pro excels in subjective visual quality across low-level vision tasks without fine-tuning but struggles with traditional reference-based quantitative metrics due to generative model stochasticity.  					AI-generated summary 				 The rapid evolution of text-to-image generation models has ...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 17. The paper proposes SAGE, a multi-turn reasoning system for video that mimics human behavior, using synthetic data and reinforcement learning to improve performance on long videos.  					AI-generated summary 				 As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iterativ...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 18. Pixio, an enhanced masked autoencoder, demonstrates competitive performance across various downstream tasks using pixel-space self-supervised learning, outperforming latent-space approaches.  					AI-generated summary 				 At the most basic level, pixels are the source of the visual information thro...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 19. TacThru-UMI, a system combining a TacThru sensor with a Transformer-based Diffusion Policy, achieves superior performance in robotic manipulation tasks by integrating simultaneous multimodal perception.  					AI-generated summary 				 Robotic manipulation requires both rich multimodal perception and...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 20. A novel framework, Class-Aware Attribution Prior (CAP), enhances language model interpretability and robustness by guiding the model to capture fine-grained class distinctions and combining with existing attribution methods.  					AI-generated summary 				 Small language models (SLMs) are widely use...
[18.12.2025 06:36] ********************************************************************************
[18.12.2025 06:36] Abstract 21. LikeBench introduces a multi-session evaluation framework to measure the likability of LLMs by their ability to adapt to user preferences across multiple dimensions, demonstrating that strong memory performance does not necessarily equate to higher likability.  					AI-generated summary 				 A perso...
[18.12.2025 06:36] Read previous papers.
[18.12.2025 06:36] Generating reviews via LLM API.
[18.12.2025 06:36] Using data from previous issue: {"categories": ["#inference", "#training", "#diffusion", "#architecture", "#optimization", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–º–µ—Å—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏: —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ DEER ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥
[18.12.2025 06:36] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–≤–æ–π—Å—Ç–≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Jacobi Forcing ‚Äî –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ
[18.12.2025 06:36] Using data from previous issue: {"categories": ["#training", "#multimodal", "#dataset", "#benchmark", "#video"], "emoji": "üé•", "ru": {"title": "–û–±—ä—è—Å–Ω–∏–º–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "Skyra ‚Äî —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±–Ω–∞—Ä—É–∂–∏
[18.12.2025 06:36] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#agi", "#architecture", "#open_source"], "emoji": "üß†", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏ —Å–∂–∞—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–Ω–∞–ª–∏–∑—É Universal Transformers –¥–ª—è –∑–∞–¥–∞—á —Å–ª–æ–∂–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ ARC-AGI. –ê–≤—Ç–æ—Ä—ã –ø
[18.12.2025 06:36] Using data from previous issue: {"categories": ["#rl", "#small_models", "#reasoning", "#training", "#rlhf", "#optimization", "#math"], "emoji": "üß≠", "ru": {"title": "–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–≤–µ–¥–∫–∞: –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —Å–∞–º–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –ø—É—Ç—å –∫ –ª—É—á—à–µ–º—É –æ–±—É—á–µ–Ω–∏—é", "desc": "G2RL ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞
[18.12.2025 06:36] Querying the API.
[18.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

IC-Effect, an instruction-guided DiT-based framework, synthesizes complex video VFX effects while preserving spatial and temporal consistency using a two-stage training strategy and spatiotemporal sparse tokenization.  					AI-generated summary 				 We propose IC-Effect, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning 15 high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.
[18.12.2025 06:36] Response: ```json
{
  "desc": "IC-Effect ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª–∏ (DiT) –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —Å –Ω–µ–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –æ–≥–æ–Ω—å –∏ —á–∞—Å—Ç–∏—Ü—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –∏ –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –±–ª–∞–≥–æ–¥–∞—Ä—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∫–∞–∫ —É—Å–ª–æ–≤–∏—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –î–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—é—â–∞—è –æ–±—â—É—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ LoRA, –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ç–æ—á–Ω–æ–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏ –Ω–∞–¥–µ–∂–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤. –í–≤–µ–¥–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ç–µ–∑–∞.",
  "emoji": "‚ú®",
  "title": "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ —É–ø—Ä–∞–≤–ª—è–µ–º—ã–π —Å–∏–Ω—Ç–µ–∑ –≤–∏–¥–µ–æ—ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏"
}
```
[18.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"IC-Effect, an instruction-guided DiT-based framework, synthesizes complex video VFX effects while preserving spatial and temporal consistency using a two-stage training strategy and spatiotemporal sparse tokenization.  					AI-generated summary 				 We propose IC-Effect, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning 15 high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation."

[18.12.2025 06:36] Response: ```python
["VIDEO", "DATASET", "ARCHITECTURE", "TRAINING"]
```
[18.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"IC-Effect, an instruction-guided DiT-based framework, synthesizes complex video VFX effects while preserving spatial and temporal consistency using a two-stage training strategy and spatiotemporal sparse tokenization.  					AI-generated summary 				 We propose IC-Effect, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning 15 high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation."

[18.12.2025 06:36] Response: ```python
["DIFFUSION", "OPEN_SOURCE"]
```
[18.12.2025 06:36] Response: ParsedChatCompletionMessage[Article](content='{"desc":"IC-Effect is a novel framework designed for video visual effects (VFX) editing that utilizes instruction-guided learning and DiT (Denoising Image Transformer) models. It effectively synthesizes complex effects like flames and particles while ensuring that the original video background remains unchanged and consistent over time. The framework employs a two-stage training approach, which includes general editing adaptation and specific learning for different effects, enhancing its ability to follow instructions and model effects accurately. Additionally, it introduces spatiotemporal sparse tokenization to improve computational efficiency while maintaining high fidelity in the generated effects.","title":"IC-Effect: Seamless Video VFX Editing with Instruction-Guided Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='IC-Effect is a novel framework designed for video visual effects (VFX) editing that utilizes instruction-guided learning and DiT (Denoising Image Transformer) models. It effectively synthesizes complex effects like flames and particles while ensuring that the original video background remains unchanged and consistent over time. The framework employs a two-stage training approach, which includes general editing adaptation and specific learning for different effects, enhancing its ability to follow instructions and model effects accurately. Additionally, it introduces spatiotemporal sparse tokenization to improve computational efficiency while maintaining high fidelity in the generated effects.', title='IC-Effect: Seamless Video VFX Editing with Instruction-Guided Learning'))
[18.12.2025 06:36] Response: ParsedChatCompletionMessage[Article](content='{"desc":"IC-EffectÊòØ‰∏ÄÁßçÂü∫‰∫éÊåá‰ª§ÂºïÂØºÁöÑDiTÊ°ÜÊû∂ÔºåÊó®Âú®ÂêàÊàêÂ§çÊùÇÁöÑËßÜÈ¢ëÁâπÊïàÔºåÂêåÊó∂‰øùÊåÅÁ©∫Èó¥ÂíåÊó∂Èó¥ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇËØ•ÊñπÊ≥ïÈááÁî®‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ÔºåÁªìÂêàÁ®ÄÁñèÊó∂Á©∫Ê†áËÆ∞ÂåñÊäÄÊúØÔºåÊúâÊïàÂú∞‰ªéÊúâÈôêÁöÑÈÖçÂØπÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÁâπÊïàÊ®°Âºè„ÄÇIC-EffectÂà©Áî®Ê∫êËßÜÈ¢ë‰Ωú‰∏∫Âπ≤ÂáÄÁöÑ‰∏ä‰∏ãÊñáÊù°‰ª∂ÔºåÁ°Æ‰øùËÉåÊôØ‰øùÊåÅ‰∏çÂèòÔºåÂπ∂ÂÆûÁé∞Ëá™ÁÑ∂ÁöÑÁâπÊïàÊ≥®ÂÖ•„ÄÇÈÄöËøáÂπøÊ≥õÁöÑÂÆûÈ™åÔºåIC-EffectÂ±ïÁ§∫‰∫ÜÈ´òË¥®Èáè„ÄÅÂèØÊéß‰∏îÊó∂Èó¥‰∏ÄËá¥ÁöÑËßÜÈ¢ëÁâπÊïàÁºñËæëËÉΩÂäõÔºåÊé®Âä®‰∫ÜËßÜÈ¢ëÂàõ‰ΩúÁöÑÊñ∞ÂèØËÉΩÊÄß„ÄÇ","title":"IC-EffectÔºöÈ´òÊïàËßÜÈ¢ëÁâπÊïàÂêàÊàêÁöÑÊñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='IC-EffectÊòØ‰∏ÄÁßçÂü∫‰∫éÊåá‰ª§ÂºïÂØºÁöÑDiTÊ°ÜÊû∂ÔºåÊó®Âú®ÂêàÊàêÂ§çÊùÇÁöÑËßÜÈ¢ëÁâπÊïàÔºåÂêåÊó∂‰øùÊåÅÁ©∫Èó¥ÂíåÊó∂Èó¥ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇËØ•ÊñπÊ≥ïÈááÁî®‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ÔºåÁªìÂêàÁ®ÄÁñèÊó∂Á©∫Ê†áËÆ∞ÂåñÊäÄÊúØÔºåÊúâÊïàÂú∞‰ªéÊúâÈôêÁöÑÈÖçÂØπÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÁâπÊïàÊ®°Âºè„ÄÇIC-EffectÂà©Áî®Ê∫êËßÜÈ¢ë‰Ωú‰∏∫Âπ≤ÂáÄÁöÑ‰∏ä‰∏ãÊñáÊù°‰ª∂ÔºåÁ°Æ‰øùËÉåÊôØ‰øùÊåÅ‰∏çÂèòÔºåÂπ∂ÂÆûÁé∞Ëá™ÁÑ∂ÁöÑÁâπÊïàÊ≥®ÂÖ•„ÄÇÈÄöËøáÂπøÊ≥õÁöÑÂÆûÈ™åÔºåIC-EffectÂ±ïÁ§∫‰∫ÜÈ´òË¥®Èáè„ÄÅÂèØÊéß‰∏îÊó∂Èó¥‰∏ÄËá¥ÁöÑËßÜÈ¢ëÁâπÊïàÁºñËæëËÉΩÂäõÔºåÊé®Âä®‰∫ÜËßÜÈ¢ëÂàõ‰ΩúÁöÑÊñ∞ÂèØËÉΩÊÄß„ÄÇ', title='IC-EffectÔºöÈ´òÊïàËßÜÈ¢ëÁâπÊïàÂêàÊàêÁöÑÊñ∞ÊñπÊ≥ï'))
[18.12.2025 06:36] Using data from previous issue: {"categories": ["#benchmark", "#multimodal"], "emoji": "üîç", "ru": {"title": "–†–µ—Å–∏–Ω—Ç–µ–∑ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–∏–ø—Ñ–µ–π–∫–æ–≤ —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –Ω–∏–∑–∫–∏—Ö –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–∏–ø—Ñ–µ–π–∫–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º–µ—Ç–æ–¥–µ —Ä–µ—Å–∏–Ω—Ç–µ–∑–∞, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ
[18.12.2025 06:36] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#training", "#open_source", "#architecture", "#inference"], "emoji": "‚ö°", "ru": {"title": "–û—Ç –∞–≤—Ç–∞—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫ –¥–∏—Ñ—Ñ—É–∑–∏–∏: –±—ã—Å—Ç—Ä—ã–µ –∏ –º–æ—â–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ DiffusionVL ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[18.12.2025 06:36] Using data from previous issue: {"categories": ["#benchmark", "#audio", "#multimodal", "#video"], "emoji": "üé¨", "ru": {"title": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "VABench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—é—â–∏–π —Ç—Ä–∏ —Ç–∏–ø–∞ 
[18.12.2025 06:36] Querying the API.
[18.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel deep learning architecture, WAY, uses nested sequence structures and spatial grids for accurate long-term vessel destination estimation from AIS data, incorporating CASP blocks and Gradient Dropout for improved performance.  					AI-generated summary 				 The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.
[18.12.2025 06:36] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è WAY –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—É–Ω–∫—Ç–∞ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è —Å—É–¥–Ω–∞ –ø–æ –¥–∞–Ω–Ω—ã–º AIS –Ω–∞ —Å—Ä–æ–∫ –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–Ω–µ–π –¥–æ –Ω–µ–¥–µ–ª—å. –ú–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—É–¥–æ—Ö–æ–¥–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–µ—Ç–∫–∏ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Å–ø–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–º–µ—â–µ–Ω–∏–π. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≤–∫–ª—é—á–∞–µ—Ç —Å–ª–æ–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏ –±–ª–æ–∫–∏ Channel-Aggregative Sequential Processing (CASP), –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–º–µ–Ω—è—é—Ç –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ Gradient Dropout –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Å–º–µ—â—ë–Ω–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö —Å –æ–¥–∏–Ω–æ—á–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏.",
  "emoji": "üö¢",
  "title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–æ–≤ —Å—É–¥–æ–≤ —á–µ—Ä–µ–∑ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –≤–Ω–∏–º–∞–Ω–∏–µ"
}
```
[18.12.2025 06:36] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel deep learning architecture, WAY, uses nested sequence structures and spatial grids for accurate long-term vessel destination estimation from AIS data, incorporating CASP blocks and Gradient Dropout for improved performance.  					AI-generated summary 				 The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation."

[18.12.2025 06:37] Response: ```python
["ARCHITECTURE", "TRAINING"]
```

**Justification:**

1. **ARCHITECTURE**: The paper proposes a novel deep learning architecture called "WAY" with specific components including a trajectory representation layer, Channel-Aggregative Sequential Processing (CASP) blocks, and multi-headed attention mechanisms. This is a core contribution of the paper.

2. **TRAINING**: The paper introduces a task-specialized Gradient Dropout (GD) technique designed to improve training by "enabling many-to-many training on single labels" and "preventing biased feedback surges." This is a novel training methodology contribution.
[18.12.2025 06:37] Error. Failed to parse JSON from LLM. ["ARCHITECTURE", "TRAINING"]


**Justification:**

1. **ARCHITECTURE**: The paper proposes a novel deep learning architecture called "WAY" with specific components including a trajectory representation layer, Channel-Aggregative Sequential Processing (CASP) blocks, and multi-headed attention mechanisms. This is a core contribution of the paper.

2. **TRAINING**: The paper introduces a task-specialized Gradient Dropout (GD) technique designed to improve training by "enabling many-to-many training on single labels" and "preventing biased feedback surges." This is a novel training methodology contribution.
[18.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel deep learning architecture, WAY, uses nested sequence structures and spatial grids for accurate long-term vessel destination estimation from AIS data, incorporating CASP blocks and Gradient Dropout for improved performance.  					AI-generated summary 				 The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation."

[18.12.2025 06:37] Response: ```python
['OPTIMIZATION', 'LONG_CONTEXT']
```

**Justification:**

- **OPTIMIZATION**: The paper introduces novel optimization techniques including CASP (Channel-Aggregative Sequential Processing) blocks and a task-specialized Gradient Dropout (GD) technique designed to improve training performance and prevent biased feedback surges.

- **LONG_CONTEXT**: The paper explicitly addresses "long-term destination estimation" and processes "long port-to-port trajectories" spanning "days to weeks in advance," which directly relates to handling extended temporal/sequential context.
[18.12.2025 06:37] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "LONG_CONTEXT"]


**Justification:**

- **OPTIMIZATION**: The paper introduces novel optimization techniques including CASP (Channel-Aggregative Sequential Processing) blocks and a task-specialized Gradient Dropout (GD) technique designed to improve training performance and prevent biased feedback surges.

- **LONG_CONTEXT**: The paper explicitly addresses "long-term destination estimation" and processes "long port-to-port trajectories" spanning "days to weeks in advance," which directly relates to handling extended temporal/sequential context.
[18.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper presents a new deep learning model called WAY, which is designed to improve the accuracy of predicting vessel destinations using data from the Automatic Identification System (AIS). It introduces a unique structure that organizes long maritime trajectories into nested sequences, allowing for better handling of spatio-temporal data. The model incorporates advanced techniques like Channel-Aggregative Sequential Processing (CASP) blocks and a specialized Gradient Dropout method to enhance learning and reduce bias. Experiments show that WAY outperforms traditional methods, making it a promising tool for real-world maritime applications such as estimated time of arrival (ETA) predictions.","title":"WAY: Revolutionizing Vessel Destination Estimation with Deep Learning"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='The paper presents a new deep learning model called WAY, which is designed to improve the accuracy of predicting vessel destinations using data from the Automatic Identification System (AIS). It introduces a unique structure that organizes long maritime trajectories into nested sequences, allowing for better handling of spatio-temporal data. The model incorporates advanced techniques like Channel-Aggregative Sequential Processing (CASP) blocks and a specialized Gradient Dropout method to enhance learning and reduce bias. Experiments show that WAY outperforms traditional methods, making it a promising tool for real-world maritime applications such as estimated time of arrival (ETA) predictions.', title='WAY: Revolutionizing Vessel Destination Estimation with Deep Learning'))
[18.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ∑±Â∫¶Â≠¶‰π†Êû∂ÊûÑWAYÔºåÊó®Âú®‰ªéAISÊï∞ÊçÆ‰∏≠ÂáÜÁ°Æ‰º∞ËÆ°ËàπËà∂ÁöÑÈïøÊúüÁõÆÁöÑÂú∞„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ∞ÜÊ∏ØÂè£Èó¥ÁöÑËΩ®ËøπÈáçÊûÑ‰∏∫ÂµåÂ•óÂ∫èÂàóÁªìÊûÑÔºåÂπ∂ÁªìÂêàÁ©∫Èó¥ÁΩëÊ†ºÔºåËß£ÂÜ≥‰∫ÜÊó∂Á©∫ÂÅèÂ∑ÆÈóÆÈ¢ò„ÄÇWAYÊû∂ÊûÑÂåÖÊã¨ËΩ®ËøπË°®Á§∫Â±ÇÂíåÈÄöÈÅìËÅöÂêàÂ∫èÂàóÂ§ÑÁêÜÔºàCASPÔºâÊ®°ÂùóÔºåÂà©Áî®Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ËøõË°å‰ø°ÊÅØËÅöÂêà„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWAYÂú®Â§ÑÁêÜAISÊï∞ÊçÆÊó∂‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÂπ∂‰∏îÂºïÂÖ•ÁöÑÊ¢ØÂ∫¶‰∏¢ÂºÉÊäÄÊúØËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊ®°ÂûãÊÄßËÉΩ„ÄÇ","title":"WAYÔºöÁ≤æÂáÜÁöÑËàπËà∂ÁõÆÁöÑÂú∞‰º∞ËÆ°Êñ∞ÊñπÊ≥ï"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ∑±Â∫¶Â≠¶‰π†Êû∂ÊûÑWAYÔºåÊó®Âú®‰ªéAISÊï∞ÊçÆ‰∏≠ÂáÜÁ°Æ‰º∞ËÆ°ËàπËà∂ÁöÑÈïøÊúüÁõÆÁöÑÂú∞„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ∞ÜÊ∏ØÂè£Èó¥ÁöÑËΩ®ËøπÈáçÊûÑ‰∏∫ÂµåÂ•óÂ∫èÂàóÁªìÊûÑÔºåÂπ∂ÁªìÂêàÁ©∫Èó¥ÁΩëÊ†ºÔºåËß£ÂÜ≥‰∫ÜÊó∂Á©∫ÂÅèÂ∑ÆÈóÆÈ¢ò„ÄÇWAYÊû∂ÊûÑÂåÖÊã¨ËΩ®ËøπË°®Á§∫Â±ÇÂíåÈÄöÈÅìËÅöÂêàÂ∫èÂàóÂ§ÑÁêÜÔºàCASPÔºâÊ®°ÂùóÔºåÂà©Áî®Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ËøõË°å‰ø°ÊÅØËÅöÂêà„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWAYÂú®Â§ÑÁêÜAISÊï∞ÊçÆÊó∂‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºåÂπ∂‰∏îÂºïÂÖ•ÁöÑÊ¢ØÂ∫¶‰∏¢ÂºÉÊäÄÊúØËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊ®°ÂûãÊÄßËÉΩ„ÄÇ', title='WAYÔºöÁ≤æÂáÜÁöÑËàπËà∂ÁõÆÁöÑÂú∞‰º∞ËÆ°Êñ∞ÊñπÊ≥ï'))
[18.12.2025 06:37] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–°–ª–æ–∏—Å—Ç–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "Qwen-Image-Layered ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–ª–∞–≥–∞–µ—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —Å–ª–æ–µ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ RGBA, –ø–æ–¥–æ–±–Ω–æ —Å–ª–æ—è–º –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ
[18.12.2025 06:37] Querying the API.
[18.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

MMSI-Video-Bench is a comprehensive benchmark for video-based spatial intelligence in MLLMs, revealing significant gaps between human and AI performance and highlighting challenges in geometric reasoning, motion grounding, and cross-video correspondence.  					AI-generated summary 				 Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.
[18.12.2025 06:37] Response: ```json
{
  "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MMSI-Video-Bench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –≤–∏–¥–µ–æ. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç 1106 –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ 1278 –≤–∏–¥–µ–æ–∫–ª–∏–ø–æ–≤ –∏ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —á–µ—Ç—ã—Ä–µ —É—Ä–æ–≤–Ω—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ –∫—Ä–æ—Å—Å-–≤–∏–¥–µ–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –¥–∞–∂–µ –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –æ—Ç—Å—Ç–∞—é—Ç –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ –Ω–∞ 60%, –∞ –º–Ω–æ–≥–∏–µ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —É–≥–∞–¥—ã–≤–∞–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –≤—ã—è–≤–∏–ª –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –º–æ–¥–µ–ª–µ–π –≤ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏, –ø—Ä–∏–≤—è–∑–∫–µ –¥–≤–∏–∂–µ–Ω–∏—è, –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –º–µ–∂–¥—É –≤–∏–¥–µ–æ.",
  "emoji": "üé¨",
  "title": "–ò–∑–º–µ—Ä—è—è –ø—Ä–æ–ø–∞—Å—Ç—å –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –ò–ò –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤ –≤–∏–¥–µ–æ"
}
```
[18.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MMSI-Video-Bench is a comprehensive benchmark for video-based spatial intelligence in MLLMs, revealing significant gaps between human and AI performance and highlighting challenges in geometric reasoning, motion grounding, and cross-video correspondence.  					AI-generated summary 				 Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence."

[18.12.2025 06:37] Response: ```python
["BENCHMARK", "VIDEO", "MULTIMODAL", "3D"]
```
[18.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"MMSI-Video-Bench is a comprehensive benchmark for video-based spatial intelligence in MLLMs, revealing significant gaps between human and AI performance and highlighting challenges in geometric reasoning, motion grounding, and cross-video correspondence.  					AI-generated summary 				 Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence."

[18.12.2025 06:37] Response: ```python
['REASONING', 'SURVEY']
```
[18.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MMSI-Video-Bench is a new benchmark designed to evaluate video-based spatial intelligence in machine learning language models (MLLMs). It identifies significant performance gaps between humans and AI, particularly in areas like geometric reasoning and motion grounding. The benchmark includes a comprehensive set of 1,106 questions based on 1,278 video clips, organized into a four-level framework: Perception, Planning, Prediction, and Cross-Video Reasoning. The findings reveal that many MLLMs struggle with these tasks, highlighting the need for improved models that can better understand and reason about spatial information in videos.","title":"Bridging the Gap: Advancing AI\'s Spatial Intelligence in Videos"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MMSI-Video-Bench is a new benchmark designed to evaluate video-based spatial intelligence in machine learning language models (MLLMs). It identifies significant performance gaps between humans and AI, particularly in areas like geometric reasoning and motion grounding. The benchmark includes a comprehensive set of 1,106 questions based on 1,278 video clips, organized into a four-level framework: Perception, Planning, Prediction, and Cross-Video Reasoning. The findings reveal that many MLLMs struggle with these tasks, highlighting the need for improved models that can better understand and reason about spatial information in videos.', title="Bridging the Gap: Advancing AI's Spatial Intelligence in Videos"))
[18.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"MMSI-Video-BenchÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑËßÜÈ¢ëÂü∫Á°ÄÁ©∫Èó¥Êô∫ËÉΩÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞Êú∫Âô®Â≠¶‰π†ËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Á©∫Èó¥ÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇËØ•Âü∫ÂáÜÊè≠Á§∫‰∫Ü‰∫∫Á±ª‰∏é‰∫∫Â∑•Êô∫ËÉΩ‰πãÈó¥ÁöÑÊòæËëóÂ∑ÆË∑ùÔºåÁâπÂà´ÊòØÂú®Âá†‰ΩïÊé®ÁêÜ„ÄÅËøêÂä®ÂÆö‰ΩçÂíåË∑®ËßÜÈ¢ëÂØπÂ∫îÁ≠âÊñπÈù¢„ÄÇÈÄöËøá1,106‰∏™ÈóÆÈ¢òÂíå1,278‰∏™ËßÜÈ¢ëÁâáÊÆµÔºåMMSI-Video-BenchÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂõõÁ∫ßÊ°ÜÊû∂ÔºåÊ∂µÁõñÊÑüÁü•„ÄÅËßÑÂàí„ÄÅÈ¢ÑÊµãÂíåË∑®ËßÜÈ¢ëÊé®ÁêÜ„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÊòæÁ§∫ÔºåËÆ∏Â§öÊ®°ÂûãÁöÑË°®Áé∞Êé•ËøëÈöèÊú∫ÔºåËÄåÊúÄ‰Ω≥Êé®ÁêÜÊ®°ÂûãÁöÑË°®Áé∞ÊØî‰∫∫Á±ª‰ΩéËøë60%„ÄÇ","title":"ËßÜÈ¢ëÂü∫Á°ÄÁ©∫Èó¥Êô∫ËÉΩÁöÑÂÖ®Èù¢ËØÑ‰º∞"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='MMSI-Video-BenchÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑËßÜÈ¢ëÂü∫Á°ÄÁ©∫Èó¥Êô∫ËÉΩÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞Êú∫Âô®Â≠¶‰π†ËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Á©∫Èó¥ÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇËØ•Âü∫ÂáÜÊè≠Á§∫‰∫Ü‰∫∫Á±ª‰∏é‰∫∫Â∑•Êô∫ËÉΩ‰πãÈó¥ÁöÑÊòæËëóÂ∑ÆË∑ùÔºåÁâπÂà´ÊòØÂú®Âá†‰ΩïÊé®ÁêÜ„ÄÅËøêÂä®ÂÆö‰ΩçÂíåË∑®ËßÜÈ¢ëÂØπÂ∫îÁ≠âÊñπÈù¢„ÄÇÈÄöËøá1,106‰∏™ÈóÆÈ¢òÂíå1,278‰∏™ËßÜÈ¢ëÁâáÊÆµÔºåMMSI-Video-BenchÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂõõÁ∫ßÊ°ÜÊû∂ÔºåÊ∂µÁõñÊÑüÁü•„ÄÅËßÑÂàí„ÄÅÈ¢ÑÊµãÂíåË∑®ËßÜÈ¢ëÊé®ÁêÜ„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÊòæÁ§∫ÔºåËÆ∏Â§öÊ®°ÂûãÁöÑË°®Áé∞Êé•ËøëÈöèÊú∫ÔºåËÄåÊúÄ‰Ω≥Êé®ÁêÜÊ®°ÂûãÁöÑË°®Áé∞ÊØî‰∫∫Á±ª‰ΩéËøë60%„ÄÇ', title='ËßÜÈ¢ëÂü∫Á°ÄÁ©∫Èó¥Êô∫ËÉΩÁöÑÂÖ®Èù¢ËØÑ‰º∞'))
[18.12.2025 06:37] Using data from previous issue: {"categories": ["#architecture", "#video", "#training"], "emoji": "üé¨", "ru": {"title": "–ë–µ–∑ —É—á–∏—Ç–µ–ª—è –∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏: –ø–µ—Ä–µ—É—á–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏ —á–µ—Ä–µ–∑ —Å–∞–º–æ–ø–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Resampling Forcing ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ
[18.12.2025 06:37] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#cv"], "emoji": "üîç", "ru": {"title": "VLM –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: –≥—Ä–∞–Ω–∏—Ü—ã —Å–∂–∞—Ç–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Å–∂–∞—Ç—ã–π –≤ –ø–ª–æ—Ç–Ω—ã–µ –≤–∏–∑
[18.12.2025 06:37] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#multimodal", "#dataset", "#training"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —Å –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–≤–∞—Ç–Ω—ã–º –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ–æ–±—É—á–∞—é—â–µ–≥–æ—Å—è –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
[18.12.2025 06:37] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#long_context"], "emoji": "üîÑ", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SCOPE ‚Äî —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —ç–≤–æ–ª—é—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.
[18.12.2025 06:37] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#open_source", "#hallucinations"], "emoji": "üé®", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ä–µ—à–∞—Ç–µ–ª–∏ –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ Nano Banana Pro –∫ –∑–∞–¥–∞—á–∞–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤
[18.12.2025 06:37] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning", "#dataset", "#training", "#open_source", "#synthetic", "#long_context", "#rl", "#video"], "emoji": "üé¨", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –∫–∞–∫ —É —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SAGE –¥–ª—è –º–Ω–æ–≥–æ
[18.12.2025 06:37] Using data from previous issue: {"categories": ["#dataset", "#training", "#cv", "#architecture", "#robotics", "#3d"], "emoji": "üñºÔ∏è", "ru": {"title": "–û—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫ –∑–Ω–∞–Ω–∏—è–º: –º–æ—â—å –ø—Ä–æ—Å—Ç—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –≤ —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "Pixio ‚Äî —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å
[18.12.2025 06:37] Using data from previous issue: {"categories": ["#robotics", "#training", "#multimodal", "#diffusion"], "emoji": "ü§ñ", "ru": {"title": "–û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ TacThru-UMI, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞—Ç—á–∏–∫ TacThru —Å –ø–æ–ª–∏—Ç–∏–∫–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Transformer –∏ –¥–∏—Ñ—Ñ—É–∑–∏
[18.12.2025 06:37] Querying the API.
[18.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

A novel framework, Class-Aware Attribution Prior (CAP), enhances language model interpretability and robustness by guiding the model to capture fine-grained class distinctions and combining with existing attribution methods.  					AI-generated summary 				 Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.
[18.12.2025 06:37] Response: ```json
{
  "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Framework Class-Aware Attribution Prior (CAP), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –∑–∞—Ö–≤–∞—Ç —Ç–æ–Ω–∫–∏—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∞—Ç—Ä–∏–±—É—Ü–∏–∏ —á–∞—Å—Ç–æ —Å–æ—Å—Ä–µ–¥–æ—Ç–∞—á–∏–≤–∞—é—Ç—Å—è –Ω–∞ –æ–±—â–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö –∫–ª–∞—Å—Å–æ–≤, —á—Ç–æ –Ω–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. CAP Hybrid –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –ø—Ä–∏–æ—Ä—ã –∏–∑ CAP —Å –ø—Ä–∏–æ—Ä–∞–º–∏ –∏–∑ –¥—Ä—É–≥–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –∞—Ç—Ä–∏–±—É—Ü–∏–∏, —Å–æ–∑–¥–∞–≤–∞—è –±–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–æ–¥—Ö–æ–¥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å, —Ç–∞–∫ –∏ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤–æ –≤—Å–µ—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.",
  "emoji": "üîç",
  "title": "–ö–ª–∞—Å—Å-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–æ—Ä—ã –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤"
}
```
[18.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel framework, Class-Aware Attribution Prior (CAP), enhances language model interpretability and robustness by guiding the model to capture fine-grained class distinctions and combining with existing attribution methods.  					AI-generated summary 				 Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness."

[18.12.2025 06:37] Response: ```python
["SMALL_MODELS", "TRAINING"]
```
[18.12.2025 06:37] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"A novel framework, Class-Aware Attribution Prior (CAP), enhances language model interpretability and robustness by guiding the model to capture fine-grained class distinctions and combining with existing attribution methods.  					AI-generated summary 				 Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness."

[18.12.2025 06:37] Response: ```python
["INTERPRETABILITY", "SECURITY"]
```

**Justification:**

1. **INTERPRETABILITY**: The paper explicitly focuses on enhancing language model interpretability through attribution methods and explanation-guided learning. The core contribution (Class-Aware Attribution Prior) is designed to improve model interpretability by guiding models to capture fine-grained class distinctions and produce more salient attributions.

2. **SECURITY**: The paper demonstrates improvements in adversarial robustness through experiments in "adversarial scenarios," which directly relates to model security and adversarial robustness.
[18.12.2025 06:37] Error. Failed to parse JSON from LLM. ["INTERPRETABILITY", "SECURITY"]


**Justification:**

1. **INTERPRETABILITY**: The paper explicitly focuses on enhancing language model interpretability through attribution methods and explanation-guided learning. The core contribution (Class-Aware Attribution Prior) is designed to improve model interpretability by guiding models to capture fine-grained class distinctions and produce more salient attributions.

2. **SECURITY**: The paper demonstrates improvements in adversarial robustness through experiments in "adversarial scenarios," which directly relates to model security and adversarial robustness.
[18.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"The paper introduces a new framework called Class-Aware Attribution Prior (CAP) that improves how language models understand and explain their decisions. It addresses the challenge of existing attribution methods that often highlight common keywords, which can confuse similar classes. By focusing on fine-grained class distinctions, CAP helps models generate clearer and more useful explanations. Additionally, the CAP Hybrid approach combines these new priors with traditional methods to enhance the model\'s learning of important features, leading to better performance in various scenarios.","title":"Enhancing Language Model Interpretability with Class-Aware Attribution"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="The paper introduces a new framework called Class-Aware Attribution Prior (CAP) that improves how language models understand and explain their decisions. It addresses the challenge of existing attribution methods that often highlight common keywords, which can confuse similar classes. By focusing on fine-grained class distinctions, CAP helps models generate clearer and more useful explanations. Additionally, the CAP Hybrid approach combines these new priors with traditional methods to enhance the model's learning of important features, leading to better performance in various scenarios.", title='Enhancing Language Model Interpretability with Class-Aware Attribution'))
[18.12.2025 06:37] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞Ê°ÜÊû∂ÔºåÁß∞‰∏∫Á±ªÊÑüÁü•ÂΩíÂõ†ÂÖàÈ™åÔºàCAPÔºâÔºåÊó®Âú®ÊèêÈ´òËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇCAPÈÄöËøáÂºïÂØºÊ®°ÂûãÊçïÊçâÁªÜÁ≤íÂ∫¶ÁöÑÁ±ªÂà´Âå∫ÂàÜÔºåÂπ∂‰∏éÁé∞ÊúâÁöÑÂΩíÂõ†ÊñπÊ≥ïÁõ∏ÁªìÂêàÔºåÊù•ÊîπÂñÑÊ®°ÂûãÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰º†ÁªüÁöÑÂΩíÂõ†ÊñπÊ≥ïÂæÄÂæÄÂè™ÂÖ≥Ê≥®ËØ≠‰πâÁõ∏‰ººÁ±ªÂà´‰πãÈó¥ÁöÑÂÖ±ÂêåÂÖ≥ÈîÆËØçÔºåÂØºËá¥Ê®°ÂûãÈöæ‰ª•Âå∫ÂàÜËøô‰∫õÁ±ªÂà´„ÄÇÈÄöËøáÂºïÂÖ•CAP HybridÔºåÁªìÂêàCAPÂíåÁé∞ÊúâÂΩíÂõ†ÊäÄÊúØÁöÑÂÖàÈ™å‰ø°ÊÅØÔºåÊàë‰ª¨ÁöÑÁ†îÁ©∂ÊúâÊïàÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ","title":"Á±ªÊÑüÁü•ÂΩíÂõ†ÂÖàÈ™åÔºöÊèêÂçáÊ®°ÂûãÂèØËß£ÈáäÊÄß‰∏éÈ≤ÅÊ£íÊÄß"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞Ê°ÜÊû∂ÔºåÁß∞‰∏∫Á±ªÊÑüÁü•ÂΩíÂõ†ÂÖàÈ™åÔºàCAPÔºâÔºåÊó®Âú®ÊèêÈ´òËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇCAPÈÄöËøáÂºïÂØºÊ®°ÂûãÊçïÊçâÁªÜÁ≤íÂ∫¶ÁöÑÁ±ªÂà´Âå∫ÂàÜÔºåÂπ∂‰∏éÁé∞ÊúâÁöÑÂΩíÂõ†ÊñπÊ≥ïÁõ∏ÁªìÂêàÔºåÊù•ÊîπÂñÑÊ®°ÂûãÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰º†ÁªüÁöÑÂΩíÂõ†ÊñπÊ≥ïÂæÄÂæÄÂè™ÂÖ≥Ê≥®ËØ≠‰πâÁõ∏‰ººÁ±ªÂà´‰πãÈó¥ÁöÑÂÖ±ÂêåÂÖ≥ÈîÆËØçÔºåÂØºËá¥Ê®°ÂûãÈöæ‰ª•Âå∫ÂàÜËøô‰∫õÁ±ªÂà´„ÄÇÈÄöËøáÂºïÂÖ•CAP HybridÔºåÁªìÂêàCAPÂíåÁé∞ÊúâÂΩíÂõ†ÊäÄÊúØÁöÑÂÖàÈ™å‰ø°ÊÅØÔºåÊàë‰ª¨ÁöÑÁ†îÁ©∂ÊúâÊïàÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇ', title='Á±ªÊÑüÁü•ÂΩíÂõ†ÂÖàÈ™åÔºöÊèêÂçáÊ®°ÂûãÂèØËß£ÈáäÊÄß‰∏éÈ≤ÅÊ£íÊÄß'))
[18.12.2025 06:37] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#dataset"], "emoji": "üé≠", "ru": {"title": "–ü—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤–∞–∂–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏", "desc": "LikeBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–Ω–æ–≥–æ—Å–µ–∞–Ω—Å–æ–≤—É—é –æ—Ü–µ–Ω–æ—á–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM, —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å –Ω–∞ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
[18.12.2025 06:37] Renaming data file.
[18.12.2025 06:37] Renaming previous data. hf_papers.json to ./d/2025-12-18.json
[18.12.2025 06:37] Saving new data file.
[18.12.2025 06:37] Generating page.
[18.12.2025 06:37] Renaming previous page.
[18.12.2025 06:37] Renaming previous data. index.html to ./d/2025-12-18.html
[18.12.2025 06:37] Writing result.
[18.12.2025 06:37] Renaming log file.
[18.12.2025 06:37] Renaming previous data. log.txt to ./logs/2025-12-18_last_log.txt
