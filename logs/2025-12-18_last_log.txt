[18.12.2025 07:24] Read previous papers.
[18.12.2025 07:24] Generating top page (month).
[18.12.2025 07:24] Writing top page (month).
[18.12.2025 08:31] Read previous papers.
[18.12.2025 08:31] Get feed.
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15176
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14681
[18.12.2025 08:31] Extract page data from URL. URL: https://huggingface.co/papers/2512.14052
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15693
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14693
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15635
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15687
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10863
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15431
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15182
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15713
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15603
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09299
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13190
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15702
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15649
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15110
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13874
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15374
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12072
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15715
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09851
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14719
[18.12.2025 08:31] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13077
[18.12.2025 08:31] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[18.12.2025 08:31] No deleted papers detected.
[18.12.2025 08:31] Downloading and parsing papers (pdf, html). Total: 24.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.15176.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.15176.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.15176.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.14681.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.14681.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.14681.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.14052.
[18.12.2025 08:31] Downloading paper 2512.14052 from https://arxiv.org/pdf/2512.14052v1...
[18.12.2025 08:31] Extracting affiliations from text.
[18.12.2025 08:31] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 2 5 0 4 1 . 2 1 5 2 : r HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices HyperAI Team, Xiaomi Corporation "
[18.12.2025 08:31] Response: ```python
["Xiaomi Corporation"]
```
[18.12.2025 08:31] Deleting PDF ./assets/pdf/2512.14052.pdf.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.15693.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.15693.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.15693.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.14693.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.14693.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.14693.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.15635.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.15635.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.15635.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.15687.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.15687.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.15687.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.10863.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.10863.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.10863.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.15431.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.15431.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.15431.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.15182.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.15182.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.15182.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.15713.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.15713.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.15713.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.15603.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.15603.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.15603.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.09299.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.09299.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.09299.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.13190.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.13190.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.13190.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.15702.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.15702.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.15702.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.15649.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.15649.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.15649.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.15110.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.15110.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.15110.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.13874.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.13874.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.13874.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.15374.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.15374.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.15374.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.12072.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.12072.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.12072.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.15715.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.15715.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.15715.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.09851.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.09851.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.09851.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.14719.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.14719.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.14719.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Downloading and parsing paper https://huggingface.co/papers/2512.13077.
[18.12.2025 08:31] Extra JSON file exists (./assets/json/2512.13077.json), skip PDF parsing.
[18.12.2025 08:31] Paper image links file exists (./assets/img_data/2512.13077.json), skip HTML parsing.
[18.12.2025 08:31] Success.
[18.12.2025 08:31] Enriching papers with extra data.
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 0. DEER framework uses diffusion large language models for efficient speculative decoding, overcoming the limitations of autoregressive drafters with better speed and draft quality.  					AI-generated summary 				 Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning system...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 1. Jacobi Forcing is a progressive distillation method that enables efficient parallel decoding of transformer-based models while maintaining performance, significantly reducing inference latency.  					AI-generated summary 				 Multi-token generation has emerged as a promising paradigm for acceleratin...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 2. HyperVL, an efficient multimodal large language model for on-device inference, uses image tiling, Visual Resolution Compressor, and Dual Consistency Learning to reduce memory usage, latency, and power consumption while maintaining performance.  					AI-generated summary 				 Current multimodal large...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 3. Skyra, a specialized multimodal large language model, detects and explains visual artifacts in AI-generated videos using a novel dataset and two-stage training strategy, outperforming existing methods.  					AI-generated summary 				 The misuse of AI-driven video generation technologies has raised s...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 4. The Universal Reasoning Model enhances Universal Transformers with short convolution and truncated backpropagation to improve reasoning performance on ARC-AGI tasks.  					AI-generated summary 				 Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sud...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 5. IC-Effect, an instruction-guided DiT-based framework, synthesizes complex video VFX effects while preserving spatial and temporal consistency using a two-stage training strategy and spatiotemporal sparse tokenization.  					AI-generated summary 				 We propose IC-Effect, an instruction-guided, DiT-b...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 6. G2RL, a gradient-guided reinforcement learning framework, enhances exploration in large language models by leveraging the model's own update geometry, leading to improved performance on various reasoning benchmarks.  					AI-generated summary 				 Reinforcement learning has become essential for stre...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 7. MMSI-Video-Bench is a comprehensive benchmark for video-based spatial intelligence in MLLMs, revealing significant gaps between human and AI performance and highlighting challenges in geometric reasoning, motion grounding, and cross-video correspondence.  					AI-generated summary 				 Spatial under...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 8. A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.  					AI-generated summary 				 Recent advances in multimodal large language models unlock unprecedented opportunities for G...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 9. A resynthesis framework enhances deepfake detection by verifying authenticity with low false positive rates and robustness against efficient adversaries, supporting multiple modalities.  					AI-generated summary 				 Generative models can synthesize highly realistic content, so-called deepfakes, th...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 10. DiffusionVL, a family of diffusion vision language models derived from autoregressive models through fine-tuning, achieves performance improvements and faster inference speeds compared to existing models.  					AI-generated summary 				 In recent multimodal research, the diffusion paradigm has emerg...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 11. Qwen-Image-Layered decomposes images into semantically disentangled RGBA layers using a diffusion model, enabling independent editing of each layer and improving decomposition quality and consistency.  					AI-generated summary 				 Recent visual generative models often struggle with consistency dur...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 12. VABench is a benchmark framework for evaluating audio-video generation models, covering text-to-audio-video, image-to-audio-video, and stereo audio-video tasks with 15 evaluation dimensions.  					AI-generated summary 				 Recent advances in video generation have been remarkable, enabling models to ...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 13. A novel deep learning architecture, WAY, uses nested sequence structures and spatial grids for accurate long-term vessel destination estimation from AIS data, incorporating CASP blocks and Gradient Dropout for improved performance.  					AI-generated summary 				 The Automatic Identification System ...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 14. Resampling Forcing is introduced as a teacher-free framework to train autoregressive video diffusion models with improved temporal consistency using self-resampling and history routing.  					AI-generated summary 				 Autoregressive video diffusion models hold promise for world simulation but are vu...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 15. A benchmark evaluates the performance of vision-language models on understanding long-context information compressed into dense visual representations, revealing significant limitations in capturing long-term dependencies.  					AI-generated summary 				 The computational and memory overheads associ...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 16. Nano Banana Pro excels in subjective visual quality across low-level vision tasks without fine-tuning but struggles with traditional reference-based quantitative metrics due to generative model stochasticity.  					AI-generated summary 				 The rapid evolution of text-to-image generation models has ...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 17. The paper proposes SAGE, a multi-turn reasoning system for video that mimics human behavior, using synthetic data and reinforcement learning to improve performance on long videos.  					AI-generated summary 				 As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iterativ...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 18. SCOPE enhances LLM agents' context management through prompt evolution, improving task success rates in dynamic environments without human intervention.  					AI-generated summary 				 Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 19. Voyager is a method that uses determinantal point processes to iteratively generate diverse synthetic datasets for model evaluation and training.  					AI-generated summary 				 Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 20. Pixio, an enhanced masked autoencoder, demonstrates competitive performance across various downstream tasks using pixel-space self-supervised learning, outperforming latent-space approaches.  					AI-generated summary 				 At the most basic level, pixels are the source of the visual information thro...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 21. TacThru-UMI, a system combining a TacThru sensor with a Transformer-based Diffusion Policy, achieves superior performance in robotic manipulation tasks by integrating simultaneous multimodal perception.  					AI-generated summary 				 Robotic manipulation requires both rich multimodal perception and...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 22. A novel framework, Class-Aware Attribution Prior (CAP), enhances language model interpretability and robustness by guiding the model to capture fine-grained class distinctions and combining with existing attribution methods.  					AI-generated summary 				 Small language models (SLMs) are widely use...
[18.12.2025 08:31] ********************************************************************************
[18.12.2025 08:31] Abstract 23. LikeBench introduces a multi-session evaluation framework to measure the likability of LLMs by their ability to adapt to user preferences across multiple dimensions, demonstrating that strong memory performance does not necessarily equate to higher likability.  					AI-generated summary 				 A perso...
[18.12.2025 08:31] Read previous papers.
[18.12.2025 08:31] Generating reviews via LLM API.
[18.12.2025 08:31] Using data from previous issue: {"categories": ["#inference", "#training", "#diffusion", "#architecture", "#optimization", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–º–µ—Å—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏: —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ DEER ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥
[18.12.2025 08:31] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–≤–æ–π—Å—Ç–≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Jacobi Forcing ‚Äî –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ
[18.12.2025 08:31] Querying the API.
[18.12.2025 08:31] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

HyperVL, an efficient multimodal large language model for on-device inference, uses image tiling, Visual Resolution Compressor, and Dual Consistency Learning to reduce memory usage, latency, and power consumption while maintaining performance.  					AI-generated summary 				 Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.
[18.12.2025 08:31] Response: ```json
{
  "desc": "HyperVL ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –≤—ã–≤–æ–¥–∞ –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–±–∏–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –ø–ª–∏—Ç–∫–∏ –∏ –∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –î–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ Vision Transformer –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–æ–≤ –ø–æ–¥ –µ–¥–∏–Ω—ã–º —ç–Ω–∫–æ–¥–µ—Ä–æ–º —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–µ–∂–¥—É –Ω–∏–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ HyperVL –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –∑–∞–¥–µ—Ä–∂–∫–∏ –∏ —ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö.",
  "emoji": "üì±",
  "title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö"
}
```
[18.12.2025 08:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HyperVL, an efficient multimodal large language model for on-device inference, uses image tiling, Visual Resolution Compressor, and Dual Consistency Learning to reduce memory usage, latency, and power consumption while maintaining performance.  					AI-generated summary 				 Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference."

[18.12.2025 08:31] Response: ```python
['MULTIMODAL', 'INFERENCE', 'SMALL_MODELS', 'ARCHITECTURE', 'TRAINING']
```
[18.12.2025 08:31] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"HyperVL, an efficient multimodal large language model for on-device inference, uses image tiling, Visual Resolution Compressor, and Dual Consistency Learning to reduce memory usage, latency, and power consumption while maintaining performance.  					AI-generated summary 				 Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference."

[18.12.2025 08:31] Response: ```python
['OPTIMIZATION']
```
[18.12.2025 08:32] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HyperVL is a multimodal large language model designed for efficient on-device inference, addressing the high computational demands of traditional models. It employs an image-tiling approach to manage memory usage and introduces a Visual Resolution Compressor to optimize encoding resolutions, reducing unnecessary computations. Additionally, HyperVL utilizes Dual Consistency Learning to synchronize multi-scale Vision Transformer encoders, allowing for flexible processing of visual data. The model achieves top performance in benchmarks while significantly lowering latency and power consumption, making it suitable for mobile applications.","title":"HyperVL: Efficient Multimodal Inference for Mobile Devices"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HyperVL is a multimodal large language model designed for efficient on-device inference, addressing the high computational demands of traditional models. It employs an image-tiling approach to manage memory usage and introduces a Visual Resolution Compressor to optimize encoding resolutions, reducing unnecessary computations. Additionally, HyperVL utilizes Dual Consistency Learning to synchronize multi-scale Vision Transformer encoders, allowing for flexible processing of visual data. The model achieves top performance in benchmarks while significantly lowering latency and power consumption, making it suitable for mobile applications.', title='HyperVL: Efficient Multimodal Inference for Mobile Devices'))
[18.12.2025 08:32] Response: ParsedChatCompletionMessage[Article](content='{"desc":"HyperVLÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºå‰∏ì‰∏∫ËÆæÂ§áÁ´ØÊé®ÁêÜËÆæËÆ°„ÄÇÂÆÉÈÄöËøáÂõæÂÉèÂàáÁâá„ÄÅËßÜËßâÂàÜËæ®ÁéáÂéãÁº©Âô®ÂíåÂèå‰∏ÄËá¥ÊÄßÂ≠¶‰π†Á≠âÊäÄÊúØÔºåÈôç‰Ωé‰∫ÜÂÜÖÂ≠ò‰ΩøÁî®„ÄÅÂª∂ËøüÂíåÂäüËÄóÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãËß£ÂÜ≥‰∫Ü‰º†ÁªüËßÜËßâÂèòÊç¢Âô®Âú®Â§ÑÁêÜÈ´òÂàÜËæ®ÁéáËæìÂÖ•Êó∂ÁöÑÂª∂ËøüÂíåÂÜÖÂ≠òÊ∂àËÄóÈóÆÈ¢ò„ÄÇÂÆûÈ™åË°®ÊòéÔºåHyperVLÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÈÄÇÂêàÂú®ÁßªÂä®ËÆæÂ§á‰∏äËøõË°åÂ§öÊ®°ÊÄÅÊé®ÁêÜ„ÄÇ","title":"HyperVLÔºöÈ´òÊïàÁöÑËÆæÂ§áÁ´ØÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ®°Âûã"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='HyperVLÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºå‰∏ì‰∏∫ËÆæÂ§áÁ´ØÊé®ÁêÜËÆæËÆ°„ÄÇÂÆÉÈÄöËøáÂõæÂÉèÂàáÁâá„ÄÅËßÜËßâÂàÜËæ®ÁéáÂéãÁº©Âô®ÂíåÂèå‰∏ÄËá¥ÊÄßÂ≠¶‰π†Á≠âÊäÄÊúØÔºåÈôç‰Ωé‰∫ÜÂÜÖÂ≠ò‰ΩøÁî®„ÄÅÂª∂ËøüÂíåÂäüËÄóÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãËß£ÂÜ≥‰∫Ü‰º†ÁªüËßÜËßâÂèòÊç¢Âô®Âú®Â§ÑÁêÜÈ´òÂàÜËæ®ÁéáËæìÂÖ•Êó∂ÁöÑÂª∂ËøüÂíåÂÜÖÂ≠òÊ∂àËÄóÈóÆÈ¢ò„ÄÇÂÆûÈ™åË°®ÊòéÔºåHyperVLÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÈÄÇÂêàÂú®ÁßªÂä®ËÆæÂ§á‰∏äËøõË°åÂ§öÊ®°ÊÄÅÊé®ÁêÜ„ÄÇ', title='HyperVLÔºöÈ´òÊïàÁöÑËÆæÂ§áÁ´ØÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ®°Âûã'))
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#training", "#multimodal", "#dataset", "#benchmark", "#video"], "emoji": "üé•", "ru": {"title": "–û–±—ä—è—Å–Ω–∏–º–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "Skyra ‚Äî —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±–Ω–∞—Ä—É–∂–∏
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#agi", "#architecture", "#open_source"], "emoji": "üß†", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏ —Å–∂–∞—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–Ω–∞–ª–∏–∑—É Universal Transformers –¥–ª—è –∑–∞–¥–∞—á —Å–ª–æ–∂–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ ARC-AGI. –ê–≤—Ç–æ—Ä—ã –ø
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#architecture", "#video", "#open_source", "#dataset", "#training", "#diffusion"], "emoji": "‚ú®", "ru": {"title": "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ —É–ø—Ä–∞–≤–ª—è–µ–º—ã–π —Å–∏–Ω—Ç–µ–∑ –≤–∏–¥–µ–æ—ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏", "desc": "IC-Effect ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª–∏ (DiT) –¥–ª—è —Ä–µ
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#rl", "#small_models", "#reasoning", "#training", "#rlhf", "#optimization", "#math"], "emoji": "üß≠", "ru": {"title": "–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–≤–µ–¥–∫–∞: –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —Å–∞–º–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –ø—É—Ç—å –∫ –ª—É—á—à–µ–º—É –æ–±—É—á–µ–Ω–∏—é", "desc": "G2RL ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#survey", "#reasoning", "#video", "#benchmark", "#3d", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ò–∑–º–µ—Ä—è—è –ø—Ä–æ–ø–∞—Å—Ç—å –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –ò–ò –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤ –≤–∏–¥–µ–æ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MMSI-Video-Bench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#multimodal", "#dataset", "#training"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —Å –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–≤–∞—Ç–Ω—ã–º –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ–æ–±—É—á–∞—é—â–µ–≥–æ—Å—è –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#benchmark", "#multimodal"], "emoji": "üîç", "ru": {"title": "–†–µ—Å–∏–Ω—Ç–µ–∑ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–∏–ø—Ñ–µ–π–∫–æ–≤ —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –Ω–∏–∑–∫–∏—Ö –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–∏–ø—Ñ–µ–π–∫–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º–µ—Ç–æ–¥–µ —Ä–µ—Å–∏–Ω—Ç–µ–∑–∞, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#training", "#open_source", "#architecture", "#inference"], "emoji": "‚ö°", "ru": {"title": "–û—Ç –∞–≤—Ç–∞—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫ –¥–∏—Ñ—Ñ—É–∑–∏–∏: –±—ã—Å—Ç—Ä—ã–µ –∏ –º–æ—â–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ DiffusionVL ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[18.12.2025 08:32] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–°–ª–æ–∏—Å—Ç–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "Qwen-Image-Layered ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–ª–∞–≥–∞–µ—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —Å–ª–æ–µ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ RGBA, –ø–æ–¥–æ–±–Ω–æ —Å–ª–æ—è–º –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#benchmark", "#audio", "#multimodal", "#video"], "emoji": "üé¨", "ru": {"title": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "VABench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—é—â–∏–π —Ç—Ä–∏ —Ç–∏–ø–∞ 
[18.12.2025 08:32] Using data from previous issue: {"categories": [], "emoji": "üö¢", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–æ–≤ —Å—É–¥–æ–≤ —á–µ—Ä–µ–∑ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –≤–Ω–∏–º–∞–Ω–∏–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è WAY –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—É–Ω–∫—Ç–∞ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è —Å—É–¥–Ω–∞ –ø–æ –¥–∞–Ω–Ω—ã–º AIS –Ω–∞ —Å—Ä–æ–∫ –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–Ω–µ–π –¥–æ –Ω–µ–¥–µ–ª—å. –ú–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#architecture", "#video", "#training"], "emoji": "üé¨", "ru": {"title": "–ë–µ–∑ —É—á–∏—Ç–µ–ª—è –∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏: –ø–µ—Ä–µ—É—á–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏ —á–µ—Ä–µ–∑ —Å–∞–º–æ–ø–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Resampling Forcing ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#cv"], "emoji": "üîç", "ru": {"title": "VLM –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: –≥—Ä–∞–Ω–∏—Ü—ã —Å–∂–∞—Ç–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Å–∂–∞—Ç—ã–π –≤ –ø–ª–æ—Ç–Ω—ã–µ –≤–∏–∑
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#open_source", "#hallucinations"], "emoji": "üé®", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ä–µ—à–∞—Ç–µ–ª–∏ –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ Nano Banana Pro –∫ –∑–∞–¥–∞—á–∞–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning", "#dataset", "#training", "#open_source", "#synthetic", "#long_context", "#rl", "#video"], "emoji": "üé¨", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –∫–∞–∫ —É —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SAGE –¥–ª—è –º–Ω–æ–≥–æ
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#long_context"], "emoji": "üîÑ", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SCOPE ‚Äî —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —ç–≤–æ–ª—é—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.
[18.12.2025 08:32] Using data from previous issue: {"categories": [], "emoji": "üó∫Ô∏è", "ru": {"title": "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –¥–µ—Ç–µ—Ä–º–∏–Ω–∞–Ω—Ç–Ω—ã–µ —Ç–æ—á–µ—á–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã", "desc": "Voyager ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞–Ω—Ç–Ω—ã–µ —Ç–æ—á–µ—á–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ü–æ–¥—Ö–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –º–∞—Ç–µ–º–∞—Ç
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#dataset", "#training", "#cv", "#architecture", "#robotics", "#3d"], "emoji": "üñºÔ∏è", "ru": {"title": "–û—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫ –∑–Ω–∞–Ω–∏—è–º: –º–æ—â—å –ø—Ä–æ—Å—Ç—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –≤ —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "Pixio ‚Äî —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#robotics", "#training", "#multimodal", "#diffusion"], "emoji": "ü§ñ", "ru": {"title": "–û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ TacThru-UMI, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞—Ç—á–∏–∫ TacThru —Å –ø–æ–ª–∏—Ç–∏–∫–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Transformer –∏ –¥–∏—Ñ—Ñ—É–∑–∏
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#training", "#small_models"], "emoji": "üîç", "ru": {"title": "–ö–ª–∞—Å—Å-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–æ—Ä—ã –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Framework Class-Aware Attribution Prior (CAP), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –Ω
[18.12.2025 08:32] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#dataset"], "emoji": "üé≠", "ru": {"title": "–ü—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤–∞–∂–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏", "desc": "LikeBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–Ω–æ–≥–æ—Å–µ–∞–Ω—Å–æ–≤—É—é –æ—Ü–µ–Ω–æ—á–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM, —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å –Ω–∞ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
[18.12.2025 08:32] Renaming data file.
[18.12.2025 08:32] Renaming previous data. hf_papers.json to ./d/2025-12-18.json
[18.12.2025 08:32] Saving new data file.
[18.12.2025 08:32] Generating page.
[18.12.2025 08:32] Renaming previous page.
[18.12.2025 08:32] Renaming previous data. index.html to ./d/2025-12-18.html
[18.12.2025 08:32] Writing result.
[18.12.2025 08:32] Renaming log file.
[18.12.2025 08:32] Renaming previous data. log.txt to ./logs/2025-12-18_last_log.txt
