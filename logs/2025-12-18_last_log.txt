[18.12.2025 20:22] Read previous papers.
[18.12.2025 20:22] Generating top page (month).
[18.12.2025 20:22] Writing top page (month).
[18.12.2025 21:22] Read previous papers.
[18.12.2025 21:22] Get feed.
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15431
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15176
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14681
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14052
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14944
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14693
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15635
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15693
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15603
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15182
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15687
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13884
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10863
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13874
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15713
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12072
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15702
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09299
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15715
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15649
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15110
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13190
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15374
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14202
[18.12.2025 21:22] Extract page data from URL. URL: https://huggingface.co/papers/2512.15699
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14719
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09851
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15340
[18.12.2025 21:22] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13077
[18.12.2025 21:22] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[18.12.2025 21:22] No deleted papers detected.
[18.12.2025 21:22] Downloading and parsing papers (pdf, html). Total: 29.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15431.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.15431.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.15431.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15176.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.15176.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.15176.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.14681.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.14681.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.14681.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.14052.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.14052.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.14052.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.14944.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.14944.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.14944.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.14693.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.14693.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.14693.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15635.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.15635.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.15635.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15693.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.15693.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.15693.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15603.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.15603.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.15603.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15182.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.15182.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.15182.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15687.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.15687.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.15687.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.13884.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.13884.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.13884.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.10863.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.10863.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.10863.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.13874.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.13874.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.13874.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15713.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.15713.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.15713.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.12072.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.12072.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.12072.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15702.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.15702.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.15702.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.09299.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.09299.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.09299.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15715.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.15715.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.15715.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15649.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.15649.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.15649.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15110.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.15110.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.15110.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.13190.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.13190.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.13190.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15374.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.15374.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.15374.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.14202.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.14202.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.14202.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15699.
[18.12.2025 21:22] Downloading paper 2512.15699 from https://arxiv.org/pdf/2512.15699v1...
[18.12.2025 21:22] Extracting affiliations from text.
[18.12.2025 21:22] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"2025-12-18 FrontierCS: Evolving Challenges for Evolving Intelligence Contributors ( equal contribution ) Qiuyang Mang1,, Wenhao Chai2,, Zhifei Li1,, Huanzhi Mao1,, Shang Zhou3,, Alexander Du1,4,, Hanchen Li1,, Shu Liu1,, Edwin Chen5, Yichuan Wang1, Xieting Chu6, Zerui Cheng2, Yuan Xu4, Tian Xia1, Zirui Wang1, Tianneng Shi1, Jianzhu Yao2, Yilong Zhao1, Qizheng Zhang7, Charlie Ruan1, Zeyu Shen2, Kaiyuan Liu8, Runyuan He1, Dong Xing4, Zerui Li4, Zirong Zeng1, Yige Jiang9, Lufeng Cheng10, Ziyi Zhao11, Youran Sun1, Wesley Zheng1, Meiyuwang Zhang5, Ruyi Ji12, Xuechang Tu6, Zihan Zheng13, Zexing Chen3, Kangyang Zhou14, Zhaozi Wang13, Jingbang Chen5 Advisors ( equal advising ) Aleksandra Korolova2, Peter Henderson2, Pramod Viswanath2, Vijay Ganesh6, Saining Xie13, Zhuang Liu2, Dawn Song1, Sewon Min1, Ion Stoica1, Joseph E. Gonzalez1,, Jingbo Shang3,, Alvin Cheung1, Affiliations 1UC Berkeley 7Stanford University 10University of Toronto 2Princeton University 3UCSD 4X-camp Academy 5Independent 6Georgia Tech 8University of Washington 11UIUC 12University of Michigan 9Nanyang Technological University 13New York University 14MIT Abstract: We introduce FrontierCS, benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and ex"
[18.12.2025 21:22] Response: ```python
[
    "UC Berkeley",
    "Princeton University",
    "UCSD",
    "X-camp Academy",
    "Independent",
    "Georgia Tech",
    "Stanford University",
    "University of Washington",
    "Nanyang Technological University",
    "New York University",
    "UIUC",
    "University of Michigan",
    "MIT",
    "University of Toronto"
]
```
[18.12.2025 21:22] Deleting PDF ./assets/pdf/2512.15699.pdf.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.14719.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.14719.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.14719.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.09851.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.09851.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.09851.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.15340.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.15340.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.15340.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Downloading and parsing paper https://huggingface.co/papers/2512.13077.
[18.12.2025 21:22] Extra JSON file exists (./assets/json/2512.13077.json), skip PDF parsing.
[18.12.2025 21:22] Paper image links file exists (./assets/img_data/2512.13077.json), skip HTML parsing.
[18.12.2025 21:22] Success.
[18.12.2025 21:22] Enriching papers with extra data.
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 0. A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.  					AI-generated summary 				 Recent advances in multimodal large language models unlock unprecedented opportunities for G...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 1. DEER framework uses diffusion large language models for efficient speculative decoding, overcoming the limitations of autoregressive drafters with better speed and draft quality.  					AI-generated summary 				 Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning system...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 2. Jacobi Forcing is a progressive distillation method that enables efficient parallel decoding of transformer-based models while maintaining performance, significantly reducing inference latency.  					AI-generated summary 				 Multi-token generation has emerged as a promising paradigm for acceleratin...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 3. HyperVL, an efficient multimodal large language model for on-device inference, uses image tiling, Visual Resolution Compressor, and Dual Consistency Learning to reduce memory usage, latency, and power consumption while maintaining performance.  					AI-generated summary 				 Current multimodal large...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 4. Puzzle Curriculum GRPO enhances visual reasoning in Vision Language Models through self-supervised environments and a difficulty-aware curriculum, improving consistency and accuracy without external annotations.  					AI-generated summary 				 Recent reinforcement learning (RL) approaches like outco...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 5. The Universal Reasoning Model enhances Universal Transformers with short convolution and truncated backpropagation to improve reasoning performance on ARC-AGI tasks.  					AI-generated summary 				 Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sud...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 6. IC-Effect, an instruction-guided DiT-based framework, synthesizes complex video VFX effects while preserving spatial and temporal consistency using a two-stage training strategy and spatiotemporal sparse tokenization.  					AI-generated summary 				 We propose IC-Effect, an instruction-guided, DiT-b...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 7. Skyra, a specialized multimodal large language model, detects and explains visual artifacts in AI-generated videos using a novel dataset and two-stage training strategy, outperforming existing methods.  					AI-generated summary 				 The misuse of AI-driven video generation technologies has raised s...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 8. Qwen-Image-Layered decomposes images into semantically disentangled RGBA layers using a diffusion model, enabling independent editing of each layer and improving decomposition quality and consistency.  					AI-generated summary 				 Recent visual generative models often struggle with consistency dur...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 9. A resynthesis framework enhances deepfake detection by verifying authenticity with low false positive rates and robustness against efficient adversaries, supporting multiple modalities.  					AI-generated summary 				 Generative models can synthesize highly realistic content, so-called deepfakes, th...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 10. G2RL, a gradient-guided reinforcement learning framework, enhances exploration in large language models by leveraging the model's own update geometry, leading to improved performance on various reasoning benchmarks.  					AI-generated summary 				 Reinforcement learning has become essential for stre...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 11. Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a datase...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 12. MMSI-Video-Bench is a comprehensive benchmark for video-based spatial intelligence in MLLMs, revealing significant gaps between human and AI performance and highlighting challenges in geometric reasoning, motion grounding, and cross-video correspondence.  					AI-generated summary 				 Spatial under...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 13. The paper proposes SAGE, a multi-turn reasoning system for video that mimics human behavior, using synthetic data and reinforcement learning to improve performance on long videos.  					AI-generated summary 				 As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iterativ...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 14. DiffusionVL, a family of diffusion vision language models derived from autoregressive models through fine-tuning, achieves performance improvements and faster inference speeds compared to existing models.  					AI-generated summary 				 In recent multimodal research, the diffusion paradigm has emerg...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 15. Voyager is a method that uses determinantal point processes to iteratively generate diverse synthetic datasets for model evaluation and training.  					AI-generated summary 				 Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 16. Resampling Forcing is introduced as a teacher-free framework to train autoregressive video diffusion models with improved temporal consistency using self-resampling and history routing.  					AI-generated summary 				 Autoregressive video diffusion models hold promise for world simulation but are vu...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 17. VABench is a benchmark framework for evaluating audio-video generation models, covering text-to-audio-video, image-to-audio-video, and stereo audio-video tasks with 15 evaluation dimensions.  					AI-generated summary 				 Recent advances in video generation have been remarkable, enabling models to ...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 18. Pixio, an enhanced masked autoencoder, demonstrates competitive performance across various downstream tasks using pixel-space self-supervised learning, outperforming latent-space approaches.  					AI-generated summary 				 At the most basic level, pixels are the source of the visual information thro...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 19. A benchmark evaluates the performance of vision-language models on understanding long-context information compressed into dense visual representations, revealing significant limitations in capturing long-term dependencies.  					AI-generated summary 				 The computational and memory overheads associ...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 20. Nano Banana Pro excels in subjective visual quality across low-level vision tasks without fine-tuning but struggles with traditional reference-based quantitative metrics due to generative model stochasticity.  					AI-generated summary 				 The rapid evolution of text-to-image generation models has ...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 21. A novel deep learning architecture, WAY, uses nested sequence structures and spatial grids for accurate long-term vessel destination estimation from AIS data, incorporating CASP blocks and Gradient Dropout for improved performance.  					AI-generated summary 				 The Automatic Identification System ...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 22. SCOPE enhances LLM agents' context management through prompt evolution, improving task success rates in dynamic environments without human intervention.  					AI-generated summary 				 Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 23. Hyper++ is a hyperbolic deep RL agent that improves stability and performance by addressing gradient issues and norm constraints in hyperbolic feature spaces.  					AI-generated summary 				 The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying fea...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 24. FrontierCS is a benchmark for evaluating models on open-ended computer science problems with unknown optimal solutions, where tasks involve implementing executable programs.  					AI-generated summary 				 We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of comput...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 25. A novel framework, Class-Aware Attribution Prior (CAP), enhances language model interpretability and robustness by guiding the model to capture fine-grained class distinctions and combining with existing attribution methods.  					AI-generated summary 				 Small language models (SLMs) are widely use...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 26. TacThru-UMI, a system combining a TacThru sensor with a Transformer-based Diffusion Policy, achieves superior performance in robotic manipulation tasks by integrating simultaneous multimodal perception.  					AI-generated summary 				 Robotic manipulation requires both rich multimodal perception and...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 27. TIMAR, a causal framework for 3D conversational head generation, models dialogue as interleaved audio-visual contexts and predicts continuous 3D head dynamics, improving coherence and expressive variability.  					AI-generated summary 				 Human conversation involves continuous exchanges of speech a...
[18.12.2025 21:22] ********************************************************************************
[18.12.2025 21:22] Abstract 28. LikeBench introduces a multi-session evaluation framework to measure the likability of LLMs by their ability to adapt to user preferences across multiple dimensions, demonstrating that strong memory performance does not necessarily equate to higher likability.  					AI-generated summary 				 A perso...
[18.12.2025 21:22] Read previous papers.
[18.12.2025 21:22] Generating reviews via LLM API.
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#multimodal", "#dataset", "#training"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —Å –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–≤–∞—Ç–Ω—ã–º –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ–æ–±—É—á–∞—é—â–µ–≥–æ—Å—è –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#inference", "#training", "#diffusion", "#architecture", "#optimization", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–º–µ—Å—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏: —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ DEER ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥
[18.12.2025 21:22] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–≤–æ–π—Å—Ç–≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Jacobi Forcing ‚Äî –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#small_models", "#multimodal", "#architecture", "#inference", "#training", "#optimization"], "emoji": "üì±", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö", "desc": "HyperVL ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#cv", "#training", "#reasoning", "#rlhf", "#rl", "#interpretability"], "emoji": "üß©", "ru": {"title": "–°–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ —É—Å–∏–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Puzzle Curriculum GRPO (PC-GRPO
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#agi", "#architecture", "#open_source"], "emoji": "üß†", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏ —Å–∂–∞—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–Ω–∞–ª–∏–∑—É Universal Transformers –¥–ª—è –∑–∞–¥–∞—á —Å–ª–æ–∂–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ ARC-AGI. –ê–≤—Ç–æ—Ä—ã –ø
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#architecture", "#video", "#open_source", "#dataset", "#training", "#diffusion"], "emoji": "‚ú®", "ru": {"title": "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ —É–ø—Ä–∞–≤–ª—è–µ–º—ã–π —Å–∏–Ω—Ç–µ–∑ –≤–∏–¥–µ–æ—ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏", "desc": "IC-Effect ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª–∏ (DiT) –¥–ª—è —Ä–µ
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#training", "#multimodal", "#dataset", "#benchmark", "#video"], "emoji": "üé•", "ru": {"title": "–û–±—ä—è—Å–Ω–∏–º–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "Skyra ‚Äî —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±–Ω–∞—Ä—É–∂–∏
[18.12.2025 21:22] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–°–ª–æ–∏—Å—Ç–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "Qwen-Image-Layered ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–ª–∞–≥–∞–µ—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —Å–ª–æ–µ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ RGBA, –ø–æ–¥–æ–±–Ω–æ —Å–ª–æ—è–º –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#benchmark", "#multimodal"], "emoji": "üîç", "ru": {"title": "–†–µ—Å–∏–Ω—Ç–µ–∑ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–∏–ø—Ñ–µ–π–∫–æ–≤ —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –Ω–∏–∑–∫–∏—Ö –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–∏–ø—Ñ–µ–π–∫–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º–µ—Ç–æ–¥–µ —Ä–µ—Å–∏–Ω—Ç–µ–∑–∞, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#rl", "#small_models", "#reasoning", "#training", "#rlhf", "#optimization", "#math"], "emoji": "üß≠", "ru": {"title": "–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–≤–µ–¥–∫–∞: –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —Å–∞–º–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –ø—É—Ç—å –∫ –ª—É—á—à–µ–º—É –æ–±—É—á–µ–Ω–∏—é", "desc": "G2RL ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#synthetic", "#data", "#transfer_learning", "#multilingual", "#low_resource"], "emoji": "üåç", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ NER –Ω–∞ 91 —è–∑—ã–∫ —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–¥–∏–≥–º—É —É—á–∏—Ç–µ–ª—è-—É—á–µ–Ω–∏–∫–∞", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç FiNERweb ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∫–æ–Ω–≤
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#survey", "#reasoning", "#video", "#benchmark", "#3d", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ò–∑–º–µ—Ä—è—è –ø—Ä–æ–ø–∞—Å—Ç—å –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –ò–ò –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤ –≤–∏–¥–µ–æ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MMSI-Video-Bench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning", "#dataset", "#training", "#open_source", "#synthetic", "#long_context", "#rl", "#video"], "emoji": "üé¨", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –∫–∞–∫ —É —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SAGE –¥–ª—è –º–Ω–æ–≥–æ
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#training", "#open_source", "#architecture", "#inference"], "emoji": "‚ö°", "ru": {"title": "–û—Ç –∞–≤—Ç–∞—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫ –¥–∏—Ñ—Ñ—É–∑–∏–∏: –±—ã—Å—Ç—Ä—ã–µ –∏ –º–æ—â–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ DiffusionVL ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[18.12.2025 21:22] Using data from previous issue: {"categories": [], "emoji": "üó∫Ô∏è", "ru": {"title": "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –¥–µ—Ç–µ—Ä–º–∏–Ω–∞–Ω—Ç–Ω—ã–µ —Ç–æ—á–µ—á–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã", "desc": "Voyager ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞–Ω—Ç–Ω—ã–µ —Ç–æ—á–µ—á–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ü–æ–¥—Ö–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –º–∞—Ç–µ–º–∞—Ç
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#architecture", "#video", "#training"], "emoji": "üé¨", "ru": {"title": "–ë–µ–∑ —É—á–∏—Ç–µ–ª—è –∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏: –ø–µ—Ä–µ—É—á–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏ —á–µ—Ä–µ–∑ —Å–∞–º–æ–ø–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Resampling Forcing ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#benchmark", "#audio", "#multimodal", "#video"], "emoji": "üé¨", "ru": {"title": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "VABench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—é—â–∏–π —Ç—Ä–∏ —Ç–∏–ø–∞ 
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#dataset", "#training", "#cv", "#architecture", "#robotics", "#3d"], "emoji": "üñºÔ∏è", "ru": {"title": "–û—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫ –∑–Ω–∞–Ω–∏—è–º: –º–æ—â—å –ø—Ä–æ—Å—Ç—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –≤ —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "Pixio ‚Äî —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#cv"], "emoji": "üîç", "ru": {"title": "VLM –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: –≥—Ä–∞–Ω–∏—Ü—ã —Å–∂–∞—Ç–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Å–∂–∞—Ç—ã–π –≤ –ø–ª–æ—Ç–Ω—ã–µ –≤–∏–∑
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#open_source", "#hallucinations"], "emoji": "üé®", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ä–µ—à–∞—Ç–µ–ª–∏ –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ Nano Banana Pro –∫ –∑–∞–¥–∞—á–∞–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤
[18.12.2025 21:22] Using data from previous issue: {"categories": [], "emoji": "üö¢", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–æ–≤ —Å—É–¥–æ–≤ —á–µ—Ä–µ–∑ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –≤–Ω–∏–º–∞–Ω–∏–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è WAY –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—É–Ω–∫—Ç–∞ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è —Å—É–¥–Ω–∞ –ø–æ –¥–∞–Ω–Ω—ã–º AIS –Ω–∞ —Å—Ä–æ–∫ –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–Ω–µ–π –¥–æ –Ω–µ–¥–µ–ª—å. –ú–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑
[18.12.2025 21:22] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#long_context"], "emoji": "üîÑ", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SCOPE ‚Äî —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —ç–≤–æ–ª—é—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.
[18.12.2025 21:22] Using data from previous issue: {"categories": [], "emoji": "‚õ∞Ô∏è", "ru": {"title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω Hyper++, –∞–≥–µ–Ω—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —Ä–∞–±–æ—Ç–∞—é—â–∏–π –≤ –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º 
[18.12.2025 21:22] Querying the API.
[18.12.2025 21:22] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

FrontierCS is a benchmark for evaluating models on open-ended computer science problems with unknown optimal solutions, where tasks involve implementing executable programs.  					AI-generated summary 				 We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.
[18.12.2025 21:23] Response: ```json
{
  "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä FrontierCS —Å 156 –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏, –≥–¥–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–µ–Ω–æ. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –≤–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞, –≤–∫–ª—é—á–∞—è –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –∏ –∑–∞–¥–∞—á–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞. –ö–∞–∂–¥–∞—è –∑–∞–¥–∞—á–∞ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ—Ç—Å—è —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ—Ü–µ–Ω–∏–≤–∞—Ç–µ–ª–µ–º. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –¥–∞–∂–µ –ø–µ—Ä–µ–¥–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –æ—Ç—Å—Ç–∞—é—Ç –æ—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, –∞ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –±—é–¥–∂–µ—Ç–∞ –Ω–µ —Ä–µ—à–∞–µ—Ç —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, —Ç–∞–∫ –∫–∞–∫ –º–æ–¥–µ–ª–∏ —á–∞—Å—Ç–æ –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—Ç –∫–æ–¥ –≤–º–µ—Å—Ç–æ –ø–æ–∏—Å–∫–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.",
  "emoji": "üöÄ",
  "title": "–ì—Ä–∞–Ω–∏—Ü–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π: –∫–æ–≥–¥–∞ –ò–ò –≤—Å—Ç—Ä–µ—á–∞–µ—Ç –Ω–µ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏"
}
```
[18.12.2025 21:23] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FrontierCS is a benchmark for evaluating models on open-ended computer science problems with unknown optimal solutions, where tasks involve implementing executable programs.  					AI-generated summary 				 We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs."

[18.12.2025 21:23] Response: ```python
["BENCHMARK", "PLP", "DATASET"]
```
[18.12.2025 21:23] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"FrontierCS is a benchmark for evaluating models on open-ended computer science problems with unknown optimal solutions, where tasks involve implementing executable programs.  					AI-generated summary 				 We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs."

[18.12.2025 21:23] Response: ```python
['REASONING', 'BENCHMARK', 'SCIENCE']
```

Wait, let me reconsider. "BENCHMARK" is not in the provided topics list. Let me re-classify:

```python
['REASONING', 'SCIENCE']
```

**Justification:**
- **REASONING**: The paper explicitly discusses "frontier reasoning models" and evaluates models on open-ended computer science problems requiring logical reasoning and problem-solving capabilities.
- **SCIENCE**: The paper addresses scientific applications by evaluating language models on computer science research problems and algorithmic challenges, which falls under scientific applications of LMs.
[18.12.2025 21:23] Error. Failed to parse JSON from LLM. ["REASONING", "BENCHMARK", "SCIENCE"]


Wait, let me reconsider. "BENCHMARK" is not in the provided topics list. Let me re-classify:


["REASONING", "SCIENCE"]


**Justification:**
- **REASONING**: The paper explicitly discusses "frontier reasoning models" and evaluates models on open-ended computer science problems requiring logical reasoning and problem-solving capabilities.
- **SCIENCE**: The paper addresses scientific applications by evaluating language models on computer science research problems and algorithmic challenges, which falls under scientific applications of LMs.
[18.12.2025 21:23] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FrontierCS is a new benchmark designed to evaluate machine learning models on complex computer science problems where the best solution is not known. It consists of 156 diverse tasks that require models to create executable programs rather than just provide answers. The benchmark includes challenging algorithmic problems and research questions, all reviewed by experts in the field. Results show that current models struggle to match human performance, often focusing on producing functional code instead of innovative solutions.","title":"Pushing the Limits of AI in Open-Ended Computer Science Challenges"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FrontierCS is a new benchmark designed to evaluate machine learning models on complex computer science problems where the best solution is not known. It consists of 156 diverse tasks that require models to create executable programs rather than just provide answers. The benchmark includes challenging algorithmic problems and research questions, all reviewed by experts in the field. Results show that current models struggle to match human performance, often focusing on producing functional code instead of innovative solutions.', title='Pushing the Limits of AI in Open-Ended Computer Science Challenges'))
[18.12.2025 21:23] Response: ParsedChatCompletionMessage[Article](content='{"desc":"FrontierCSÊòØ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞Ê®°ÂûãÂú®ÂºÄÊîæÂºèËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÈóÆÈ¢ò‰∏äÁöÑÂü∫ÂáÜÔºåÊ∂âÂèäÂÆûÁé∞ÂèØÊâßË°åÁ®ãÂ∫èÁöÑ‰ªªÂä°„ÄÇËøô‰∫õÈóÆÈ¢òÁî±ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÂçöÂ£´ÂíåÈ°∂Á∫ßÁ´ûËµõÁºñÁ®ãÂèÇ‰∏éËÄÖËÆæËÆ°ÂíåÂÆ°Ê†∏ÔºåÊ∂µÁõñ‰∫Ü156‰∏™‰∏çÂêåÈ¢ÜÂüüÁöÑÂºÄÊîæÂºèÈóÆÈ¢ò„ÄÇ‰∏éÁé∞ÊúâÂü∫ÂáÜ‰∏çÂêåÔºåFrontierCSÂÖ≥Ê≥®ÁöÑÊòØÊúÄ‰ºòËß£Êú™Áü•ÁöÑÈóÆÈ¢òÔºåÊ®°ÂûãÈÄöËøáÂÆûÁé∞ÂèØÊâßË°åÁ®ãÂ∫èÊù•Ëß£ÂÜ≥Ëøô‰∫õ‰ªªÂä°ÔºåËÄå‰∏çÊòØÁõ¥Êé•ËæìÂá∫Á≠îÊ°à„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ΩÁÆ°Êé®ÁêÜÊ®°ÂûãÂú®ÁÆóÊ≥ïÂíåÁ†îÁ©∂ËΩ®ÈÅì‰∏ä‰ªçËøúËøúËêΩÂêé‰∫é‰∫∫Á±ª‰∏ìÂÆ∂Ôºå‰ΩÜÂÆÉ‰ª¨Âú®ÁîüÊàêÂèØË°å‰ª£Á†ÅÊó∂ÂæÄÂæÄËøáÂ∫¶‰ºòÂåñÔºåËÄåÊú™ËÉΩÂèëÁé∞È´òË¥®ÈáèÁöÑÁÆóÊ≥ïÂíåÁ≥ªÁªüËÆæËÆ°„ÄÇ","title":"FrontierCSÔºöËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÈóÆÈ¢òÁöÑÂºÄÊîæÂºèÂü∫ÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='FrontierCSÊòØ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞Ê®°ÂûãÂú®ÂºÄÊîæÂºèËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÈóÆÈ¢ò‰∏äÁöÑÂü∫ÂáÜÔºåÊ∂âÂèäÂÆûÁé∞ÂèØÊâßË°åÁ®ãÂ∫èÁöÑ‰ªªÂä°„ÄÇËøô‰∫õÈóÆÈ¢òÁî±ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÂçöÂ£´ÂíåÈ°∂Á∫ßÁ´ûËµõÁºñÁ®ãÂèÇ‰∏éËÄÖËÆæËÆ°ÂíåÂÆ°Ê†∏ÔºåÊ∂µÁõñ‰∫Ü156‰∏™‰∏çÂêåÈ¢ÜÂüüÁöÑÂºÄÊîæÂºèÈóÆÈ¢ò„ÄÇ‰∏éÁé∞ÊúâÂü∫ÂáÜ‰∏çÂêåÔºåFrontierCSÂÖ≥Ê≥®ÁöÑÊòØÊúÄ‰ºòËß£Êú™Áü•ÁöÑÈóÆÈ¢òÔºåÊ®°ÂûãÈÄöËøáÂÆûÁé∞ÂèØÊâßË°åÁ®ãÂ∫èÊù•Ëß£ÂÜ≥Ëøô‰∫õ‰ªªÂä°ÔºåËÄå‰∏çÊòØÁõ¥Êé•ËæìÂá∫Á≠îÊ°à„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ΩÁÆ°Êé®ÁêÜÊ®°ÂûãÂú®ÁÆóÊ≥ïÂíåÁ†îÁ©∂ËΩ®ÈÅì‰∏ä‰ªçËøúËøúËêΩÂêé‰∫é‰∫∫Á±ª‰∏ìÂÆ∂Ôºå‰ΩÜÂÆÉ‰ª¨Âú®ÁîüÊàêÂèØË°å‰ª£Á†ÅÊó∂ÂæÄÂæÄËøáÂ∫¶‰ºòÂåñÔºåËÄåÊú™ËÉΩÂèëÁé∞È´òË¥®ÈáèÁöÑÁÆóÊ≥ïÂíåÁ≥ªÁªüËÆæËÆ°„ÄÇ', title='FrontierCSÔºöËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÈóÆÈ¢òÁöÑÂºÄÊîæÂºèÂü∫ÂáÜ'))
[18.12.2025 21:23] Using data from previous issue: {"categories": ["#training", "#small_models"], "emoji": "üîç", "ru": {"title": "–ö–ª–∞—Å—Å-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–æ—Ä—ã –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Framework Class-Aware Attribution Prior (CAP), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –Ω
[18.12.2025 21:23] Using data from previous issue: {"categories": ["#robotics", "#training", "#multimodal", "#diffusion"], "emoji": "ü§ñ", "ru": {"title": "–û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ TacThru-UMI, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞—Ç—á–∏–∫ TacThru —Å –ø–æ–ª–∏—Ç–∏–∫–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Transformer –∏ –¥–∏—Ñ—Ñ—É–∑–∏
[18.12.2025 21:23] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#audio", "#3d", "#video", "#robotics", "#open_source"], "emoji": "üó£Ô∏è", "ru": {"title": "–ü—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π –≥–æ–ª–æ–≤—ã –≤ –¥–∏–∞–ª–æ–≥–æ–≤–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏", "desc": "TIMAR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è 
[18.12.2025 21:23] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#dataset"], "emoji": "üé≠", "ru": {"title": "–ü—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤–∞–∂–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏", "desc": "LikeBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–Ω–æ–≥–æ—Å–µ–∞–Ω—Å–æ–≤—É—é –æ—Ü–µ–Ω–æ—á–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM, —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å –Ω–∞ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
[18.12.2025 21:23] Renaming data file.
[18.12.2025 21:23] Renaming previous data. hf_papers.json to ./d/2025-12-18.json
[18.12.2025 21:23] Saving new data file.
[18.12.2025 21:23] Generating page.
[18.12.2025 21:23] Renaming previous page.
[18.12.2025 21:23] Renaming previous data. index.html to ./d/2025-12-18.html
[18.12.2025 21:23] Writing result.
[18.12.2025 21:23] Renaming log file.
[18.12.2025 21:23] Renaming previous data. log.txt to ./logs/2025-12-18_last_log.txt
