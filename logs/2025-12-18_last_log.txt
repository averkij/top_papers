[18.12.2025 04:39] Read previous papers.
[18.12.2025 04:39] Generating top page (month).
[18.12.2025 04:39] Writing top page (month).
[18.12.2025 05:24] Read previous papers.
[18.12.2025 05:24] Get feed.
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15176
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14681
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15693
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15687
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14693
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09299
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15182
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15713
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15603
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15374
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15649
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15431
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15715
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15110
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13874
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09851
[18.12.2025 05:24] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15702
[18.12.2025 05:24] Extract page data from URL. URL: https://huggingface.co/papers/2512.13077
[18.12.2025 05:24] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[18.12.2025 05:24] No deleted papers detected.
[18.12.2025 05:24] Downloading and parsing papers (pdf, html). Total: 18.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.15176.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.15176.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.15176.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.14681.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.14681.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.14681.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.15693.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.15693.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.15693.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.15687.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.15687.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.15687.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.14693.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.14693.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.14693.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.09299.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.09299.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.09299.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.15182.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.15182.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.15182.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.15713.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.15713.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.15713.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.15603.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.15603.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.15603.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.15374.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.15374.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.15374.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.15649.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.15649.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.15649.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.15431.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.15431.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.15431.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.15715.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.15715.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.15715.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.15110.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.15110.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.15110.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.13874.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.13874.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.13874.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.09851.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.09851.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.09851.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.15702.
[18.12.2025 05:24] Extra JSON file exists (./assets/json/2512.15702.json), skip PDF parsing.
[18.12.2025 05:24] Paper image links file exists (./assets/img_data/2512.15702.json), skip HTML parsing.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Downloading and parsing paper https://huggingface.co/papers/2512.13077.
[18.12.2025 05:24] Downloading paper 2512.13077 from https://arxiv.org/pdf/2512.13077v1...
[18.12.2025 05:24] Extracting affiliations from text.
[18.12.2025 05:24] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 5 1 ] . [ 1 7 7 0 3 1 . 2 1 5 2 : r LIKEBENCH: EVALUATING SUBJECTIVE LIKABILITY IN LLMS FOR PERSONALIZATION Md Awsafur Rahman UC Santa Barbara awsaf@ucsb.edu Adam Gabrys Amazon gabrysa@amazon.com Doug Kang Amazon dougkang@amazon.com Jingjing Sun Amazon jingjins@amazon.com Tian Tan Amazon tianta@amazon.com Ashwin Chandramouli Amazon ashwic@amazon.com "
[18.12.2025 05:24] Response: ```python
["UC Santa Barbara", "Amazon"]
```
[18.12.2025 05:24] Deleting PDF ./assets/pdf/2512.13077.pdf.
[18.12.2025 05:24] Success.
[18.12.2025 05:24] Enriching papers with extra data.
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 0. DEER framework uses diffusion large language models for efficient speculative decoding, overcoming the limitations of autoregressive drafters with better speed and draft quality.  					AI-generated summary 				 Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning system...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 1. Jacobi Forcing is a progressive distillation method that enables efficient parallel decoding of transformer-based models while maintaining performance, significantly reducing inference latency.  					AI-generated summary 				 Multi-token generation has emerged as a promising paradigm for acceleratin...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 2. Skyra, a specialized multimodal large language model, detects and explains visual artifacts in AI-generated videos using a novel dataset and two-stage training strategy, outperforming existing methods.  					AI-generated summary 				 The misuse of AI-driven video generation technologies has raised s...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 3. G2RL, a gradient-guided reinforcement learning framework, enhances exploration in large language models by leveraging the model's own update geometry, leading to improved performance on various reasoning benchmarks.  					AI-generated summary 				 Reinforcement learning has become essential for stre...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 4. The Universal Reasoning Model enhances Universal Transformers with short convolution and truncated backpropagation to improve reasoning performance on ARC-AGI tasks.  					AI-generated summary 				 Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sud...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 5. VABench is a benchmark framework for evaluating audio-video generation models, covering text-to-audio-video, image-to-audio-video, and stereo audio-video tasks with 15 evaluation dimensions.  					AI-generated summary 				 Recent advances in video generation have been remarkable, enabling models to ...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 6. A resynthesis framework enhances deepfake detection by verifying authenticity with low false positive rates and robustness against efficient adversaries, supporting multiple modalities.  					AI-generated summary 				 Generative models can synthesize highly realistic content, so-called deepfakes, th...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 7. DiffusionVL, a family of diffusion vision language models derived from autoregressive models through fine-tuning, achieves performance improvements and faster inference speeds compared to existing models.  					AI-generated summary 				 In recent multimodal research, the diffusion paradigm has emerg...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 8. Qwen-Image-Layered decomposes images into semantically disentangled RGBA layers using a diffusion model, enabling independent editing of each layer and improving decomposition quality and consistency.  					AI-generated summary 				 Recent visual generative models often struggle with consistency dur...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 9. SCOPE enhances LLM agents' context management through prompt evolution, improving task success rates in dynamic environments without human intervention.  					AI-generated summary 				 Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 10. A benchmark evaluates the performance of vision-language models on understanding long-context information compressed into dense visual representations, revealing significant limitations in capturing long-term dependencies.  					AI-generated summary 				 The computational and memory overheads associ...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 11. A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.  					AI-generated summary 				 Recent advances in multimodal large language models unlock unprecedented opportunities for G...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 12. Pixio, an enhanced masked autoencoder, demonstrates competitive performance across various downstream tasks using pixel-space self-supervised learning, outperforming latent-space approaches.  					AI-generated summary 				 At the most basic level, pixels are the source of the visual information thro...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 13. Nano Banana Pro excels in subjective visual quality across low-level vision tasks without fine-tuning but struggles with traditional reference-based quantitative metrics due to generative model stochasticity.  					AI-generated summary 				 The rapid evolution of text-to-image generation models has ...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 14. The paper proposes SAGE, a multi-turn reasoning system for video that mimics human behavior, using synthetic data and reinforcement learning to improve performance on long videos.  					AI-generated summary 				 As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iterativ...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 15. TacThru-UMI, a system combining a TacThru sensor with a Transformer-based Diffusion Policy, achieves superior performance in robotic manipulation tasks by integrating simultaneous multimodal perception.  					AI-generated summary 				 Robotic manipulation requires both rich multimodal perception and...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 16. Resampling Forcing is introduced as a teacher-free framework to train autoregressive video diffusion models with improved temporal consistency using self-resampling and history routing.  					AI-generated summary 				 Autoregressive video diffusion models hold promise for world simulation but are vu...
[18.12.2025 05:24] ********************************************************************************
[18.12.2025 05:24] Abstract 17. LikeBench introduces a multi-session evaluation framework to measure the likability of LLMs by their ability to adapt to user preferences across multiple dimensions, demonstrating that strong memory performance does not necessarily equate to higher likability.  					AI-generated summary 				 A perso...
[18.12.2025 05:24] Read previous papers.
[18.12.2025 05:24] Generating reviews via LLM API.
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#inference", "#training", "#diffusion", "#architecture", "#optimization", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–º–µ—Å—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏: —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ DEER ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥
[18.12.2025 05:24] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–≤–æ–π—Å—Ç–≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Jacobi Forcing ‚Äî –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#training", "#multimodal", "#dataset", "#benchmark", "#video"], "emoji": "üé•", "ru": {"title": "–û–±—ä—è—Å–Ω–∏–º–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "Skyra ‚Äî —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±–Ω–∞—Ä—É–∂–∏
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#rl", "#small_models", "#reasoning", "#training", "#rlhf", "#optimization", "#math"], "emoji": "üß≠", "ru": {"title": "–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–≤–µ–¥–∫–∞: –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —Å–∞–º–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –ø—É—Ç—å –∫ –ª—É—á—à–µ–º—É –æ–±—É—á–µ–Ω–∏—é", "desc": "G2RL ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#agi", "#architecture", "#open_source"], "emoji": "üß†", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏ —Å–∂–∞—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–Ω–∞–ª–∏–∑—É Universal Transformers –¥–ª—è –∑–∞–¥–∞—á —Å–ª–æ–∂–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ ARC-AGI. –ê–≤—Ç–æ—Ä—ã –ø
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#benchmark", "#audio", "#multimodal", "#video"], "emoji": "üé¨", "ru": {"title": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "VABench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—é—â–∏–π —Ç—Ä–∏ —Ç–∏–ø–∞ 
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#benchmark", "#multimodal"], "emoji": "üîç", "ru": {"title": "–†–µ—Å–∏–Ω—Ç–µ–∑ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–∏–ø—Ñ–µ–π–∫–æ–≤ —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –Ω–∏–∑–∫–∏—Ö –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–∏–ø—Ñ–µ–π–∫–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º–µ—Ç–æ–¥–µ —Ä–µ—Å–∏–Ω—Ç–µ–∑–∞, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#training", "#open_source", "#architecture", "#inference"], "emoji": "‚ö°", "ru": {"title": "–û—Ç –∞–≤—Ç–∞—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫ –¥–∏—Ñ—Ñ—É–∑–∏–∏: –±—ã—Å—Ç—Ä—ã–µ –∏ –º–æ—â–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ DiffusionVL ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[18.12.2025 05:24] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–°–ª–æ–∏—Å—Ç–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "Qwen-Image-Layered ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–ª–∞–≥–∞–µ—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —Å–ª–æ–µ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ RGBA, –ø–æ–¥–æ–±–Ω–æ —Å–ª–æ—è–º –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#long_context"], "emoji": "üîÑ", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SCOPE ‚Äî —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —ç–≤–æ–ª—é—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#cv"], "emoji": "üîç", "ru": {"title": "VLM –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: –≥—Ä–∞–Ω–∏—Ü—ã —Å–∂–∞—Ç–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Å–∂–∞—Ç—ã–π –≤ –ø–ª–æ—Ç–Ω—ã–µ –≤–∏–∑
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#multimodal", "#dataset", "#training"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —Å –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–≤–∞—Ç–Ω—ã–º –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ–æ–±—É—á–∞—é—â–µ–≥–æ—Å—è –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#dataset", "#training", "#cv", "#architecture", "#robotics", "#3d"], "emoji": "üñºÔ∏è", "ru": {"title": "–û—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫ –∑–Ω–∞–Ω–∏—è–º: –º–æ—â—å –ø—Ä–æ—Å—Ç—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –≤ —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "Pixio ‚Äî —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#open_source", "#hallucinations"], "emoji": "üé®", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ä–µ—à–∞—Ç–µ–ª–∏ –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ Nano Banana Pro –∫ –∑–∞–¥–∞—á–∞–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning", "#dataset", "#training", "#open_source", "#synthetic", "#long_context", "#rl", "#video"], "emoji": "üé¨", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –∫–∞–∫ —É —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SAGE –¥–ª—è –º–Ω–æ–≥–æ
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#robotics", "#training", "#multimodal", "#diffusion"], "emoji": "ü§ñ", "ru": {"title": "–û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ TacThru-UMI, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞—Ç—á–∏–∫ TacThru —Å –ø–æ–ª–∏—Ç–∏–∫–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Transformer –∏ –¥–∏—Ñ—Ñ—É–∑–∏
[18.12.2025 05:24] Using data from previous issue: {"categories": ["#architecture", "#video", "#training"], "emoji": "üé¨", "ru": {"title": "–ë–µ–∑ —É—á–∏—Ç–µ–ª—è –∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏: –ø–µ—Ä–µ—É—á–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏ —á–µ—Ä–µ–∑ —Å–∞–º–æ–ø–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Resampling Forcing ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ
[18.12.2025 05:24] Querying the API.
[18.12.2025 05:24] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

LikeBench introduces a multi-session evaluation framework to measure the likability of LLMs by their ability to adapt to user preferences across multiple dimensions, demonstrating that strong memory performance does not necessarily equate to higher likability.  					AI-generated summary 				 A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.
[18.12.2025 05:24] Response: ```json
{
  "desc": "LikeBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–Ω–æ–≥–æ—Å–µ–∞–Ω—Å–æ–≤—É—é –æ—Ü–µ–Ω–æ—á–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM, —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å –Ω–∞ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤—É –∏–∑–º–µ—Ä–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç —Å–µ–º—å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫ –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏, –∞–¥–∞–ø—Ç–∞—Ü–∏—é –∑–Ω–∞–Ω–∏–π, –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –ø–æ–¥—Ö–æ–¥—è—â—É—é –¥–ª–∏–Ω—É –±–µ—Å–µ–¥—ã, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —é–º–æ—Ä—É –∏ –æ–±—Ä–∞—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–∏–π –±–∞–ª–ª –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –ë–µ–Ω—á–º–∞—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –ø–µ—Ä—Å–æ–Ω—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –∏ —Ä–∞–∑–ª–∏—á–∏–º–æ–≥–æ —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
  "emoji": "üé≠",
  "title": "–ü—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤–∞–∂–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏"
}
```
[18.12.2025 05:24] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LikeBench introduces a multi-session evaluation framework to measure the likability of LLMs by their ability to adapt to user preferences across multiple dimensions, demonstrating that strong memory performance does not necessarily equate to higher likability.  					AI-generated summary 				 A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions."

[18.12.2025 05:24] Response: ```python
["BENCHMARK", "DATASET"]
```
[18.12.2025 05:24] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"LikeBench introduces a multi-session evaluation framework to measure the likability of LLMs by their ability to adapt to user preferences across multiple dimensions, demonstrating that strong memory performance does not necessarily equate to higher likability.  					AI-generated summary 				 A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions."

[18.12.2025 05:24] Response: ```python
['ALIGNMENT']
```
[18.12.2025 05:24] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LikeBench is a new evaluation framework designed to assess how well large language models (LLMs) can adapt to user preferences over time, focusing on likability as a key metric. It introduces a multi-session approach where LLMs interact with a simulated user, learning and adjusting based on ongoing dialogue. Unlike previous benchmarks that primarily measure memory accuracy and application, LikeBench emphasizes likability across seven dimensions, such as emotional adaptation and humor fit. The findings reveal that high memory performance does not always correlate with higher likability, highlighting the importance of user experience in LLM interactions.","title":"Measuring Likability: Beyond Memory in LLM Adaptation"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LikeBench is a new evaluation framework designed to assess how well large language models (LLMs) can adapt to user preferences over time, focusing on likability as a key metric. It introduces a multi-session approach where LLMs interact with a simulated user, learning and adjusting based on ongoing dialogue. Unlike previous benchmarks that primarily measure memory accuracy and application, LikeBench emphasizes likability across seven dimensions, such as emotional adaptation and humor fit. The findings reveal that high memory performance does not always correlate with higher likability, highlighting the importance of user experience in LLM interactions.', title='Measuring Likability: Beyond Memory in LLM Adaptation'))
[18.12.2025 05:24] Response: ParsedChatCompletionMessage[Article](content='{"desc":"LikeBench ÊòØ‰∏Ä‰∏™Â§ö‰ºöËØùËØÑ‰º∞Ê°ÜÊû∂ÔºåÁî®‰∫éÊµãÈáèÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ÈÄÇÂ∫îÁî®Êà∑ÂÅèÂ•ΩÊñπÈù¢ÁöÑÂèØÂñúÁà±Á®ãÂ∫¶„ÄÇËØ•Ê°ÜÊû∂Âº∫Ë∞ÉÂèØÂñúÁà±ÊÄß‰∏ç‰ªÖ‰ªÖ‰æùËµñ‰∫éËÆ∞ÂøÜÊÄßËÉΩÔºåËøòÊ∂âÂèäÊÉÖÊÑüÈÄÇÂ∫î„ÄÅÊ≠£ÂºèÊÄßÂåπÈÖçÁ≠âÂ§ö‰∏™Áª¥Â∫¶„ÄÇÈÄöËøá‰∏éÊ®°ÊãüÁî®Êà∑ÁöÑÂØπËØùÔºåLLM ËÉΩÂ§üÂ≠¶‰π†Áî®Êà∑ÁöÑÂÅèÂ•ΩÂπ∂ËøõË°åÂä®ÊÄÅË∞ÉÊï¥Ôºå‰ªéËÄåÊèê‰æõÊõ¥Á¨¶ÂêàÁî®Êà∑ÊúüÊúõÁöÑÂìçÂ∫î„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂº∫Â§ßÁöÑËÆ∞ÂøÜËÉΩÂäõÂπ∂‰∏ç‰∏ÄÂÆöÊÑèÂë≥ÁùÄÊõ¥È´òÁöÑÂèØÂñúÁà±ÊÄßÔºåÊüê‰∫õÊ®°ÂûãÂú®ÂèØÂñúÁà±ÊÄßËØÑÂàÜ‰∏äË°®Áé∞Êõ¥‰Ω≥ÔºåÂç≥‰ΩøÂÆÉ‰ª¨ÁöÑËÆ∞ÂøÜÂáÜÁ°ÆÊÄßËæÉ‰Ωé„ÄÇ","title":"ÂèØÂñúÁà±ÊÄßÔºöË∂ÖË∂äËÆ∞ÂøÜÁöÑËØÑ‰º∞Ê†áÂáÜ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='LikeBench ÊòØ‰∏Ä‰∏™Â§ö‰ºöËØùËØÑ‰º∞Ê°ÜÊû∂ÔºåÁî®‰∫éÊµãÈáèÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ÈÄÇÂ∫îÁî®Êà∑ÂÅèÂ•ΩÊñπÈù¢ÁöÑÂèØÂñúÁà±Á®ãÂ∫¶„ÄÇËØ•Ê°ÜÊû∂Âº∫Ë∞ÉÂèØÂñúÁà±ÊÄß‰∏ç‰ªÖ‰ªÖ‰æùËµñ‰∫éËÆ∞ÂøÜÊÄßËÉΩÔºåËøòÊ∂âÂèäÊÉÖÊÑüÈÄÇÂ∫î„ÄÅÊ≠£ÂºèÊÄßÂåπÈÖçÁ≠âÂ§ö‰∏™Áª¥Â∫¶„ÄÇÈÄöËøá‰∏éÊ®°ÊãüÁî®Êà∑ÁöÑÂØπËØùÔºåLLM ËÉΩÂ§üÂ≠¶‰π†Áî®Êà∑ÁöÑÂÅèÂ•ΩÂπ∂ËøõË°åÂä®ÊÄÅË∞ÉÊï¥Ôºå‰ªéËÄåÊèê‰æõÊõ¥Á¨¶ÂêàÁî®Êà∑ÊúüÊúõÁöÑÂìçÂ∫î„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂº∫Â§ßÁöÑËÆ∞ÂøÜËÉΩÂäõÂπ∂‰∏ç‰∏ÄÂÆöÊÑèÂë≥ÁùÄÊõ¥È´òÁöÑÂèØÂñúÁà±ÊÄßÔºåÊüê‰∫õÊ®°ÂûãÂú®ÂèØÂñúÁà±ÊÄßËØÑÂàÜ‰∏äË°®Áé∞Êõ¥‰Ω≥ÔºåÂç≥‰ΩøÂÆÉ‰ª¨ÁöÑËÆ∞ÂøÜÂáÜÁ°ÆÊÄßËæÉ‰Ωé„ÄÇ', title='ÂèØÂñúÁà±ÊÄßÔºöË∂ÖË∂äËÆ∞ÂøÜÁöÑËØÑ‰º∞Ê†áÂáÜ'))
[18.12.2025 05:24] Renaming data file.
[18.12.2025 05:24] Renaming previous data. hf_papers.json to ./d/2025-12-18.json
[18.12.2025 05:24] Saving new data file.
[18.12.2025 05:24] Generating page.
[18.12.2025 05:24] Renaming previous page.
[18.12.2025 05:24] Renaming previous data. index.html to ./d/2025-12-18.html
[18.12.2025 05:24] Writing result.
[18.12.2025 05:24] Renaming log file.
[18.12.2025 05:24] Renaming previous data. log.txt to ./logs/2025-12-18_last_log.txt
