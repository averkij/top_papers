[18.12.2025 17:24] Read previous papers.
[18.12.2025 17:24] Generating top page (month).
[18.12.2025 17:24] Writing top page (month).
[18.12.2025 18:34] Read previous papers.
[18.12.2025 18:34] Get feed.
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15431
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15176
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14681
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14052
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15635
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15693
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15603
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14693
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15182
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15687
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13884
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10863
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13874
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15713
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14944
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15702
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09299
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15649
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15110
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13190
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15715
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15374
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14202
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12072
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14719
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09851
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15340
[18.12.2025 18:34] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13077
[18.12.2025 18:34] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[18.12.2025 18:34] No deleted papers detected.
[18.12.2025 18:34] Downloading and parsing papers (pdf, html). Total: 28.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.15431.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.15431.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.15431.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.15176.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.15176.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.15176.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.14681.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.14681.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.14681.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.14052.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.14052.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.14052.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.15635.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.15635.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.15635.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.15693.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.15693.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.15693.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.15603.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.15603.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.15603.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.14693.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.14693.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.14693.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.15182.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.15182.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.15182.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.15687.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.15687.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.15687.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.13884.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.13884.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.13884.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.10863.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.10863.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.10863.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.13874.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.13874.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.13874.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.15713.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.15713.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.15713.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.14944.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.14944.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.14944.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.15702.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.15702.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.15702.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.09299.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.09299.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.09299.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.15649.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.15649.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.15649.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.15110.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.15110.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.15110.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.13190.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.13190.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.13190.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.15715.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.15715.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.15715.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.15374.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.15374.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.15374.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.14202.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.14202.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.14202.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.12072.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.12072.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.12072.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.14719.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.14719.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.14719.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.09851.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.09851.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.09851.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.15340.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.15340.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.15340.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Downloading and parsing paper https://huggingface.co/papers/2512.13077.
[18.12.2025 18:34] Extra JSON file exists (./assets/json/2512.13077.json), skip PDF parsing.
[18.12.2025 18:34] Paper image links file exists (./assets/img_data/2512.13077.json), skip HTML parsing.
[18.12.2025 18:34] Success.
[18.12.2025 18:34] Enriching papers with extra data.
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 0. A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.  					AI-generated summary 				 Recent advances in multimodal large language models unlock unprecedented opportunities for G...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 1. DEER framework uses diffusion large language models for efficient speculative decoding, overcoming the limitations of autoregressive drafters with better speed and draft quality.  					AI-generated summary 				 Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning system...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 2. Jacobi Forcing is a progressive distillation method that enables efficient parallel decoding of transformer-based models while maintaining performance, significantly reducing inference latency.  					AI-generated summary 				 Multi-token generation has emerged as a promising paradigm for acceleratin...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 3. HyperVL, an efficient multimodal large language model for on-device inference, uses image tiling, Visual Resolution Compressor, and Dual Consistency Learning to reduce memory usage, latency, and power consumption while maintaining performance.  					AI-generated summary 				 Current multimodal large...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 4. IC-Effect, an instruction-guided DiT-based framework, synthesizes complex video VFX effects while preserving spatial and temporal consistency using a two-stage training strategy and spatiotemporal sparse tokenization.  					AI-generated summary 				 We propose IC-Effect, an instruction-guided, DiT-b...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 5. Skyra, a specialized multimodal large language model, detects and explains visual artifacts in AI-generated videos using a novel dataset and two-stage training strategy, outperforming existing methods.  					AI-generated summary 				 The misuse of AI-driven video generation technologies has raised s...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 6. Qwen-Image-Layered decomposes images into semantically disentangled RGBA layers using a diffusion model, enabling independent editing of each layer and improving decomposition quality and consistency.  					AI-generated summary 				 Recent visual generative models often struggle with consistency dur...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 7. The Universal Reasoning Model enhances Universal Transformers with short convolution and truncated backpropagation to improve reasoning performance on ARC-AGI tasks.  					AI-generated summary 				 Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sud...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 8. A resynthesis framework enhances deepfake detection by verifying authenticity with low false positive rates and robustness against efficient adversaries, supporting multiple modalities.  					AI-generated summary 				 Generative models can synthesize highly realistic content, so-called deepfakes, th...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 9. G2RL, a gradient-guided reinforcement learning framework, enhances exploration in large language models by leveraging the model's own update geometry, leading to improved performance on various reasoning benchmarks.  					AI-generated summary 				 Reinforcement learning has become essential for stre...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 10. Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a datase...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 11. MMSI-Video-Bench is a comprehensive benchmark for video-based spatial intelligence in MLLMs, revealing significant gaps between human and AI performance and highlighting challenges in geometric reasoning, motion grounding, and cross-video correspondence.  					AI-generated summary 				 Spatial under...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 12. The paper proposes SAGE, a multi-turn reasoning system for video that mimics human behavior, using synthetic data and reinforcement learning to improve performance on long videos.  					AI-generated summary 				 As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iterativ...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 13. DiffusionVL, a family of diffusion vision language models derived from autoregressive models through fine-tuning, achieves performance improvements and faster inference speeds compared to existing models.  					AI-generated summary 				 In recent multimodal research, the diffusion paradigm has emerg...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 14. Puzzle Curriculum GRPO enhances visual reasoning in Vision Language Models through self-supervised environments and a difficulty-aware curriculum, improving consistency and accuracy without external annotations.  					AI-generated summary 				 Recent reinforcement learning (RL) approaches like outco...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 15. Resampling Forcing is introduced as a teacher-free framework to train autoregressive video diffusion models with improved temporal consistency using self-resampling and history routing.  					AI-generated summary 				 Autoregressive video diffusion models hold promise for world simulation but are vu...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 16. VABench is a benchmark framework for evaluating audio-video generation models, covering text-to-audio-video, image-to-audio-video, and stereo audio-video tasks with 15 evaluation dimensions.  					AI-generated summary 				 Recent advances in video generation have been remarkable, enabling models to ...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 17. A benchmark evaluates the performance of vision-language models on understanding long-context information compressed into dense visual representations, revealing significant limitations in capturing long-term dependencies.  					AI-generated summary 				 The computational and memory overheads associ...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 18. Nano Banana Pro excels in subjective visual quality across low-level vision tasks without fine-tuning but struggles with traditional reference-based quantitative metrics due to generative model stochasticity.  					AI-generated summary 				 The rapid evolution of text-to-image generation models has ...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 19. A novel deep learning architecture, WAY, uses nested sequence structures and spatial grids for accurate long-term vessel destination estimation from AIS data, incorporating CASP blocks and Gradient Dropout for improved performance.  					AI-generated summary 				 The Automatic Identification System ...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 20. Pixio, an enhanced masked autoencoder, demonstrates competitive performance across various downstream tasks using pixel-space self-supervised learning, outperforming latent-space approaches.  					AI-generated summary 				 At the most basic level, pixels are the source of the visual information thro...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 21. SCOPE enhances LLM agents' context management through prompt evolution, improving task success rates in dynamic environments without human intervention.  					AI-generated summary 				 Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 22. Hyper++ is a hyperbolic deep RL agent that improves stability and performance by addressing gradient issues and norm constraints in hyperbolic feature spaces.  					AI-generated summary 				 The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying fea...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 23. Voyager is a method that uses determinantal point processes to iteratively generate diverse synthetic datasets for model evaluation and training.  					AI-generated summary 				 Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 24. A novel framework, Class-Aware Attribution Prior (CAP), enhances language model interpretability and robustness by guiding the model to capture fine-grained class distinctions and combining with existing attribution methods.  					AI-generated summary 				 Small language models (SLMs) are widely use...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 25. TacThru-UMI, a system combining a TacThru sensor with a Transformer-based Diffusion Policy, achieves superior performance in robotic manipulation tasks by integrating simultaneous multimodal perception.  					AI-generated summary 				 Robotic manipulation requires both rich multimodal perception and...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 26. TIMAR, a causal framework for 3D conversational head generation, models dialogue as interleaved audio-visual contexts and predicts continuous 3D head dynamics, improving coherence and expressive variability.  					AI-generated summary 				 Human conversation involves continuous exchanges of speech a...
[18.12.2025 18:34] ********************************************************************************
[18.12.2025 18:34] Abstract 27. LikeBench introduces a multi-session evaluation framework to measure the likability of LLMs by their ability to adapt to user preferences across multiple dimensions, demonstrating that strong memory performance does not necessarily equate to higher likability.  					AI-generated summary 				 A perso...
[18.12.2025 18:34] Read previous papers.
[18.12.2025 18:34] Generating reviews via LLM API.
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#multimodal", "#dataset", "#training"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —Å –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–≤–∞—Ç–Ω—ã–º –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ–æ–±—É—á–∞—é—â–µ–≥–æ—Å—è –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#inference", "#training", "#diffusion", "#architecture", "#optimization", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–º–µ—Å—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏: —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ DEER ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥
[18.12.2025 18:34] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–≤–æ–π—Å—Ç–≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Jacobi Forcing ‚Äî –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#small_models", "#multimodal", "#architecture", "#inference", "#training", "#optimization"], "emoji": "üì±", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö", "desc": "HyperVL ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#architecture", "#video", "#open_source", "#dataset", "#training", "#diffusion"], "emoji": "‚ú®", "ru": {"title": "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ —É–ø—Ä–∞–≤–ª—è–µ–º—ã–π —Å–∏–Ω—Ç–µ–∑ –≤–∏–¥–µ–æ—ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏", "desc": "IC-Effect ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª–∏ (DiT) –¥–ª—è —Ä–µ
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#training", "#multimodal", "#dataset", "#benchmark", "#video"], "emoji": "üé•", "ru": {"title": "–û–±—ä—è—Å–Ω–∏–º–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "Skyra ‚Äî —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±–Ω–∞—Ä—É–∂–∏
[18.12.2025 18:34] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–°–ª–æ–∏—Å—Ç–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "Qwen-Image-Layered ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–ª–∞–≥–∞–µ—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —Å–ª–æ–µ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ RGBA, –ø–æ–¥–æ–±–Ω–æ —Å–ª–æ—è–º –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#agi", "#architecture", "#open_source"], "emoji": "üß†", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏ —Å–∂–∞—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–Ω–∞–ª–∏–∑—É Universal Transformers –¥–ª—è –∑–∞–¥–∞—á —Å–ª–æ–∂–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ ARC-AGI. –ê–≤—Ç–æ—Ä—ã –ø
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#benchmark", "#multimodal"], "emoji": "üîç", "ru": {"title": "–†–µ—Å–∏–Ω—Ç–µ–∑ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–∏–ø—Ñ–µ–π–∫–æ–≤ —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –Ω–∏–∑–∫–∏—Ö –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–∏–ø—Ñ–µ–π–∫–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º–µ—Ç–æ–¥–µ —Ä–µ—Å–∏–Ω—Ç–µ–∑–∞, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#rl", "#small_models", "#reasoning", "#training", "#rlhf", "#optimization", "#math"], "emoji": "üß≠", "ru": {"title": "–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–≤–µ–¥–∫–∞: –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —Å–∞–º–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –ø—É—Ç—å –∫ –ª—É—á—à–µ–º—É –æ–±—É—á–µ–Ω–∏—é", "desc": "G2RL ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#dataset", "#open_source", "#synthetic", "#data", "#transfer_learning", "#multilingual", "#low_resource"], "emoji": "üåç", "ru": {"title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ NER –Ω–∞ 91 —è–∑—ã–∫ —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–¥–∏–≥–º—É —É—á–∏—Ç–µ–ª—è-—É—á–µ–Ω–∏–∫–∞", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç FiNERweb ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∫–æ–Ω–≤
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#survey", "#reasoning", "#video", "#benchmark", "#3d", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ò–∑–º–µ—Ä—è—è –ø—Ä–æ–ø–∞—Å—Ç—å –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –ò–ò –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤ –≤–∏–¥–µ–æ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MMSI-Video-Bench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning", "#dataset", "#training", "#open_source", "#synthetic", "#long_context", "#rl", "#video"], "emoji": "üé¨", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –∫–∞–∫ —É —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SAGE –¥–ª—è –º–Ω–æ–≥–æ
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#training", "#open_source", "#architecture", "#inference"], "emoji": "‚ö°", "ru": {"title": "–û—Ç –∞–≤—Ç–∞—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫ –¥–∏—Ñ—Ñ—É–∑–∏–∏: –±—ã—Å—Ç—Ä—ã–µ –∏ –º–æ—â–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ DiffusionVL ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#optimization", "#multimodal", "#cv", "#training", "#reasoning", "#rlhf", "#rl", "#interpretability"], "emoji": "üß©", "ru": {"title": "–°–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ —É—Å–∏–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Puzzle Curriculum GRPO (PC-GRPO
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#architecture", "#video", "#training"], "emoji": "üé¨", "ru": {"title": "–ë–µ–∑ —É—á–∏—Ç–µ–ª—è –∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏: –ø–µ—Ä–µ—É—á–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏ —á–µ—Ä–µ–∑ —Å–∞–º–æ–ø–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Resampling Forcing ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#benchmark", "#audio", "#multimodal", "#video"], "emoji": "üé¨", "ru": {"title": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "VABench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—é—â–∏–π —Ç—Ä–∏ —Ç–∏–ø–∞ 
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#cv"], "emoji": "üîç", "ru": {"title": "VLM –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: –≥—Ä–∞–Ω–∏—Ü—ã —Å–∂–∞—Ç–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Å–∂–∞—Ç—ã–π –≤ –ø–ª–æ—Ç–Ω—ã–µ –≤–∏–∑
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#open_source", "#hallucinations"], "emoji": "üé®", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ä–µ—à–∞—Ç–µ–ª–∏ –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ Nano Banana Pro –∫ –∑–∞–¥–∞—á–∞–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤
[18.12.2025 18:34] Using data from previous issue: {"categories": [], "emoji": "üö¢", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–æ–≤ —Å—É–¥–æ–≤ —á–µ—Ä–µ–∑ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –≤–Ω–∏–º–∞–Ω–∏–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è WAY –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—É–Ω–∫—Ç–∞ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è —Å—É–¥–Ω–∞ –ø–æ –¥–∞–Ω–Ω—ã–º AIS –Ω–∞ —Å—Ä–æ–∫ –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–Ω–µ–π –¥–æ –Ω–µ–¥–µ–ª—å. –ú–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#dataset", "#training", "#cv", "#architecture", "#robotics", "#3d"], "emoji": "üñºÔ∏è", "ru": {"title": "–û—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫ –∑–Ω–∞–Ω–∏—è–º: –º–æ—â—å –ø—Ä–æ—Å—Ç—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –≤ —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "Pixio ‚Äî —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#long_context"], "emoji": "üîÑ", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SCOPE ‚Äî —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —ç–≤–æ–ª—é—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.
[18.12.2025 18:34] Using data from previous issue: {"categories": [], "emoji": "‚õ∞Ô∏è", "ru": {"title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö", "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω Hyper++, –∞–≥–µ–Ω—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —Ä–∞–±–æ—Ç–∞—é—â–∏–π –≤ –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º 
[18.12.2025 18:34] Using data from previous issue: {"categories": [], "emoji": "üó∫Ô∏è", "ru": {"title": "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –¥–µ—Ç–µ—Ä–º–∏–Ω–∞–Ω—Ç–Ω—ã–µ —Ç–æ—á–µ—á–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã", "desc": "Voyager ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞–Ω—Ç–Ω—ã–µ —Ç–æ—á–µ—á–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ü–æ–¥—Ö–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –º–∞—Ç–µ–º–∞—Ç
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#training", "#small_models"], "emoji": "üîç", "ru": {"title": "–ö–ª–∞—Å—Å-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–æ—Ä—ã –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Framework Class-Aware Attribution Prior (CAP), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –Ω
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#robotics", "#training", "#multimodal", "#diffusion"], "emoji": "ü§ñ", "ru": {"title": "–û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ TacThru-UMI, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞—Ç—á–∏–∫ TacThru —Å –ø–æ–ª–∏—Ç–∏–∫–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Transformer –∏ –¥–∏—Ñ—Ñ—É–∑–∏
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#audio", "#3d", "#video", "#robotics", "#open_source"], "emoji": "üó£Ô∏è", "ru": {"title": "–ü—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π –≥–æ–ª–æ–≤—ã –≤ –¥–∏–∞–ª–æ–≥–æ–≤–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏", "desc": "TIMAR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è 
[18.12.2025 18:34] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#dataset"], "emoji": "üé≠", "ru": {"title": "–ü—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤–∞–∂–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏", "desc": "LikeBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–Ω–æ–≥–æ—Å–µ–∞–Ω—Å–æ–≤—É—é –æ—Ü–µ–Ω–æ—á–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM, —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å –Ω–∞ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
[18.12.2025 18:34] Renaming data file.
[18.12.2025 18:34] Renaming previous data. hf_papers.json to ./d/2025-12-18.json
[18.12.2025 18:34] Saving new data file.
[18.12.2025 18:34] Generating page.
[18.12.2025 18:34] Renaming previous page.
[18.12.2025 18:34] Renaming previous data. index.html to ./d/2025-12-18.html
[18.12.2025 18:34] Writing result.
[18.12.2025 18:34] Renaming log file.
[18.12.2025 18:34] Renaming previous data. log.txt to ./logs/2025-12-18_last_log.txt
