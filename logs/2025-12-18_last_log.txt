[18.12.2025 11:21] Read previous papers.
[18.12.2025 11:21] Generating top page (month).
[18.12.2025 11:21] Writing top page (month).
[18.12.2025 12:45] Read previous papers.
[18.12.2025 12:45] Get feed.
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15431
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15176
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14681
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14052
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15693
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15635
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14693
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15182
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15687
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.10863
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15713
[18.12.2025 12:45] Extract page data from URL. URL: https://huggingface.co/papers/2512.13884
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15603
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13874
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15702
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09299
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15649
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15110
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13190
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15715
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15374
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.12072
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.14719
[18.12.2025 12:45] Extract page data from URL. URL: https://huggingface.co/papers/2512.14202
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.09851
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.15340
[18.12.2025 12:45] Get page data from previous paper. URL: https://huggingface.co/papers/2512.13077
[18.12.2025 12:45] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[18.12.2025 12:45] No deleted papers detected.
[18.12.2025 12:45] Downloading and parsing papers (pdf, html). Total: 27.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.15431.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.15431.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.15431.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.15176.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.15176.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.15176.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.14681.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.14681.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.14681.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.14052.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.14052.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.14052.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.15693.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.15693.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.15693.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.15635.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.15635.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.15635.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.14693.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.14693.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.14693.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.15182.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.15182.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.15182.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.15687.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.15687.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.15687.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.10863.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.10863.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.10863.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.15713.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.15713.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.15713.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.13884.
[18.12.2025 12:45] Downloading paper 2512.13884 from https://arxiv.org/pdf/2512.13884v1...
[18.12.2025 12:45] Extracting affiliations from text.
[18.12.2025 12:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"FINERWEB: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition Patrick Haller Humboldt Universit√§t zu Berlin jonas.max.golde.1@hu-berlin.de 5 2 0 2 5 1 ] . [ 1 4 8 8 3 1 . 2 1 5 2 : r a "
[18.12.2025 12:45] Response: ```python
["Humboldt Universit√§t zu Berlin"]
```
[18.12.2025 12:45] Deleting PDF ./assets/pdf/2512.13884.pdf.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.15603.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.15603.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.15603.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.13874.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.13874.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.13874.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.15702.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.15702.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.15702.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.09299.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.09299.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.09299.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.15649.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.15649.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.15649.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.15110.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.15110.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.15110.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.13190.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.13190.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.13190.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.15715.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.15715.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.15715.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.15374.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.15374.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.15374.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.12072.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.12072.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.12072.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.14719.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.14719.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.14719.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.14202.
[18.12.2025 12:45] Downloading paper 2512.14202 from https://arxiv.org/pdf/2512.14202v1...
[18.12.2025 12:45] Extracting affiliations from text.
[18.12.2025 12:45] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"5 2 0 2 6 1 ] . [ 1 2 0 2 4 1 . 2 1 5 2 : r a Timo Klein,1,2, Thomas Lang,1,2, Andrii Shkabrii1,2, Alexander Sturm1,2, Kevin Sidak1,2 Lukas Miklautz3, Claudia Plant1,4, Yllka Velaj1,4, Sebastian Tschiatschek1,4 1 Faculty of Computer Science, University of Vienna, Vienna, Austria 2 Doctoral School Computer Science, University of Vienna, Vienna, Austria 3 Department of Machine Learning and Systems Biology, Max Planck Institute of Biochemistry, Martinsried, Germany 4 ds:UniVie, University of Vienna, Vienna, Austria Joint first authors firstname.lastname@univie.ac.at "
[18.12.2025 12:45] Response: ```python
[
    "Faculty of Computer Science, University of Vienna",
    "Doctoral School Computer Science, University of Vienna",
    "Department of Machine Learning and Systems Biology, Max Planck Institute of Biochemistry",
    "ds:UniVie, University of Vienna"
]
```
[18.12.2025 12:45] Deleting PDF ./assets/pdf/2512.14202.pdf.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.09851.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.09851.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.09851.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.15340.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.15340.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.15340.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Downloading and parsing paper https://huggingface.co/papers/2512.13077.
[18.12.2025 12:45] Extra JSON file exists (./assets/json/2512.13077.json), skip PDF parsing.
[18.12.2025 12:45] Paper image links file exists (./assets/img_data/2512.13077.json), skip HTML parsing.
[18.12.2025 12:45] Success.
[18.12.2025 12:45] Enriching papers with extra data.
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 0. A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.  					AI-generated summary 				 Recent advances in multimodal large language models unlock unprecedented opportunities for G...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 1. DEER framework uses diffusion large language models for efficient speculative decoding, overcoming the limitations of autoregressive drafters with better speed and draft quality.  					AI-generated summary 				 Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning system...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 2. Jacobi Forcing is a progressive distillation method that enables efficient parallel decoding of transformer-based models while maintaining performance, significantly reducing inference latency.  					AI-generated summary 				 Multi-token generation has emerged as a promising paradigm for acceleratin...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 3. HyperVL, an efficient multimodal large language model for on-device inference, uses image tiling, Visual Resolution Compressor, and Dual Consistency Learning to reduce memory usage, latency, and power consumption while maintaining performance.  					AI-generated summary 				 Current multimodal large...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 4. Skyra, a specialized multimodal large language model, detects and explains visual artifacts in AI-generated videos using a novel dataset and two-stage training strategy, outperforming existing methods.  					AI-generated summary 				 The misuse of AI-driven video generation technologies has raised s...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 5. IC-Effect, an instruction-guided DiT-based framework, synthesizes complex video VFX effects while preserving spatial and temporal consistency using a two-stage training strategy and spatiotemporal sparse tokenization.  					AI-generated summary 				 We propose IC-Effect, an instruction-guided, DiT-b...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 6. The Universal Reasoning Model enhances Universal Transformers with short convolution and truncated backpropagation to improve reasoning performance on ARC-AGI tasks.  					AI-generated summary 				 Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sud...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 7. A resynthesis framework enhances deepfake detection by verifying authenticity with low false positive rates and robustness against efficient adversaries, supporting multiple modalities.  					AI-generated summary 				 Generative models can synthesize highly realistic content, so-called deepfakes, th...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 8. G2RL, a gradient-guided reinforcement learning framework, enhances exploration in large language models by leveraging the model's own update geometry, leading to improved performance on various reasoning benchmarks.  					AI-generated summary 				 Reinforcement learning has become essential for stre...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 9. MMSI-Video-Bench is a comprehensive benchmark for video-based spatial intelligence in MLLMs, revealing significant gaps between human and AI performance and highlighting challenges in geometric reasoning, motion grounding, and cross-video correspondence.  					AI-generated summary 				 Spatial under...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 10. DiffusionVL, a family of diffusion vision language models derived from autoregressive models through fine-tuning, achieves performance improvements and faster inference speeds compared to existing models.  					AI-generated summary 				 In recent multimodal research, the diffusion paradigm has emerg...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 11. Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a datase...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 12. Qwen-Image-Layered decomposes images into semantically disentangled RGBA layers using a diffusion model, enabling independent editing of each layer and improving decomposition quality and consistency.  					AI-generated summary 				 Recent visual generative models often struggle with consistency dur...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 13. The paper proposes SAGE, a multi-turn reasoning system for video that mimics human behavior, using synthetic data and reinforcement learning to improve performance on long videos.  					AI-generated summary 				 As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iterativ...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 14. Resampling Forcing is introduced as a teacher-free framework to train autoregressive video diffusion models with improved temporal consistency using self-resampling and history routing.  					AI-generated summary 				 Autoregressive video diffusion models hold promise for world simulation but are vu...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 15. VABench is a benchmark framework for evaluating audio-video generation models, covering text-to-audio-video, image-to-audio-video, and stereo audio-video tasks with 15 evaluation dimensions.  					AI-generated summary 				 Recent advances in video generation have been remarkable, enabling models to ...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 16. A benchmark evaluates the performance of vision-language models on understanding long-context information compressed into dense visual representations, revealing significant limitations in capturing long-term dependencies.  					AI-generated summary 				 The computational and memory overheads associ...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 17. Nano Banana Pro excels in subjective visual quality across low-level vision tasks without fine-tuning but struggles with traditional reference-based quantitative metrics due to generative model stochasticity.  					AI-generated summary 				 The rapid evolution of text-to-image generation models has ...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 18. A novel deep learning architecture, WAY, uses nested sequence structures and spatial grids for accurate long-term vessel destination estimation from AIS data, incorporating CASP blocks and Gradient Dropout for improved performance.  					AI-generated summary 				 The Automatic Identification System ...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 19. Pixio, an enhanced masked autoencoder, demonstrates competitive performance across various downstream tasks using pixel-space self-supervised learning, outperforming latent-space approaches.  					AI-generated summary 				 At the most basic level, pixels are the source of the visual information thro...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 20. SCOPE enhances LLM agents' context management through prompt evolution, improving task success rates in dynamic environments without human intervention.  					AI-generated summary 				 Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 21. Voyager is a method that uses determinantal point processes to iteratively generate diverse synthetic datasets for model evaluation and training.  					AI-generated summary 				 Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 22. A novel framework, Class-Aware Attribution Prior (CAP), enhances language model interpretability and robustness by guiding the model to capture fine-grained class distinctions and combining with existing attribution methods.  					AI-generated summary 				 Small language models (SLMs) are widely use...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 23. Hyper++ is a hyperbolic deep RL agent that improves stability and performance by addressing gradient issues and norm constraints in hyperbolic feature spaces.  					AI-generated summary 				 The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying fea...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 24. TacThru-UMI, a system combining a TacThru sensor with a Transformer-based Diffusion Policy, achieves superior performance in robotic manipulation tasks by integrating simultaneous multimodal perception.  					AI-generated summary 				 Robotic manipulation requires both rich multimodal perception and...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 25. TIMAR, a causal framework for 3D conversational head generation, models dialogue as interleaved audio-visual contexts and predicts continuous 3D head dynamics, improving coherence and expressive variability.  					AI-generated summary 				 Human conversation involves continuous exchanges of speech a...
[18.12.2025 12:45] ********************************************************************************
[18.12.2025 12:45] Abstract 26. LikeBench introduces a multi-session evaluation framework to measure the likability of LLMs by their ability to adapt to user preferences across multiple dimensions, demonstrating that strong memory performance does not necessarily equate to higher likability.  					AI-generated summary 				 A perso...
[18.12.2025 12:45] Read previous papers.
[18.12.2025 12:45] Generating reviews via LLM API.
[18.12.2025 12:45] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#multimodal", "#dataset", "#training"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ —Å –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–≤–∞—Ç–Ω—ã–º –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ–æ–±—É—á–∞—é—â–µ–≥–æ—Å—è –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
[18.12.2025 12:45] Using data from previous issue: {"categories": ["#inference", "#training", "#diffusion", "#architecture", "#optimization", "#open_source"], "emoji": "‚ö°", "ru": {"title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–º–µ—Å—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏: —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ DEER ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥
[18.12.2025 12:45] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–≤–æ–π—Å—Ç–≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Jacobi Forcing ‚Äî –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ
[18.12.2025 12:45] Using data from previous issue: {"categories": ["#small_models", "#multimodal", "#architecture", "#inference", "#training", "#optimization"], "emoji": "üì±", "ru": {"title": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö", "desc": "HyperVL ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å
[18.12.2025 12:45] Using data from previous issue: {"categories": ["#training", "#multimodal", "#dataset", "#benchmark", "#video"], "emoji": "üé•", "ru": {"title": "–û–±—ä—è—Å–Ω–∏–º–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏", "desc": "Skyra ‚Äî —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±–Ω–∞—Ä—É–∂–∏
[18.12.2025 12:45] Using data from previous issue: {"categories": ["#architecture", "#video", "#open_source", "#dataset", "#training", "#diffusion"], "emoji": "‚ú®", "ru": {"title": "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ —É–ø—Ä–∞–≤–ª—è–µ–º—ã–π —Å–∏–Ω—Ç–µ–∑ –≤–∏–¥–µ–æ—ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏", "desc": "IC-Effect ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –º–æ–¥–µ–ª–∏ (DiT) –¥–ª—è —Ä–µ
[18.12.2025 12:45] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#agi", "#architecture", "#open_source"], "emoji": "üß†", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏ —Å–∂–∞—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "desc": "–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –∞–Ω–∞–ª–∏–∑—É Universal Transformers –¥–ª—è –∑–∞–¥–∞—á —Å–ª–æ–∂–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ ARC-AGI. –ê–≤—Ç–æ—Ä—ã –ø
[18.12.2025 12:45] Using data from previous issue: {"categories": ["#benchmark", "#multimodal"], "emoji": "üîç", "ru": {"title": "–†–µ—Å–∏–Ω—Ç–µ–∑ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–∏–ø—Ñ–µ–π–∫–æ–≤ —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –Ω–∏–∑–∫–∏—Ö –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–∏–ø—Ñ–µ–π–∫–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º–µ—Ç–æ–¥–µ —Ä–µ—Å–∏–Ω—Ç–µ–∑–∞, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥–ª–∏–Ω–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ
[18.12.2025 12:45] Using data from previous issue: {"categories": ["#rl", "#small_models", "#reasoning", "#training", "#rlhf", "#optimization", "#math"], "emoji": "üß≠", "ru": {"title": "–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–∑–≤–µ–¥–∫–∞: –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —Å–∞–º–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –ø—É—Ç—å –∫ –ª—É—á—à–µ–º—É –æ–±—É—á–µ–Ω–∏—é", "desc": "G2RL ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ä–∞
[18.12.2025 12:45] Using data from previous issue: {"categories": ["#survey", "#reasoning", "#video", "#benchmark", "#3d", "#multimodal"], "emoji": "üé¨", "ru": {"title": "–ò–∑–º–µ—Ä—è—è –ø—Ä–æ–ø–∞—Å—Ç—å –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –ò–ò –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤ –≤–∏–¥–µ–æ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω MMSI-Video-Bench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç
[18.12.2025 12:45] Using data from previous issue: {"categories": ["#multimodal", "#diffusion", "#training", "#open_source", "#architecture", "#inference"], "emoji": "‚ö°", "ru": {"title": "–û—Ç –∞–≤—Ç–∞—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫ –¥–∏—Ñ—Ñ—É–∑–∏–∏: –±—ã—Å—Ç—Ä—ã–µ –∏ –º–æ—â–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ DiffusionVL ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ
[18.12.2025 12:45] Querying the API.
[18.12.2025 12:45] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition.
[18.12.2025 12:46] Response: ```json
{
  "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç FiNERweb ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –Ω–∞ 91 —è–∑—ã–∫–µ, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ø–∞—Ä–∞–¥–∏–≥–º—É \"—É—á–∏—Ç–µ–ª—å-—É—á–µ–Ω–∏–∫\" —Å –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –û–Ω–∏ –æ–±—É—á–∞—é—Ç –º–æ–¥–µ–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–ª—è NER —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∞–Ω–Ω–æ—Ç–∏—Ä—É—é—Ç –∏—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º–∏ LLM, –ø–æ–ª—É—á–∏–≤ –æ–∫–æ–ª–æ 225k –æ—Ç—Ä—ã–≤–∫–æ–≤ —Å 235k —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ FiNERweb, –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Å—Ä–∞–≤–Ω–∏–º—ã—Ö –∏–ª–∏ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –Ω—É–ª–µ–≤–æ–º –ø–µ—Ä–µ–Ω–æ—Å–µ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ, —Ç–∞–∏–ª–∞–Ω–¥—Å–∫–æ–≥–æ –∏ —Å—É–∞—Ö–∏–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—è –≤ 19 —Ä–∞–∑ –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö —á–µ–º baseline. –ê–≤—Ç–æ—Ä—ã –≤—ã–ø—É—Å–∫–∞—é—Ç –¥–∞—Ç–∞—Å–µ—Ç —Å –∞–Ω–≥–ª–∏–π—Å–∫–∏–º–∏ –º–µ—Ç–∫–∞–º–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥–∞–º–∏ –Ω–∞ —Ü–µ–ª–µ–≤—ã–µ —è–∑—ã–∫–∏, —á—Ç–æ–±—ã —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π-—É—á–µ–Ω–∏–∫–æ–≤ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π.",
  "emoji": "üåç",
  "title": "–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ NER –Ω–∞ 91 —è–∑—ã–∫ —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–¥–∏–≥–º—É —É—á–∏—Ç–µ–ª—è-—É—á–µ–Ω–∏–∫–∞"
}
```
[18.12.2025 12:46] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition."

[18.12.2025 12:46] Response: ```python
["DATASET", "MULTILINGUAL", "DATA"]
```
[18.12.2025 12:46] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition."

[18.12.2025 12:46] Response: ```python
['SYNTHETIC', 'TRANSFER_LEARNING', 'LOW_RESOURCE', 'OPEN_SOURCE']
```
[18.12.2025 12:46] Response: ParsedChatCompletionMessage[Article](content='{"desc":"This paper presents FiNERweb, a novel dataset-creation pipeline designed for multilingual named entity recognition (NER) using large language models (LLMs). The approach employs a teacher-student paradigm across 91 languages and 25 scripts, generating approximately 225,000 passages annotated with 235,000 distinct entity labels. The regression model used in this pipeline achieves an impressive F1 score of over 84, demonstrating its effectiveness even with significantly less training data compared to existing baselines. Additionally, the quality of annotations is validated through LLM assessments, showing high scores for both faithfulness and completeness, making FiNERweb a valuable resource for enhancing multilingual NER tasks.","title":"FiNERweb: Scalable Multilingual NER with LLMs"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents FiNERweb, a novel dataset-creation pipeline designed for multilingual named entity recognition (NER) using large language models (LLMs). The approach employs a teacher-student paradigm across 91 languages and 25 scripts, generating approximately 225,000 passages annotated with 235,000 distinct entity labels. The regression model used in this pipeline achieves an impressive F1 score of over 84, demonstrating its effectiveness even with significantly less training data compared to existing baselines. Additionally, the quality of annotations is validated through LLM assessments, showing high scores for both faithfulness and completeness, making FiNERweb a valuable resource for enhancing multilingual NER tasks.', title='FiNERweb: Scalable Multilingual NER with LLMs'))
[18.12.2025 12:46] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Êú¨Êñá‰ªãÁªç‰∫ÜFiNERwebÔºå‰∏Ä‰∏™Áî®‰∫éÂ§öËØ≠Ë®ÄÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÔºàNERÔºâÊï∞ÊçÆÈõÜÂàõÂª∫ÁöÑÁÆ°ÈÅì„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®ÊïôÂ∏à-Â≠¶ÁîüËåÉÂºèÔºåÊâ©Â±ïÂà∞91ÁßçËØ≠Ë®ÄÂíå25Áßç‰π¶ÂÜôÁ≥ªÁªüÔºåÁîüÊàêÁ∫¶225,000‰∏™ÊÆµËêΩÂíå235,000‰∏™‰∏çÂêåÁöÑÂÆû‰ΩìÊ†áÁ≠æ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂõûÂΩíÊ®°ÂûãÂú®Èõ∂Ê†∑Êú¨ËøÅÁßªËÆæÁΩÆ‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºåÂ∞ΩÁÆ°ËÆ≠ÁªÉÊï∞ÊçÆÈáè‰ªÖ‰∏∫Âº∫Âü∫Á∫øÁöÑ19ÂÄçÔºå‰ΩÜÂú®Ëã±ËØ≠„ÄÅÊ≥∞ËØ≠ÂíåÊñØÁì¶Â∏åÈáåËØ≠‰∏äÂèñÂæó‰∫ÜÂèØÊØîÊàñÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËØÑ‰º∞‰∫ÜÊ≥®ÈáäË¥®ÈáèÔºåÂèëÁé∞ÂÖ∂Âú®ÂáÜÁ°ÆÊÄßÂíåÂÆåÊï¥ÊÄßÊñπÈù¢ÂùáË°®Áé∞Âá∫Ëâ≤ÔºåÊèê‰æõ‰∫ÜÂèØÈù†ÁöÑ‰ø°ÊÅØÊ≥®Èáä„ÄÇ","title":"FiNERwebÔºöÂ§öËØ≠Ë®ÄÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÁöÑÊñ∞ËµÑÊ∫ê"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨Êñá‰ªãÁªç‰∫ÜFiNERwebÔºå‰∏Ä‰∏™Áî®‰∫éÂ§öËØ≠Ë®ÄÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÔºàNERÔºâÊï∞ÊçÆÈõÜÂàõÂª∫ÁöÑÁÆ°ÈÅì„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®ÊïôÂ∏à-Â≠¶ÁîüËåÉÂºèÔºåÊâ©Â±ïÂà∞91ÁßçËØ≠Ë®ÄÂíå25Áßç‰π¶ÂÜôÁ≥ªÁªüÔºåÁîüÊàêÁ∫¶225,000‰∏™ÊÆµËêΩÂíå235,000‰∏™‰∏çÂêåÁöÑÂÆû‰ΩìÊ†áÁ≠æ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂõûÂΩíÊ®°ÂûãÂú®Èõ∂Ê†∑Êú¨ËøÅÁßªËÆæÁΩÆ‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºåÂ∞ΩÁÆ°ËÆ≠ÁªÉÊï∞ÊçÆÈáè‰ªÖ‰∏∫Âº∫Âü∫Á∫øÁöÑ19ÂÄçÔºå‰ΩÜÂú®Ëã±ËØ≠„ÄÅÊ≥∞ËØ≠ÂíåÊñØÁì¶Â∏åÈáåËØ≠‰∏äÂèñÂæó‰∫ÜÂèØÊØîÊàñÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËØÑ‰º∞‰∫ÜÊ≥®ÈáäË¥®ÈáèÔºåÂèëÁé∞ÂÖ∂Âú®ÂáÜÁ°ÆÊÄßÂíåÂÆåÊï¥ÊÄßÊñπÈù¢ÂùáË°®Áé∞Âá∫Ëâ≤ÔºåÊèê‰æõ‰∫ÜÂèØÈù†ÁöÑ‰ø°ÊÅØÊ≥®Èáä„ÄÇ', title='FiNERwebÔºöÂ§öËØ≠Ë®ÄÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÁöÑÊñ∞ËµÑÊ∫ê'))
[18.12.2025 12:46] Using data from previous issue: {"categories": [], "emoji": "üé®", "ru": {"title": "–°–ª–æ–∏—Å—Ç–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "Qwen-Image-Layered ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–ª–∞–≥–∞–µ—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö —Å–ª–æ–µ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ RGBA, –ø–æ–¥–æ–±–Ω–æ —Å–ª–æ—è–º –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ
[18.12.2025 12:46] Using data from previous issue: {"categories": ["#benchmark", "#agents", "#reasoning", "#dataset", "#training", "#open_source", "#synthetic", "#long_context", "#rl", "#video"], "emoji": "üé¨", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –∫–∞–∫ —É —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ SAGE –¥–ª—è –º–Ω–æ–≥–æ
[18.12.2025 12:46] Using data from previous issue: {"categories": ["#architecture", "#video", "#training"], "emoji": "üé¨", "ru": {"title": "–ë–µ–∑ —É—á–∏—Ç–µ–ª—è –∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏: –ø–µ—Ä–µ—É—á–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏ —á–µ—Ä–µ–∑ —Å–∞–º–æ–ø–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Resampling Forcing ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ
[18.12.2025 12:46] Using data from previous issue: {"categories": ["#benchmark", "#audio", "#multimodal", "#video"], "emoji": "üé¨", "ru": {"title": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", "desc": "VABench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—é—â–∏–π —Ç—Ä–∏ —Ç–∏–ø–∞ 
[18.12.2025 12:46] Using data from previous issue: {"categories": ["#benchmark", "#multimodal", "#cv"], "emoji": "üîç", "ru": {"title": "VLM –Ω–µ –ø–æ–Ω–∏–º–∞—é—Ç –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: –≥—Ä–∞–Ω–∏—Ü—ã —Å–∂–∞—Ç–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø–µ—Ä–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Å–∂–∞—Ç—ã–π –≤ –ø–ª–æ—Ç–Ω—ã–µ –≤–∏–∑
[18.12.2025 12:46] Using data from previous issue: {"categories": ["#benchmark", "#cv", "#open_source", "#hallucinations"], "emoji": "üé®", "ru": {"title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ä–µ—à–∞—Ç–µ–ª–∏ –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ Nano Banana Pro –∫ –∑–∞–¥–∞—á–∞–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤
[18.12.2025 12:46] Using data from previous issue: {"categories": [], "emoji": "üö¢", "ru": {"title": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–æ–≤ —Å—É–¥–æ–≤ —á–µ—Ä–µ–∑ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –≤–Ω–∏–º–∞–Ω–∏–µ", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è WAY –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—É–Ω–∫—Ç–∞ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è —Å—É–¥–Ω–∞ –ø–æ –¥–∞–Ω–Ω—ã–º AIS –Ω–∞ —Å—Ä–æ–∫ –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–Ω–µ–π –¥–æ –Ω–µ–¥–µ–ª—å. –ú–µ—Ç–æ–¥ –ø—Ä–µ–æ–±—Ä–∞–∑
[18.12.2025 12:46] Using data from previous issue: {"categories": ["#dataset", "#training", "#cv", "#architecture", "#robotics", "#3d"], "emoji": "üñºÔ∏è", "ru": {"title": "–û—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫ –∑–Ω–∞–Ω–∏—è–º: –º–æ—â—å –ø—Ä–æ—Å—Ç—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –≤ —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–º –æ–±—É—á–µ–Ω–∏–∏", "desc": "Pixio ‚Äî —ç—Ç–æ —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å
[18.12.2025 12:46] Using data from previous issue: {"categories": ["#optimization", "#open_source", "#long_context"], "emoji": "üîÑ", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º LLM-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SCOPE ‚Äî —Å–∏—Å—Ç–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —ç–≤–æ–ª—é—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.
[18.12.2025 12:46] Using data from previous issue: {"categories": [], "emoji": "üó∫Ô∏è", "ru": {"title": "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –¥–µ—Ç–µ—Ä–º–∏–Ω–∞–Ω—Ç–Ω—ã–µ —Ç–æ—á–µ—á–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã", "desc": "Voyager ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞–Ω—Ç–Ω—ã–µ —Ç–æ—á–µ—á–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –¥–ª—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –ü–æ–¥—Ö–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –º–∞—Ç–µ–º–∞—Ç
[18.12.2025 12:46] Using data from previous issue: {"categories": ["#training", "#small_models"], "emoji": "üîç", "ru": {"title": "–ö–ª–∞—Å—Å-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–æ—Ä—ã –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Framework Class-Aware Attribution Prior (CAP), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –Ω
[18.12.2025 12:46] Querying the API.
[18.12.2025 12:46] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Hyper++ is a hyperbolic deep RL agent that improves stability and performance by addressing gradient issues and norm constraints in hyperbolic feature spaces.  					AI-generated summary 				 The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincar√© Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .
[18.12.2025 12:46] Response: ```json
{
  "desc": "–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω Hyper++, –∞–≥–µ–Ω—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —Ä–∞–±–æ—Ç–∞—é—â–∏–π –≤ –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –º–æ–¥–µ–ª–∏—Ä—É—é—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤ —Å–ª–æ–∂–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –æ–±—É—á–µ–Ω–∏—è –≤ –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö: –±–æ–ª—å—à–∏–µ –Ω–æ—Ä–º—ã –≤–ª–æ–∂–µ–Ω–∏–π –¥–µ—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É—é—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –∏ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –Ω–∞—Ä—É—à–µ–Ω–∏—è–º –æ–±–ª–∞—Å—Ç–µ–π –¥–æ–≤–µ—Ä–∏—è –≤ PPO. –†–µ—à–µ–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏–∫–∞ —á–µ—Ä–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å, —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –Ω–æ—Ä–º–∞–º–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏—Ö —Å–ª–æ—ë–≤ —Å–µ—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ ProcGen –∏ Atari –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ –±–∞–∑–æ–≤—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ 30 –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤.",
  "emoji": "‚õ∞Ô∏è",
  "title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö"
}
```
[18.12.2025 12:46] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Hyper++ is a hyperbolic deep RL agent that improves stability and performance by addressing gradient issues and norm constraints in hyperbolic feature spaces.  					AI-generated summary 				 The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincar√© Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl ."

[18.12.2025 12:46] Response: ```python
["RL", "ARCHITECTURE", "TRAINING"]
```

**Justification:**

- **RL**: The paper is fundamentally about reinforcement learning agents and addresses optimization challenges in deep RL training, specifically with PPO and Double DQN algorithms.

- **ARCHITECTURE**: The paper proposes novel neural architecture components, specifically hyperbolic network layers and modifications to how hyperbolic feature spaces are utilized in RL agents (Poincar√© Ball and Hyperboloid models).

- **TRAINING**: The paper focuses on improving model training methods, addressing gradient issues, norm constraints, and stability during training of hyperbolic deep RL agents through three specific training improvements (categorical value loss, feature regularization, and optimization-friendly formulations).
[18.12.2025 12:46] Error. Failed to parse JSON from LLM. ["RL", "ARCHITECTURE", "TRAINING"]


**Justification:**

- **RL**: The paper is fundamentally about reinforcement learning agents and addresses optimization challenges in deep RL training, specifically with PPO and Double DQN algorithms.

- **ARCHITECTURE**: The paper proposes novel neural architecture components, specifically hyperbolic network layers and modifications to how hyperbolic feature spaces are utilized in RL agents (Poincar√© Ball and Hyperboloid models).

- **TRAINING**: The paper focuses on improving model training methods, addressing gradient issues, norm constraints, and stability during training of hyperbolic deep RL agents through three specific training improvements (categorical value loss, feature regularization, and optimization-friendly formulations).
[18.12.2025 12:46] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Hyper++ is a hyperbolic deep RL agent that improves stability and performance by addressing gradient issues and norm constraints in hyperbolic feature spaces.  					AI-generated summary 				 The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincar√© Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl ."

[18.12.2025 12:46] Response: ```python
["OPTIMIZATION", "OPEN_SOURCE"]
```

**Justification:**

- **OPTIMIZATION**: The paper directly addresses training optimization methods for reinforcement learning agents. It identifies gradient issues, norm constraints, and optimization challenges in hyperbolic feature spaces, and proposes solutions (Hyper++) to improve stability and performance through better optimization techniques.

- **OPEN_SOURCE**: The paper explicitly states "We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl", indicating the authors are contributing code to the public.
[18.12.2025 12:46] Error. Failed to parse JSON from LLM. ["OPTIMIZATION", "OPEN_SOURCE"]


**Justification:**

- **OPTIMIZATION**: The paper directly addresses training optimization methods for reinforcement learning agents. It identifies gradient issues, norm constraints, and optimization challenges in hyperbolic feature spaces, and proposes solutions (Hyper++) to improve stability and performance through better optimization techniques.

- **OPEN_SOURCE**: The paper explicitly states "We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl", indicating the authors are contributing code to the public.
[18.12.2025 12:46] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Hyper++ is a novel deep reinforcement learning (RL) agent that enhances stability and performance by tackling gradient issues in hyperbolic feature spaces. These spaces are effective for representing complex hierarchical structures in RL tasks, but they pose optimization challenges due to their nonstationarity. The paper identifies critical factors affecting the training of hyperbolic agents and introduces solutions such as stable critic training, feature regularization, and improved hyperbolic network formulations. Experimental results demonstrate that Hyper++ not only stabilizes learning but also significantly outperforms existing hyperbolic agents and reduces training time.","title":"Hyper++: Stability and Performance in Hyperbolic Deep RL"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Hyper++ is a novel deep reinforcement learning (RL) agent that enhances stability and performance by tackling gradient issues in hyperbolic feature spaces. These spaces are effective for representing complex hierarchical structures in RL tasks, but they pose optimization challenges due to their nonstationarity. The paper identifies critical factors affecting the training of hyperbolic agents and introduces solutions such as stable critic training, feature regularization, and improved hyperbolic network formulations. Experimental results demonstrate that Hyper++ not only stabilizes learning but also significantly outperforms existing hyperbolic agents and reduces training time.', title='Hyper++: Stability and Performance in Hyperbolic Deep RL'))
[18.12.2025 12:46] Response: ParsedChatCompletionMessage[Article](content='{"desc":"Hyper++ÊòØ‰∏ÄÁßçË∂ÖÊõ≤ÁéáÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†‰ª£ÁêÜÔºåÈÄöËøáËß£ÂÜ≥Ë∂ÖÊõ≤ÁéáÁâπÂæÅÁ©∫Èó¥‰∏≠ÁöÑÊ¢ØÂ∫¶ÈóÆÈ¢òÂíåËåÉÊï∞Á∫¶ÊùüÔºåÊèêÈ´ò‰∫ÜÁ®≥ÂÆöÊÄßÂíåÊÄßËÉΩ„ÄÇË∂ÖÊõ≤ÁéáÁâπÂæÅÁ©∫Èó¥ËÉΩÂ§üÊúâÊïàÊçïÊçâÂ§çÊùÇÂº∫ÂåñÂ≠¶‰π†ÁéØÂ¢É‰∏≠ÁöÑÂ±ÇÊ¨°ÂíåÂÖ≥Á≥ªÁªìÊûÑÔºå‰ΩÜÂú®‰ºòÂåñËøáÁ®ã‰∏≠Â∏∏Â∏∏Èù¢‰∏¥ÊåëÊàò„ÄÇÊú¨ÊñáÂàÜÊûê‰∫ÜË∂ÖÊõ≤ÁéáÂá†‰Ωï‰∏≠Poincar√©ÁêÉÂíåË∂ÖÊõ≤Èù¢Ê®°ÂûãÁöÑÊ†∏ÂøÉÊìç‰ΩúÊ¢ØÂ∫¶ÔºåÂèëÁé∞Â§ßËåÉÊï∞ÂµåÂÖ•‰ºöÂØºËá¥Âü∫‰∫éÊ¢ØÂ∫¶ÁöÑËÆ≠ÁªÉ‰∏çÁ®≥ÂÆö„ÄÇÂü∫‰∫éËøô‰∫õÂèëÁé∞ÔºåHyper++ÂºïÂÖ•‰∫ÜÁ®≥ÂÆöÁöÑËØÑËÆ∫ÂëòËÆ≠ÁªÉ„ÄÅÁâπÂæÅÊ≠£ÂàôÂåñÂíå‰ºòÂåñÂèãÂ•ΩÁöÑË∂ÖÊõ≤ÁéáÁΩëÁªúÂ±ÇÁöÑ‰∏âÂ§ßÁªÑ‰ª∂ÔºåÊòæËëóÊèêÂçá‰∫ÜÂ≠¶‰π†ÁöÑÁ®≥ÂÆöÊÄßÂíåÊïàÁéá„ÄÇ","title":"Hyper++ÔºöÊèêÂçáË∂ÖÊõ≤ÁéáÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÁöÑÁ®≥ÂÆöÊÄß‰∏éÊÄßËÉΩ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Hyper++ÊòØ‰∏ÄÁßçË∂ÖÊõ≤ÁéáÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†‰ª£ÁêÜÔºåÈÄöËøáËß£ÂÜ≥Ë∂ÖÊõ≤ÁéáÁâπÂæÅÁ©∫Èó¥‰∏≠ÁöÑÊ¢ØÂ∫¶ÈóÆÈ¢òÂíåËåÉÊï∞Á∫¶ÊùüÔºåÊèêÈ´ò‰∫ÜÁ®≥ÂÆöÊÄßÂíåÊÄßËÉΩ„ÄÇË∂ÖÊõ≤ÁéáÁâπÂæÅÁ©∫Èó¥ËÉΩÂ§üÊúâÊïàÊçïÊçâÂ§çÊùÇÂº∫ÂåñÂ≠¶‰π†ÁéØÂ¢É‰∏≠ÁöÑÂ±ÇÊ¨°ÂíåÂÖ≥Á≥ªÁªìÊûÑÔºå‰ΩÜÂú®‰ºòÂåñËøáÁ®ã‰∏≠Â∏∏Â∏∏Èù¢‰∏¥ÊåëÊàò„ÄÇÊú¨ÊñáÂàÜÊûê‰∫ÜË∂ÖÊõ≤ÁéáÂá†‰Ωï‰∏≠Poincar√©ÁêÉÂíåË∂ÖÊõ≤Èù¢Ê®°ÂûãÁöÑÊ†∏ÂøÉÊìç‰ΩúÊ¢ØÂ∫¶ÔºåÂèëÁé∞Â§ßËåÉÊï∞ÂµåÂÖ•‰ºöÂØºËá¥Âü∫‰∫éÊ¢ØÂ∫¶ÁöÑËÆ≠ÁªÉ‰∏çÁ®≥ÂÆö„ÄÇÂü∫‰∫éËøô‰∫õÂèëÁé∞ÔºåHyper++ÂºïÂÖ•‰∫ÜÁ®≥ÂÆöÁöÑËØÑËÆ∫ÂëòËÆ≠ÁªÉ„ÄÅÁâπÂæÅÊ≠£ÂàôÂåñÂíå‰ºòÂåñÂèãÂ•ΩÁöÑË∂ÖÊõ≤ÁéáÁΩëÁªúÂ±ÇÁöÑ‰∏âÂ§ßÁªÑ‰ª∂ÔºåÊòæËëóÊèêÂçá‰∫ÜÂ≠¶‰π†ÁöÑÁ®≥ÂÆöÊÄßÂíåÊïàÁéá„ÄÇ', title='Hyper++ÔºöÊèêÂçáË∂ÖÊõ≤ÁéáÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÁöÑÁ®≥ÂÆöÊÄß‰∏éÊÄßËÉΩ'))
[18.12.2025 12:46] Using data from previous issue: {"categories": ["#robotics", "#training", "#multimodal", "#diffusion"], "emoji": "ü§ñ", "ru": {"title": "–û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ TacThru-UMI, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞—Ç—á–∏–∫ TacThru —Å –ø–æ–ª–∏—Ç–∏–∫–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Transformer –∏ –¥–∏—Ñ—Ñ—É–∑–∏
[18.12.2025 12:46] Using data from previous issue: {"categories": ["#diffusion", "#multimodal", "#audio", "#3d", "#video", "#robotics", "#open_source"], "emoji": "üó£Ô∏è", "ru": {"title": "–ü—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π –≥–æ–ª–æ–≤—ã –≤ –¥–∏–∞–ª–æ–≥–æ–≤–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏", "desc": "TIMAR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è 
[18.12.2025 12:46] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#dataset"], "emoji": "üé≠", "ru": {"title": "–ü—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤–∞–∂–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏", "desc": "LikeBench –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–Ω–æ–≥–æ—Å–µ–∞–Ω—Å–æ–≤—É—é –æ—Ü–µ–Ω–æ—á–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM, —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å –Ω–∞ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
[18.12.2025 12:46] Renaming data file.
[18.12.2025 12:46] Renaming previous data. hf_papers.json to ./d/2025-12-18.json
[18.12.2025 12:46] Saving new data file.
[18.12.2025 12:46] Generating page.
[18.12.2025 12:46] Renaming previous page.
[18.12.2025 12:46] Renaming previous data. index.html to ./d/2025-12-18.html
[18.12.2025 12:46] Writing result.
[18.12.2025 12:46] Renaming log file.
[18.12.2025 12:46] Renaming previous data. log.txt to ./logs/2025-12-18_last_log.txt
