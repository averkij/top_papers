[15.10.2024 08:42] Read previous papers.
[15.10.2024 08:42] Get feed.
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10139
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09732
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09584
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10563
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07985
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10783
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10774
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10306
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10818
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06634
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10792
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07752
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09335
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10813
[15.10.2024 08:42] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10803
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 0. Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks ...
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 1. With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large mu...
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 2. Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instruction-following (IF) alignment within the RAG domain remains limited. To add...
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 3. We present MEGA-Bench, an evaluation suite that scales multimodal evaluation to over 500 real-world tasks, to address the highly heterogeneous daily use cases of end users. Our objective is to optimize for a set of high-quality data samples that cover a highly diverse and rich set of multimodal task...
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 4. Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for ...
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 5. The large-scale training of multi-modal models on data scraped from the web has shown outstanding utility in infusing these models with the required world knowledge to perform effectively on multiple downstream tasks. However, one downside of scraping data from the web can be the potential sacrifice...
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 6. In recent years there have been remarkable breakthroughs in image-to-video generation. However, the 3D consistency and camera controllability of generated frames have remained unsolved. Recent studies have attempted to incorporate camera control into the generation process, but their results are oft...
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 7. Character image animation, which generates high-quality videos from a reference image and target pose sequence, has seen significant progress in recent years. However, most existing methods only apply to human figures, which usually do not generalize well on anthropomorphic characters commonly used ...
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 8. Understanding fine-grained temporal dynamics is crucial for multimodal video comprehension and generation. Due to the lack of fine-grained temporal annotations, existing video benchmarks mostly resemble static image benchmarks and are incompetent at evaluating models for temporal understanding. In t...
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 9. Large Language Models (LLMs) have demonstrated remarkable performance across multiple tasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. None...
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 10. Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). A...
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 11. Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating these video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently m...
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 12. Supervised fine-tuning (SFT) is crucial for aligning Large Language Models (LLMs) with human instructions. The primary goal during SFT is to select a small yet representative subset of training data from the larger pool, such that fine-tuning with this subset achieves results comparable to or even e...
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 13. Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. This paper introdu...
[15.10.2024 08:42] ********************************************************************************
[15.10.2024 08:42] Abstract 14. Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills. Recent advances in ...
[15.10.2024 08:42] Read previous papers.
[15.10.2024 08:42] Generating reviews via LLM API.
[15.10.2024 08:42] Using data from previous issue: {"desc": "MMIE - это новый масштабный бенчмарк для оценки моделей с чередующимся мультимодальным пониманием и генерацией (LVLMs). Он содержит 20 000 тщательно отобранных мультимодальных запросов из 12 областей знаний. MMIE поддерживает как чередующиеся входные данные, так и выходные, предлагая разли
[15.10.2024 08:42] Using data from previous issue: {"desc": "LOKI - это новый бенчмарк для оценки способности больших мультимодальных моделей (LMM) обнаруживать синтетические данные в различных модальностях. Он включает в себя 18 тысяч вопросов по видео, изображениям, 3D, тексту и аудио, разделенных на 26 подкатегорий с четкими уровнями сложности. L
[15.10.2024 08:42] Using data from previous issue: {"desc": "VIF-RAG - это первый автоматизированный и масштабируемый конвейер для улучшения следования инструкциям в системах RAG. Авторы создали набор атомарных инструкций, правила их комбинирования и верификации, а также модели для перефразирования инструкций. На основе этого был сгенерирован большо
[15.10.2024 08:42] Using data from previous issue: {"desc": "MEGA-Bench - это новый набор данных для оценки мультимодальных моделей, включающий более 500 реальных задач. Он охватывает широкий спектр форматов вывода и использует более 40 метрик для оценки. MEGA-Bench предоставляет подробный отчет о возможностях моделей по различным измерениям. Авторы
[15.10.2024 08:42] Using data from previous issue: {"desc": "Статья представляет новый эталонный набор данных для оценки математических способностей больших языковых моделей на уровне олимпиад. Набор данных содержит 4428 задач олимпиадного уровня с тщательной человеческой аннотацией, разделенных на 33 поддомена и 10 уровней сложности. Эксперименты п
[15.10.2024 08:42] Using data from previous issue: {"desc": "LiveXiv - это масштабируемый развивающийся бенчмарк, основанный на научных статьях ArXiv. Он автоматически генерирует пары вопрос-ответ по визуальному контенту (VQA) из рукописей, используя графики, диаграммы и таблицы. Бенчмарк предлагает эффективный подход к оценке производительности мод
[15.10.2024 08:42] Using data from previous issue: {"desc": "Cavia - это новая система для генерации видео с контролируемой камерой и возможностью создания нескольких ракурсов. Она расширяет модули пространственного и временного внимания, улучшая согласованность ракурсов и временную согласованность. Cavia позволяет пользователю точно указывать движе
[15.10.2024 08:42] Using data from previous issue: {"desc": "Статья представляет Animate-X - универсальную систему анимации персонажей на основе LDM, включая антропоморфных. Авторы вводят Pose Indicator для улучшенного представления движений, используя как неявные, так и явные методы. Система использует визуальные признаки CLIP для извлечения сути д
[15.10.2024 08:42] Using data from previous issue: {"desc": "TemporalBench - это новый бенчмарк для оценки детального понимания временной динамики в видео. Он состоит из примерно 10 тысяч пар вопрос-ответ по видео, основанных на высококачественных аннотациях временной динамики в видеоклипах. Бенчмарк позволяет оценивать различные аспекты временного 
[15.10.2024 08:42] Using data from previous issue: {"desc": "В статье предлагается новый метод под названием Tree of Problems (ToP) для решения сложных задач с помощью больших языковых моделей. ToP является упрощенной версией метода Tree of Thoughts (ToT) и предназначен для задач, которые можно разделить на идентичные подзадачи. Авторы провели эмпир
[15.10.2024 08:42] Using data from previous issue: {"desc": "Статья представляет новый метод инверсии и редактирования изображений с использованием стохастических эквивалентов моделей выпрямленного потока (Rectified Flow). Авторы предлагают инверсию RF с помощью динамического оптимального управления, полученного через линейный квадратичный регулятор
[15.10.2024 08:42] Using data from previous issue: {"desc": "В статье рассматриваются проблемы существующих бенчмарков для оценки моделей видео-языкового понимания. Авторы выявили, что многие задачи можно решить без глубокого анализа временной информации в видео. Предложен новый бенчмарк TVBench, требующий высокого уровня понимания временных аспекто
[15.10.2024 08:42] Using data from previous issue: {"desc": "Статья посвящена проблеме выбора данных для обучения больших языковых моделей (LLM) с помощью контролируемой доводки (SFT). Авторы провели репликацию нескольких методов самооценки на наборах данных из двух миллионов примеров и обнаружили, что большинство методов не превосходят случайный вы
[15.10.2024 08:42] Using data from previous issue: {"desc": "Статья представляет LongMemEval - комплексный бенчмарк для оценки долгосрочной памяти чат-ассистентов на основе больших языковых моделей (LLM). Авторы выделяют пять ключевых способностей долгосрочной памяти и создают набор из 500 вопросов для их тестирования. Исследование показывает значит
[15.10.2024 08:42] Using data from previous issue: {"desc": "Статья представляет новый подход к автономной манипуляции гуманоидных роботов - Improved 3D Diffusion Policy (iDP3). Этот метод использует эгоцентрические 3D визуальные представления, что позволяет избежать ограничений, связанных с калибровкой камеры и сегментацией облака точек. iDP3 демон
[15.10.2024 08:42] Trying to get texts in Chinese.
[15.10.2024 08:42] Mistral request. Write simple and brief explanation (2-3 sentences) of an article in Chinese. Text:

Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. We publicly release our benchmark and code in https://mmie-bench.github.io/.
[15.10.2024 08:42] Mistral response. {"id": "8c458976b68746fcaf63def7a778c42b", "object": "chat.completion", "created": 1728981748, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aMMIE\u7684\u5927\u89c4\u6a21\u77e5\u8bc6\u5bc6\u96c6\u578b\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u7684\u4ea4\u9519\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002MMIE\u5305\u542b20,000\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u591a\u6a21\u6001\u67e5\u8be2\uff0c\u6db5\u76d63\u4e2a\u7c7b\u522b\u300112\u4e2a\u9886\u57df\u548c102\u4e2a\u5b50\u9886\u57df\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u51cf\u5c11\u504f\u5dee\u5e76\u63d0\u9ad8\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 380, "total_tokens": 521, "completion_tokens": 141}}
[15.10.2024 08:42] Mistral request. Write pinyin transcription for text. Text:

这篇文章介绍了一种名为MMIE的大规模知识密集型基准，用于评估大型视觉语言模型（LVLMs）中的交错多模态理解和生成能力。MMIE包含20,000个精心策划的多模态查询，涵盖3个类别、12个领域和102个子领域，并提供了可靠的自动评估指标，以减少偏差并提高评估准确性。
[15.10.2024 08:42] Mistral response. {"id": "550d5fc7a69b433c90d9d43b4e0a2ea8", "object": "chat.completion", "created": 1728981754, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "Zh\u00e8 pi\u0101n w\u00e9n zh\u0101ng ji\u00e8 sh\u00e0o le y\u012b zh\u01d2ng m\u00edng w\u00e8i MMIE de d\u00e0 gu\u012b m\u00f3 zh\u012b shi m\u00ec du\u014d x\u00edng j\u012b zh\u01d4n, y\u00f2ng y\u00fa p\u00edng gu\u0101 d\u00e0 x\u00edng sh\u00ec ju\u00e9 y\u01d4 m\u00f3 x\u00ecng (LVLMs) zh\u014dng de ji\u0101o cu\u00f2 du\u014d m\u00f3 sh\u00ec li\u01ceo j\u00ec h\u00e9 sh\u0113ng ch\u01cen n\u00e9ng l\u00ec. MMIE b\u0101o h\u00e1n 20,000 g\u00e8 j\u012bng x\u012bn c\u00e8 hu\u00e0 de du\u014d m\u00f3 sh\u00ec ch\u00e1 x\u00fan, h\u00e1n gu\u00e0 3 g\u00e8 l\u00e8i bi\u00e9, 12 g\u00e8 l\u01d0ng y\u00f9 h\u00e9 102 g\u00e8 z\u01d0 l\u01d0ng y\u00f9, b\u00ecng t\u00ed g\u014dng le k\u011b k\u00e0o de z\u00ec d\u00f2ng p\u00edng gu\u0101 zh\u01d0 bi\u01ceo, y\u01d0 ji\u01cen sh\u01ceo pi\u00e0n ch\u00e1 \u00e9r t\u00ed g\u0101o p\u00edng gu\u0101 zh\u01d4n qu\u00e8 x\u00ecng.", "tool_calls": null}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 156, "total_tokens": 427, "completion_tokens": 271}}
[15.10.2024 08:42] Mistral request. Write vocab of difficult words for this text as an array of objects with fields 'word', 'pinyin', 'trans'. Return as python list without formatting. Return list and nothing else. Text:

这篇文章介绍了一种名为MMIE的大规模知识密集型基准，用于评估大型视觉语言模型（LVLMs）中的交错多模态理解和生成能力。MMIE包含20,000个精心策划的多模态查询，涵盖3个类别、12个领域和102个子领域，并提供了可靠的自动评估指标，以减少偏差并提高评估准确性。
[15.10.2024 08:42] Mistral response. {"id": "054e3b23468241eaa1c3e8281a5e58ec", "object": "chat.completion", "created": 1728981761, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "content": "[\n    {\"word\": \"\u5927\u89c4\u6a21\", \"pinyin\": \"d\u00e0gu\u012bm\u00f3\", \"trans\": \"large-scale\"},\n    {\"word\": \"\u77e5\u8bc6\u5bc6\u96c6\u578b\", \"pinyin\": \"zh\u012bshi m\u00ecj\u012b x\u00edng\", \"trans\": \"knowledge-intensive\"},\n    {\"word\": \"\u57fa\u51c6\", \"pinyin\": \"j\u012bzh\u01d4n\", \"trans\": \"benchmark\"},\n    {\"word\": \"\u8bc4\u4f30\", \"pinyin\": \"p\u00edngg\u016b\", \"trans\": \"evaluate\"},\n    {\"word\": \"\u4ea4\u9519\", \"pinyin\": \"ji\u0101ocu\u00f2\", \"trans\": \"interleaved\"},\n    {\"word\": \"\u591a\u6a21\u6001\", \"pinyin\": \"du\u014d m\u00f3sh\u00ec\", \"trans\": \"multimodal\"},\n    {\"word\": \"\u7406\u89e3\", \"pinyin\": \"l\u01d0ji\u011b\", \"trans\": \"understanding\"},\n    {\"word\": \"\u751f\u6210\", \"pinyin\": \"sh\u0113ngch\u00e9ng\", \"trans\": \"generation\"},\n    {\"word\": \"\u80fd\u529b\", \"pinyin\": \"n\u00e9ngl\u00ec\", \"trans\": \"ability\"},\n    {\"word\": \"\u7cbe\u5fc3\u7b56\u5212\", \"pinyin\": \"j\u012bngx\u012bn c\u00e8hu\u00e0\", \"trans\": \"carefully designed\"},\n    {\"word\": \"\u67e5\u8be2\", \"pinyin\": \"ch\u00e1x\u00fan\", \"trans\": \"query\"},\n    {\"word\": \"\u6db5\u76d6\", \"pinyin\": \"h\u00e1ng\u00e0i\", \"trans\": \"cover\"},\n    {\"word\": \"\u7c7b\u522b\", \"pinyin\": \"l\u00e8ibi\u00e9\", \"trans\": \"category\"},\n    {\"word\": \"\u9886\u57df\", \"pinyin\": \"l\u01d0ngy\u00f9\", \"trans\": \"domain\"},\n    {\"word\": \"\u5b50\u9886\u57df\", \"pinyin\": \"z\u01d0 l\u01d0ngy\u00f9\", \"trans\": \"subdomain\"},\n    {\"word\": \"\u53ef\u9760\", \"pinyin\": \"k\u011bk\u00e0o\", \"trans\": \"reliable\"},\n    {\"word\": \"\u81ea\u52a8\u8bc4\u4f30\", \"pinyin\": \"z\u00ecd\u00f2ng p\u00edngg\u016b\", \"trans\": \"", "tool_calls": null}, "finish_reason": "length"}], "usage": {"prompt_tokens": 188, "total_tokens": 700, "completion_tokens": 512}}
[15.10.2024 08:42] Renaming data file.
[15.10.2024 08:42] Renaming previous data. hf_papers.json to 2024-10-14_hf_papers.json
[15.10.2024 08:42] Saving new data file.
[15.10.2024 08:42] Generating page.
[15.10.2024 08:42] Generating Chinese page for reading.
[15.10.2024 08:42] Renaming previous page.
[15.10.2024 08:42] Renaming previous data. index.html to 2024-10-14_hf_papers.html
[15.10.2024 08:42] Renaming previous Chinese page.
[15.10.2024 08:42] Renaming previous data. zh.html to 2024-10-14_zh_reading_task.html
[15.10.2024 08:42] Writing result.
[15.10.2024 08:42] Writing Chinese reading task.
[15.10.2024 08:42] Renaming log file.
[15.10.2024 08:42] Renaming previous data. log.txt to 2024-10-14_last_log.txt
