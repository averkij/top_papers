[15.10.2024 16:15] Read previous papers.
[15.10.2024 16:15] Get feed.
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09732
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10139
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09584
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10306
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10563
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10783
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07985
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10774
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10792
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10818
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10594
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09335
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.06634
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10803
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10813
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.07752
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.09733
[15.10.2024 16:15] Extract page data from URL. URL: https://huggingface.co/papers/2410.09223
[15.10.2024 16:15] Get page data from previous paper. URL: https://huggingface.co/papers/2410.10630
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 0. With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large mu...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 1. Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks ...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 2. Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instruction-following (IF) alignment within the RAG domain remains limited. To add...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 3. Character image animation, which generates high-quality videos from a reference image and target pose sequence, has seen significant progress in recent years. However, most existing methods only apply to human figures, which usually do not generalize well on anthropomorphic characters commonly used ...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 4. We present MEGA-Bench, an evaluation suite that scales multimodal evaluation to over 500 real-world tasks, to address the highly heterogeneous daily use cases of end users. Our objective is to optimize for a set of high-quality data samples that cover a highly diverse and rich set of multimodal task...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 5. The large-scale training of multi-modal models on data scraped from the web has shown outstanding utility in infusing these models with the required world knowledge to perform effectively on multiple downstream tasks. However, one downside of scraping data from the web can be the potential sacrifice...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 6. Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for ...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 7. In recent years there have been remarkable breakthroughs in image-to-video generation. However, the 3D consistency and camera controllability of generated frames have remained unsolved. Recent studies have attempted to incorporate camera control into the generation process, but their results are oft...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 8. Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). A...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 9. Understanding fine-grained temporal dynamics is crucial for multimodal video comprehension and generation. Due to the lack of fine-grained temporal annotations, existing video benchmarks mostly resemble static image benchmarks and are incompetent at evaluating models for temporal understanding. In t...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 10. Retrieval-augmented generation (RAG) is an effective technique that enables large language models (LLMs) to utilize external knowledge sources for generation. However, current RAG systems are solely based on text, rendering it impossible to utilize vision information like layout and images that play...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 11. Supervised fine-tuning (SFT) is crucial for aligning Large Language Models (LLMs) with human instructions. The primary goal during SFT is to select a small yet representative subset of training data from the larger pool, such that fine-tuning with this subset achieves results comparable to or even e...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 12. Large Language Models (LLMs) have demonstrated remarkable performance across multiple tasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. None...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 13. Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills. Recent advances in ...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 14. Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. This paper introdu...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 15. Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating these video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently m...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 16. The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video captioning, visual question answering, and cross-modal retriev...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 17. We employ new tools from mechanistic interpretability in order to ask whether the internal structure of large language models (LLMs) shows correspondence to the linguistic structures which underlie the languages on which they are trained. In particular, we ask (1) when two languages employ the same ...
[15.10.2024 16:15] ********************************************************************************
[15.10.2024 16:15] Abstract 18. LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and pl...
[15.10.2024 16:15] Read previous papers.
[15.10.2024 16:15] Generating reviews via LLM API.
[15.10.2024 16:15] Using data from previous issue: {"desc": "LOKI - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è 18 —Ç—ã—Å—è—á –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ –≤–∏–¥–µ–æ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, 3D, —Ç–µ–∫—Å—Ç—É –∏ –∞—É–¥–∏–æ, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –Ω–∞ 26 –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å —á–µ—Ç–∫–∏–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. L
[15.10.2024 16:15] Using data from previous issue: {"desc": "MMIE - —ç—Ç–æ –Ω–æ–≤—ã–π –º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Å —á–µ—Ä–µ–¥—É—é—â–∏–º—Å—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π (LVLMs). –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç 20 000 —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ 12 –æ–±–ª–∞—Å—Ç–µ–π –∑–Ω–∞–Ω–∏–π. MMIE –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ —á–µ—Ä–µ–¥—É—é—â–∏–µ—Å—è –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, —Ç–∞–∫ –∏ –≤—ã—Ö–æ–¥–Ω—ã–µ, –ø—Ä–µ–¥–ª–∞–≥–∞—è —Ä–∞–∑–ª–∏
[15.10.2024 16:15] Using data from previous issue: {"desc": "VIF-RAG - —ç—Ç–æ –ø–µ—Ä–≤—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ —Å–∏—Å—Ç–µ–º–∞—Ö RAG. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –∞—Ç–æ–º–∞—Ä–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –ø—Ä–∞–≤–∏–ª–∞ –∏—Ö –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –±–æ–ª—å—à–æ
[15.10.2024 16:15] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Animate-X - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ LDM, –≤–∫–ª—é—á–∞—è –∞–Ω—Ç—Ä–æ–ø–æ–º–æ—Ä—Ñ–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç Pose Indicator –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è –∫–∞–∫ –Ω–µ—è–≤–Ω—ã–µ, —Ç–∞–∫ –∏ —è–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ CLIP –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—Ç–∏ –¥
[15.10.2024 16:15] Using data from previous issue: {"desc": "MEGA-Bench - —ç—Ç–æ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—é—â–∏–π –±–æ–ª–µ–µ 500 —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä —Ñ–æ—Ä–º–∞—Ç–æ–≤ –≤—ã–≤–æ–¥–∞ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª–µ–µ 40 –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏. MEGA-Bench –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –º–æ–¥–µ–ª–µ–π –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∏–∑–º–µ—Ä–µ–Ω–∏—è–º. –ê–≤—Ç–æ—Ä—ã
[15.10.2024 16:15] Using data from previous issue: {"desc": "LiveXiv - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π —Ä–∞–∑–≤–∏–≤–∞—é—â–∏–π—Å—è –±–µ–Ω—á–º–∞—Ä–∫, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö ArXiv. –û–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –ø–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –∫–æ–Ω—Ç–µ–Ω—Ç—É (VQA) –∏–∑ —Ä—É–∫–æ–ø–∏—Å–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –≥—Ä–∞—Ñ–∏–∫–∏, –¥–∏–∞–≥—Ä–∞–º–º—ã –∏ —Ç–∞–±–ª–∏—Ü—ã. –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥
[15.10.2024 16:15] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ–ª–∏–º–ø–∏–∞–¥. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç 4428 –∑–∞–¥–∞—á –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è —Å —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –Ω–∞ 33 –ø–æ–¥–¥–æ–º–µ–Ω–∞ –∏ 10 —É—Ä–æ–≤–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø
[15.10.2024 16:15] Using data from previous issue: {"desc": "Cavia - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –∫–∞–º–µ—Ä–æ–π –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —Å–æ–∑–¥–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –û–Ω–∞ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –º–æ–¥—É–ª–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, —É–ª—É—á—à–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ä–∞–∫—É—Ä—Å–æ–≤ –∏ –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å. Cavia –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Ç–æ—á–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å –¥–≤–∏–∂–µ
[15.10.2024 16:15] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω–≤–µ—Ä—Å–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏—Ö —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π –≤—ã–ø—Ä—è–º–ª–µ–Ω–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞ (Rectified Flow). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏–Ω–≤–µ—Ä—Å–∏—é RF —Å –ø–æ–º–æ—â—å—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω—ã–π –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã–π —Ä–µ–≥—É–ª—è—Ç–æ—Ä
[15.10.2024 16:15] Using data from previous issue: {"desc": "TemporalBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏ –≤ –≤–∏–¥–µ–æ. –û–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –ø—Ä–∏–º–µ—Ä–Ω–æ 10 —Ç—ã—Å—è—á –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –ø–æ –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏ –≤ –≤–∏–¥–µ–æ–∫–ª–∏–ø–∞—Ö. –ë–µ–Ω—á–º–∞—Ä–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ 
[15.10.2024 16:15] Using data from previous issue: {"desc": "VisRAG - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (RAG), –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö RAG-—Å–∏—Å—Ç–µ–º, VisRAG —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—Å—é –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç
[15.10.2024 16:15] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –¥–æ–≤–æ–¥–∫–∏ (SFT). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤ —Å–∞–º–æ–æ—Ü–µ–Ω–∫–∏ –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–≤—É—Ö –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–µ—Ç–æ–¥–æ–≤ –Ω–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å–ª—É—á–∞–π–Ω—ã–π –≤—ã
[15.10.2024 16:15] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Tree of Problems (ToP) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. ToP —è–≤–ª—è–µ—Ç—Å—è —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π –º–µ—Ç–æ–¥–∞ Tree of Thoughts (ToT) –∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –ø–æ–¥–∑–∞–¥–∞—á–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —ç–º–ø–∏—Ä
[15.10.2024 16:15] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤ - Improved 3D Diffusion Policy (iDP3). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏–µ 3D –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π –∫–∞–º–µ—Ä—ã –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π –æ–±–ª–∞–∫–∞ —Ç–æ—á–µ–∫. iDP3 –¥–µ–º–æ–Ω
[15.10.2024 16:15] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LongMemEval - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏ —á–∞—Ç-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã–¥–µ–ª—è—é—Ç –ø—è—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏ –∏ —Å–æ–∑–¥–∞—é—Ç –Ω–∞–±–æ—Ä –∏–∑ 500 –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∏—Ö —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç
[15.10.2024 16:15] Using data from previous issue: {"desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—Ä–æ–±–ª–µ–º—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ –º–Ω–æ–≥–∏–µ –∑–∞–¥–∞—á–∏ –º–æ–∂–Ω–æ —Ä–µ—à–∏—Ç—å –±–µ–∑ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –≤–∏–¥–µ–æ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ TVBench, —Ç—Ä–µ–±—É—é—â–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ
[15.10.2024 16:15] Using data from previous issue: {"desc": "MMCOMPOSITION - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM). –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞—Ç—å –∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –Ω–æ–≤—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–ª—É–±
[15.10.2024 16:15] Querying the API.
[15.10.2024 16:15] Got response. {
  "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –º–µ—Ç–æ–¥—ã –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏ –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º —è–∑—ã–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∏ –∏–∑—É—á–∞–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ª–∏ LLM –æ–±—â–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Ö–µ–º—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ö–æ–∂–∏—Ö –º–æ—Ä—Ñ–æ—Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –≤ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, –∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ö–µ–º—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–ª–∏—á–∞—é—â–∏—Ö—Å—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤. –ê–Ω–∞–ª–∏–∑ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –∏ –∫–∏—Ç–∞–π—Å–∫–∏—Ö –º–æ–Ω–æ- –∏ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Å—Ö–µ–º—ã –¥–ª—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —è–∑—ã–∫–∞, –¥–∞–∂–µ –≤ —Å–ª—É—á–∞–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–Ω–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —è–∑—ã–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–æ–ª—å–∫–æ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —è–∑—ã–∫–∞—Ö.",
  "categories": ["#nlp", "#benchmark", "#multimodal"],
  "emoji": "üß†",
  "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –≤ LLM"
}
[15.10.2024 16:15] Get embedding for a paper via LLM API.
[15.10.2024 16:15] Using data from previous issue: {"desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —è–≤–Ω–æ–º—É –º—ã—à–ª–µ–Ω–∏—é –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø—Ä–æ—Ü–µ–¥—É—Ä—É –ø–æ–∏—Å–∫–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –º—ã—Å–ª–µ–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω
[15.10.2024 16:15] Loading Chinese text from previous data.
[15.10.2024 16:15] Renaming data file.
[15.10.2024 16:15] Renaming previous data. hf_papers.json to 2024-10-14_hf_papers.json
[15.10.2024 16:15] Saving new data file.
[15.10.2024 16:15] Generating page.
[15.10.2024 16:15] Generating Chinese page for reading.
[15.10.2024 16:15] Chinese vocab [{'word': 'Â§öÊ®°ÊÄÅ', 'pinyin': 'du≈ç m√≥ shu√†i', 'trans': 'multimodal'}, {'word': 'ËØÑ‰º∞', 'pinyin': 'p√≠ng g≈´', 'trans': 'evaluation'}, {'word': 'Êï∞ÊçÆËßÑÊ®°', 'pinyin': 'sh√π j√π guƒ´ m√≥', 'trans': 'data scale'}, {'word': 'ËåÉÂõ¥', 'pinyin': 'f√†n w√©i', 'trans': 'scope'}, {'word': 'ËØÑ‰º∞Ê∑±Â∫¶', 'pinyin': 'p√≠ng g≈´ shƒìn d√π', 'trans': 'evaluation depth'}, {'word': 'ÈôêÂà∂', 'pinyin': 'xi√†n zh√¨', 'trans': 'limitations'}, {'word': 'ËÄóÊó∂', 'pinyin': 'h√†o sh√≠', 'trans': 'time-consuming'}, {'word': 'ÂÅèÂ∑Æ', 'pinyin': 'piƒÅn chƒÅ', 'trans': 'bias'}, {'word': 'Ëß£ÂÜ≥', 'pinyin': 'jiƒõ ju√©', 'trans': 'solve'}, {'word': 'ÊèêÂá∫', 'pinyin': 't√≠ ch≈´', 'trans': 'propose'}, {'word': 'MMIE', 'pinyin': 'MMIE', 'trans': 'MMIE'}, {'word': 'Â§ßËßÑÊ®°', 'pinyin': 'd√† guƒ´ m√≥', 'trans': 'large-scale'}, {'word': 'Áü•ËØÜÂØÜÈõÜÂûã', 'pinyin': 'zhƒ´ sh√¨ m√¨ j√≠ x√≠ng', 'trans': 'knowledge-intensive'}, {'word': 'Âü∫ÂáÜ', 'pinyin': 'jƒ´ zh«în', 'trans': 'benchmark'}, {'word': 'ËßÜËßâËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'sh√¨ ju√© y«î y√°n m√≥ x√≠ng', 'trans': 'vision-language model'}, {'word': 'Â§öÊ®°ÊÄÅÁêÜËß£', 'pinyin': 'du≈ç m√≥ shu√†i l«ê jiƒõ', 'trans': 'multimodal understanding'}, {'word': 'ÁîüÊàêËÉΩÂäõ', 'pinyin': 'shƒìng ch√©ng n√©ng l√¨', 'trans': 'generative capability'}, {'word': 'Á≤æÂøÉÁ≠ñÂàí', 'pinyin': 'jƒ´ng xƒ´n c√® hu√†', 'trans': 'carefully designed'}, {'word': 'Á±ªÂà´', 'pinyin': 'l√®i bi√©', 'trans': 'categories'}, {'word': 'È¢ÜÂüü', 'pinyin': 'l«êng y√π', 'trans': 'domains'}, {'word': 'Â≠êÈ¢ÜÂüü', 'pinyin': 'z«ê l«êng y√π', 'trans': 'sub-domains'}, {'word': 'Ëá™Âä®Âåñ', 'pinyin': 'z√¨ d√≤ng hu√†', 'trans': 'automated'}, {'word': 'ÂáÜÁ°ÆÊÄß', 'pinyin': 'zh«în qu√® x√¨ng', 'trans': 'accuracy'}, {'word': 'ÁªºÂêà', 'pinyin': 'z≈çng h√©', 'trans': 'comprehensive'}, {'word': 'ÊúâÊïà', 'pinyin': 'y«íu xi√†o', 'trans': 'effective'}]
[15.10.2024 16:15] Renaming previous page.
[15.10.2024 16:15] Renaming previous data. index.html to 2024-10-14_hf_papers.html
[15.10.2024 16:15] Renaming previous Chinese page.
[15.10.2024 16:15] Renaming previous data. zh.html to 2024-10-14_zh_reading_task.html
[15.10.2024 16:15] Writing result.
[15.10.2024 16:15] Writing Chinese reading task.
[15.10.2024 16:15] Renaming log file.
[15.10.2024 16:15] Renaming previous data. log.txt to 2024-10-14_last_log.txt
