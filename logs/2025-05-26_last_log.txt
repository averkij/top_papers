[26.05.2025 18:16] Read previous papers.
[26.05.2025 18:16] Generating top page (month).
[26.05.2025 18:16] Writing top page (month).
[26.05.2025 19:09] Read previous papers.
[26.05.2025 19:09] Get feed.
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18125
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17667
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14669
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18129
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17225
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17612
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15929
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18092
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17618
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17561
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17873
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17941
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16211
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17412
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17399
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17558
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15692
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16479
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16134
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13508
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17955
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16483
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17417
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16770
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17826
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14146
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15389
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17063
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17295
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16270
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17540
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15182
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17091
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17508
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17016
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15805
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18078
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16293
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16056
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17373
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16022
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14256
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12891
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11881
[26.05.2025 19:09] Extract page data from URL. URL: https://huggingface.co/papers/2505.17552
[26.05.2025 19:09] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16409
[26.05.2025 19:09] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.05.2025 19:09] No deleted papers detected.
[26.05.2025 19:09] Downloading and parsing papers (pdf, html). Total: 46.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18125.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18125.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18125.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17667.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17667.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17667.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.14669.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.14669.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.14669.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18129.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18129.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18129.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17225.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17225.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17225.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17612.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17612.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17612.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.15929.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.15929.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.15929.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18092.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18092.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18092.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17618.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17618.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17618.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17561.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17561.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17561.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17873.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17873.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17873.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17941.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17941.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17941.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16211.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16211.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16211.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17412.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17412.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17412.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17399.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17399.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17399.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17558.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17558.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17558.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.15692.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.15692.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.15692.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16479.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16479.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16479.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16134.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16134.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16134.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.13508.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.13508.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.13508.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17955.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17955.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17955.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16483.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16483.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16483.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17417.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17417.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17417.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16770.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16770.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16770.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17826.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17826.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17826.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.14146.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.14146.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.14146.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.15389.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.15389.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.15389.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17063.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17063.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17063.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17295.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17295.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17295.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16270.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16270.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16270.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17540.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17540.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17540.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.15182.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.15182.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.15182.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17091.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17091.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17091.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17508.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17508.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17508.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17016.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17016.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17016.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.15805.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.15805.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.15805.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.18078.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.18078.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.18078.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16293.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16293.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16293.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16056.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16056.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16056.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17373.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.17373.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.17373.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16022.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16022.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16022.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.14256.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.14256.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.14256.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.12891.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.12891.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.12891.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.11881.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.11881.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.11881.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.17552.
[26.05.2025 19:09] Downloading paper 2505.17552 from http://arxiv.org/pdf/2505.17552v1...
[26.05.2025 19:09] Extracting affiliations from text.
[26.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Zijie Qiu * 1 2 Jiaqi Wei * 3 2 Xiang Zhang * 4 2 Sheng Xu 1 2 Kai Zou 5 6 Zhi Jin 7 2 Zhiqiang Gao 2 Nanqing Dong 2 Siqi Sun 1 2 5 2 0 2 3 2 ] . [ 1 2 5 5 7 1 . 5 0 5 2 : r a "
[26.05.2025 19:09] Response: []
[26.05.2025 19:09] Extracting affiliations from text.
[26.05.2025 19:09] Mistral request. Model: mistral-large-latest. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Zijie Qiu * 1 2 Jiaqi Wei * 3 2 Xiang Zhang * 4 2 Sheng Xu 1 2 Kai Zou 5 6 Zhi Jin 7 2 Zhiqiang Gao 2 Nanqing Dong 2 Siqi Sun 1 2 5 2 0 2 3 2 ] . [ 1 2 5 5 7 1 . 5 0 5 2 : r aDe novo peptide sequencing is critical task in proteomics. However, the performance of current deep learning-based methods is limited by the inherent complexity of mass spectrometry data and the heterogeneous distribution of noise signals, leading to data-specific biases. We present RankNovo, the first deep reranking framework that enhances de novo peptide sequencing by leveraging the complementary strengths of multiple sequencing models. RankNovo employs listwise reranking approach, modeling candidate peptides as multiple sequence alignments and utilizing axial attention to extract informative features across candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass Deviation) and RMD (Residual Mass Deviation), which offer delicate supervision by quantifying mass differences between peptides at both the sequence and residue levels. Extensive experiments demonstrate that RankNovo not only surpasses its base models used to generate training candidates for reranking pre-training, but also sets new state-ofthe-art benchmark. Moreover, RankNovo exhibits strong zero-shot generalization to unseen modelsthose whose generations were not exposed during training, highlighting its robustness and potential as universal reranking framework for peptide sequencing. Our work presents novel reranking strategy that fundamentally challenges existing single-model paradigms and advances the frontier of accurate de novo sequencing. Our source code is provided on GitHub 1. *Equal contribution 1Fudan University 2Shanghai Artificial Intelligence Laboratory 3Zhejiang University 4University of British Columbia 5NetMind.AI 6ProtagoLabs Inc 7Soochow University. Correspondence to: Siqi Sun <siqisun@fudan.edu.cn>, Nanqing Dong <dongnanqing@pjlab.org.cn>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1https://github.com/BEAM-Labs/denovo 1 Figure 1. (A) De Novo Peptide Sequencing Workflow Using Tandem Mass Spectrometry: Our objective is to predict peptide sequences from MS/MS spectra, as illustrated in the final two steps. (B) Motivation for RankNovo: Current de novo peptide sequencing models exhibit data preference in their peptide predictions. RankNovo improves overall prediction accuracy by reranking the outputs of these models to identify the optimal sequence. 1. Introduction Identifying proteins is critical task in proteomics, with mass spectrometry-based shotgun proteomics being widely regarded as the predominant technique for this purpose (Aebersold & Mann, 2003). As shown in Figure 1, this process begins with the enzymatic digestion of proteins into smaller peptide fragments, which are then analyzed using tandem mass spectrometry (MS/MS) to generate spectra (Nesvizhskii et al., 2003). These spectra are subsequently interpreted to infer peptide sequences, enabling precise identification and characterization of proteins. This foundational approach is pivotal for advancing research in proteomics (Aebersold & Mann, 2003). Proteomics utilizes two primary methodologies for peptide sequence identification: database searching (Ma et al., 2003; Chen et al., 2020; Leprevost et al., 2014; Shteynberg et al., 2011; Chi et al., 2018) and de novo sequencing (DanÀácƒ±k et al., 1999; Chi et al., 2013). In database searching, experimental spectra are matched against pre-existing entries in protein databases to identify the most likely sequences. Although effective for identifying known peptides, this approach is inherently constrained by the completeness of the database, posing challenges when encountering novel RankNovo: Universal Reranking Approach for Robust De Novo Peptide Sequencing or uncharacterized sequences (Karunratanakul et al., 2019; Hettich et al., 2013). On the other hand, de novo sequencing leverages the intrinsic patterns of tandem mass spectra to directly infer peptide sequences without requiring reference database, enabling the discovery of novel peptides. Consequently, de novo sequencing has emerged as critical technique for peptide identification, significantly advancing the scope of proteomic analysis (Ng et al., 2023). Over the past two decades, de novo sequencing has made substantial progress, evolving from graph-theoretic and dynamic programming-based methods to more sophisticated approaches driven by deep learning (Ma et al., 2003; Liu et al., 2022; LeCun et al., 2015; Zhang et al., 2025a; Gao et al., 2023). DeepNovo (Tran et al., 2017) was the first to apply deep learning to de novo sequencing, which inspired series of subsequent models(Zhou et al., 2017; Karunratanakul et al., 2019; Yang et al., 2019; Liu et al., 2023). More recently, Transformer architectures have been introduced to model de novo sequencing as machine translation task (Yilmaz et al., 2022; Zhang et al., 2024b; Mao et al., 2023; Eloff et al., 2023a; Yang et al., 2024b; Xia et al., 2024a). Building upon this foundation, ContraNovo (Jin et al., 2024) further advanced the field by incorporating multimodal alignment strategies, achieving state-of-the-art performance. Despite recent advancements in de novo peptide sequencing, these methods still exhibit notable accuracy limitations compared to traditional database search approaches (Muth et al., 2018). The primary challenge stems from the inherent complexity of mass spectrometry data, which consists of mixture of heterogeneous distributions. This complexity is driven by variations in experimental conditions, such as differences in instrumentation, protocols, and target protein species, each of which introduces distinct noise patterns into the acquired spectra (Zubarev & Mann, 2007; Chang et al., 2016). As shown in Fig. 1(B), no model is exempt from issues of generalization and preferential bias, as evidenced by the presence of unique correct predictions from models that otherwise exhibit weaker overall performance. This observation motivates rethinking of de novo peptide sequencing as reranking task, where trained meta-model selects the optimal prediction from collection of outputs generated by multiple de novo models. In this paper, we introduce RankNovo, novel deep reranking framework designed to address the preferential bias challenges inherent in peptide sequencing. In such complex task, peptide candi"
[26.05.2025 19:09] Mistral response. {"id": "ccba46c214b9449b91453e61dc0d765d", "object": "chat.completion", "created": 1748286589, "model": "mistral-large-latest", "choices": [{"index": 0, "message": {"role": "assistant", "tool_calls": null, "content": "```python\n['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Zhejiang University', 'University of British Columbia', 'NetMind.AI', 'ProtagoLabs Inc', 'Soochow University']\n```"}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 1819, "total_tokens": 1878, "completion_tokens": 59}}
[26.05.2025 19:09] Response: ```python
['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Zhejiang University', 'University of British Columbia', 'NetMind.AI', 'ProtagoLabs Inc', 'Soochow University']
```
[26.05.2025 19:09] Deleting PDF ./assets/pdf/2505.17552.pdf.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Downloading and parsing paper https://huggingface.co/papers/2505.16409.
[26.05.2025 19:09] Extra JSON file exists (./assets/json/2505.16409.json), skip PDF parsing.
[26.05.2025 19:09] Paper image links file exists (./assets/img_data/2505.16409.json), skip HTML parsing.
[26.05.2025 19:09] Success.
[26.05.2025 19:09] Enriching papers with extra data.
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 0. TabSTAR, a tabular foundation model with semantically target-aware representations, achieves state-of-the-art performance in classification tasks with text features through transfer learning without dataset-specific parameters.  					AI-generated summary 				 While deep learning has achieved remarka...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 1. A framework called QwenLong-L1 enhances large reasoning models for long-context reasoning through reinforcement learning, achieving leading performance on document question-answering benchmarks.  					AI-generated summary 				 Recent large reasoning models (LRMs) have demonstrated strong reasoning c...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 2. Quartet, a hardware-supported FP4 training approach for large language models, demonstrates state-of-the-art accuracy while significantly reducing computational costs compared to standard or FP8 precision.  					AI-generated summary 				 The rapid advancement of large language models (LLMs) has been...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 3. A unified reinforcement learning system, V-Triune, combines visual reasoning and perception tasks in vision-language models through a single training pipeline, achieving significant improvements across various tasks.  					AI-generated summary 				 Reinforcement learning (RL) has significantly advan...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 4. A diagnostic set examines and categorizes reasoning rigidity in large language models, identifying patterns where models ignore instructions and default to familiar reasoning.  					AI-generated summary 				 Large language models have demonstrated remarkable proficiency in long and complex reasoning...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 5. Agent Distillation transfers reasoning and task-solving capabilities from large language models to smaller models using enhanced prompts and self-consistent actions, matching performance of larger models on various reasoning tasks.  					AI-generated summary 				 Large language models (LLMs) excel a...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 6. A new benchmark, PhyX, evaluates models' physics-grounded reasoning in visual scenarios, revealing significant limitations in current models' physical understanding compared to human experts.  					AI-generated summary 				 Existing benchmarks fail to capture a crucial aspect of intelligence: physic...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 7. QwenLong-CPRS enhances large language models with multi-granularity context compression, dynamic optimization guided by natural language, and efficient bidirectional reasoning and parallel inference, achieving superior performance and context management.  					AI-generated summary 				 This technica...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 8. EvoSearch, an evolutionary search method, enhances test-time scaling for diffusion and flow-based generative models, improving image and video generation quality, diversity, and generalizability.  					AI-generated summary 				 As the marginal cost of scaling computation (data and parameters) during...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 9. ANSE enhances video diffusion models by selecting noise seeds based on model confidence, improving video quality and temporal coherence with minimal increase in inference time.  					AI-generated summary 				 The choice of initial noise significantly affects the quality and prompt alignment of video...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 10. A novel simulator and experiment-guided ranking method improve hypothesis prioritization in scientific discovery by incorporating simulated experimental outcomes.  					AI-generated summary 				 Hypothesis ranking is a crucial component of automated scientific discovery, particularly in natural scie...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 11. VeriThinker reduces the length of complex reasoning chains in Large Reasoning Models (LRMs) by fine-tuning them on a verification task, thereby decreasing inference costs without significantly sacrificing accuracy.  					AI-generated summary 				 Large Reasoning Models (LRMs) excel at complex tasks ...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 12. AudioTrust evaluates the trustworthiness of Audio Large Language Models across multifaceted dimensions, using a comprehensive dataset and specific metrics to assess their performance in real-world audio scenarios.  					AI-generated summary 				 The rapid advancement and expanding applications of Au...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 13. A scalable 3D shape generation framework using sparse volumes and spatial sparse attention, enabling high-resolution generation with reduced computational requirements.  					AI-generated summary 				 Generating high resolution 3D shapes using volumetric representations such as Signed Distance Funct...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 14. FullFront is a benchmark evaluating Multimodal Large Language Models across conceptualization, comprehension, and implementation phases in front-end engineering.  					AI-generated summary 				 Front-end engineering involves a complex workflow where engineers conceptualize designs, translate them in...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 15. The use of carefully crafted hallucinations in a curriculum learning approach within the DPO alignment procedure significantly enhances LLMs' hallucination detection abilities.  					AI-generated summary 				 Aligning large language models (LLMs) to accurately detect hallucinations remains a signifi...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 16. A novel RL framework, TAPO, integrates external guidance to enhance model performance and exploration compared to existing methods.  					AI-generated summary 				 Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically ...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 17. A unified framework for restoring nighttime images under diverse weather conditions using dual priors and adaptive collaboration.  					AI-generated summary 				 Restoring nighttime images affected by multiple adverse weather conditions is a practical yet under-explored research problem, as multiple...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 18. Large language models exhibit positional bias -- systematic neglect of information at specific context positions -- yet its interplay with linguistic diversity remains poorly understood. We present a cross-linguistic study across five typologically distinct languages (English, Russian, German, Hindi...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 19. A novel framework, Time-R1, enhances moderate-sized LLMs with comprehensive temporal abilities through a reinforcement learning curriculum, outperforming larger models on future event prediction and creative scenario generation benchmarks.  					AI-generated summary 				 Large Language Models (LLMs)...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 20. A study of diffusion classifiers across multiple datasets and tasks reveals their compositional understanding, highlighting domain-specific performance effects and timestep weighting importance.  					AI-generated summary 				 Understanding visual scenes is fundamental to human intelligence. While d...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 21. CANOE improves LLM faithfulness in generation tasks using synthetic QA data and Dual-GRPO reinforcement learning without human annotations.  					AI-generated summary 				 Teaching large language models (LLMs) to be faithful in the provided context is crucial for building reliable information-seekin...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 22. The rapid growth of voice assistants powered by large language models (LLM) has highlighted a need for speech instruction data to train these systems. Despite the abundance of speech recognition data, there is a notable scarcity of speech instruction data, which is essential for fine-tuning models t...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 23. A benchmark called RBench-V evaluates multi-modal models' vision-indispensable reasoning through image manipulation and auxiliary line construction, demonstrating that current models struggle with multi-modal outputs.  					AI-generated summary 				 The rapid advancement of native multi-modal models...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 24. Trinity-RFT is a flexible and scalable framework for reinforcement fine-tuning of large language models, supporting various interaction modes and data pipelines.  					AI-generated summary 				 Trinity-RFT is a general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 25. A lightweight, model-agnostic framework decouples the retrieval and generation processes in RAG systems, enhancing performance with minimal training data.  					AI-generated summary 				 Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge du...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 26. VLMs are more vulnerable to harmful meme-based prompts than to synthetic images, and while multi-turn interactions offer some protection, significant vulnerabilities remain.  					AI-generated summary 				 Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet most evaluations...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 27. Synthetic Data RL enhances foundation models through reinforcement learning using only synthetic data, achieving performance comparable to models trained with full human-labeled data.  					AI-generated summary 				 Reinforcement learning (RL) is a powerful way to adapt foundation models to speciali...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 28. The ScanBot dataset, focusing on instruction-conditioned high-precision robotic surface scanning, showcases challenges for vision-language action models in achieving precise scanning trajectories under real-world constraints.  					AI-generated summary 				 We introduce ScanBot, a novel dataset desi...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 29. The Transformer Copilot framework enhances large language model performance through a Copilot model that refines the Pilot's logits based on a Mistake Log, leading to consistent performance improvements across various benchmarks.  					AI-generated summary 				 Large language models are typically ad...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 30. RePrompt, a reprompting framework using reinforcement learning, enhances text-to-image generation by optimizing for image-level outcomes, significantly improving spatial layout and compositional generalization.  					AI-generated summary 				 Despite recent progress in text-to-image (T2I) generation...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 31. ReflAct, a new reasoning backbone for LLM agents, improves goal alignment and reduces hallucinations by continuously reflecting on the agent's state, surpassing ReAct and other enhanced variants.  					AI-generated summary 				 Recent advances in LLM agents have largely built on reasoning backbones ...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 32. Auto-regressive text LLMs trained on text can develop internal capabilities for understanding images and audio, enabling them to perform classification tasks across different modalities without fine-tuning.  					AI-generated summary 				 This paper presents a fascinating find: By training an auto-r...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 33. A regularized policy gradient framework is introduced to explore KL divergence formulations for enhancing the reasoning capabilities of LLMs in online reinforcement learning, demonstrating improved training stability and performance.  					AI-generated summary 				 Policy gradient algorithms have be...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 34. We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and ...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 35. LLMs frequently violate contextual security policies by leaking sensitive information, particularly under indirect attacks, indicating a critical gap in current safety mechanisms.  					AI-generated summary 				 As Large Language Models (LLMs) are increasingly deployed in sensitive domains such as e...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 36. DanceTogether, an end-to-end diffusion framework, generates long, photorealistic multi-actor interaction videos from single reference images and pose-mask streams, outperforming existing systems.  					AI-generated summary 				 Controllable video generation (CVG) has advanced rapidly, yet current sy...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 37. Notes Writing enhances iterative RAG by generating concise notes at each step, improving reasoning and performance while minimizing output increase.  					AI-generated summary 				 Iterative RAG for multi-hop question answering faces challenges with lengthy contexts and the buildup of irrelevant inf...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 38. MoE models achieve efficient scaling in LLMs with expert offloading, emphasizing the importance of local routing consistency and cache effectiveness.  					AI-generated summary 				 Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts dur...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 39. A simple and efficient method for value model training on long-context reasoning traces improves test-time performance and reduces computational cost compared to existing methods.  					AI-generated summary 				 In this paper, we propose a simple and efficient method for value model training on long...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 40. NOVER, a reinforcement learning framework that eliminates the need for external verifiers, enhances language model performance across text-to-text tasks.  					AI-generated summary 				 Recent advances such as DeepSeek R1-Zero highlight the effectiveness of incentive training, a reinforcement learni...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 41. FuxiMT, a Chinese-centric multilingual machine translation model utilizing a sparsified large language model, demonstrates superior performance in low-resource scenarios and strong zero-shot capabilities across 65 languages.  					AI-generated summary 				 In this paper, we present FuxiMT, a novel C...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 42. A benchmark called TIME assesses temporal reasoning in LLMs across varied real-world challenges, including intensive temporal information, fast-changing event dynamics, and complex social interactions, and evaluates the impact of test-time scaling.  					AI-generated summary 				 Temporal reasoning ...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 43. Orthogonal Residual Updates enhance feature learning and training stability by decomposing module outputs to contribute primarily novel features.  					AI-generated summary 				 Residual connections are pivotal for deep neural networks, enabling greater depth by mitigating vanishing gradients. Howev...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 44. RankNovo is a deep reranking framework that enhances de novo peptide sequencing using multiple models and axial attention, achieving superior performance and generalization.  					AI-generated summary 				 De novo peptide sequencing is a critical task in proteomics. However, the performance of curre...
[26.05.2025 19:09] ********************************************************************************
[26.05.2025 19:09] Abstract 45. FREESON, a novel framework that integrates retrieval and reasoning roles within LRMs using CT-MCTS, improves the performance of multistep reasoning models in QA tasks by reducing representation bottlenecks.  					AI-generated summary 				 Large Reasoning Models (LRMs) have demonstrated remarkable ca...
[26.05.2025 19:09] Read previous papers.
[26.05.2025 19:09] Generating reviews via LLM API.
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#transfer_learning", "#architecture", "#benchmark", "#training"], "emoji": "üìä", "ru": {"title": "TabSTAR: –£–º–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "TabSTAR - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#long_context", "#optimization", "#benchmark", "#rl", "#reasoning"], "emoji": "üß†", "ru": {"title": "QwenLong-L1: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "QwenLong-L1 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, —É–ª—É—á—à–∞—é—â–∏–π –º–æ–¥–µ–ª–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (LRM) –¥
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#architecture", "#training", "#inference", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é 4-–±–∏—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Quartet - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º 4-–±–∏—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ 
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#rl", "#dataset", "#multimodal", "#optimization", "#training", "#open_source", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "V-Triune - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –∑–∞–¥–∞—á–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#math", "#dataset", "#data", "#interpretability", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∂–µ—Å—Ç–∫–æ—Å—Ç–∏ –º—ã—à–ª–µ–Ω–∏—è –≤ –ò–ò: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ –∂–µ—Å—Ç–∫–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#math", "#small_models", "#agents", "#transfer_learning", "#training", "#hallucinations", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ü–µ—Ä–µ–¥–∞—á–∞ –Ω–∞–≤—ã–∫–æ–≤ –∞–≥–µ–Ω—Ç–∞: –æ—Ç –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–∞–ª—ã–º", "desc": "–ú–µ—Ç–æ–¥ Agent Distillation –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –Ω–∞–≤—ã–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –æ—Ç 
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#multimodal"], "emoji": "üß†", "ru": {"title": "PhyX: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò", "desc": "–ù–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç PhyX –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –¢–µ—Å—Ç –≤–∫–ª—é—á–∞–µ—Ç 3000 —Ç—â–∞—Ç–µ–ª—å–Ω–æ 
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#benchmark", "#training", "#long_context"], "emoji": "üß†", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö", "desc": "QwenLong-CPRS - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#video", "#optimization", "#cv", "#inference", "#diffusion", "#training"], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "EvoSearch - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç–µ—Å—Ç-—Ç–∞–π–º —Å–∫–µ–π–ª–∏–Ω–≥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ –ø–æ—Ç–æ–∫–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#video", "#inference", "#diffusion", "#optimization"], "emoji": "üé¨", "ru": {"title": "–£–º–Ω—ã–π –≤—ã–±–æ—Ä —à—É–º–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –≤–∏–¥–µ–æ-—Å–∏–Ω—Ç–µ–∑–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ANSE - –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º –≤—ã–±–æ—Ä–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö —à—É–º–æ–≤—ã—Ö —Å–∏–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –í –æ—Å–Ω–æ
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#science", "#optimization", "#data", "#dataset", "#benchmark"], "emoji": "üß™", "ru": {"title": "–°–∏–º—É–ª—è—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —É–ª—É—á—à–∞–µ—Ç —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–æ—Ç–µ–∑ –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#math", "#optimization", "#inference", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "VeriThinker - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∂–∞—Ç–∏—é —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#hallucinations", "#benchmark", "#security", "#ethics", "#open_source", "#dataset", "#audio"], "emoji": "üéôÔ∏è", "ru": {"title": "AudioTrust: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞—É–¥–∏–æ –ò–ò", "desc": "AudioTrust - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ê—É–¥–∏–æ –ë–æ–ª—å—à–∏—Ö –Ø–∑—ã–∫–æ–≤—ã—Ö –ú–æ–¥–µ–ª–µ–π 
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#3d", "#training", "#optimization", "#diffusion"], "emoji": "üßä", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –Ω–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Direct3D S2 - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é —Ä–∞–∑—Ä–µ
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#games", "#multimodal", "#optimization", "#survey", "#benchmark"], "emoji": "üñ•Ô∏è", "ru": {"title": "FullFront: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ MLLM –≤ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ", "desc": "FullFront - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –æ–±–ª–∞—Å—Ç–∏ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#rlhf", "#training", "#hallucinations"], "emoji": "üîç", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ LLM —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é —Å–∞–º–∏—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#reasoning", "#training", "#interpretability", "#rl", "#rlhf"], "emoji": "üß†", "ru": {"title": "TAPO: –£—Å–∏–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤–Ω–µ—à–Ω–∏–º–∏ –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏", "desc": "TAPO - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç –≤–Ω–µ—à–Ω–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#cv", "#dataset"], "emoji": "üåô", "ru": {"title": "–ï–¥–∏–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –Ω–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Å–ª–æ–∂–Ω—ã—Ö –ø–æ–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –Ω–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç AllWeatherNight —Å
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#hallucinations", "#alignment", "#multilingual"], "emoji": "üß†", "ru": {"title": "–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö: –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –ø—è—Ç–∏ —Ç–∏–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, 
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#rl", "#optimization", "#dataset", "#training", "#benchmark", "#reasoning"], "emoji": "‚è≥", "ru": {"title": "–ú–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–∏–º —á—É–≤—Å—Ç–≤–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Time-R1 - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ò
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#cv", "#dataset", "#benchmark", "#diffusion"], "emoji": "üß†", "ru": {"title": "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–∏—Ñ—Ñ—É–∑–∏–∏: –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å —Å —É—Å–ª–æ–≤–∏—è–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–¥–∞—á–∞—Ö –≤—ã—è–≤–ª—è–µ—Ç –∏—Ö –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ. –û–Ω–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Å
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#rl", "#dataset", "#rlhf", "#optimization", "#training", "#synthetic"], "emoji": "üõ∂", "ru": {"title": "–î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏: CANOE —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "CANOE - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –±–µ–∑
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#low_resource", "#data", "#synthetic", "#training", "#audio"], "emoji": "üó£Ô∏è", "ru": {"title": "–ì–æ–ª–æ—Å–æ–≤—ã–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã –¥–ª—è —Ä–µ–¥–∫–∏—Ö —è–∑—ã–∫–æ–≤: –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ TTS", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#games", "#multimodal", "#benchmark", "#open_source", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –ò–ò: —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –∑—Ä–µ–Ω–∏—è", "desc": "RBench-V - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∑—Ä–µ–Ω–∏—è. –û–Ω –≤–∫
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#rl", "#rlhf", "#agi", "#optimization", "#training"], "emoji": "üß†", "ru": {"title": "Trinity-RFT: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Trinity-RFT - —ç—Ç–æ –≥–∏–±–∫–∞—è –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#healthcare", "#rag", "#rl", "#dataset", "#benchmark", "#reasoning", "#training", "#optimization"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è RAG —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è, –º–æ–¥–µ–ª—å–Ω–æ-–∞–≥–Ω–æ—Å—Ç–∏—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ s3 –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ R
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#security", "#ethics", "#benchmark", "#multimodal"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ú–µ–º—ã vs –ò–ò: –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —É–≥—Ä–æ–∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (VLM) –±–æ–ª–µ–µ —É—è–∑–≤–∏–º—ã –∫ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–º –º–µ–º–∞–º, —á–µ–º –∫ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º 
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#optimization", "#training", "#synthetic"], "emoji": "ü§ñ", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Synthetic Data RL, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#robotics", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "ScanBot: –≤—ã–∑–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ScanBot –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–º—É —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#architecture", "#transfer_learning", "#training"], "emoji": "üöÄ", "ru": {"title": "Transformer Copilot: –£—á–∏–º—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ò–ò", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Transformer Copilot, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#cv", "#rl", "#rag", "#optimization", "#reasoning", "#training"], "emoji": "üé®", "ru": {"title": "–£–º–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "RePrompt - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –æ–±—É—á–µ–Ω–∏–µ 
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#alignment", "#reasoning", "#rl", "#hallucinations", "#agents"], "emoji": "üß†", "ru": {"title": "ReflAct: –†–µ—Ñ–ª–µ–∫—Å–∏—è –¥–ª—è –Ω–∞–¥–µ–∂–Ω—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "ReflAct - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω —É–ª—É—á—à–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#optimization", "#architecture", "#transfer_learning", "#audio"], "emoji": "üß†", "ru": {"title": "–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ–±—Ä–µ—Ç–∞—é—Ç –∑—Ä–µ–Ω–∏–µ –∏ —Å–ª—É—Ö —á–µ—Ä–µ–∑ —á—Ç–µ–Ω–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–ø–æ—Å–æ–±
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#rl", "#rlhf", "#optimization", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º RPG (regularized policy gradient) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#games", "#training", "#optimization", "#rlhf", "#multimodal", "#rl", "#transfer_learning"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏–µ VLA –º–æ–¥–µ–ª–µ–π —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –Ω–∞–¥–∑–æ—Ä–æ–º", "desc": "RIPT-VLA - —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞-–¥–µ–π—Å—Ç–≤–∏—è (VLA) —Å –∏—Å–ø–æ
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#leakage", "#multimodal", "#alignment", "#security", "#dataset", "#benchmark"], "emoji": "üîê", "ru": {"title": "LLM –Ω–∞—Ä—É—à–∞—é—Ç –ø–æ–ª–∏—Ç–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: urgent call –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∑–∞—â–∏—Ç—ã –¥–∞–Ω–Ω—ã—Ö", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM) –∫–æ–Ω—Ç–µ
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#cv", "#games", "#video", "#diffusion", "#robotics", "#dataset", "#benchmark"], "emoji": "üíÉ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ: –æ—Ç —Ö–æ—Ä–µ–æ–≥—Ä–∞—Ñ–∏–∏ –∫ –º–Ω–æ–≥–æ–∞–∫—Ç–µ—Ä–Ω–æ–º—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—é", "desc": "DanceTogether - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –û–Ω–∞ 
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#multimodal", "#rag", "#long_context"], "emoji": "üìù", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ RAG —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫—Ä–∞—Ç–∫–∏—Ö –∑–∞–º–µ—Ç–æ–∫", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ Notes Writing –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ RAG –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã. 
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#inference", "#optimization", "#architecture"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è MoE –º–æ–¥–µ–ª–µ–π: –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é LLM", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization", "#math", "#dataset", "#long_context", "#open_source"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Å—Ç–æ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Ü–µ–Ω–Ω–æ—Å—Ç
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#optimization", "#training", "#rl", "#rlhf", "#reasoning"], "emoji": "üß†", "ru": {"title": "NOVER: –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "NOVER - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–µ—à–Ω–∏—Ö 
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#training", "#translation", "#low_resource", "#multilingual"], "emoji": "üåê", "ru": {"title": "FuxiMT: –ü—Ä–æ—Ä—ã–≤ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–π —è–∑—ã–∫", "desc": "FuxiMT - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–π —è–∑—ã–∫ –∏ –∏—Å–ø–æ–ª
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#dataset", "#survey", "#reasoning", "#benchmark", "#open_source"], "emoji": "‚è≥", "ru": {"title": "TIME: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ TIME –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ë–µ–Ω—á–º–∞
[26.05.2025 19:09] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–û—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–π: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º —Å–≤—è–∑—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–û—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ–µ –û—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ' –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ
[26.05.2025 19:09] Querying the API.
[26.05.2025 19:09] Claude request. Model: claude-3-5-sonnet-20240620. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

RankNovo is a deep reranking framework that enhances de novo peptide sequencing using multiple models and axial attention, achieving superior performance and generalization.  					AI-generated summary 				 De novo peptide sequencing is a critical task in proteomics. However, the performance of current deep learning-based methods is limited by the inherent complexity of mass spectrometry data and the heterogeneous distribution of noise signals, leading to data-specific biases. We present RankNovo, the first deep reranking framework that enhances de novo peptide sequencing by leveraging the complementary strengths of multiple sequencing models. RankNovo employs a list-wise reranking approach, modeling candidate peptides as multiple sequence alignments and utilizing axial attention to extract informative features across candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass Deviation) and RMD (residual Mass Deviation), which offer delicate supervision by quantifying mass differences between peptides at both the sequence and residue levels. Extensive experiments demonstrate that RankNovo not only surpasses its base models used to generate training candidates for reranking pre-training, but also sets a new state-of-the-art benchmark. Moreover, RankNovo exhibits strong zero-shot generalization to unseen models whose generations were not exposed during training, highlighting its robustness and potential as a universal reranking framework for peptide sequencing. Our work presents a novel reranking strategy that fundamentally challenges existing single-model paradigms and advances the frontier of accurate de novo sequencing. Our source code is provided on GitHub.
[26.05.2025 19:09] Response: {
  "desc": "RankNovo - —ç—Ç–æ –≥–ª—É–±–æ–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è de novo —Å–µ–∫–≤–µ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø–µ–ø—Ç–∏–¥–æ–≤. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –∏ –∞–∫—Å–∏–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–µ–Ω–∏—è. RankNovo –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø–æ–¥—Ö–æ–¥ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ø–∏—Å–∫–æ–≤, –º–æ–¥–µ–ª–∏—Ä—É—è –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –ø–µ–ø—Ç–∏–¥–æ–≤ –∫–∞–∫ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –°–∏—Å—Ç–µ–º–∞ –≤–≤–æ–¥–∏—Ç –Ω–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ PMD –∏ RMD –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –º–∞—Å—Å–æ–≤—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –ø–µ–ø—Ç–∏–¥–∞–º–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –æ—Å—Ç–∞—Ç–∫–æ–≤.",
  "emoji": "üß¨",
  "title": "RankNovo: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–µ–∫–≤–µ–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–µ–ø—Ç–∏–¥–æ–≤ —Å –ø–æ–º–æ—â—å—é –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"
}
[26.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RankNovo is a deep reranking framework that enhances de novo peptide sequencing using multiple models and axial attention, achieving superior performance and generalization.  					AI-generated summary 				 De novo peptide sequencing is a critical task in proteomics. However, the performance of current deep learning-based methods is limited by the inherent complexity of mass spectrometry data and the heterogeneous distribution of noise signals, leading to data-specific biases. We present RankNovo, the first deep reranking framework that enhances de novo peptide sequencing by leveraging the complementary strengths of multiple sequencing models. RankNovo employs a list-wise reranking approach, modeling candidate peptides as multiple sequence alignments and utilizing axial attention to extract informative features across candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass Deviation) and RMD (residual Mass Deviation), which offer delicate supervision by quantifying mass differences between peptides at both the sequence and residue levels. Extensive experiments demonstrate that RankNovo not only surpasses its base models used to generate training candidates for reranking pre-training, but also sets a new state-of-the-art benchmark. Moreover, RankNovo exhibits strong zero-shot generalization to unseen models whose generations were not exposed during training, highlighting its robustness and potential as a universal reranking framework for peptide sequencing. Our work presents a novel reranking strategy that fundamentally challenges existing single-model paradigms and advances the frontier of accurate de novo sequencing. Our source code is provided on GitHub."

[26.05.2025 19:09] Response: ```python
['DATASET', 'BENCHMARK', 'TRAINING']
```
[26.05.2025 19:09] OpenAI request. Model: gpt-4o-mini. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"RankNovo is a deep reranking framework that enhances de novo peptide sequencing using multiple models and axial attention, achieving superior performance and generalization.  					AI-generated summary 				 De novo peptide sequencing is a critical task in proteomics. However, the performance of current deep learning-based methods is limited by the inherent complexity of mass spectrometry data and the heterogeneous distribution of noise signals, leading to data-specific biases. We present RankNovo, the first deep reranking framework that enhances de novo peptide sequencing by leveraging the complementary strengths of multiple sequencing models. RankNovo employs a list-wise reranking approach, modeling candidate peptides as multiple sequence alignments and utilizing axial attention to extract informative features across candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass Deviation) and RMD (residual Mass Deviation), which offer delicate supervision by quantifying mass differences between peptides at both the sequence and residue levels. Extensive experiments demonstrate that RankNovo not only surpasses its base models used to generate training candidates for reranking pre-training, but also sets a new state-of-the-art benchmark. Moreover, RankNovo exhibits strong zero-shot generalization to unseen models whose generations were not exposed during training, highlighting its robustness and potential as a universal reranking framework for peptide sequencing. Our work presents a novel reranking strategy that fundamentally challenges existing single-model paradigms and advances the frontier of accurate de novo sequencing. Our source code is provided on GitHub."

[26.05.2025 19:09] Response: ```python
["OPTIMIZATION", "SCIENCE", "OPEN_SOURCE"]
```
[26.05.2025 19:09] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RankNovo is a novel deep reranking framework designed to improve de novo peptide sequencing by integrating multiple models and axial attention mechanisms. It addresses the challenges posed by mass spectrometry data, which often contains complex noise and biases, by employing a list-wise reranking strategy that treats candidate peptides as multiple sequence alignments. The introduction of new metrics, PMD and RMD, allows for more precise supervision by measuring mass differences at both the sequence and residue levels. RankNovo not only outperforms existing models but also demonstrates impressive zero-shot generalization, making it a robust tool for peptide sequencing.","title":"Revolutionizing Peptide Sequencing with RankNovo"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RankNovo is a novel deep reranking framework designed to improve de novo peptide sequencing by integrating multiple models and axial attention mechanisms. It addresses the challenges posed by mass spectrometry data, which often contains complex noise and biases, by employing a list-wise reranking strategy that treats candidate peptides as multiple sequence alignments. The introduction of new metrics, PMD and RMD, allows for more precise supervision by measuring mass differences at both the sequence and residue levels. RankNovo not only outperforms existing models but also demonstrates impressive zero-shot generalization, making it a robust tool for peptide sequencing.', title='Revolutionizing Peptide Sequencing with RankNovo'))
[26.05.2025 19:10] Response: ParsedChatCompletionMessage[Article](content='{"desc":"RankNovo ÊòØ‰∏Ä‰∏™Ê∑±Â∫¶ÈáçÊéíÂ∫èÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂ§öÊ®°ÂûãÂíåËΩ¥ÂêëÊ≥®ÊÑèÂäõÊù•Â¢ûÂº∫ de novo ËÇΩÂ∫èÂàóÁöÑÊµãÂÆö„ÄÇÂΩìÂâçÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÊñπÊ≥ïÂú®Ë¥®Ë∞±Êï∞ÊçÆÁöÑÂ§çÊùÇÊÄßÂíåÂô™Â£∞‰ø°Âè∑ÁöÑÂºÇË¥®ÂàÜÂ∏É‰∏ãË°®Áé∞ÊúâÈôêÔºåÂØºËá¥Êï∞ÊçÆÁâπÂÆöÁöÑÂÅèÂ∑Æ„ÄÇRankNovo ÈááÁî®ÂàóË°®ÈáçÊéíÂ∫èÁöÑÊñπÊ≥ïÔºåÂ∞ÜÂÄôÈÄâËÇΩÂª∫Ê®°‰∏∫Â§ö‰∏™Â∫èÂàóÊØîÂØπÔºåÂπ∂Âà©Áî®ËΩ¥ÂêëÊ≥®ÊÑèÂäõÊèêÂèñÂÄôÈÄâËÇΩ‰πãÈó¥ÁöÑ‰ø°ÊÅØÁâπÂæÅ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏§ÁßçÊñ∞ÊåáÊ†á PMDÔºàËÇΩË¥®ÈáèÂÅèÂ∑ÆÔºâÂíå RMDÔºàÊÆã‰ΩôË¥®ÈáèÂÅèÂ∑ÆÔºâÔºå‰∏∫ËÇΩÂ∫èÂàóÁöÑË¥®ÈáèÂ∑ÆÂºÇÊèê‰æõÁ≤æÁªÜÁöÑÁõëÁù£„ÄÇ","title":"RankNovoÔºöËÇΩÂ∫èÂàóÈáçÊéíÂ∫èÁöÑÊñ∞Á™ÅÁ†¥"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='RankNovo ÊòØ‰∏Ä‰∏™Ê∑±Â∫¶ÈáçÊéíÂ∫èÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂ§öÊ®°ÂûãÂíåËΩ¥ÂêëÊ≥®ÊÑèÂäõÊù•Â¢ûÂº∫ de novo ËÇΩÂ∫èÂàóÁöÑÊµãÂÆö„ÄÇÂΩìÂâçÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÊñπÊ≥ïÂú®Ë¥®Ë∞±Êï∞ÊçÆÁöÑÂ§çÊùÇÊÄßÂíåÂô™Â£∞‰ø°Âè∑ÁöÑÂºÇË¥®ÂàÜÂ∏É‰∏ãË°®Áé∞ÊúâÈôêÔºåÂØºËá¥Êï∞ÊçÆÁâπÂÆöÁöÑÂÅèÂ∑Æ„ÄÇRankNovo ÈááÁî®ÂàóË°®ÈáçÊéíÂ∫èÁöÑÊñπÊ≥ïÔºåÂ∞ÜÂÄôÈÄâËÇΩÂª∫Ê®°‰∏∫Â§ö‰∏™Â∫èÂàóÊØîÂØπÔºåÂπ∂Âà©Áî®ËΩ¥ÂêëÊ≥®ÊÑèÂäõÊèêÂèñÂÄôÈÄâËÇΩ‰πãÈó¥ÁöÑ‰ø°ÊÅØÁâπÂæÅ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏§ÁßçÊñ∞ÊåáÊ†á PMDÔºàËÇΩË¥®ÈáèÂÅèÂ∑ÆÔºâÂíå RMDÔºàÊÆã‰ΩôË¥®ÈáèÂÅèÂ∑ÆÔºâÔºå‰∏∫ËÇΩÂ∫èÂàóÁöÑË¥®ÈáèÂ∑ÆÂºÇÊèê‰æõÁ≤æÁªÜÁöÑÁõëÁù£„ÄÇ', title='RankNovoÔºöËÇΩÂ∫èÂàóÈáçÊéíÂ∫èÁöÑÊñ∞Á™ÅÁ†¥'))
[26.05.2025 19:10] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#reasoning"], "emoji": "üß†", "ru": {"title": "FREESON: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –µ–¥–∏–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "FREESON - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π 
[26.05.2025 19:10] Loading Chinese text from previous data.
[26.05.2025 19:10] Renaming data file.
[26.05.2025 19:10] Renaming previous data. hf_papers.json to ./d/2025-05-26.json
[26.05.2025 19:10] Saving new data file.
[26.05.2025 19:10] Generating page.
[26.05.2025 19:10] Renaming previous page.
[26.05.2025 19:10] Renaming previous data. index.html to ./d/2025-05-26.html
[26.05.2025 19:10] [Experimental] Generating Chinese page for reading.
[26.05.2025 19:10] Chinese vocab [{'word': 'ËøÅÁßªÂ≠¶‰π†', 'pinyin': 'qiƒÅn y√≠ xu√© x√≠', 'trans': 'transfer learning'}, {'word': 'ÂàÜÁ±ª‰ªªÂä°', 'pinyin': 'fƒìn l√®i r√®n w√π', 'trans': 'classification task'}, {'word': 'ÂèÇÊï∞', 'pinyin': 'cƒÅn sh«î', 'trans': 'parameter'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pre-training'}, {'word': 'ÊñáÊú¨ÁºñÁ†ÅÂô®', 'pinyin': 'w√©n bƒõn biƒÅn m«é q√¨', 'trans': 'text encoder'}, {'word': 'ÁõÆÊ†á‰ª§Áâå', 'pinyin': 'm√π biƒÅo l√¨ng p√°i', 'trans': 'target token'}, {'word': 'ÂµåÂÖ•', 'pinyin': 'qi√†n r√π', 'trans': 'embedding'}, {'word': 'Ë°®Áé∞Âá∫Ëâ≤', 'pinyin': 'bi«éo xi√†n ch≈´ s√®', 'trans': 'perform excellently'}, {'word': 'Êâ©Â±ïËßÑÂæã', 'pinyin': 'ku√≤ zh«én guƒ´ l«ú', 'trans': 'expansion pattern'}]
[26.05.2025 19:10] Renaming previous Chinese page.
[26.05.2025 19:10] Renaming previous data. zh.html to ./d/2025-05-25_zh_reading_task.html
[26.05.2025 19:10] Writing Chinese reading task.
[26.05.2025 19:10] Writing result.
[26.05.2025 19:10] Renaming log file.
[26.05.2025 19:10] Renaming previous data. log.txt to ./logs/2025-05-26_last_log.txt
