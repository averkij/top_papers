[26.05.2025 22:10] Read previous papers.
[26.05.2025 22:10] Generating top page (month).
[26.05.2025 22:10] Writing top page (month).
[26.05.2025 23:10] Read previous papers.
[26.05.2025 23:10] Get feed.
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18125
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17667
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14669
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18129
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17225
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17612
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15929
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18092
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17618
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17873
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17561
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17941
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16211
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17558
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17412
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17399
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17955
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15692
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16479
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16134
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.13508
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16483
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17417
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17295
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16770
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14146
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17826
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15389
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17063
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16270
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17540
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17508
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15182
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17091
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17016
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.18078
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.15805
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17373
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16293
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16056
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16409
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.16022
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.14256
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.12891
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.11881
[26.05.2025 23:10] Get page data from previous paper. URL: https://huggingface.co/papers/2505.17552
[26.05.2025 23:10] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[26.05.2025 23:10] No deleted papers detected.
[26.05.2025 23:10] Downloading and parsing papers (pdf, html). Total: 46.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.18125.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.18125.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.18125.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17667.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17667.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17667.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.14669.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.14669.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.14669.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.18129.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.18129.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.18129.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17225.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17225.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17225.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17612.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17612.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17612.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.15929.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.15929.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.15929.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.18092.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.18092.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.18092.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17618.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17618.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17618.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17873.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17873.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17873.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17561.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17561.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17561.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17941.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17941.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17941.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.16211.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.16211.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.16211.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17558.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17558.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17558.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17412.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17412.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17412.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17399.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17399.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17399.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17955.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17955.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17955.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.15692.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.15692.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.15692.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.16479.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.16479.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.16479.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.16134.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.16134.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.16134.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.13508.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.13508.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.13508.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.16483.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.16483.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.16483.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17417.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17417.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17417.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17295.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17295.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17295.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.16770.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.16770.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.16770.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.14146.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.14146.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.14146.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17826.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17826.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17826.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.15389.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.15389.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.15389.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17063.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17063.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17063.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.16270.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.16270.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.16270.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17540.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17540.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17540.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17508.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17508.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17508.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.15182.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.15182.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.15182.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17091.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17091.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17091.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17016.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17016.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17016.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.18078.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.18078.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.18078.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.15805.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.15805.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.15805.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17373.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17373.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17373.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.16293.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.16293.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.16293.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.16056.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.16056.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.16056.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.16409.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.16409.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.16409.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.16022.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.16022.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.16022.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.14256.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.14256.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.14256.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.12891.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.12891.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.12891.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.11881.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.11881.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.11881.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Downloading and parsing paper https://huggingface.co/papers/2505.17552.
[26.05.2025 23:10] Extra JSON file exists (./assets/json/2505.17552.json), skip PDF parsing.
[26.05.2025 23:10] Paper image links file exists (./assets/img_data/2505.17552.json), skip HTML parsing.
[26.05.2025 23:10] Success.
[26.05.2025 23:10] Enriching papers with extra data.
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 0. TabSTAR, a tabular foundation model with semantically target-aware representations, achieves state-of-the-art performance in classification tasks with text features through transfer learning without dataset-specific parameters.  					AI-generated summary 				 While deep learning has achieved remarka...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 1. A framework called QwenLong-L1 enhances large reasoning models for long-context reasoning through reinforcement learning, achieving leading performance on document question-answering benchmarks.  					AI-generated summary 				 Recent large reasoning models (LRMs) have demonstrated strong reasoning c...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 2. Quartet, a hardware-supported FP4 training approach for large language models, demonstrates state-of-the-art accuracy while significantly reducing computational costs compared to standard or FP8 precision.  					AI-generated summary 				 The rapid advancement of large language models (LLMs) has been...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 3. A unified reinforcement learning system, V-Triune, combines visual reasoning and perception tasks in vision-language models through a single training pipeline, achieving significant improvements across various tasks.  					AI-generated summary 				 Reinforcement learning (RL) has significantly advan...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 4. A diagnostic set examines and categorizes reasoning rigidity in large language models, identifying patterns where models ignore instructions and default to familiar reasoning.  					AI-generated summary 				 Large language models have demonstrated remarkable proficiency in long and complex reasoning...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 5. Agent Distillation transfers reasoning and task-solving capabilities from large language models to smaller models using enhanced prompts and self-consistent actions, matching performance of larger models on various reasoning tasks.  					AI-generated summary 				 Large language models (LLMs) excel a...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 6. A new benchmark, PhyX, evaluates models' physics-grounded reasoning in visual scenarios, revealing significant limitations in current models' physical understanding compared to human experts.  					AI-generated summary 				 Existing benchmarks fail to capture a crucial aspect of intelligence: physic...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 7. QwenLong-CPRS enhances large language models with multi-granularity context compression, dynamic optimization guided by natural language, and efficient bidirectional reasoning and parallel inference, achieving superior performance and context management.  					AI-generated summary 				 This technica...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 8. EvoSearch, an evolutionary search method, enhances test-time scaling for diffusion and flow-based generative models, improving image and video generation quality, diversity, and generalizability.  					AI-generated summary 				 As the marginal cost of scaling computation (data and parameters) during...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 9. A novel simulator and experiment-guided ranking method improve hypothesis prioritization in scientific discovery by incorporating simulated experimental outcomes.  					AI-generated summary 				 Hypothesis ranking is a crucial component of automated scientific discovery, particularly in natural scie...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 10. ANSE enhances video diffusion models by selecting noise seeds based on model confidence, improving video quality and temporal coherence with minimal increase in inference time.  					AI-generated summary 				 The choice of initial noise significantly affects the quality and prompt alignment of video...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 11. VeriThinker reduces the length of complex reasoning chains in Large Reasoning Models (LRMs) by fine-tuning them on a verification task, thereby decreasing inference costs without significantly sacrificing accuracy.  					AI-generated summary 				 Large Reasoning Models (LRMs) excel at complex tasks ...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 12. AudioTrust evaluates the trustworthiness of Audio Large Language Models across multifaceted dimensions, using a comprehensive dataset and specific metrics to assess their performance in real-world audio scenarios.  					AI-generated summary 				 The rapid advancement and expanding applications of Au...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 13. The use of carefully crafted hallucinations in a curriculum learning approach within the DPO alignment procedure significantly enhances LLMs' hallucination detection abilities.  					AI-generated summary 				 Aligning large language models (LLMs) to accurately detect hallucinations remains a signifi...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 14. A scalable 3D shape generation framework using sparse volumes and spatial sparse attention, enabling high-resolution generation with reduced computational requirements.  					AI-generated summary 				 Generating high resolution 3D shapes using volumetric representations such as Signed Distance Funct...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 15. FullFront is a benchmark evaluating Multimodal Large Language Models across conceptualization, comprehension, and implementation phases in front-end engineering.  					AI-generated summary 				 Front-end engineering involves a complex workflow where engineers conceptualize designs, translate them in...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 16. A study of diffusion classifiers across multiple datasets and tasks reveals their compositional understanding, highlighting domain-specific performance effects and timestep weighting importance.  					AI-generated summary 				 Understanding visual scenes is fundamental to human intelligence. While d...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 17. A novel RL framework, TAPO, integrates external guidance to enhance model performance and exploration compared to existing methods.  					AI-generated summary 				 Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically ...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 18. A unified framework for restoring nighttime images under diverse weather conditions using dual priors and adaptive collaboration.  					AI-generated summary 				 Restoring nighttime images affected by multiple adverse weather conditions is a practical yet under-explored research problem, as multiple...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 19. Large language models exhibit positional bias -- systematic neglect of information at specific context positions -- yet its interplay with linguistic diversity remains poorly understood. We present a cross-linguistic study across five typologically distinct languages (English, Russian, German, Hindi...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 20. A novel framework, Time-R1, enhances moderate-sized LLMs with comprehensive temporal abilities through a reinforcement learning curriculum, outperforming larger models on future event prediction and creative scenario generation benchmarks.  					AI-generated summary 				 Large Language Models (LLMs)...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 21. CANOE improves LLM faithfulness in generation tasks using synthetic QA data and Dual-GRPO reinforcement learning without human annotations.  					AI-generated summary 				 Teaching large language models (LLMs) to be faithful in the provided context is crucial for building reliable information-seekin...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 22. The rapid growth of voice assistants powered by large language models (LLM) has highlighted a need for speech instruction data to train these systems. Despite the abundance of speech recognition data, there is a notable scarcity of speech instruction data, which is essential for fine-tuning models t...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 23. The ScanBot dataset, focusing on instruction-conditioned high-precision robotic surface scanning, showcases challenges for vision-language action models in achieving precise scanning trajectories under real-world constraints.  					AI-generated summary 				 We introduce ScanBot, a novel dataset desi...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 24. A benchmark called RBench-V evaluates multi-modal models' vision-indispensable reasoning through image manipulation and auxiliary line construction, demonstrating that current models struggle with multi-modal outputs.  					AI-generated summary 				 The rapid advancement of native multi-modal models...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 25. A lightweight, model-agnostic framework decouples the retrieval and generation processes in RAG systems, enhancing performance with minimal training data.  					AI-generated summary 				 Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge du...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 26. Trinity-RFT is a flexible and scalable framework for reinforcement fine-tuning of large language models, supporting various interaction modes and data pipelines.  					AI-generated summary 				 Trinity-RFT is a general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 27. VLMs are more vulnerable to harmful meme-based prompts than to synthetic images, and while multi-turn interactions offer some protection, significant vulnerabilities remain.  					AI-generated summary 				 Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet most evaluations...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 28. Synthetic Data RL enhances foundation models through reinforcement learning using only synthetic data, achieving performance comparable to models trained with full human-labeled data.  					AI-generated summary 				 Reinforcement learning (RL) is a powerful way to adapt foundation models to speciali...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 29. The Transformer Copilot framework enhances large language model performance through a Copilot model that refines the Pilot's logits based on a Mistake Log, leading to consistent performance improvements across various benchmarks.  					AI-generated summary 				 Large language models are typically ad...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 30. RePrompt, a reprompting framework using reinforcement learning, enhances text-to-image generation by optimizing for image-level outcomes, significantly improving spatial layout and compositional generalization.  					AI-generated summary 				 Despite recent progress in text-to-image (T2I) generation...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 31. A regularized policy gradient framework is introduced to explore KL divergence formulations for enhancing the reasoning capabilities of LLMs in online reinforcement learning, demonstrating improved training stability and performance.  					AI-generated summary 				 Policy gradient algorithms have be...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 32. ReflAct, a new reasoning backbone for LLM agents, improves goal alignment and reduces hallucinations by continuously reflecting on the agent's state, surpassing ReAct and other enhanced variants.  					AI-generated summary 				 Recent advances in LLM agents have largely built on reasoning backbones ...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 33. Auto-regressive text LLMs trained on text can develop internal capabilities for understanding images and audio, enabling them to perform classification tasks across different modalities without fine-tuning.  					AI-generated summary 				 This paper presents a fascinating find: By training an auto-r...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 34. We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and ...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 35. DanceTogether, an end-to-end diffusion framework, generates long, photorealistic multi-actor interaction videos from single reference images and pose-mask streams, outperforming existing systems.  					AI-generated summary 				 Controllable video generation (CVG) has advanced rapidly, yet current sy...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 36. LLMs frequently violate contextual security policies by leaking sensitive information, particularly under indirect attacks, indicating a critical gap in current safety mechanisms.  					AI-generated summary 				 As Large Language Models (LLMs) are increasingly deployed in sensitive domains such as e...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 37. A simple and efficient method for value model training on long-context reasoning traces improves test-time performance and reduces computational cost compared to existing methods.  					AI-generated summary 				 In this paper, we propose a simple and efficient method for value model training on long...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 38. Notes Writing enhances iterative RAG by generating concise notes at each step, improving reasoning and performance while minimizing output increase.  					AI-generated summary 				 Iterative RAG for multi-hop question answering faces challenges with lengthy contexts and the buildup of irrelevant inf...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 39. MoE models achieve efficient scaling in LLMs with expert offloading, emphasizing the importance of local routing consistency and cache effectiveness.  					AI-generated summary 				 Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts dur...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 40. FREESON, a novel framework that integrates retrieval and reasoning roles within LRMs using CT-MCTS, improves the performance of multistep reasoning models in QA tasks by reducing representation bottlenecks.  					AI-generated summary 				 Large Reasoning Models (LRMs) have demonstrated remarkable ca...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 41. NOVER, a reinforcement learning framework that eliminates the need for external verifiers, enhances language model performance across text-to-text tasks.  					AI-generated summary 				 Recent advances such as DeepSeek R1-Zero highlight the effectiveness of incentive training, a reinforcement learni...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 42. FuxiMT, a Chinese-centric multilingual machine translation model utilizing a sparsified large language model, demonstrates superior performance in low-resource scenarios and strong zero-shot capabilities across 65 languages.  					AI-generated summary 				 In this paper, we present FuxiMT, a novel C...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 43. A benchmark called TIME assesses temporal reasoning in LLMs across varied real-world challenges, including intensive temporal information, fast-changing event dynamics, and complex social interactions, and evaluates the impact of test-time scaling.  					AI-generated summary 				 Temporal reasoning ...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 44. Orthogonal Residual Updates enhance feature learning and training stability by decomposing module outputs to contribute primarily novel features.  					AI-generated summary 				 Residual connections are pivotal for deep neural networks, enabling greater depth by mitigating vanishing gradients. Howev...
[26.05.2025 23:10] ********************************************************************************
[26.05.2025 23:10] Abstract 45. RankNovo is a deep reranking framework that enhances de novo peptide sequencing using multiple models and axial attention, achieving superior performance and generalization.  					AI-generated summary 				 De novo peptide sequencing is a critical task in proteomics. However, the performance of curre...
[26.05.2025 23:10] Read previous papers.
[26.05.2025 23:10] Generating reviews via LLM API.
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#optimization", "#dataset", "#transfer_learning", "#architecture", "#benchmark", "#training"], "emoji": "üìä", "ru": {"title": "TabSTAR: –£–º–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "TabSTAR - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#training", "#long_context", "#optimization", "#benchmark", "#rl", "#reasoning"], "emoji": "üß†", "ru": {"title": "QwenLong-L1: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è", "desc": "QwenLong-L1 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, —É–ª—É—á—à–∞—é—â–∏–π –º–æ–¥–µ–ª–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (LRM) –¥
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#architecture", "#training", "#inference", "#optimization"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é 4-–±–∏—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Quartet - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º 4-–±–∏—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ 
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#rl", "#dataset", "#multimodal", "#optimization", "#training", "#open_source", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ò–ò", "desc": "V-Triune - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –∑–∞–¥–∞—á–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#math", "#dataset", "#data", "#interpretability", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∂–µ—Å—Ç–∫–æ—Å—Ç–∏ –º—ã—à–ª–µ–Ω–∏—è –≤ –ò–ò: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ –∂–µ—Å—Ç–∫–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#math", "#small_models", "#agents", "#transfer_learning", "#training", "#hallucinations", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ü–µ—Ä–µ–¥–∞—á–∞ –Ω–∞–≤—ã–∫–æ–≤ –∞–≥–µ–Ω—Ç–∞: –æ—Ç –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–∞–ª—ã–º", "desc": "–ú–µ—Ç–æ–¥ Agent Distillation –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –Ω–∞–≤—ã–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –æ—Ç 
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#reasoning", "#benchmark", "#multimodal"], "emoji": "üß†", "ru": {"title": "PhyX: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –ò–ò", "desc": "–ù–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ—Å—Ç PhyX –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –¢–µ—Å—Ç –≤–∫–ª—é—á–∞–µ—Ç 3000 —Ç—â–∞—Ç–µ–ª—å–Ω–æ 
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#optimization", "#architecture", "#benchmark", "#training", "#long_context"], "emoji": "üß†", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö", "desc": "QwenLong-CPRS - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#video", "#optimization", "#cv", "#inference", "#diffusion", "#training"], "emoji": "üß¨", "ru": {"title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "EvoSearch - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ç–µ—Å—Ç-—Ç–∞–π–º —Å–∫–µ–π–ª–∏–Ω–≥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ –ø–æ—Ç–æ–∫–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#science", "#optimization", "#data", "#dataset", "#benchmark"], "emoji": "üß™", "ru": {"title": "–°–∏–º—É–ª—è—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —É–ª—É—á—à–∞–µ—Ç —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–æ—Ç–µ–∑ –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#video", "#inference", "#diffusion", "#optimization"], "emoji": "üé¨", "ru": {"title": "–£–º–Ω—ã–π –≤—ã–±–æ—Ä —à—É–º–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –≤–∏–¥–µ–æ-—Å–∏–Ω—Ç–µ–∑–∞", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ANSE - –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º –≤—ã–±–æ—Ä–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö —à—É–º–æ–≤—ã—Ö —Å–∏–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –í –æ—Å–Ω–æ
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#math", "#optimization", "#inference", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏", "desc": "VeriThinker - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∂–∞—Ç–∏—é —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#hallucinations", "#benchmark", "#security", "#ethics", "#open_source", "#dataset", "#audio"], "emoji": "üéôÔ∏è", "ru": {"title": "AudioTrust: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞—É–¥–∏–æ –ò–ò", "desc": "AudioTrust - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –º–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ê—É–¥–∏–æ –ë–æ–ª—å—à–∏—Ö –Ø–∑—ã–∫–æ–≤—ã—Ö –ú–æ–¥–µ–ª–µ–π 
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#alignment", "#benchmark", "#rlhf", "#training", "#hallucinations"], "emoji": "üîç", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ LLM —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é —Å–∞–º–∏—Ö –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#dataset", "#3d", "#training", "#optimization", "#diffusion"], "emoji": "üßä", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –Ω–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Direct3D S2 - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é —Ä–∞–∑—Ä–µ
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#games", "#multimodal", "#optimization", "#survey", "#benchmark"], "emoji": "üñ•Ô∏è", "ru": {"title": "FullFront: –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ MLLM –≤ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ", "desc": "FullFront - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –æ–±–ª–∞—Å—Ç–∏ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#cv", "#dataset", "#benchmark", "#diffusion"], "emoji": "üß†", "ru": {"title": "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–∏—Ñ—Ñ—É–∑–∏–∏: –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å —Å —É—Å–ª–æ–≤–∏—è–º–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–¥–∞—á–∞—Ö –≤—ã—è–≤–ª—è–µ—Ç –∏—Ö –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ. –û–Ω–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Å
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#reasoning", "#training", "#interpretability", "#rl", "#rlhf"], "emoji": "üß†", "ru": {"title": "TAPO: –£—Å–∏–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤–Ω–µ—à–Ω–∏–º–∏ –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏", "desc": "TAPO - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç –≤–Ω–µ—à–Ω–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#cv", "#dataset"], "emoji": "üåô", "ru": {"title": "–ï–¥–∏–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –Ω–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Å–ª–æ–∂–Ω—ã—Ö –ø–æ–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –Ω–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç AllWeatherNight —Å
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#hallucinations", "#alignment", "#multilingual"], "emoji": "üß†", "ru": {"title": "–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö: –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –ø—è—Ç–∏ —Ç–∏–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, 
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#rl", "#optimization", "#dataset", "#training", "#benchmark", "#reasoning"], "emoji": "‚è≥", "ru": {"title": "–ú–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–∏–º —á—É–≤—Å—Ç–≤–æ–º –≤—Ä–µ–º–µ–Ω–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Time-R1 - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ò
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#rl", "#dataset", "#rlhf", "#optimization", "#training", "#synthetic"], "emoji": "üõ∂", "ru": {"title": "–î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏: CANOE —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏", "desc": "CANOE - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –±–µ–∑
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#multilingual", "#dataset", "#low_resource", "#data", "#synthetic", "#training", "#audio"], "emoji": "üó£Ô∏è", "ru": {"title": "–ì–æ–ª–æ—Å–æ–≤—ã–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã –¥–ª—è —Ä–µ–¥–∫–∏—Ö —è–∑—ã–∫–æ–≤: –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ TTS", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#robotics", "#multimodal"], "emoji": "ü§ñ", "ru": {"title": "ScanBot: –≤—ã–∑–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–π —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ScanBot –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–º—É —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#games", "#multimodal", "#benchmark", "#open_source", "#reasoning"], "emoji": "üß†", "ru": {"title": "–ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –ò–ò: —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –∑—Ä–µ–Ω–∏—è", "desc": "RBench-V - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∑—Ä–µ–Ω–∏—è. –û–Ω –≤–∫
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#healthcare", "#rag", "#rl", "#dataset", "#benchmark", "#reasoning", "#training", "#optimization"], "emoji": "üîç", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è RAG —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º", "desc": "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è, –º–æ–¥–µ–ª—å–Ω–æ-–∞–≥–Ω–æ—Å—Ç–∏—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ s3 –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ R
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#rl", "#rlhf", "#agi", "#optimization", "#training"], "emoji": "üß†", "ru": {"title": "Trinity-RFT: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "Trinity-RFT - —ç—Ç–æ –≥–∏–±–∫–∞—è –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#security", "#ethics", "#benchmark", "#multimodal"], "emoji": "üõ°Ô∏è", "ru": {"title": "–ú–µ–º—ã vs –ò–ò: –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —É–≥—Ä–æ–∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (VLM) –±–æ–ª–µ–µ —É—è–∑–≤–∏–º—ã –∫ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–º –º–µ–º–∞–º, —á–µ–º –∫ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º 
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#rlhf", "#rl", "#optimization", "#training", "#synthetic"], "emoji": "ü§ñ", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Synthetic Data RL, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#benchmark", "#optimization", "#architecture", "#transfer_learning", "#training"], "emoji": "üöÄ", "ru": {"title": "Transformer Copilot: –£—á–∏–º—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ò–ò", "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Transformer Copilot, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#cv", "#rl", "#rag", "#optimization", "#reasoning", "#training"], "emoji": "üé®", "ru": {"title": "–£–º–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "RePrompt - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –æ–±—É—á–µ–Ω–∏–µ 
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#rl", "#rlhf", "#optimization", "#training", "#reasoning"], "emoji": "üß†", "ru": {"title": "–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º RPG (regularized policy gradient) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#alignment", "#reasoning", "#rl", "#hallucinations", "#agents"], "emoji": "üß†", "ru": {"title": "ReflAct: –†–µ—Ñ–ª–µ–∫—Å–∏—è –¥–ª—è –Ω–∞–¥–µ–∂–Ω—ã—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤", "desc": "ReflAct - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω —É–ª—É—á—à–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#optimization", "#architecture", "#transfer_learning", "#audio"], "emoji": "üß†", "ru": {"title": "–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ–±—Ä–µ—Ç–∞—é—Ç –∑—Ä–µ–Ω–∏–µ –∏ —Å–ª—É—Ö —á–µ—Ä–µ–∑ —á—Ç–µ–Ω–∏–µ", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–ø–æ—Å–æ–±
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#games", "#training", "#optimization", "#rlhf", "#multimodal", "#rl", "#transfer_learning"], "emoji": "ü§ñ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏–µ VLA –º–æ–¥–µ–ª–µ–π —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –Ω–∞–¥–∑–æ—Ä–æ–º", "desc": "RIPT-VLA - —ç—Ç–æ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞-–¥–µ–π—Å—Ç–≤–∏—è (VLA) —Å –∏—Å–ø–æ
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#cv", "#games", "#video", "#diffusion", "#robotics", "#dataset", "#benchmark"], "emoji": "üíÉ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ: –æ—Ç —Ö–æ—Ä–µ–æ–≥—Ä–∞—Ñ–∏–∏ –∫ –º–Ω–æ–≥–æ–∞–∫—Ç–µ—Ä–Ω–æ–º—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—é", "desc": "DanceTogether - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –û–Ω–∞ 
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#leakage", "#multimodal", "#alignment", "#security", "#dataset", "#benchmark"], "emoji": "üîê", "ru": {"title": "LLM –Ω–∞—Ä—É—à–∞—é—Ç –ø–æ–ª–∏—Ç–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: urgent call –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∑–∞—â–∏—Ç—ã –¥–∞–Ω–Ω—ã—Ö", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM) –∫–æ–Ω—Ç–µ
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#inference", "#training", "#optimization", "#math", "#dataset", "#long_context", "#open_source"], "emoji": "üß†", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò", "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –ø—Ä–æ—Å—Ç–æ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Ü–µ–Ω–Ω–æ—Å—Ç
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#optimization", "#reasoning", "#multimodal", "#rag", "#long_context"], "emoji": "üìù", "ru": {"title": "–£—Å–∏–ª–µ–Ω–∏–µ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ RAG —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫—Ä–∞—Ç–∫–∏—Ö –∑–∞–º–µ—Ç–æ–∫", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ Notes Writing –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ RAG –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã. 
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#training", "#inference", "#optimization", "#architecture"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è MoE –º–æ–¥–µ–ª–µ–π: –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é LLM", "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#multimodal", "#rag", "#reasoning"], "emoji": "üß†", "ru": {"title": "FREESON: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –µ–¥–∏–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º", "desc": "FREESON - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π 
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#optimization", "#training", "#rl", "#rlhf", "#reasoning"], "emoji": "üß†", "ru": {"title": "NOVER: –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "NOVER - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–µ—à–Ω–∏—Ö 
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#training", "#machine_translation", "#low_resource", "#multilingual"], "emoji": "üåê", "ru": {"title": "FuxiMT: –ü—Ä–æ—Ä—ã–≤ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–π —è–∑—ã–∫", "desc": "FuxiMT - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–π —è–∑—ã–∫
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#dataset", "#survey", "#reasoning", "#benchmark", "#open_source"], "emoji": "‚è≥", "ru": {"title": "TIME: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ LLM", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ TIME –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ë–µ–Ω—á–º–∞
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#architecture", "#training", "#optimization"], "emoji": "üß†", "ru": {"title": "–û—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–π: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º —Å–≤—è–∑—è–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º '–û—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ–µ –û—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ' –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ
[26.05.2025 23:10] Using data from previous issue: {"categories": ["#open_source", "#optimization", "#benchmark", "#science", "#training", "#dataset"], "emoji": "üß¨", "ru": {"title": "RankNovo: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–µ–∫–≤–µ–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–µ–ø—Ç–∏–¥–æ–≤ —Å –ø–æ–º–æ—â—å—é –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è", "desc": "RankNovo - —ç—Ç–æ –≥–ª—É–±–æ–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è de novo —Å–µ–∫–≤–µ–Ω–∏—Ä–æ–≤–∞
[26.05.2025 23:10] Loading Chinese text from previous data.
[26.05.2025 23:10] Renaming data file.
[26.05.2025 23:10] Renaming previous data. hf_papers.json to ./d/2025-05-26.json
[26.05.2025 23:10] Saving new data file.
[26.05.2025 23:10] Generating page.
[26.05.2025 23:10] Renaming previous page.
[26.05.2025 23:10] Renaming previous data. index.html to ./d/2025-05-26.html
[26.05.2025 23:10] [Experimental] Generating Chinese page for reading.
[26.05.2025 23:10] Chinese vocab [{'word': 'ËøÅÁßªÂ≠¶‰π†', 'pinyin': 'qiƒÅn y√≠ xu√© x√≠', 'trans': 'transfer learning'}, {'word': 'ÂàÜÁ±ª‰ªªÂä°', 'pinyin': 'fƒìn l√®i r√®n w√π', 'trans': 'classification task'}, {'word': 'ÂèÇÊï∞', 'pinyin': 'cƒÅn sh«î', 'trans': 'parameter'}, {'word': 'È¢ÑËÆ≠ÁªÉ', 'pinyin': 'y√π x√πn li√†n', 'trans': 'pre-training'}, {'word': 'ÊñáÊú¨ÁºñÁ†ÅÂô®', 'pinyin': 'w√©n bƒõn biƒÅn m«é q√¨', 'trans': 'text encoder'}, {'word': 'ÁõÆÊ†á‰ª§Áâå', 'pinyin': 'm√π biƒÅo l√¨ng p√°i', 'trans': 'target token'}, {'word': 'ÂµåÂÖ•', 'pinyin': 'qi√†n r√π', 'trans': 'embedding'}, {'word': 'Ë°®Áé∞Âá∫Ëâ≤', 'pinyin': 'bi«éo xi√†n ch≈´ s√®', 'trans': 'perform excellently'}, {'word': 'Êâ©Â±ïËßÑÂæã', 'pinyin': 'ku√≤ zh«én guƒ´ l«ú', 'trans': 'expansion pattern'}]
[26.05.2025 23:10] Renaming previous Chinese page.
[26.05.2025 23:10] Renaming previous data. zh.html to ./d/2025-05-25_zh_reading_task.html
[26.05.2025 23:10] Writing Chinese reading task.
[26.05.2025 23:10] Writing result.
[26.05.2025 23:10] Renaming log file.
[26.05.2025 23:10] Renaming previous data. log.txt to ./logs/2025-05-26_last_log.txt
