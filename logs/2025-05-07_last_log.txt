[07.05.2025 05:13] Read previous papers.
[07.05.2025 05:13] Generating top page (month).
[07.05.2025 05:13] Writing top page (month).
[07.05.2025 06:16] Read previous papers.
[07.05.2025 06:16] Get feed.
[07.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03318
[07.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03335
[07.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03730
[07.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03005
[07.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02922
[07.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02214
[07.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03735
[07.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.03164
[07.05.2025 06:16] Get page data from previous paper. URL: https://huggingface.co/papers/2505.02311
[07.05.2025 06:16] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[07.05.2025 06:16] No deleted papers detected.
[07.05.2025 06:16] Downloading and parsing papers (pdf, html). Total: 9.
[07.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.03318.
[07.05.2025 06:16] Extra JSON file exists (./assets/json/2505.03318.json), skip PDF parsing.
[07.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.03318.json), skip HTML parsing.
[07.05.2025 06:16] Success.
[07.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.03335.
[07.05.2025 06:16] Downloading paper 2505.03335 from http://arxiv.org/pdf/2505.03335v1...
[07.05.2025 06:16] Failed to download and parse paper https://huggingface.co/papers/2505.03335: 'LTChar' object is not iterable
[07.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.03730.
[07.05.2025 06:16] Extra JSON file exists (./assets/json/2505.03730.json), skip PDF parsing.
[07.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.03730.json), skip HTML parsing.
[07.05.2025 06:16] Success.
[07.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.03005.
[07.05.2025 06:16] Extra JSON file exists (./assets/json/2505.03005.json), skip PDF parsing.
[07.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.03005.json), skip HTML parsing.
[07.05.2025 06:16] Success.
[07.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.02922.
[07.05.2025 06:16] Extra JSON file exists (./assets/json/2505.02922.json), skip PDF parsing.
[07.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.02922.json), skip HTML parsing.
[07.05.2025 06:16] Success.
[07.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.02214.
[07.05.2025 06:16] Extra JSON file exists (./assets/json/2505.02214.json), skip PDF parsing.
[07.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.02214.json), skip HTML parsing.
[07.05.2025 06:16] Success.
[07.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.03735.
[07.05.2025 06:16] Extra JSON file exists (./assets/json/2505.03735.json), skip PDF parsing.
[07.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.03735.json), skip HTML parsing.
[07.05.2025 06:16] Success.
[07.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.03164.
[07.05.2025 06:16] Extra JSON file exists (./assets/json/2505.03164.json), skip PDF parsing.
[07.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.03164.json), skip HTML parsing.
[07.05.2025 06:16] Success.
[07.05.2025 06:16] Downloading and parsing paper https://huggingface.co/papers/2505.02311.
[07.05.2025 06:16] Extra JSON file exists (./assets/json/2505.02311.json), skip PDF parsing.
[07.05.2025 06:16] Paper image links file exists (./assets/img_data/2505.02311.json), skip HTML parsing.
[07.05.2025 06:16] Success.
[07.05.2025 06:16] Enriching papers with extra data.
[07.05.2025 06:16] ********************************************************************************
[07.05.2025 06:16] Abstract 0. Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, ofte...
[07.05.2025 06:16] ********************************************************************************
[07.05.2025 06:16] Abstract 1. Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but sti...
[07.05.2025 06:16] ********************************************************************************
[07.05.2025 06:16] Abstract 2. Action customization involves generating videos where the subject performs actions dictated by input control signals. Current methods use pose-guided or global motion customization but are limited by strict constraints on spatial structure, such as layout, skeleton, and viewpoint consistency, reduci...
[07.05.2025 06:16] ********************************************************************************
[07.05.2025 06:16] Abstract 3. We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models ...
[07.05.2025 06:16] ********************************************************************************
[07.05.2025 06:16] Abstract 4. The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the i...
[07.05.2025 06:16] ********************************************************************************
[07.05.2025 06:16] Abstract 5. The Qwen series has emerged as a leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in de...
[07.05.2025 06:16] ********************************************************************************
[07.05.2025 06:16] Abstract 6. Recent advancements in AI-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. To bridge this gap, we propose a comprehensive framework for holistic soccer understanding. Specifically, we make the following contributio...
[07.05.2025 06:16] ********************************************************************************
[07.05.2025 06:16] Abstract 7. Traditional data presentations typically separate the presenter and visualization into two separate spaces--the 3D world and a 2D screen--enforcing visualization-centric stories. To create a more human-centric viewing experience, we establish a more equitable relationship between the visualization a...
[07.05.2025 06:16] ********************************************************************************
[07.05.2025 06:16] Abstract 8. The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing t...
[07.05.2025 06:16] Read previous papers.
[07.05.2025 06:16] Generating reviews via LLM API.
[07.05.2025 06:16] Using data from previous issue: {"categories": ["#rlhf", "#alignment", "#multimodal", "#training", "#optimization", "#reasoning"], "emoji": "üß†", "ru": {"title": "–£–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω UnifiedReward-Think - –ø–µ—Ä–≤–∞—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º
[07.05.2025 06:16] Using data from previous issue: {"categories": ["#rlhf", "#training", "#rl", "#math", "#optimization", "#reasoning"], "emoji": "ü§ñ", "ru": {"title": "–°–∞–º–æ–æ–±—É—á–∞—é—â–∏–π—Å—è –ò–ò: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ (RLVR) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Absolute
[07.05.2025 06:16] Using data from previous issue: {"categories": ["#open_source", "#transfer_learning", "#multimodal", "#video"], "emoji": "üé≠", "ru": {"title": "–ì–∏–±–∫–∏–π –ø–µ—Ä–µ–Ω–æ—Å –¥–µ–π—Å—Ç–≤–∏–π –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ —Å—É–±—ä–µ–∫—Ç–∞–º–∏ –∏ —Å—Ü–µ–Ω–∞—Ä–∏—è–º–∏", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç FlexiAct - –º–µ—Ç–æ–¥ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –¥–µ–π—Å—Ç–≤–∏–π —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –≤–∏–¥–µ–æ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–µ —Ü–µ–ª–µ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
[07.05.2025 06:16] Using data from previous issue: {"categories": ["#architecture", "#training", "#open_source", "#inference", "#benchmark", "#optimization"], "emoji": "üöÄ", "ru": {"title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏ —Å –ª–∏–Ω–µ–π–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Rapid Attention Distillation to Linear Attention Dec
[07.05.2025 06:16] Using data from previous issue: {"categories": ["#long_context", "#inference", "#benchmark", "#architecture", "#optimization"], "emoji": "üöÄ", "ru": {"title": "RetroInfer: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞ LLM —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º", "desc": "RetroInfer - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏–≤–∞–µ—Ç –∫—ç—à –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∫ —Å–∏—Å—Ç–µ–º—É —Ö—Ä–∞–Ω–µ–Ω
[07.05.2025 06:16] Using data from previous issue: {"categories": ["#inference", "#optimization", "#training", "#open_source", "#low_resource"], "emoji": "üî¨", "ru": {"title": "–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ Qwen3: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –æ—Ü–µ–Ω–∫–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ Qwen3, –æ–¥–Ω–æ–π –∏–∑ –≤–µ–¥—É—â–∏—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –±–æ–ª—å—à–∏—Ö —è
[07.05.2025 06:16] Using data from previous issue: {"categories": ["#open_source", "#survey", "#benchmark", "#dataset", "#reasoning", "#multimodal", "#agents"], "emoji": "‚öΩ", "ru": {"title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ò–ò-–∞–Ω–∞–ª–∏–∑–µ —Ñ—É—Ç–±–æ–ª–∞: –æ—Ç –∑–Ω–∞–Ω–∏–π –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ñ—É—Ç–±–æ–ª–∞ —Å –ø–æ–º–æ—â—å—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞
[07.05.2025 06:16] Using data from previous issue: {"categories": ["#multimodal", "#video"], "emoji": "üé≠", "ru": {"title": "InfoVids: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —á–µ–ª–æ–≤–µ–∫–∞", "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é InfoVids - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–æ–π. –û–Ω–∏ –ø—Ä–∏–∑–≤–∞–Ω—ã —Å–æ–∑–¥–∞—Ç—å –±–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–Ω–æ
[07.05.2025 06:16] Using data from previous issue: {"categories": ["#hallucinations", "#small_models", "#training", "#optimization", "#reasoning"], "emoji": "üîç", "ru": {"title": "–£–º–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ AttenHScore –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã
[07.05.2025 06:16] Loading Chinese text from previous data.
[07.05.2025 06:16] Renaming data file.
[07.05.2025 06:16] Renaming previous data. hf_papers.json to ./d/2025-05-07.json
[07.05.2025 06:16] Saving new data file.
[07.05.2025 06:16] Generating page.
[07.05.2025 06:16] Renaming previous page.
[07.05.2025 06:16] Renaming previous data. index.html to ./d/2025-05-07.html
[07.05.2025 06:16] [Experimental] Generating Chinese page for reading.
[07.05.2025 06:16] Chinese vocab [{'word': 'ËØ≠Èü≥', 'pinyin': 'y«îyƒ´n', 'trans': 'voice'}, {'word': 'AI', 'pinyin': 'ƒìi-√†i', 'trans': 'artificial intelligence'}, {'word': '‰ª£ÁêÜ', 'pinyin': 'd√†il«ê', 'trans': 'agent'}, {'word': 'Ëá™Âä®', 'pinyin': 'z√¨d√≤ng', 'trans': 'automatic'}, {'word': 'ÂÆûÊó∂', 'pinyin': 'sh√≠sh√≠', 'trans': 'real-time'}, {'word': 'ÂØåÊúâ', 'pinyin': 'f√πy«íu', 'trans': 'rich in'}, {'word': 'ÊÉÖÊÑü', 'pinyin': 'q√≠ngg«én', 'trans': 'emotion'}, {'word': '‰∫íÂä®', 'pinyin': 'h√πd√≤ng', 'trans': 'interaction'}, {'word': 'Á´ØÂà∞Á´Ø', 'pinyin': 'duƒÅnd√†oduƒÅn', 'trans': 'end-to-end'}, {'word': 'Êû∂ÊûÑ', 'pinyin': 'ji√†g√≤u', 'trans': 'architecture'}, {'word': 'ÂÆûÁé∞', 'pinyin': 'sh√≠xi√†n', 'trans': 'achieve'}, {'word': '‰ΩéÂª∂Ëøü', 'pinyin': 'dƒ´ y√°nch√≠', 'trans': 'low latency'}, {'word': 'ÂÖ®ÂèåÂ∑•', 'pinyin': 'qu√°n shuƒÅngg≈çng', 'trans': 'full duplex'}, {'word': 'ÂØπËØù', 'pinyin': 'du√¨hu√†', 'trans': 'dialogue'}, {'word': 'ÁªìÂêà', 'pinyin': 'ji√©h√©', 'trans': 'combine'}, {'word': 'Â§ßËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'd√† y«îy√°n m√≥x√≠ng', 'trans': 'large language model'}, {'word': 'Êé®ÁêÜ', 'pinyin': 'tuƒ´l«ê', 'trans': 'reasoning'}, {'word': 'ËÉΩÂäõ', 'pinyin': 'n√©ngl√¨', 'trans': 'ability'}, {'word': 'Âº∫Â§ß', 'pinyin': 'qi√°ngd√†', 'trans': 'powerful'}, {'word': 'Â£∞Â≠¶', 'pinyin': 'shƒìngxu√©', 'trans': 'acoustics'}, {'word': 'Âª∫Ê®°', 'pinyin': 'ji√†nm√≥', 'trans': 'modeling'}, {'word': 'ÊîØÊåÅ', 'pinyin': 'zhƒ´ch√≠', 'trans': 'support'}, {'word': 'Ëá™ÁÑ∂', 'pinyin': 'z√¨r√°n', 'trans': 'natural'}, {'word': 'ÁîüÊàê', 'pinyin': 'shƒìngch√©ng', 'trans': 'generation'}, {'word': 'Ë∂ÖËøá', 'pinyin': 'chƒÅogu√≤', 'trans': 'exceed'}, {'word': 'È¢ÑËÆæ', 'pinyin': 'y√πsh√®', 'trans': 'preset'}, {'word': 'Â£∞Èü≥', 'pinyin': 'shƒìngyƒ´n', 'trans': 'sound'}, {'word': 'È´òÊïà', 'pinyin': 'gƒÅoxi√†o', 'trans': 'efficient'}, {'word': 'ÂÆöÂà∂', 'pinyin': 'd√¨ngzh√¨', 'trans': 'customize'}, {'word': 'Êñ∞', 'pinyin': 'xƒ´n', 'trans': 'new'}]
[07.05.2025 06:16] Renaming previous Chinese page.
[07.05.2025 06:16] Renaming previous data. zh.html to ./d/2025-05-06_zh_reading_task.html
[07.05.2025 06:16] Writing Chinese reading task.
[07.05.2025 06:16] Writing result.
[07.05.2025 06:16] Renaming log file.
[07.05.2025 06:16] Renaming previous data. log.txt to ./logs/2025-05-07_last_log.txt
