[25.02.2026 15:55] Read previous papers.
[25.02.2026 15:55] Generating top page (month).
[25.02.2026 15:55] Writing top page (month).
[25.02.2026 17:02] Read previous papers.
[25.02.2026 17:02] Get feed.
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21193
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.12192
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20739
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21015
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21204
[25.02.2026 17:02] Extract page data from URL. URL: https://huggingface.co/papers/2602.21202
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.14337
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18940
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20309
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16990
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21198
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21185
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20731
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16813
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21201
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21196
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20424
[25.02.2026 17:02] Extract page data from URL. URL: https://huggingface.co/papers/2602.16745
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20945
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20743
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.18735
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.16603
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21053
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.21042
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20903
[25.02.2026 17:02] Get page data from previous paper. URL: https://huggingface.co/papers/2602.20540
[25.02.2026 17:02] Obtaining deleted papers (sometimes HF Daily Papers move some articles from today to past days).
[25.02.2026 17:02] No deleted papers detected.
[25.02.2026 17:02] Downloading and parsing papers (pdf, html). Total: 26.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.21193.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.21193.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.21193.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.12192.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.12192.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.12192.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.20739.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.20739.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.20739.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.21015.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.21015.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.21015.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.21204.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.21204.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.21204.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.21202.
[25.02.2026 17:02] Downloading paper 2602.21202 from https://arxiv.org/pdf/2602.21202v1...
[25.02.2026 17:02] Extracting affiliations from text.
[25.02.2026 17:02] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"Multi-Vector Index Compression in Any Modality Hanxiang Qin hqin14@jhu.edu Johns Hopkins University Baltimore, MD, USA Chunsheng Zuo czuo3@jhu.edu Johns Hopkins University Baltimore, MD, USA Alexander Martin amart233@jhu.edu Johns Hopkins University Baltimore, MD, USA Reno Kriz rkriz1@jhu.edu Johns Hopkins University Baltimore, MD, USA Rohan Jha rjha5@jhu.edu Johns Hopkins University Baltimore, MD, USA Benjamin Van Durme vandurme@jhu.edu Johns Hopkins University Baltimore, MD, USA 6 2 0 2 4 2 ] . [ 1 2 0 2 1 2 . 2 0 6 2 : r a "
[25.02.2026 17:02] Response: ```python
["Johns Hopkins University"]
```
[25.02.2026 17:02] Deleting PDF ./assets/pdf/2602.21202.pdf.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.14337.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.14337.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.14337.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.18940.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.18940.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.18940.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.20309.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.20309.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.20309.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.16990.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.16990.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.16990.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.21198.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.21198.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.21198.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.21185.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.21185.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.21185.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.20731.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.20731.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.20731.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.16813.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.16813.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.16813.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.21201.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.21201.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.21201.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.21196.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.21196.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.21196.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.20424.
[25.02.2026 17:02] Extra JSON file exists (./assets/json/2602.20424.json), skip PDF parsing.
[25.02.2026 17:02] Paper image links file exists (./assets/img_data/2602.20424.json), skip HTML parsing.
[25.02.2026 17:02] Success.
[25.02.2026 17:02] Downloading and parsing paper https://huggingface.co/papers/2602.16745.
[25.02.2026 17:02] Downloading paper 2602.16745 from https://arxiv.org/pdf/2602.16745v1...
[25.02.2026 17:04] Extracting affiliations from text.
[25.02.2026 17:04] Claude request. Model: claude-haiku-4-5. Prompt: I give you a contaminated text with start of ML paper. Extract all authors affiliations as a single institute, firm, company, etc. Return items as a Python plain list only with affiliations. Do not provide commentaries. If there are no affiliations return empty list.

Text:"6 2 0 2 8 1 ] . [ 1 5 4 7 6 1 . 2 0 6 2 : r PETS: Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency Zhangyi Liu4, Huaizhi Qu*1, Xiaowei Yin*1, He Sun3, Yanjun Han2, Tianlong Chen1, and Zhun Deng 1 1University of North Carolina at Chapel Hill 2New York University 3Yale University 4Independent Researcher Abstract Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-Time Self-Consistency), which initiates principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, new measure defined as agreement with the infinite-budget majority vote. This formulation makes sampleefficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at: this https URL. Test-time scaling methods can substantially enhance "
[25.02.2026 17:04] Response: ```python
[
    "University of North Carolina at Chapel Hill",
    "New York University",
    "Yale University"
]
```
[25.02.2026 17:04] Deleting PDF ./assets/pdf/2602.16745.pdf.
[25.02.2026 17:04] Success.
[25.02.2026 17:04] Downloading and parsing paper https://huggingface.co/papers/2602.20945.
[25.02.2026 17:04] Extra JSON file exists (./assets/json/2602.20945.json), skip PDF parsing.
[25.02.2026 17:04] Paper image links file exists (./assets/img_data/2602.20945.json), skip HTML parsing.
[25.02.2026 17:04] Success.
[25.02.2026 17:04] Downloading and parsing paper https://huggingface.co/papers/2602.20743.
[25.02.2026 17:04] Extra JSON file exists (./assets/json/2602.20743.json), skip PDF parsing.
[25.02.2026 17:04] Paper image links file exists (./assets/img_data/2602.20743.json), skip HTML parsing.
[25.02.2026 17:04] Success.
[25.02.2026 17:04] Downloading and parsing paper https://huggingface.co/papers/2602.18735.
[25.02.2026 17:04] Extra JSON file exists (./assets/json/2602.18735.json), skip PDF parsing.
[25.02.2026 17:04] Paper image links file exists (./assets/img_data/2602.18735.json), skip HTML parsing.
[25.02.2026 17:04] Success.
[25.02.2026 17:04] Downloading and parsing paper https://huggingface.co/papers/2602.16603.
[25.02.2026 17:04] Extra JSON file exists (./assets/json/2602.16603.json), skip PDF parsing.
[25.02.2026 17:04] Paper image links file exists (./assets/img_data/2602.16603.json), skip HTML parsing.
[25.02.2026 17:04] Success.
[25.02.2026 17:04] Downloading and parsing paper https://huggingface.co/papers/2602.21053.
[25.02.2026 17:04] Extra JSON file exists (./assets/json/2602.21053.json), skip PDF parsing.
[25.02.2026 17:04] Paper image links file exists (./assets/img_data/2602.21053.json), skip HTML parsing.
[25.02.2026 17:04] Success.
[25.02.2026 17:04] Downloading and parsing paper https://huggingface.co/papers/2602.21042.
[25.02.2026 17:04] Extra JSON file exists (./assets/json/2602.21042.json), skip PDF parsing.
[25.02.2026 17:04] Paper image links file exists (./assets/img_data/2602.21042.json), skip HTML parsing.
[25.02.2026 17:04] Success.
[25.02.2026 17:04] Downloading and parsing paper https://huggingface.co/papers/2602.20903.
[25.02.2026 17:04] Extra JSON file exists (./assets/json/2602.20903.json), skip PDF parsing.
[25.02.2026 17:04] Paper image links file exists (./assets/img_data/2602.20903.json), skip HTML parsing.
[25.02.2026 17:04] Success.
[25.02.2026 17:04] Downloading and parsing paper https://huggingface.co/papers/2602.20540.
[25.02.2026 17:04] Extra JSON file exists (./assets/json/2602.20540.json), skip PDF parsing.
[25.02.2026 17:04] Paper image links file exists (./assets/img_data/2602.20540.json), skip HTML parsing.
[25.02.2026 17:04] Success.
[25.02.2026 17:04] Enriching papers with extra data.
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 0. Abstract Researchers developed a synthetic task generation pipeline and analyzed data strategies to improve terminal agent performance, creating a large-scale dataset and models that outperform larger counterparts on benchmark tests.  					AI-generated summary Despite rapid recent progress in the te...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 1. Abstract A lightweight reranking framework uses attention scores from selected heads to estimate passage-query relevance, achieving strong performance across multiple domains and benchmarks.  					AI-generated summary Built upon the existing analysis of retrieval heads in large language models, we p...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 2. Abstract PyVision-RL framework addresses interaction collapse in multimodal models through enhanced reinforcement learning techniques and efficient video processing strategies.  					AI-generated summary Reinforcement learning for agentic multimodal models often suffers from interaction collapse, wh...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 3. Abstract Current vision-language models lack capability to understand physical structures and causal constraints needed for complex, interactive 3D tasks, as demonstrated by the CHAIN benchmark evaluating structured action planning under physical constraints.  					AI-generated summary Understanding...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 4. Abstract Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.  					AI-generated summary Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of onlin...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 5. Abstract Attention-guided clustering method for compressing multi-vector document representations in late interaction retrieval tasks shows superior performance compared to other compression techniques across text, visual-document, and video modalities.  					AI-generated summary We study efficient ...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 6. Abstract LongCLI-Bench evaluates AI agents' ability to complete complex, multi-step programming tasks through command-line interfaces with detailed failure analysis and human-agent collaboration insights.  					AI-generated summary Recent advances in AI-assisted programming have empowered agents to ...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 7. Abstract Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 8. Abstract QuantVLA is a post-training quantization framework for vision-language-action models that enables efficient deployment through selective quantization, attention temperature matching, and output head balancing while maintaining performance and reducing memory and latency.  					AI-generated ...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 9. Abstract A new conversational financial recommendation benchmark evaluates large language models' ability to balance rational decision-making with user behavior alignment using multi-view references derived from real market data and human decision trajectories.  					AI-generated summary Most recomm...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 10. Abstract Reflective Test-Time Planning enhances robot decision-making by integrating multiple reflection mechanisms that enable learning from experience and improving long-horizon task performance.  					AI-generated summary Embodied LLMs endow robots with high-level task reasoning, but they cannot ...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 11. Abstract Discrete diffusion models with predictor-corrector samplers surpass traditional methods in generation quality and efficiency, challenging assumptions about masked diffusion's necessity in language modeling.  					AI-generated summary Uniform-state discrete diffusion models excel at few-step...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 12. Abstract COMiT framework learns structured discrete visual tokens through iterative encoding and flow-matching decoding, improving object-centric representation and compositional generalization.  					AI-generated summary Discrete image tokenizers have emerged as a key component of modern vision and...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 13. Abstract Flow-based language models outperform discrete diffusion models in both quality and speed by using Euclidean denoising over one-hot token encodings with improved training stability through time reparameterization.  					AI-generated summary Language models based on discrete diffusion have a...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 14. Abstract  We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to ...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 15. Abstract UPipe enables efficient processing of long sequences in Transformer models through fine-grained chunking at the attention head level, significantly reducing activation memory usage while maintaining training speed.  					AI-generated summary Efficiently processing long sequences with Transf...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 16. Abstract AI agents struggle to interpret implicitly specified real-world requests that require contextual reasoning beyond explicit instructions, as demonstrated by an evaluation framework using interactive YAML-defined worlds.  					AI-generated summary Real-world requests to AI agents are fundamen...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 17. Abstract Principled and efficient test-time self-consistency method improves model performance by optimizing trajectory allocation through an optimization framework that reduces sampling requirements while maintaining accuracy.  					AI-generated summary Test-time scaling can improve model performan...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 18. Abstract Large language models benefit from scaled chain-of-thought reasoning through efficient training methods that balance trajectory length and accuracy using reinforcement learning with reward shaping.  					AI-generated summary Large Language Models (LLMs) consistently benefit from scaled Chai...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 19. Abstract Adaptive text anonymization framework automatically adjusts anonymization strategies based on privacy-utility requirements using prompt optimization for language models across diverse domains and constraints.  					AI-generated summary Anonymizing textual documents is a highly context-sensi...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 20. Abstract LaS-Comp presents a zero-shot 3D shape completion method using 3D foundation models with a two-stage approach for faithful reconstruction and seamless boundary refinement.  					AI-generated summary This paper introduces LaS-Comp, a zero-shot and category-agnostic approach that leverages th...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 21. Abstract FlowPrefill addresses head-of-line blocking in large language model serving by decoupling preemption granularity from scheduling frequency through operator-level preemption and event-driven scheduling, achieving up to 5.6x better goodput than existing systems.  					AI-generated summary The...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 22. Abstract A novel iterative self-correction framework enhances vision-language models' reasoning robustness through capability reflection and memory reflection mechanisms, achieving superior performance on visual understanding benchmarks.  					AI-generated summary Large Vision-Language Models (VLMs)...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 23. Abstract OmniOCR presents a universal framework for ethnic minority scripts using Dynamic LoRA and sparsity regularization to achieve state-of-the-art accuracy with improved parameter efficiency in low-resource settings.  					AI-generated summary Optical character recognition (OCR) has advanced rap...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 24. Abstract TextPecker enhances visual text rendering by addressing structural anomalies through a reinforcement learning approach that improves text-to-image generation quality and fidelity.  					AI-generated summary Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation...
[25.02.2026 17:04] ********************************************************************************
[25.02.2026 17:04] Abstract 25. Abstract A collaborative framework integrating generative artificial intelligence with machine learning improves container dwell time prediction by standardizing unstructured text data, leading to reduced rehandling operations in container terminals.  					AI-generated summary Import container dwell...
[25.02.2026 17:04] Read previous papers.
[25.02.2026 17:04] Generating reviews via LLM API.
[25.02.2026 17:04] Using data from previous issue: {"categories": ["#open_source", "#agents", "#benchmark", "#long_context", "#training", "#synthetic", "#dataset", "#data"], "emoji": "‚öôÔ∏è", "ru": {"title": "–ò—Å–∫—É—Å—Å—Ç–≤–æ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö: –∫–∞–∫ –º–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ —É—á–∞—Ç –∫–æ–º–∞–Ω–¥–æ–≤–∞—Ç—å —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–º", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫–æ–Ω–≤–µ–π–µ—Ä —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞
[25.02.2026 17:04] Using data from previous issue: {"categories": ["#small_models", "#training", "#rag", "#benchmark"], "emoji": "üîÑ", "ru": {"title": "–õ—ë–≥–∫–æ–µ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è –±–µ–∑ –º–∞—Å—à—Ç–∞–±–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –ª—ë–≥–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ü–µ–Ω–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏–∑ –≤—ã–±—Ä–∞–Ω
[25.02.2026 17:04] Using data from previous issue: {"categories": ["#open_source", "#agents", "#multimodal", "#rl", "#reasoning", "#video", "#training", "#optimization"], "emoji": "üé¨", "ru": {"title": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è PyVision-RL ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è 
[25.02.2026 17:04] Using data from previous issue: {"categories": ["#agents", "#multimodal", "#robotics", "#benchmark", "#3d", "#cv"], "emoji": "üß©", "ru": {"title": "–§–∏–∑–∏–∫–∞ –¥–µ–π—Å—Ç–≤–∏–π: –æ—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∫ –ø—Ä–∏—á–∏–Ω–Ω–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ CHAIN –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ –ø–æ–Ω–∏–º–∞—Ç—å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –ø—Ä–∏
[25.02.2026 17:04] Using data from previous issue: {"categories": [], "emoji": "‚ö°", "ru": {"title": "Test-time –æ–±—É—á–µ–Ω–∏–µ –∫–∞–∫ –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤–º–µ—Å—Ç–æ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª—è–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥ test-time training (–æ–±—É—á–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è) –∫–∞–∫ –∏–∑—É—á–µ–Ω–Ω–æ–µ –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –∞ –Ω–µ –∫–∞–∫ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —à–∏—Ä–æ–∫–∏–π –∫–ª–∞—Å—Å TTT
[25.02.2026 17:04] Querying the API.
[25.02.2026 17:04] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract Attention-guided clustering method for compressing multi-vector document representations in late interaction retrieval tasks shows superior performance compared to other compression techniques across text, visual-document, and video modalities.  					AI-generated summary We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.
[25.02.2026 17:04] Response: ```json
{
  "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–Ω–æ–≥–æ–≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–∏—Å–∫–∞ —Å –ø–æ–∑–¥–Ω–∏–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º –∑–∞–ø—Ä–æ—Å–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —á–µ—Ç—ã—Ä–µ –º–µ—Ç–æ–¥–∞ —Å–∂–∞—Ç–∏—è –∏–Ω–¥–µ–∫—Å–∞, –≤–∫–ª—é—á–∞—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–Ω–∏–º–∞–Ω–∏—è (attention-guided clustering), –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã—Ö —á–∞—Å—Ç–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Å–∂–∞—Ç–∏—è –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö (BEIR), –≤–∏–∑—É–∞–ª—å–Ω–æ-–¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã—Ö (ViDoRe) –∏ –≤–∏–¥–µ–æ (MSR-VTT, MultiVENT 2.0) –∑–∞–¥–∞—á –ø–æ–∏—Å–∫–∞. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–π –∏–ª–∏ –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –Ω–µ—Å–∂–∞—Ç—ã–º –∏–Ω–¥–µ–∫—Å–æ–º –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –∑–∞—Ç—Ä–∞—Ç –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.",
  "emoji": "üóúÔ∏è",
  "title": "–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –º–Ω–æ–≥–æ–≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è"
}
```
[25.02.2026 17:04] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Attention-guided clustering method for compressing multi-vector document representations in late interaction retrieval tasks shows superior performance compared to other compression techniques across text, visual-document, and video modalities.  					AI-generated summary We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press."

[25.02.2026 17:04] Response: ```python
['RAG', 'MULTIMODAL', 'INFERENCE', 'BENCHMARK']
```
[25.02.2026 17:04] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Attention-guided clustering method for compressing multi-vector document representations in late interaction retrieval tasks shows superior performance compared to other compression techniques across text, visual-document, and video modalities.  					AI-generated summary We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press."

[25.02.2026 17:04] Response: ```python
['OPTIMIZATION', 'OPEN_SOURCE']
```
[25.02.2026 17:04] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper presents an attention-guided clustering method aimed at improving the efficiency of multi-vector document retrieval in late interaction tasks. The authors address the challenge of high computation and storage costs associated with long documents by proposing four compression techniques, with a focus on the novel attention-guided clustering (AGC) approach. AGC identifies key semantic areas in documents to create cluster centroids, enhancing the aggregation of tokens for better retrieval performance. The results demonstrate that AGC outperforms traditional compression methods across various modalities, including text, images, and videos, while maintaining competitive retrieval accuracy.","title":"Efficient Retrieval with Attention-Guided Clustering"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='This paper presents an attention-guided clustering method aimed at improving the efficiency of multi-vector document retrieval in late interaction tasks. The authors address the challenge of high computation and storage costs associated with long documents by proposing four compression techniques, with a focus on the novel attention-guided clustering (AGC) approach. AGC identifies key semantic areas in documents to create cluster centroids, enhancing the aggregation of tokens for better retrieval performance. The results demonstrate that AGC outperforms traditional compression methods across various modalities, including text, images, and videos, while maintaining competitive retrieval accuracy.', title='Efficient Retrieval with Attention-Guided Clustering'))
[25.02.2026 17:05] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ≥®ÊÑèÂäõÂºïÂØºÁöÑËÅöÁ±ªÊñπÊ≥ïÔºåÁî®‰∫éÂú®ÂêéÊúü‰∫§‰∫íÊ£ÄÁ¥¢‰ªªÂä°‰∏≠ÂéãÁº©Â§öÂêëÈáèÊñáÊ°£Ë°®Á§∫„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËØÜÂà´ÊñáÊ°£‰∏≠ÊúÄÂÖ∑ËØ≠‰πâÈáçË¶ÅÊÄßÁöÑÂå∫Âüü‰Ωú‰∏∫ËÅöÁ±ª‰∏≠ÂøÉÔºåÂπ∂Âä†ÊùÉ‰ª§ÁâåËÅöÂêàÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ£ÄÁ¥¢ÊïàÁéá„ÄÇÊàë‰ª¨Âú®ÊñáÊú¨„ÄÅËßÜËßâÊñáÊ°£ÂíåËßÜÈ¢ëÁ≠âÂ§öÁßçÊ®°ÊÄÅÁöÑÊ£ÄÁ¥¢‰ªªÂä°‰∏≠ËØÑ‰º∞‰∫ÜËØ•ÊñπÊ≥ïÔºåÁªìÊûúÊòæÁ§∫ÂÖ∂ÊÄßËÉΩ‰ºò‰∫éÂÖ∂‰ªñÂéãÁº©ÊäÄÊúØ„ÄÇÊ≥®ÊÑèÂäõÂºïÂØºËÅöÁ±ªÊñπÊ≥ïÂú®Á¥¢ÂºïÂ§ßÂ∞è‰∏äÊèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑÁÅµÊ¥ªÊÄßÔºåÂπ∂Âú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÂÆûÁé∞‰∫Ü‰∏éÂÆåÊï¥Êú™ÂéãÁº©Á¥¢ÂºïÁõ∏ÂΩìÊàñÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ","title":"Ê≥®ÊÑèÂäõÂºïÂØºËÅöÁ±ªÔºöÊèêÂçáÂ§öÂêëÈáèÊñáÊ°£Ê£ÄÁ¥¢ÊïàÁéá"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ≥®ÊÑèÂäõÂºïÂØºÁöÑËÅöÁ±ªÊñπÊ≥ïÔºåÁî®‰∫éÂú®ÂêéÊúü‰∫§‰∫íÊ£ÄÁ¥¢‰ªªÂä°‰∏≠ÂéãÁº©Â§öÂêëÈáèÊñáÊ°£Ë°®Á§∫„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËØÜÂà´ÊñáÊ°£‰∏≠ÊúÄÂÖ∑ËØ≠‰πâÈáçË¶ÅÊÄßÁöÑÂå∫Âüü‰Ωú‰∏∫ËÅöÁ±ª‰∏≠ÂøÉÔºåÂπ∂Âä†ÊùÉ‰ª§ÁâåËÅöÂêàÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ£ÄÁ¥¢ÊïàÁéá„ÄÇÊàë‰ª¨Âú®ÊñáÊú¨„ÄÅËßÜËßâÊñáÊ°£ÂíåËßÜÈ¢ëÁ≠âÂ§öÁßçÊ®°ÊÄÅÁöÑÊ£ÄÁ¥¢‰ªªÂä°‰∏≠ËØÑ‰º∞‰∫ÜËØ•ÊñπÊ≥ïÔºåÁªìÊûúÊòæÁ§∫ÂÖ∂ÊÄßËÉΩ‰ºò‰∫éÂÖ∂‰ªñÂéãÁº©ÊäÄÊúØ„ÄÇÊ≥®ÊÑèÂäõÂºïÂØºËÅöÁ±ªÊñπÊ≥ïÂú®Á¥¢ÂºïÂ§ßÂ∞è‰∏äÊèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑÁÅµÊ¥ªÊÄßÔºåÂπ∂Âú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÂÆûÁé∞‰∫Ü‰∏éÂÆåÊï¥Êú™ÂéãÁº©Á¥¢ÂºïÁõ∏ÂΩìÊàñÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ', title='Ê≥®ÊÑèÂäõÂºïÂØºËÅöÁ±ªÔºöÊèêÂçáÂ§öÂêëÈáèÊñáÊ°£Ê£ÄÁ¥¢ÊïàÁéá'))
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#agents", "#plp", "#long_context", "#benchmark"], "emoji": "üõ†Ô∏è", "ru": {"title": "–≠—Ç–∞–ª–æ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π AI-–∞–≥–µ–Ω—Ç–æ–≤ –∫ –¥–ª–∏–Ω–Ω–æ–≥–æ—Ä–∏–∑–æ–Ω—Ç–Ω–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω LongCLI-Bench ‚Äî —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω—è
[25.02.2026 17:05] Using data from previous issue: {"categories": [], "emoji": "üîç", "ru": {"title": "–ê–≥–µ–Ω—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å—Ç–∏–Ω—ã –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–∞—Ö", "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç—á—ë—Ç–æ–≤, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö Deep Research Agents. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–π –±–µ–≥–ª–æ—Å—Ç–∏ —Ç–µ
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#inference", "#multimodal", "#open_source", "#robotics", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏", "desc": "QuantVLA ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ—Å—Ç–æ–±—É—á–∞—é—â–µ–≥–æ—Å—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–µ–π vision-language-action, –∫–æ—Ç–æ
[25.02.2026 17:05] Using data from previous issue: {"categories": [], "emoji": "üìà", "ru": {"title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –∏ –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö LLM", "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Conv-FinRe ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–∞–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∞–∫—Ü–∏—è–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#agents", "#robotics", "#dataset", "#benchmark"], "emoji": "ü§ñ", "ru": {"title": "–û–±—É—á–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞ —á–µ—Ä–µ–∑ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å –º–∞—à–∏–Ω—É —É—á–∏—Ç—å—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö", "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Reflective Test-Time Planning, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π —Ä–æ–±–æ—Ç–∞ –ø—É—Ç—ë–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –º–µ—Ö–∞–Ω–∏
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#diffusion", "#optimization", "#open_source"], "emoji": "üîÑ", "ru": {"title": "Predictor-Corrector —Å—ç–º–ø–ª–µ—Ä—ã –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ —è–∑—ã–∫–æ–≤–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã Predictor-Corrector –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#multimodal", "#interpretability", "#reasoning", "#architecture", "#cv", "#training"], "emoji": "üß©", "ru": {"title": "–ö–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –æ–±—ä–µ–∫—Ç-—Ü–µ–Ω—Ç—Ä–∏—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", "desc": "COMiT - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Ç–æ
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#diffusion", "#architecture", "#open_source", "#training", "#optimization"], "emoji": "üåä", "ru": {"title": "–ü–æ—Ç–æ–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–±–µ–¥–∏–ª–∏ –¥–∏—Å–∫—Ä–µ—Ç–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é –≤ —è–∑—ã–∫–æ–≤–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å —è–∑—ã–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Ç–æ–∫–æ–≤ (FLM), –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é –¥–µ–Ω–æ
[25.02.2026 17:05] Using data from previous issue: {"categories": [], "emoji": "üßÆ", "ru": {"title": "–ú–∞—à–∏–Ω—ã –º–æ–≥—É—Ç –¥–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ç–µ–æ—Ä–µ–º—ã: –∞–≥–µ–Ω—Ç Aletheia —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏", "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞ Aletheia, —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Gemini 3 Deep Think, –Ω–∞ –∫–æ–Ω–∫—É—Ä—Å–µ FirstProof. –ê–≥–µ–Ω—Ç –∞–≤—Ç–æ–Ω–æ–º–Ω–æ —Ä–µ—à–∏–ª 6 –∏–∑ 10
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#inference", "#long_context", "#architecture", "#training", "#optimization"], "emoji": "‚ö°", "ru": {"title": "–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è", "desc": "UPipe ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–±–∏–≤–∞–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#agents", "#reasoning", "#alignment", "#benchmark"], "emoji": "üéØ", "ru": {"title": "–û—Ç —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é: –æ—Ü–µ–Ω–∫–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ AI –∞–≥–µ–Ω—Ç–æ–≤", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ AI –∞–≥–µ–Ω—Ç–æ–≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—è–≤–Ω–æ –∑–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ
[25.02.2026 17:05] Querying the API.
[25.02.2026 17:05] Claude request. Model: claude-haiku-4-5. Prompt: Read an abstract of the ML paper and return a JSON with fields: 'desc': explanation of the paper in Russian (4 sentences), use correct machine learning terms. 'emoji': emoji that will reflect the theme of an article somehow, only one emoji. 'title': a slogan of a main idea of the article in Russian. Return only JSON and nothing else.

Abstract Principled and efficient test-time self-consistency method improves model performance by optimizing trajectory allocation through an optimization framework that reduces sampling requirements while maintaining accuracy.  					AI-generated summary Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS.
[25.02.2026 17:05] Response: ```json
{
  "desc": "–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç PETS ‚Äî –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –º–µ—Ç—Ä–∏–∫—É self-consistency rate, –∫–æ—Ç–æ—Ä–∞—è –∏–∑–º–µ—Ä—è–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–º –±—é–¥–∂–µ—Ç–æ–º –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–¥–µ–ª–∞—Ç—å –≤—ã–±–æ—Ä–∫—É —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π. –ü–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –¥–≤–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏: –æ—Ñ–ª–∞–π–Ω-—Ä–µ–∂–∏–º, –≥–¥–µ –≤–æ–ø—Ä–æ—Å—ã –∏–∑–≤–µ—Å—Ç–Ω—ã –∑–∞—Ä–∞–Ω–µ–µ –∏ –∑–∞–¥–∞—á–∞ —Å–≤—è–∑–∞–Ω–∞ —Å —Ç–µ–æ—Ä–∏–µ–π –∫—Ä–∞—É–¥—Å–æ—Ä—Å–∏–Ω–≥–∞, –∏ –æ–Ω–ª–∞–π–Ω-—Ä–µ–∂–∏–º, –≥–¥–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ—Å—Ç—É–ø–∞—é—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –∏ –∞–ª–≥–æ—Ä–∏—Ç–º –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –±—é–¥–∂–µ—Ç –∫ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ PETS –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç —Ç—Ä–µ–±—É–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (–¥–æ 75% –≤ –æ—Ñ–ª–∞–π–Ω –∏ 55% –≤ –æ–Ω–ª–∞–π–Ω —Ä–µ–∂–∏–º–µ) –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–ª–∏ —É–ª—É—á—à–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏.",
  "emoji": "üéØ",
  "title": "–£–º–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: —Å–Ω–∏–∂–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞"
}
```
[25.02.2026 17:05] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

DATASET: Papers that introduce new datasets or make significant modifications to existing ones
DATA: Papers focusing on data processing, cleaning, collection, or curation methodologies
BENCHMARK: Papers proposing or analyzing model evaluation frameworks and benchmarks
AGENTS: Papers exploring autonomous agents, web agents, or agent-based architectures
CV: Papers developing computer vision methods or visual processing systems
RL: Papers investigating reinforcement learning theory or applications
RLHF: Papers specifically about human feedback in RL (PPO, DPO, etc.)
RAG: Papers advancing retrieval-augmented generation techniques
PLP: Papers about Programming Language Processing models or programming benchmarks
INFERENCE: Papers optimizing model deployment (quantization, pruning, etc.)
3D: Papers on 3D content generation, processing, or understanding
AUDIO: Papers advancing speech/audio processing or generation
VIDEO: Papers on video analysis, generation, or understanding
MULTIMODAL: Papers combining multiple input/output modalities
MATH: Papers focused on mathematical theory and algorithms
MULTILINGUAL: Papers addressing multiple languages or cross-lingual capabilities, including all non English models
ARCHITECTURE: Papers proposing novel neural architectures or components
HEALTHCARE: Papers applying ML to medical/healthcare domains
TRAINING: Papers improving model training or fine-tuning methods
ROBOTICS: Papers on robotic systems and embodied AI
SMALL_MODELS: Papers that describe models considering small, below 1 billion parameters or similar 

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Principled and efficient test-time self-consistency method improves model performance by optimizing trajectory allocation through an optimization framework that reduces sampling requirements while maintaining accuracy.  					AI-generated summary Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS."

[25.02.2026 17:05] Response: ```python
["BENCHMARK", "TRAINING"]
```

**Justification:**

- **BENCHMARK**: The paper proposes an evaluation framework (PETS) for optimizing test-time model performance through principled trajectory allocation. It introduces a new metric (self-consistency rate) and evaluates performance on benchmarks like GPQA, which aligns with analyzing model evaluation frameworks.

- **TRAINING**: The paper focuses on improving model performance through test-time scaling and optimization methods, which relates to enhancing how models are evaluated and optimized during inference/deployment, falling under training and fine-tuning methodologies.
[25.02.2026 17:05] Error. Failed to parse JSON from LLM. ["BENCHMARK", "TRAINING"]


**Justification:**

- **BENCHMARK**: The paper proposes an evaluation framework (PETS) for optimizing test-time model performance through principled trajectory allocation. It introduces a new metric (self-consistency rate) and evaluates performance on benchmarks like GPQA, which aligns with analyzing model evaluation frameworks.

- **TRAINING**: The paper focuses on improving model performance through test-time scaling and optimization methods, which relates to enhancing how models are evaluated and optimized during inference/deployment, falling under training and fine-tuning methodologies.
[25.02.2026 17:05] Claude request. Model: claude-haiku-4-5. Prompt: Analyze the following research paper text and classify it into one or more relevant topics from the list below. Consider only information from the provided text. Don't add a tag if the topic is not directly related to the article.

Topics:

AGI: Papers discussing artificial general intelligence concepts
GAMES: Papers applying ML to games or game development
INTERPRETABILITY: Papers analyzing model behavior and explanations
REASONING: Papers enhancing logical reasoning capabilities
TRANSFER_LEARNING: Papers on knowledge transfer between models/domains
GRAPHS: Papers advancing graph neural networks and applications
ETHICS: Papers addressing AI ethics, fairness, and bias
SECURITY: Papers on model security and adversarial robustness
OPTIMIZATION: Papers advancing training optimization methods
SURVEY: Papers comprehensively reviewing research areas
DIFFUSION: Papers on diffusion-based generative models
ALIGNMENT: Papers about aligning language models with human values, preferences, and intended behavior
STORY_GENERATION: Papers on story generation, including plot generation and author style adaptation
HALLUCINATIONS: Papers about the hallucinations, hallucinations analysis and mitigation
LONG_CONTEXT: Papers about long context handling, including techniques to extend context length
SYNTHETIC: Papers about using synthetic data for training, including methods for generating and leveraging artificial data
TRANSLATION: Papers on machine translation, including techniques, data and applications for translating between languages
LEAKAGE: Papers about data leakage, including issues of unintended data exposure and methods to detect or prevent it
OPEN_SOURCE: Papers that contribute to open-source projects by releasing models, datasets, or frameworks to the public
SCIENCE: Papers on scientific applications of LM including understanding of science articles and research automatization
LOW_RESOURCE: Papers that mention low-resource languages

Return only a Python flat list of topics that match the given text.

Paper text to classify:

"Abstract Principled and efficient test-time self-consistency method improves model performance by optimizing trajectory allocation through an optimization framework that reduces sampling requirements while maintaining accuracy.  					AI-generated summary Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS."

[25.02.2026 17:05] Response: ```python
['OPTIMIZATION', 'REASONING', 'OPEN_SOURCE']
```
[25.02.2026 17:05] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"This paper presents PETS, a method that enhances model performance by optimizing how resources are allocated during test-time self-consistency. It introduces a new measure called the self-consistency rate, which helps in determining how well the model\'s predictions align with a theoretical ideal of infinite resources. The authors explore both offline and online scenarios, connecting trajectory allocation to established crowdsourcing theories for better efficiency. Experimental results demonstrate that PETS significantly reduces the number of samples needed while maintaining high accuracy, outperforming traditional uniform allocation methods.","title":"Optimizing Test-Time Self-Consistency for Efficient Model Performance"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc="This paper presents PETS, a method that enhances model performance by optimizing how resources are allocated during test-time self-consistency. It introduces a new measure called the self-consistency rate, which helps in determining how well the model's predictions align with a theoretical ideal of infinite resources. The authors explore both offline and online scenarios, connecting trajectory allocation to established crowdsourcing theories for better efficiency. Experimental results demonstrate that PETS significantly reduces the number of samples needed while maintaining high accuracy, outperforming traditional uniform allocation methods.", title='Optimizing Test-Time Self-Consistency for Efficient Model Performance'))
[25.02.2026 17:05] Response: ParsedChatCompletionMessage[~ResponseFormatT](content='{"desc":"Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫PETSÁöÑÊµãËØïÊó∂Ëá™‰∏ÄËá¥ÊÄßÊñπÊ≥ïÔºåÈÄöËøá‰ºòÂåñËΩ®ËøπÂàÜÈÖçÊù•ÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÂú®ÊúâÈôêÈ¢ÑÁÆó‰∏ãÂÆûÁé∞‰∫ÜÊ†∑Êú¨È´òÊïàÁöÑËá™‰∏ÄËá¥ÊÄßÔºåÂà©Áî®Êñ∞ÁöÑËá™‰∏ÄËá¥ÊÄßÁéáÂ∫¶ÈáèÊù•ËØÑ‰º∞Ê®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨Â∞ÜËΩ®ËøπÂàÜÈÖç‰∏é‰ºóÂåÖÊ®°ÂûãÁõ∏ÁªìÂêàÔºåÊèê‰æõ‰∫ÜÁêÜËÆ∫‰øùËØÅÂíåÈ´òÊïàÁöÑÂàÜÈÖçÁÆóÊ≥ï„ÄÇÂú®Âú®Á∫øÊµÅÂºèËÆæÁΩÆ‰∏≠ÔºåPETSÊ†πÊçÆÈóÆÈ¢òÈöæÂ∫¶Âä®ÊÄÅË∞ÉÊï¥È¢ÑÁÆóÔºå‰øùÊåÅ‰∫ÜÂº∫Â§ßÁöÑÁêÜËÆ∫‰øùËØÅÂíåËÆ°ÁÆóÊïàÁéá„ÄÇ","title":"‰ºòÂåñËΩ®ËøπÂàÜÈÖçÔºåÊèêÂçáÊ®°ÂûãÊÄßËÉΩ"}', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None, parsed=Article(desc='Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫PETSÁöÑÊµãËØïÊó∂Ëá™‰∏ÄËá¥ÊÄßÊñπÊ≥ïÔºåÈÄöËøá‰ºòÂåñËΩ®ËøπÂàÜÈÖçÊù•ÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÂú®ÊúâÈôêÈ¢ÑÁÆó‰∏ãÂÆûÁé∞‰∫ÜÊ†∑Êú¨È´òÊïàÁöÑËá™‰∏ÄËá¥ÊÄßÔºåÂà©Áî®Êñ∞ÁöÑËá™‰∏ÄËá¥ÊÄßÁéáÂ∫¶ÈáèÊù•ËØÑ‰º∞Ê®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨Â∞ÜËΩ®ËøπÂàÜÈÖç‰∏é‰ºóÂåÖÊ®°ÂûãÁõ∏ÁªìÂêàÔºåÊèê‰æõ‰∫ÜÁêÜËÆ∫‰øùËØÅÂíåÈ´òÊïàÁöÑÂàÜÈÖçÁÆóÊ≥ï„ÄÇÂú®Âú®Á∫øÊµÅÂºèËÆæÁΩÆ‰∏≠ÔºåPETSÊ†πÊçÆÈóÆÈ¢òÈöæÂ∫¶Âä®ÊÄÅË∞ÉÊï¥È¢ÑÁÆóÔºå‰øùÊåÅ‰∫ÜÂº∫Â§ßÁöÑÁêÜËÆ∫‰øùËØÅÂíåËÆ°ÁÆóÊïàÁéá„ÄÇ', title='‰ºòÂåñËΩ®ËøπÂàÜÈÖçÔºåÊèêÂçáÊ®°ÂûãÊÄßËÉΩ'))
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#rl", "#training", "#benchmark"], "emoji": "üß†", "ru": {"title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è LLM —á–µ—Ä–µ–∑ RL", "desc": "–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Chain-of-Thought) —á–µ—Ä–µ–∑ –æ–±
[25.02.2026 17:05] Using data from previous issue: {"categories": [], "emoji": "üîê", "ru": {"title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–æ–º–ø—Ç–æ–≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π", "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞—â–∏—Ç—ã –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#benchmark", "#dataset", "#3d"], "emoji": "üß©", "ru": {"title": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Ñ–æ—Ä–º –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏", "desc": "LaS-Comp –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Ñ–æ—Ä–º —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ 3D –º–æ–¥–µ–ª–∏ —Å –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º.
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#optimization", "#inference"], "emoji": "‚ö°", "ru": {"title": "–†–∞—Å—Ü–µ–ø–ª–µ–Ω–∏—é –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è –æ—Ç —á–∞—Å—Ç–æ—Ç—ã –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è ‚Äî –ø–æ–±–µ–¥–∞ –Ω–∞–¥ –æ—á–µ—Ä–µ–¥–Ω–æ–π –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π", "desc": "FlowPrefill ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –æ—á–µ—Ä–µ–¥–∏ –∑–∞
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#reasoning", "#agents", "#multimodal", "#interpretability", "#open_source", "#benchmark", "#cv", "#training"], "emoji": "üîÑ", "ru": {"title": "–°–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è —á–µ—Ä–µ–∑ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é: –∫–∞–∫ –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö", "desc": "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#multilingual", "#small_models", "#transfer_learning", "#low_resource", "#optimization", "#open_source", "#cv", "#training"], "emoji": "üß©", "ru": {"title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö –ø–∏—Å—å–º–µ–Ω–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é", "desc": "OmniOCR –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ—Ä–µ
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#cv", "#multimodal", "#dataset", "#rl"], "emoji": "‚úçÔ∏è", "ru": {"title": "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –¥–µ—Ñ–µ–∫—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º", "desc": "TextPecker ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
[25.02.2026 17:05] Using data from previous issue: {"categories": ["#optimization"], "emoji": "üö¢", "ru": {"title": "Generative AI —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –≤ –ø–æ—Ä—Ç–∞—Ö", "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ, –æ–±—ä–µ–¥–∏–Ω—è—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π AI
[25.02.2026 17:05] Renaming data file.
[25.02.2026 17:05] Renaming previous data. hf_papers.json to ./d/2026-02-25.json
[25.02.2026 17:05] Saving new data file.
[25.02.2026 17:05] Generating page.
[25.02.2026 17:05] Renaming previous page.
[25.02.2026 17:05] Renaming previous data. index.html to ./d/2026-02-25.html
[25.02.2026 17:05] Writing result.
[25.02.2026 17:05] Renaming log file.
[25.02.2026 17:05] Renaming previous data. log.txt to ./logs/2026-02-25_last_log.txt
